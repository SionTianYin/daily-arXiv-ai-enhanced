<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 52]
- [cs.CV](#cs.CV) [Total: 64]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.LG](#cs.LG) [Total: 53]
- [eess.SY](#eess.SY) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.GR](#cs.GR) [Total: 1]
- [hep-lat](#hep-lat) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.NE](#cs.NE) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [stat.ML](#stat.ML) [Total: 4]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.MA](#cs.MA) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [cs.SD](#cs.SD) [Total: 6]
- [cs.CR](#cs.CR) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs](https://arxiv.org/abs/2509.09699)
*Mingyang Li,Viktor Schlegel,Tingting Mu,Warren Del-Pinto,Goran Nenadic*

Main category: cs.CL

TL;DR: 本文提出使用文档级知识图谱来结构化表示临床文档，用于自动化ICD编码任务，在保持90%信息的同时将文本压缩至23%，并在PLM-ICD架构上实现了最高3.20%的Macro-F1提升。


<details>
  <summary>Details</summary>
Motivation: 临床文档标准化编码对临床研究、医院管理和患者护理至关重要，但人工编码耗时费力。自动化编码面临高维长尾目标空间的挑战，现有方法主要关注输出编码表示，而输入文档的外部知识表示研究不足。

Method: 构建文档级知识图谱来结构化表示输入文档，提供患者状况的全面结构化视图。将这种知识图谱集成到最先进的ICD编码架构PLM-ICD中，评估其在自动化ICD-9编码中的有效性。

Result: 知识图谱仅用原文本23%的篇幅保留了90%的信息。在流行基准测试中，Macro-F1分数最高提升3.20%，同时提高了训练效率。知识图谱中的不同实体和关系类型带来了性能提升。

Conclusion: 基于知识图谱的结构化文档表示方法在自动化ICD编码任务中表现出色，不仅提高了性能指标，还增强了方法的可解释性，相比纯文本基线具有明显优势。

Abstract: Mapping clinical documents to standardised clinical vocabularies is an
important task, as it provides structured data for information retrieval and
analysis, which is essential to clinical research, hospital administration and
improving patient care. However, manual coding is both difficult and
time-consuming, making it impractical at scale. Automated coding can
potentially alleviate this burden, improving the availability and accuracy of
structured clinical data. The task is difficult to automate, as it requires
mapping to high-dimensional and long-tailed target spaces, such as the
International Classification of Diseases (ICD). While external knowledge
sources have been readily utilised to enhance output code representation, the
use of external resources for representing the input documents has been
underexplored. In this work, we compute a structured representation of the
input documents, making use of document-level knowledge graphs (KGs) that
provide a comprehensive structured view of a patient's condition. The resulting
knowledge graph efficiently represents the patient-centred input documents with
23\% of the original text while retaining 90\% of the information. We assess
the effectiveness of this graph for automated ICD-9 coding by integrating it
into the state-of-the-art ICD coding architecture PLM-ICD. Our experiments
yield improved Macro-F1 scores by up to 3.20\% on popular benchmarks, while
improving training efficiency. We attribute this improvement to different types
of entities and relationships in the KG, and demonstrate the improved
explainability potential of the approach over the text-only baseline.

</details>


### [2] [Cross-Layer Attention Probing for Fine-Grained Hallucination Detection](https://arxiv.org/abs/2509.09700)
*Malavika Suresh,Rahaf Aljundi,Ikechukwu Nkisi-Orji,Nirmalie Wiratunga*

Main category: cs.CL

TL;DR: 提出Cross-Layer Attention Probing (CLAP)方法，通过处理LLM残差流中的激活信息来检测幻觉，在多个模型和任务上表现优于基线，支持细粒度检测和检测后缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用，其生成不准确文本（幻觉）的可靠性问题日益突出，需要有效的检测方法来提高模型可靠性。

Method: 提出CLAP激活探测技术，将整个残差流中的激活信息作为联合序列进行处理，用于幻觉检测。

Result: 在5个LLM和3个任务上的实验表明，CLAP在贪婪解码和高温采样响应中都优于基线检测方法，支持细粒度幻觉识别，并能通过检测后缓解策略减少幻觉。

Conclusion: CLAP方法能有效检测大语言模型的幻觉问题，提高模型可靠性，且在分布外场景下仍保持高可靠性。

Abstract: With the large-scale adoption of Large Language Models (LLMs) in various
applications, there is a growing reliability concern due to their tendency to
generate inaccurate text, i.e. hallucinations. In this work, we propose
Cross-Layer Attention Probing (CLAP), a novel activation probing technique for
hallucination detection, which processes the LLM activations across the entire
residual stream as a joint sequence. Our empirical evaluations using five LLMs
and three tasks show that CLAP improves hallucination detection compared to
baselines on both greedy decoded responses as well as responses sampled at
higher temperatures, thus enabling fine-grained detection, i.e. the ability to
disambiguate hallucinations and non-hallucinations among different sampled
responses to a given prompt. This allows us to propose a detect-then-mitigate
strategy using CLAP to reduce hallucinations and improve LLM reliability
compared to direct mitigation approaches. Finally, we show that CLAP maintains
high reliability even when applied out-of-distribution.

</details>


### [3] [Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task](https://arxiv.org/abs/2509.09701)
*JungHo Jung,Junhyun Lee*

Main category: cs.CL

TL;DR: 通过多任务学习模型，结合一致性正则化、R-drop正则化和机器翻译损失系数调节，在语音到文本翻译任务中实现了接近最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决端到端语音到文本翻译中成对语音-文本数据稀缺的问题，利用机器翻译任务的双语文本数据通过多任务学习来提升性能。

Method: 从正则化角度构建多任务学习模型，研究不同模态间的一致性正则化和相同模态内的R-drop正则化，并将机器翻译损失系数作为第三种正则化来源。提出正则化地平线概念来指寻最优调参。

Result: 在MuST-C数据集上达到了接近最先进的性能水平。

Conclusion: 通过系统性地结合三种正则化来源，可以在高维参数空间中找到最优的正则化组合，有效提升语音到文本翻译的性能。

Abstract: End-to-end speech-to-text translation typically suffers from the scarcity of
paired speech-text data. One way to overcome this shortcoming is to utilize the
bitext data from the Machine Translation (MT) task and perform Multi-Task
Learning (MTL). In this paper, we formulate MTL from a regularization
perspective and explore how sequences can be regularized within and across
modalities. By thoroughly investigating the effect of consistency
regularization (different modality) and R-drop (same modality), we show how
they respectively contribute to the total regularization. We also demonstrate
that the coefficient of MT loss serves as another source of regularization in
the MTL setting. With these three sources of regularization, we introduce the
optimal regularization contour in the high-dimensional space, called the
regularization horizon. Experiments show that tuning the hyperparameters within
the regularization horizon achieves near state-of-the-art performance on the
MuST-C dataset.

</details>


### [4] [Creativity Benchmark: A benchmark for marketing creativity for LLM models](https://arxiv.org/abs/2509.09702)
*Ninad Bhat,Kieran Browne,Pip Bingemann*

Main category: cs.CL

TL;DR: 创意性测试基准：一个专门评估大语言模型在营销创意领域表现的框架，通过人类专业评估发现各模型表现紧凑集中且没有显著优势，自动化评分方法与人类评价相差远


<details>
  <summary>Details</summary>
Motivation: 建立一个专门用于评估大语言模型在营销创意领域表现的标准化框架，解决当前缺乏专业化评测工具的问题

Method: 涉及100个品牌（12个类别）和三种提示类型，收集678名专业创意人员对11,012次匿名化对比评价，使用Bradley-Terry模型分析，并通过余弦距离测量模型多样性

Result: 各模型表现紧凑集中，最高分模型仅在61%的情况下胜过最低分模型；自动化评分方法与人类排名相关性弱且不一致；传统创意测试在品牌约束任务中转移效果有限

Conclusion: 强调专业人员评估的必要性，自动化评分方法无法替代人类判断，需要重视模型多样性的工作流程

Abstract: We introduce Creativity Benchmark, an evaluation framework for large language
models (LLMs) in marketing creativity. The benchmark covers 100 brands (12
categories) and three prompt types (Insights, Ideas, Wild Ideas). Human
pairwise preferences from 678 practising creatives over 11,012 anonymised
comparisons, analysed with Bradley-Terry models, show tightly clustered
performance with no model dominating across brands or prompt types: the
top-bottom spread is $\Delta\theta \approx 0.45$, which implies a head-to-head
win probability of $0.61$; the highest-rated model beats the lowest only about
$61\%$ of the time. We also analyse model diversity using cosine distances to
capture intra- and inter-model variation and sensitivity to prompt reframing.
Comparing three LLM-as-judge setups with human rankings reveals weak,
inconsistent correlations and judge-specific biases, underscoring that
automated judges cannot substitute for human evaluation. Conventional
creativity tests also transfer only partially to brand-constrained tasks.
Overall, the results highlight the need for expert human evaluation and
diversity-aware workflows.

</details>


### [5] [CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor](https://arxiv.org/abs/2509.09703)
*Zhenhua Xu,Xixiang Zhao,Xubin Yue,Shengwei Tian,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: CTCC是一种新颖的基于规则的语言模型指纹框架，通过多轮对话中的上下文关联来嵌入所有权验证痕迹，相比现有方法在隐蔽性、鲁棒性和通用性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型广泛部署引发了知识产权保护担忧，现有指纹方法在隐蔽性、鲁棒性和通用性方面存在固有权衡，容易被检测或攻击。

Method: 提出CTCC框架，采用规则驱动方式编码多轮对话中的上下文关联（如反事实关系），而非依赖词级或单轮触发，支持黑盒访问下的指纹验证。

Result: 在多个LLM架构上的广泛实验表明，CTCC相比先前工作 consistently 实现更强的隐蔽性和鲁棒性，有效减少误报和指纹泄露。

Conclusion: CTCC为现实世界LLM部署场景中的所有权验证提供了可靠实用的解决方案，支持在部分触发暴露情况下的持续构建。

Abstract: The widespread deployment of large language models (LLMs) has intensified
concerns around intellectual property (IP) protection, as model theft and
unauthorized redistribution become increasingly feasible. To address this,
model fingerprinting aims to embed verifiable ownership traces into LLMs.
However, existing methods face inherent trade-offs between stealthness,
robustness, and generalizability, being either detectable via distributional
shifts, vulnerable to adversarial modifications, or easily invalidated once the
fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven
fingerprinting framework that encodes contextual correlations across multiple
dialogue turns, such as counterfactual, rather than relying on token-level or
single-turn triggers. CTCC enables fingerprint verification under black-box
access while mitigating false positives and fingerprint leakage, supporting
continuous construction under a shared semantic rule even if partial triggers
are exposed. Extensive experiments across multiple LLM architectures
demonstrate that CTCC consistently achieves stronger stealth and robustness
than prior work. Our findings position CTCC as a reliable and practical
solution for ownership verification in real-world LLM deployment scenarios. Our
code and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.

</details>


### [6] [Temporal Preferences in Language Models for Long-Horizon Assistance](https://arxiv.org/abs/2509.09704)
*Ali Mazyaki,Mohammad Naghizadeh,Samaneh Ranjkhah Zonouzaghi,Hossein Setareh*

Main category: cs.CL

TL;DR: 语言模型在时间偏好选择中显现出未来导向性，并可通过提示词系统操纵，推出了时间定向可操纵性指标MTO


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在跨期选择中的时间偏好特征，以及这些偏好是否可以系统地被操纵，为AI助手对齐异质化长期目标提供设计指南

Method: 采用适配的人类实验协议，在时间交换任务中评估多个LM，并与人类决策者进行对标，定义时间定向可操纵性(MTO)指标

Result: 理性模型(如DeepSeek-Reasoner和grok-3-mini)在未来导向提示下选择迟期选项，但在身份和地理个性化决策中仅部分个性化，能够正确理解时间定向的模型会内化作为AI决策者的未来导向

Conclusion: 研究为AI助手对齐异质化长期目标提供了设计启示，并提出了个性化上下文检测和社会意识部署的研究议程

Abstract: We study whether language models (LMs) exhibit future- versus
present-oriented preferences in intertemporal choice and whether those
preferences can be systematically manipulated. Using adapted human experimental
protocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them
against a sample of human decision makers. We introduce an operational metric,
the Manipulability of Time Orientation (MTO), defined as the change in an LM's
revealed time preference between future- and present-oriented prompts. In our
tests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)
choose later options under future-oriented prompts but only partially
personalize decisions across identities or geographies. Moreover, models that
correctly reason about time orientation internalize a future orientation for
themselves as AI decision makers. We discuss design implications for AI
assistants that should align with heterogeneous, long-horizon goals and outline
a research agenda on personalized contextual calibration and socially aware
deployment.

</details>


### [7] [The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks](https://arxiv.org/abs/2509.09705)
*Claudio Pinhanez,Paulo Cavalin,Cassia Sanctos,Marcelo Grave,Yago Primerano*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This work explores the consistency of small LLMs (2B-8B parameters) in
answering multiple times the same question. We present a study on known,
open-source LLMs responding to 10 repetitions of questions from the
multiple-choice benchmarks MMLU-Redux and MedQA, considering different
inference temperatures, small vs. medium models (50B-80B), finetuned vs. base
models, and other parameters. We also look into the effects of requiring
multi-trial answer consistency on accuracy and the trade-offs involved in
deciding which model best provides both of them. To support those studies, we
propose some new analytical and graphical tools. Results show that the number
of questions which can be answered consistently vary considerably among models
but are typically in the 50%-80% range for small models at low inference
temperatures. Also, accuracy among consistent answers seems to reasonably
correlate with overall accuracy. Results for medium-sized models seem to
indicate much higher levels of answer consistency.

</details>


### [8] [Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal](https://arxiv.org/abs/2509.09708)
*Nirmalendu Prakash,Yeo Wei Jie,Amir Abdullah,Ranjan Satapathy,Erik Cambria,Roy Ka Wei Lee*

Main category: cs.CL

TL;DR: 使用稀疏自编码器分析指令调优大语言模型的拒绝行为机制，通过特征消融实现从拒绝到合规的转换，揭示了安全行为的可解释潜在空间。


<details>
  <summary>Details</summary>
Motivation: 理解指令调优大语言模型中拒绝有害提示的内部机制，目前对这一安全行为的内在原因了解不足。

Method: 采用三阶段搜索流程：1)寻找拒绝调节方向并收集相关SAE特征；2)贪心过滤得到最小特征集；3)使用因子分解机捕捉非线性交互作用。在两个公开模型上进行实验。

Result: 发现了广泛的越狱关键特征集，揭示了拒绝行为的机制基础，并发现了冗余特征的存在。

Conclusion: 研究展示了通过操纵可解释潜在空间进行细粒度审计和针对性干预安全行为的潜力。

Abstract: Refusal on harmful prompts is a key safety behaviour in instruction-tuned
large language models (LLMs), yet the internal causes of this behaviour remain
poorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT
and LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on
residual-stream activations. Given a harmful prompt, we search the SAE latent
space for feature sets whose ablation flips the model from refusal to
compliance, demonstrating causal influence and creating a jailbreak. Our search
proceeds in three stages: (1) Refusal Direction: find a refusal-mediating
direction and collect SAE features near that direction; (2) Greedy Filtering:
prune to a minimal set; and (3) Interaction Discovery: fit a factorization
machine (FM) that captures nonlinear interactions among the remaining active
features and the minimal set. This pipeline yields a broad set of
jailbreak-critical features, offering insight into the mechanistic basis of
refusal. Moreover, we find evidence of redundant features that remain dormant
unless earlier features are suppressed. Our findings highlight the potential
for fine-grained auditing and targeted intervention in safety behaviours by
manipulating the interpretable latent space.

</details>


### [9] [Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement](https://arxiv.org/abs/2509.09709)
*Jing Ren,Weiqi Wang*

Main category: cs.CL

TL;DR: 通过内容质量和参考文献有效性两个关键指标，给出了量化评估ChatGPT学术写作的客观框架，并通过迭代提示方法显著提升内容质量和减少参考文献错误


<details>
  <summary>Details</summary>
Motivation: 大语言模型在学术写作中常出现错误或虚构的参考文献，而现有评估方法主要依靠主观人工判断，缺乏客观性和一致性

Method: 提出了内容质量和参考文献有效性两个量化评估指标，并基于这两个指标的得分设计了迭代提示方法

Result: 实验结果显示提出的指标提供了客观的量化评估框架，迭代提示方法显著提高了内容质量，同时减少了参考文献的不准确和虚构问题

Conclusion: 该研究为评估ChatGPT学术写作性能提供了客观的量化框架，并通过迭代提示有效解决了学术上的关键伦理挑战

Abstract: Large language models (LLMs) like ChatGPT are increasingly used in academic
writing, yet issues such as incorrect or fabricated references raise ethical
concerns. Moreover, current content quality evaluations often rely on
subjective human judgment, which is labor-intensive and lacks objectivity,
potentially compromising the consistency and reliability. In this study, to
provide a quantitative evaluation and enhance research proposal writing
capabilities of LLMs, we propose two key evaluation metrics--content quality
and reference validity--and an iterative prompting method based on the scores
derived from these two metrics. Our extensive experiments show that the
proposed metrics provide an objective, quantitative framework for assessing
ChatGPT's writing performance. Additionally, iterative prompting significantly
enhances content quality while reducing reference inaccuracies and
fabrications, addressing critical ethical challenges in academic contexts.

</details>


### [10] [Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data](https://arxiv.org/abs/2509.09710)
*Sepehr Golrokh Amin,Devin Rhoads,Fatemeh Fakhrmoosavi,Nicholas E. Lownes,John N. Ivan*

Main category: cs.CL

TL;DR: 本研究提出使用大语言模型生成基于代理的交通模型中的个体出行日记，通过开源数据生成虚拟人物并直接提示合成日记，验证显示LLM生成的日记在整体真实性和统计代表性方面与传统方法相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖大量专有家庭出行调查数据，成本高且获取困难。本研究旨在利用开源数据和大语言模型技术，开发一种更高效、可扩展的个体出行日记生成方法。

Method: 使用美国社区调查和智能位置数据库的开源数据随机生成虚拟人物，通过直接提示大语言模型合成出行日记。提出四指标综合真实性评分（行程数、间隔、目的、方式），并使用Jensen-Shannon散度验证生成日记与真实日记的分布相似性。

Result: LLM生成的日记整体真实性评分（0.485）与传统方法（0.455）相当。LLM在确定出行目的方面表现优异，且具有更好的一致性（真实性评分分布更窄），而传统模型在行程数和活动时长的数值估计方面领先。聚合验证显示LLM具有更好的统计代表性（0.612 vs 0.435）。

Conclusion: 大语言模型在零样本条件下即可生成具有统计代表性的出行日记，建立了可量化的日记真实性度量标准，为未来合成日记评估系统提供了可行方案，证明了LLM在交通建模中的实用价值。

Abstract: This study introduces a Large Language Model (LLM) scheme for generating
individual travel diaries in agent-based transportation models. While
traditional approaches rely on large quantities of proprietary household travel
surveys, the method presented in this study generates personas stochastically
from open-source American Community Survey (ACS) and Smart Location Database
(SLD) data, then synthesizes diaries through direct prompting. This study
features a novel one-to-cohort realism score: a composite of four metrics (Trip
Count Score, Interval Score, Purpose Score, and Mode Score) validated against
the Connecticut Statewide Transportation Study (CSTS) diaries, matched across
demographic variables. The validation utilizes Jensen-Shannon Divergence to
measure distributional similarities between generated and real diaries. When
compared to diaries generated with classical methods (Negative Binomial for
trip generation; Multinomial Logit for mode/purpose) calibrated on the
validation set, LLM-generated diaries achieve comparable overall realism (LLM
mean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and
demonstrates greater consistency (narrower realism score distribution), while
classical models lead in numerical estimates of trip count and activity
duration. Aggregate validation confirms the LLM's statistical
representativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot
viability and establishing a quantifiable metric of diary realism for future
synthetic diary evaluation systems.

</details>


### [11] [Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry](https://arxiv.org/abs/2509.09711)
*Aya E. Fouda,Abdelrahamn A. Hassan,Radwa J. Hanafy,Mohammed E. Fouda*

Main category: cs.CL

TL;DR: PsychiatryBench是一个基于权威精神病学教科书和案例精心构建的基准测试，包含11个问答任务、5300多个专家标注项目，用于评估LLMs在精神病学实践中的表现，发现现有模型在临床一致性和安全性方面存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有评估资源主要依赖小型临床访谈语料库、社交媒体帖子或合成对话，临床有效性有限，无法捕捉精神病学推理的复杂性，需要更权威的评估基准。

Method: 基于专家验证的精神病学教科书和案例构建PsychiatryBench基准，包含诊断推理、治疗计划、纵向随访等11个任务类型，评估多种前沿LLMs和医疗模型，使用传统指标和LLM-as-judge相似性评分框架。

Result: 评估结果显示，在多次随访和管理任务中，模型存在显著的临床一致性和安全性差距，特别是在多轮交互场景下表现不佳。

Conclusion: 需要专门的模型调优和更强大的评估范式，PsychiatryBench为高风险心理健康应用中的LLM性能基准测试和改进提供了模块化、可扩展的平台。

Abstract: Large language models (LLMs) hold great promise in enhancing psychiatric
practice, from improving diagnostic accuracy to streamlining clinical
documentation and therapeutic support. However, existing evaluation resources
heavily rely on small clinical interview corpora, social media posts, or
synthetic dialogues, which limits their clinical validity and fails to capture
the full complexity of psychiatric reasoning. In this work, we introduce
PsychiatryBench, a rigorously curated benchmark grounded exclusively in
authoritative, expert-validated psychiatric textbooks and casebooks.
PsychiatryBench comprises eleven distinct question-answering tasks ranging from
diagnostic reasoning and treatment planning to longitudinal follow-up,
management planning, clinical approach, sequential case analysis, and
multiple-choice/extended matching formats totaling over 5,300 expert-annotated
items. We evaluate a diverse set of frontier LLMs (including Google Gemini,
DeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models
(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an
"LLM-as-judge" similarity scoring framework. Our results reveal substantial
gaps in clinical consistency and safety, particularly in multi-turn follow-up
and management tasks, underscoring the need for specialized model tuning and
more robust evaluation paradigms. PsychiatryBench offers a modular, extensible
platform for benchmarking and improving LLM performance in high-stakes mental
health applications.

</details>


### [12] [The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization](https://arxiv.org/abs/2509.09712)
*Talha Tahir*

Main category: cs.CL

TL;DR: 这篇论文研究了不同训练方法（SFT和ORPO）和显式推理（COT）对小型语言模型执行接纳与承诺疗法（ACT）的影响。结果显示ORPO训练模型在疗法保真度和共情能力方面显著优于SFT和基础模型，而COT仅对SFT模型有显著改善作用。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过合适的训练方法和推理机制来提升小型语言模型在接纳与承诺疗法（ACT）运用中的性能，以支持精神健康领域的应用。

Method: 使用50组由Mistral-Large生成的合成ACT语料，对Llama-3.2-3b-Instruct模型进行两种训练：监督细调（SFT）和比率政策优化（ORPO），每种方法分别包含和不包含显式链式思维（COT）推理步骤。通过仿真疗法会议进行性能评估，使用ACT保真度量表（ACT-FM）和治疗师共情量表（TES）进行量化评估。

Result: ORPO训练模型在ACT保真度（χ²(5) = 185.15, p < .001）和疗法共情（χ²(5) = 140.37, p < .001）方面显著优于SFT和基础模型。COT仅对SFT模型有显著改善作用（ACT-FM平均提升2.68分，p < .001），对ORPO模型无显著影响。

Conclusion: 偏好对齐政策优化（ORPO）能够有效培养小型语言模型的ACT能力，其优势在于学习疗法过程而非简单模仿内容；显式推理（COT）的作用依赖于基础训练方法，仅在仿真训练方式下作为必要的脚手架。

Abstract: Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral
therapy with emerging evidence of efficacy in several psychiatric conditions.
This study investigates the impact of post-training methodology and explicit
reasoning on the ability of a small open-weight large language model (LLM) to
deliver ACT. Using 50 sets of synthetic ACT transcripts generated by
Mistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,
supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each
with and without an explicit chain-of-thought (COT) reasoning step. Performance
was evaluated by comparing these four post-trained variants against the base
Instruct model. These models were benchmarked in simulated therapy sessions,
with performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)
and the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned
on human evaluations. Our findings demonstrate that the ORPO-trained models
significantly outperformed both their SFT and Instruct counterparts on ACT
fidelity ($\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\chi^2(5) =
140.37, p < .001$). The effect of COT was conditional as it provided a
significant benefit to SFT models, improving ACT-FM scores by an average of
2.68 points ($p < .001$), while offering no discernible advantage to the
superior ORPO or instruct-tuned variants. We posit that the superiority of ORPO
stems from its ability to learn the therapeutic `process' over imitating
`content,' a key aspect of ACT, while COT acts as a necessary scaffold for
models trained only via imitation. This study establishes that
preference-aligned policy optimization can effectively instill ACT competencies
in small LLMs, and that the utility of explicit reasoning is highly dependent
on the underlying training paradigm.

</details>


### [13] [HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2509.09713)
*Duolin Sun,Dan Yang,Yue Shen,Yihan Jiao,Zhehao Tan,Jie Feng,Lianzhen Zhong,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: HANRAG是一个基于启发式的检索增强生成框架，通过查询路由、子查询分解和噪声过滤，有效解决多跳查询中的噪声积累和效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法在处理多跳查询时面临迭代检索效率低、复杂查询检索内容噪声大、噪声积累等问题，需要更高效的解决方案。

Method: 提出HANRAG框架，使用强大的revealator进行查询路由、查询分解为子查询，并对检索文档进行噪声过滤，提高系统适应性和抗噪能力。

Result: 在多个基准测试中，HANRAG在单跳和多跳问答任务上都表现出优于其他主流方法的性能。

Conclusion: HANRAG框架通过创新的启发式方法有效解决了RAG在多跳查询中的关键问题，显著提升了问答系统的性能和鲁棒性。

Abstract: The Retrieval-Augmented Generation (RAG) approach enhances question-answering
systems and dialogue generation tasks by integrating information retrieval (IR)
technologies with large language models (LLMs). This strategy, which retrieves
information from external knowledge bases to bolster the response capabilities
of generative models, has achieved certain successes. However, current RAG
methods still face numerous challenges when dealing with multi-hop queries. For
instance, some approaches overly rely on iterative retrieval, wasting too many
retrieval steps on compound queries. Additionally, using the original complex
query for retrieval may fail to capture content relevant to specific
sub-queries, resulting in noisy retrieved content. If the noise is not managed,
it can lead to the problem of noise accumulation. To address these issues, we
introduce HANRAG, a novel heuristic-based framework designed to efficiently
tackle problems of varying complexity. Driven by a powerful revelator, HANRAG
routes queries, decomposes them into sub-queries, and filters noise from
retrieved documents. This enhances the system's adaptability and noise
resistance, making it highly capable of handling diverse queries. We compare
the proposed framework against other leading industry methods across various
benchmarks. The results demonstrate that our framework obtains superior
performance in both single-hop and multi-hop question-answering tasks.

</details>


### [14] [How Small Transformation Expose the Weakness of Semantic Similarity Measures](https://arxiv.org/abs/2509.09714)
*Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Aziz Bonkoungou,Micheline Bénédicte Moumoula,Jordan Samhi,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.CL

TL;DR: 这项研究系统性测试18种语义相似性测量方法，发现常用指标存在显著问题，嵌入方法经常错误识别相反语义，而LLM方法在区分语义差异方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 评估不同语义相似性测量方法的效果，确定LLM是否真正理解语义关系还是仅仅识别表面模式，以支持代码搜索、API推荐等软件工程应用。

Method: 采用18种不同的相似性测量方法，包括词汇基础方法、嵌入技术、LLM基础系统和结构感知算法，通过系统性测试框架应用受控变更来评估各方法的表现。

Result: 嵌入方法将语义相反内容错误识别为相似的比例高达99.9%，某些变换器方法甚至将相反含义评为比同义词更相似。从欧几里得距离切换为余弦相似性可提高24-66%的性能。LLM方法在区分真正语义差异时表现更好，产生低相似性分数（0.00-0.29）。

Conclusion: 当前常用的语义相似性测量指标存在显著缺陷，嵌入方法的距离计算方式影响性能，LLM方法在识别语义差异方面表现更优异，为软件工程应用提供了更可靠的语义理解能力。

Abstract: This research examines how well different methods measure semantic
similarity, which is important for various software engineering applications
such as code search, API recommendations, automated code reviews, and
refactoring tools. While large language models are increasingly used for these
similarity assessments, questions remain about whether they truly understand
semantic relationships or merely recognize surface patterns.
  The study tested 18 different similarity measurement approaches, including
word-based methods, embedding techniques, LLM-based systems, and
structure-aware algorithms. The researchers created a systematic testing
framework that applies controlled changes to text and code to evaluate how well
each method handles different types of semantic relationships.
  The results revealed significant issues with commonly used metrics. Some
embedding-based methods incorrectly identified semantic opposites as similar up
to 99.9 percent of the time, while certain transformer-based approaches
occasionally rated opposite meanings as more similar than synonymous ones. The
study found that embedding methods' poor performance often stemmed from how
they calculate distances; switching from Euclidean distance to cosine
similarity improved results by 24 to 66 percent. LLM-based approaches performed
better at distinguishing semantic differences, producing low similarity scores
(0.00 to 0.29) for genuinely different meanings, compared to embedding methods
that incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.

</details>


### [15] [Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA](https://arxiv.org/abs/2509.09715)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: 这篇论文研究了大语言模型中的幻觉问题，识别出了导致幻觉的关键符号属性，并发现即使模型规模增大，对修饰词和专有名词的幻觉率仍然很高。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM中的幻觉问题已经得到广泛研究，但导致幻觉的内在弱点属性并未被识别和深入研究。本研究旨在找出这些关键属性并分析模型内部机制的脏点。

Method: 使用HaluEval和TruthfulQA两个数据集，将原有的问答格式转换为多种其他格式，通过这种方式来突出幻觉的关键属性。

Result: 研究发现Gemma-2-2B模型在各种任务和数据集上的平均幻觉率达79.0%。随着模型规模增大，幻觉率下降：Gemma-2-9B为73.6%，Gemma-2-27B为63.9%，总体下降15个百分点。但修饰词(84.76%-94.98%)和专有名词(83.87%-93.96%)的幻觉率仍然非常高。

Conclusion: 符号元素仍然会让LLM模型感到困惑，这表明这些模型在处理这类输入时存在基础性的弱点，而且这种问题不会随着模型规模的增大而完全消失。

Abstract: Hallucination in Large Language Models (LLMs) is a well studied problem.
However, the properties that make LLM intrinsically vulnerable to
hallucinations have not been identified and studied. This research identifies
and characterizes the key properties, allowing us to pinpoint vulnerabilities
within the model's internal mechanisms. To solidify on these properties, we
utilized two established datasets, HaluEval and TruthfulQA and convert their
existing format of question answering into various other formats to narrow down
these properties as the reason for the hallucinations. Our findings reveal that
hallucination percentages across symbolic properties are notably high for
Gemma-2-2B, averaging 79.0% across tasks and datasets. With increased model
scale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,
reflecting a 15 percentage point reduction overall. Although the hallucination
rate decreases as the model size increases, a substantial amount of
hallucination caused by symbolic properties still persists. This is especially
evident for modifiers (ranging from 84.76% to 94.98%) and named entities
(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.
These findings indicate that symbolic elements continue to confuse the models,
pointing to a fundamental weakness in how these LLMs process such
inputs--regardless of their scale.

</details>


### [16] [ALIGNS: Unlocking nomological networks in psychological measurement through a large language model](https://arxiv.org/abs/2509.09723)
*Kai R. Larsen,Sen Yan,Roland Müller,Lan Sang,Mikko Rönkkö,Ravi Starzl,Donald Edmondson*

Main category: cs.CL

TL;DR: ALIGNS是一个基于大语言模型的系统，用于生成包含55万+指标的综合nomological网络，解决心理测量中构建理论网络的挑战。


<details>
  <summary>Details</summary>
Motivation: 心理测量中构建nomological网络（概念与测量关系的理论图谱）70年来一直是验证效度的核心挑战，这导致临床试验可能无法检测治疗效果，公共政策可能针对错误结果。

Method: 开发基于大语言模型的ALIGNS系统，使用经过验证的问卷测量进行训练，提供三个综合nomological网络，涵盖心理学、医学、社会政策等领域。

Result: 1) NIH PROMIS焦虑抑郁工具收敛为单一情绪困扰维度；2) 儿童气质测量识别出现有框架未捕捉的四个潜在维度；3) 心理测量学专家评估显示系统重要性、可访问性和适用性良好。

Conclusion: ALIGNS是大语言模型首次应用于解决测量验证基础问题，通过大规模nomological分析补充传统验证方法，系统可在nomologicalnetwork.org免费获取。

Abstract: Psychological measurement is critical to many disciplines. Despite advances
in measurement, building nomological networks, theoretical maps of how concepts
and measures relate to establish validity, remains a challenge 70 years after
Cronbach and Meehl proposed them as fundamental to validation. This limitation
has practical consequences: clinical trials may fail to detect treatment
effects, and public policy may target the wrong outcomes. We introduce Analysis
of Latent Indicators to Generate Nomological Structures (ALIGNS), a large
language model-based system trained with validated questionnaire measures.
ALIGNS provides three comprehensive nomological networks containing over
550,000 indicators across psychology, medicine, social policy, and other
fields. This represents the first application of large language models to solve
a foundational problem in measurement validation. We report classification
accuracy tests used to develop the model, as well as three evaluations. In the
first evaluation, the widely used NIH PROMIS anxiety and depression instruments
are shown to converge into a single dimension of emotional distress. The second
evaluation examines child temperament measures and identifies four potential
dimensions not captured by current frameworks, and questions one existing
dimension. The third evaluation, an applicability check, engages expert
psychometricians who assess the system's importance, accessibility, and
suitability. ALIGNS is freely available at nomologicalnetwork.org,
complementing traditional validation methods with large-scale nomological
analysis.

</details>


### [17] [DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model](https://arxiv.org/abs/2509.09724)
*Wonyoung Kim,Sujeong Seo,Juhyun Lee*

Main category: cs.CL

TL;DR: 基于专利数据的时间关系分析框架，利用大语言模型提取技术主题和关系，识别新兴技术机会


<details>
  <summary>Details</summary>
Motivation: 技术机会是推动技术进步、产业发展和创新的关键信息，需要有效方法来识别新兴技术趋势

Method: 从专利数据提取文本，用LLM映射文本主题发现技术间关系，根据主题时间变化识别机会，使用chat模型提示支持发现

Result: 在美国专利局人工智能专利数据集上验证，结果显示AI技术正向日常化可访问方向发展

Conclusion: 该框架有效能够发现未来技术机会，为技术预测和创新提供了有力工具

Abstract: Technology opportunities are critical information that serve as a foundation
for advancements in technology, industry, and innovation. This paper proposes a
framework based on the temporal relationships between technologies to identify
emerging technology opportunities. The proposed framework begins by extracting
text from a patent dataset, followed by mapping text-based topics to discover
inter-technology relationships. Technology opportunities are then identified by
tracking changes in these topics over time. To enhance efficiency, the
framework leverages a large language model to extract topics and employs a
prompt for a chat-based language model to support the discovery of technology
opportunities. The framework was evaluated using an artificial intelligence
patent dataset provided by the United States Patent and Trademark Office. The
experimental results suggest that artificial intelligence technology is
evolving into forms that facilitate everyday accessibility. This approach
demonstrates the potential of the proposed framework to identify future
technology opportunities.

</details>


### [18] [BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025](https://arxiv.org/abs/2509.09725)
*Chunyu Li,Xindi Zheng,Siqi Liu*

Main category: cs.CL

TL;DR: 提出了一个轻量级的两阶段实体链接系统BIBERT-Pipe，用于处理生物医学文本中的多语言嵌套命名实体链接任务，在BioNNE 2025多语言赛道中排名第三


<details>
  <summary>Details</summary>
Motivation: 现有的生物医学实体链接基准主要针对英语和平坦提及，缺乏对多语言和嵌套提及的现实场景研究

Method: 采用两阶段检索-排序架构：检索阶段使用原始预训练模型，排序阶段进行领域特定微调；使用可学习的[Ms]/[Me]标签包装提及；通过三种数据源自动扩展训练语料

Result: 在BioNNE 2025多语言赛道中排名第三，证明了这些最小但原则性修改的有效性和竞争力

Conclusion: 该方法通过保持原始EL模型完整，仅修改三个任务对齐组件，成功解决了多语言生物医学嵌套实体链接问题，代码已开源

Abstract: Entity linking (EL) for biomedical text is typically benchmarked on
English-only corpora with flat mentions, leaving the more realistic scenario of
nested and multilingual mentions largely unexplored. We present our system for
the BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task
(English & Russian), closing this gap with a lightweight pipeline that keeps
the original EL model intact and modifies only three task-aligned components:
Two-stage retrieval-ranking. We leverage the same base encoder model in both
stages: the retrieval stage uses the original pre-trained model, while the
ranking stage applies domain-specific fine-tuning. Boundary cues. In the
ranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing
the encoder with an explicit, language-agnostic span before robustness to
overlap and nesting. Dataset augmentation. We also automatically expand the
ranking training corpus with three complementary data sources, enhancing
coverage without extra manual annotation. On the BioNNE 2025 leaderboard, our
two stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual
track, demonstrating the effectiveness and competitiveness of these minimal yet
principled modifications. Code are publicly available at
https://github.com/Kaggle-Competitions-Code/BioNNE-L.

</details>


### [19] [Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure](https://arxiv.org/abs/2509.09726)
*Seiji Hattori,Takuya Matsuzaki,Makoto Fujiwara*

Main category: cs.CL

TL;DR: 利用LLM将机器可验证的形式化证明翻译为自然语言的方法，通过非形式化和摘要化能力生成高质量的自然语言证明


<details>
  <summary>Details</summary>
Motivation: 为了解决形式化证明难以理解的问题，需要将机器可验证的形式化证明转换为人类可读的自然语言证明

Method: 利用大型语言模型(LLM)的非形式化(形式语言证明步骤的言语化)和摘要化能力，将形式化证明翻译为自然语言

Result: 该方法在本科教材的自然语言证明数据集上评估，生成的自然语言证明质量与原始证明相当，并在Lean证明助手的现有形式化证明库中验证了其可读性和准确性

Conclusion: 提出的方法能够输出高可读性和准确性的自然语言证明，有效弥合了形式化证明与人类理解之间的鸿沟

Abstract: This paper proposes a natural language translation method for
machine-verifiable formal proofs that leverages the informalization
(verbalization of formal language proof steps) and summarization capabilities
of LLMs. For evaluation, it was applied to formal proof data created in
accordance with natural language proofs taken from an undergraduate-level
textbook, and the quality of the generated natural language proofs was analyzed
in comparison with the original natural language proofs. Furthermore, we will
demonstrate that this method can output highly readable and accurate natural
language proofs by applying it to existing formal proof library of the Lean
proof assistant.

</details>


### [20] [A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs](https://arxiv.org/abs/2509.09727)
*Andy Zhu,Yingjun Du*

Main category: cs.CL

TL;DR: 一种多段式金融问答框架，通过角色演绎和批判精炼提高了LLM在金融领域的准确性，在Study.com数据集上实现了6.6-8.3%的性能提升


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在金融问答中常常无法满足专业化的多步数量推理、领域术语和实际场景理解的需求

Method: 提出了一个多段式框架，包含Base Generator、Evidence Retriever和Expert Reviewer三个模块，采用单次迭代流程生成精炼答案，结合RAG技术从6本金融教科书中获取上下文信息

Result: 在3,532道专家设计的金融教育问题上，评估显示批判精炼方法比零检查链基线提高了6.6-8.3%的准确性，Gemini-2.0-Flash表现最佳，GPT-4o-mini能达到专门调整的FinGPT模型的类似性能

Conclusion: 该方法提供了一种成本效益高的方案来提升金融问答性能，为多段式金融LLM系统的进一步研究提供了见解

Abstract: Question answering (QA) plays a central role in financial education, yet
existing large language model (LLM) approaches often fail to capture the
nuanced and specialized reasoning required for financial problem-solving. The
financial domain demands multistep quantitative reasoning, familiarity with
domain-specific terminology, and comprehension of real-world scenarios. We
present a multi-agent framework that leverages role-based prompting to enhance
performance on domain-specific QA. Our framework comprises a Base Generator, an
Evidence Retriever, and an Expert Reviewer agent that work in a single-pass
iteration to produce a refined answer. We evaluated our framework on a set of
3,532 expert-designed finance education questions from Study.com, an online
learning platform. We leverage retrieval-augmented generation (RAG) for
contextual evidence from 6 finance textbooks and prompting strategies for a
domain-expert reviewer. Our experiments indicate that critique-based refinement
improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,
with the highest performance from Gemini-2.0-Flash. Furthermore, our method
enables GPT-4o-mini to achieve performance comparable to the finance-tuned
FinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to
enhancing financial QA and offer insights for further research in multi-agent
financial LLM systems.

</details>


### [21] [A meta-analysis on the performance of machine-learning based language models for sentiment analysis](https://arxiv.org/abs/2509.09728)
*Elena Rohde,Jonas Klingwort,Christian Borgs*

Main category: cs.CL

TL;DR: 对Twitter情感分析中机器学习性能的元分析，发现平均准确率为0.80，强调需要标准化性能报告和考虑类别不平衡问题


<details>
  <summary>Details</summary>
Motivation: 评估Twitter情感分析中ML模型的平均性能，分析研究间的异质性，并探讨研究特征如何影响模型性能

Method: 使用PRISMA指南搜索学术数据库，选取20项研究的195个试验，采用双反正弦变换和三层次随机效应模型分析总体准确率

Result: AIC优化模型的平均总体准确率为0.80 [0.76, 0.84]，发现总体准确率因对类别不平衡和情感类别数量的敏感性而容易产生误导

Conclusion: 需要规范模型性能报告标准，包括报告独立测试集的混淆矩阵，以实现跨研究的可靠ML分类器比较

Abstract: This paper presents a meta-analysis evaluating ML performance in sentiment
analysis for Twitter data. The study aims to estimate the average performance,
assess heterogeneity between and within studies, and analyze how study
characteristics influence model performance. Using PRISMA guidelines, we
searched academic databases and selected 195 trials from 20 studies with 12
study features. Overall accuracy, the most reported performance metric, was
analyzed using double arcsine transformation and a three-level random effects
model. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,
0.84]. This paper provides two key insights: 1) Overall accuracy is widely used
but often misleading due to its sensitivity to class imbalance and the number
of sentiment classes, highlighting the need for normalization. 2) Standardized
reporting of model performance, including reporting confusion matrices for
independent test sets, is essential for reliable comparisons of ML classifiers
across studies, which seems far from common practice.

</details>


### [22] [MultimodalHugs: Enabling Sign Language Processing in Hugging Face](https://arxiv.org/abs/2509.09729)
*Gerard Sant,Zifan Jiang,Carlos Escolano,Amit Moryossef,Mathias Müller,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

TL;DR: MultimodalHugs是一个基于Hugging Face构建的多模态框架，专门解决手语处理研究中存在的代码复杂、可复现性差和比较不公平等问题，支持更丰富的数据模态和任务。


<details>
  <summary>Details</summary>
Motivation: 手语处理研究相比口语语言研究面临更多挑战，包括复杂的临时代码、低可复现性和不公平比较。现有工具如Hugging Face不够灵活，无法无缝集成手语实验。

Method: 在Hugging Face基础上构建MultimodalHugs框架，增加抽象层以支持更多样化的数据模态和任务，特别是手语姿态估计数据和文本字符像素数据等。

Result: 通过定量实验证明MultimodalHugs能够有效适应多种模态数据，为手语处理和其他非标准模板用例提供更好的实验支持。

Conclusion: MultimodalHugs框架成功解决了手语处理研究中的技术障碍，提高了研究的可复现性和公平性，同时具有更广泛的适用性。

Abstract: In recent years, sign language processing (SLP) has gained importance in the
general field of Natural Language Processing. However, compared to research on
spoken languages, SLP research is hindered by complex ad-hoc code,
inadvertently leading to low reproducibility and unfair comparisons. Existing
tools that are built for fast and reproducible experimentation, such as Hugging
Face, are not flexible enough to seamlessly integrate sign language
experiments. This view is confirmed by a survey we conducted among SLP
researchers.
  To address these challenges, we introduce MultimodalHugs, a framework built
on top of Hugging Face that enables more diverse data modalities and tasks,
while inheriting the well-known advantages of the Hugging Face ecosystem. Even
though sign languages are our primary focus, MultimodalHugs adds a layer of
abstraction that makes it more widely applicable to other use cases that do not
fit one of the standard templates of Hugging Face. We provide quantitative
experiments to illustrate how MultimodalHugs can accommodate diverse modalities
such as pose estimation data for sign languages, or pixel data for text
characters.

</details>


### [23] [Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning](https://arxiv.org/abs/2509.09731)
*Haiyang Yu,Yuchuan Wu,Fan Shi,Lei Liao,Jinghui Lu,Xiaodong Ge,Han Wang,Minghan Zhuo,Xuecheng Wu,Xiang Fei,Hao Feng,Guozhi Tang,An-Lan Wang,Hanshen Zhu,Yangfan He,Quanhuan Liang,Liyuan Meng,Chao Feng,Can Huang,Jingqun Tang,Bin Li*

Main category: cs.CL

TL;DR: AncientDoc是首个针对中文古籍文档的基准测试，包含5个任务（页面级OCR、白话翻译、推理问答、知识问答、语言变体问答），涵盖14种文档类型、100多本书籍和约3000页内容，用于评估视觉语言模型在中文古籍处理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 中文古籍作为中华历史文化的重要载体，在数字化和理解方面面临挑战。传统方法仅扫描图像，而现有视觉语言模型难以处理其视觉和语言复杂性。现有文档基准主要针对英文印刷文本或简体中文，缺乏对中文古籍的评估标准。

Method: 构建AncientDoc基准测试，包含5个不同难度的任务：页面级OCR、白话翻译、推理问答、知识问答、语言变体问答。数据集涵盖14种文档类型、100多本书籍、约3000页内容。使用多种指标评估主流视觉语言模型，并辅以人工对齐的大语言模型进行评分。

Result: 创建了首个专门针对中文古籍的综合性基准测试AncientDoc，为评估视觉语言模型在中文古籍处理能力提供了标准化的测试平台。

Conclusion: AncientDoc填补了中文古籍文档评估的空白，为提升视觉语言模型在复杂中文古籍处理方面的能力提供了重要的基准测试工具，有助于推动中文古籍数字化和理解技术的发展。

Abstract: Chinese ancient documents, invaluable carriers of millennia of Chinese
history and culture, hold rich knowledge across diverse fields but face
challenges in digitization and understanding, i.e., traditional methods only
scan images, while current Vision-Language Models (VLMs) struggle with their
visual and linguistic complexity. Existing document benchmarks focus on English
printed texts or simplified Chinese, leaving a gap for evaluating VLMs on
ancient Chinese documents. To address this, we present AncientDoc, the first
benchmark for Chinese ancient documents, designed to assess VLMs from OCR to
knowledge reasoning. AncientDoc includes five tasks (page-level OCR, vernacular
translation, reasoning-based QA, knowledge-based QA, linguistic variant QA) and
covers 14 document types, over 100 books, and about 3,000 pages. Based on
AncientDoc, we evaluate mainstream VLMs using multiple metrics, supplemented by
a human-aligned large language model for scoring.

</details>


### [24] [MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools](https://arxiv.org/abs/2509.09734)
*Zikang Guo,Benfeng Xu,Chiwei Zhu,Wentao Hong,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: MCP-AgentBench是一个专门为评估语言代理在MCP协议下的工具交互能力而设计的综合基准测试，包含33个服务器、188个工具和600个系统化查询，采用面向结果的评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试无法准确评估MCP协议下AI代理的真实性能，导致对其操作价值的扭曲认知和无法可靠区分能力差异。

Method: 建立包含33个操作服务器和188个不同工具的MCP测试床，开发600个系统化设计的查询（分布在6个不同复杂度的交互类别），并引入MCP-Eval这种新颖的面向结果的评估方法。

Result: 通过对领先语言代理的广泛实证评估，提供了基础性见解，展示了不同代理在MCP环境下的性能差异。

Conclusion: MCP-AgentBench为研究社区提供了一个标准化、可靠的框架，用于构建、验证和推进能够充分利用MCP变革性优势的AI代理，加速真正有能力且可互操作的AI系统的发展。

Abstract: The Model Context Protocol (MCP) is rapidly emerging as a pivotal open
standard, designed to enhance agent-tool integration and interoperability, and
is positioned to unlock a new era of powerful, interconnected, and genuinely
utilitarian agentic AI. However, despite MCP's growing adoption, existing
benchmarks often fail to capture real-world agent performance within this new
paradigm, leading to a distorted perception of their true operational value and
an inability to reliably differentiate proficiencies. To bridge this critical
evaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark
specifically engineered to rigorously assess language agent capabilities in
MCP-mediated tool interactions. Core contributions of MCP-AgentBench include:
the establishment of a robust MCP testbed comprising 33 operational servers
with 188 distinct tools; the development of a benchmark featuring 600
systematically designed queries distributed across 6 distinct categories of
varying interaction complexity; and the introduction of MCP-Eval, a novel
outcome-oriented evaluation methodology prioritizing real-world task success.
Through extensive empirical evaluation of leading language agents, we provide
foundational insights. MCP-AgentBench aims to equip the research community with
a standardized and reliable framework to build, validate, and advance agents
capable of fully leveraging MCP's transformative benefits, thereby accelerating
progress toward truly capable and interoperable AI systems.

</details>


### [25] [Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation](https://arxiv.org/abs/2509.09735)
*Willem Huijzer,Jieying Chen*

Main category: cs.CL

TL;DR: 这项研究调查了大语言模型在决策和摘要任务中的偏见问题，发现GPT-3.5和GPT-4o在决策中存在显著偏见，偏向女性、年轻者和非洲美后装，而摘要任务偏见较少。跨语言分析显示偏见模式相似，提示策略能部分减少偏见。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛集成，对社会不平等和信息偏见的担忧日益增加。研究意图识别和分析LLM在不同人口统计特征（背景、性别、年龄）方面的偏见，并评估策略缓解方法的效果。

Method: 使用Tamkin等人（2023）数据集的荷兰语版本，创建了151,200个决策任务提示和176,400个摘要任务提示。在GPT-3.5和GPT-4o上测试了各种人口统计变量、指令类型、显著性水平和语言。进行了跨语言偏见传播分析和提示策略效果评估。

Result: 两款模型在决策任务中都显示出显著偏见：偏向女性、年轻年龄段和非洲美背景。摘要任务偏见较少，仅GPT-3.5在英语中显示出年龄相关差异。跨语言偏见模式大体相似，但特定人口组间存在差异。提示策略虽不能完全消除偏见，但最佳指令能平均减少27%的最大偏差。GPT-4o在英语中显示出更低的偏见水平。

Conclusion: 研究强调了谨慎采用LLM的重要性和上下文特定偏见测试的必要性。新提出的缓解指令展示了减少偏见的潜力，尤其是在GPT-4o等更新模型中。研究引导应用者根据具体使用场景进行偏见测试，并继续开发有效的缓解策略以确保AI的负责任部署。

Abstract: The rapid integration of Large Language Models (LLMs) into various domains
raises concerns about societal inequalities and information bias. This study
examines biases in LLMs related to background, gender, and age, with a focus on
their impact on decision-making and summarization tasks. Additionally, the
research examines the cross-lingual propagation of these biases and evaluates
the effectiveness of prompt-instructed mitigation strategies. Using an adapted
version of the dataset by Tamkin et al. (2023) translated into Dutch, we
created 151,200 unique prompts for the decision task and 176,400 for the
summarisation task. Various demographic variables, instructions, salience
levels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed
that both models were significantly biased during decision-making, favouring
female gender, younger ages, and certain backgrounds such as the
African-American background. In contrast, the summarisation task showed minimal
evidence of bias, though significant age-related differences emerged for
GPT-3.5 in English. Cross-lingual analysis showed that bias patterns were
broadly similar between English and Dutch, though notable differences were
observed across specific demographic categories. The newly proposed mitigation
instructions, while unable to eliminate biases completely, demonstrated
potential in reducing them. The most effective instruction achieved a 27\% mean
reduction in the gap between the most and least favorable demographics.
Notably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts
in English, indicating the specific potential for prompt-based mitigation
within newer models. This research underscores the importance of cautious
adoption of LLMs and context-specific bias testing, highlighting the need for
continued development of effective mitigation strategies to ensure responsible
deployment of AI.

</details>


### [26] [HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning](https://arxiv.org/abs/2509.09801)
*Brennen Hill*

Main category: cs.CL

TL;DR: HEFT是一种分层高效微调策略，结合权重空间和表示空间的PEFT方法，在BoolQ基准测试中仅用3个epoch就达到85.17%准确率，超越单一方法20个epoch的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的专业化推理任务适配受计算资源限制，不同PEFT方法各有优势但缺乏协同，需要探索组合方法以获得更好的性能和效率。

Method: 提出HEFT分层适配策略：先在权重空间使用LoRA进行基础适配，然后在表示空间使用ReFT进行精确细化，形成从粗到细的微调过程。

Result: 在Llama-2-7B模型和BoolQ基准测试中，HEFT仅用3个epoch就达到85.17%准确率，优于LoRA-only（85.05%）和ReFT-only（83.36%）20个epoch的结果。

Conclusion: PEFT方法的精心组合是强大的算法创新，为提升语言模型推理能力提供了更高效有效的路径，能以更少计算预算获得更优结果。

Abstract: The adaptation of large language models (LLMs) to specialized reasoning tasks
is fundamentally constrained by computational resources. Parameter-Efficient
Fine-Tuning (PEFT) methods have emerged as a powerful solution, yet the
landscape of these techniques is diverse, with distinct methods operating in
either the model's weight space or its representation space. This paper
investigates the hypothesis that a synergistic combination of these paradigms
can unlock superior performance and efficiency. We introduce HEFT (Hierarchical
Efficient Fine-Tuning), a novel hierarchical adaptation strategy that composes
two distinct PEFT methods in a coarse-to-fine manner: first, a broad,
foundational adaptation in the weight space using Low-Rank Adaptation (LoRA),
followed by a precise, surgical refinement of internal activations using
Representation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a
Llama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential
reasoning. Our results reveal a profound synergistic effect. A model fine-tuned
for only three epochs with our HEFT strategy achieves an accuracy of 85.17\%,
exceeding the performance of models trained for 20 epochs with either LoRA-only
(85.05\%) or ReFT-only (83.36\%) methodologies. This work demonstrates that the
thoughtful composition of PEFT methods is a potent algorithmic innovation,
offering a more efficient and effective path toward advancing the reasoning
capabilities of language models. By achieving superior results with a fraction
of the computational budget, our findings present a principled approach to
overcoming the obstacles inherent in adapting large-scale models for complex
cognitive tasks.

</details>


### [27] [Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization](https://arxiv.org/abs/2509.09804)
*Helen de Andrade Abreu,Tiago Timponi Torrent,Ely Edison da Silva Matos*

Main category: cs.CL

TL;DR: 本文提出了一个多模态对话轮流组织建模框架，通过分析语言与交互手势的关联，并在Frame2数据集中添加了语用框架注释。


<details>
  <summary>Details</summary>
Motivation: 将对话轮流组织特别是手势策略编码到可用于机器学习的数据集中，以填补现有研究的空白。

Method: 开发了一种注释方法，在多模态数据集Frame2（已注释语义框架）中增加了对话轮流组织手势的语用框架注释，分析了巴西电视剧中的10集内容。

Result: 证实了交流者在面对面对话中使用手势来控制对话轮流（轮换、获取、保持），并发现了之前未被文档记录的手势变体。

Conclusion: 这些手势的使用来自于语用框架的概念化过程，包括心智空间、混合和概念隐喻，语用框架注释有助于更深入理解人类认知和语言。

Abstract: This paper proposes a framework for modeling multimodal conversational turn
organization via the proposition of correlations between language and
interactive gestures, based on analysis as to how pragmatic frames are
conceptualized and evoked by communicators. As a means to provide evidence for
the analysis, we developed an annotation methodology to enrich a multimodal
dataset (annotated for semantic frames) with pragmatic frames modeling
conversational turn organization. Although conversational turn organization has
been studied by researchers from diverse fields, the specific strategies,
especially gestures used by communicators, had not yet been encoded in a
dataset that can be used for machine learning. To fill this gap, we enriched
the Frame2 dataset with annotations of gestures used for turn organization. The
Frame2 dataset features 10 episodes from the Brazilian TV series Pedro Pelo
Mundo annotated for semantic frames evoked in both video and text. This dataset
allowed us to closely observe how communicators use interactive gestures
outside a laboratory, in settings, to our knowledge, not previously recorded in
related literature. Our results have confirmed that communicators involved in
face-to-face conversation make use of gestures as a tool for passing, taking
and keeping conversational turns, and also revealed variations of some gestures
that had not been documented before. We propose that the use of these gestures
arises from the conceptualization of pragmatic frames, involving mental spaces,
blending and conceptual metaphors. In addition, our data demonstrate that the
annotation of pragmatic frames contributes to a deeper understanding of human
cognition and language.

</details>


### [28] [Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization](https://arxiv.org/abs/2509.09852)
*Chuyuan Li,Austin Xu,Shafiq Joty,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 提出基于主题引导强化学习的方法改进多文档摘要的内容选择，通过主题奖励机制在GRPO框架中提升摘要与源文档的主题对齐度


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单文档摘要表现优异，但在多文档摘要中整合多个来源信息时仍存在改进空间，需要更好地保持连贯性和主题相关性

Method: 首先通过显式提示主题标签增强摘要信息量，然后在GRPO框架中引入新颖的主题奖励机制来衡量生成摘要与源文档的主题对齐程度

Result: 在Multi-News和Multi-XScience数据集上的实验结果表明，该方法 consistently 优于强基线模型

Conclusion: 利用主题线索在多文档摘要中是有效的，主题引导的强化学习方法能够显著提升多文档摘要的质量

Abstract: A key challenge in Multi-Document Summarization (MDS) is effectively
integrating information from multiple sources while maintaining coherence and
topical relevance. While Large Language Models have shown impressive results in
single-document summarization, their performance on MDS still leaves room for
improvement. In this paper, we propose a topic-guided reinforcement learning
approach to improve content selection in MDS. We first show that explicitly
prompting models with topic labels enhances the informativeness of the
generated summaries. Building on this insight, we propose a novel topic reward
within the Group Relative Policy Optimization (GRPO) framework to measure topic
alignment between the generated summary and source documents. Experimental
results on the Multi-News and Multi-XScience datasets demonstrate that our
method consistently outperforms strong baselines, highlighting the
effectiveness of leveraging topical cues in MDS.

</details>


### [29] [Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case](https://arxiv.org/abs/2509.09871)
*Bastián González-Bustamante,Nando Verelst,Carla Cisternas*

Main category: cs.CL

TL;DR: 评估LLM生成合成调查回复的可靠性，与智利概率调查的人类回复对比，发现GPT-4o等模型在信任项目上表现优异，但与人类对齐存在项目异质性和年龄差异。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在调查研究中作为合成受访者的潜力，但需要评估其回复的可靠性、偏见问题以及与人类回复的一致性。

Method: 使用128个提示-模型-问题三元组生成189,696个合成配置文件，通过准确性、精确度、召回率和F1分数等指标，在128个问题-子样本对上进行元分析，测试关键社会人口统计维度的偏见。

Result: 1) 合成回复在信任项目上表现优异(F1分数和准确性>0.90)；2) GPT-4o、GPT-4o-mini和Llama 4 Maverick表现相当；3) 45-59岁受访者的合成-人类对齐度最高。

Conclusion: LLM合成样本能够近似概率样本的回复，但存在显著的项目级异质性，需要仔细校准和额外分布测试以确保算法保真度和减少错误。

Abstract: Large Language Models (LLMs) offer promising avenues for methodological and
applied innovations in survey research by using synthetic respondents to
emulate human answers and behaviour, potentially mitigating measurement and
representation errors. However, the extent to which LLMs recover aggregate item
distributions remains uncertain and downstream applications risk reproducing
social stereotypes and biases inherited from training data. We evaluate the
reliability of LLM-generated synthetic survey responses against ground-truth
human responses from a Chilean public opinion probabilistic survey.
Specifically, we benchmark 128 prompt-model-question triplets, generating
189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,
precision, recall, and F1-score) in a meta-analysis across 128
question-subsample pairs to test for biases along key sociodemographic
dimensions. The evaluation spans OpenAI's GPT family and o-series reasoning
models, as well as Llama and Qwen checkpoints. Three results stand out. First,
synthetic responses achieve excellent performance on trust items (F1-score and
accuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform
comparably on this task. Third, synthetic-human alignment is highest among
respondents aged 45-59. Overall, LLM-based synthetic samples approximate
responses from a probabilistic sample, though with substantial item-level
heterogeneity. Capturing the full nuance of public opinion remains challenging
and requires careful calibration and additional distributional tests to ensure
algorithmic fidelity and reduce errors.

</details>


### [30] [Large Language Models Meet Legal Artificial Intelligence: A Survey](https://arxiv.org/abs/2509.09969)
*Zhitian Hou,Zihan Ye,Nanli Zeng,Tianyong Hao,Kun Zeng*

Main category: cs.CL

TL;DR: 这篇论文系统评估了16个法律LLM系列和47个法律任务框架，汇总了15个测试标准和29个数据集，为法律AI领域提供了全面的资源和研究指南。


<details>
  <summary>Details</summary>
Motivation: 推动大语言模型在法律人工智能领域的研究和应用发展，提升法律任务的效率和准确性。

Method: 通过系统性评估和综述分析，涵盖了多个法律LLM模型、框架、测试标准和数据集，并分析挖掘了相关挑战和未来发展方向。

Result: 构建了丰富的法律AI资源库，包含模型、框架、数据集和评估标准，为该领域提供了全面的研究基础。

Conclusion: 本研究为法律AI领域提供了系统性的入门指南和资源支持，有助于推动该领域的未来研究和应用发展。

Abstract: Large Language Models (LLMs) have significantly advanced the development of
Legal Artificial Intelligence (Legal AI) in recent years, enhancing the
efficiency and accuracy of legal tasks. To advance research and applications of
LLM-based approaches in legal domain, this paper provides a comprehensive
review of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and
also gather 15 benchmarks and 29 datasets to evaluate different legal
capabilities. Additionally, we analyse the challenges and discuss future
directions for LLM-based approaches in the legal domain. We hope this paper
provides a systematic introduction for beginners and encourages future research
in this field. Resources are available at
https://github.com/ZhitianHou/LLMs4LegalAI.

</details>


### [31] [CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China](https://arxiv.org/abs/2509.09990)
*Guixian Xu,Zeli Su,Ziyin Zhang,Jianing Liu,XU Han,Ting Zhang,Yushuang Dong*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Minority languages in China, such as Tibetan, Uyghur, and Traditional
Mongolian, face significant challenges due to their unique writing systems,
which differ from international standards. This discrepancy has led to a severe
lack of relevant corpora, particularly for supervised tasks like headline
generation. To address this gap, we introduce a novel dataset, Chinese Minority
Headline Generation (CMHG), which includes 100,000 entries for Tibetan, and
50,000 entries each for Uyghur and Mongolian, specifically curated for headline
generation tasks. Additionally, we propose a high-quality test set annotated by
native speakers, designed to serve as a benchmark for future research in this
domain. We hope this dataset will become a valuable resource for advancing
headline generation in Chinese minority languages and contribute to the
development of related benchmarks.

</details>


### [32] [Unsupervised Hallucination Detection by Inspecting Reasoning Processes](https://arxiv.org/abs/2509.10004)
*Ponhvoan Srey,Xiaobao Wu,Anh Tuan Luu*

Main category: cs.CL

TL;DR: IRIS是一个无监督幻觉检测框架，利用LLM内部表征来识别生成内容的事实正确性，无需标注数据，计算成本低且适用于实时检测


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法依赖与事实正确性无关的代理信号，导致检测偏向表面特征，限制了跨数据集和场景的泛化能力

Method: 通过提示LLM仔细验证给定陈述的真实性，获取其上下文嵌入作为特征，并将响应不确定性作为真实性的软伪标签

Result: 实验结果表明IRIS在无监督方法中表现一致优于现有方法，即使在少量训练数据下也能良好工作

Conclusion: IRIS是一个完全无监督、计算成本低的框架，能够有效利用LLM内部表征进行幻觉检测，适用于实时应用场景

Abstract: Unsupervised hallucination detection aims to identify hallucinated content
generated by large language models (LLMs) without relying on labeled data.
While unsupervised methods have gained popularity by eliminating
labor-intensive human annotations, they frequently rely on proxy signals
unrelated to factual correctness. This misalignment biases detection probes
toward superficial or non-truth-related aspects, limiting generalizability
across datasets and scenarios. To overcome these limitations, we propose IRIS,
an unsupervised hallucination detection framework, leveraging internal
representations intrinsic to factual correctness. IRIS prompts the LLM to
carefully verify the truthfulness of a given statement, and obtain its
contextualized embedding as informative features for training. Meanwhile, the
uncertainty of each response is considered a soft pseudolabel for truthfulness.
Experimental results demonstrate that IRIS consistently outperforms existing
unsupervised methods. Our approach is fully unsupervised, computationally low
cost, and works well even with few training data, making it suitable for
real-time detection.

</details>


### [33] [Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs](https://arxiv.org/abs/2509.10010)
*Adnan Ahmad,Philine Kowol,Stefan Hillmann,Sebastian Möller*

Main category: cs.CL

TL;DR: 本文通过多标签意图分类任务对比分析了三种开源大语言模型（LLama2-7B、Mistral-7B、Yi-6B）的性能，发现Mistral-7B在少量示例设置下表现最佳，但仍落后于传统的BERT监督分类器。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于评估开源大语言模型在消费硬件上执行复杂多标签意图分类任务的效果，以提升任务导向对话系统的自然语言理解能力。

Method: 使用MultiWOZ 2.1数据集，在少量示例设置下测试三种开源LLM模型，并与BERT监督分类器进行对比。评价指标包括准确率、友识率、F1分数等，同时记录推理时间和显存需求。

Result: Mistral-7B模型在14个意图类别中的11个上F1分数最高，加权平均F1为0.50，表现最佳。但所有生成式LLM的性能都落后于传统的BERT监督分类器。

Conclusion: 研究为小型开源LLM在复杂多意图对话检测中提供了框架支持，虽然生成式模型有一定效果，但监督学习方法仍保持性能优势。

Abstract: In this paper, we provide an extensive analysis of multi-label intent
classification using Large Language Models (LLMs) that are open-source,
publicly available, and can be run in consumer hardware. We use the MultiWOZ
2.1 dataset, a benchmark in the dialogue system domain, to investigate the
efficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,
Mistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot
setup, giving 20 examples in the prompt with some instructions. Our approach
focuses on the differences in performance of these models across several
performance metrics by methodically assessing these models on multi-label
intent classification tasks. Additionally, we compare the performance of the
instruction-based fine-tuning approach with supervised learning using the
smaller transformer model BertForSequenceClassification as a baseline. To
evaluate the performance of the models, we use evaluation metrics like
accuracy, precision, and recall as well as micro, macro, and weighted F1 score.
We also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1
outperforms two other generative models on 11 intent classes out of 14 in terms
of F-Score, with a weighted average of 0.50. It also has relatively lower
Humming Loss and higher Jaccard Similarity, making it the winning model in the
few-shot setting. We find BERT based supervised classifier having superior
performance compared to the best performing few-shot generative LLM. The study
provides a framework for small open-source LLMs in detecting complex
multi-intent dialogues, enhancing the Natural Language Understanding aspect of
task-oriented chatbots.

</details>


### [34] [Linguistic trajectories of bipolar disorder on social media](https://arxiv.org/abs/2509.10035)
*Laurin Plank,Armin Zlomuzica*

Main category: cs.CL

TL;DR: 通过分析社交媒体语言，研究发现双相情感障碍诊断前后的语言变化迹象，包括情绪涉及、养合症、子周期性等特征


<details>
  <summary>Details</summary>
Motivation: 语言是情感障碍的重要标记物，但临床评估规模有限。社交媒体语言分析能提供高时间分辨率和纵向视量

Method: 开发方法确定用户诊断时间，分析发现双相障碍前3年到诊断后21年的语言轨迹，并与单相抑郁和健康对照组进行比较

Result: 发现双相障碍诊断后出现普遍语言改变，反映出情绪扰动、精神养合症、物质滥用等。诊断后两十年内观察到周期性情绪相关语言变化，具有12个月周期性，可能与季节性情绪发作相关

Conclusion: 研究为双相障碍急性期和慢性期的语言改变提供了证据，验证了利用社交媒体进行可扩展的精神健康监测的最新尝试

Abstract: Language provides valuable markers of affective disorders such as bipolar
disorder (BD), yet clinical assessments remain limited in scale. In response,
analyses of social media (SM) language have gained prominence due to their high
temporal resolution and longitudinal scope. Here, we introduce a method to
determine the timing of users' diagnoses and apply it to study language
trajectories from 3 years before to 21 years after BD diagnosis - contrasted
with uses reporting unipolar depression (UD) and non-affected users (HC). We
show that BD diagnosis is accompanied by pervasive linguistic alterations
reflecting mood disturbance, psychiatric comorbidity, substance abuse,
hospitalization, medical comorbidities, unusual thought content, and
disorganized thought. We further observe recurring mood-related language
changes across two decades after the diagnosis, with a pronounced 12-month
periodicity suggestive of seasonal mood episodes. Finally, trend-level evidence
suggests an increased periodicity in users estimated to be female. In sum, our
findings provide evidence for language alterations in the acute and chronic
phase of BD. This validates and extends recent efforts leveraging SM for
scalable monitoring of mental health.

</details>


### [35] [!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment](https://arxiv.org/abs/2509.10040)
*Mohamed Basem,Mohamed Younes,Seif Ahmed,Abdelrahman Moustafa*

Main category: cs.CL

TL;DR: MSA团队在BAREC 2025阿拉伯语可读性评估任务中获胜，通过集成四个Transformer模型、处理类别不平衡和数据稀缺问题，实现了87.5%的QWK分数。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语细粒度可读性评估中的类别不平衡和数据稀缺问题，提升模型在稀有难度级别上的预测性能。

Method: 使用四个互补的Transformer模型（AraBERTv2、AraELECTRA、MARBERT、CAMeLBERT）集成，采用加权训练、SAMER语料库重新标注、通过Gemini 2.5 Flash生成约10,000个合成样本，并进行针对性后处理。

Result: 在六个赛道中获得第一名，句子级别QWK达到87.5%，文档级别达到87.4%，后处理带来6.3%的QWK提升。

Conclusion: 模型和损失函数的多样性、置信度信息融合以及智能数据增强对于构建鲁棒的阿拉伯语可读性预测系统至关重要。

Abstract: We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained
Arabic readability assessment, achieving first place in six of six tracks. Our
approach is a confidence-weighted ensemble of four complementary transformer
models (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with
distinct loss functions to capture diverse readability signals. To tackle
severe class imbalance and data scarcity, we applied weighted training,
advanced preprocessing, SAMER corpus relabeling with our strongest model, and
synthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level
samples. A targeted post-processing step corrected prediction distribution
skew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system
reached 87.5 percent QWK at the sentence level and 87.4 percent at the document
level, demonstrating the power of model and loss diversity, confidence-informed
fusion, and intelligent augmentation for robust Arabic readability prediction.

</details>


### [36] [Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models](https://arxiv.org/abs/2509.10078)
*Dongmin Choi,Woojung Song,Jongwook Han,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 研究对比了传统心理测量问卷和生态有效问卷在LLM人格测量中的差异，发现传统问卷存在多项问题，建议谨慎使用。


<details>
  <summary>Details</summary>
Motivation: 现有研究将人类心理测量问卷直接应用于LLM，但缺乏生态有效性，需要明确两类问卷的差异和含义。

Method: 进行了两类问卷的综合比较分析，包括测量结果差异、测量稳定性、构建稳定性和人设提示效果等方面。

Result: 发现传统问卷：(1)与生态有效问卷结果差异显著，(2)项目数量不足导致测量不稳定，(3)造成LLM具有稳定构建的误导印象，(4)对人设提示的LLM产生夸张的人格描述。

Conclusion: 建议谨慎使用传统心理测量问卷来测量LLM的人格特征，需要考虑生态有效性问题。

Abstract: Researchers have applied established psychometric questionnaires (e.g., BFI,
PVQ) to measure the personality traits and values reflected in the responses of
Large Language Models (LLMs). However, concerns have been raised about applying
these human-designed questionnaires to LLMs. One such concern is their lack of
ecological validity--the extent to which survey questions adequately reflect
and resemble real-world contexts in which LLMs generate texts in response to
user queries. However, it remains unclear how established questionnaires and
ecologically valid questionnaires differ in their outcomes, and what insights
these differences may provide. In this paper, we conduct a comprehensive
comparative analysis of the two types of questionnaires. Our analysis reveals
that established questionnaires (1) yield substantially different profiles of
LLMs from ecologically valid ones, deviating from the psychological
characteristics expressed in the context of user queries, (2) suffer from
insufficient items for stable measurement, (3) create misleading impressions
that LLMs possess stable constructs, and (4) yield exaggerated profiles for
persona-prompted LLMs. Overall, our work cautions against the use of
established psychological questionnaires for LLMs. Our code will be released
upon publication.

</details>


### [37] [Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery](https://arxiv.org/abs/2509.10087)
*Mustapha Adamu,Qi Zhang,Huitong Pan,Longin Jan Latecki,Eduard C. Dragut*

Main category: cs.CL

TL;DR: 构建气候科学领域知识图谱，支持语义查询和RAG系统集成，提升气候研究信息检索的精确性和可靠性


<details>
  <summary>Details</summary>
Motivation: 气候科学文献日益复杂和庞大，传统关键词搜索难以满足研究人员跨模型、数据集、区域和变量查找相关信息的需求

Method: 从气候出版物和科学文本构建领域特定知识图谱，支持Cypher查询语言进行结构化语义查询，并与大语言模型集成构建RAG系统

Result: 知识图谱能够精确回答如特定区域验证的模型、与特定遥相关模式常用数据集等问题，提高了信息检索的准确性和上下文相关性

Conclusion: 该研究不仅构建了知识图谱，更展示了其在气候研究、模型开发等实际应用中的价值，为依赖准确科学信息的研究者提供了可靠工具

Abstract: The growing complexity and volume of climate science literature make it
increasingly difficult for researchers to find relevant information across
models, datasets, regions, and variables. This paper introduces a
domain-specific Knowledge Graph (KG) built from climate publications and
broader scientific texts, aimed at improving how climate knowledge is accessed
and used. Unlike keyword based search, our KG supports structured, semantic
queries that help researchers discover precise connections such as which models
have been validated in specific regions or which datasets are commonly used
with certain teleconnection patterns. We demonstrate how the KG answers such
questions using Cypher queries, and outline its integration with large language
models in RAG systems to improve transparency and reliability in
climate-related question answering. This work moves beyond KG construction to
show its real world value for climate researchers, model developers, and others
who rely on accurate, contextual scientific information.

</details>


### [38] [Arabic Large Language Models for Medical Text Generation](https://arxiv.org/abs/2509.10095)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

TL;DR: 该研究通过微调大型语言模型(如Mistral-7B、LLaMA-2-7B和GPT-2)来生成阿拉伯语医疗文本，旨在解决医院管理系统在实时医疗建议方面的不足，特别是在处理非正规输入和少数语言方面。


<details>
  <summary>Details</summary>
Motivation: 现有医院管理系统缺乏提供准确实时医疗建议的能力，尤其对非正规输入和阿拉伯语等代表性不足的语言支持不足。需要开发能够处理多方言阿拉伯语并提供可靠医疗建议的AI系统。

Method: 收集社交媒体上的真实医患对话数据集，进行数据清洗和预处理以支持多阿拉伯方言。微调Mistral-7B-Instruct-v0.2、LLaMA-2-7B和GPT-2 Medium等先进生成模型。

Result: 微调后的Mistral-7B模型表现最佳，在精确率、召回率和F1分数上分别达到68.5%、69.08%和68.5%的平均BERT分数。定性和定量评估验证了系统生成连贯相关医疗回复的能力。

Conclusion: 生成式AI在推进医院管理系统方面具有巨大潜力，特别是在语言和文化多样化的环境中，提供了可扩展和适应性强的全球医疗解决方案。

Abstract: Efficient hospital management systems (HMS) are critical worldwide to address
challenges such as overcrowding, limited resources, and poor availability of
urgent health care. Existing methods often lack the ability to provide
accurate, real-time medical advice, particularly for irregular inputs and
underrepresented languages. To overcome these limitations, this study proposes
an approach that fine-tunes large language models (LLMs) for Arabic medical
text generation. The system is designed to assist patients by providing
accurate medical advice, diagnoses, drug recommendations, and treatment plans
based on user input. The research methodology required the collection of a
unique dataset from social media platforms, capturing real-world medical
conversations between patients and doctors. The dataset, which includes patient
complaints together with medical advice, was properly cleaned and preprocessed
to account for multiple Arabic dialects. Fine-tuning state-of-the-art
generative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2
Medium, optimized the system's ability to generate reliable medical text.
Results from evaluations indicate that the fine-tuned Mistral-7B model
outperformed the other models, achieving average BERT (Bidirectional Encoder
Representations from Transformers) Score values in precision, recall, and
F1-scores of 68.5\%, 69.08\%, and 68.5\%, respectively. Comparative
benchmarking and qualitative assessments validate the system's ability to
produce coherent and relevant medical replies to informal input. This study
highlights the potential of generative artificial intelligence (AI) in
advancing HMS, offering a scalable and adaptable solution for global healthcare
challenges, especially in linguistically and culturally diverse environments.

</details>


### [39] [Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records](https://arxiv.org/abs/2509.10108)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Khaled Shaban*

Main category: cs.CL

TL;DR: 通过ChatGPT-4o和Gemini生成合成数据扩充阿拉伯语医疗数据集，将训练集扩展到10万条，优化后的LLM模型在医疗聊天机器人任务上表现更优。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语医疗数据集稀缺且质量不高，限制了医疗聊天机器语言模型的可扩展性和通用性。

Method: 使用ChatGPT-4o和Gemini 2.5 Pro生成80,000条上下文相关的医学问答对，经过语义筛选和手动验证。对Mistral-7B等五个LLM进行微调，使用BERTScore和专家评估。

Result: ChatGPT-4o生成的数据在所有模型中都导致更高的F1分数和更少的幻觉现象。

Conclusion: 合成数据扩充是提升低资源医疗NLP领域语言模型的可行方案，为更包容、可扩展和准确的阿拉伯语健康聊天机器开启了道路。

Abstract: The development of medical chatbots in Arabic is significantly constrained by
the scarcity of large-scale, high-quality annotated datasets. While prior
efforts compiled a dataset of 20,000 Arabic patient-doctor interactions from
social media to fine-tune large language models (LLMs), model scalability and
generalization remained limited. In this study, we propose a scalable synthetic
data augmentation strategy to expand the training corpus to 100,000 records.
Using advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated
80,000 contextually relevant and medically coherent synthetic question-answer
pairs grounded in the structure of the original dataset. These synthetic
samples were semantically filtered, manually validated, and integrated into the
training pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,
and evaluated their performance using BERTScore metrics and expert-driven
qualitative assessments. To further analyze the effectiveness of synthetic
sources, we conducted an ablation study comparing ChatGPT-4o and
Gemini-generated data independently. The results showed that ChatGPT-4o data
consistently led to higher F1-scores and fewer hallucinations across all
models. Overall, our findings demonstrate the viability of synthetic
augmentation as a practical solution for enhancing domain-specific language
models in-low resource medical NLP, paving the way for more inclusive,
scalable, and accurate Arabic healthcare chatbot systems.

</details>


### [40] [Prominence-aware automatic speech recognition for conversational speech](https://arxiv.org/abs/2509.10116)
*Julian Linke,Barbara Schuppler*

Main category: cs.CL

TL;DR: 该论文研究通过结合重音检测和语音识别来开发奥地利德语对话中的重音感知自动语音识别系统，使用wav2vec2模型进行重音分类，并在大型语料库中自动标注韵律重音，最终训练出能同时转录单词及其重音水平的ASR系统。


<details>
  <summary>Details</summary>
Motivation: 研究如何将韵律重音信息整合到自动语音识别系统中，以提升对奥地利德语对话的理解能力，为语言学研究和对韵律敏感的对话系统提供潜在应用。

Method: 1. 微调wav2vec2模型开发词级重音检测器；2. 使用检测器在大型语料库中自动标注韵律重音；3. 基于标注训练能同时转录单词和重音水平的重音感知ASR系统。

Result: 整合重音信息后ASR性能与基线系统相当，在识别正确的语句中重音检测准确率达到85.53%。基于transformer的模型能有效编码韵律信息。

Conclusion: 该研究展示了transformer模型在编码韵律信息方面的有效性，为韵律增强的语音识别做出了新颖贡献，具有语言学研究和对韵律敏感的对话系统的应用潜力。

Abstract: This paper investigates prominence-aware automatic speech recognition (ASR)
by combining prominence detection and speech recognition for conversational
Austrian German. First, prominence detectors were developed by fine-tuning
wav2vec2 models to classify word-level prominence. The detector was then used
to automatically annotate prosodic prominence in a large corpus. Based on those
annotations, we trained novel prominence-aware ASR systems that simultaneously
transcribe words and their prominence levels. The integration of prominence
information did not change performance compared to our baseline ASR system,
while reaching a prominence detection accuracy of 85.53% for utterances where
the recognized word sequence was correct. This paper shows that
transformer-based models can effectively encode prosodic information and
represents a novel contribution to prosody-enhanced ASR, with potential
applications for linguistic research and prosody-informed dialogue systems.

</details>


### [41] [Population-Aligned Persona Generation for LLM-based Social Simulation](https://arxiv.org/abs/2509.10127)
*Zhengyu Hu,Zheyuan Xiao,Max Xiong,Yuxuan Lei,Tianfu Wang,Jianxun Lian,Kaize Ding,Ziang Xiao,Nicholas Jing Yuan,Xing Xie*

Main category: cs.CL

TL;DR: 提出了一个系统框架，用于合成高质量、与人口分布对齐的LLM驱动社交模拟角色集，通过社交媒体数据生成、质量评估和重要性采样来减少偏见。


<details>
  <summary>Details</summary>
Motivation: 现有LLM社交模拟研究主要关注代理框架和模拟环境设计，忽视了角色生成的复杂性和非代表性角色集带来的潜在偏见问题。

Method: 利用LLM从长期社交媒体数据生成叙事角色，进行严格质量评估过滤低质量档案，应用重要性采样实现与参考心理测量分布（如大五人格）的全局对齐，并引入任务特定模块适应目标子群体。

Result: 实验表明该方法显著减少了人口层面的偏见，为广泛的研究和政策应用实现了准确、灵活的社交模拟。

Conclusion: 该框架能够生成高质量、与真实人口分布对齐的角色集，为LLM驱动的社交模拟提供了更可靠的基础。

Abstract: Recent advances in large language models (LLMs) have enabled human-like
social simulations at unprecedented scale and fidelity, offering new
opportunities for computational social science. A key challenge, however, is
the construction of persona sets that authentically represent the diversity and
distribution of real-world populations. Most existing LLM-based social
simulation studies focus primarily on designing agentic frameworks and
simulation environments, often overlooking the complexities of persona
generation and the potential biases introduced by unrepresentative persona
sets. In this paper, we propose a systematic framework for synthesizing
high-quality, population-aligned persona sets for LLM-driven social simulation.
Our approach begins by leveraging LLMs to generate narrative personas from
long-term social media data, followed by rigorous quality assessment to filter
out low-fidelity profiles. We then apply importance sampling to achieve global
alignment with reference psychometric distributions, such as the Big Five
personality traits. To address the needs of specific simulation contexts, we
further introduce a task-specific module that adapts the globally aligned
persona set to targeted subpopulations. Extensive experiments demonstrate that
our method significantly reduces population-level bias and enables accurate,
flexible social simulation for a wide range of research and policy
applications.

</details>


### [42] [Towards Reliable and Interpretable Document Question Answering via VLMs](https://arxiv.org/abs/2509.10129)
*Alessio Chen,Simone Giovannini,Andrea Gemelli,Fabio Coppini,Simone Marinai*

Main category: cs.CL

TL;DR: DocExplainerV0是一个即插即用的边界框预测模块，将答案生成与空间定位解耦，解决了现有视觉语言模型在文档理解中答案定位不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在文档理解方面表现出强大能力，但准确地在文档中定位答案仍然是一个主要挑战，这限制了模型的可解释性和实际应用。

Method: 提出了DocExplainerV0模块，该模块可以与现有VLM系统配合使用，包括无法进行微调的专有系统，通过解耦答案生成和空间定位来实现精确的边界框预测。

Result: 系统评估揭示了文本准确性和空间定位之间的差距，显示正确答案往往缺乏可靠的空间定位。该框架为未来研究建立了基准。

Conclusion: DocExplainerV0为解决VLM在文档信息提取中的空间定位问题提供了一个标准化框架，推动了更可解释和鲁棒的文档理解模型的发展。

Abstract: Vision-Language Models (VLMs) have shown strong capabilities in document
understanding, particularly in identifying and extracting textual information
from complex documents. Despite this, accurately localizing answers within
documents remains a major challenge, limiting both interpretability and
real-world applicability. To address this, we introduce
\textit{DocExplainerV0}, a plug-and-play bounding-box prediction module that
decouples answer generation from spatial localization. This design makes it
applicable to existing VLMs, including proprietary systems where fine-tuning is
not feasible. Through systematic evaluation, we provide quantitative insights
into the gap between textual accuracy and spatial grounding, showing that
correct answers often lack reliable localization. Our standardized framework
highlights these shortcomings and establishes a benchmark for future research
toward more interpretable and robust document information extraction VLMs.

</details>


### [43] [Benchmark of stylistic variation in LLM-generated texts](https://arxiv.org/abs/2509.10179)
*Jiří Milička,Anna Marklová,Václav Cvrček*

Main category: cs.CL

TL;DR: 这研究使用Biber多维分析法比较了人类与大语言模型文本的语域变异，创建了可解释的测试基准来评估各模型的表现。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型与人类文本在语域特征上的系统性差异，并考虑到非英语语言在LLM训练数据中的代表性不足问题。

Method: 采用Biber多维分析法，对比分析人类写作文本与AI生成文本的语域变异。使用AI-Brown语料库（与BE-21对应）和AI-Koditex捷克语语料库，研究了16个前沿LLM模型在不同设置和提示下的表现。

Result: 识别了LLM与人类文本在多维度变异上最显著和最系统的差异，并创建了一个可用于模型比较和排名的解释性基准。

Conclusion: 研究为评估大语言模型的语域特征提供了系统方法，显示了基础模型与指令微调模型的差异，并为多语言研究提供了参考。

Abstract: This study investigates the register variation in texts written by humans and
comparable texts produced by large language models (LLMs). Biber's
multidimensional analysis (MDA) is applied to a sample of human-written texts
and AI-created texts generated to be their counterparts to find the dimensions
of variation in which LLMs differ most significantly and most systematically
from humans. As textual material, a new LLM-generated corpus AI-Brown is used,
which is comparable to BE-21 (a Brown family corpus representing contemporary
British English). Since all languages except English are underrepresented in
the training data of frontier LLMs, similar analysis is replicated on Czech
using AI-Koditex corpus and Czech multidimensional model. Examined were 16
frontier models in various settings and prompts, with emphasis placed on the
difference between base models and instruction-tuned models. Based on this, a
benchmark is created through which models can be compared with each other and
ranked in interpretable dimensions.

</details>


### [44] [Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations](https://arxiv.org/abs/2509.10184)
*Leen Almajed,Abeer ALdayel*

Main category: cs.CL

TL;DR: 本文研究了情感支持对话中不协调的积极性现象，分析了人类和LLM生成回应中的过度乐观、轻视或脱离现实的积极支持表达，特别是在高风险情境下。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解在情感支持对话中，善意的积极回应有时会适得其反，产生轻视、最小化或不切实际乐观的回应，这种现象在人类和LLM生成内容中都存在。

Method: 方法包括：收集Reddit真实用户-助手对话数据；按情感强度分类为轻度（关系紧张、一般建议）和重度（悲伤、焦虑）；微调LLM在不同情感反应数据集上；开发弱监督多标签分类器集成（DeBERTa和MentalBERT）来检测不协调积极性类型。

Result: 研究发现LLM更容易在高风险情境下通过轻视和最小化的语气表现出不切实际的积极性。开发的分类器集成在检测两种关注类型（轻度和重度）的不协调积极性方面表现改善。

Conclusion: 结论指出需要超越生成通用积极回应，研究协调的支持措施来平衡积极情感与情感认同，为在线支持性对话中与情感期望对齐的大语言模型提供见解，推动上下文感知和信任保护的在线对话系统发展。

Abstract: In emotionally supportive conversations, well-intended positivity can
sometimes misfire, leading to responses that feel dismissive, minimizing, or
unrealistically optimistic. We examine this phenomenon of incongruent
positivity as miscalibrated expressions of positive support in both human and
LLM generated responses. To this end, we collected real user-assistant
dialogues from Reddit across a range of emotional intensities and generated
additional responses using large language models for the same context. We
categorize these conversations by intensity into two levels: Mild, which covers
relationship tension and general advice, and Severe, which covers grief and
anxiety conversations. This level of categorization enables a comparative
analysis of how supportive responses vary across lower and higher stakes
contexts. Our analysis reveals that LLMs are more prone to unrealistic
positivity through dismissive and minimizing tone, particularly in high-stakes
contexts. To further study the underlying dimensions of this phenomenon, we
finetune LLMs on datasets with strong and weak emotional reactions. Moreover,
we developed a weakly supervised multilabel classifier ensemble (DeBERTa and
MentalBERT) that shows improved detection of incongruent positivity types
across two sorts of concerns (Mild and Severe). Our findings shed light on the
need to move beyond merely generating generic positive responses and instead
study the congruent support measures to balance positive affect with emotional
acknowledgment. This approach offers insights into aligning large language
models with affective expectations in the online supportive dialogue, paving
the way toward context-aware and trust preserving online conversation systems.

</details>


### [45] [Beyond Token Limits: Assessing Language Model Performance on Long Text Classification](https://arxiv.org/abs/2509.10199)
*Miklós Sebők,Viktor Kovács,Martin Bánóczy,Daniel Møller Eriksen,Nathalie Neptune,Philippe Roussille*

Main category: cs.CL

TL;DR: 本文比较了多种大语言模型在处理长文本分类任务（特别是法律文件）时的性能，发现专门为长文本设计的Longformer模型并无明显优势，开源模型在某些方面表现优于GPT变体。


<details>
  <summary>Details</summary>
Motivation: 现有主流语言模型（如BERT及其衍生模型）存在输入长度限制，无法有效处理长达数百页的法律文件和草案，这在社会科学的长文本分类任务中是一个亟待解决的问题。

Method: 在5种语言上实验了XLM-RoBERTa、Longformer、GPT-3.5和GPT-4模型，使用比较议程项目的21个政策主题标签进行多类分类任务评估。

Result: 专门为长文本预训练的Longformer模型没有显示出明显优势；开源模型在比较中表现优于GPT变体；类别层面的分析显示，特定类别间的支持度和内容重叠对长文本输入性能有重要影响。

Conclusion: 对于长文本分类任务，专门的长文本模型不一定优于通用模型，开源模型在某些场景下可能比商业GPT模型更具优势，类别间的相似性会影响模型性能。

Abstract: The most widely used large language models in the social sciences (such as
BERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text
length that they can process to produce predictions. This is a particularly
pressing issue for some classification tasks, where the aim is to handle long
input texts. One such area deals with laws and draft laws (bills), which can
have a length of multiple hundred pages and, therefore, are not particularly
amenable for processing with models that can only handle e.g. 512 tokens. In
this paper, we show results from experiments covering 5 languages with
XLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass
classification task of the Comparative Agendas Project, which has a codebook of
21 policy topic labels from education to health care. Results show no
particular advantage for the Longformer model, pre-trained specifically for the
purposes of handling long inputs. The comparison between the GPT variants and
the best-performing open model yielded an edge for the latter. An analysis of
class-level factors points to the importance of support and substance overlaps
between specific categories when it comes to performance on long text inputs.

</details>


### [46] [SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning](https://arxiv.org/abs/2509.10208)
*Shengqiang Fu*

Main category: cs.CL

TL;DR: 自我改进忠实性矩阵对比调整框架(SI FACT)，通过自指导机制自动生成对比学习数据，提升LLM在知识冲突任务中的上下文忠实性


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在知识密集任务中优先依赖内部参数化知识而非提供上下文的忠实性问题

Method: 使用自指导机制自动生成高质量结构化对比学习数据(锚点样本、语义等价正样本、不忠实负样本)，通过对比学习训练模型在表征空间中拉近忠实响应并推开不忠实响应

Result: 在ECARE KRE和COSE KRE知识冲突评估基准上，Llama3 8B Instruct基础的SI FACT模型将上下文回忆率提高6.2%，显著减少了对内部记忆的依赖

Conclusion: SI FACT在提升LLM上下文忠实性方面具有强大效果和高数据效率，为构建更主动可信语言模型提供了实用途径

Abstract: Large Language Models often generate unfaithful responses in knowledge
intensive tasks due to knowledge conflict,that is,a preference for relying on
internal parametric knowledge rather than the provided context.To address this
issue,we propose a novel self improving framework,Self Improving Faithfulness
Aware Contrastive Tuning.The framework uses a self instruct mechanism that
allows the base LLM to automatically generate high quality,structured
contrastive learning data,including anchor samples,semantically equivalent
positive samples,and negative samples simulating unfaithful scenarios.This
approach significantly reduces the cost of manual
annotation.Subsequently,contrastive learning is applied to train the
model,enabling it to pull faithful responses closer and push unfaithful
responses farther apart in the representation space.Experiments on knowledge
conflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT
model based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%
over the best baseline method,while significantly reducing dependence on
internal memory.The results indicate that SI FACT provides strong effectiveness
and high data efficiency in enhancing the contextual faithfulness of
LLMs,offering a practical pathway toward building more proactive and
trustworthy language models.

</details>


### [47] [Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs](https://arxiv.org/abs/2509.10377)
*Yixiao Zhou,Ziyu Zhao,Dongzhou Cheng,zhiliang wu,Jie Gui,Yi Yang,Fei Wu,Yu Cheng,Hehe Fan*

Main category: cs.CL

TL;DR: DERN是一种无需重新训练的任务无关框架，通过专家剪枝和神经元重组来减少SMoE模型的内存占用，在50%专家稀疏度下性能提升超过5%


<details>
  <summary>Details</summary>
Motivation: SMoE架构虽然计算效率高，但仍需加载所有专家参数，导致内存占用高和部署困难。现有方法主要关注专家级操作，忽视了神经元级结构的优化潜力

Method: 三步骤框架：1) 基于路由器统计剪枝冗余专家；2) 将专家分解为神经元级片段并分配到最兼容的保留专家；3) 在保留专家内合并片段构建紧凑表示

Result: 在Mixtral、Qwen和DeepSeek SMoE模型上，50%专家稀疏度下常识推理和MMLU基准性能提升超过5%，同时显著减少专家数量和内存使用

Conclusion: DERN有效解决了SMoE模型的内存占用问题，通过神经元级重组实现了性能提升和部署便利性的双重优化，无需额外训练

Abstract: Sparse Mixture-of-Experts (SMoE) architectures are widely used in large
language models (LLMs) due to their computational efficiency. However, though
only a few experts are activated for each token, SMoE still requires loading
all expert parameters, leading to high memory usage and challenges in
deployment. Previous work has tried to reduce the overhead by pruning and
merging experts, but primarily focused on expert-level operations, leaving
neuron-level structure underexplored. We propose DERN (Dropping Experts,
Recombining Neurons), a task-agnostic and retraining-free framework for expert
pruning and reconstruction. We observe that experts are often misaligned and
contain semantic conflicts at the neuron level, which poses challenges for
direct merging. To solve this, DERN works in three steps: it first prunes
redundant experts using router statistics; then it decomposes them into
neuron-level expert segments, assigning each segment to its most compatible
retained expert; and finally, it merges segments within each retained expert to
build a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE
models show that DERN improves performance by more than 5% on commonsense
reasoning and MMLU benchmarks under 50% expert sparsity, without extra
training. It also greatly reduces the number of experts and memory usage,
making SMoE LLMs easier to deploy in practice.

</details>


### [48] [Is In-Context Learning Learning?](https://arxiv.org/abs/2509.10414)
*Adrian de Wynter*

Main category: cs.CL

TL;DR: 本文通过大规模实验分析发现，上下文学习(ICL)确实是一种有效的学习范式，但其学习能力和泛化到未见任务的能力有限。ICL主要从提示中的规律性推断模式，而非真正的学习，导致分布敏感性，特别是在思维链等提示风格中。


<details>
  <summary>Details</summary>
Motivation: 尽管自回归模型通过上下文学习(ICL)能够在无需额外训练的情况下解决任务，但ICL是否真正构成学习仍存在争议。研究者希望从数学和实证角度分析ICL的学习本质及其局限性。

Method: 进行了大规模ICL分析实验，通过消融实验控制记忆化、预训练、分布偏移、提示风格和措辞等因素，系统评估ICL在不同条件下的表现。

Result: ICL是一种有效的学习范式，但学习能力和泛化能力有限。当示例数量足够多时，准确率对示例分布、模型、提示风格和输入语言特征不敏感。ICL主要从提示中的规律性推断模式，导致分布敏感性，特别是在思维链提示中。

Conclusion: 自回归模型的临时编码机制不够鲁棒，表明其通用泛化能力有限。ICL更多是基于提示中的模式推断而非真正的学习过程。

Abstract: In-context learning (ICL) allows some autoregressive models to solve tasks
via next-token prediction and without needing further training. This has led to
claims about these model's ability to solve (learn) unseen tasks with only a
few shots (exemplars) in the prompt. However, deduction does not always imply
learning, as ICL does not explicitly encode a given observation. Instead, the
models rely on their prior knowledge and the exemplars given, if any. We argue
that, mathematically, ICL does constitute learning, but its full
characterisation requires empirical work. We then carry out a large-scale
analysis of ICL ablating out or accounting for memorisation, pretraining,
distributional shifts, and prompting style and phrasing. We find that ICL is an
effective learning paradigm, but limited in its ability to learn and generalise
to unseen tasks. We note that, in the limit where exemplars become more
numerous, accuracy is insensitive to exemplar distribution, model, prompt
style, and the input's linguistic features. Instead, it deduces patterns from
regularities in the prompt, which leads to distributional sensitivity,
especially in prompting styles such as chain-of-thought. Given the varied
accuracies on formally similar tasks, we conclude that autoregression's ad-hoc
encoding is not a robust mechanism, and suggests limited all-purpose
generalisability.

</details>


### [49] [Long Context Automated Essay Scoring with Language Models](https://arxiv.org/abs/2509.10417)
*Christopher Ormerod,Gitit Kehat*

Main category: cs.CL

TL;DR: 这篇论文研究了Transformer模型在自动论文评分中的长文本处理问题，评估了多种支持长上下文的模型等效析


<details>
  <summary>Details</summary>
Motivation: 高分论文经常超过Transformer模型的最大长度限制，截断处理会影响对论文组织结构的评估，引发效度疑虑

Method: 使用Kaggle ASAP 2.0数据集，对XLNet、Longformer、ModernBERT、Mamba和Llama模型进行微调，评估它们在长文本处理方面的表现

Result: 文中没有明确提供具体的评估结果数据，但提出了多种模型的对比分析框架

Conclusion: 需要使用支持长上下文的模型来解决自动论文评分中的长文本处理问题，以保证评估的完整性和有效性

Abstract: Transformer-based language models are architecturally constrained to process
text of a fixed maximum length. Essays written by higher-grade students
frequently exceed the maximum allowed length for many popular open-source
models. A common approach to addressing this issue when using these models for
Automated Essay Scoring is to truncate the input text. This raises serious
validity concerns as it undermines the model's ability to fully capture and
evaluate organizational elements of the scoring rubric, which requires long
contexts to assess. In this study, we evaluate several models that incorporate
architectural modifications of the standard transformer architecture to
overcome these length limitations using the Kaggle ASAP 2.0 dataset. The models
considered in this study include fine-tuned versions of XLNet, Longformer,
ModernBERT, Mamba, and Llama models.

</details>


### [50] [RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment](https://arxiv.org/abs/2509.10436)
*Shadikur Rahman,Aroosa Hameed,Gautam Srivastava,Syed Muhammad Danish*

Main category: cs.CL

TL;DR: 这篇论文提出了一种云边协同的多段代理推理框架，通过GuideLLM、SolverLLM和JudgeLLM三个专门模块来提升LLM的编码问题解决能力，并创建了RefactorCoderQA标准数据集进行评估，实验结果显示该方法在多域编码任务上达到了独创性能。


<details>
  <summary>Details</summary>
Motivation: 为了充分发挥云端大模型的计算能力和边缘设备的实时响应优势，克服现有标准化测试集的局限性，提升大语言模型在多技术领域编码任务中的推理和问题解决能力。

Method: 设计云边协同架构：GuideLLM（边缘轻量指导模型）提供方法论指导，SolverLLM（云端强大模型）生成代码解决方案，JudgeLLM（自动评估器）评估解决方案的正确性和质量。创建RefactorCoderQA标准化测试集，涵盖软件工程、数据科学、机器学习和自然语言处理等多个技术领域的真实编码挑战。

Result: 细化调整后的RefactorCoder-MoE模型在RefactorCoderQA数据集上达到了独创的性能水平，总体准确率为76.84%，显著超过了领先的开源和商业基线模型。人工评估进一步验证了生成解决方案的可解释性、准确性和实践相关性。系统级指标（如吞吐量和延迟）分析也揭示了该架构的性能特征和权衡。

Conclusion: 这种云边协同的多段代理推理框架能够有效提升大语言模型在复杂编码任务中的表现，通过合理的资源分配和专门化模块设计，实现了性能与效率的最优平衡。RefactorCoderQA标准数据集为多域编码能力评估提供了可靠的基准。

Abstract: To optimize the reasoning and problem-solving capabilities of Large Language
Models (LLMs), we propose a novel cloud-edge collaborative architecture that
enables a structured, multi-agent prompting framework. This framework comprises
three specialized components: GuideLLM, a lightweight model deployed at the
edge to provide methodological guidance; SolverLLM, a more powerful model
hosted in the cloud responsible for generating code solutions; and JudgeLLM, an
automated evaluator for assessing solution correctness and quality. To evaluate
and demonstrate the effectiveness of this architecture in realistic settings,
we introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate
and enhance the performance of Large Language Models (LLMs) across multi-domain
coding tasks. Motivated by the limitations of existing benchmarks,
RefactorCoderQA systematically covers various technical domains, including
Software Engineering, Data Science, Machine Learning, and Natural Language
Processing, using authentic coding challenges from Stack Overflow. Extensive
experiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves
state-of-the-art performance, significantly outperforming leading open-source
and commercial baselines with an overall accuracy of 76.84%. Human evaluations
further validate the interpretability, accuracy, and practical relevance of the
generated solutions. In addition, we evaluate system-level metrics, such as
throughput and latency, to gain deeper insights into the performance
characteristics and trade-offs of the proposed architecture.

</details>


### [51] [DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL](https://arxiv.org/abs/2509.10446)
*Rui Lu,Zhenyu Hou,Zihan Wang,Hanchen Zhang,Xiao Liu,Yujiang Li,Shi Feng,Jie Tang,Yuxiao Dong*

Main category: cs.CL

TL;DR: DeepDive通过自动合成复杂问题和多轮强化学习训练，提升了开源大语言模型在深度搜索任务中的表现，在BrowseComp基准上取得了新的开源竞争性结果


<details>
  <summary>Details</summary>
Motivation: 当前开源大语言模型在深度搜索任务中表现不佳，主要受限于长程推理能力和缺乏足够难度的监督数据

Method: 1) 从开放知识图谱自动合成复杂、困难且难以找到的问题；2) 应用端到端多轮强化学习来增强LLMs的深度搜索长程推理能力

Result: DeepDive-32B在BrowseComp基准上超越WebSailor、DeepSeek-R1-Browse和Search-o1，取得了新的开源竞争性结果。多轮RL训练显著提升了深度搜索能力，并支持测试时工具调用扩展和并行采样

Conclusion: DeepDive通过自动数据合成和多轮强化学习的结合，有效解决了开源LLMs在深度搜索任务中的性能瓶颈，为构建更强大的深度搜索智能体提供了可行方案

Abstract: Augmenting large language models (LLMs) with browsing tools substantially
improves their potential as deep search agents to solve complex, real-world
tasks. Yet, open LLMs still perform poorly in such settings due to limited
long-horizon reasoning capacity with browsing tools and the lack of
sufficiently difficult supervised data. To address these challenges, we present
DeepDive to advance deep search agents. First, we propose a strategy to
automatically synthesize complex, difficult, and hard-to-find questions from
open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement
learning (RL) to enhance LLMs' long-horizon reasoning with deep search.
Experiments show that DeepDive-32B achieves a new open-source competitive
result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and
Search-o1. We demonstrate that multi-turn RL training improves deep search
ability and significantly contributes to the performance improvements across
multiple benchmarks. We observe that DeepDive enables test-time scaling of tool
calls and parallel sampling. All datasets, models, and code are publicly
available at https://github.com/THUDM/DeepDive.

</details>


### [52] [WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers](https://arxiv.org/abs/2509.10452)
*Akshat Pandey,Karun Kumar,Raphael Tang*

Main category: cs.CL

TL;DR: WhisTLE是一种仅使用文本进行语音识别模型领域适应的深度监督方法，通过变分自编码器建模编码器输出并微调解码器，无需额外推理成本即可显著降低词错误率。


<details>
  <summary>Details</summary>
Motivation: 预训练语音识别模型如Whisper在未见词汇和方言上表现不佳，但在许多实际场景中收集语音数据不现实，需要仅使用文本进行领域适应。

Method: 提出WhisTLE方法：训练变分自编码器(VAE)从文本建模编码器输出，使用学习的文本到潜在编码器微调解码器，可选结合文本到语音(TTS)适应。推理时恢复原始编码器，无额外运行时成本。

Result: 在四个领域外数据集和四个ASR模型上，WhisTLE结合TTS相比仅使用TTS适应相对降低词错误率12.3%，在32个场景中的27个优于所有非WhisTLE基线方法。

Conclusion: WhisTLE是一种有效的文本only领域适应方法，能够显著提升预训练ASR模型在未见领域上的性能，且不增加推理成本。

Abstract: Pretrained automatic speech recognition (ASR) models such as Whisper perform
well but still need domain adaptation to handle unseen vocabulary and parlance.
In many real-world settings, collecting speech data is impractical,
necessitating text-only adaptation. We propose WhisTLE, a deeply supervised,
text-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE
trains a variational autoencoder (VAE) to model encoder outputs from text and
fine-tunes the decoder using the learned text-to-latent encoder, optionally
combined with text-to-speech (TTS) adaptation. At inference, the original
encoder is restored, incurring no extra runtime cost. Across four out-of-domain
datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by
12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines
in 27 of 32 scenarios.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [53] [Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision](https://arxiv.org/abs/2509.09720)
*Akansel Cosgun,Lachlan Chumbley,Benjamin J. Meyer*

Main category: cs.CV

TL;DR: ASOS是一个包含50种澳大利亚超市常见物品的高质量3D纹理网格数据集，专为机器人和计算机视觉基准测试设计


<details>
  <summary>Details</summary>
Motivation: 现有数据集多依赖合成模型或难以获取的专业物品，缺乏真实世界应用的实用性，需要成本效益高且易于获取的真实物品数据集

Method: 使用运动结构恢复技术和高分辨率成像采集物品的3D网格，生成水密网格，涵盖10个不同类别，具有多样化的形状、尺寸和重量

Result: 创建了包含50种超市常见物品的综合性数据集，所有物品均可从澳大利亚主要超市连锁店轻松获取，提供了高质量的3D纹理网格

Conclusion: ASOS数据集强调可访问性和真实世界适用性，对物体检测、姿态估计和机器人应用等基准测试具有重要价值

Abstract: This paper introduces the Australian Supermarket Object Set (ASOS), a
comprehensive dataset comprising 50 readily available supermarket items with
high-quality 3D textured meshes designed for benchmarking in robotics and
computer vision applications. Unlike existing datasets that rely on synthetic
models or specialized objects with limited accessibility, ASOS provides a
cost-effective collection of common household items that can be sourced from a
major Australian supermarket chain. The dataset spans 10 distinct categories
with diverse shapes, sizes, and weights. 3D meshes are acquired by a
structure-from-motion techniques with high-resolution imaging to generate
watertight meshes. The dataset's emphasis on accessibility and real-world
applicability makes it valuable for benchmarking object detection, pose
estimation, and robotics applications.

</details>


### [54] [A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval](https://arxiv.org/abs/2509.09721)
*Jiayi Miao,Dingxin Lu,Zhuqi Wang*

Main category: cs.CV

TL;DR: 一种多模态检索增强生成框架(MM-RAG)，用于灾后房屋损坏评估，通过双分支编码器结构实现图像和文本的跨模态语义对齐，在损坏程度分类和检索准确率方面取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 自然灾害后对房屋损坏进行准确评估对保险赔付和资源规划至关重要，需要一种能够合并图像和文本信息的多模态解决方案。

Method: 提出双分支多模态编码器结构：图像分支使用ResNet和Transformer编码器，文本分支使用BERT检索器。集成跨模态交互模块通过多头注意力机制实现语义对齐。使用模态注意力间控制机制动态调控生成过程中的视觉证据和文本信息。采用端到端训练和多任务优化目标。

Result: 在检索准确率和损坏程度分类指标上表现优异，Top-1检索准确率提高了9.6%。

Conclusion: 该MM-RAG框架通过有效的多模态整合和协同学习，能够在灾后房屋损坏评估任务中实现更准确的图像理解和政策匹配。

Abstract: After natural disasters, accurate evaluations of damage to housing are
important for insurance claims response and planning of resources. In this
work, we introduce a novel multimodal retrieval-augmented generation (MM-RAG)
framework. On top of classical RAG architecture, we further the framework to
devise a two-branch multimodal encoder structure that the image branch employs
a visual encoder composed of ResNet and Transformer to extract the
characteristic of building damage after disaster, and the text branch harnesses
a BERT retriever for the text vectorization of posts as well as insurance
policies and for the construction of a retrievable restoration index. To impose
cross-modal semantic alignment, the model integrates a cross-modal interaction
module to bridge the semantic representation between image and text via
multi-head attention. Meanwhile, in the generation module, the introduced modal
attention gating mechanism dynamically controls the role of visual evidence and
text prior information during generation. The entire framework takes end-to-end
training, and combines the comparison loss, the retrieval loss and the
generation loss to form multi-task optimization objectives, and achieves image
understanding and policy matching in collaborative learning. The results
demonstrate superior performance in retrieval accuracy and classification index
on damage severity, where the Top-1 retrieval accuracy has been improved by
9.6%.

</details>


### [55] [Improving MLLM Historical Record Extraction with Test-Time Image](https://arxiv.org/abs/2509.09722)
*Taylor Archibald,Tony Martinez*

Main category: cs.CV

TL;DR: 通过多重增广图像和美元系列模型进行编阵对齐融合，提高噪声历史文档文本提取的稳定性和准确性


<details>
  <summary>Details</summary>
Motivation: 解决噪声历史文档中文本提取不稳定的问题，提高识别准确度

Method: 使用Gemini 2.0 Flash对增广后的多个图像变体进行转写，然后用自定义Needleman Wunsch类型对齐器融合输出，产生共识转写和信心度评分

Result: 在622份庞法尼亚死亡记录数据集上，识别准确度提高4个百分点，填充和模糊增广效果最佳，网格扭曲变形最利于区分不同信心度的案例

Conclusion: 该方法简单、可扩展、立即可部署到其他文档集和转写模型中

Abstract: We present a novel ensemble framework that stabilizes LLM based text
extraction from noisy historical documents. We transcribe multiple augmented
variants of each image with Gemini 2.0 Flash and fuse these outputs with a
custom Needleman Wunsch style aligner that yields both a consensus
transcription and a confidence score. We present a new dataset of 622
Pennsylvania death records, and demonstrate our method improves transcription
accuracy by 4 percentage points relative to a single shot baseline. We find
that padding and blurring are the most useful for improving accuracy, while
grid warp perturbations are best for separating high and low confidence cases.
The approach is simple, scalable, and immediately deployable to other document
collections and transcription models.

</details>


### [56] [MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance](https://arxiv.org/abs/2509.09730)
*Kaikai Zhao,Zhaoxiang Liu,Peng Wang,Xin Wang,Zhicheng Ma,Yajun Xu,Wenjing Zhang,Yibing Nan,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: 这篇论文提出了首个专门用于智能交通监控领域的大规模多模态数据集MITS，包含17万张实际监控图片和500万指令回应对，显著提升了多个LMM模型在ITS任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有通用大多模态模型在智能交通监控领域表现有限，主要因为缺乏专门的多模态数据集。

Method: 构建MITS数据集：收集170,400张实际监控图片，进行八个主类和24个子类的标注，通过系统化流程生成图片描述和500万指令回应对，涵盖五项关键ITS任务。

Result: 在MITS上微调的LMM模型性能显著提升：LLaVA-1.5从0.494提升到0.905(+83.2%)，LLaVA-1.6从0.678到0.921(+35.8%)，Qwen2-VL从0.584到0.926(+58.6%)，Qwen2.5-VL从0.732到0.930(+27.0%)。

Conclusion: MITS数据集有效解决了ITS领域缺乏专业数据的问题，显著提升了LMM模型在交通监控任务上的性能，为ITS和LMM研究提供了高价值资源。

Abstract: General-domain large multimodal models (LMMs) have achieved significant
advances in various image-text tasks. However, their performance in the
Intelligent Traffic Surveillance (ITS) domain remains limited due to the
absence of dedicated multimodal datasets. To address this gap, we introduce
MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale
multimodal benchmark dataset specifically designed for ITS. MITS includes
170,400 independently collected real-world ITS images sourced from traffic
surveillance cameras, annotated with eight main categories and 24 subcategories
of ITS-specific objects and events under diverse environmental conditions.
Additionally, through a systematic data generation pipeline, we generate
high-quality image captions and 5 million instruction-following visual
question-answer pairs, addressing five critical ITS tasks: object and event
recognition, object counting, object localization, background analysis, and
event reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream
LMMs on this dataset, enabling the development of ITS-specific applications.
Experimental results show that MITS significantly improves LMM performance in
ITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905
(+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to
0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the
dataset, code, and models as open-source, providing high-value resources to
advance both ITS and LMM research.

</details>


### [57] [Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs](https://arxiv.org/abs/2509.09732)
*Sary Elmansoury,Islam Mesabah,Gerrit Großmann,Peter Neigel,Raj Bhalwankar,Daniel Kondermann,Sebastian J. Vollmer*

Main category: cs.CV

TL;DR: 本文研究了基于决策树的结构化推理是否能提升视觉语言模型在细粒度分类任务中的性能，发现虽然模型能很好理解树状知识，但树基推理始终不如标准零样本提示方法。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在零样本视觉分类方面表现出色，但在细粒度任务和大规模层次化标签空间中的性能尚未充分研究，需要探索结构化推理是否能提升性能。

Method: 引入一个框架，使用决策树将分类分解为可解释的决策过程，并在细粒度(GTSRB)和粗粒度(CIFAR-10)数据集上进行评估，同时探索用LLM生成的类别和图像描述来增强树提示。

Result: 模型在理解树知识方面达到98.2%准确率，但树基推理始终表现不如标准零样本提示。添加图像描述后，树基方法和零样本方法的性能都得到提升。

Conclusion: 研究揭示了结构化推理在视觉分类中的局限性，为设计更可解释的VLM系统提供了重要见解。

Abstract: Vision language models (VLMs) excel at zero-shot visual classification, but
their performance on fine-grained tasks and large hierarchical label spaces is
understudied. This paper investigates whether structured, tree-based reasoning
can enhance VLM performance. We introduce a framework that decomposes
classification into interpretable decisions using decision trees and evaluates
it on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Although the
model achieves 98.2% accuracy in understanding the tree knowledge, tree-based
reasoning consistently underperforms standard zero-shot prompting. We also
explore enhancing the tree prompts with LLM-generated classes and image
descriptions to improve alignment. The added description enhances the
performance of the tree-based and zero-shot methods. Our findings highlight
limitations of structured reasoning in visual classification and offer insights
for designing more interpretable VLM systems.

</details>


### [58] [World Modeling with Probabilistic Structure Integration](https://arxiv.org/abs/2509.09737)
*Klemen Kotar,Wanhee Lee,Rahul Venkatesh,Honglin Chen,Daniel Bear,Jared Watrous,Simon Kim,Khai Loong Aw,Lilian Naing Chen,Stefan Stojanov,Kevin Feigelis,Imran Thobani,Alex Durango,Khaled Jedoui,Atlas Kazemian,Dan Yamins*

Main category: cs.CV

TL;DR: PSI是一个从数据中学习可控制、可提示的世界模型的系统，通过概率预测、结构提取和集成三个步骤的循环来增强模型能力


<details>
  <summary>Details</summary>
Motivation: 构建能够从数据中学习丰富控制功能和灵活提示功能的世界模型，支持对视频等多种数据的预测和理解

Method: 三步骤循环：1)概率预测-构建概率图模型；2)结构提取-通过因果推断提取低维结构；3)集成-将结构转化为新token类型并重新训练

Result: 在1.4万亿视频token上训练，实现了最先进的光流、自监督深度和对象分割，支持完整的预测改进循环

Conclusion: PSI系统通过循环增强方法成功构建了可控制的世界模型，为视频理解和预测提供了新的通用提示语言框架

Abstract: We present Probabilistic Structure Integration (PSI), a system for learning
richly controllable and flexibly promptable world models from data. PSI
consists of a three-step cycle. The first step, Probabilistic prediction,
involves building a probabilistic graphical model Psi of the data, in the form
of a random-access autoregressive sequence model. Psi supports a complete set
of learned conditional distributions describing the dependence of any variables
in the data on any other set of variables. In step 2, Structure extraction, we
show how to extract underlying low-dimensional properties in the data,
corresponding to a diverse set of meaningful "intermediate structures", in a
zero-shot fashion via causal inference on Psi. Step 3, Integration, completes
the cycle by converting these structures into new token types that are then
continually mixed back into the training diet as conditioning signals and
prediction targets. Each such cycle augments the capabilities of Psi, both
allowing it to model the underlying data better, and creating new control
handles -- akin to an LLM-like universal prompting language. We train an
instance of Psi on 1.4 trillion tokens of internet video data; we use it to
perform a variety of useful video prediction and understanding inferences; we
extract state-of-the-art optical flow, self-supervised depth and object
segmentation; and we use these structures to support a full cycle of predictive
improvements.

</details>


### [59] [Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning](https://arxiv.org/abs/2509.09742)
*Md Fazle Rasul,Alanood Alqobaisi,Bruhadeshwar Bezawada,Indrakshi Ray*

Main category: cs.CV

TL;DR: 本文首次分析了联邦学习中视频数据的梯度反演攻击风险，发现特征提取器能提供更好保护但仍有泄露可能，超分辨率技术可提升攻击效果。


<details>
  <summary>Details</summary>
Motivation: 联邦学习通过交换模型更新而非原始数据来保护隐私，但梯度反演攻击能从中恢复敏感数据。虽然图像、文本和表格数据的攻击影响已知，但视频数据的风险尚未研究。

Method: 评估两种视频分类方法：使用预训练特征提取器的方法和处理原始视频帧的简单变换方法。测试在不同攻击场景下（零参考帧、一个参考帧、多个参考帧）的梯度反演攻击效果，并应用超分辨率技术提升重建质量。

Result: 特征提取器对梯度反演攻击具有更强韧性，但分类器复杂度不足时仍可能泄露。超分辨率技术能显著提升攻击者重建的视频质量。视频数据在联邦学习中存在可行的泄露威胁。

Conclusion: 视频数据在联邦学习中的泄露是真实存在的威胁，需要进一步研究其发生条件并开发更有效的防护措施。

Abstract: Federated learning (FL) allows multiple entities to train a shared model
collaboratively. Its core, privacy-preserving principle is that participants
only exchange model updates, such as gradients, and never their raw, sensitive
data. This approach is fundamental for applications in domains where privacy
and confidentiality are important. However, the security of this very mechanism
is threatened by gradient inversion attacks, which can reverse-engineer private
training data directly from the shared gradients, defeating the purpose of FL.
While the impact of these attacks is known for image, text, and tabular data,
their effect on video data remains an unexamined area of research. This paper
presents the first analysis of video data leakage in FL using gradient
inversion attacks. We evaluate two common video classification approaches: one
employing pre-trained feature extractors and another that processes raw video
frames with simple transformations. Our initial results indicate that the use
of feature extractors offers greater resilience against gradient inversion
attacks. We also demonstrate that image super-resolution techniques can enhance
the frames extracted through gradient inversion attacks, enabling attackers to
reconstruct higher-quality videos. Our experiments validate this across
scenarios where the attacker has access to zero, one, or more reference frames
from the target environment. We find that although feature extractors make
attacks more challenging, leakage is still possible if the classifier lacks
sufficient complexity. We, therefore, conclude that video data leakage in FL is
a viable threat, and the conditions under which it occurs warrant further
investigation.

</details>


### [60] [A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images](https://arxiv.org/abs/2509.09750)
*Hossein Yazdanjouei,Arash Mansouri,Mohammad Shokouhifar*

Main category: cs.CV

TL;DR: 一种半监督协同训练框架，结合Faster R-CNN和YOLO优势，通过伪标签交换和集成分类提升突出物体检测精度，适用于集市环境中的物体检测任务。


<details>
  <summary>Details</summary>
Motivation: 解决集市环境中标签数据有限、遮挡严重、物体重叠等挑战，降低手动标注成本，适应商品和布局变化频繁的实际需求。

Method: 结合Faster R-CNN(ResNet背景)进行精确定位和YOLO(Darknet背景)获取全局上下文，通过互相伪标签交换提升检测精度；使用XGBoost、Random Forest和SVM集成分类器增强分类稳健性；采用元神经算法优化超参数。

Result: 在SKU-110k数据集上表现优异，显示出该框架在集市环境中的可扩展性和实用性。

Conclusion: 该半监督协同训练框架能够有效解决集市环境中的物体检测挑战，减少对手动标注的依赖，适合自动化库存跟踪、商品监控和结账系统等实际应用。

Abstract: This study proposes a semi-supervised co-training framework for object
detection in densely packed retail environments, where limited labeled data and
complex conditions pose major challenges. The framework combines Faster R-CNN
(utilizing a ResNet backbone) for precise localization with YOLO (employing a
Darknet backbone) for global context, enabling mutual pseudo-label exchange
that improves accuracy in scenes with occlusion and overlapping objects. To
strengthen classification, it employs an ensemble of XGBoost, Random Forest,
and SVM, utilizing diverse feature representations for higher robustness.
Hyperparameters are optimized using a metaheuristic-driven algorithm, enhancing
precision and efficiency across models. By minimizing reliance on manual
labeling, the approach reduces annotation costs and adapts effectively to
frequent product and layout changes common in retail. Experiments on the
SKU-110k dataset demonstrate strong performance, highlighting the scalability
and practicality of the proposed framework for real-world retail applications
such as automated inventory tracking, product monitoring, and checkout systems.

</details>


### [61] [Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging](https://arxiv.org/abs/2509.09785)
*Moslem Yazdanpanah,Ali Bahri,Mehrdad Noori,Sahar Dastani,Gustavo Adolfo Vargas Hakim,David Osowiechi,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

TL;DR: Token Purging (PG)是一种无需反向传播的测试时自适应方法，通过移除受域偏移影响严重的token来提升3D点云分类性能，具有高效快速的特点。


<details>
  <summary>Details</summary>
Motivation: 解决3D点云分类中因分布偏移导致的性能下降问题，现有TTA方法需要迭代更新，效率较低。

Method: 提出Token Purging方法，在token级别操作，在attention层之前移除受域偏移影响的token。包含两个变体：PG-SP（利用源统计信息）和PG-SF（完全无源的CLS-token驱动自适应）。

Result: 在ModelNet40-C、ShapeNet-C和ScanObjectNN-C数据集上，PG-SP比最先进的无反向传播方法平均准确率高10.3%，PG-SF在无源自适应方面创下新基准。PG比基线快12.4倍，内存效率高5.5倍。

Conclusion: Token Purging是一种高效、内存友好的测试时自适应方法，无需反向传播即可有效处理3D点云分类中的域偏移问题，适合实际部署。

Abstract: Test-time adaptation (TTA) is crucial for mitigating performance degradation
caused by distribution shifts in 3D point cloud classification. In this work,
we introduce Token Purging (PG), a novel backpropagation-free approach that
removes tokens highly affected by domain shifts before they reach attention
layers. Unlike existing TTA methods, PG operates at the token level, ensuring
robust adaptation without iterative updates. We propose two variants: PG-SP,
which leverages source statistics, and PG-SF, a fully source-free version
relying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,
ShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of
+10.3\% higher accuracy than state-of-the-art backpropagation-free methods,
while PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is
12.4 times faster and 5.5 times more memory efficient than our baseline, making
it suitable for real-world deployment. Code is available at
\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}

</details>


### [62] [Fine-Grained Cross-View Localization via Local Feature Matching and Monocular Depth Priors](https://arxiv.org/abs/2509.09792)
*Zimin Xia,Chenghao Xu,Alexandre Alahi*

Main category: cs.CV

TL;DR: 一种高精度可解释的细粒度跨视角定位方法，通过直接匹配地面与航拍图像特征点，利用单目深度预测将匹配点升级到上下视图空间进行定位，避免传统BEV转换的信息损失问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法将地面图像转换为鸟视图（BEV）时容易造成视角扭曲或高度信息压缩，导致与航拍图像对齐质量下降。需要一种更直接、减少信息损失的定位方法。

Method: 直接建立地面与航拍图像特征点对应关系，仅将匹配的关键点利用单目深度预测升级到BEV空间。支持谱深度和相对深度两种模式，采用尺度敏感的Procrustes对齐算法估计相机位姿，并可选性恢复相对深度的尺度。

Result: 在仅需弱监督的情况下，方法能够学习到准确的局部特征对应关系，在跨区域法则化和未知方向等挑战性条件下都表现出优异的定位性能。同时兼容各种相对深度模型而无需每个模型的细调。

Conclusion: 该方法通过直接特征匹配和仅升级匹配点的方式，有效避免了传统BEV转换的缺点，实现了高精度、高可解释性的跨视角定位，具有良好的灵活性和实际部署价值。

Abstract: We propose an accurate and highly interpretable fine-grained cross-view
localization method that estimates the 3 Degrees of Freedom pose of a
ground-level image by matching its local features with a reference aerial
image. Previous methods typically transform the ground image into a bird's-eye
view (BEV) representation and then align it with the aerial image for
localization. However, this transformation often leads to information loss due
to perspective distortion or compression of height information, thereby
degrading alignment quality with the aerial view. In contrast, our method
directly establishes correspondences between ground and aerial images and lifts
only the matched keypoints to BEV space using monocular depth prior. Notably,
modern depth predictors can provide reliable metric depth when the test samples
are similar to the training data. When the depth distribution differs, they
still produce consistent relative depth, i.e., depth accurate up to an unknown
scale. Our method supports both metric and relative depth. It employs a
scale-aware Procrustes alignment to estimate the camera pose from the
correspondences and optionally recover the scale when using relative depth.
Experimental results demonstrate that, with only weak supervision on camera
pose, our method learns accurate local feature correspondences and achieves
superior localization performance under challenging conditions, such as
cross-area generalization and unknown orientation. Moreover, our method is
compatible with various relative depth models without requiring per-model
finetuning. This flexibility, combined with strong localization performance,
makes it well-suited for real-world deployment.

</details>


### [63] [Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test](https://arxiv.org/abs/2509.09808)
*Judith Massmann,Alexander Lichtenstein,Francisco M. López*

Main category: cs.CV

TL;DR: 基于深度学习的手机应用KidsVisionCheck，通过红眼反射图像进行儿童视力筛查，准确率达90%，无需专业设备


<details>
  <summary>Details</summary>
Motivation: 利用智能手机和AI技术重现Bruckner测试，实现可访问的儿科视力筛查和早期干预

Method: 使用深度神经网络训练模型，训练数据来自眼科医生收集和标注的儿童学生网膜图像

Result: 在未见测试数据上达到90%的准确率，能够识别最优数据收集条件并提供即时反馈

Conclusion: 这项工作是向全球可访问儿科视力筛查和视力异常早期干预的第一步

Abstract: Numerous visual impairments can be detected in red-eye reflex images from
young children. The so-called Bruckner test is traditionally performed by
ophthalmologists in clinical settings. Thanks to the recent technological
advances in smartphones and artificial intelligence, it is now possible to
recreate the Bruckner test using a mobile device. In this paper, we present a
first study conducted during the development of KidsVisionCheck, a free
application that can perform vision screening with a mobile device using
red-eye reflex images. The underlying model relies on deep neural networks
trained on children's pupil images collected and labeled by an ophthalmologist.
With an accuracy of 90% on unseen test data, our model provides highly reliable
performance without the necessity of specialist equipment. Furthermore, we can
identify the optimal conditions for data collection, which can in turn be used
to provide immediate feedback to the users. In summary, this work marks a first
step toward accessible pediatric vision screenings and early intervention for
vision abnormalities worldwide.

</details>


### [64] [DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception](https://arxiv.org/abs/2509.09828)
*Tim Broedermannn,Christos Sakaridis,Luigi Piccinelli,Wim Abbeloos,Luc Van Gool*

Main category: cs.CV

TL;DR: 提出DGFusion方法，通过深度引导的多模态融合提升自动驾驶语义感知性能，利用激光雷达数据学习深度感知特征，实现空间变化的传感器融合。


<details>
  <summary>Details</summary>
Motivation: 现有传感器融合方法在空间上统一处理传感器数据，在挑战性条件下性能受限。需要根据深度信息动态调整传感器融合策略。

Method: 提出深度引导的多模态融合网络DGFusion，将多模态分割作为多任务问题，利用激光雷达数据作为输入和深度真值，学习深度感知特征并编码为局部深度token，结合全局条件token动态调整传感器融合。

Result: 在MUSES和DELIVER数据集上实现了最先进的全景和语义分割性能。

Conclusion: 深度引导的传感器融合方法能够有效提升自动驾驶语义感知在挑战性条件下的鲁棒性，通过深度信息动态调整传感器可靠性权重。

Abstract: Robust semantic perception for autonomous vehicles relies on effectively
combining multiple sensors with complementary strengths and weaknesses.
State-of-the-art sensor fusion approaches to semantic perception often treat
sensor data uniformly across the spatial extent of the input, which hinders
performance when faced with challenging conditions. By contrast, we propose a
novel depth-guided multimodal fusion method that upgrades condition-aware
fusion by integrating depth information. Our network, DGFusion, poses
multimodal segmentation as a multi-task problem, utilizing the lidar
measurements, which are typically available in outdoor sensor suites, both as
one of the model's inputs and as ground truth for learning depth. Our
corresponding auxiliary depth head helps to learn depth-aware features, which
are encoded into spatially varying local depth tokens that condition our
attentive cross-modal fusion. Together with a global condition token, these
local depth tokens dynamically adapt sensor fusion to the spatially varying
reliability of each sensor across the scene, which largely depends on depth. In
addition, we propose a robust loss for our depth, which is essential for
learning from lidar inputs that are typically sparse and noisy in adverse
conditions. Our method achieves state-of-the-art panoptic and semantic
segmentation performance on the challenging MUSES and DELIVER datasets. Code
and models will be available at https://github.com/timbroed/DGFusion

</details>


### [65] [Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework](https://arxiv.org/abs/2509.09841)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

TL;DR: 提出基于图像块的ResNet-18深度学习框架，用于玫瑰痤疮自动检测，通过局部图像块分析提高检测准确性、鲁棒性和患者隐私保护


<details>
  <summary>Details</summary>
Motivation: 玫瑰痤疮是一种慢性炎症性皮肤病，需要精确早期检测以提高治疗效果，传统全图像方法存在隐私泄露和检测精度不足的问题

Method: 使用ResNet-18深度学习框架，从面部图像提取不同大小、形状和位置的各种图像块，研究局部视觉信息对模型性能的影响

Result: 基于图像块的检测策略在准确性和敏感性方面达到或优于全图像方法，能够引导模型关注临床相关区域，增强鲁棒性和可解释性

Conclusion: 提出的图像块策略为改进自动化皮肤病诊断提供了实用见解，在保护患者隐私的同时提高了检测性能

Abstract: Rosacea, which is a chronic inflammatory skin condition that manifests with
facial redness, papules, and visible blood vessels, often requirs precise and
early detection for significantly improving treatment effectiveness. This paper
presents new patch-based automatic rosacea detection strategies using the
ResNet-18 deep learning framework. The contributions of the proposed strategies
come from the following aspects. First, various image pateches are extracted
from the facial images of people in different sizes, shapes, and locations.
Second, a number of investigation studies are carried out to evaluate how the
localized visual information influences the deep learing model performance.
Third, thorough experiments are implemented to reveal that several patch-based
automatic rosacea detection strategies achieve competitive or superior accuracy
and sensitivity than the full-image based methods. And finally, the proposed
patch-based strategies, which use only localized patches, inherently preserve
patient privacy by excluding any identifiable facial features from the data.
The experimental results indicate that the proposed patch-based strategies
guide the deep learning model to focus on clinically relevant regions, enhance
robustness and interpretability, and protect patient privacy. As a result, the
proposed strategies offer practical insights for improving automated
dermatological diagnostics.

</details>


### [66] [Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection](https://arxiv.org/abs/2509.09844)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

TL;DR: 一种基于合成数据和临床前知知识的隐私保护蜗红痘自动检测方法，通过红色区域掩码和ResNet-18模型实现了更高的准确性和回召率。


<details>
  <summary>Details</summary>
Motivation: 蜗红痘作为一种常见的皮肤疾病缺乏标注数据集，且面部图像存在隐私漏涼风险，需要开发隐私保护的自动检测方法。

Method: 首先构建固定的红色区域掩码，重点关注面部中央红痘区域，然后使用ResNet-18深度学习模型在合成数据上进行训练。

Result: 在真实数据上评估显示，该方法在准确性、回召率和F1分数方面都显著超过了全面部基线方法。

Conclusion: 合成数据与临床前知知识结合可以实现准确且符合道德规范的皮肤科AI系统，特别适用于隐私敏感的远程医疗和大规模筛查应用。

Abstract: Rosacea is a common but underdiagnosed inflammatory skin condition that
primarily affects the central face and presents with subtle redness, pustules,
and visible blood vessels. Automated detection remains challenging due to the
diffuse nature of symptoms, the scarcity of labeled datasets, and privacy
concerns associated with using identifiable facial images. A novel
privacy-preserving automated rosacea detection method inspired by clinical
priors and trained entirely on synthetic data is presented in this paper.
Specifically, the proposed method, which leverages the observation that rosacea
manifests predominantly through central facial erythema, first constructs a
fixed redness-informed mask by selecting regions with consistently high red
channel intensity across facial images. The mask thus is able to focus on
diagnostically relevant areas such as the cheeks, nose, and forehead and
exclude identity-revealing features. Second, the ResNet-18 deep learning
method, which is trained on the masked synthetic images, achieves superior
performance over the full-face baselines with notable gains in terms of
accuracy, recall and F1 score when evaluated using the real-world test data.
The experimental results demonstrate that the synthetic data and clinical
priors can jointly enable accurate and ethical dermatological AI systems,
especially for privacy sensitive applications in telemedicine and large-scale
screening.

</details>


### [67] [Investigating the Impact of Various Loss Functions and Learnable Wiener Filter for Laparoscopic Image Desmoking](https://arxiv.org/abs/2509.09849)
*Chengyu Yang,Chengjun Liu*

Main category: cs.CV

TL;DR: 本文对ULW腹腔镜图像去烟框架进行了全面的消融研究，评估了可学习维纳滤波器模块和复合损失函数中各个损失项的具体贡献。


<details>
  <summary>Details</summary>
Motivation: 为了严格评估ULW框架中各个组件的有效性和必要性，需要系统性地分析每个组件对整体性能的具体贡献。

Method: 采用消融研究方法，包括：(1)移除可学习维纳滤波器；(2)选择性使用复合损失函数中的各个损失项（MSE、SSIM损失、感知损失）。所有变体在公开的腹腔镜图像数据集上进行基准测试。

Result: 使用定量指标（SSIM、PSNR、MSE、CIEDE-2000）和定性视觉比较对所有变体进行了评估。

Conclusion: 通过系统性的消融研究，明确了ULW框架中各个组件的重要性，为腹腔镜图像去烟技术的优化提供了重要依据。

Abstract: To rigorously assess the effectiveness and necessity of individual components
within the recently proposed ULW framework for laparoscopic image desmoking,
this paper presents a comprehensive ablation study. The ULW approach combines a
U-Net based backbone with a compound loss function that comprises mean squared
error (MSE), structural similarity index (SSIM) loss, and perceptual loss. The
framework also incorporates a differentiable, learnable Wiener filter module.
In this study, each component is systematically ablated to evaluate its
specific contribution to the overall performance of the whole framework. The
analysis includes: (1) removal of the learnable Wiener filter, (2) selective
use of individual loss terms from the composite loss function. All variants are
benchmarked on a publicly available paired laparoscopic images dataset using
quantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative
visual comparisons.

</details>


### [68] [WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector](https://arxiv.org/abs/2509.09859)
*Razvan Stefanescu,Ethan Oh,Ruben Vazquez,Chris Mesterharm,Constantin Serban,Ritu Chadha*

Main category: cs.CV

TL;DR: WAVE-DETR是一种结合可见光RGB和声学信号的多模态无人机检测器，基于Deformable DETR和Wav2Vec2架构，在挑战性环境条件下实现鲁棒检测性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决在复杂环境下单一视觉模态无人机检测的局限性，通过融合视觉和声学信息来提高检测的鲁棒性和准确性。

Method: 开发了四种融合配置（门控机制、线性层、MLP和交叉注意力），将Wav2Vec2声学嵌入与Deformable DETR的多分辨率特征映射融合。

Result: 最佳的门控融合方法在ARDrone数据集上将Deformable DETR检测器的mAP提升了11.1%-15.3%（小型无人机）和3.27%-5.84%（所有尺寸无人机）。

Conclusion: 声学信息能显著提升无人机检测性能，特别是在小型无人机检测方面，多模态融合是提高现实场景中无人机检测鲁棒性的有效方法。

Abstract: We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and
acoustic signals for robust real-life UAV object detection. Our approach fuses
visual and acoustic features in a unified object detector model relying on the
Deformable DETR and Wav2Vec2 architectures, achieving strong performance under
challenging environmental conditions. Our work leverage the existing
Drone-vs-Bird dataset and the newly generated ARDrone dataset containing more
than 7,500 synchronized images and audio segments. We show how the acoustic
information is used to improve the performance of the Deformable DETR object
detector on the real ARDrone dataset. We developed, trained and tested four
different fusion configurations based on a gated mechanism, linear layer, MLP
and cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi
resolution feature mappings of the Deformable DETR and enhance the object
detection performance over all drones dimensions. The best performer is the
gated fusion approach, which improves the mAP of the Deformable DETR object
detector on our in-distribution and out-of-distribution ARDrone datasets by
11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.
The mAP scores for medium and large drones are also enhanced, with overall
gains across all drone sizes ranging from 3.27% to 5.84%.

</details>


### [69] [Surrogate Supervision for Robust and Generalizable Deformable Image Registration](https://arxiv.org/abs/2509.09869)
*Yihao Liu,Junyu Chen,Lianrui Zuo,Shuwen Wei,Brian D. Boyd,Carmen Andreescu,Olusola Ajilore,Warren D. Taylor,Aaron Carass,Bennett A. Landman*

Main category: cs.CV

TL;DR: 提出了一种称为代理监督的新训练范式，通过将估计的空间变换应用于代理图像，将输入域与监督域解耦，从而提高深度学习配准网络对输入图像变化的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习配准方法虽然精度高，但对输入图像特性变化（如伪影、视野不匹配、模态差异）敏感，需要开发能提高配准网络鲁棒性和泛化性的通用训练方法。

Method: 引入代理监督框架，将输入域与监督域解耦，通过将估计的空间变换应用于代理图像来确保在相似性定义良好的域中进行监督计算，支持在异构输入上进行训练。

Result: 在三个代表性应用（抗伪影脑MR配准、掩码无关肺CT配准、多模态MR配准）中，代理监督显示出对输入变化的强韧性，包括不均匀场、不一致视野和模态差异，同时在精心整理的数据上保持高性能。

Conclusion: 代理监督提供了一个原则性框架，在不增加复杂性的情况下训练鲁棒且可泛化的深度学习配准模型，为医学图像配准在多样化生物医学成像场景中的更广泛应用提供了实用途径。

Abstract: Objective: Deep learning-based deformable image registration has achieved
strong accuracy, but remains sensitive to variations in input image
characteristics such as artifacts, field-of-view mismatch, or modality
difference. We aim to develop a general training paradigm that improves the
robustness and generalizability of registration networks. Methods: We introduce
surrogate supervision, which decouples the input domain from the supervision
domain by applying estimated spatial transformations to surrogate images. This
allows training on heterogeneous inputs while ensuring supervision is computed
in domains where similarity is well defined. We evaluate the framework through
three representative applications: artifact-robust brain MR registration,
mask-agnostic lung CT registration, and multi-modal MR registration. Results:
Across tasks, surrogate supervision demonstrated strong resilience to input
variations including inhomogeneity field, inconsistent field-of-view, and
modality differences, while maintaining high performance on well-curated data.
Conclusions: Surrogate supervision provides a principled framework for training
robust and generalizable deep learning-based registration models without
increasing complexity. Significance: Surrogate supervision offers a practical
pathway to more robust and generalizable medical image registration, enabling
broader applicability in diverse biomedical imaging scenarios.

</details>


### [70] [An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars](https://arxiv.org/abs/2509.09911)
*Barkin Buyukcakir,Jannick De Tobel,Patrick Thevissen,Dirk Vandermeulen,Peter Claes*

Main category: cs.CV

TL;DR: 提出结合卷积自编码器和Vision Transformer的框架，提升牙齿年龄估计的准确性和可解释性，发现第三磨牙的高类内形态变异是性能限制因素


<details>
  <summary>Details</summary>
Motivation: 深度学习在法医年龄估计等高风险应用中存在'黑盒'问题，需要同时提升模型性能和透明度

Method: 使用卷积自编码器(AE)与Vision Transformer(ViT)结合的框架，以下颌第二磨牙(37)和第三磨牙(38)的分期作为案例研究

Result: 分类准确率显著提升：37号牙从0.712提高到0.815，38号牙从0.462提高到0.543；通过AE潜在空间分析和图像重建发现38号牙数据集的高类内形态变异是主要限制因素

Conclusion: 单一可解释性方法(如注意力图)不足，需要多角度诊断方法；该框架既能提高准确性又能解释模型不确定性，为法医年龄估计提供更可靠的工具支持

Abstract: The practical adoption of deep learning in high-stakes forensic applications,
such as dental age estimation, is often limited by the 'black box' nature of
the models. This study introduces a framework designed to enhance both
performance and transparency in this context. We use a notable performance
disparity in the automated staging of mandibular second (tooth 37) and third
(tooth 38) molars as a case study. The proposed framework, which combines a
convolutional autoencoder (AE) with a Vision Transformer (ViT), improves
classification accuracy for both teeth over a baseline ViT, increasing from
0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond
improving performance, the framework provides multi-faceted diagnostic
insights. Analysis of the AE's latent space metrics and image reconstructions
indicates that the remaining performance gap is data-centric, suggesting high
intra-class morphological variability in the tooth 38 dataset is a primary
limiting factor. This work highlights the insufficiency of relying on a single
mode of interpretability, such as attention maps, which can appear anatomically
plausible yet fail to identify underlying data issues. By offering a
methodology that both enhances accuracy and provides evidence for why a model
may be uncertain, this framework serves as a more robust tool to support expert
decision-making in forensic age estimation.

</details>


### [71] [SCoDA: Self-supervised Continual Domain Adaptation](https://arxiv.org/abs/2509.09935)
*Chirayu Agrawal,Snehasis Mukherjee*

Main category: cs.CV

TL;DR: SCoDA是一种无源域自适应方法，通过自监督预训练和几何流形对齐，在无需源域数据的情况下显著提升目标域适应性能


<details>
  <summary>Details</summary>
Motivation: 现有SFDA方法依赖全监督预训练源模型，使用余弦相似度对齐实例级特征时会丢失重要的几何流形信息，需要新的方法来克服这些限制

Method: 使用自监督预训练的教师模型初始化框架，采用几何流形对齐原则，结合实例级特征匹配和空间相似性损失的复合目标训练学生模型，通过EMA更新教师参数防止灾难性遗忘

Result: 在基准数据集上的大量实验表明，SCoDA显著优于最先进的SFDA方法

Conclusion: SCoDA通过自监督预训练和几何流形对齐成功解决了SFDA中的关键挑战，为无源域自适应提供了有效的新方法

Abstract: Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a
model to a target domain without access to the data of the source domain.
Prevailing methods typically start with a source model pre-trained with full
supervision and distill the knowledge by aligning instance-level features.
However, these approaches, relying on cosine similarity over L2-normalized
feature vectors, inadvertently discard crucial geometric information about the
latent manifold of the source model. We introduce Self-supervised Continual
Domain Adaptation (SCoDA) to address these limitations. We make two key
departures from standard practice: first, we avoid the reliance on supervised
pre-training by initializing the proposed framework with a teacher model
pre-trained entirely via self-supervision (SSL). Second, we adapt the principle
of geometric manifold alignment to the SFDA setting. The student is trained
with a composite objective combining instance-level feature matching with a
Space Similarity Loss. To combat catastrophic forgetting, the teacher's
parameters are updated via an Exponential Moving Average (EMA) of the student's
parameters. Extensive experiments on benchmark datasets demonstrate that SCoDA
significantly outperforms state-of-the-art SFDA methods.

</details>


### [72] [Segment Anything for Cell Tracking](https://arxiv.org/abs/2509.09943)
*Zhu Chen,Mert Edgü,Er Jin,Johannes Stegmaier*

Main category: cs.CV

TL;DR: 基于SAM2的无监督细胞踪踪框架，无需训练数据集即可在2D/3D显微镜影像中实现竞争性的踪踪精度


<details>
  <summary>Details</summary>
Motivation: 解决传统深度学习方法对手动标注数据的依赖，以及在多样化显微镜数据上的沿用性问题

Method: 集成Segment Anything 2 (SAM2)大型基础模型到踪踪流程中，实现完全无监督的细胞踪踪方法

Result: 在2D和大规模3D时间拉显微镜视频中达到竞争性的质量，无需数据集特定适配

Conclusion: 该方法免除了对特定训练数据的依赖，能够在多样化显微镜数据集上实现良好的沿用性

Abstract: Tracking cells and detecting mitotic events in time-lapse microscopy image
sequences is a crucial task in biomedical research. However, it remains highly
challenging due to dividing objects, low signal-tonoise ratios, indistinct
boundaries, dense clusters, and the visually similar appearance of individual
cells. Existing deep learning-based methods rely on manually labeled datasets
for training, which is both costly and time-consuming. Moreover, their
generalizability to unseen datasets remains limited due to the vast diversity
of microscopy data. To overcome these limitations, we propose a zero-shot cell
tracking framework by integrating Segment Anything 2 (SAM2), a large foundation
model designed for general image and video segmentation, into the tracking
pipeline. As a fully-unsupervised approach, our method does not depend on or
inherit biases from any specific training dataset, allowing it to generalize
across diverse microscopy datasets without finetuning. Our approach achieves
competitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos
while eliminating the need for dataset-specific adaptation.

</details>


### [73] [Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation](https://arxiv.org/abs/2509.09946)
*Vu-Minh Le,Thao-Anh Tran,Duc Huy Do,Xuan Canh Do,Huong Ninh,Hai Tran*

Main category: cs.CV

TL;DR: 提出了一种将2D多相机跟踪系统扩展到3D空间的方法，通过深度信息重建目标点云，并利用聚类和偏航角优化恢复3D边界框，在AI City Challenge 2025中获得第三名


<details>
  <summary>Details</summary>
Motivation: 现有的多目标多相机跟踪系统主要基于2D空间，而3D跟踪需要完全重构系统组件，对现有系统不友好。本文旨在利用深度信息将现有2D跟踪系统扩展到3D空间

Method: 利用深度信息重建目标点云空间，通过聚类和偏航角优化恢复3D边界框，并引入增强的在线数据关联机制，利用目标局部ID一致性进行全局ID分配

Result: 在2025 AI City Challenge的3D MTMC数据集上进行评估，在排行榜上获得第三名

Conclusion: 该方法成功实现了将现有2D多相机跟踪系统扩展到3D空间，无需完全重构系统，为大规模监控系统的3D感知提供了可行方案

Abstract: Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision
task for automating large-scale surveillance. With camera calibration and depth
information, the targets in the scene can be projected into 3D space, offering
unparalleled levels of automatic perception of a 3D environment. However,
tracking in the 3D space requires replacing all 2D tracking components from the
ground up, which may be infeasible for existing MTMC systems. In this paper, we
present an approach for extending any online 2D multi-camera tracking system
into 3D space by utilizing depth information to reconstruct a target in
point-cloud space, and recovering its 3D box through clustering and yaw
refinement following tracking. We also introduced an enhanced online data
association mechanism that leverages the target's local ID consistency to
assign global IDs across frames. The proposed framework is evaluated on the
2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the
leaderboard.

</details>


### [74] [Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification](https://arxiv.org/abs/2509.09958)
*Jeffrey Liu,Rongbin Hu*

Main category: cs.CV

TL;DR: 通过将指代表达式理解重构为直观语言验证任务，使用通用检测器和VLM实现了无需任务特定训练的零样本高性能


<details>
  <summary>Details</summary>
Motivation: 探索如何通过工作流程设计而非任务特定领域领先训练来实现强大的零样本指代表达式理解能力

Method: 将REC重构为直观语言验证问题：使用YOLO-World通用检测器获取建议框，然后用通用VLM对每个区域进行True/False查询验证

Result: 在RefCOCO、RefCOCO+、RefCOCOg数据集上超越了零样本GroundingDINO基线，甚至超过了经过REC训练的GroundingDINO和GroundingDINO+CRG的报告结果

Conclusion: 工作流程设计比任务特定领域领先训练更能驱动零样本REC性能，验证方法显著优于选择基于提示方法

Abstract: Referring Expression Comprehension (REC) is usually addressed with
task-trained grounding models. We show that a zero-shot workflow, without any
REC-specific training, can achieve competitive or superior performance. Our
approach reformulates REC as box-wise visual-language verification: given
proposals from a COCO-clean generic detector (YOLO-World), a general-purpose
VLM independently answers True/False queries for each region. This simple
procedure reduces cross-box interference, supports abstention and multiple
matches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our
method not only surpasses a zero-shot GroundingDINO baseline but also exceeds
reported results for GroundingDINO trained on REC and GroundingDINO+CRG.
Controlled studies with identical proposals confirm that verification
significantly outperforms selection-based prompting, and results hold with open
VLMs. Overall, we show that workflow design, rather than task-specific
pretraining, drives strong zero-shot REC performance.

</details>


### [75] [Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and Pest Segmentation](https://arxiv.org/abs/2509.09961)
*Tianqi Wei,Xin Yu,Zhi Chen,Scott Chapman,Zi Huang*

Main category: cs.CV

TL;DR: 通过随机投影复制粘贴数据增帽技术，解决小麦叶病虫害分割中的极端像素不平衡问题


<details>
  <summary>Details</summary>
Motivation: 农业作物病虫害分割中，虫害区域像素占比极小，导致模型过拟合常见类别而忽略稀缺类别，影响整体分割性能

Method: 提出RPCP数据增帽方法：提取稀缺虫害补丁，应用随机几何变换模拟变化，粘贴到合适区域避免重叠，并通过随机投影滤波精炼局部特征确保自然融合

Result: 方法显著提升了虫害类别的分割性能，同时保持或轻微提升其他类别的准确性

Conclusion: 目标化数据增帽技术能有效缓解极端像素不平衡问题，为农业图像分割提供了简单有效的解决方案

Abstract: Accurate segmentation of foliar diseases and insect damage in wheat is
crucial for effective crop management and disease control. However, the insect
damage typically occupies only a tiny fraction of annotated pixels. This
extreme pixel-level imbalance poses a significant challenge to the segmentation
performance, which can result in overfitting to common classes and insufficient
learning of rare classes, thereby impairing overall performance. In this paper,
we propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to
address the pixel imbalance problem. Specifically, we extract rare
insect-damage patches from annotated training images and apply random geometric
transformations to simulate variations. The transformed patches are then pasted
in appropriate regions while avoiding overlaps with lesions or existing damaged
regions. In addition, we apply a random projection filter to the pasted
regions, refining local features and ensuring a natural blend with the new
background. Experiments show that our method substantially improves
segmentation performance on the insect damage class, while maintaining or even
slightly enhancing accuracy on other categories. Our results highlight the
effectiveness of targeted augmentation in mitigating extreme pixel imbalance,
offering a straightforward yet effective solution for agricultural segmentation
problems.

</details>


### [76] [An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock](https://arxiv.org/abs/2509.09962)
*Anne Marthe Sophie Ngo Bibinbe,Chiron Bang,Patrick Gagnon,Jamie Ahloy-Dallaire,Eric R. Paquet*

Main category: cs.CV

TL;DR: 基于隐马尔可夫模型的不确定身份识别长期多目标跟踪框架，利用偶然身份信息改善踪踪性能


<details>
  <summary>Details</summary>
Motivation: 现有多目标跟踪方法在长时间视频中因身份切换问题导致性能下降，而在生畜等实际应用中可以获得偶然的身份识别信息

Method: 通过隐马尔可夫模型结合不确定身份识别与跟踪，利用偶然获得的身份信息来提升踪踪精度

Result: 在10分钟猪踪踪数据集上支援21次识别的情况下，F1分数比ByteTrack更高；在MOT17和MOT20数据集上也验证了性能提升

Conclusion: 该HMM框架能够有效利用偶然身份信息改善长期踪踪性能，并且对识别不确定性具有稳健性

Abstract: The need for long-term multi-object tracking (MOT) is growing due to the
demand for analyzing individual behaviors in videos that span several minutes.
Unfortunately, due to identity switches between objects, the tracking
performance of existing MOT approaches decreases over time, making them
difficult to apply for long-term tracking. However, in many real-world
applications, such as in the livestock sector, it is possible to obtain
sporadic identifications for some of the animals from sources like feeders. To
address the challenges of long-term MOT, we propose a new framework that
combines both uncertain identities and tracking using a Hidden Markov Model
(HMM) formulation. In addition to providing real-world identities to animals,
our HMM framework improves the F1 score of ByteTrack, a leading MOT approach
even with re-identification, on a 10 minute pig tracking dataset with 21
identifications at the pen's feeding station. We also show that our approach is
robust to the uncertainty of identifications, with performance increasing as
identities are provided more frequently. The improved performance of our HMM
framework was also validated on the MOT17 and MOT20 benchmark datasets using
both ByteTrack and FairMOT. The code for this new HMM framework and the new
10-minute pig tracking video dataset are available at:
https://github.com/ngobibibnbe/uncertain-identity-aware-tracking

</details>


### [77] [Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey](https://arxiv.org/abs/2509.09971)
*Aupendu Kar,Vishnu Raj,Guan-Ming Su*

Main category: cs.CV

TL;DR: 这篇调查性论文系统分析了事件相机与传统框率相机融合的技术进展，重点关注在视频恢复和3D重建任务中的深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有低延迟、低功耗和超高抓抓率的优势，但需要与传统框率相机融合才能发挥最大效果。本文动机在于系统性评估这种融合技术在视觉增强和恢复任务中的效果。

Method: 论文采用系统调查方法，从两个维度全面分析深度学习方法：时间增强（如框间插值、运动去模糊）和空间增强（包括超分辨率、低光和HDR增强、伪影降低），同时深入探讨3D重建领域的进展。

Result: 调查显示事件相机与传统框率相机的融合在多个视觉任务中都取得了显著成效，特别是在具有挑战性条件的场景下。论文还编辑了公开数据集清博，支持可复现研究和性能测试。

Conclusion: 这份调查结合了最新进展和见解，旨在激励更多研究者利用事件相机系统（特别是与深度学习结合）进行高级视觉媒体恢复和增强研究。

Abstract: Event camera sensors are bio-inspired sensors which asynchronously capture
per-pixel brightness changes and output a stream of events encoding the
polarity, location and time of these changes. These systems are witnessing
rapid advancements as an emerging field, driven by their low latency, reduced
power consumption, and ultra-high capture rates. This survey explores the
evolution of fusing event-stream captured with traditional frame-based capture,
highlighting how this synergy significantly benefits various video restoration
and 3D reconstruction tasks. The paper systematically reviews major deep
learning contributions to image/video enhancement and restoration, focusing on
two dimensions: temporal enhancement (such as frame interpolation and motion
deblurring) and spatial enhancement (including super-resolution, low-light and
HDR enhancement, and artifact reduction). This paper also explores how the 3D
reconstruction domain evolves with the advancement of event driven fusion.
Diverse topics are covered, with in-depth discussions on recent works for
improving visual quality under challenging conditions. Additionally, the survey
compiles a comprehensive list of openly available datasets, enabling
reproducible research and benchmarking. By consolidating recent progress and
insights, this survey aims to inspire further research into leveraging event
camera systems, especially in combination with deep learning, for advanced
visual media restoration and enhancement.

</details>


### [78] [ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking](https://arxiv.org/abs/2509.09977)
*Siying Liu,Zikai Wang,Hanle Zheng,Yifan Hu,Xilin Wang,Qingkai Yang,Jibin Wu,Hao Guo,Lei Deng*

Main category: cs.CV

TL;DR: ISTASTrack是首个基于Transformer的ANN-SNN混合跟踪器，通过ISTA适配器实现RGB和事件数据的有效融合，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有ANN网络难以充分利用事件流的稀疏和异步特性，而ANN-SNN混合架构在RGB-Event感知中面临特征融合挑战。

Method: 采用双分支架构：视觉Transformer处理RGB输入，脉冲Transformer处理事件流；设计基于ISTA算法的适配器进行双向特征交互；加入时序下采样注意力模块对齐特征。

Result: 在FE240hz、VisEvent、COESOT和FELT等基准测试中实现了最先进的性能，同时保持高能效。

Conclusion: ISTASTrack证明了ANN-SNN混合设计在鲁棒视觉跟踪中的有效性和实用性，为跨模态特征融合提供了新思路。

Abstract: RGB-Event tracking has become a promising trend in visual object tracking to
leverage the complementary strengths of both RGB images and dynamic spike
events for improved performance. However, existing artificial neural networks
(ANNs) struggle to fully exploit the sparse and asynchronous nature of event
streams. Recent efforts toward hybrid architectures combining ANNs and spiking
neural networks (SNNs) have emerged as a promising solution in RGB-Event
perception, yet effectively fusing features across heterogeneous paradigms
remains a challenge. In this work, we propose ISTASTrack, the first
transformer-based \textbf{A}NN-\textbf{S}NN hybrid \textbf{Track}er equipped
with \textbf{ISTA} adapters for RGB-Event tracking. The two-branch model
employs a vision transformer to extract spatial context from RGB inputs and a
spiking transformer to capture spatio-temporal dynamics from event streams. To
bridge the modality and paradigm gap between ANN and SNN features, we
systematically design a model-based ISTA adapter for bidirectional feature
interaction between the two branches, derived from sparse representation theory
by unfolding the iterative shrinkage thresholding algorithm. Additionally, we
incorporate a temporal downsampling attention module within the adapter to
align multi-step SNN features with single-step ANN features in the latent
space, improving temporal fusion. Experimental results on RGB-Event tracking
benchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that
ISTASTrack achieves state-of-the-art performance while maintaining high energy
efficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN
designs for robust visual tracking. The code is publicly available at
https://github.com/lsying009/ISTASTrack.git.

</details>


### [79] [FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction](https://arxiv.org/abs/2509.09988)
*Yusuke Takagi,Shunya Nagashima,Komei Sugiura*

Main category: cs.CV

TL;DR: 提出基于多深度状态空间模型和FLARE损失函数的太阳耀斑预测方法，有效解决类别不平衡问题，在11年太阳活动周期数据集上性能优于基线方法


<details>
  <summary>Details</summary>
Motivation: 当前太阳耀斑预测性能不足，特别是在处理严重类别不平衡问题上存在缺陷，需要更准确可靠的预测方法来保护关键基础设施

Method: 使用多深度状态空间模型构建预测框架，并引入频率和局部边界感知可靠性损失（FLARE损失）来处理类别不平衡问题

Result: 在完整的11年太阳活动周期多波长太阳图像数据集上，该方法在Gandin-Murphy-Gerrity分数和真实技能统计量等标准指标上均优于基线方法

Conclusion: 所提出的多深度状态空间模型结合FLARE损失函数能够有效提升太阳耀斑预测的性能和可靠性，特别是在处理类别不平衡问题上表现出色

Abstract: Accurate and reliable solar flare predictions are essential to mitigate
potential impacts on critical infrastructure. However, the current performance
of solar flare forecasting is insufficient. In this study, we address the task
of predicting the class of the largest solar flare expected to occur within the
next 72 hours. Existing methods often fail to adequately address the severe
class imbalance across flare classes. To address this issue, we propose a solar
flare prediction model based on multiple deep state space models. In addition,
we introduce the frequency & local-boundary-aware reliability loss (FLARE loss)
to improve predictive performance and reliability under class imbalance.
Experiments were conducted on a multi-wavelength solar image dataset covering a
full 11-year solar activity cycle. As a result, our method outperformed
baseline approaches in terms of both the Gandin-Murphy-Gerrity score and the
true skill statistic, which are standard metrics in terms of the performance
and reliability.

</details>


### [80] [TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion](https://arxiv.org/abs/2509.10005)
*Xiaodong Guo,Tong Liu,Yike Li,Zi'ang Lin,Zhihong Deng*

Main category: cs.CV

TL;DR: TUNI是一个创新的RGB-热成像语义分割模型，通过统一的编码器同时处理多模态特征提取和跨模态融合，实现了更紧凑的架构和实时性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有RGB-T语义分割模型中热特征提取有限、跨模态融合效果不佳以及编码器冗余导致的实时效率问题。

Method: 使用大规模RGB和伪热数据预训练的RGB-T编码器，采用多模态特征提取和跨模态融合的统一方式；通过精简热分支实现紧凑架构；引入RGB-T局部模块，使用自适应余弦相似度选择性地强调跨模态的显著一致和差异局部特征。

Result: 在FMB、PST900和CART数据集上达到与最先进模型竞争的性能，参数量和计算成本更低；在Jetson Orin NX上实现27 FPS的推理速度，具备实时部署能力。

Conclusion: TUNI通过统一的编码器设计和局部特征融合增强，在保持高性能的同时显著提升了模型的效率和实时能力，为自动驾驶平台的环境感知提供了有效的解决方案。

Abstract: RGB-thermal (RGB-T) semantic segmentation improves the environmental
perception of autonomous platforms in challenging conditions. Prevailing models
employ encoders pre-trained on RGB images to extract features from both RGB and
infrared inputs, and design additional modules to achieve cross-modal feature
fusion. This results in limited thermal feature extraction and suboptimal
cross-modal fusion, while the redundant encoders further compromises the
model's real-time efficiency. To address the above issues, we propose TUNI,
with an RGB-T encoder consisting of multiple stacked blocks that simultaneously
perform multi-modal feature extraction and cross-modal fusion. By leveraging
large-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder
learns to integrate feature extraction and fusion in a unified manner. By
slimming down the thermal branch, the encoder achieves a more compact
architecture. Moreover, we introduce an RGB-T local module to strengthen the
encoder's capacity for cross-modal local feature fusion. The RGB-T local module
employs adaptive cosine similarity to selectively emphasize salient consistent
and distinct local features across RGB-T modalities. Experimental results show
that TUNI achieves competitive performance with state-of-the-art models on FMB,
PST900 and CART, with fewer parameters and lower computational cost. Meanwhile,
it achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its
real-time capability in deployment. Codes are available at
https://github.com/xiaodonguo/TUNI.

</details>


### [81] [Few-Part-Shot Font Generation](https://arxiv.org/abs/2509.10006)
*Masaki Akiba,Shumpei Takezaki,Daichi Haraguchi,Seiichi Uchida*

Main category: cs.CV

TL;DR: 提出了一种基于局部设计元素的少部件字体生成模型，只需输入部分形状而非完整字符即可生成整个字体


<details>
  <summary>Details</summary>
Motivation: 传统少样本字体生成需要完整字符形状，而该方法只需局部设计元素，提高字体创建效率并探索局部设计细节对整体字符结构的影响

Method: 设计基于局部形状的少部件字体生成模型，通过部分设计元素推断和生成整个字体

Result: 模型能够仅通过输入部分形状生成完整字体，提高了字体创建的效率

Conclusion: 该方法为字体设计提供了更高效的解决方案，并揭示了局部设计元素对整体字符结构的重要影响

Abstract: This paper proposes a novel model of few-part-shot font generation, which
designs an entire font based on a set of partial design elements, i.e., partial
shapes. Unlike conventional few-shot font generation, which requires entire
character shapes for a couple of character classes, our approach only needs
partial shapes as input. The proposed model not only improves the efficiency of
font creation but also provides insights into how partial design details
influence the entire structure of the individual characters.

</details>


### [82] [Efficient and Accurate Downfacing Visual Inertial Odometry](https://arxiv.org/abs/2509.10021)
*Jonas Kühne,Christian Vogt,Michele Magno,Luca Benini*

Main category: cs.CV

TL;DR: 这篇论文提出了一种专门为微纳无人机优化的高效准确视觉悬洞定位管道，采用量化优化的特征检测跟踪算法和刚体运动模型，在超低功耗RISC-V SoC上实现了计算要求低但准确性高的实时VIO解决方案


<details>
  <summary>Details</summary>
Motivation: 解决高准确VIO管道需要强大计算能力而轻量级实现适用于微控制器的问题，为微纳无人机提供高效准确的运动估计方案

Method: 采用SuperPoint、PX4FLOW、ORB等现代特征检测跟踪算法，进行量化和优化，结合刚体运动模型减少估计误差，在RISC-V超低功耗SoC上实现

Result: 在GAP9低功耗SoC上，优化后的管道比基准管道RMSE平均减少3.65倍（ORB跟踪器），PX4FLOW在低速运动时保持与ORB相当的跟踪准确性但运行时间更短

Conclusion: 该管道成功在超低功耗硬件平台上实现了高准确的实时VIO，为微纳无人机提供了计算效率和准确性之间的良好平衡，特别是PX4FLOW在低速场景中显示出优异的性能

Abstract: Visual Inertial Odometry (VIO) is a widely used computer vision method that
determines an agent's movement through a camera and an IMU sensor. This paper
presents an efficient and accurate VIO pipeline optimized for applications on
micro- and nano-UAVs. The proposed design incorporates state-of-the-art feature
detection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and
quantized for emerging RISC-V-based ultra-low-power parallel systems on chips
(SoCs). Furthermore, by employing a rigid body motion model, the pipeline
reduces estimation errors and achieves improved accuracy in planar motion
scenarios. The pipeline's suitability for real-time VIO is assessed on an
ultra-low-power SoC in terms of compute requirements and tracking accuracy
after quantization. The pipeline, including the three feature tracking methods,
was implemented on the SoC for real-world validation. This design bridges the
gap between high-accuracy VIO pipelines that are traditionally run on
computationally powerful systems and lightweight implementations suitable for
microcontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates
an average reduction in RMSE of up to a factor of 3.65x over the baseline
pipeline when using the ORB feature tracker. The analysis of the computational
complexity of the feature trackers further shows that PX4FLOW achieves on-par
tracking accuracy with ORB at a lower runtime for movement speeds below 24
pixels/frame.

</details>


### [83] [Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction From Single Images](https://arxiv.org/abs/2509.10024)
*Danling Cao*

Main category: cs.CV

TL;DR: 提出基于卷积神经网络的层次化多级注意力网络MLANet，从单张野外图像重建3D人脸模型，预测几何、纹理、姿态和光照参数


<details>
  <summary>Details</summary>
Motivation: 从2D野外图像恢复3D人脸模型具有广泛应用前景，但缺乏真实标注数据集和复杂现实环境是主要挑战

Method: 使用预训练层次化主干网络，在2D人脸特征提取不同阶段引入多级注意力机制，采用半监督训练策略结合3DMM参数和可微分渲染器实现端到端训练

Result: 在AFLW2000-3D和MICC Florence基准数据集上进行了对比和消融实验，在3D人脸重建和对齐任务上进行了定量和定性评估

Conclusion: 该方法在3D人脸重建任务中表现出有效性，多级注意力机制和半监督训练策略有助于提升模型性能

Abstract: Recovering 3D face models from 2D in-the-wild images has gained considerable
attention in the computer vision community due to its wide range of potential
applications. However, the lack of ground-truth labeled datasets and the
complexity of real-world environments remain significant challenges. In this
chapter, we propose a convolutional neural network-based approach, the
Hierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face
models from single in-the-wild images. Our model predicts detailed facial
geometry, texture, pose, and illumination parameters from a single image.
Specifically, we employ a pre-trained hierarchical backbone network and
introduce multi-level attention mechanisms at different stages of 2D face image
feature extraction. A semi-supervised training strategy is employed,
incorporating 3D Morphable Model (3DMM) parameters from publicly available
datasets along with a differentiable renderer, enabling an end-to-end training
process. Extensive experiments, including both comparative and ablation
studies, were conducted on two benchmark datasets, AFLW2000-3D and MICC
Florence, focusing on 3D face reconstruction and 3D face alignment tasks. The
effectiveness of the proposed method was evaluated both quantitatively and
qualitatively.

</details>


### [84] [LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA](https://arxiv.org/abs/2509.10026)
*Jing Huang,Zhiya Tan,Shutao Gong,Fanwei Zeng,Jianshu Li*

Main category: cs.CV

TL;DR: LaV-CoT是一个语言感知的视觉思维链框架，通过多阶段推理流程和多方面奖励优化，显著提升多语言视觉问答性能，在多个数据集上超越开源和专有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要依赖文本思维链，对多语言多模态推理支持有限，限制了在实际应用中的部署。

Method: 采用多阶段推理流程（文本摘要+边界框、语言识别、空间对象级描述、逐步逻辑推理），结合自动化数据生成和两阶段训练范式（SFT+GRPO），使用多方面奖励进行优化。

Result: 在多个数据集上达到9.5%的准确率提升，超越相似规模的开源模型和更大规模的模型，甚至优于GPT-4o和Gemini等专有模型。

Conclusion: LaV-CoT框架有效解决了多语言视觉推理的挑战，通过可解释的推理流程和奖励优化机制，展现了在实际工业部署中的潜力。

Abstract: As large vision language models (VLMs) advance, their capabilities in
multilingual visual question answering (mVQA) have significantly improved.
Chain-of-thought (CoT) reasoning has been proven to enhance interpretability
and complex reasoning. However, most existing approaches rely primarily on
textual CoT and provide limited support for multilingual multimodal reasoning,
constraining their deployment in real-world applications. To address this gap,
we introduce \textbf{LaV-CoT}, the first Language-aware Visual CoT framework
with Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable
multi-stage reasoning pipeline consisting of Text Summary with Bounding Box
(BBox), Language Identification, Spatial Object-level Captioning, and
Step-by-step Logical Reasoning. Following this reasoning pipeline, we design an
automated data curation method that generates multilingual CoT annotations
through iterative generation, correction, and refinement, enabling scalable and
high-quality training data. To improve reasoning and generalization, LaV-CoT
adopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)
with Language-aware Group Relative Policy Optimization (GRPO), guided by
verifiable multi-aspect rewards including language consistency, structural
accuracy, and semantic alignment. Extensive evaluations on public datasets
including MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up
to \(\sim\)9.5\% accuracy improvements over open-source baselines of similar
size and even surpasses models with 2$\times$ larger scales by \(\sim\)2.6\%.
Moreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513
and Gemini-2.5-flash. We further conducted an online A/B test to validate our
method on real-world data, highlighting its effectiveness for industrial
deployment. Our code is available at this link:
\href{https://github.com/HJNVR/LaV-CoT}

</details>


### [85] [Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation](https://arxiv.org/abs/2509.10058)
*Sung-Lin Tsai,Bo-Lun Huang,Yu Ting Shen,Cheng Yu Yeo,Chiang Tseng,Bo-Kai Ruan,Wen-Sheng Lien,Hong-Han Shuai*

Main category: cs.CV

TL;DR: 提出无需训练的框架，通过大语言模型解析模糊颜色描述并在CIELAB色彩空间优化文本嵌入，提升文本到图像生成的颜色准确性


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在处理复杂颜色术语（如蒂芙尼蓝、柠檬绿）时存在颜色对齐问题，现有方法无法系统性地解决模糊颜色描述

Method: 使用大语言模型解析文本提示中的模糊颜色术语，基于CIELAB色彩空间的空间关系优化文本嵌入，指导颜色混合操作

Result: 实验表明该方法在不影响图像质量的情况下显著提高了颜色对齐精度，无需额外训练或参考图像

Conclusion: 该框架有效弥合了文本语义与视觉生成之间的差距，为精确颜色渲染提供了训练免费的解决方案

Abstract: Accurate color alignment in text-to-image (T2I) generation is critical for
applications such as fashion, product visualization, and interior design, yet
current diffusion models struggle with nuanced and compound color terms (e.g.,
Tiffany blue, lime green, hot pink), often producing images that are misaligned
with human intent. Existing approaches rely on cross-attention manipulation,
reference images, or fine-tuning but fail to systematically resolve ambiguous
color descriptions. To precisely render colors under prompt ambiguity, we
propose a training-free framework that enhances color fidelity by leveraging a
large language model (LLM) to disambiguate color-related prompts and guiding
color blending operations directly in the text embedding space. Our method
first employs a large language model (LLM) to resolve ambiguous color terms in
the text prompt, and then refines the text embeddings based on the spatial
relationships of the resulting color terms in the CIELAB color space. Unlike
prior methods, our approach improves color accuracy without requiring
additional training or external reference images. Experimental results
demonstrate that our framework improves color alignment without compromising
image quality, bridging the gap between text semantics and visual generation.

</details>


### [86] [Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration](https://arxiv.org/abs/2509.10059)
*Yue Zhou,Litong Feng,Mengcheng Lan,Xue Yang,Qingyun Li,Yiping Ke,Xue Jiang,Wayne Zhang*

Main category: cs.CV

TL;DR: AVI-Math是首个针对无人机航拍图像多模态数学推理的基准测试，包含3,773个高质量数学问题，涵盖6个数学学科和20个主题，评估了14个主流视觉语言模型，发现它们在数学推理方面存在显著局限性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在无人机遥感任务中的数学推理能力（如精确距离计算、轨迹估计和空间分析）尚未得到充分测试，需要专门的基准来评估这一领域的能力。

Method: 构建AVI-Math数据集，包含从不同高度和角度采集的无人机图像相关的数学问题，涵盖几何、逻辑和代数等学科，对14个主流VLMs进行全面评估，并探索思维链提示和微调技术。

Result: 尽管这些模型在以往的多模态基准测试中表现良好，但在AVI-Math的推理任务中表现不佳，显示出当前VLMs在数学推理能力上的显著局限性。思维链提示和微调技术显示出改善推理挑战的潜力。

Conclusion: 研究不仅揭示了VLMs在数学推理方面的局限性，还为推进无人机应用中可信赖的VLMs发展提供了有价值的见解，数据集和代码将开源以促进未来研究。

Abstract: Mathematical reasoning is critical for tasks such as precise distance and
area computations, trajectory estimations, and spatial analysis in unmanned
aerial vehicle (UAV) based remote sensing, yet current vision-language models
(VLMs) have not been adequately tested in this domain. To address this gap, we
introduce AVI-Math, the first benchmark to rigorously evaluate multimodal
mathematical reasoning in aerial vehicle imagery, moving beyond simple counting
tasks to include domain-specific knowledge in areas such as geometry, logic,
and algebra. The dataset comprises 3,773 high-quality vehicle-related questions
captured from UAV views, covering 6 mathematical subjects and 20 topics. The
data, collected at varying altitudes and from multiple UAV angles, reflects
real-world UAV scenarios, ensuring the diversity and complexity of the
constructed mathematical problems. In this paper, we benchmark 14 prominent
VLMs through a comprehensive evaluation and demonstrate that, despite their
success on previous multimodal benchmarks, these models struggle with the
reasoning tasks in AVI-Math. Our detailed analysis highlights significant
limitations in the mathematical reasoning capabilities of current VLMs and
suggests avenues for future research. Furthermore, we explore the use of
Chain-of-Thought prompting and fine-tuning techniques, which show promise in
addressing the reasoning challenges in AVI-Math. Our findings not only expose
the limitations of VLMs in mathematical reasoning but also offer valuable
insights for advancing UAV-based trustworthy VLMs in real-world applications.
The code, and datasets will be released at
https://github.com/VisionXLab/avi-math

</details>


### [87] [BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals](https://arxiv.org/abs/2509.10080)
*Minsang Kong,Myeongjun Kim,Sang Gu Kang,Sang Hun Lee*

Main category: cs.CV

TL;DR: BEVTraj是一个新颖的轨迹预测框架，直接在鸟瞰图空间利用实时传感器数据，无需依赖预建高清地图，通过可变形注意力和稀疏目标候选提议模块实现端到端预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预建高清地图或实时地图构建模块，但预建地图局限于特定区域且无法适应瞬时变化，而地图构建模块可能遗漏关键场景细节或引入错误，影响预测性能。

Method: 提出BEVTraj框架，在BEV空间直接操作，使用可变形注意力从密集BEV特征中高效提取相关上下文，并引入稀疏目标候选提议(SGCP)模块实现完全端到端预测，无需后处理步骤。

Result: 大量实验表明，BEVTraj实现了与最先进的基于高清地图模型相当的性能，同时通过消除对预建地图的依赖提供了更大的灵活性。

Conclusion: BEVTraj框架成功克服了传统方法对预建地图的依赖限制，在保持高性能的同时提供了更好的适应性和灵活性，为自动驾驶轨迹预测提供了新的解决方案。

Abstract: In autonomous driving, trajectory prediction is essential for ensuring safe
and efficient navigation. To improve prediction accuracy, recent approaches
often rely on pre-built high-definition (HD) maps or real-time local map
construction modules to incorporate static environmental information. However,
pre-built HD maps are limited to specific regions and cannot adapt to transient
changes. In addition, local map construction modules, which recognize only
predefined elements, may fail to capture critical scene details or introduce
errors that degrade prediction performance. To overcome these limitations, we
propose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory
prediction framework that operates directly in the bird's-eye view (BEV) space
utilizing real-time sensor data without relying on any pre-built maps. The
BEVTraj leverages deformable attention to efficiently extract relevant context
from dense BEV features. Furthermore, we introduce a Sparse Goal Candidate
Proposal (SGCP) module, which enables full end-to-end prediction without
requiring any post-processing steps. Extensive experiments demonstrate that the
BEVTraj achieves performance comparable to state-of-the-art HD map-based models
while offering greater flexibility by eliminating the dependency on pre-built
maps. The source code is available at https://github.com/Kongminsang/bevtraj.

</details>


### [88] [Leveraging Multi-View Weak Supervision for Occlusion-Aware Multi-Human Parsing](https://arxiv.org/abs/2509.10093)
*Laura Bragagnolo,Matteo Terreran,Leonardo Barcellona,Stefano Ghidoni*

Main category: cs.CV

TL;DR: 提出了一种利用多视角信息改进多人解析模型在遮挡情况下的训练框架，通过弱监督和一致性损失提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有方法在人体重叠遮挡情况下表现不佳，而多视角信息可以帮助分离重叠的人体

Method: 提出基于多视角信息的训练框架，包括弱监督实例分割和多视角一致性损失，并设计了半自动标注策略生成多视角RGB+D数据的标注

Result: 在遮挡场景下相比基线模型获得了4.20%的相对性能提升

Conclusion: 多视角信息能有效提升多人解析模型在遮挡情况下的性能，提出的训练框架和标注策略具有实用价值

Abstract: Multi-human parsing is the task of segmenting human body parts while
associating each part to the person it belongs to, combining instance-level and
part-level information for fine-grained human understanding. In this work, we
demonstrate that, while state-of-the-art approaches achieved notable results on
public datasets, they struggle considerably in segmenting people with
overlapping bodies. From the intuition that overlapping people may appear
separated from a different point of view, we propose a novel training framework
exploiting multi-view information to improve multi-human parsing models under
occlusions. Our method integrates such knowledge during the training process,
introducing a novel approach based on weak supervision on human instances and a
multi-view consistency loss. Given the lack of suitable datasets in the
literature, we propose a semi-automatic annotation strategy to generate human
instance segmentation masks from multi-view RGB+D data and 3D human skeletons.
The experiments demonstrate that the approach can achieve up to a 4.20\%
relative improvement on human parsing over the baseline model in occlusion
scenarios.

</details>


### [89] [VARCO-VISION-2.0 Technical Report](https://arxiv.org/abs/2509.10105)
*Young-rok Cha,Jeongho Ju,SunYoung Park,Jong-Hyeon Lee,Younghyun Yu,Youngjune Kim*

Main category: cs.CV

TL;DR: VARCO-VISION-2.0是一个开源的韩英双语视觉语言模型，相比前代模型能力提升，支持多图像理解、文档图表处理和空间感知OCR，在14B和1.7B两个规模版本上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够同时处理韩语和英语的双语视觉语言模型，提升多图像理解能力，特别是对文档、图表和表格等复杂输入的处理，并实现空间感知的OCR功能。

Method: 采用四阶段课程训练和内存高效技术，通过偏好优化提升安全性，同时保持核心语言能力。模型支持多图像输入并能够预测文本内容及其空间位置。

Result: 在广泛基准评估中表现出强大的空间定位能力，14B模型在OpenCompass VLM排行榜上位列同规模模型第8名，两个版本模型均取得竞争性结果。

Conclusion: VARCO-VISION-2.0模型推动了双语视觉语言模型的发展，为实际应用提供了14B完整规模和1.7B轻量级两个优化版本，已在Hugging Face发布。

Abstract: We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model
(VLM) for Korean and English with improved capabilities compared to the
previous model VARCO-VISION-14B. The model supports multi-image understanding
for complex inputs such as documents, charts, and tables, and delivers
layoutaware OCR by predicting both textual content and its spatial location.
Trained with a four-stage curriculum with memory-efficient techniques, the
model achieves enhanced multimodal alignment, while preserving core language
abilities and improving safety via preference optimization. Extensive benchmark
evaluations demonstrate strong spatial grounding and competitive results for
both languages, with the 14B model achieving 8th place on the OpenCompass VLM
leaderboard among models of comparable scale. Alongside the 14B-scale model, we
release a 1.7B version optimized for on-device deployment. We believe these
models advance the development of bilingual VLMs and their practical
applications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a
full-scale 14B model and a lightweight 1.7B model.

</details>


### [90] [A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss](https://arxiv.org/abs/2509.10114)
*MohammadAli Hamidi,Hadi Amirpour,Luigi Atzori,Christian Timmerer*

Main category: cs.CV

TL;DR: 这篇论文提出了一种轻量级的面部图像质量评估方法，通过组合两个紧凑卷积神经网络和相关性损失函数，在保持高准确性的同时实现了较低的计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的面部图像质量评估方法要么无法抓取面部特定的降级特征，要么计算复杂度过高，限制了在实际环境中的应用。需要一种既高效又能够准确评估复杂环境下面部图像质量的方法。

Method: 集成了两个紧凑卷积神经网络（MobileNetV3-Small和ShuffleNetV2），通过简单平均法进行预测级融合。使用MSECorrLoss损失函数，结合均方误差和相关性正则化器，以提高与人类感知判断的一致性。

Result: 在VQualA FIQA测试集上达到SRCC 0.9829和PLCC 0.9894的高相关性系数，同时满足竞赛效率要求，实现了准确性和计算效率的良好平衡。

Conclusion: 该方法提供了一种高效、轻量级的面部图像质量评估方案，适合在实际面部识别系统中部署，能够在复杂环境下进行准确的感知质量评估。

Abstract: Face image quality assessment (FIQA) plays a critical role in face
recognition and verification systems, especially in uncontrolled, real-world
environments. Although several methods have been proposed, general-purpose
no-reference image quality assessment techniques often fail to capture
face-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be
computationally intensive, limiting their practical applicability. We propose a
lightweight and efficient method for FIQA, designed for the perceptual
evaluation of face images in the wild. Our approach integrates an ensemble of
two compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2,
with prediction-level fusion via simple averaging. To enhance alignment with
human perceptual judgments, we employ a correlation-aware loss (MSECorrLoss),
combining mean squared error (MSE) with a Pearson correlation regularizer. Our
method achieves a strong balance between accuracy and computational cost,
making it suitable for real-world deployment. Experiments on the VQualA FIQA
benchmark demonstrate that our model achieves a Spearman rank correlation
coefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient
(PLCC) of 0.9894, remaining within competition efficiency constraints.

</details>


### [91] [Realism Control One-step Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2509.10122)
*Zongliang Wu,Siming Zheng,Peng-Tao Jiang,Xin Yuan*

Main category: cs.CV

TL;DR: 通过提出RCOD框架，在一步滑散模型中实现了信实度与现实性的灵活控制，在保持计算效率的同时提升了超分辨率的质量


<details>
  <summary>Details</summary>
Motivation: 一步滑散方法虽然效率高，但缺乏对信实度与现实性之间变换的灵活控制机制，而这在多步方法中可通过调整采样步数来实现

Method: 提出RCOD框架：1)潜在域分组策略实现显式控制；2)适配性采样策略对齐推射正则化；3)视觉提示注入模块替代文本提示，提升恢复准确性

Result: 在各种定量指标和视觉质量上都超过了最先进的一步滑散方法，同时在推理阶段具有灵活的现实性控制能力

Conclusion: RCOD框架有效解决了一步滑散模型在实际图像超分辨率任务中的信实度-现实性平衡问题，为高效高质量的实际应用提供了新的解决方案

Abstract: Pre-trained diffusion models have shown great potential in real-world image
super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.
While one-step diffusion (OSD) methods significantly improve efficiency
compared to traditional multi-step approaches, they still have limitations in
balancing fidelity and realism across diverse scenarios. Since the OSDs for SR
are usually trained or distilled by a single timestep, they lack flexible
control mechanisms to adaptively prioritize these competing objectives, which
are inherently manageable in multi-step methods through adjusting sampling
steps. To address this challenge, we propose a Realism Controlled One-step
Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping
strategy that enables explicit control over fidelity-realism trade-offs during
the noise prediction phase with minimal training paradigm modifications and
original training data. A degradation-aware sampling strategy is also
introduced to align distillation regularization with the grouping strategy and
enhance the controlling of trade-offs. Moreover, a visual prompt injection
module is used to replace conventional text prompts with degradation-aware
visual tokens, enhancing both restoration accuracy and semantic consistency.
Our method achieves superior fidelity and perceptual quality while maintaining
computational efficiency. Extensive experiments demonstrate that RCOD
outperforms state-of-the-art OSD methods in both quantitative metrics and
visual qualities, with flexible realism control capabilities in the inference
stage. The code will be released.

</details>


### [92] [Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment](https://arxiv.org/abs/2509.10134)
*Rini Smita Thakur,Rajeev Ranjan Dwivedi,Vinod K Kurmi*

Main category: cs.CV

TL;DR: Grad-CL是一种源自由域适应框架，通过梯度引导的伪标签优化和余弦相似度对比学习，在无需源数据的情况下提升视盘和视杯分割的跨域性能


<details>
  <summary>Details</summary>
Motivation: 解决视盘和视杯分割模型在不同成像协议或条件下性能显著下降的问题，特别是在无法访问原始源数据的情况下实现鲁棒的域适应

Method: 两阶段方法：第一阶段使用梯度机制提取类别特定特征进行不确定性量化和原型估计以优化伪标签；第二阶段使用基于余弦相似度的对比损失增强视杯和视盘特征的类间分离性

Result: 在具有挑战性的跨域眼底成像数据集上，Grad-CL超越了最先进的无监督和源自由域适应方法，实现了优越的分割精度和改善的边界描绘

Conclusion: Grad-CL框架有效解决了医学图像分割中的域适应问题，特别是在源数据不可用的情况下，为眼科疾病的早期诊断提供了可靠的技术支持

Abstract: Accurate segmentation of the optic disc and cup is critical for the early
diagnosis and management of ocular diseases such as glaucoma. However,
segmentation models trained on one dataset often suffer significant performance
degradation when applied to target data acquired under different imaging
protocols or conditions. To address this challenge, we propose
\textbf{Grad-CL}, a novel source-free domain adaptation framework that
leverages a pre-trained source model and unlabeled target data to robustly
adapt segmentation performance without requiring access to the original source
data. Grad-CL combines a gradient-guided pseudolabel refinement module with a
cosine similarity-based contrastive learning strategy. In the first stage,
salient class-specific features are extracted via a gradient-based mechanism,
enabling more accurate uncertainty quantification and robust prototype
estimation for refining noisy pseudolabels. In the second stage, a contrastive
loss based on cosine similarity is employed to explicitly enforce inter-class
separability between the gradient-informed features of the optic cup and disc.
Extensive experiments on challenging cross-domain fundus imaging datasets
demonstrate that Grad-CL outperforms state-of-the-art unsupervised and
source-free domain adaptation methods, achieving superior segmentation accuracy
and improved boundary delineation. Project and code are available at
https://visdomlab.github.io/GCL/.

</details>


### [93] [Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization](https://arxiv.org/abs/2509.10140)
*Yifan Chang,Jie Qin,Limeng Qiao,Xiaofeng Wang,Zheng Zhu,Lin Ma,Xingang Wang*

Main category: cs.CV

TL;DR: VQBridge方法通过compress-process-recover流水线解决VQ训练中的不稳定问题，实现100%代码本使用率，提升重建性能和图像生成质量


<details>
  <summary>Details</summary>
Motivation: 解决向量量化(VQ)在训练过程中遇到的直通估计偏差、一步延迟更新和代码本梯度稀疏等问题，这些问题导致重建性能低下和代码本使用率低

Method: 提出VQBridge方法，通过compress-process-recover流水线优化代码向量，结合学习逆逆减技术，实现稳定有效的代码本训练

Result: FVQ方法在各种代码本配置下实现100%代码本使用率，甚至262k大规模代码本也能完全使用，达到状态前沿的重建性能，并在LlamaGen中显著提升图像生成质量

Conclusion: VQBridge提供了一种简单有效的方法来解决VQ训练的核心挑战，高质量的离散化标记器对自回归图像生成至关重要

Abstract: Vector quantization (VQ) is a key component in discrete tokenizers for image
generation, but its training is often unstable due to straight-through
estimation bias, one-step-behind updates, and sparse codebook gradients, which
lead to suboptimal reconstruction performance and low codebook usage. In this
work, we analyze these fundamental challenges and provide a simple yet
effective solution. To maintain high codebook usage in VQ networks (VQN) during
learning annealing and codebook size expansion, we propose VQBridge, a robust,
scalable, and efficient projector based on the map function method. VQBridge
optimizes code vectors through a compress-process-recover pipeline, enabling
stable and effective codebook training. By combining VQBridge with learning
annealing, our VQN achieves full (100%) codebook usage across diverse codebook
configurations, which we refer to as FVQ (FullVQ). Through extensive
experiments, we demonstrate that FVQ is effective, scalable, and generalizable:
it attains 100% codebook usage even with a 262k-codebook, achieves
state-of-the-art reconstruction performance, consistently improves with larger
codebooks, higher vector channels, or longer training, and remains effective
across different VQ variants. Moreover, when integrated with LlamaGen, FVQ
significantly enhances image generation performance, surpassing visual
autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID,
highlighting the importance of high-quality tokenizers for strong
autoregressive image generation.

</details>


### [94] [LayerLock: Non-collapsing Representation Learning with Progressive Freezing](https://arxiv.org/abs/2509.10156)
*Goker Erdogan,Nikhil Parthasarathy,Catalin Ionescu,Drew Hudson,Alexander Lerchner,Andrew Zisserman,Mehdi Sajjadi,Joao Carreira*

Main category: cs.CV

TL;DR: LayerLock是一种通过渐进层冻结从像素预测过渡到潜在预测的自监督视觉表示学习方法，可加速训练并避免表示坍塌


<details>
  <summary>Details</summary>
Motivation: 观察到视频掩码自编码(MAE)训练中ViT层按深度顺序收敛的现象，希望利用这一规律来加速训练并实现有效的潜在预测

Method: 通过明确的进度表在训练过程中逐步冻结模型层，从像素预测逐渐过渡到潜在预测

Result: 在高达40亿参数的大型模型上应用LayerLock，在4DS感知套件上的表现超过了非潜在掩码预测方法

Conclusion: LayerLock提供了一种简单有效的自监督学习方法，能够显著加速训练并实现更好的表示学习效果

Abstract: We introduce LayerLock, a simple yet effective approach for self-supervised
visual representation learning, that gradually transitions from pixel to latent
prediction through progressive layer freezing. First, we make the observation
that during training of video masked-autoencoding (MAE) models, ViT layers
converge in the order of their depth: shallower layers converge early, deeper
layers converge late. We then show that this observation can be exploited to
accelerate standard MAE by progressively freezing the model according to an
explicit schedule, throughout training. Furthermore, this same schedule can be
used in a simple and scalable approach to latent prediction that does not
suffer from "representation collapse". We apply our proposed approach,
LayerLock, to large models of up to 4B parameters with results surpassing those
of non-latent masked prediction on the 4DS perception suite.

</details>


### [95] [On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints](https://arxiv.org/abs/2509.10241)
*Elias De Smijter,Renaud Detry,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: 隐式和显式新视点合成方法在太空场景中的系统比较，外观嵌入提高照明保真度但不改善几何精度，凸净化方法更适合安全关键应用


<details>
  <summary>Details</summary>
Motivation: 评估外视嵌入在太空基地三维物体重建中对几何精度的影响，这是空间机器人应用的关键要求

Method: 使用SPEED+数据集比较K-Planes、高斯抽射和凸净化抽射等隐式和显式方法，分析外视嵌入对照明保真度和几何精度的作用

Result: 外视嵌入主要减少显式方法需要的原语数量，而不是提高几何精度；凸净化抽射比高斯抽射获得更紧凑、无杂乱的表示

Conclusion: 外视嵌入在以几何为中心的任务中有限，凸净化方法在重建质量和表示效率之间提供更优的平衡，适合交互和碰撞避免等安全关键应用

Abstract: We present the first systematic comparison of implicit and explicit Novel
View Synthesis methods for space-based 3D object reconstruction, evaluating the
role of appearance embeddings. While embeddings improve photometric fidelity by
modeling lighting variation, we show they do not translate into meaningful
gains in geometric accuracy - a critical requirement for space robotics
applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian
Splatting, and Convex Splatting, and demonstrate that embeddings primarily
reduce the number of primitives needed for explicit methods rather than
enhancing geometric fidelity. Moreover, convex splatting achieves more compact
and clutter-free representations than Gaussian splatting, offering advantages
for safety-critical applications such as interaction and collision avoidance.
Our findings clarify the limits of appearance embeddings for geometry-centric
tasks and highlight trade-offs between reconstruction quality and
representation efficiency in space scenarios.

</details>


### [96] [GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection](https://arxiv.org/abs/2509.10250)
*Haozhen Yan,Yan Hong,Suning Lang,Jiahui Zhan,Yikun Ji,Yujie Gao,Jun Lan,Huijia Zhu,Weiqiang Wang,Jianfu Zhang*

Main category: cs.CV

TL;DR: GAMMA是一个新的AI生成图像检测框架，通过多样化操作策略和多任务监督来减少领域偏差，提高对未知生成模型的泛化能力，在GenImage基准上实现了5.8%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器主要依赖特定生成模型的伪影特征，在面对新的、未见过的生成模型时泛化能力有限，需要解决领域偏差问题。

Method: 提出GAMMA训练框架：1）引入多样化操作策略（基于修复的操作和语义保持扰动）；2）采用多任务监督，包含双分割头和分类头；3）使用反向交叉注意力机制让分割头指导分类分支。

Result: 在GenImage基准上达到最先进的泛化性能，准确率提升5.8%，对新发布的生成模型（如GPT-4o）保持强鲁棒性。

Conclusion: GAMMA通过减少领域偏差和增强语义对齐，有效提高了AI生成图像检测器对未知生成模型的泛化能力，为解决生成模型多样性带来的检测挑战提供了有效方案。

Abstract: With generative models becoming increasingly sophisticated and diverse,
detecting AI-generated images has become increasingly challenging. While
existing AI-genereted Image detectors achieve promising performance on
in-distribution generated images, their generalization to unseen generative
models remains limited. This limitation is largely attributed to their reliance
on generation-specific artifacts, such as stylistic priors and compression
patterns. To address these limitations, we propose GAMMA, a novel training
framework designed to reduce domain bias and enhance semantic alignment. GAMMA
introduces diverse manipulation strategies, such as inpainting-based
manipulation and semantics-preserving perturbations, to ensure consistency
between manipulated and authentic content. We employ multi-task supervision
with dual segmentation heads and a classification head, enabling pixel-level
source attribution across diverse generative domains. In addition, a reverse
cross-attention mechanism is introduced to allow the segmentation heads to
guide and correct biased representations in the classification branch. Our
method achieves state-of-the-art generalization performance on the GenImage
benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on
newly released generative model such as GPT-4o.

</details>


### [97] [Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI](https://arxiv.org/abs/2509.10257)
*Ema Masterl,Tina Vipotnik Vesnaver,Žiga Špiclin*

Main category: cs.CV

TL;DR: 这篇论文对比了三种元胎脑部MRI超分辨率重建方法（NiftyMIC、SVRTK、NeSVoR）在健康和病理案例中的性能。NeSVoR显示出最高的重建成功率，虽然不同方法在体积测量上存在差异，但诊断分类性能受影响小。


<details>
  <summary>Details</summary>
Motivation: 元胎脑部MRI存在分辨率低、运动伪影和3D解剖信息不充分等问题。虽然有多种超分辨率重建方法，但对它们在病理案例中的比较性能以及对下游任务的影响仍然研究不充。

Method: 对140例元胎脑部MRI扫描（包括健康和脑室扩大病例）进行三种SOTA SRR方法处理，使用BoUNTi算法进行脑部结构分割，评估视觉质量、重建成功率、体积测量一致性和诊断分类性能。

Result: NeSVoR在健康和病理组都展现了最高和最一致的重建成功率（>90%）。虽然不同SRR方法在体积估计上存在显著差异，但对脑室扩大的诊断分类性能并无影响。

Conclusion: NeSVoR方法具有更强的稳健性，而诊断性能在SRR导致的体积变异下仍保持良好。这为元胎脑部MRI重建方法的选择提供了重要参考。

Abstract: Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce
motion artifacts caused by fetal movement. However, these stacks are typically
low resolution, may suffer from motion corruption, and do not adequately
capture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to
address these limitations by combining slice-to-volume registration and
super-resolution techniques to generate high-resolution (HR) 3D volumes. While
several SRR methods have been proposed, their comparative performance -
particularly in pathological cases - and their influence on downstream
volumetric analysis and diagnostic tasks remain underexplored. In this study,
we applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to
140 fetal brain MRI scans, including both healthy controls (HC) and
pathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was
segmented using the BoUNTi algorithm to extract volumes of nine principal brain
structures. We evaluated visual quality, SRR success rates, volumetric
measurement agreement, and diagnostic classification performance. NeSVoR
demonstrated the highest and most consistent reconstruction success rate (>90%)
across both HC and PC groups. Although significant differences in volumetric
estimates were observed between SRR methods, classification performance for VM
was not affected by the choice of SRR method. These findings highlight NeSVoR's
robustness and the resilience of diagnostic performance despite SRR-induced
volumetric variability.

</details>


### [98] [Mask Consistency Regularization in Object Removal](https://arxiv.org/abs/2509.10259)
*Hua Yuan,Jin Yuan,Yicheng Jiang,Yao Zhang,Xin Geng,Yong Rui*

Main category: cs.CV

TL;DR: 通过提出面具一致性正则化(MCR)训练策略，解决了图像修复中的面具幻觉和面具形状偏异两大挑战，提升了物体移除的效果。


<details>
  <summary>Details</summary>
Motivation: 当前涵散模型在物体移除任务中存在面具幻觉(生成无关内容)和面具形状偏异(根据面具形状生成对象而非周围内容)两大问题。

Method: 提出Mask Consistency Regularization (MCR)训练策略，通过扩张和重形两种面具扰动方式，强制学习模型在不同面具下输出一致性。扩张面具帮助对齐周围内容，重形面具破除面具形状偏异。

Result: 实验结果显示MCR能够显著减少幻觉现象和面具形状偏异，提升了物体移除的性能。

Conclusion: MCR通过面具一致性正则化有效解决了图像修复中的关键问题，产生了更稳健和上下文一致的修复结果。

Abstract: Object removal, a challenging task within image inpainting, involves
seamlessly filling the removed region with content that matches the surrounding
context. Despite advancements in diffusion models, current methods still face
two critical challenges. The first is mask hallucination, where the model
generates irrelevant or spurious content inside the masked region, and the
second is mask-shape bias, where the model fills the masked area with an object
that mimics the mask's shape rather than surrounding content. To address these
issues, we propose Mask Consistency Regularization (MCR), a novel training
strategy designed specifically for object removal tasks. During training, our
approach introduces two mask perturbations: dilation and reshape, enforcing
consistency between the outputs of these perturbed branches and the original
mask. The dilated masks help align the model's output with the surrounding
content, while reshaped masks encourage the model to break the mask-shape bias.
This combination of strategies enables MCR to produce more robust and
contextually coherent inpainting results. Our experiments demonstrate that MCR
significantly reduces hallucinations and mask-shape bias, leading to improved
performance in object removal.

</details>


### [99] [MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation](https://arxiv.org/abs/2509.10260)
*Jia Wang,Jie Hu,Xiaoqi Ma,Hanghang Ma,Yanbing Zeng,Xiaoming Wei*

Main category: cs.CV

TL;DR: MagicMirror是一个全面的文本到图像生成伪影评估框架，包含首个大规模人工标注数据集MagicData340K、基于VLM的评估模型MagicAssessor，以及自动化基准MagicBench，揭示了当前顶级T2I模型仍存在严重伪影问题。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成虽然在指令遵循和美学方面取得显著进展，但普遍存在解剖和结构缺陷等物理伪影，严重影响了感知质量并限制了应用。缺乏系统性和细粒度的评估框架来应对这些多样复杂的伪影问题。

Method: 1) 建立生成图像伪影的详细分类法；2) 人工标注340K图像的大规模数据集MagicData340K；3) 训练Vision-Language模型MagicAssessor进行详细评估；4) 设计新颖的数据采样策略和多级奖励系统的GRPO方法；5) 构建自动化基准MagicBench评估T2I模型。

Result: 评估发现即使像GPT-image-1这样的顶级模型也持续存在显著伪影问题，表明伪影减少是未来T2I发展的关键前沿。

Conclusion: MagicMirror框架填补了当前基准测试的空白，为系统评估和减少文本到图像生成中的伪影提供了重要工具，揭示了该领域仍需解决的关键挑战。

Abstract: Text-to-image (T2I) generation has achieved remarkable progress in
instruction following and aesthetics. However, a persistent challenge is the
prevalence of physical artifacts, such as anatomical and structural flaws,
which severely degrade perceptual quality and limit application. Given the
diversity and complexity of these artifacts, a systematic and fine-grained
evaluation framework is required, which is lacking in current benchmarks. To
fill this gap, we introduce MagicMirror, a comprehensive framework for
artifacts assessment. We first establish a detailed taxonomy of generated image
artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the
first human-annotated large-scale dataset of 340K generated images with
fine-grained artifact labels. Building on this dataset, we train MagicAssessor,
a Vision-Language Model (VLM) that provides detailed assessments and
corresponding labels. To overcome challenges like class imbalance and reward
hacking, we design a novel data sampling strategy and a multi-level reward
system for Group Relative Policy Optimization (GRPO). Finally, we leverage
MagicAssessor to construct MagicBench, an automated benchmark for evaluating
the image artifacts of current T2I models. Our evaluation with MagicBench
reveals that despite their widespread adoption, even top-tier models like
GPT-image-1 are consistently plagued by significant artifacts, highlighting
artifact reduction as a critical frontier for future T2I development. Project
page: https://wj-inf.github.io/MagicMirror-page/.

</details>


### [100] [SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion](https://arxiv.org/abs/2509.10266)
*Wenfang Wu,Tingting Yuan,Yupeng Li,Daling Wang,Xiaoming Fu*

Main category: cs.CV

TL;DR: SignClip是一个新的手语翻译框架，通过融合手势和唇部运动特征，并采用分层对比学习来提升翻译准确性，在PHOENIX14T数据集上取得了SOTA效果


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译方法主要关注手势信号，忽略了唇部运动等非手动线索，而这些线索对于区分视觉相似的手势至关重要

Method: 提出SignClip框架，融合空间手势和唇部运动特征，引入分层对比学习框架，实现手语-唇部和视觉-文本模态的多层次对齐

Result: 在PHOENIX14T数据集上，无词汇表设置下BLEU-4从24.32提升到24.71，ROUGE从46.57提升到48.38；在How2Sign数据集上也表现出优越性

Conclusion: 融合手动和非手动线索的手语翻译方法能够显著提升翻译准确性，证明了唇部运动特征在手语理解中的重要性

Abstract: Sign language translation (SLT) aims to translate natural language from sign
language videos, serving as a vital bridge for inclusive communication. While
recent advances leverage powerful visual backbones and large language models,
most approaches mainly focus on manual signals (hand gestures) and tend to
overlook non-manual cues like mouthing. In fact, mouthing conveys essential
linguistic information in sign languages and plays a crucial role in
disambiguating visually similar signs. In this paper, we propose SignClip, a
novel framework to improve the accuracy of sign language translation. It fuses
manual and non-manual cues, specifically spatial gesture and lip movement
features. Besides, SignClip introduces a hierarchical contrastive learning
framework with multi-level alignment objectives, ensuring semantic consistency
across sign-lip and visual-text modalities. Extensive experiments on two
benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our
approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip
surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from
24.32 to 24.71, and ROUGE from 46.57 to 48.38.

</details>


### [101] [Detecting Text Manipulation in Images using Vision Language Models](https://arxiv.org/abs/2509.10278)
*Vidit Vidit,Pavel Korshunov,Amir Mohammadi,Christophe Ecabert,Ketan Kotwal,Sébastien Marcel*

Main category: cs.CV

TL;DR: 这篇论文分析了大型视觉语言模型在文本精编检测中的表现，发现开源模型稍较闭源模型差距显著，且专门的图像精编检测模型在文本检测中普遍存在泛化问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在图像精编检测方面取得了显著成效，但文本精编检测领域研究缺失，需要填补这一知识空白。

Method: 对比分析闭源和开源视觉语言模型在不同文本精编数据集上的表现，并测试专门的图像精编检测模型在文本检测任务上的性能。

Result: 开源模型性能逐渐接近闭源模型（如GPT-4o），但仍有明显差距；专门的图像精编检测模型在文本检测中普遍存在泛化性问题。

Conclusion: 文本精编检测仍面临挑战，需要进一步研究提升模型在真实场景和高效效的文本精编检测能力。

Abstract: Recent works have shown the effectiveness of Large Vision Language Models
(VLMs or LVLMs) in image manipulation detection. However, text manipulation
detection is largely missing in these studies. We bridge this knowledge gap by
analyzing closed- and open-source VLMs on different text manipulation datasets.
Our results suggest that open-source models are getting closer, but still
behind closed-source ones like GPT- 4o. Additionally, we benchmark image
manipulation detection-specific VLMs for text manipulation detection and show
that they suffer from the generalization problem. We benchmark VLMs for
manipulations done on in-the-wild scene texts and on fantasy ID cards, where
the latter mimic a challenging real-world misuse.

</details>


### [102] [MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection](https://arxiv.org/abs/2509.10282)
*Gang Li,Tianjiao Chen,Mingle Zhou,Min Li,Delong Han,Jin Wan*

Main category: cs.CV

TL;DR: MCL-AD是一个新颖的多模态协作学习框架，通过整合点云、RGB图像和文本语义，实现了卓越的零样本3D异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本3D异常检测方法主要关注点云数据，忽略了RGB图像和文本先验等互补模态提供的丰富语义线索。为了解决数据稀缺、隐私和高标注成本等约束场景下的缺陷检测问题，需要充分利用多模态信息。

Method: 提出了多模态提示学习机制（MPLM），通过对象无关的解耦文本提示和多模态对比损失增强模态内表示能力和模态间协作学习。同时设计了协作调制机制（CMM），通过联合调制RGB图像引导和点云引导分支来充分利用点云和RGB图像的互补表示。

Result: 大量实验证明，MCL-AD框架在零样本3D异常检测中实现了最先进的性能。

Conclusion: 该研究通过多模态协作学习成功解决了现有方法仅依赖点云数据的局限性，为3D异常检测提供了更全面的解决方案，在数据稀缺场景下表现出色。

Abstract: Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects
without relying on labeled training data, making it especially valuable in
scenarios constrained by data scarcity, privacy, or high annotation cost.
However, most existing methods focus exclusively on point clouds, neglecting
the rich semantic cues available from complementary modalities such as RGB
images and texts priors. This paper introduces MCL-AD, a novel framework that
leverages multimodal collaboration learning across point clouds, RGB images,
and texts semantics to achieve superior zero-shot 3D anomaly detection.
Specifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that
enhances the intra-modal representation capability and inter-modal
collaborative learning by introducing an object-agnostic decoupled text prompt
and a multimodal contrastive loss. In addition, a collaborative modulation
mechanism (CMM) is proposed to fully leverage the complementary representations
of point clouds and RGB images by jointly modulating the RGB image-guided and
point cloud-guided branches. Extensive experiments demonstrate that the
proposed MCL-AD framework achieves state-of-the-art performance in ZS-3D
anomaly detection.

</details>


### [103] [Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks](https://arxiv.org/abs/2509.10298)
*Laith Nayal,Mahmoud Mousatat,Bader Rasheed*

Main category: cs.CV

TL;DR: 提出基于Lipschitz约束的随机深度方法，通过深度相关的DropPath概率控制网络Lipschitz常数，在保持精度的同时提升对抗鲁棒性并降低计算量


<details>
  <summary>Details</summary>
Motivation: 深度神经网络和Vision Transformers在计算机视觉中表现优异但对对抗扰动高度脆弱，现有防御方法计算成本高或缺乏形式化保证

Method: Lipschitz引导的随机深度(DropPath)方法，drop概率随深度增加以控制网络有效Lipschitz常数，对深层进行正则化

Result: 在CIFAR-10和ViT-Tiny上的实验表明，该方法保持接近基线的干净精度，提升了对FGSM、PGD-20和AutoAttack的鲁棒性，并显著降低了FLOPs

Conclusion: 深度相关的DropPath调度能有效平衡模型精度、鲁棒性和计算效率，为对抗防御提供了新的正则化思路

Abstract: Deep neural networks and Vision Transformers achieve state-of-the-art
performance in computer vision but are highly vulnerable to adversarial
perturbations. Standard defenses often incur high computational cost or lack
formal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath)
method, where drop probabilities increase with depth to control the effective
Lipschitz constant of the network. This approach regularizes deeper layers,
improving robustness while preserving clean accuracy and reducing computation.
Experiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent
schedule maintains near-baseline clean accuracy, enhances robustness under
FGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to
baseline and linear DropPath schedules.

</details>


### [104] [A Stochastic Birth-and-Death Approach for Street Furniture Geolocation in Urban Environments](https://arxiv.org/abs/2509.10310)
*Evan Murphy,Marco Viola,Vladimir A. Krylov*

Main category: cs.CV

TL;DR: 提出基于能量地图的概率框架，用于复杂城市环境中街道家具的精确定位，通过随机生死优化算法整合地理空间信息，提高定位精度


<details>
  <summary>Details</summary>
Motivation: 解决城市街道家具精确定位问题，为地方当局和私人利益相关者提供有效的公共基础设施监控和维护手段

Method: 基于能量地图的概率框架，将空间位置可能性编码为地图格式，结合GIS图层、道路地图等外部地理空间信息，使用随机生死优化算法推断最可能的资产配置

Result: 通过在都柏林市中心街灯基础设施数据集上的真实模拟评估，证明了该方法具有可扩展性和高精度的城市资产映射潜力

Conclusion: 该方法为城市资产映射提供了有效的解决方案，算法实现已在GitHub开源

Abstract: In this paper we address the problem of precise geolocation of street
furniture in complex urban environments, which is a critical task for effective
monitoring and maintenance of public infrastructure by local authorities and
private stakeholders. To this end, we propose a probabilistic framework based
on energy maps that encode the spatial likelihood of object locations.
Representing the energy in a map-based geopositioned format allows the
optimisation process to seamlessly integrate external geospatial information,
such as GIS layers, road maps, or placement constraints, which improves
contextual awareness and localisation accuracy. A stochastic birth-and-death
optimisation algorithm is introduced to infer the most probable configuration
of assets. We evaluate our approach using a realistic simulation informed by a
geolocated dataset of street lighting infrastructure in Dublin city centre,
demonstrating its potential for scalable and accurate urban asset mapping. The
implementation of the algorithm will be made available in the GitHub repository
https://github.com/EMurphy0108/SBD_Street_Furniture.

</details>


### [105] [Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching](https://arxiv.org/abs/2509.10312)
*Zhixin Zheng,Xinyu Wang,Chang Zou,Shaobo Wang,Linfeng Zhang*

Main category: cs.CV

TL;DR: ClusCa通过空间聚类减少扩散变换器中的token数量，实现4.96倍加速，同时保持图像质量


<details>
  <summary>Details</summary>
Motivation: 扩散变换器计算成本高，现有特征缓存方法只利用时间相似性而忽略了空间相似性

Method: 对每个时间步的token进行空间聚类，每个聚类只计算一个token并将其信息传播给其他token，减少90%以上token数量

Result: 在DiT、FLUX和HunyuanVideo上验证有效性，FLUX实现4.96倍加速，ImageReward达到99.49%（比原模型提高0.51%）

Conclusion: ClusCa是一种无需训练即可直接应用于任何扩散变换器的有效加速方法，能够显著降低计算成本同时保持生成质量

Abstract: Diffusion transformers have gained significant attention in recent years for
their ability to generate high-quality images and videos, yet still suffer from
a huge computational cost due to their iterative denoising process. Recently,
feature caching has been introduced to accelerate diffusion transformers by
caching the feature computation in previous timesteps and reusing it in the
following timesteps, which leverage the temporal similarity of diffusion models
while ignoring the similarity in the spatial dimension. In this paper, we
introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and
complementary perspective for previous feature caching. Specifically, ClusCa
performs spatial clustering on tokens in each timestep, computes only one token
in each cluster and propagates their information to all the other tokens, which
is able to reduce the number of tokens by over 90%. Extensive experiments on
DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image
and text-to-video generation. Besides, it can be directly applied to any
diffusion transformer without requirements for training. For instance, ClusCa
achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing
the original model by 0.51%. The code is available at
https://github.com/Shenyi-Z/Cache4Diffusion.

</details>


### [106] [I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation](https://arxiv.org/abs/2509.10334)
*Jordan Sassoon,Michal Szczepanski,Martyna Poreba*

Main category: cs.CV

TL;DR: I-Segmenter是首个完全整数化的ViT分割框架，通过系统替换浮点运算为整数运算，结合新激活函数λ-ShiftGELU，在保持精度的同时显著减少模型大小和加速推理。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在语义分割中表现优异，但在资源受限设备上部署困难，主要由于高内存占用和计算成本。量化是提高效率的有效策略，但ViT分割模型在低精度下表现脆弱。

Method: 基于Segmenter架构，系统替换浮点运算为整数运算；提出λ-ShiftGELU激活函数处理长尾分布；移除L2归一化层；用最近邻上采样替换双线性插值。

Result: I-Segmenter在精度上与FP32基线相差平均5.1%，同时模型大小减少3.8倍，推理速度提升1.2倍。单张校准图像的PTQ也能获得有竞争力的精度。

Conclusion: I-Segmenter是首个完全整数化的ViT分割框架，具有实际部署价值，为资源受限设备上的高效语义分割提供了可行解决方案。

Abstract: Vision Transformers (ViTs) have recently achieved strong results in semantic
segmentation, yet their deployment on resource-constrained devices remains
limited due to their high memory footprint and computational cost. Quantization
offers an effective strategy to improve efficiency, but ViT-based segmentation
models are notoriously fragile under low precision, as quantization errors
accumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the
first fully integer-only ViT segmentation framework. Building on the Segmenter
architecture, I-Segmenter systematically replaces floating-point operations
with integer-only counterparts. To further stabilize both training and
inference, we propose $\lambda$-ShiftGELU, a novel activation function that
mitigates the limitations of uniform quantization in handling long-tailed
activation distributions. In addition, we remove the L2 normalization layer and
replace bilinear interpolation in the decoder with nearest neighbor upsampling,
ensuring integer-only execution throughout the computational graph. Extensive
experiments show that I-Segmenter achieves accuracy within a reasonable margin
of its FP32 baseline (5.1 % on average), while reducing model size by up to
3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,
even in one-shot PTQ with a single calibration image, I-Segmenter delivers
competitive accuracy, underscoring its practicality for real-world deployment.

</details>


### [107] [GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT](https://arxiv.org/abs/2509.10341)
*Botond Fazekas,Thomas Pinetz,Guilherme Aresta,Taha Emre,Hrvoje Bogunovic*

Main category: cs.CV

TL;DR: 提出GARD方法，基于伽马扩散模型和降噪保真项，有效去除OCT图像散斑噪声并保留解剖结构细节


<details>
  <summary>Details</summary>
Motivation: OCT图像存在固有散斑噪声，传统方法难以平衡噪声去除与结构保留，需要更准确的噪声统计建模和更好的去噪指导

Method: 采用Denoising Diffusion Gamma Model替代高斯假设，引入Noise-Reduced Fidelity Term使用预处理图像指导去噪，并基于DDIM框架加速推理

Result: 在配对OCT数据集上，GARD在PSNR、SSIM和MSE指标上显著优于传统方法和先进深度学习模型，定性结果显示边缘更锐利、解剖细节保留更好

Conclusion: GARD通过伽马扩散模型和降噪保真项，成功解决了OCT图像去噪中噪声统计建模不准和细节保留的难题，实现了优异的去噪效果

Abstract: Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing
and monitoring retinal diseases. However, OCT images are inherently degraded by
speckle noise, which obscures fine details and hinders accurate interpretation.
While numerous denoising methods exist, many struggle to balance noise
reduction with the preservation of crucial anatomical structures. This paper
introduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel
deep learning approach for OCT image despeckling that leverages the strengths
of diffusion probabilistic models. Unlike conventional diffusion models that
assume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more
accurately reflect the statistical properties of speckle. Furthermore, we
introduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,
less-noisy image to guide the denoising process. This crucial addition prevents
the reintroduction of high-frequency noise. We accelerate the inference process
by adapting the Denoising Diffusion Implicit Model framework to our Gamma-based
model. Experiments on a dataset with paired noisy and less-noisy OCT B-scans
demonstrate that GARD significantly outperforms traditional denoising methods
and state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.
Qualitative results confirm that GARD produces sharper edges and better
preserves fine anatomical details.

</details>


### [108] [GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography](https://arxiv.org/abs/2509.10344)
*Yuexi Du,Lihui Chen,Nicha C. Dvornek*

Main category: cs.CV

TL;DR: GLAM模型通过几何引导的多视图对齐方法，在乳腺X线摄影视觉语言模型预训练中实现全局和局部对齐，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有乳腺X线摄影VLM从自然图像适配而来，忽略了多视图关系等医学影像特有特征，无法像放射科医生那样同时分析双侧视图，导致几何上下文信息丢失和预测性能不佳

Method: 利用乳腺X线摄影多视图成像过程的先验知识，通过联合全局和局部、视觉-视觉和视觉-语言的对比学习，学习局部跨视图对齐和细粒度局部特征

Result: 在最大的公开乳腺X线摄影数据集EMBED上预训练后，该模型在不同设置下的多个数据集上均优于基线方法

Conclusion: GLAM模型通过几何引导的多视图对齐学习，有效解决了乳腺X线摄影VLM预训练中的领域特异性问题，提升了模型性能

Abstract: Mammography screening is an essential tool for early detection of breast
cancer. The speed and accuracy of mammography interpretation have the potential
to be improved with deep learning methods. However, the development of a
foundation visual language model (VLM) is hindered by limited data and domain
differences between natural and medical images. Existing mammography VLMs,
adapted from natural images, often ignore domain-specific characteristics, such
as multi-view relationships in mammography. Unlike radiologists who analyze
both views together to process ipsilateral correspondence, current methods
treat them as independent images or do not properly model the multi-view
correspondence learning, losing critical geometric context and resulting in
suboptimal prediction. We propose GLAM: Global and Local Alignment for
Multi-view mammography for VLM pretraining using geometry guidance. By
leveraging the prior knowledge about the multi-view imaging process of
mammograms, our model learns local cross-view alignments and fine-grained local
features through joint global and local, visual-visual, and visual-language
contrastive learning. Pretrained on EMBED [14], one of the largest open
mammography datasets, our model outperforms baselines across multiple datasets
under different settings.

</details>


### [109] [Towards Understanding Visual Grounding in Visual Language Models](https://arxiv.org/abs/2509.10345)
*Georgios Pantazopoulos,Eda B. Özyiğit*

Main category: cs.CV

TL;DR: 这是一份关于视觉基础化的综述性论文，审视了现代通用视觉语言模型的核心组成、应用场景、评估指标以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 视视基础化能力对于模型在多领域的应用至关重要，包括指代表达理解、细粒度图像问答、实体参考描述以及环境控制等。需要系统性总结现有研究进展。

Method: 综述性研究方法：首先论述基础化的重要性，描述现代基础模型的核心组成部分，分析实际应用场景，考察评测指标体系，并探讨视觉基础化与多模态链式思绪、推理能力的关联性。

Result: 本文对视觉基础化领域进行了系统性的归纳和分析，提供了完整的研究框架，包括理论基础、技术方法、应用场景和评估体系。同时识别了该领域面临的核心挑战。

Conclusion: 视觉基础化是通用视觉语言模型的关键技术，具有广泛的应用前景。本综述为该领域的研究提供了结构化的分析框架，并指出了未来研究的潜在方向，对于推动视视基础化技术的发展具有重要意义。

Abstract: Visual grounding refers to the ability of a model to identify a region within
some visual input that matches a textual description. Consequently, a model
equipped with visual grounding capabilities can target a wide range of
applications in various domains, including referring expression comprehension,
answering questions pertinent to fine-grained details in images or videos,
caption visual context by explicitly referring to entities, as well as low and
high-level control in simulated and real environments. In this survey paper, we
review representative works across the key areas of research on modern
general-purpose vision language models (VLMs). We first outline the importance
of grounding in VLMs, then delineate the core components of the contemporary
paradigm for developing grounded models, and examine their practical
applications, including benchmarks and evaluation metrics for grounded
multimodal generation. We also discuss the multifaceted interrelations among
visual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,
we analyse the challenges inherent to visual grounding and suggest promising
directions for future research.

</details>


### [110] [Immunizing Images from Text to Image Editing via Adversarial Cross-Attention](https://arxiv.org/abs/2509.10359)
*Matteo Trippodo,Federico Becattini,Lorenzo Seidenari*

Main category: cs.CV

TL;DR: 通过自动生成的图片描述作为代理提示，攻击文本-图像编辑方法的跨注意力机制，破坏图像内容与文本描述的对齐性


<details>
  <summary>Details</summary>
Motivation: 现有的文本基于图像编辑方法容易受到对手攻击，需要一种新的攻击方法来干扰其视觉组件

Method: 提出Attention Attack，利用源图像自动生成的标题作为编辑提示的代理，打断文本提示与图像表征之间的跨注意力机制

Result: 在TEDBench++基准上进行实验，证明该攻出显著降低了编辑性能但保持不可见性

Conclusion: 该攻击方法无需知道编辑方法或编辑提示的具体信息，并提出了两种新的评估策略来量化攻击效果

Abstract: Recent advances in text-based image editing have enabled fine-grained
manipulation of visual content guided by natural language. However, such
methods are susceptible to adversarial attacks. In this work, we propose a
novel attack that targets the visual component of editing methods. We introduce
Attention Attack, which disrupts the cross-attention between a textual prompt
and the visual representation of the image by using an automatically generated
caption of the source image as a proxy for the edit prompt. This breaks the
alignment between the contents of the image and their textual description,
without requiring knowledge of the editing method or the editing prompt.
Reflecting on the reliability of existing metrics for immunization success, we
propose two novel evaluation strategies: Caption Similarity, which quantifies
semantic consistency between original and adversarial edits, and semantic
Intersection over Union (IoU), which measures spatial layout disruption via
segmentation masks. Experiments conducted on the TEDBench++ benchmark
demonstrate that our attack significantly degrades editing performance while
remaining imperceptible.

</details>


### [111] [Efficient Learned Image Compression Through Knowledge Distillation](https://arxiv.org/abs/2509.10366)
*Fabien Allemand,Attilio Fiandrotti,Sumanta Chaudhuri,Alaa Eddine Mazouz*

Main category: cs.CV

TL;DR: 该研究通过知识蒸馏技术降低神经网络图像压缩模型的资源需求，使小型网络能够在大模型指导下获得更好的性能，适用于不同架构大小和质量/比特率权衡。


<details>
  <summary>Details</summary>
Motivation: 虽然基于神经网络的图像压缩方法在性能上优于传统编解码器，但需要大量计算资源，难以在资源受限平台上实时应用，这限制了其在实际部署中的使用。

Method: 采用知识蒸馏训练范式，让较小的神经网络部分基于大型复杂模型的输出进行训练，从而在保持性能的同时显著降低处理能力和能耗需求。

Result: 研究表明知识蒸馏可有效应用于图像压缩任务，适用于不同架构规模，能够实现不同的图像质量/比特率权衡，并显著节省处理和能源资源。

Conclusion: 知识蒸馏为神经网络图像压缩提供了有效的资源优化方案，未来可探索不同教师模型和损失函数的影响，并扩展到基于Transformer的模型。

Abstract: Learned image compression sits at the intersection of machine learning and
image processing. With advances in deep learning, neural network-based
compression methods have emerged. In this process, an encoder maps the image to
a low-dimensional latent space, which is then quantized, entropy-coded into a
binary bitstream, and transmitted to the receiver. At the receiver end, the
bitstream is entropy-decoded, and a decoder reconstructs an approximation of
the original image. Recent research suggests that these models consistently
outperform conventional codecs. However, they require significant processing
power, making them unsuitable for real-time use on resource-constrained
platforms, which hinders their deployment in mainstream applications. This
study aims to reduce the resource requirements of neural networks used for
image compression by leveraging knowledge distillation, a training paradigm
where smaller neural networks, partially trained on the outputs of larger, more
complex models, can achieve better performance than when trained independently.
Our work demonstrates that knowledge distillation can be effectively applied to
image compression tasks: i) across various architecture sizes, ii) to achieve
different image quality/bit rate tradeoffs, and iii) to save processing and
energy resources. This approach introduces new settings and hyperparameters,
and future research could explore the impact of different teacher models, as
well as alternative loss functions. Knowledge distillation could also be
extended to transformer-based models. The code is publicly available at:
https://github.com/FABallemand/PRIM .

</details>


### [112] [Ordinality of Visible-Thermal Image Intensities for Intrinsic Image Decomposition](https://arxiv.org/abs/2509.10388)
*Zeqing Leo Yuan,Mani Ramanagopal,Aswin C. Sankaranarayanan,Srinivasa G. Narasimhan*

Main category: cs.CV

TL;DR: 一种无需训练的新方法，利用可见光和热成像国对来实现图像本质分解，通过热量吸收原理推断照明和反射率的顺序关系


<details>
  <summary>Details</summary>
Motivation: 图像本质分解（照明和反射率分离）面临真实场景缺乏大量真实标签数据的挑战，现有方法依赖合成数据或稀疏标注

Method: 利用可见光和热成像国对，基于光线被不透明表面吸收转化为热量的原理，通过分析图像强度的顺序关系来自监督优化神经网络，恢复照明和反射率

Result: 在自然光和人造光照明条件下进行了定量评估，并在多样化户外场景中进行了定性实验，结果显示性能超过最近的学习基于模型

Conclusion: 该方法为获取真实世界的顺序监督数据提供了可扩展的路径，而这之前通过手动标注是不可行的

Abstract: Decomposing an image into its intrinsic photometric factors--shading and
reflectance--is a long-standing challenge due to the lack of extensive
ground-truth data for real-world scenes. Recent methods rely on synthetic data
or sparse annotations for limited indoor and even fewer outdoor scenes. We
introduce a novel training-free approach for intrinsic image decomposition
using only a pair of visible and thermal images. We leverage the principle that
light not reflected from an opaque surface is absorbed and detected as heat by
a thermal camera. This allows us to relate the ordinalities between visible and
thermal image intensities to the ordinalities of shading and reflectance, which
can densely self-supervise an optimizing neural network to recover shading and
reflectance. We perform quantitative evaluations with known reflectance and
shading under natural and artificial lighting, and qualitative experiments
across diverse outdoor scenes. The results demonstrate superior performance
over recent learning-based models and point toward a scalable path to curating
real-world ordinal supervision, previously infeasible via manual labeling.

</details>


### [113] [Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards](https://arxiv.org/abs/2509.10407)
*Xiem HoangVan,Dang BuiDinh,Sang NguyenQuang,Wen-Hsiao Peng*

Main category: cs.CV

TL;DR: 本文提出了压缩视频质量增强(CVQE)的系统性综述，包括新的分类法、统一基准测试框架和性能-复杂度权衡分析，以解决现有调查的不足。


<details>
  <summary>Details</summary>
Motivation: 现有CVQE调查存在系统性分类不足、架构范式比较分析不够、基准测试实践不完善等问题，需要建立更全面的评估框架。

Method: 提出三方面贡献：1)基于架构范式、编码标准和压缩域特征利用的新分类法；2)集成现代压缩协议和标准测试序列的统一基准测试框架；3)系统分析重建性能与计算复杂度的权衡关系。

Result: 建立了CVQE方法的系统性分类体系，提供了公平的多标准评估框架，揭示了现有方法的性能-复杂度权衡规律。

Conclusion: 该综述为CVQE研究和部署提供了一致的评估基础和明智的模型选择指南，指明了未来研究的 promising 方向。

Abstract: Compressed video quality enhancement (CVQE) is crucial for improving user
experience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.
While deep learning based CVQE has driven significant progress, existing
surveys still suffer from limitations: lack of systematic classification
linking methods to specific standards and artifacts, insufficient comparative
analysis of architectural paradigms across coding types, and underdeveloped
benchmarking practices. To address these gaps, this paper presents three key
contributions. First, it introduces a novel taxonomy classifying CVQE methods
across architectural paradigms, coding standards, and compressed-domain feature
utilization. Second, it proposes a unified benchmarking framework integrating
modern compression protocols and standard test sequences for fair
multi-criteria evaluation. Third, it provides a systematic analysis of the
critical trade-offs between reconstruction performance and computational
complexity observed in state-of-the-art methods and highlighting promising
directions for future research. This comprehensive review aims to establish a
foundation for consistent assessment and informed model selection in CVQE
research and deployment.

</details>


### [114] [Multimodal SAM-adapter for Semantic Segmentation](https://arxiv.org/abs/2509.10408)
*Iacopo Curti,Pierluigi Zama Ramirez,Alioscia Petrelli,Luigi Di Stefano*

Main category: cs.CV

TL;DR: MM SAM-adapter是一个新颖的多模态语义分割框架，通过适配器网络将融合的多模态特征注入到SAM的RGB特征中，在保持RGB特征强泛化能力的同时选择性利用辅助模态信息，在多个挑战性基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前语义分割方法在恶劣光照、遮挡和恶劣天气等挑战性条件下表现脆弱，需要多模态方法来整合辅助传感器数据（如LiDAR、红外）以提供互补信息并增强鲁棒性。

Method: 提出MM SAM-adapter框架，使用适配器网络将融合的多模态特征注入到Segment Anything Model (SAM)的丰富RGB特征中，实现多模态信息的平衡高效利用。

Result: 在DeLiVER、FMB和MUSES三个挑战性基准测试中实现了最先进的性能。在RGB-easy和RGB-hard子集上的结果一致表明，该框架在有利和不利条件下都优于竞争方法。

Conclusion: 多模态适配对于鲁棒场景理解非常有效，MM SAM-adapter框架成功扩展了SAM的能力，实现了多模态语义分割的优异性能。

Abstract: Semantic segmentation, a key task in computer vision with broad applications
in autonomous driving, medical imaging, and robotics, has advanced
substantially with deep learning. Nevertheless, current approaches remain
vulnerable to challenging conditions such as poor lighting, occlusions, and
adverse weather. To address these limitations, multimodal methods that
integrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,
providing complementary information that enhances robustness. In this work, we
present MM SAM-adapter, a novel framework that extends the capabilities of the
Segment Anything Model (SAM) for multimodal semantic segmentation. The proposed
method employs an adapter network that injects fused multimodal features into
SAM's rich RGB features. This design enables the model to retain the strong
generalization ability of RGB features while selectively incorporating
auxiliary modalities only when they contribute additional cues. As a result, MM
SAM-adapter achieves a balanced and efficient use of multimodal information. We
evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,
where MM SAM-adapter delivers state-of-the-art performance. To further analyze
modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard
subsets. Results consistently demonstrate that our framework outperforms
competing methods in both favorable and adverse conditions, highlighting the
effectiveness of multimodal adaptation for robust scene understanding. The code
is available at the following link:
https://github.com/iacopo97/Multimodal-SAM-Adapter.

</details>


### [115] [InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis](https://arxiv.org/abs/2509.10441)
*Tao Han,Wanghan Xu,Junchao Gong,Xiaoyu Yue,Song Guo,Luping Zhou,Lei Bai*

Main category: cs.CV

TL;DR: InfGen是一种新的图像生成方法，通过将扩散模型生成的固定潜在表示作为内容表征，使用一步生成器解码任意分辨率图像，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决当前扩散模型在生成高分辨率图像时计算需求呈二次增长的问题，4K图像生成延迟超过100秒，需要更高效的任意分辨率图像生成方案。

Method: 基于潜在扩散模型的第二代方法，将扩散模型生成的固定潜在表示作为内容表征，使用紧凑的一步生成器替换VAE解码器，无需重新训练扩散模型即可生成任意分辨率图像。

Result: 实验表明InfGen能够将4K图像生成时间从100多秒缩短到10秒以内，同时可以将多种模型升级到任意高分辨率生成时代。

Conclusion: InfGen提供了一种简单高效的解决方案，显著降低了计算复杂度，适用于使用相同潜在空间的任何模型，实现了任意分辨率图像生成的突破。

Abstract: Arbitrary resolution image generation provides a consistent visual experience
across devices, having extensive applications for producers and consumers.
Current diffusion models increase computational demand quadratically with
resolution, causing 4K image generation delays over 100 seconds. To solve this,
we explore the second generation upon the latent diffusion models, where the
fixed latent generated by diffusion models is regarded as the content
representation and we propose to decode arbitrary resolution images with a
compact generated latent using a one-step generator. Thus, we present the
\textbf{InfGen}, replacing the VAE decoder with the new generator, for
generating images at any resolution from a fixed-size latent without retraining
the diffusion models, which simplifies the process, reducing computational
complexity and can be applied to any model using the same latent space.
Experiments show InfGen is capable of improving many models into the arbitrary
high-resolution era while cutting 4K image generation time to under 10 seconds.

</details>


### [116] [SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets](https://arxiv.org/abs/2509.10453)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: 本研究将三种先进的时序自监督学习方法应用于3D脑部MRI分析，通过处理变长输入和增强空间特征学习，在阿尔茨海默病预测任务中优于监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病预测的深度学习模型面临标注数据稀缺、跨数据集泛化能力差、以及对不同扫描数量和间隔时间适应性不足的问题。

Method: 采用时序自监督学习（SSL）方法，包括时序顺序预测和对比学习，处理变长输入并学习鲁棒的空间特征，使用四个公开数据集（3,161名患者）进行预训练。

Result: SSL模型在七个下游任务中的六个任务上表现优于监督学习，展示了跨任务和不同输入图像数量的适应性和泛化能力。

Conclusion: 时序自监督学习方法在阿尔茨海默病预测中表现出色，具有强大的临床应用潜力，代码和模型已公开。

Abstract: Alzheimer's disease is a progressive, neurodegenerative disorder that causes
memory loss and cognitive decline. While there has been extensive research in
applying deep learning models to Alzheimer's prediction tasks, these models
remain limited by lack of available labeled data, poor generalization across
datasets, and inflexibility to varying numbers of input scans and time
intervals between scans. In this study, we adapt three state-of-the-art
temporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,
and add novel extensions designed to handle variable-length inputs and learn
robust spatial features. We aggregate four publicly available datasets
comprising 3,161 patients for pre-training, and show the performance of our
model across multiple Alzheimer's prediction tasks including diagnosis
classification, conversion detection, and future conversion prediction.
Importantly, our SSL model implemented with temporal order prediction and
contrastive learning outperforms supervised learning on six out of seven
downstream tasks. It demonstrates adaptability and generalizability across
tasks and number of input images with varying time intervals, highlighting its
capacity for robust performance across clinical applications. We release our
code and model publicly at https://github.com/emilykaczmarek/SSL-AD.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [117] [Human-AI Collaboration Increases Efficiency in Regulatory Writing](https://arxiv.org/abs/2509.09738)
*Umut Eser,Yael Gozin,L. Jay Stallons,Ari Caroline,Martin Preusse,Brandon Rice,Scott Wright,Andrew Robertson*

Main category: cs.AI

TL;DR: AutoIND LLM平台可将IND申请文件起草时间减少约97%，从约100小时缩短至3-4小时，质量评分达70-78%，无关键监管错误，但仍需专家完善以提高质量。


<details>
  <summary>Details</summary>
Motivation: IND申请准备耗时且依赖专业知识，拖慢早期临床开发进程，需要寻找自动化解决方案来加速监管文件起草。

Method: 使用AutoIND LLM平台生成IND非临床书面总结(eCTD模块2.6.2, 2.6.4, 2.6.6)，记录起草时间并与人工起草时间对比，由盲评监管写作评估员从正确性、完整性等7个维度评估质量。

Result: 起草时间减少97%(从~100小时降至3.7小时和2.6小时)，质量评分分别为69.6%和77.9%，无关键监管错误，但在重点突出、简洁性和清晰度方面存在不足。

Conclusion: AutoIND能显著加速IND起草，但专家监管写作者仍需完善输出质量至可提交水平，发现的系统性缺陷为模型针对性改进提供了路线图。

Abstract: Background: Investigational New Drug (IND) application preparation is
time-intensive and expertise-dependent, slowing early clinical development.
Objective: To evaluate whether a large language model (LLM) platform (AutoIND)
can reduce first-draft composition time while maintaining document quality in
regulatory submissions. Methods: Drafting times for IND nonclinical written
summaries (eCTD modules 2.6.2, 2.6.4, 2.6.6) generated by AutoIND were directly
recorded. For comparison, manual drafting times for IND summaries previously
cleared by the U.S. FDA were estimated from the experience of regulatory
writers ($\geq$6 years) and used as industry-standard benchmarks. Quality was
assessed by a blinded regulatory writing assessor using seven pre-specified
categories: correctness, completeness, conciseness, consistency, clarity,
redundancy, and emphasis. Each sub-criterion was scored 0-3 and normalized to a
percentage. A critical regulatory error was defined as any misrepresentation or
omission likely to alter regulatory interpretation (e.g., incorrect NOAEL,
omission of mandatory GLP dose-formulation analysis). Results: AutoIND reduced
initial drafting time by $\sim$97% (from $\sim$100 h to 3.7 h for 18,870
pages/61 reports in IND-1; and to 2.6 h for 11,425 pages/58 reports in IND-2).
Quality scores were 69.6\% and 77.9\% for IND-1 and IND-2. No critical
regulatory errors were detected, but deficiencies in emphasis, conciseness, and
clarity were noted. Conclusions: AutoIND can dramatically accelerate IND
drafting, but expert regulatory writers remain essential to mature outputs to
submission-ready quality. Systematic deficiencies identified provide a roadmap
for targeted model improvements.

</details>


### [118] [Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture](https://arxiv.org/abs/2509.09775)
*Aleksandr Boldachev*

Main category: cs.AI

TL;DR: boldsea是一个基于语义事件的可执行本体架构，用于建模复杂动态系统，通过语义模型直接控制流程执行，解决了传统BPM系统和面向对象语义技术的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决传统业务流程管理系统和面向对象语义技术在动态系统建模中的局限性，提供更灵活、实时的流程控制和语义集成。

Method: 提出boldsea架构，包括正式的BSL语义语言（含BNF语法）和boldsea-engine引擎，直接解释语义模型为可执行算法，无需编译。

Result: 实现了运行时事件模型修改、时间透明性，并在统一语义框架内无缝融合数据和业务逻辑。

Conclusion: boldsea架构通过语义事件方法有效解决了复杂动态系统建模的挑战，为业务流程管理提供了更灵活和实时的解决方案。

Abstract: This paper presents boldsea, Boldachev's semantic-event approach -- an
architecture for modeling complex dynamic systems using executable ontologies
-- semantic models that act as dynamic structures, directly controlling process
execution. We demonstrate that integrating event semantics with a dataflow
architecture addresses the limitations of traditional Business Process
Management (BPM) systems and object-oriented semantic technologies. The paper
presents the formal BSL (boldsea Semantic Language), including its BNF grammar,
and outlines the boldsea-engine's architecture, which directly interprets
semantic models as executable algorithms without compilation. It enables the
modification of event models at runtime, ensures temporal transparency, and
seamlessly merges data and business logic within a unified semantic framework.

</details>


### [119] [How well can LLMs provide planning feedback in grounded environments?](https://arxiv.org/abs/2509.09790)
*Yuxuan Li,Victor Zhong*

Main category: cs.AI

TL;DR: 大型基础模型能够在多种环境中提供高质量的规划反馈，减少对奖励设计和示范数据的依赖


<details>
  <summary>Details</summary>
Motivation: 解决基于环境的规划学习通常需要精心设计奖励函数或高质量注释示范的问题，而领先训练的基础模型可能提供有用的背景知识

Method: 评估LLM和VLM在符号、语言和连续控制环境中提供反馈的能力，包括二进制反馈、偏好反馈、动作建议、目标建议和差分动作反馈，以及各种推理方法

Result: 基础模型能够提供多样化高质量反馈，更大、更理性的模型反馈更准确、偏见更少，从增强推理方法中获益更多

Conclusion: 基础模型是有效的规划反馈提供者，但反馈质量在复杂动态或连续状态、动作空间的环境中会降低

Abstract: Learning to plan in grounded environments typically requires carefully
designed reward functions or high-quality annotated demonstrations. Recent
works show that pretrained foundation models, such as large language models
(LLMs) and vision language models (VLMs), capture background knowledge helpful
for planning, which reduces the amount of reward design and demonstrations
needed for policy learning. We evaluate how well LLMs and VLMs provide feedback
across symbolic, language, and continuous control environments. We consider
prominent types of feedback for planning including binary feedback, preference
feedback, action advising, goal advising, and delta action feedback. We also
consider inference methods that impact feedback performance, including
in-context learning, chain-of-thought, and access to environment dynamics. We
find that foundation models can provide diverse high-quality feedback across
domains. Moreover, larger and reasoning models consistently provide more
accurate feedback, exhibit less bias, and benefit more from enhanced inference
methods. Finally, feedback quality degrades for environments with complex
dynamics or continuous state spaces and action spaces.

</details>


### [120] [A Modular and Multimodal Generative AI Framework for Urban Building Energy Data: Generating Synthetic Homes](https://arxiv.org/abs/2509.09794)
*Jackson Eshbaugh,Chetan Tiwari,Jorge Silveyra*

Main category: cs.AI

TL;DR: 提出模块化多模态框架，利用生成式AI从公开住宅信息和图像生成能源建模所需数据，解决数据获取难题


<details>
  <summary>Details</summary>
Motivation: 计算模型需要大量数据，但一些数据难以获取、昂贵或存在隐私问题，需要替代方案

Method: 开发模块化多模态框架，使用生成式AI从公开住宅信息和图像生成所需数据，并提供完整实现管道

Result: 实验表明该框架能避免生成模型的常见问题，产生真实、标注良好的数据

Conclusion: 通过减少对昂贵或受限数据源的依赖，为更易获取和可重复的研究铺平道路

Abstract: Computational models have emerged as powerful tools for energy modeling
research, touting scalability and quantitative results. However, these models
require a plethora of data, some of which is inaccessible, expensive, or raises
privacy concerns. We introduce a modular multimodal framework to produce this
data from publicly accessible residential information and images using
generative artificial intelligence (AI). Additionally, we provide a pipeline
demonstrating this framework, and we evaluate its generative AI components. Our
experiments show that our framework's use of AI avoids common issues with
generative models. Our framework produces realistic, labeled data. By reducing
dependence on costly or restricted data sources, we pave a path towards more
accessible and reproducible research.

</details>


### [121] [Towards a Common Framework for Autoformalization](https://arxiv.org/abs/2509.09810)
*Agnieszka Mensfelt,David Tena Cucala,Santiago Franco,Angeliki Koutsoukou-Argyraki,Vince Trencsenyi,Kostas Stathis*

Main category: cs.AI

TL;DR: 自动形式化是指将非形式输入翻译为形式逻辑表示的过程，包括数学式化和更广泛的知识表示任务。近期深度学习和大语言模型的进步促进了该领域发展，但各研究领域相对孤立开发，需要统一框架来加速进步。


<details>
  <summary>Details</summary>
Motivation: 自动形式化领域在数学式化和更广泛的形式表示任务中得到迅速发展，但各研究领域相对孤立，缺乏共享方法、标准测试集和理论框架。需要统一框架来促进不同领域间的交叉融合，以推动下一代AI系统的发展。

Method: 本文将回顾明显或隐含的自动形式化实例，并提出一个统一的框架。通过综合分析不同研究领域的方法和技术，以促进跨领域知识的交流与融合。

Result: 文章建立了一个统一的自动形式化框架，将数学式化和其他形式表示任务统一在同一个概念下。这为不同研究领域提供了共享方法、标准测试集和理论基础的机会，有助于加速整个领域的进步。

Conclusion: 自动形式化作为一个统一的框架，可以促进不同研究领域间的知识交流和技术融合，从而推动下一代AI系统的发展。该框架有助于解决目前各领域相对孤立开发的问题，为进一步研究提供了基础。

Abstract: Autoformalization has emerged as a term referring to the automation of
formalization - specifically, the formalization of mathematics using
interactive theorem provers (proof assistants). Its rapid development has been
driven by progress in deep learning, especially large language models (LLMs).
More recently, the term has expanded beyond mathematics to describe the broader
task of translating informal input into formal logical representations. At the
same time, a growing body of research explores using LLMs to translate informal
language into formal representations for reasoning, planning, and knowledge
representation - often without explicitly referring to this process as
autoformalization. As a result, despite addressing similar tasks, the largely
independent development of these research areas has limited opportunities for
shared methodologies, benchmarks, and theoretical frameworks that could
accelerate progress. The goal of this paper is to review - explicit or implicit
- instances of what can be considered autoformalization and to propose a
unified framework, encouraging cross-pollination between different fields to
advance the development of next generation AI systems.

</details>


### [122] [Towards an AI-based knowledge assistant for goat farmers based on Retrieval-Augmented Generation](https://arxiv.org/abs/2509.09848)
*Nana Han,Dong Liu,Tomas Norton*

Main category: cs.AI

TL;DR: 本研究开发了一个基于RAG的智能知识助手系统，专门用于山羊养殖健康管理，通过表格文本化和决策树文本化方法融合异构知识，在验证集和测试集上分别达到87.90%和84.22%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在畜牧业应用受限，主要受限于知识源的可用性、多样性和复杂性，特别是在山羊养殖健康管理领域缺乏专门的知识支持系统。

Method: 采用检索增强生成(RAG)技术，提出表格文本化和决策树文本化两种结构化知识处理方法，建立覆盖疾病防治、营养管理、饲养管理等五个关键领域的山羊养殖知识库，并集成在线搜索模块实现实时信息检索。

Result: 异构知识融合方法取得最佳效果，验证集平均准确率87.90%，测试集84.22%。在文本、表格、决策树三类问答任务中准确率均超过85%，证明了模块化设计中结构化知识融合的有效性。

Conclusion: 该系统展现了在实际山羊养殖应用中的鲁棒性和可靠性，错误分析表明遗漏是主要错误类型，为进一步提高检索覆盖率和上下文整合提供了改进方向。

Abstract: Large language models (LLMs) are increasingly being recognised as valuable
knowledge communication tools in many industries. However, their application in
livestock farming remains limited, being constrained by several factors not
least the availability, diversity and complexity of knowledge sources. This
study introduces an intelligent knowledge assistant system designed to support
health management in farmed goats. Leveraging the Retrieval-Augmented
Generation (RAG), two structured knowledge processing methods, table
textualization and decision-tree textualization, were proposed to enhance large
language models' (LLMs) understanding of heterogeneous data formats. Based on
these methods, a domain-specific goat farming knowledge base was established to
improve LLM's capacity for cross-scenario generalization. The knowledge base
spans five key domains: Disease Prevention and Treatment, Nutrition Management,
Rearing Management, Goat Milk Management, and Basic Farming Knowledge.
Additionally, an online search module is integrated to enable real-time
retrieval of up-to-date information. To evaluate system performance, six
ablation experiments were conducted to examine the contribution of each
component. The results demonstrated that heterogeneous knowledge fusion method
achieved the best results, with mean accuracies of 87.90% on the validation set
and 84.22% on the test set. Across the text-based, table-based, decision-tree
based Q&A tasks, accuracy consistently exceeded 85%, validating the
effectiveness of structured knowledge fusion within a modular design. Error
analysis identified omission as the predominant error category, highlighting
opportunities to further improve retrieval coverage and context integration. In
conclusion, the results highlight the robustness and reliability of the
proposed system for practical applications in goat farming.

</details>


### [123] [LLMs as Agentic Cooperative Players in Multiplayer UNO](https://arxiv.org/abs/2509.09867)
*Yago Romano Matinez,Jesse Roberts*

Main category: cs.AI

TL;DR: 本文研究了LLM在UNO游戏中作为助手的能力，发现虽然所有模型都能超越随机基准表现，但只有少数模型能显著帮助其他玩家获胜。


<details>
  <summary>Details</summary>
Motivation: 测试基于大语言模型的智能体是否能作为主动参与者帮助人类实现目标，特别是在UNO游戏中协助其他玩家获胜。

Method: 构建工具让仅解码器LLM在RLCard游戏环境中作为智能体参与，接收完整游戏状态信息，使用两种不同的提示策略进行文本响应，评估从1B到70B参数的不同规模模型。

Result: 所有模型在玩UNO时都能成功超越随机基准表现，但只有少数模型能够显著帮助其他玩家获胜。

Conclusion: 模型规模对性能有影响，但即使是大模型也难以有效充当游戏中的助手角色，表明LLM在主动协助方面的能力仍有局限。

Abstract: LLMs promise to assist humans -- not just by answering questions, but by
offering useful guidance across a wide range of tasks. But how far does that
assistance go? Can a large language model based agent actually help someone
accomplish their goal as an active participant? We test this question by
engaging an LLM in UNO, a turn-based card game, asking it not to win but
instead help another player to do so. We built a tool that allows decoder-only
LLMs to participate as agents within the RLCard game environment. These models
receive full game-state information and respond using simple text prompts under
two distinct prompting strategies. We evaluate models ranging from small (1B
parameters) to large (70B parameters) and explore how model scale impacts
performance. We find that while all models were able to successfully outperform
a random baseline when playing UNO, few were able to significantly aid another
player.

</details>


### [124] [The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards Autonomous Science](https://arxiv.org/abs/2509.09915)
*Woong Shin,Renan Souza,Daniel Rosendo,Frédéric Suter,Feiyi Wang,Prasanna Balaprakash,Rafael Ferreira da Silva*

Main category: cs.AI

TL;DR: 提出了一个从当前工作流管理系统向完全自主分布式科学实验室演化的概念框架，通过智能化和群体化两个维度来加速科学发现


<details>
  <summary>Details</summary>
Motivation: 现代科学发现需要协调分布式设施和异构资源，研究人员被迫成为手动工作流协调员而非科学家，AI智能体技术为加速科学发现提供了新机遇

Method: 提出了一个概念框架，工作流沿着智能（从静态到智能）和组合（从单一到群体）两个维度演化，并提供了架构蓝图

Result: 框架为社区提供了向自主科学发展下一步的指导，有望实现100倍发现加速和变革性科学工作流

Conclusion: 该框架为利用AI智能体技术实现自主科学提供了演化路径和架构指导，具有加速科学发现的巨大潜力

Abstract: Modern scientific discovery increasingly requires coordinating distributed
facilities and heterogeneous resources, forcing researchers to act as manual
workflow coordinators rather than scientists. Advances in AI leading to AI
agents show exciting new opportunities that can accelerate scientific discovery
by providing intelligence as a component in the ecosystem. However, it is
unclear how this new capability would materialize and integrate in the real
world. To address this, we propose a conceptual framework where workflows
evolve along two dimensions which are intelligence (from static to intelligent)
and composition (from single to swarm) to chart an evolutionary path from
current workflow management systems to fully autonomous, distributed scientific
laboratories. With these trajectories in mind, we present an architectural
blueprint that can help the community take the next steps towards harnessing
the opportunities in autonomous science with the potential for 100x discovery
acceleration and transformational scientific workflows.

</details>


### [125] [A Markovian Framing of WaveFunctionCollapse for Procedurally Generating Aesthetically Complex Environments](https://arxiv.org/abs/2509.09919)
*Franklin Yiu,Mohan Lu,Nina Li,Kevin Joseph,Tianxu Zhang,Julian Togelius,Timothy Merino,Sam Earle*

Main category: cs.AI

TL;DR: 将WaveFunctionCollapse重新表述为马尔可夫决策过程，实现约束满足与目标优化的解耦，相比传统联合优化方法在复杂任务中表现更优


<details>
  <summary>Details</summary>
Motivation: 解决程序化内容生成中需要同时满足设计者指定目标和瓦片集隐含邻接约束的挑战，传统联合优化方法在任务复杂度增加时表现不佳

Method: 将WaveFunctionCollapse重新表述为马尔可夫决策过程(MDP)，利用WFC的传播机制强制执行约束满足，让外部优化算法专注于目标最大化

Result: 在多个不同难度的领域中，联合优化方法随着任务复杂度增加而表现下降，且始终不如基于WFC-MDP的优化方法

Conclusion: 将局部约束满足与全局目标优化解耦的方法具有明显优势，WFC-MDP框架能够更有效地处理程序化内容生成中的复杂约束优化问题

Abstract: Procedural content generation often requires satisfying both
designer-specified objectives and adjacency constraints implicitly imposed by
the underlying tile set. To address the challenges of jointly optimizing both
constraints and objectives, we reformulate WaveFunctionCollapse (WFC) as a
Markov Decision Process (MDP), enabling external optimization algorithms to
focus exclusively on objective maximization while leveraging WFC's propagation
mechanism to enforce constraint satisfaction. We empirically compare optimizing
this MDP to traditional evolutionary approaches that jointly optimize global
metrics and local tile placement. Across multiple domains with various
difficulties, we find that joint optimization not only struggles as task
complexity increases, but consistently underperforms relative to optimization
over the WFC-MDP, underscoring the advantages of decoupling local constraint
satisfaction from global objective optimization.

</details>


### [126] [Evaluation of Black-Box XAI Approaches for Predictors of Values of Boolean Formulae](https://arxiv.org/abs/2509.09982)
*Stav Armoni-Friedmann,Hana Chockler,David A. Kelly*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于实际因果关系的形式化量度来评估可解释人工智能(XAI)方法，并介绍了新的XAI工具B-ReX，证明其在布尔函数预测任务上优于其他黑盒XAI工具。


<details>
  <summary>Details</summary>
Motivation: 评估可解释人工智能(XAI)方法面临解释主观性的挑战，需要更为形式化和精确的评估方法。论文采用表格数据和布尔函数预测的具体用例来解决这一问题。

Method: 提出基于实际因果关系的形式化变量重要性测量方法，并用其评估现有的XAI工具。同时提出新的XAI工具B-ReX（基于ReX工具）。

Result: 在大规模基准测试中，B-ReX工具表现优于其他黑盒XAI工具，在10变量随机布尔公式上达到了0.072 ± 0.012的Jensen-Shannon散度。

Conclusion: 论文通过形式化的因果性量度提供了更实际的XAI评估方法，并且B-ReX工具在布尔函数预测任务上显示出优异的性能。

Abstract: Evaluating explainable AI (XAI) approaches is a challenging task in general,
due to the subjectivity of explanations. In this paper, we focus on tabular
data and the specific use case of AI models predicting the values of Boolean
functions. We extend the previous work in this domain by proposing a formal and
precise measure of importance of variables based on actual causality, and we
evaluate state-of-the-art XAI tools against this measure. We also present a
novel XAI tool B-ReX, based on the existing tool ReX, and demonstrate that it
is superior to other black-box XAI tools on a large-scale benchmark.
Specifically, B-ReX achieves a Jensen-Shannon divergence of 0.072 $\pm$ 0.012
on random 10-valued Boolean formulae

</details>


### [127] [GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Method](https://arxiv.org/abs/2509.10018)
*Hailong Yang,Renhuo Zhao,Guanjin Wang,Zhaohong Deng*

Main category: cs.AI

TL;DR: GAMA是一个保护隐私的多智能体系统，通过将工作空间分为私有和公共区域，使用匿名化机制保护敏感数据，同时通过知识增强和逻辑增强模块减少语义损失。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在多智能体系统中的广泛应用，当任务涉及隐私数据时，需要在不牺牲性能的情况下实现隐私保护，因为高性能LLM通常部署在公共服务器上。

Method: 提出GAMA系统，划分私有和公共空间，在私有空间处理敏感数据，公共空间只使用匿名化数据。包含DRKE（基于领域规则的知识增强）和DLE（基于反证的逻辑增强）两个关键模块来减少匿名化带来的语义损失。

Result: 在两个公开问答数据集（Trivia Creative Writing和Logic Grid Puzzle）上表现优于最先进模型。在专门设计的隐私保护数据集（Knowledge Privacy Preservation和Logic Privacy Preservation）上也显示出卓越的隐私保护效果。

Conclusion: GAMA系统在保持任务处理性能的同时，有效实现了隐私保护，为LLM-based多智能体系统的隐私安全提供了可行解决方案。

Abstract: With the rapid advancement of Large Language Model (LLM), LLM-based agents
exhibit exceptional abilities in understanding and generating natural language,
facilitating human-like collaboration and information transmission in LLM-based
Multi-Agent System (MAS). High-performance LLMs are often hosted on remote
servers in public spaces. When tasks involve privacy data, MAS cannot securely
utilize these LLMs without implementing privacy-preserving mechanisms. To
address this challenge, we propose a General Anonymizing Multi-Agent system
(GAMA), which divides the agents' workspace into private and public spaces and
protects privacy through the anonymizing mechanism. In the private space,
agents handle sensitive data, while in the public space, only anonymized data
is utilized. GAMA incorporates two key modules to mitigate semantic loss caused
by anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and
Disproof-based Logic Enhancement (DLE). We evaluate GAMA on two public
question-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The
results demonstrate that GAMA has superior performance compared to the
state-of-the-art models. To further assess its privacy-preserving capabilities,
we designed two new datasets: Knowledge Privacy Preservation and Logic Privacy
Preservation. The final results highlight GAMA's exceptional effectiveness in
both task processing and privacy preservation.

</details>


### [128] [XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN Rules and Multipolar Task Processing Graph](https://arxiv.org/abs/2509.10054)
*Hailong Yang,Mingxian Gu,Jianqi Wang,Guanjin Wang,Zhaohong Deng*

Main category: cs.AI

TL;DR: XAgents是一个基于多极任务处理图和IF-THEN规则的多智能体协作框架，用于处理具有不确定性的复杂任务，在知识型和逻辑型问答任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型增强了多智能体系统处理复杂任务的能力，但在处理具有不确定性的高度复杂任务时，现有系统仍面临任务规划效果不佳的问题，容易产生误导性输出。

Method: 提出XAgents框架，采用多极任务处理图实现动态任务规划和处理任务不确定性，在子任务处理中集成领域特定的IF-THEN规则约束智能体行为，并通过全局规则增强智能体间协作。

Result: 在三个不同数据集上的评估显示，XAgents在知识型和逻辑型问答任务中持续超越最先进的单智能体和多智能体方法。

Conclusion: XAgents通过多极任务处理图和规则约束机制，有效解决了多智能体系统在复杂任务规划中的不确定性挑战，提升了任务执行效果。

Abstract: The rapid advancement of Large Language Models (LLMs) has significantly
enhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans
with complex, real-world tasks. However, MAS still face challenges in effective
task planning when handling highly complex tasks with uncertainty, often
resulting in misleading or incorrect outputs that hinder task execution. To
address this, we propose XAgents, a unified multi-agent cooperative framework
built on a multipolar task processing graph and IF-THEN rules. XAgents uses the
multipolar task processing graph to enable dynamic task planning and handle
task uncertainty. During subtask processing, it integrates domain-specific
IF-THEN rules to constrain agent behaviors, while global rules enhance
inter-agent collaboration. We evaluate the performance of XAgents across three
distinct datasets, demonstrating that it consistently surpasses
state-of-the-art single-agent and multi-agent approaches in both
knowledge-typed and logic-typed question-answering tasks. The codes for XAgents
are available at: https://github.com/AGI-FHBC/XAgents.

</details>


### [129] [AI Harmonics: a human-centric and harms severity-adaptive AI risk assessment framework](https://arxiv.org/abs/2509.10104)
*Sofia Vei,Paolo Giudici,Pavlos Sermpezis,Athena Vakali,Adelaide Emma Bernardelli*

Main category: cs.AI

TL;DR: 提出AI Harmonics框架，采用基于实证事件数据的人类中心、危害严重性自适应方法，通过新的AI危害评估指标(AIH)来识别和优先处理AI危害，特别关注政治和物理危害。


<details>
  <summary>Details</summary>
Motivation: 现有AI风险评估模型过于关注内部合规性，忽视了多元利益相关者视角和现实世界后果，需要转向更关注实际危害的人类中心方法。

Method: 开发AI Harmonics框架，包含新颖的AIH评估指标，利用序数严重性数据捕捉相对影响，无需精确数值估计，结合数据驱动和利益相关者感知的方法。

Result: 实验证明政治和物理危害集中度最高，政治危害侵蚀公众信任，物理危害造成严重甚至生命威胁，AI Harmonics能一致识别不均匀的危害分布。

Conclusion: 该框架使政策制定者和组织能够有效针对性地开展缓解工作，强调了方法在现实世界中的相关性。

Abstract: The absolute dominance of Artificial Intelligence (AI) introduces
unprecedented societal harms and risks. Existing AI risk assessment models
focus on internal compliance, often neglecting diverse stakeholder perspectives
and real-world consequences. We propose a paradigm shift to a human-centric,
harm-severity adaptive approach grounded in empirical incident data. We present
AI Harmonics, which includes a novel AI harm assessment metric (AIH) that
leverages ordinal severity data to capture relative impact without requiring
precise numerical estimates. AI Harmonics combines a robust, generalized
methodology with a data-driven, stakeholder-aware framework for exploring and
prioritizing AI harms. Experiments on annotated incident data confirm that
political and physical harms exhibit the highest concentration and thus warrant
urgent mitigation: political harms erode public trust, while physical harms
pose serious, even life-threatening risks, underscoring the real-world
relevance of our approach. Finally, we demonstrate that AI Harmonics
consistently identifies uneven harm distributions, enabling policymakers and
organizations to target their mitigation efforts effectively.

</details>


### [130] [Virtual Agent Economies](https://arxiv.org/abs/2509.10147)
*Nenad Tomasev,Matija Franklin,Joel Z. Leibo,Julian Jacobs,William A. Cunningham,Iason Gabriel,Simon Osindero*

Main category: cs.AI

TL;DR: 本文提出"沙箱经济"框架分析AI自主代理人经济系统，讨论了从自发出现到主动设计的路径选择，以实现可控制的代理人市场。


<details>
  <summary>Details</summary>
Motivation: 自主AI代理人的快速采用形成了新的经济层，这些代理人在超出人类监管范围的规模和速度上进行交易和协调，需要框架来分析和对出现的机遇与挑战。

Method: 使用两个关键维度对沙箱经济进行特征化：起源（自发出现vs意图性）和与人类经济的分离程度（可透性vs不可透性），考虑了竞价机制、AI"任务经济"设计和社会技术基础设施。

Result: 分析显示当前趋势指向一个自发出现的广泛且高度可透的AI代理人经济，这将带来无与伦比的协调机遇，但也带来系统性经济风险和加剧的不平等挑战。

Conclusion: 建议主动设计可控制的代理人市场，通过竞价机制、任务经济和相关基础设施来确保即将来临的技术变革与人类长期集体繁荣相对齐。

Abstract: The rapid adoption of autonomous AI agents is giving rise to a new economic
layer where agents transact and coordinate at scales and speeds beyond direct
human oversight. We propose the "sandbox economy" as a framework for analyzing
this emergent system, characterizing it along two key dimensions: its origins
(emergent vs. intentional) and its degree of separateness from the established
human economy (permeable vs. impermeable). Our current trajectory points toward
a spontaneous emergence of a vast and highly permeable AI agent economy,
presenting us with opportunities for an unprecedented degree of coordination as
well as significant challenges, including systemic economic risk and
exacerbated inequality. Here we discuss a number of possible design choices
that may lead to safely steerable AI agent markets. In particular, we consider
auction mechanisms for fair resource allocation and preference resolution, the
design of AI "mission economies" to coordinate around achieving collective
goals, and socio-technical infrastructure needed to ensure trust, safety, and
accountability. By doing this, we argue for the proactive design of steerable
agent markets to ensure the coming technological shift aligns with humanity's
long-term collective flourishing.

</details>


### [131] [Online Robust Planning under Model Uncertainty: A Sample-Based Approach](https://arxiv.org/abs/2509.10162)
*Tamir Shazman,Idan Lev-Yehudi,Ron Benchetit,Vadim Indelman*

Main category: cs.AI

TL;DR: 提出了Robust Sparse Sampling (RSS)算法，这是第一个具有有限样本理论性能保证的鲁棒MDP在线规划算法，能够在模型不确定的环境中实现可处理的鲁棒策略计算。


<details>
  <summary>Details</summary>
Motivation: 在线规划方法在现实环境中面临模型近似误差问题，可能导致性能下降或不安全行为。现有鲁棒MDP方法计算量大，不适合实时应用。

Method: 基于Sample Average Approximation (SAA)方法，通过计算鲁棒价值函数而非名义价值函数，扩展了Sparse Sampling算法。适用于无限或连续状态空间，样本和计算复杂度与状态空间大小无关。

Result: RSS在具有不确定动态的环境中优于标准Sparse Sampling方法，并提供了理论性能保证。

Conclusion: RSS是第一个适用于在线设置的鲁棒MDP规划算法，具有理论保证和实际应用价值，能够有效处理模型不确定性。

Abstract: Online planning in Markov Decision Processes (MDPs) enables agents to make
sequential decisions by simulating future trajectories from the current state,
making it well-suited for large-scale or dynamic environments. Sample-based
methods such as Sparse Sampling and Monte Carlo Tree Search (MCTS) are widely
adopted for their ability to approximate optimal actions using a generative
model. However, in practical settings, the generative model is often learned
from limited data, introducing approximation errors that can degrade
performance or lead to unsafe behaviors. To address these challenges, Robust
MDPs (RMDPs) offer a principled framework for planning under model uncertainty,
yet existing approaches are typically computationally intensive and not suited
for real-time use. In this work, we introduce Robust Sparse Sampling (RSS), the
first online planning algorithm for RMDPs with finite-sample theoretical
performance guarantees. Unlike Sparse Sampling, which estimates the nominal
value function, RSS computes a robust value function by leveraging the
efficiency and theoretical properties of Sample Average Approximation (SAA),
enabling tractable robust policy computation in online settings. RSS is
applicable to infinite or continuous state spaces, and its sample and
computational complexities are independent of the state space size. We provide
theoretical performance guarantees and empirically show that RSS outperforms
standard Sparse Sampling in environments with uncertain dynamics.

</details>


### [132] [Towards Fully Automated Molecular Simulations: Multi-Agent Framework for Simulation Setup and Force Field Extraction](https://arxiv.org/abs/2509.10210)
*Marko Petković,Vlado Menkovski,Sofía Calero*

Main category: cs.AI

TL;DR: 提出了一个基于LLM的多智能体框架，用于自动化多孔材料表征，包括文献提取力场和自动设置RASPA模拟


<details>
  <summary>Details</summary>
Motivation: 自动化多孔材料表征可以加速材料发现，但目前受限于模拟设置和力场选择的复杂性

Method: 使用基于LLM的多智能体系统，能够自主理解表征任务、规划模拟、组装力场、执行模拟并解释结果以指导后续步骤

Result: 初步评估显示出高度的正确性和可重现性

Conclusion: 该方法有潜力实现完全自主、可扩展的材料表征

Abstract: Automated characterization of porous materials has the potential to
accelerate materials discovery, but it remains limited by the complexity of
simulation setup and force field selection. We propose a multi-agent framework
in which LLM-based agents can autonomously understand a characterization task,
plan appropriate simulations, assemble relevant force fields, execute them and
interpret their results to guide subsequent steps. As a first step toward this
vision, we present a multi-agent system for literature-informed force field
extraction and automated RASPA simulation setup. Initial evaluations
demonstrate high correctness and reproducibility, highlighting this approach's
potential to enable fully autonomous, scalable materials characterization.

</details>


### [133] [Compartmentalised Agentic Reasoning for Clinical NLI](https://arxiv.org/abs/2509.10222)
*Maël Jullien,Lei Xu,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: CARENLI框架通过将临床NLI任务分解为四个推理家族并采用模块化架构，显著提升了推理准确性和可审计性，最高提升42个百分点


<details>
  <summary>Details</summary>
Motivation: 质疑数据和参数规模扩大必然带来结构化、可泛化内部表示的假设，特别是在临床自然语言推理领域需要更安全、可审计的推理框架

Method: 提出CARENLI框架，将临床NLI分解为四个推理家族（因果归因、组合基础、认知验证、风险状态抽象），采用规划器、验证器、精炼器的模块化架构，分离知识访问和原则推理

Result: 在四个LLM上，CARENLI将准确率提升高达42个百分点，因果归因达到98.0%，风险状态抽象达到81.2%。验证器能近乎完美地标记违规，精炼器能修正大量认知错误

Conclusion: LLM通常保留相关事实但在推理不明确时默认使用启发式方法，CARENLI通过显式分离推理过程为解决这一分离问题提供了框架，同时实现了更安全、可审计的临床推理

Abstract: A common assumption holds that scaling data and parameters yields
increasingly structured, generalisable internal representations. We interrogate
this assumption in clinical natural language inference (NLI) by adopting a
benchmark decomposed into four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction,
and introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI
that separates knowledge access from principled inference. CARENLI routes each
premise, statement pair to a family specific solver and enforces auditable
procedures via a planner, verifier, and refiner.
  Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching
98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag
violations with near-ceiling reliability, while refiners correct a substantial
share of epistemic errors. Remaining failures cluster in routing, identifying
family classification as the main bottleneck. These results show that LLMs
often retain relevant facts but default to heuristics when inference is
underspecified, a dissociation CARENLI makes explicit while offering a
framework for safer, auditable reasoning.

</details>


### [134] [Investigating Language Model Capabilities to Represent and Process Formal Knowledge: A Preliminary Study to Assist Ontology Engineering](https://arxiv.org/abs/2509.10249)
*Hanna Abi Akl*

Main category: cs.AI

TL;DR: 研究表明用紧凑的逻辑语言替代自然语言可以在保持推理性能的同时提升小语言模型在推理任务中的表现，为小语言模型在ontology工程中的应用提供新方向


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在推理能力方面存在明显不足，特别是在ontology工程领域。研究旨在探索形式化方法如何提升小语言模型在推理任务中的性能

Method: 通过一系列初步实验，研究不同语法表达逻辑问题对小语言模型在预定义推理任务中表现的影响，比较自然语言与紧凑逻辑语言的效果

Result: 研究发现使用更紧凑的逻辑语言替代自然语言可以在保持强大推理性能的同时提升效率

Conclusion: 这些结果为小语言模型在ontology工程中的角色进一步细化提供了基础，展示了形式化方法在提升语言模型推理能力方面的潜力

Abstract: Recent advances in Language Models (LMs) have failed to mask their
shortcomings particularly in the domain of reasoning. This limitation impacts
several tasks, most notably those involving ontology engineering. As part of a
PhD research, we investigate the consequences of incorporating formal methods
on the performance of Small Language Models (SLMs) on reasoning tasks.
Specifically, we aim to orient our work toward using SLMs to bootstrap ontology
construction and set up a series of preliminary experiments to determine the
impact of expressing logical problems with different grammars on the
performance of SLMs on a predefined reasoning task. Our findings show that it
is possible to substitute Natural Language (NL) with a more compact logical
language while maintaining a strong performance on reasoning tasks and hope to
use these results to further refine the role of SLMs in ontology engineering.

</details>


### [135] [The Morality of Probability: How Implicit Moral Biases in LLMs May Shape the Future of Human-AI Symbiosis](https://arxiv.org/abs/2509.10297)
*Eoin O'Doherty,Nicole Weinrauch,Andrew Talone,Uri Klempner,Xiaoyuan Yi,Xing Xie,Yi Zeng*

Main category: cs.AI

TL;DR: 这篇论文通过实验研究大语言模型的道德偏好，发现所有模型都一致偏向关怀和美德价值，而自由意志主义选择被扣分。推理模型显示出更好的上下文敏感性和解释能力。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统如何与人类道德价值对齐，以及模型架构、文化起源和可解释性如何影响道德偏好。

Method: 对六个大语言模型进行定量实验，通过评分和排名18个代表五种道德框架的困境来分析其道德偏好。

Result: 所有模型都显示出一致的价值偏向：关怀和美德价值被评为最道德，自由意志主义选择被一贵扣分。推理模型具有更好的上下文敏感性和丰富的解释，非推理模型则做出更统一但不透明的判断。

Conclusion: 研究强调了可解释性和文化意识作为关键设计原则的重要性，以指导AI向透明、对齐和共生的未来发展。

Abstract: Artificial intelligence (AI) is advancing at a pace that raises urgent
questions about how to align machine decision-making with human moral values.
This working paper investigates how leading AI systems prioritize moral
outcomes and what this reveals about the prospects for human-AI symbiosis. We
address two central questions: (1) What moral values do state-of-the-art large
language models (LLMs) implicitly favour when confronted with dilemmas? (2) How
do differences in model architecture, cultural origin, and explainability
affect these moral preferences? To explore these questions, we conduct a
quantitative experiment with six LLMs, ranking and scoring outcomes across 18
dilemmas representing five moral frameworks. Our findings uncover strikingly
consistent value biases. Across all models, Care and Virtue values outcomes
were rated most moral, while libertarian choices were consistently penalized.
Reasoning-enabled models exhibited greater sensitivity to context and provided
richer explanations, whereas non-reasoning models produced more uniform but
opaque judgments. This research makes three contributions: (i) Empirically, it
delivers a large-scale comparison of moral reasoning across culturally distinct
LLMs; (ii) Theoretically, it links probabilistic model behaviour with
underlying value encodings; (iii) Practically, it highlights the need for
explainability and cultural awareness as critical design principles to guide AI
toward a transparent, aligned, and symbiotic future.

</details>


### [136] [State Algebra for Propositional Logic](https://arxiv.org/abs/2509.10326)
*Dmitry Lesnik,Tobias Schäfer*

Main category: cs.AI

TL;DR: 新的代数框架State Algebra，通过集合、坐标和行分解三层表示来处理命题逻辑，在保持灵活性的同时可得到规范形式。


<details>
  <summary>Details</summary>
Motivation: 为命题逻辑提供一种代数化的表示和操作框架，合并理解的语义和高效的计算引擎。

Method: 构建三层表示层次：Set、Coordinate和Row Decomposition，通过固定变量顺序实现规范化，在灵活性和规范性之间找到平衡。

Result: 框架能够表达搜索基于和知识编译算法，并自然扩展到概率逻辑咊权重模型计数。

Conclusion: State Algebra为逻辑表示和操作提供了灵活而强大的代数工具，在不保证规范性的情况下优化了表达能力。

Abstract: This paper presents State Algebra, a novel framework designed to represent
and manipulate propositional logic using algebraic methods. The framework is
structured as a hierarchy of three representations: Set, Coordinate, and Row
Decomposition. These representations anchor the system in well-known semantics
while facilitating the computation using a powerful algebraic engine. A key
aspect of State Algebra is its flexibility in representation. We show that
although the default reduction of a state vector is not canonical, a unique
canonical form can be obtained by applying a fixed variable order during the
reduction process. This highlights a trade-off: by foregoing guaranteed
canonicity, the framework gains increased flexibility, potentially leading to
more compact representations of certain classes of problems. We explore how
this framework provides tools to articulate both search-based and knowledge
compilation algorithms and discuss its natural extension to probabilistic logic
and Weighted Model Counting.

</details>


### [137] [Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems](https://arxiv.org/abs/2509.10401)
*Alva West,Yixuan Weng,Minjun Zhu,Zhen Lin,Yue Zhang*

Main category: cs.AI

TL;DR: A2P Scaffolding框架通过结构化因果推理方法，将多智能体系统中的故障归因从模式识别任务转变为因果推理任务，显著提高了步骤级准确率（2.85倍提升）。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统中的故障归因方法准确率极低（低于17%），无法进行有效的反事实推理来确定纠正单个动作是否能避免任务失败。

Method: 提出Abduct-Act-Predict (A2P) Scaffolding框架，通过三个结构化步骤指导大语言模型进行推理：溯因推理（推断隐藏原因）、行动（定义最小纠正干预）、预测（模拟后续轨迹验证干预效果）。

Result: 在Algorithm-Generated数据集上达到47.46%的步骤级准确率（比基线16.67%提升2.85倍），在Hand-Crafted数据集上达到29.31%准确率（比基线12.07%提升2.43倍）。

Conclusion: 通过因果推理框架重构问题，A2P Scaffolding为自动化故障归因提供了更鲁棒、可验证且准确度显著提升的解决方案。

Abstract: Failure attribution in multi-agent systems -- pinpointing the exact step
where a decisive error occurs -- is a critical yet unsolved challenge. Current
methods treat this as a pattern recognition task over long conversation logs,
leading to critically low step-level accuracy (below 17\%), which renders them
impractical for debugging complex systems. Their core weakness is a fundamental
inability to perform robust counterfactual reasoning: to determine if
correcting a single action would have actually averted the task failure. To
bridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)
Scaffolding, a novel agent framework that transforms failure attribution from
pattern recognition into a structured causal inference task. A2P explicitly
guides a large language model through a formal three-step reasoning process
within a single inference pass: (1) Abduction, to infer the hidden root causes
behind an agent's actions; (2) Action, to define a minimal corrective
intervention; and (3) Prediction, to simulate the subsequent trajectory and
verify if the intervention resolves the failure. This structured approach
leverages the holistic context of the entire conversation while imposing a
rigorous causal logic on the model's analysis. Our extensive experiments on the
Who\&When benchmark demonstrate its efficacy. On the Algorithm-Generated
dataset, A2P achieves 47.46\% step-level accuracy, a 2.85$\times$ improvement
over the 16.67\% of the baseline. On the more complex Hand-Crafted dataset, it
achieves 29.31\% step accuracy, a 2.43$\times$ improvement over the baseline's
12.07\%. By reframing the problem through a causal lens, A2P Scaffolding
provides a robust, verifiable, and significantly more accurate solution for
automated failure attribution.

</details>


### [138] [Mutual Information Tracks Policy Coherence in Reinforcement Learning](https://arxiv.org/abs/2509.10423)
*Cameron Reid,Wael Hafez,Amirhossein Nazeri*

Main category: cs.AI

TL;DR: 信息论框架用于分析RL学习动态和识别系统故障，通过相互信息模式进行故障诊断和定位


<details>
  <summary>Details</summary>
Motivation: 实际部署的RL代理缺乏内置的故障检测和诊断机制，需要一种无需修改系统架构的健康监测方法

Method: 使用信息论框架分析状态-动作相互信息模式，通过受控扰动实验验证不同类型故障的特征信息签名

Result: 成功学习呈现特征性信息模式：状态-动作相互信息从0.84增长到2.83比特；观测器故障导致所有信息通道广泛崩溃，执行器故障仅影响动作-结果预测性

Conclusion: 信息模式既是学习过程的特征标志，也是系统健康的诊断工具，为自主故障检测和策略调整的适应性RL系统奠定基础

Abstract: Reinforcement Learning (RL) agents deployed in real-world environments face
degradation from sensor faults, actuator wear, and environmental shifts, yet
lack intrinsic mechanisms to detect and diagnose these failures. We present an
information-theoretic framework that reveals both the fundamental dynamics of
RL and provides practical methods for diagnosing deployment-time anomalies.
Through analysis of state-action mutual information patterns in a robotic
control task, we first demonstrate that successful learning exhibits
characteristic information signatures: mutual information between states and
actions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing
state entropy, indicating that agents develop increasingly selective attention
to task-relevant patterns. Intriguingly, states, actions and next states joint
mutual information, MI(S,A;S'), follows an inverted U-curve, peaking during
early learning before declining as the agent specializes suggesting a
transition from broad exploration to efficient exploitation. More immediately
actionable, we show that information metrics can differentially diagnose system
failures: observation-space, i.e., states noise (sensor faults) produces broad
collapses across all information channels with pronounced drops in state-action
coupling, while action-space noise (actuator faults) selectively disrupts
action-outcome predictability while preserving state-action relationships. This
differential diagnostic capability demonstrated through controlled perturbation
experiments enables precise fault localization without architectural
modifications or performance degradation. By establishing information patterns
as both signatures of learning and diagnostic for system health, we provide the
foundation for adaptive RL systems capable of autonomous fault detection and
policy adjustment based on information-theoretic principles.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [139] [Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis](https://arxiv.org/abs/2509.09744)
*Mujie Liu,Chenze Wang,Liping Chen,Nguyen Linh Dan Le,Niharika Tewari,Ting Dang,Jiangang Ma,Feng Xia*

Main category: cs.LG

TL;DR: SAM-BG是一个两阶段的自监督学习框架，通过结构语义保护来学习脑网络表示，在精神病诊断中表现优异，特别是在小样本标注数据场景下。


<details>
  <summary>Details</summary>
Motivation: 脑网络标注数据有限，现有自监督学习方法的数据增强策略可能破坏脑图的关键结构语义，需要开发能够保护结构语义的表示学习方法。

Method: 提出两阶段框架：预训练阶段使用小标注子集训练边缘掩码器捕获关键结构语义；自监督学习阶段使用提取的结构先验指导结构感知的数据增强过程。

Result: 在两个真实精神病数据集上的实验表明，SAM-BG优于现有最先进方法，特别是在小标注数据设置下，并能发现临床相关的连接模式增强可解释性。

Conclusion: SAM-BG通过结构语义保护成功解决了脑网络数据有限的问题，为精神病诊断提供了更准确和可解释的解决方案。

Abstract: The limited availability of labeled brain network data makes it challenging
to achieve accurate and interpretable psychiatric diagnoses. While
self-supervised learning (SSL) offers a promising solution, existing methods
often rely on augmentation strategies that can disrupt crucial structural
semantics in brain graphs. To address this, we propose SAM-BG, a two-stage
framework for learning brain graph representations with structural semantic
preservation. In the pre-training stage, an edge masker is trained on a small
labeled subset to capture key structural semantics. In the SSL stage, the
extracted structural priors guide a structure-aware augmentation process,
enabling the model to learn more semantically meaningful and robust
representations. Experiments on two real-world psychiatric datasets demonstrate
that SAM-BG outperforms state-of-the-art methods, particularly in small-labeled
data settings, and uncovers clinically relevant connectivity patterns that
enhance interpretability. Our code is available at
https://github.com/mjliu99/SAM-BG.

</details>


### [140] [D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference](https://arxiv.org/abs/2509.09747)
*Leen Daher,Zhaobo Wang,Malcolm Mielle*

Main category: cs.LG

TL;DR: 提出D-CAT框架，实现无需推理时配对传感器的跨模态迁移学习，通过交叉注意力对齐损失来对齐不同模态的特征空间，在资源受限环境中实现单传感器推理。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态迁移学习方法在训练和推理时都需要配对传感器数据，限制了在资源受限环境中的部署，因为完整的传感器套件在经济和技术上不可行。

Method: D-CAT框架结合自注意力模块进行特征提取和新型交叉注意力对齐损失，强制对齐不同传感器的特征空间，而不需要耦合两种模态的分类流程。

Result: 在三个多模态人类活动数据集上评估，在分布内场景中，从高性能模态迁移可获得10%的F1分数提升；在分布外场景中，即使较弱的源模态也能改善目标性能。

Conclusion: D-CAT通过跨模态知识实现单传感器推理，减少了感知系统的硬件冗余，同时保持准确性，对成本敏感或自适应部署至关重要。

Abstract: Cross-modal transfer learning is used to improve multi-modal classification
models (e.g., for human activity recognition in human-robot collaboration).
However, existing methods require paired sensor data at both training and
inference, limiting deployment in resource-constrained environments where full
sensor suites are not economically and technically usable. To address this, we
propose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns
modality-specific representations without requiring joint sensor modality
during inference. Our approach combines a self-attention module for feature
extraction with a novel cross-attention alignment loss, which enforces the
alignment of sensors' feature spaces without requiring the coupling of the
classification pipelines of both modalities. We evaluate D-CAT on three
multi-modal human activity datasets (IMU, video, and audio) under both
in-distribution and out-of-distribution scenarios, comparing against uni-modal
models. Results show that in in-distribution scenarios, transferring from
high-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains
over uni-modal training. In out-of-distribution scenarios, even weaker source
modalities (e.g., IMU to video) improve target performance, as long as the
target model isn't overfitted on the training data. By enabling single-sensor
inference with cross-modal knowledge, D-CAT reduces hardware redundancy for
perception systems while maintaining accuracy, which is critical for
cost-sensitive or adaptive deployments (e.g., assistive robots in homes with
variable sensor availability). Code is available at
https://github.com/Schindler-EPFL-Lab/D-CAT.

</details>


### [141] [Meta-Learning Reinforcement Learning for Crypto-Return Prediction](https://arxiv.org/abs/2509.09751)
*Junqiao Wang,Zhaoyang Guan,Guanyu Liu,Tianze Xia,Xianzhi Li,Shuo Yin,Xinyuan Song,Chuhan Cheng,Tianyu Shi,Alex Lee*

Main category: cs.LG

TL;DR: 基于Transformer的元学习和强化学习统一框架，构建自我改进的动态交易组件，无需人工监督即可领先其他LLM基线模型


<details>
  <summary>Details</summary>
Motivation: 动态混合的币均价格驱动因素（链上活动、新闻流、社交情感）以及标签训练数据的稀缺和费用高异常增加了预测难度

Method: 从普通指令调整LLM出发，通过代理人、判官和元判官三个角色的闭环迭代学习，支持多模态市场输入和内部偏好反馈，持续精炼交易策略和评估标准

Result: 在多样市场治理下的实验显示，方案在实际市场技术指标上表现良好，且超越其他LLM基线模型

Conclusion: Meta-RL-Crypto提供了一种元学习与强化学习相结合的统一框架，能构建自我改进的动态交易组件，无需人工监督即可领先其他LLM基线模型

Abstract: Predicting cryptocurrency returns is notoriously difficult: price movements
are driven by a fast-shifting blend of on-chain activity, news flow, and social
sentiment, while labeled training data are scarce and expensive. In this paper,
we present Meta-RL-Crypto, a unified transformer-based architecture that
unifies meta-learning and reinforcement learning (RL) to create a fully
self-improving trading agent. Starting from a vanilla instruction-tuned LLM,
the agent iteratively alternates between three roles-actor, judge, and
meta-judge-in a closed-loop architecture. This learning process requires no
additional human supervision. It can leverage multimodal market inputs and
internal preference feedback. The agent in the system continuously refines both
the trading policy and evaluation criteria. Experiments across diverse market
regimes demonstrate that Meta-RL-Crypto shows good performance on the technical
indicators of the real market and outperforming other LLM-based baselines.

</details>


### [142] [LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation](https://arxiv.org/abs/2509.09754)
*Yiqun Shen,Song Yuan,Zhengze Zhang,Xiaoliang Wang,Daxin Jiang,Nguyen Cam-Tu*

Main category: cs.LG

TL;DR: LAVa是一个统一的KV缓存压缩框架，通过最小化Transformer残差流中的信息损失来实现动态预算分配，在长上下文推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的KV缓存压缩方法大多是启发式的，缺乏动态预算分配机制，无法有效处理长上下文推理中的高内存需求问题。

Method: 通过分析层注意力输出损失，推导出跨头比较的新度量标准，实现层间压缩和动态头预算分配；通过对比跨层信息实现动态层预算分配。

Result: 在多个基准测试（LongBench、Needle-In-A-Haystack、Ruler、InfiniteBench）中表现出优越性能，发现动态层预算对生成任务关键，动态头预算对抽取任务重要。

Conclusion: LAVa是首个统一的缓存淘汰和动态预算分配策略，无需训练或多策略组合，在各种任务类型中保持顶级性能。

Abstract: KV Cache is commonly used to accelerate LLM inference with long contexts, yet
its high memory demand drives the need for cache compression. Existing
compression methods, however, are largely heuristic and lack dynamic budget
allocation. To address this limitation, we introduce a unified framework for
cache compression by minimizing information loss in Transformer residual
streams. Building on it, we analyze the layer attention output loss and derive
a new metric to compare cache entries across heads, enabling layer-wise
compression with dynamic head budgets. Additionally, by contrasting cross-layer
information, we also achieve dynamic layer budgets. LAVa is the first unified
strategy for cache eviction and dynamic budget allocation that, unlike prior
methods, does not rely on training or the combination of multiple strategies.
Experiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and
InfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a
new insight: dynamic layer budgets are crucial for generation tasks (e.g., code
completion), while dynamic head budgets play a key role in extraction tasks
(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently
maintains top performance across task types. Our code is available at
https://github.com/MGDDestiny/Lava.

</details>


### [143] [Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management](https://arxiv.org/abs/2509.09772)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: HACO框架通过分离风险校准和偏好优化，为医疗补助人群的健康管理提供安全、公平、可审计的决策支持，在控制不良事件风险的同时保持高安全覆盖率。


<details>
  <summary>Details</summary>
Motivation: 医疗补助人群的健康管理项目需要协调纵向服务，必须确保安全、公平和可审计性。传统方法难以在控制风险的同时优化服务协调决策。

Method: 提出混合自适应符合离线强化学习(HACO)框架：1)训练轻量级风险模型预测不良事件；2)推导符合阈值以屏蔽不安全行动；3)在安全子集上学习偏好策略。使用270万条决策数据进行验证。

Result: HACO实现了强大的风险区分能力(AUC~0.81)，校准阈值在α=0.10时为τ~0.038，同时保持高安全覆盖率。亚组分析显示不同人口统计特征间存在系统性价值差异。

Conclusion: 符合风险门控与离线强化学习相结合，能够为人口健康管理团队提供保守且可审计的决策支持，强调了公平性审计的重要性。

Abstract: Population health management programs for Medicaid populations coordinate
longitudinal outreach and services (e.g., benefits navigation, behavioral
health, social needs support, and clinical scheduling) and must be safe, fair,
and auditable. We present a Hybrid Adaptive Conformal Offline Reinforcement
Learning (HACO) framework that separates risk calibration from preference
optimization to generate conservative action recommendations at scale. In our
setting, each step involves choosing among common coordination actions (e.g.,
which member to contact, by which modality, and whether to route to a
specialized service) while controlling the near-term risk of adverse
utilization events (e.g., unplanned emergency department visits or
hospitalizations). Using a de-identified operational dataset from Waymark
comprising 2.77 million sequential decisions across 168,126 patients, HACO (i)
trains a lightweight risk model for adverse events, (ii) derives a conformal
threshold to mask unsafe actions at a target risk level, and (iii) learns a
preference policy on the resulting safe subset. We evaluate policies with a
version-agnostic fitted Q evaluation (FQE) on stratified subsets and audit
subgroup performance across age, sex, and race. HACO achieves strong risk
discrimination (AUC ~0.81) with a calibrated threshold ( {\tau} ~0.038 at
{\alpha} = 0.10), while maintaining high safe coverage. Subgroup analyses
reveal systematic differences in estimated value across demographics,
underscoring the importance of fairness auditing. Our results show that
conformal risk gating integrates cleanly with offline RL to deliver
conservative, auditable decision support for population health management
teams.

</details>


### [144] [One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection](https://arxiv.org/abs/2509.09782)
*Roshini Pulishetty,Mani Kishan Ghantasala,Keerthy Kaushik Dasoju,Niti Mangwani,Vishal Garimella,Aditya Mate,Somya Chatterjee,Yue Kang,Ehi Nosakhare,Sadid Hasan,Soundar Srinivasan*

Main category: cs.LG

TL;DR: 一种统一的LLM路由框架，通过单头交叉注意力机制动态选择最优模型，在RouterBench评测中实现了性能和成本的显著提升


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型部署中的计算成本和性能平衡挑战，需要一种可扩展、成本敏感的动态路由方案

Method: 使用单头交叉注意力机制共同建模查询和模型嵌入，预测响应质量和生成成本，提出指数奖励函数来稳定地平衡性能与成本

Result: 在RouterBench评测中实现了平均质量收益(AIQ)提升6.6%，最大性能提升2.9%，较现有路由器更高效

Conclusion: 该轻量级架构具有良好的领域适配性，在性能和成本方面都显著优于以前方法，为成本敏感的LLM路由设定了新标准

Abstract: The proliferation of large language models (LLMs) with varying computational
costs and performance profiles presents a critical challenge for scalable,
cost-effective deployment in real-world applications. We introduce a unified
routing framework that leverages a single-head cross-attention mechanism to
jointly model query and model embeddings, enabling dynamic selection of the
optimal LLM for each input query. Our approach is evaluated on RouterBench, a
large-scale, publicly available benchmark encompassing diverse LLM pools and
domains. By explicitly capturing fine-grained query-model interactions, our
router predicts both response quality and generation cost, achieving up to 6.6%
improvement in Average Improvement in Quality (AIQ) and 2.9% in maximum
performance over existing routers. To robustly balance performance and cost, we
propose an exponential reward function that enhances stability across user
preferences. The resulting architecture is lightweight, generalizes effectively
across domains, and demonstrates improved efficiency compared to prior methods,
establishing a new standard for cost-aware LLM routing.

</details>


### [145] [From the Gradient-Step Denoiser to the Proximal Denoiser and their associated convergent Plug-and-Play algorithms](https://arxiv.org/abs/2509.09793)
*Vincent Herfeld,Baudouin Denis de Senneville,Arthur Leclaire,Nicolas Papadakis*

Main category: cs.LG

TL;DR: 分析梯度步去噪器及其在即插即用算法中的应用，该去噪器被训练为显式函数梯度的精确算子，同时保持最先进的去噪能力


<details>
  <summary>Details</summary>
Motivation: 即插即用优化算法使用现成去噪器替代图像先验的邻近算子或梯度下降算子，但通常这些先验是隐式的无法表达

Method: 训练梯度步去噪器，使其成为显式函数梯度的精确算子（梯度下降算子或邻近算子）

Result: 梯度步去噪器能够保持最先进的去噪性能

Conclusion: 梯度步去噪器为即插即用算法提供了显式的函数梯度算子，解决了传统方法中先验隐式表达的问题

Abstract: In this paper we analyze the Gradient-Step Denoiser and its usage in
Plug-and-Play algorithms. The Plug-and-Play paradigm of optimization algorithms
uses off the shelf denoisers to replace a proximity operator or a gradient
descent operator of an image prior. Usually this image prior is implicit and
cannot be expressed, but the Gradient-Step Denoiser is trained to be exactly
the gradient descent operator or the proximity operator of an explicit
functional while preserving state-of-the-art denoising capabilities.

</details>


### [146] [Distinguishing Startle from Surprise Events Based on Physiological Signals](https://arxiv.org/abs/2509.09799)
*Mansi Sharma,Alexandre Duchevet,Florian Daiber,Jean-Paul Imbert,Maurice Rekrut*

Main category: cs.LG

TL;DR: 使用机器学习和多模态融合策略，通过生理信号区分惊吓和惊讶事件，最高准确率达85.7%，并能区分三种状态（惊吓、惊讶、基线）达到74.9%准确率


<details>
  <summary>Details</summary>
Motivation: 意外事件会损害注意力和延迟决策，在高风险环境如航空中构成严重安全风险。惊吓和惊讶反应以不同方式影响飞行员表现，但实践中难以区分，现有研究多单独研究这些反应，缺乏对其综合效应和生理数据区分方法的研究

Method: 使用机器学习方法和多模态融合策略，基于生理信号来区分惊吓和惊讶事件

Result: 能够可靠预测这些事件，SVM和Late Fusion达到最高平均准确率85.7%；扩展评估包括基线条件后，XGBoost和Late Fusion能区分惊吓、惊讶和基线三种状态，最高平均准确率达74.9%

Conclusion: 该方法能有效区分惊吓和惊讶反应，验证了模型的鲁棒性，为高风险环境中意外事件的生理信号识别提供了有效解决方案

Abstract: Unexpected events can impair attention and delay decision-making, posing
serious safety risks in high-risk environments such as aviation. In particular,
reactions like startle and surprise can impact pilot performance in different
ways, yet are often hard to distinguish in practice. Existing research has
largely studied these reactions separately, with limited focus on their
combined effects or how to differentiate them using physiological data. In this
work, we address this gap by distinguishing between startle and surprise events
based on physiological signals using machine learning and multi-modal fusion
strategies. Our results demonstrate that these events can be reliably
predicted, achieving a highest mean accuracy of 85.7% with SVM and Late Fusion.
To further validate the robustness of our model, we extended the evaluation to
include a baseline condition, successfully differentiating between Startle,
Surprise, and Baseline states with a highest mean accuracy of 74.9% with
XGBoost and Late Fusion.

</details>


### [147] [Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning](https://arxiv.org/abs/2509.09838)
*Reza Asad,Reza Babanezhad,Sharan Vaswani*

Main category: cs.LG

TL;DR: 重新设计离散动作环境中的基于策略的偏离策学习方法，通过解耦演员与评判器的熵概来提升性能，达到与DQN相当的表现。


<details>
  <summary>Details</summary>
Motivation: 价值基方法如DQN在离散动作环境中表现优异，而常见的策略基方法或不能有效利用偏离策数据，或者在离散动作设置中表现较差。需要重新设计基于策略的偏离策学习方法来提升其在这些环境中的性能。

Method: 提出了一个灵活的偏离策演员-评判器框架，通过解耦演员与评判器的熵概来改善DSAC的性能。该框架允许使用m步贝尔曼运算符进行评判器更新，并结合标准策略优化方法与熵概正则化来实例化演员目标。

Result: 在理论上证明了方法能够保证收敛到最优正则化价值函数。在实验中，这些方法在标准Atari游戏中接近DQN的性能，甚至在没有熵概正则化或显式探索的情况下也能实现。

Conclusion: 通过解耦演员与评判器的熵概，基于策略的偏离策学习方法可以在离散动作环境中达到与价值基方法相当的性能，为离散动作偏离策学习提供了更灵活有效的框架。

Abstract: Value-based approaches such as DQN are the default methods for off-policy
reinforcement learning with discrete-action environments such as Atari. Common
policy-based methods are either on-policy and do not effectively learn from
off-policy data (e.g. PPO), or have poor empirical performance in the
discrete-action setting (e.g. SAC). Consequently, starting from discrete SAC
(DSAC), we revisit the design of actor-critic methods in this setting. First,
we determine that the coupling between the actor and critic entropy is the
primary reason behind the poor performance of DSAC. We demonstrate that by
merely decoupling these components, DSAC can have comparable performance as
DQN. Motivated by this insight, we introduce a flexible off-policy actor-critic
framework that subsumes DSAC as a special case. Our framework allows using an
m-step Bellman operator for the critic update, and enables combining standard
policy optimization methods with entropy regularization to instantiate the
resulting actor objective. Theoretically, we prove that the proposed methods
can guarantee convergence to the optimal regularized value function in the
tabular setting. Empirically, we demonstrate that these methods can approach
the performance of DQN on standard Atari games, and do so even without entropy
regularization or explicit exploration.

</details>


### [148] [HGEN: Heterogeneous Graph Ensemble Networks](https://arxiv.org/abs/2509.09843)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: HGEN是首个针对异质图的集成学习框架，通过元路径和变换优化管道集成多个学习器，提升分类准确率


<details>
  <summary>Details</summary>
Motivation: 异质图中节点类型、节点特征和局部邻域拓扑的异质性给集成学习带来挑战，需要适应多样化的图学习器

Method: 使用元路径结合随机丢弃创建Allele GNNs，采用残差注意力机制校准不同元路径的GNNs，并通过相关性正则化项增大嵌入矩阵差异

Result: 在五个异质网络上的实验验证HGEN始终大幅优于最先进的竞争对手

Conclusion: HGEN通过有效的集成学习框架解决了异质图学习中的挑战，显著提升了分类性能

Abstract: This paper presents HGEN that pioneers ensemble learning for heterogeneous
graphs. We argue that the heterogeneity in node types, nodal features, and
local neighborhood topology poses significant challenges for ensemble learning,
particularly in accommodating diverse graph learners. Our HGEN framework
ensembles multiple learners through a meta-path and transformation-based
optimization pipeline to uplift classification accuracy. Specifically, HGEN
uses meta-path combined with random dropping to create Allele Graph Neural
Networks (GNNs), whereby the base graph learners are trained and aligned for
later ensembling. To ensure effective ensemble learning, HGEN presents two key
components: 1) a residual-attention mechanism to calibrate allele GNNs of
different meta-paths, thereby enforcing node embeddings to focus on more
informative graphs to improve base learner accuracy, and 2) a
correlation-regularization term to enlarge the disparity among embedding
matrices generated from different meta-paths, thereby enriching base learner
diversity. We analyze the convergence of HGEN and attest its higher
regularization magnitude over simple voting. Experiments on five heterogeneous
networks validate that HGEN consistently outperforms its state-of-the-art
competitors by substantial margin.

</details>


### [149] [Latency and Token-Aware Test-Time Compute](https://arxiv.org/abs/2509.09864)
*Jenny Y. Huang,Mehul Damani,Yousef El-Kurdi,Ramon Astudillo,Wei Sun*

Main category: cs.LG

TL;DR: 该论文提出了一种动态计算分配框架，通过在每个查询基础上选择最优的推理策略和计算资源分配，在考虑令牌成本和延迟的同时提高大语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的推理时扩展方法主要关注并行生成方法（如best-of-N），忽视了增量解码方法，且大多只考虑令牌使用而忽略了延迟对用户体验和自治工作流的重要性。

Method: 形式化推理时扩展为动态计算分配和方法选择问题，明确考虑令牌成本和实际延迟两个关键指标，通过实验验证框架在各种推理策略之间的动态选择能力。

Result: 在理由性能测试中，该方法一贯性超过静态策略，实现了更优的准确性-成本交换效果，同时保持了实际部署的可行性。

Conclusion: 该研究提供了一种综合考虑性能和效率的动态推理优化框架，为大语言模型的实际应用提供了更加实用的性能提升方案。

Abstract: Inference-time scaling has emerged as a powerful way to improve large
language model (LLM) performance by generating multiple candidate responses and
selecting among them. However, existing work on dynamic allocation for
test-time compute typically considers only parallel generation methods such as
best-of-N, overlooking incremental decoding methods like beam search, and has
largely ignored latency, focusing only on token usage. We formulate
inference-time scaling as a problem of dynamic compute allocation and method
selection, where the system must decide which strategy to apply and how much
compute to allocate on a per-query basis. Our framework explicitly incorporates
both token cost and wall-clock latency, the latter being critical for user
experience and particularly for agentic workflows where models must issue
multiple queries efficiently. Experiments on reasoning benchmarks show that our
approach consistently outperforms static strategies, achieving favorable
accuracy-cost trade-offs while remaining practical for deployment.

</details>


### [150] [Variational Neural Networks for Observable Thermodynamics (V-NOTS)](https://arxiv.org/abs/2509.09899)
*Christopher Eldred,François Gay-Balmaz,Vakhtang Putkaradze*

Main category: cs.LG

TL;DR: 提出了一种基于可观测变量的数据驱动计算方法，通过热力学拉格朗日量和神经网络来预测耗散动力系统的演化，确保熵不减特性。


<details>
  <summary>Details</summary>
Motivation: 许多物理系统的演化预测需要相空间数据，但实际可观测的变量往往不包含完整的相空间信息（如动量和熵），特别是在耗散动力系统中。

Method: 构建基于热力学拉格朗日量的神经网络框架，仅使用可观测变量，同时保证热力学约束和熵不减演化特性。

Result: 该方法能够基于有限数据点和较少参数，有效描述相空间演化。

Conclusion: 所提出的框架为仅使用可观测变量进行耗散系统演化预测提供了有效解决方案，具有参数效率高和数据需求少的优势。

Abstract: Much attention has recently been devoted to data-based computing of evolution
of physical systems. In such approaches, information about data points from
past trajectories in phase space is used to reconstruct the equations of motion
and to predict future solutions that have not been observed before. However, in
many cases, the available data does not correspond to the variables that define
the system's phase space. We focus our attention on the important example of
dissipative dynamical systems. In that case, the phase space consists of
coordinates, momenta and entropies; however, the momenta and entropies cannot,
in general, be observed directly. To address this difficulty, we develop an
efficient data-based computing framework based exclusively on observable
variables, by constructing a novel approach based on the \emph{thermodynamic
Lagrangian}, and constructing neural networks that respect the thermodynamics
and guarantees the non-decreasing entropy evolution. We show that our network
can provide an efficient description of phase space evolution based on a
limited number of data points and a relatively small number of parameters in
the system.

</details>


### [151] [LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios](https://arxiv.org/abs/2509.09926)
*Jiahao Chen,Zhiyuan Huang,Yurou Liu,Bing Su*

Main category: cs.LG

TL;DR: 提出了LoFT框架，通过参数高效微调基础模型来解决长尾半监督学习问题，并在开放世界场景下扩展为LoFT-OW，显著提升了性能


<details>
  <summary>Details</summary>
Motivation: 现有长尾半监督学习方法从零训练模型存在过度自信和伪标签质量低的问题，需要利用基础模型的强大能力来改善长尾学习效果

Method: 将长尾半监督学习扩展到基础模型微调范式，提出LoFT框架进行参数高效微调，生成更可靠的伪标签；针对开放世界场景提出LoFT-OW提升判别能力

Result: 在多个基准测试中取得了优于先前方法的性能，即使仅使用1%的未标记数据也能超越之前的工作

Conclusion: 通过基础模型微调可以有效解决长尾半监督学习问题，在开放世界场景下也具有很好的适应性，为实际应用提供了有效解决方案

Abstract: Long-tailed learning has garnered increasing attention due to its wide
applicability in real-world scenarios. Among existing approaches, Long-Tailed
Semi-Supervised Learning (LTSSL) has emerged as an effective solution by
incorporating a large amount of unlabeled data into the imbalanced labeled
dataset. However, most prior LTSSL methods are designed to train models from
scratch, which often leads to issues such as overconfidence and low-quality
pseudo-labels. To address these challenges, we extend LTSSL into the foundation
model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed
semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate
that fine-tuned foundation models can generate more reliable pseudolabels,
thereby benefiting imbalanced learning. Furthermore, we explore a more
practical setting by investigating semi-supervised learning under open-world
conditions, where the unlabeled data may include out-of-distribution (OOD)
samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World
scenarios) to improve the discriminative ability. Experimental results on
multiple benchmarks demonstrate that our method achieves superior performance
compared to previous approaches, even when utilizing only 1\% of the unlabeled
data compared with previous works.

</details>


### [152] [Multi-Play Combinatorial Semi-Bandit Problem](https://arxiv.org/abs/2509.09933)
*Shintaro Nakamura,Yuko Kuroki,Wei Chen*

Main category: cs.LG

TL;DR: 这篇论文提出了多次组合半户勒机(MP-CSB)模型，解决了传统组合半户勒机仅能处理二进制决策空间的限制，为流量分配和背包问题等应用提供了更好的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统组合半户勒机(CSB)仅支持二进制决策空间，无法处理非负整数流量或分配问题，如最优运输和背包问题。需要扩展模型以涵盖更广泛的应用场景。

Method: 提出多次组合半户勒机(MP-CSB)模型，允许选择非负整数动作并从单个臂获得多个反馈。提出两种算法：基于Thompson采样的高效算法，以及适用于随机和对抗两种情况的最佳两界算法。

Result: Thompson采样算法在随机渡勒中达到O(log T)分布依赖后悔；最佳两界算法在随机渡勒中达到O(log T)方差依赖后悔，在对抗渡勒中达到最坏情况的次线性后悔，并能根据损失序列特征进行数据依赖适应。数值实验证明算法性能超过现有方法。

Conclusion: MP-CSB模型有效扩展了组合半户勒机的应用范围，为处理非负整数流量分配问题提供了高效的解决方案。提出的两种算法在理论性能和实践效果上都表现优异，将推动组合优化问题在学习领域的进一步发展。

Abstract: In the combinatorial semi-bandit (CSB) problem, a player selects an action
from a combinatorial action set and observes feedback from the base arms
included in the action. While CSB is widely applicable to combinatorial
optimization problems, its restriction to binary decision spaces excludes
important cases involving non-negative integer flows or allocations, such as
the optimal transport and knapsack problems.To overcome this limitation, we
propose the multi-play combinatorial semi-bandit (MP-CSB), where a player can
select a non-negative integer action and observe multiple feedbacks from a
single arm in each round. We propose two algorithms for the MP-CSB. One is a
Thompson-sampling-based algorithm that is computationally feasible even when
the action space is exponentially large with respect to the number of arms, and
attains $O(\log T)$ distribution-dependent regret in the stochastic regime,
where $T$ is the time horizon. The other is a best-of-both-worlds algorithm,
which achieves $O(\log T)$ variance-dependent regret in the stochastic regime
and the worst-case $\tilde{\mathcal{O}}\left( \sqrt{T} \right)$ regret in the
adversarial regime. Moreover, its regret in adversarial one is data-dependent,
adapting to the cumulative loss of the optimal action, the total quadratic
variation, and the path-length of the loss sequence. Finally, we numerically
show that the proposed algorithms outperform existing methods in the CSB
literature.

</details>


### [153] [SciML Agents: Write the Solver, Not the Solution](https://arxiv.org/abs/2509.09936)
*Saarth Gaonkar,Xiang Zheng,Haocheng Xi,Rishabh Tiwari,Kurt Keutzer,Dmitriy Morozov,Michael W. Mahoney,Amir Gholami*

Main category: cs.LG

TL;DR: 这篇论文探索了使用LLM作为科学机器学习代理，通过生成代码来解决常微分方程问题，而非直接预测解决方案。研究介绍了两个新的数据集来评估LLM在科学计算任务中的能力，并发现通过精心设计的提示和微调，LLM能够可靠地解决简单的ODE问题。


<details>
  <summary>Details</summary>
Motivation: 尽管现有科学机器学习方法直接预测目标值，但达到高精度和稳健性一直是挑战。研究者探索了一种替代方案：利用LLM编写代码来派生数值算法，将负担从学习解决方案转移到做出领域知识驱动的数值选择。

Method: 研究介绍了两个新数据集：一个诊断性的对抗性"误导"问题集，和一个包含1000个多样化ODE任务的大规模测试集。评估了开源和闭源LLM模型，测量了代码的可执行性和数值有效性。研究了两个方向：(i)无指导与领域知识指导的提示；(ii)开箱即用与细调变体。

Result: 研究发现，在充分上下文和精心设计的指导提示下，新的指令跟随模型在代码可执行性和数值有效性上都达到了高精度。在许多情况下，最新的开源系统在不需细调的情况下表现强劲，而更旧或更小的模型仍能从细调中获益。

Conclusion: 初步结果表明，通过精心的提示设计和微调，可以培育出一个专门的LLM代理，能够可靠地解决简单的ODE问题。这为科学计算任务中使用LLM提供了有前景的方向。

Abstract: Recent work in scientific machine learning aims to tackle scientific tasks
directly by predicting target values with neural networks (e.g.,
physics-informed neural networks, neural ODEs, neural operators, etc.), but
attaining high accuracy and robustness has been challenging. We explore an
alternative view: use LLMs to write code that leverages decades of numerical
algorithms. This shifts the burden from learning a solution function to making
domain-aware numerical choices. We ask whether LLMs can act as SciML agents
that, given a natural-language ODE description, generate runnable code that is
scientifically appropriate, selecting suitable solvers (stiff vs. non-stiff),
and enforcing stability checks. There is currently no benchmark to measure this
kind of capability for scientific computing tasks. As such, we first introduce
two new datasets: a diagnostic dataset of adversarial "misleading" problems;
and a large-scale benchmark of 1,000 diverse ODE tasks. The diagnostic set
contains problems whose superficial appearance suggests stiffness, and that
require algebraic simplification to demonstrate non-stiffness; and the
large-scale benchmark spans stiff and non-stiff ODE regimes. We evaluate open-
and closed-source LLM models along two axes: (i) unguided versus guided
prompting with domain-specific knowledge; and (ii) off-the-shelf versus
fine-tuned variants. Our evaluation measures both executability and numerical
validity against reference solutions. We find that with sufficient context and
guided prompts, newer instruction-following models achieve high accuracy on
both criteria. In many cases, recent open-source systems perform strongly
without fine-tuning, while older or smaller models still benefit from
fine-tuning. Overall, our preliminary results indicate that careful prompting
and fine-tuning can yield a specialized LLM agent capable of reliably solving
simple ODE problems.

</details>


### [154] [DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition](https://arxiv.org/abs/2509.09940)
*Yifei Wang,Wenbin Wang,Yong Luo*

Main category: cs.LG

TL;DR: DyKen-Hyena通过将音频-视觉线索转换为动态的每token卷积核来直接调制文本特征提取，避免了传统多模态融合中的噪声干扰问题，在MIR任务中实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态意图识别方法通过多头注意力等机制融合多模态特征，但可能因模态间存在意图无关和冲突信息而损害主要语言特征，需要更细粒度的调制而非简单融合。

Method: 提出DyKen-Hyena模型，将问题从特征融合重新定义为处理调制，将音频-视觉线索转换为动态的每token卷积核来直接调制文本特征提取过程。

Result: 在MIntRec和MIntRec2.0基准测试中达到最先进结果，特别在超出范围检测中获得+10.46% F1分数提升。

Conclusion: 该方法创建了更鲁棒的意图表示，验证了通过动态卷积核进行细粒度调制的有效性。

Abstract: Though Multimodal Intent Recognition (MIR) proves effective by utilizing rich
information from multiple sources (e.g., language, video, and audio), the
potential for intent-irrelevant and conflicting information across modalities
may hinder performance from being further improved. Most current models attempt
to fuse modalities by applying mechanisms like multi-head attention to unimodal
feature sequences and then adding the result back to the original
representation. This process risks corrupting the primary linguistic features
with noisy or irrelevant non-verbal signals, as it often fails to capture the
fine-grained, token-level influence where non-verbal cues should modulate, not
just augment, textual meaning. To address this, we introduce DyKen-Hyena, which
reframes the problem from feature fusion to processing modulation. Our model
translates audio-visual cues into dynamic, per-token convolutional kernels that
directly modulate textual feature extraction. This fine-grained approach
achieves state-of-the-art results on the MIntRec and MIntRec2.0 benchmarks.
Notably, it yields a +10.46% F1-score improvement in out-of-scope detection,
validating that our method creates a fundamentally more robust intent
representation.

</details>


### [155] [Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge](https://arxiv.org/abs/2509.09955)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis,Sami Muhaidat*

Main category: cs.LG

TL;DR: 这篇论文提出了一种无需重新训练的适应性token合并框架，通过动态合并语义冗余的token来压缩Transformer模型，在保持准确性的同时大幅降低计算和通信成本。


<details>
  <summary>Details</summary>
Motivation: 解决大规模Transformer模型在边缘设备上部署时遍历的高计算和通信成本问题，提供一种灵活的压缩方案以适应不同资源约束和输入特征。

Method: 提出适应性token合并机制，基于每层相似性阈值选择性合并语义冗余token；将合并策略发现派生为多目标优化问题，利用贝叶斯优化寻找准确性、推理成本和通信成本之间的Pareto最优偏微。

Result: 在ImageNet分类任务中，在准确性不变的情况下减少30%的FLOPs和80%以上的通信成本；在VQA任务中以不过三分之一的计算成本和十分之一的带宽达到与原模型相当的性能。方法还具有潜在隐私保护效果。

Conclusion: 该框架为在资源有限的边缘智能场景中部署Transformer模型提供了一种实用且灵活的解决方案，能够在保持任务性能的同时显著降低计算和通信开销。

Abstract: Large-scale transformers are central to modern semantic communication, yet
their high computational and communication costs hinder deployment on
resource-constrained edge devices. This paper introduces a training-free
framework for adaptive token merging, a novel mechanism that compresses
transformer representations at runtime by selectively merging semantically
redundant tokens under per-layer similarity thresholds. Unlike prior
fixed-ratio reduction, our approach couples merging directly to input
redundancy, enabling data-dependent adaptation that balances efficiency and
task relevance without retraining. We cast the discovery of merging strategies
as a multi-objective optimization problem and leverage Bayesian optimization to
obtain Pareto-optimal trade-offs between accuracy, inference cost, and
communication cost. On ImageNet classification, we match the accuracy of the
unmodified transformer with 30\% fewer floating-point operations per second and
under 20\% of the original communication cost, while for visual question
answering our method achieves performance competitive with the full LLaVA model
at less than one-third of the compute and one-tenth of the bandwidth. Finally,
we show that our adaptive merging is robust across varying channel conditions
and provides inherent privacy benefits, substantially degrading the efficacy of
model inversion attacks. Our framework provides a practical and versatile
solution for deploying powerful transformer models in resource-limited edge
intelligence scenarios.

</details>


### [156] [Limited Reference, Reliable Generation: A Two-Component Framework for Tabular Data Generation in Low-Data Regimes](https://arxiv.org/abs/2509.09960)
*Mingxuan Jiang,Yongxin Wang,Ziyue Dai,Yicun Liu,Hongyi Nie,Sen Liu,Hongfeng Chai*

Main category: cs.LG

TL;DR: ReFine框架通过从可解释模型提取符号规则嵌入提示词，并采用双粒度过滤策略，在数据稀缺场景下显著提升合成表格数据生成质量


<details>
  <summary>Details</summary>
Motivation: 解决现有表格生成方法在领域特定数据库中数据稀缺时的局限性，以及提示式LLM无法有效捕捉数据集特定特征-标签依赖关系的问题

Method: 从可解释模型推导符号"if-then"规则嵌入提示词，并应用双粒度过滤策略抑制过采样模式并精炼稀有样本

Result: 在多个回归和分类基准测试中 consistently 优于最先进方法，回归任务R平方绝对提升0.44，分类任务F1分数相对提升10.0%

Conclusion: ReFine框架有效解决了数据稀缺场景下的表格生成问题，通过规则引导和智能过滤显著提升了生成数据的质量和下游任务性能

Abstract: Synthetic tabular data generation is increasingly essential in data
management, supporting downstream applications when real-world and high-quality
tabular data is insufficient. Existing tabular generation approaches, such as
generative adversarial networks (GANs), diffusion models, and fine-tuned Large
Language Models (LLMs), typically require sufficient reference data, limiting
their effectiveness in domain-specific databases with scarce records. While
prompt-based LLMs offer flexibility without parameter tuning, they often fail
to capture dataset-specific feature-label dependencies and generate redundant
data, leading to degradation in downstream task performance. To overcome these
issues, we propose ReFine, a framework that (i) derives symbolic "if-then"
rules from interpretable models and embeds them into prompts to explicitly
guide generation toward domain-specific feature distribution, and (ii) applies
a dual-granularity filtering strategy that suppresses over-sampling patterns
and selectively refines rare but informative samples to reduce distributional
imbalance. Extensive experiments on various regression and classification
benchmarks demonstrate that ReFine consistently outperforms state-of-the-art
methods, achieving up to 0.44 absolute improvement in R-squared for regression
and 10.0 percent relative improvement in F1 score for classification tasks.

</details>


### [157] [Data-Driven Energy Estimation for Virtual Servers Using Combined System Metrics and Machine Learning](https://arxiv.org/abs/2509.09991)
*Amandip Sangha*

Main category: cs.LG

TL;DR: 通过机器学习方法，仅使用虚拟机资源利用率指标来预测能消耗，无需物理能消测量接口


<details>
  <summary>Details</summary>
Motivation: 解决虚拟化环境（如云计算）中无法直接测量能消的问题，提供一种仅依赖客户端资源数据的能消估算方法

Method: 使用Gradient Boosting Regressor算法，基于虚拟机客户端收集的资源利用率指标进行训练，预测通过RAPL测量的主机能消

Result: 在多种工作负载下实验，得到高准确率和高方差解释率（0.90 ≤ R² ≤ 0.97），证明了客户端能消估算的可行性

Conclusion: 该方法可以实现能消意识调度、成本优化和不依赖主机的能消估算，为虚拟化环境提供了一种新的能消监测方案

Abstract: This paper presents a machine learning-based approach to estimate the energy
consumption of virtual servers without access to physical power measurement
interfaces. Using resource utilization metrics collected from guest virtual
machines, we train a Gradient Boosting Regressor to predict energy consumption
measured via RAPL on the host. We demonstrate, for the first time, guest-only
resource-based energy estimation without privileged host access with
experiments across diverse workloads, achieving high predictive accuracy and
variance explained ($0.90 \leq R^2 \leq 0.97$), indicating the feasibility of
guest-side energy estimation. This approach can enable energy-aware scheduling,
cost optimization and physical host independent energy estimates in virtualized
environments. Our approach addresses a critical gap in virtualized environments
(e.g. cloud) where direct energy measurement is infeasible.

</details>


### [158] [Neural Scaling Laws for Deep Regression](https://arxiv.org/abs/2509.10000)
*Tilen Cadez,Kyoung-Min Kim*

Main category: cs.LG

TL;DR: 本文实证研究了深度回归模型中的神经缩放定律，发现在扭曲范德瓦尔斯磁体的参数估计模型中，损失与训练数据集大小和模型容量之间存在幂律关系，缩放指数范围为1到2。


<details>
  <summary>Details</summary>
Motivation: 神经缩放定律对于在有限资源下开发可靠模型至关重要，但现有研究主要关注大型语言模型，深度回归模型中的缩放定律尚未充分探索。

Method: 使用扭曲范德瓦尔斯磁体的参数估计模型，采用全连接网络、残差网络和视觉变换器等多种架构，在不同数据集大小和模型容量下进行实验。

Result: 观察到损失与训练数据集大小和模型容量之间存在幂律关系，缩放指数范围为1到2，具体值取决于回归参数和模型细节。

Conclusion: 一致的缩放行为和大缩放指数表明，深度回归模型的性能可以随着数据量的增加而显著提升。

Abstract: Neural scaling laws--power-law relationships between generalization errors
and characteristics of deep learning models--are vital tools for developing
reliable models while managing limited resources. Although the success of large
language models highlights the importance of these laws, their application to
deep regression models remains largely unexplored. Here, we empirically
investigate neural scaling laws in deep regression using a parameter estimation
model for twisted van der Waals magnets. We observe power-law relationships
between the loss and both training dataset size and model capacity across a
wide range of values, employing various architectures--including fully
connected networks, residual networks, and vision transformers. Furthermore,
the scaling exponents governing these relationships range from 1 to 2, with
specific values depending on the regressed parameters and model details. The
consistent scaling behaviors and their large scaling exponents suggest that the
performance of deep regression models can improve substantially with increasing
data size.

</details>


### [159] [Intrinsic Dimension Estimating Autoencoder (IDEA) Using CancelOut Layer and a Projected Loss](https://arxiv.org/abs/2509.10011)
*Antoine Orioua,Philipp Krah,Julian Koellermeier*

Main category: cs.LG

TL;DR: IDEA是一种能够估计数据集内在维度的自编码器，通过投影重建损失项指导训练，在理论和实际流体力学数据上都表现出良好的准确性和重建能力


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确估计非线性流形数据的内在维度，且缺乏重建能力。需要一种既能准确估计内在维度又能重建原始数据的通用方法

Method: 使用重加权双CancelOut层构建潜在空间，引入投影重建损失项来评估去除潜在维度后的重建质量，指导模型训练

Result: 在理论基准测试中表现出良好的准确性和鲁棒性，在流体力学数值解数据上成功估计内在维度并重建原始解

Conclusion: IDEA是一种通用且准确的内在维度估计方法，能够同时处理线性和非线性流形，并具备重建能力，在复杂科学数据上有很好的应用前景

Abstract: This paper introduces the Intrinsic Dimension Estimating Autoencoder (IDEA),
which identifies the underlying intrinsic dimension of a wide range of datasets
whose samples lie on either linear or nonlinear manifolds. Beyond estimating
the intrinsic dimension, IDEA is also able to reconstruct the original dataset
after projecting it onto the corresponding latent space, which is structured
using re-weighted double CancelOut layers. Our key contribution is the
introduction of the projected reconstruction loss term, guiding the training of
the model by continuously assessing the reconstruction quality under the
removal of an additional latent dimension. We first assess the performance of
IDEA on a series of theoretical benchmarks to validate its robustness. These
experiments allow us to test its reconstruction ability and compare its
performance with state-of-the-art intrinsic dimension estimators. The
benchmarks show good accuracy and high versatility of our approach.
Subsequently, we apply our model to data generated from the numerical solution
of a vertically resolved one-dimensional free-surface flow, following a
pointwise discretization of the vertical velocity profile in the horizontal
direction, vertical direction, and time. IDEA succeeds in estimating the
dataset's intrinsic dimension and then reconstructs the original solution by
working directly within the projection space identified by the network.

</details>


### [160] [Exploring Expert Specialization through Unsupervised Training in Sparse Mixture of Experts](https://arxiv.org/abs/2509.10025)
*Strahinja Nikolic,Ilker Oguz,Demetri Psaltis*

Main category: cs.LG

TL;DR: 提出稀疏专家混合变分自编码器(SMoE-VAE)，在QuickDraw数据集上发现无监督路由比有监督基线重建性能更好，专家学习到超越人工类别边界的有意义子类别结构


<details>
  <summary>Details</summary>
Motivation: 理解神经网络内部组织是深度学习可解释性的基本挑战，需要探索新的架构来揭示数据的内在结构

Method: 使用稀疏专家混合变分自编码器(SMoE-VAE)，在QuickDraw数据集上比较无监督专家路由和有监督基线，通过t-SNE可视化和重建分析研究模型结构

Result: 无监督路由始终获得更好的重建性能，专家学习到超越人工类别边界的有意义子类别结构，数据集大小研究揭示了数据量与专家专业化之间的权衡

Conclusion: MoE模型能够发现比预定义标签更符合模型目标的基本数据结构，为设计高效MoE架构提供了指导

Abstract: Understanding the internal organization of neural networks remains a
fundamental challenge in deep learning interpretability. We address this
challenge by exploring a novel Sparse Mixture of Experts Variational
Autoencoder (SMoE-VAE) architecture. We test our model on the QuickDraw
dataset, comparing unsupervised expert routing against a supervised baseline
guided by ground-truth labels. Surprisingly, we find that unsupervised routing
consistently achieves superior reconstruction performance. The experts learn to
identify meaningful sub-categorical structures that often transcend
human-defined class boundaries. Through t-SNE visualizations and reconstruction
analysis, we investigate how MoE models uncover fundamental data structures
that are more aligned with the model's objective than predefined labels.
Furthermore, our study on the impact of dataset size provides insights into the
trade-offs between data quantity and expert specialization, offering guidance
for designing efficient MoE architectures.

</details>


### [161] [Sparse Coding Representation of 2-way Data](https://arxiv.org/abs/2509.10033)
*Boya Ma,Abram Magner,Maxwell McNeil,Petko Bogdanov*

Main category: cs.LG

TL;DR: 提出AODL方法，通过低秩编码模型解决多字典学习中的数据复杂度问题，在保证重建质量的同时获得更稀疏的解


<details>
  <summary>Details</summary>
Motivation: 解决多字典场景下同时学习字典和编码系数的挑战，特别是编码系数对应所有字典原子组合时的复杂度问题

Method: 提出低秩编码模型，使用凸松弛解决方案AODL，通过稀疏编码矩阵和学习字典之间的交替优化进行求解

Result: AODL相比非低秩和固定字典基线，在相同重建质量下学习到稀疏度提高90%的解决方案，并在合成和真实数据集上展示了良好的重建和缺失值填补性能

Conclusion: AODL方法有效解决了多字典学习的数据复杂度问题，不仅提供更稀疏的解决方案，还能通过学习到的字典揭示训练样本中的可解释模式

Abstract: Sparse dictionary coding represents signals as linear combinations of a few
dictionary atoms. It has been applied to images, time series, graph signals and
multi-way spatio-temporal data by jointly employing temporal and spatial
dictionaries. Data-agnostic analytical dictionaries, such as the discrete
Fourier transform, wavelets and graph Fourier, have seen wide adoption due to
efficient implementations and good practical performance. On the other hand,
dictionaries learned from data offer sparser and more accurate solutions but
require learning of both the dictionaries and the coding coefficients. This
becomes especially challenging for multi-dictionary scenarios since encoding
coefficients correspond to all atom combinations from the dictionaries. To
address this challenge, we propose a low-rank coding model for 2-dictionary
scenarios and study its data complexity. Namely, we establish a bound on the
number of samples needed to learn dictionaries that generalize to unseen
samples from the same distribution. We propose a convex relaxation solution,
called AODL, whose exact solution we show also solves the original problem. We
then solve this relaxation via alternating optimization between the sparse
coding matrices and the learned dictionaries, which we prove to be convergent.
We demonstrate its quality for data reconstruction and missing value imputation
in both synthetic and real-world datasets. For a fixed reconstruction quality,
AODL learns up to 90\% sparser solutions compared to non-low-rank and
analytical (fixed) dictionary baselines. In addition, the learned dictionaries
reveal interpretable insights into patterns present within the samples used for
training.

</details>


### [162] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: FSPO是一种序列级强化学习方法，通过在重要性采样权重空间实施长度公平裁剪来解决PPO/GRPO方法在序列长度处理上的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有序列级RL方法在移植PPO/GRPO式裁剪时存在固定裁剪范围系统性地重加权短响应与长响应的问题，导致有效目标失真。

Method: 提出FSPO方法，采用高斯启发式解决方案：用KL校正漂移项和√L缩放的带对序列对数IS比率进行裁剪。

Result: FSPO在多个评估数据集上平坦化了长度分箱的裁剪率，稳定了训练过程，并优于所有基线方法。

Conclusion: FSPO通过理论形式化的长度公平性（LRE）和实际有效的裁剪策略，为序列级RL提供了长度公平的优化方法。

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [163] [Symbolic Feedforward Networks for Probabilistic Finite Automata: Exact Simulation and Learnability](https://arxiv.org/abs/2509.10034)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: 本文提出了一个形式化理论，证明概率有限自动机(PFAs)可以通过符号前馈神经网络精确模拟，建立了概率自动机理论与神经架构的代数统一框架。


<details>
  <summary>Details</summary>
Motivation: 弥合符号计算与深度学习之间的差距，将概率自动机理论与神经网络架构在严格的代数框架下统一起来。

Method: 使用符号前馈神经网络架构，将状态分布表示为向量，转移表示为随机矩阵，通过矩阵-向量乘积实现概率状态传播，采用软更新而非递归的并行可解释可微分模拟。

Result: 证明了PFAs与特定类别神经网络的等价性，展示了这些符号模拟器不仅具有表达性而且可学习：通过标准梯度下降优化在标记序列数据上训练，能够精确恢复真实PFAs的行为。

Conclusion: 该工作通过命题5.1形式化了可学习性这一核心贡献，为概率自动机理论与神经架构的统一提供了严格的代数基础，成功桥接了符号计算与深度学习领域。

Abstract: We present a formal and constructive theory showing that probabilistic finite
automata (PFAs) can be exactly simulated using symbolic feedforward neural
networks. Our architecture represents state distributions as vectors and
transitions as stochastic matrices, enabling probabilistic state propagation
via matrix-vector products. This yields a parallel, interpretable, and
differentiable simulation of PFA dynamics using soft updates-without
recurrence. We formally characterize probabilistic subset construction,
$\varepsilon$-closure, and exact simulation via layered symbolic computation,
and prove equivalence between PFAs and specific classes of neural networks. We
further show that these symbolic simulators are not only expressive but
learnable: trained with standard gradient descent-based optimization on labeled
sequence data, they recover the exact behavior of ground-truth PFAs. This
learnability, formalized in Proposition 5.1, is the crux of this work. Our
results unify probabilistic automata theory with neural architectures under a
rigorous algebraic framework, bridging the gap between symbolic computation and
deep learning.

</details>


### [164] [AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings](https://arxiv.org/abs/2509.09470)
*Om Vishesh,Harshad Khadilkar,Deepak Akkil*

Main category: cs.LG

TL;DR: 一种全自动化的AI系统，利用专门设计的Agent-E代理从会议议程中识别特定地理区域的论文，并通过RPA完成预定操作，实现了超高的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决学术文献快速增长带来的挑战，减少研究人员、资助机构和学术社会在学术发现过程中的手动劳动强度。

Method: 开发了一种专门的AI代理'Agent-E'，构建了从数据发现到直接执行的自动化流程，包括使用机器人过程自动化（RPA）来完成预定动作。

Result: 在5个不同会议的586篇论文上验证，系统成功识别了所有目标论文（回收率100%），准确率达到99.4%。

Conclusion: 该系统显示了任务导向的AI代理不仅能够过滤信息，还能积极参与并加速学术社区的工作流程。

Abstract: Keeping pace with the rapid growth of academia literature presents a
significant challenge for researchers, funding bodies, and academic societies.
To address the time-consuming manual effort required for scholarly discovery,
we present a novel, fully automated system that transitions from data discovery
to direct action. Our pipeline demonstrates how a specialized AI agent,
'Agent-E', can be tasked with identifying papers from specific geographic
regions within conference proceedings and then executing a Robotic Process
Automation (RPA) to complete a predefined action, such as submitting a
nomination form. We validated our system on 586 papers from five different
conferences, where it successfully identified every target paper with a recall
of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the
potential of task-oriented AI agents to not only filter information but also to
actively participate in and accelerate the workflows of the academic community.

</details>


### [165] [FedRP: A Communication-Efficient Approach for Differentially Private Federated Learning Using Random Projection](https://arxiv.org/abs/2509.10041)
*Mohammad Hasan Narimani,Mostafa Tavassolipour*

Main category: cs.LG

TL;DR: FedRP是一种新颖的联邦学习算法，通过随机投影技术和ADMM优化框架结合，在保护隐私的同时降低通信成本，并提供强差分隐私保证。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在物联网和医疗数据分析等敏感领域具有重要价值，但面临用户隐私保护和通信成本管理的挑战。需要一种既能保护隐私又能降低通信开销的解决方案。

Method: 提出FedRP算法，将随机投影技术与ADMM优化框架集成。使用随机投影降低模型参数的维度后再传输到中央服务器，减少通信成本。

Result: 实验结果表明FedRP不仅保持高模型准确性，而且在隐私保护和通信效率方面优于现有方法，包括传统差分隐私方法和FedADMM。

Conclusion: FedRP算法通过随机投影和ADMM的有机结合，成功解决了联邦学习中的隐私保护和通信效率问题，提供了强大的(ε,δ)-差分隐私保证。

Abstract: Federated learning (FL) offers an innovative paradigm for collaborative model
training across decentralized devices, such as smartphones, balancing enhanced
predictive performance with the protection of user privacy in sensitive areas
like Internet of Things (IoT) and medical data analysis. Despite its
advantages, FL encounters significant challenges related to user privacy
protection against potential attacks and the management of communication costs.
This paper introduces a novel federated learning algorithm called FedRP, which
integrates random projection techniques with the Alternating Direction Method
of Multipliers (ADMM) optimization framework. This approach enhances privacy by
employing random projection to reduce the dimensionality of model parameters
prior to their transmission to a central server, reducing the communication
cost. The proposed algorithm offers a strong $(\epsilon, \delta)$-differential
privacy guarantee, demonstrating resilience against data reconstruction
attacks. Experimental results reveal that FedRP not only maintains high model
accuracy but also outperforms existing methods, including conventional
differential privacy approaches and FedADMM, in terms of both privacy
preservation and communication efficiency.

</details>


### [166] [Uncertainty-Aware Tabular Prediction: Evaluating VBLL-Enhanced TabPFN in Safety-Critical Medical Data](https://arxiv.org/abs/2509.10048)
*Madhushan Ramalingam*

Main category: cs.LG

TL;DR: 评估VBLL集成TabPFN在表格数据不确定性检验中的性能，结果显示原版TabPFN在所有医疗数据集上均更优


<details>
  <summary>Details</summary>
Motivation: 在医疗诊断等安全关键应用中，需要可靠的不确定性估计，而TabPFN作为新的基础模型，VBLL是轻量级的变分方法，本研究旨在评估二者结合的效果

Method: 在三个标准医疗表格数据集上对比原版TabPFN和VBLL集成版本的不确定性检验性能

Result: 与预期相反，原版TabPFN在所有数据集上均比VBLL集成版本表现更好

Conclusion: 虽然VBLL在其他模型中显示出良好的不确定性估计能力，但与TabPFN结合后却没有提升效果，TabPFN自身已具备优秀的不确定性检验能力

Abstract: Predictive models are being increasingly used across a wide range of domains,
including safety-critical applications such as medical diagnosis and criminal
justice. Reliable uncertainty estimation is a crucial task in such settings.
Tabular Prior-data Fitted Network (TabPFN) is a recently proposed machine
learning foundation model for tabular dataset, which uses a generative
transformer architecture. Variational Bayesian Last Layers (VBLL) is a
state-of-the-art lightweight variational formulation that effectively improves
uncertainty estimation with minimal computational overhead. In this work we aim
to evaluate the performance of VBLL integrated with the recently proposed
TabPFN in uncertainty calibration. Our experiments, conducted on three
benchmark medical tabular datasets, compare the performance of the original
TabPFN and the VBLL-integrated version. Contrary to expectations, we observed
that original TabPFN consistently outperforms VBLL integrated TabPFN in
uncertainty calibration across all datasets.

</details>


### [167] [KAN-SR: A Kolmogorov-Arnold Network Guided Symbolic Regression Framework](https://arxiv.org/abs/2509.10089)
*Marco Andrea Bühler,Gonzalo Guillén-Gosálbez*

Main category: cs.LG

TL;DR: KAN-SR是一个基于Kolmogorov Arnold Networks的符号回归框架，采用分治方法，能够准确恢复Feynman SRSD数据集中的真实方程，并可结合神经控制微分方程精确建模生物过程系统动力学。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归通常使用遗传编程方法，本文旨在利用深度学习技术、专门的KAN网络和简化策略来改进符号回归的准确性和效率。

Method: 使用Kolmogorov Arnold Networks (KANs)构建符号回归框架，采用分治方法，结合平移对称性和可分离性等简化策略，并与神经控制微分方程结合。

Result: 成功恢复了Feynman SRSD数据集中的真实方程，并能够精确建模硅内生物过程系统的动力学。

Conclusion: KAN-SR框架为工程系统的动态建模开辟了新途径，展示了深度学习在符号回归中的潜力。

Abstract: We introduce a novel symbolic regression framework, namely KAN-SR, built on
Kolmogorov Arnold Networks (KANs) which follows a divide-and-conquer approach.
Symbolic regression searches for mathematical equations that best fit a given
dataset and is commonly solved with genetic programming approaches. We show
that by using deep learning techniques, more specific KANs, and combining them
with simplification strategies such as translational symmetries and
separabilities, we are able to recover ground-truth equations of the Feynman
Symbolic Regression for Scientific Discovery (SRSD) dataset. Additionally, we
show that by combining the proposed framework with neural controlled
differential equations, we are able to model the dynamics of an in-silico
bioprocess system precisely, opening the door for the dynamic modeling of other
engineering systems.

</details>


### [168] [Cost-Free Personalization via Information-Geometric Projection in Bayesian Federated Learning](https://arxiv.org/abs/2509.10132)
*Nour Jamoussi,Giuseppe Serra,Photios A. Stavrou,Marios Kountouris*

Main category: cs.LG

TL;DR: 提出了一种基于信息几何投影的贝叶斯联邦学习个性化框架，通过将全局模型投影到用户本地模型的邻域，实现全局泛化与本地特化的可调节权衡。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯联邦学习方法通常依赖MCMC采样或变分推断，需要个性化机制来适应异构数据分布，但计算成本较高。

Method: 使用信息几何投影框架，将全局模型投影到用户本地模型的统计流形邻域，证明该投影等价于计算统计流形上的重心，获得闭式解实现零成本个性化。

Result: 在异构数据分布下的实证评估表明，该方法能有效平衡全局和本地性能，且计算开销最小。

Conclusion: 该信息几何投影框架为贝叶斯联邦学习提供了一种计算高效的个性化方法，实现了全局泛化与本地特化的最优权衡。

Abstract: Bayesian Federated Learning (BFL) combines uncertainty modeling with
decentralized training, enabling the development of personalized and reliable
models under data heterogeneity and privacy constraints. Existing approaches
typically rely on Markov Chain Monte Carlo (MCMC) sampling or variational
inference, often incorporating personalization mechanisms to better adapt to
local data distributions. In this work, we propose an information-geometric
projection framework for personalization in parametric BFL. By projecting the
global model onto a neighborhood of the user's local model, our method enables
a tunable trade-off between global generalization and local specialization.
Under mild assumptions, we show that this projection step is equivalent to
computing a barycenter on the statistical manifold, allowing us to derive
closed-form solutions and achieve cost-free personalization. We apply the
proposed approach to a variational learning setup using the Improved
Variational Online Newton (IVON) optimizer and extend its application to
general aggregation schemes in BFL. Empirical evaluations under heterogeneous
data distributions confirm that our method effectively balances global and
local performance with minimal computational overhead.

</details>


### [169] [BenchECG and xECG: a benchmark and baseline for ECG foundation models](https://arxiv.org/abs/2509.10151)
*Riccardo Lunelli,Angus Nicolson,Samuel Martin Pröll,Sebastian Johannes Reinstadler,Axel Bauer,Clemens Dlaska*

Main category: cs.LG

TL;DR: 提出了BenchECG标准化基准和xECG模型，xECG在ECG基础模型评估中表现最佳，为ECG表示学习建立了新的基准


<details>
  <summary>Details</summary>
Motivation: 现有ECG基础模型研究缺乏一致的评估标准，使用不同的任务和数据集，难以进行公平比较

Method: 开发BenchECG标准化基准，包含全面的公开ECG数据集和多样化任务；提出基于xLSTM和SimDINOv2自监督学习的xECG模型

Result: xECG在BenchECG基准测试中表现最佳，是唯一在所有数据集和任务上都表现优异的公开可用模型

Conclusion: BenchECG标准化评估促进了ECG基础模型的严格比较，xECG为未来ECG基础模型设立了新的性能基准

Abstract: Electrocardiograms (ECGs) are inexpensive, widely used, and well-suited to
deep learning. Recently, interest has grown in developing foundation models for
ECGs - models that generalise across diverse downstream tasks. However,
consistent evaluation has been lacking: prior work often uses narrow task
selections and inconsistent datasets, hindering fair comparison. Here, we
introduce BenchECG, a standardised benchmark comprising a comprehensive suite
of publicly available ECG datasets and versatile tasks. We also propose xECG,
an xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning,
which achieves the best BenchECG score compared to publicly available
state-of-the-art models. In particular, xECG is the only publicly available
model to perform strongly on all datasets and tasks. By standardising
evaluation, BenchECG enables rigorous comparison and aims to accelerate
progress in ECG representation learning. xECG achieves superior performance
over earlier approaches, defining a new baseline for future ECG foundation
models.

</details>


### [170] [FedBiF: Communication-Efficient Federated Learning via Bits Freezing](https://arxiv.org/abs/2509.10161)
*Shiwei Li,Qunwei Li,Haozhao Wang,Ruixuan Li,Jianbin Lin,Wenliang Zhong*

Main category: cs.LG

TL;DR: FedBiF是一种新颖的联邦学习框架，通过在本地训练期间直接学习量化模型参数，实现逐比特更新策略，显著减少通信开销同时保持模型精度


<details>
  <summary>Details</summary>
Motivation: 联邦学习存在显著的通信开销问题，现有量化方法通常在本地训练后应用量化，导致量化误差影响模型精度

Method: 服务器先量化模型参数并传输给客户端，每个客户端每次只更新多比特参数表示中的单个比特，冻结其余比特

Result: 在5个数据集上的实验表明，FedBiF在IID和非IID设置下都能实现优异的通信压缩，同时获得与FedAvg相当的精度，仅使用1bpp上行和3bpp下行通信

Conclusion: FedBiF通过逐比特更新策略有效解决了联邦学习的通信效率问题，在保持模型精度的同时显著降低了通信成本，并促进了模型稀疏性

Abstract: Federated learning (FL) is an emerging distributed machine learning paradigm
that enables collaborative model training without sharing local data. Despite
its advantages, FL suffers from substantial communication overhead, which can
affect training efficiency. Recent efforts have mitigated this issue by
quantizing model updates to reduce communication costs. However, most existing
methods apply quantization only after local training, introducing quantization
errors into the trained parameters and potentially degrading model accuracy. In
this paper, we propose Federated Bit Freezing (FedBiF), a novel FL framework
that directly learns quantized model parameters during local training. In each
communication round, the server first quantizes the model parameters and
transmits them to the clients. FedBiF then allows each client to update only a
single bit of the multi-bit parameter representation, freezing the remaining
bits. This bit-by-bit update strategy reduces each parameter update to one bit
while maintaining high precision in parameter representation. Extensive
experiments are conducted on five widely used datasets under both IID and
Non-IID settings. The results demonstrate that FedBiF not only achieves
superior communication compression but also promotes sparsity in the resulting
models. Notably, FedBiF attains accuracy comparable to FedAvg, even when using
only 1 bit-per-parameter (bpp) for uplink and 3 bpp for downlink communication.
The code is available at https://github.com/Leopold1423/fedbif-tpds25.

</details>


### [171] [Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks](https://arxiv.org/abs/2509.10163)
*Francisco Javier Esono Nkulu Andong,Qi Min*

Main category: cs.LG

TL;DR: 提出了一种联邦多智能体强化学习框架Fed-MARL，用于6G超密集边缘网络中的隐私保护、实时资源管理，通过跨层协同优化MAC层和应用层资源分配。


<details>
  <summary>Details</summary>
Motivation: 6G网络向超密集智能边缘环境发展，需要在严格隐私、移动性和能耗约束下实现高效资源管理，传统集中式方法面临隐私泄露和可扩展性问题。

Method: 采用联邦多智能体强化学习框架，每个智能体使用深度循环Q网络学习分散式策略，结合椭圆曲线Diffie-Hellman密钥交换的安全聚合协议保护隐私，将问题建模为部分可观测多智能体马尔可夫决策过程。

Result: 仿真结果表明Fed-MARL在任务成功率、延迟、能效和公平性方面优于集中式MARL和启发式基线方法，同时确保强大的隐私保护和动态6G边缘网络中的可扩展性。

Conclusion: Fed-MARL框架为6G边缘网络提供了一种有效的隐私保护、实时资源管理解决方案，能够满足URLLC、eMBB和mMTC等6G特定服务要求。

Abstract: As sixth-generation (6G) networks move toward ultra-dense, intelligent edge
environments, efficient resource management under stringent privacy, mobility,
and energy constraints becomes critical. This paper introduces a novel
Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that
incorporates cross-layer orchestration of both the MAC layer and application
layer for energy-efficient, privacy-preserving, and real-time resource
management across heterogeneous edge devices. Each agent uses a Deep Recurrent
Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum
access, and CPU energy adaptation based on local observations (e.g., queue
length, energy, CPU usage, and mobility). To protect privacy, we introduce a
secure aggregation protocol based on elliptic curve Diffie Hellman key
exchange, which ensures accurate model updates without exposing raw data to
semi-honest adversaries. We formulate the resource management problem as a
partially observable multi-agent Markov decision process (POMMDP) with a
multi-objective reward function that jointly optimizes latency, energy
efficiency, spectral efficiency, fairness, and reliability under 6G-specific
service requirements such as URLLC, eMBB, and mMTC. Simulation results
demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines
in task success rate, latency, energy efficiency, and fairness, while ensuring
robust privacy protection and scalability in dynamic, resource-constrained 6G
edge networks.

</details>


### [172] [A Symmetry-Integrated Approach to Surface Code Decoding](https://arxiv.org/abs/2509.10164)
*Hoshitaro Ohnishi,Hideo Mukai*

Main category: cs.LG

TL;DR: 提出了一种通过神经网络近似综合测量来重新优化表面码解码器的方法，解决了传统解码器因非唯一正确预测而只能获取误差概率分布的问题。


<details>
  <summary>Details</summary>
Motivation: 量子纠错码中表面码虽然具有高错误阈值，但传统解码器由于输入的非唯一性只能获取误差概率分布，无法准确预测错误。

Method: 使用神经网络将综合测量近似为连续函数进行数学插值，将表面码解码问题重新构建为回归问题，采用多层感知机、卷积神经网络、循环神经网络和Transformer等架构。

Result: 在码距5和7的所有测试中，重新优化的解码器都比原始模型具有更好的准确率，证明了该方法对码距和网络架构的普适有效性。

Conclusion: 将表面码解码问题重新构建为可通过深度学习解决的回归问题是一个有效的策略，神经网络插值方法能够显著提高解码精度。

Abstract: Quantum error correction, which utilizes logical qubits that are encoded as
redundant multiple physical qubits to find and correct errors in physical
qubits, is indispensable for practical quantum computing. Surface code is
considered to be a promising encoding method with a high error threshold that
is defined by stabilizer generators. However, previous methods have suffered
from the problem that the decoder acquires solely the error probability
distribution because of the non-uniqueness of correct prediction obtained from
the input. To circumvent this problem, we propose a technique to reoptimize the
decoder model by approximating syndrome measurements with a continuous function
that is mathematically interpolated by neural network. We evaluated the
improvement in accuracy of a multilayer perceptron based decoder for code
distances of 5 and 7 as well as for decoders based on convolutional and
recurrent neural networks and transformers for a code distance of 5. In all
cases, the reoptimized decoder gave better accuracy than the original models,
demonstrating the universal effectiveness of the proposed method that is
independent of code distance or network architecture. These results suggest
that re-framing the problem of surface code decoding into a regression problem
that can be tackled by deep learning is a useful strategy.

</details>


### [173] [The Hidden Width of Deep ResNets: Tight Error Bounds and Phase Diagrams](https://arxiv.org/abs/2509.10167)
*Lénaïc Chizat*

Main category: cs.LG

TL;DR: 该论文研究了深度残差网络在梯度训练下的动力学行为，证明了当深度趋于无穷时，训练过程收敛到神经平均ODE动力学，并分析了不同缩放参数对特征学习能力的影响。


<details>
  <summary>Details</summary>
Motivation: 研究深度残差网络在标准随机初始化下的梯度训练动力学，特别是当网络深度趋于无穷时的极限行为，以及不同参数缩放对特征学习能力的影响。

Method: 通过数学分析证明深度残差网络的训练动力学收敛到神经平均ODE，使用传播混沌理论分析单元间的渐近独立性，并通过实验验证误差界的紧致性。

Result: 获得了模型输出与极限之间的误差界O(1/L + α/√(LM))，证明了当α=Θ(1)时存在完全特征学习，而当α→∞时进入惰性ODE机制。对于两层感知器块，唯一导致完全特征学习的残差缩放是Θ(√D/(LM))。

Conclusion: 深度残差网络的训练动力学在深度趋于无穷时收敛到平均ODE，不同的残差缩放参数决定了特征学习的能力，为理解深度网络训练提供了新的数学视角。

Abstract: We study the gradient-based training of large-depth residual networks
(ResNets) from standard random initializations. We show that with a diverging
depth $L$, a fixed embedding dimension $D$, and an arbitrary hidden width $M$,
the training dynamics converges to a Neural Mean ODE training dynamics.
Remarkably, the limit is independent of the scaling of $M$, covering practical
cases of, say, Transformers, where $M$ (the number of hidden units or attention
heads per layer) is typically of the order of $D$. For a residual scale
$\Theta_D\big(\frac{\alpha}{LM}\big)$, we obtain the error bound
$O_D\big(\frac{1}{L}+ \frac{\alpha}{\sqrt{LM}}\big)$ between the model's output
and its limit after a fixed number gradient of steps, and we verify empirically
that this rate is tight. When $\alpha=\Theta(1)$, the limit exhibits complete
feature learning, i.e. the Mean ODE is genuinely non-linearly parameterized. In
contrast, we show that $\alpha \to \infty$ yields a \lazy ODE regime where the
Mean ODE is linearly parameterized. We then focus on the particular case of
ResNets with two-layer perceptron blocks, for which we study how these scalings
depend on the embedding dimension $D$. We show that for this model, the only
residual scale that leads to complete feature learning is
$\Theta\big(\frac{\sqrt{D}}{LM}\big)$. In this regime, we prove the error bound
$O\big(\frac{1}{L}+ \frac{\sqrt{D}}{\sqrt{LM}}\big)$ between the ResNet and its
limit after a fixed number of gradient steps, which is also empirically tight.
Our convergence results rely on a novel mathematical perspective on ResNets :
(i) due to the randomness of the initialization, the forward and backward pass
through the ResNet behave as the stochastic approximation of certain mean ODEs,
and (ii) by propagation of chaos (that is, asymptotic independence of the
units) this behavior is preserved through the training dynamics.

</details>


### [174] [P3D: Scalable Neural Surrogates for High-Resolution 3D Physics Simulations with Global Context](https://arxiv.org/abs/2509.10186)
*Benjamin Holzschuh,Georg Kohl,Florian Redinger,Nils Thuerey*

Main category: cs.LG

TL;DR: 提出了一种可扩展的框架，用于学习高分辨率3D物理模拟的确定性和概率性神经代理模型，采用混合CNN-Transformer架构，在速度和准确性上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决高分辨率3D物理模拟中计算成本高、内存需求大的问题，需要开发高效的神经代理模型来替代传统的数值模拟方法。

Method: 使用混合CNN-Transformer骨干架构，支持在小块模拟域上进行预训练，然后融合获得全局解，通过序列到序列模型包含长程依赖关系，降低内存和计算需求。

Result: 在14种不同类型3D PDE动力学学习任务中显著优于基线方法，可扩展到512^3空间分辨率的高分辨率各向同性湍流，并能作为扩散模型生成不同雷诺数下高度湍流3D通道流的概率样本。

Conclusion: 该框架为高分辨率3D物理模拟提供了一种高效、可扩展的神经代理解决方案，在保持准确性的同时大幅降低了计算成本。

Abstract: We present a scalable framework for learning deterministic and probabilistic
neural surrogates for high-resolution 3D physics simulations. We introduce a
hybrid CNN-Transformer backbone architecture targeted for 3D physics
simulations, which significantly outperforms existing architectures in terms of
speed and accuracy. Our proposed network can be pretrained on small patches of
the simulation domain, which can be fused to obtain a global solution,
optionally guided via a fast and scalable sequence-to-sequence model to include
long-range dependencies. This setup allows for training large-scale models with
reduced memory and compute requirements for high-resolution datasets. We
evaluate our backbone architecture against a large set of baseline methods with
the objective to simultaneously learn the dynamics of 14 different types of
PDEs in 3D. We demonstrate how to scale our model to high-resolution isotropic
turbulence with spatial resolutions of up to $512^3$. Finally, we demonstrate
the versatility of our network by training it as a diffusion model to produce
probabilistic samples of highly turbulent 3D channel flows across varying
Reynolds numbers, accurately capturing the underlying flow statistics.

</details>


### [175] [Hadamard-Riemannian Optimization for Margin-Variance Ensemble](https://arxiv.org/abs/2509.10189)
*Zexu Jin*

Main category: cs.LG

TL;DR: 提出了一种新的集成学习框架，通过将边际方差纳入损失函数并重新参数化集成权重到单位球面，解决了传统边际方法忽视方差问题和计算效率低下的限制。


<details>
  <summary>Details</summary>
Motivation: 传统基于边际的集成方法主要关注最大化期望边际而忽略边际方差的关键作用，这限制了模型的泛化能力并增加了过拟合风险，特别是在噪声或不平衡数据集中。同时，传统方法在概率单纯形中优化集成权重存在计算效率低和可扩展性挑战。

Method: 提出新颖的集成学习框架，将边际方差明确纳入损失函数，联合优化负期望边际及其方差。通过将集成权重重新参数化到单位球面，简化优化过程并提高计算效率。

Result: 在多个基准数据集上的广泛实验表明，所提出的方法 consistently 优于传统的基于边际的集成技术。

Conclusion: 该方法通过考虑边际方差和优化权重参数化，显著提高了集成学习的鲁棒性和泛化性能，同时提升了计算效率，具有实际应用价值。

Abstract: Ensemble learning has been widely recognized as a pivotal technique for
boosting predictive performance by combining multiple base models.
Nevertheless, conventional margin-based ensemble methods predominantly focus on
maximizing the expected margin while neglecting the critical role of margin
variance, which inherently restricts the generalization capability of the model
and heightens its vulnerability to overfitting, particularly in noisy or
imbalanced datasets. Additionally, the conventional approach of optimizing
ensemble weights within the probability simplex often introduces computational
inefficiency and scalability challenges, complicating its application to
large-scale problems. To tackle these limitations, this paper introduces a
novel ensemble learning framework that explicitly incorporates margin variance
into the loss function. Our method jointly optimizes the negative expected
margin and its variance, leading to enhanced robustness and improved
generalization performance. Moreover, by reparameterizing the ensemble weights
onto the unit sphere, we substantially simplify the optimization process and
improve computational efficiency. Extensive experiments conducted on multiple
benchmark datasets demonstrate that the proposed approach consistently
outperforms traditional margin-based ensemble techniques, underscoring its
effectiveness and practical utility.

</details>


### [176] [A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures](https://arxiv.org/abs/2509.10227)
*Ángel Ladrón,Miguel Sánchez-Domínguez,Javier Rozalén,Fernando R. Sánchez,Javier de Vicente,Lucas Lacasa,Eusebio Valero,Gonzalo Rubio*

Main category: cs.LG

TL;DR: 基于机器学习的飞机机翼疲劳寿命预测管道，通过飞行参数快速估算疲劳寿命，减少传统有限元模拟的计算成本


<details>
  <summary>Details</summary>
Motivation: 传统疲劳寿命预测方法耗时且复杂，需要多团队协作和大量有限元模拟。机器学习可以提供快速估算，作为传统方法的补充

Method: 开发基于机器学习的管道，根据飞机不同任务的飞行参数来预测机翼各位置的疲劳寿命

Result: 在真实疲劳寿命估算用例中验证了管道的准确性，提供了全面的统计验证和不确定性量化

Conclusion: 该机器学习管道能够显著减少昂贵模拟的需求，降低计算和人力资源要求，是传统方法的有效补充

Abstract: Fatigue life prediction is essential in both the design and operational
phases of any aircraft, and in this sense safety in the aerospace industry
requires early detection of fatigue cracks to prevent in-flight failures.
Robust and precise fatigue life predictors are thus essential to ensure safety.
Traditional engineering methods, while reliable, are time consuming and involve
complex workflows, including steps such as conducting several Finite Element
Method (FEM) simulations, deriving the expected loading spectrum, and applying
cycle counting techniques like peak-valley or rainflow counting. These steps
often require collaboration between multiple teams and tools, added to the
computational time and effort required to achieve fatigue life predictions.
Machine learning (ML) offers a promising complement to traditional fatigue life
estimation methods, enabling faster iterations and generalization, providing
quick estimates that guide decisions alongside conventional simulations.
  In this paper, we present a ML-based pipeline that aims to estimate the
fatigue life of different aircraft wing locations given the flight parameters
of the different missions that the aircraft will be operating throughout its
operational life. We validate the pipeline in a realistic use case of fatigue
life estimation, yielding accurate predictions alongside a thorough statistical
validation and uncertainty quantification. Our pipeline constitutes a
complement to traditional methodologies by reducing the amount of costly
simulations and, thereby, lowering the required computational and human
resources.

</details>


### [177] [Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications](https://arxiv.org/abs/2509.10248)
*Janis Keuper*

Main category: cs.LG

TL;DR: 本文通过系统评估发现：简单的提示注入攻击对LLM审稿高度有效（可达100%接受率），且LLM审稿普遍存在接受偏向（>95%）。这对LLM在同行评审中的使用讨论具有重大影响。


<details>
  <summary>Details</summary>
Motivation: 针对作者使用隐藏提示注入操纵审稿分数的报道，研究这种攻击的可行性和技术成功率，以影响关于LLM在科学同行评审中使用的最新讨论。

Method: 使用多种LLM对2024年ICLR论文的1000篇评审进行系统评估，测试简单提示注入的有效性。

Result: 1) 非常简单提示注入高度有效，达到100%接受率；2) LLM评审普遍偏向接受（许多模型>95%）。

Conclusion: 研究结果对LLM在同行评审中的使用讨论具有重大影响，揭示了系统的脆弱性和偏见问题。

Abstract: The ongoing intense discussion on rising LLM usage in the scientific
peer-review process has recently been mingled by reports of authors using
hidden prompt injections to manipulate review scores. Since the existence of
such "attacks" - although seen by some commentators as "self-defense" - would
have a great impact on the further debate, this paper investigates the
practicability and technical success of the described manipulations. Our
systematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide
range of LLMs shows two distinct results: I) very simple prompt injections are
indeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews
are generally biased toward acceptance (>95% in many models). Both results have
great impact on the ongoing discussions on LLM usage in peer-review.

</details>


### [178] [Property prediction for ionic liquids without prior structural knowledge using limited experimental data: A data-driven neural recommender system leveraging transfer learning](https://arxiv.org/abs/2509.10273)
*Sahil Sethi,Kai Sundmacher,Caroline Ganzer*

Main category: cs.LG

TL;DR: 基于神经推荐系统的转移学习框架，利用COSMO-RS模拟数据预训练后通过小量实验数据微调，实现了积分电子液体关键物理性质的准确预测


<details>
  <summary>Details</summary>
Motivation: 积分电子液体物理化学性质预测面临化学设计空间庞大且实验数据稀缩的挑战，需要开发能够充分利用限的实验数据进行可靠预测的方法

Method: 两步转移学习框架：首先使用COSMO-RS模拟数据在固定温压下预训练神经推荐系统获取积分电子液体离子的结构嵌入，然后用小量实验数据微调简单前饰神经网络进行变温压预测

Result: 对密度、糖度、表面张力、热容和熔点五种关键性质进行预测，通过跨性质知识转移在4个性质上显著提升了性能，能够向未见积分电子液体外推，最终模型可预测70万种IL组合

Conclusion: 该研究验证了结合模拟数据和转移学习的有效性，为克服实验数据稀缩问题提供了可扩展的解决方案，在积分电子液体过程设计中具有重要应用价值

Abstract: Ionic liquids (ILs) have emerged as versatile replacements for traditional
solvents because their physicochemical properties can be precisely tailored to
various applications. However, accurately predicting key thermophysical
properties remains challenging due to the vast chemical design space and the
limited availability of experimental data. In this study, we present a
data-driven transfer learning framework that leverages a neural recommender
system (NRS) to enable reliable property prediction for ILs using sparse
experimental datasets. The approach involves a two-stage process: first,
pre-training NRS models on COSMO-RS-based simulated data at fixed temperature
and pressure to learn property-specific structural embeddings for cations and
anions; and second, fine-tuning simple feedforward neural networks using these
embeddings with experimental data at varying temperatures and pressures. In
this work, five essential IL properties are considered: density, viscosity,
surface tension, heat capacity, and melting point. The framework supports both
within-property and cross-property knowledge transfer. Notably, pre-trained
models for density, viscosity, and heat capacity are used to fine-tune models
for all five target properties, achieving improved performance by a substantial
margin for four of them. The model exhibits robust extrapolation to previously
unseen ILs. Moreover, the final trained models enable property prediction for
over 700,000 IL combinations, offering a scalable solution for IL screening in
process design. This work highlights the effectiveness of combining simulated
data and transfer learning to overcome sparsity in the experimental data.

</details>


### [179] [Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case](https://arxiv.org/abs/2509.10291)
*Salih Toprak,Muge Erel-Ozcevik*

Main category: cs.LG

TL;DR: 本文提出了一种基于AutoML的新型块链验证机制Proof of AutoML，利用机器学习回归模型生成随机数作为块链交易的nonce值，以支撑灾难情况下的可靠能源交易系统。


<details>
  <summary>Details</summary>
Motivation: 在灾难场景中传统能源基础设施受损时，太阳能家庭与移动充电单元之间需要安全可追溯的能源交易。为保证块链网络上交易的完整性，需要稳健且不可预测的nonce生成机制。

Method: 提出SDN启用的架构，利用5种AutoML选择的回归模型（Gradient Boosting、LightGBM、Random Forest、Extra Trees和K-Nearest Neighbors）不是用于预测准确性，而是利用其生成随机化值的能力来作为nonce候选值。使用9000样本数据集进行随机性分析。

Result: 随机性分析显示：Random Forest和Extra Trees回归器显示完全的随机性依赖，而Gradient Boosting、K-Nearest Neighbors和LightGBM显示强但略低的随机性分数（97.6%、98.8%和99.9%）。

Conclusion: 架构树集成模型（特别是Random Forest和Extra Trees）可以作为块链安全的、SDN基础能源交易基础设施中的高效轻量级nonce生成器，适用于灾难条件下的弹性响应。

Abstract: In disaster scenarios where conventional energy infrastructure is
compromised, secure and traceable energy trading between solar-powered
households and mobile charging units becomes a necessity. To ensure the
integrity of such transactions over a blockchain network, robust and
unpredictable nonce generation is vital. This study proposes an SDN-enabled
architecture where machine learning regressors are leveraged not for their
accuracy, but for their potential to generate randomized values suitable as
nonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN
allows flexible control over data flows and energy routing policies even in
fragmented or degraded networks, ensuring adaptive response during emergencies.
Using a 9000-sample dataset, we evaluate five AutoML-selected regression models
- Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest
Neighbors - not by their prediction accuracy, but by their ability to produce
diverse and non-deterministic outputs across shuffled data inputs. Randomness
analysis reveals that Random Forest and Extra Trees regressors exhibit complete
dependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and
LightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and
99.9%, respectively). These findings highlight that certain machine learning
models, particularly tree-based ensembles, may serve as effective and
lightweight nonce generators within blockchain-secured, SDN-based energy
trading infrastructures resilient to disaster conditions.

</details>


### [180] [Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data](https://arxiv.org/abs/2509.10303)
*Jesse van Remmerden,Zaharah Bukhsh,Yingqian Zhang*

Main category: cs.LG

TL;DR: 提出CDQAC离线强化学习算法，直接从历史数据学习作业车间调度策略，无需在线交互，在样本效率和性能上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统在线RL方法需要数百万次模拟环境交互且样本效率低，无法捕捉真实世界复杂性，需要直接从历史数据学习调度策略的方法

Method: CDQAC算法结合分位数critic和延迟策略更新，估计每个机器-操作对的回报分布而非直接选择对

Result: CDQAC显著优于原始数据生成启发式方法，超越最先进的离线和在线RL基线，仅需10-20个训练实例即可学习高质量策略

Conclusion: CDQAC是从历史数据学习调度策略的有效方法，在随机启发式生成的数据上表现更好，具有高样本效率和优异性能

Abstract: The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling
Problem (FJSP), are canonical combinatorial optimization problems with
wide-ranging applications in industrial operations. In recent years, many
online reinforcement learning (RL) approaches have been proposed to learn
constructive heuristics for JSP and FJSP. Although effective, these online RL
methods require millions of interactions with simulated environments that may
not capture real-world complexities, and their random policy initialization
leads to poor sample efficiency. To address these limitations, we introduce
Conservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL
algorithm that learns effective scheduling policies directly from historical
data, eliminating the need for costly online interactions, while maintaining
the ability to improve upon suboptimal training data. CDQAC couples a
quantile-based critic with a delayed policy update, estimating the return
distribution of each machine-operation pair rather than selecting pairs
outright. Our extensive experiments demonstrate CDQAC's remarkable ability to
learn from diverse data sources. CDQAC consistently outperforms the original
data-generating heuristics and surpasses state-of-the-art offline and online RL
baselines. In addition, CDQAC is highly sample efficient, requiring only 10-20
training instances to learn high-quality policies. Surprisingly, we find that
CDQAC performs better when trained on data generated by a random heuristic than
when trained on higher-quality data from genetic algorithms and priority
dispatching rules.

</details>


### [181] [GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction](https://arxiv.org/abs/2509.10308)
*Joshua Dimasaka,Christian Geiß,Robert Muir-Wood,Emily So*

Main category: cs.LG

TL;DR: 这篇论文提出了GraphCSVAE模型，通过结合深度学习、图表征和概率推理来解决灾后风险监测中物理脆弱性建模的挑战，并在孟加拉布和塞拉利昂的灾后区域进行了验证。


<details>
  <summary>Details</summary>
Motivation: 当前在灾后风险监测中，物理脆弱性建模进展较慢，限制了对联合国消防灾风险减少框架进展的评估能力。

Method: 提出GraphCSVAE模型，结合深度学习、图表征和分类概率推理，使用时间序列卫星数据和专家知识系统，并引入弱监督一阶迁移矩阵来反映物理脆弱性的时空变化。

Result: 在孟加拉布和塞拉利昂的灾后区域进行实验，揭示了灾后物理脆弱性的区域动态变化。

Conclusion: 该模型为本地化时空审计和可持续的灾后风险减少策略提供了价值见解。

Abstract: In the aftermath of disasters, many institutions worldwide face challenges in
continually monitoring changes in disaster risk, limiting the ability of key
decision-makers to assess progress towards the UN Sendai Framework for Disaster
Risk Reduction 2015-2030. While numerous efforts have substantially advanced
the large-scale modeling of hazard and exposure through Earth observation and
data-driven methods, progress remains limited in modeling another equally
important yet challenging element of the risk equation: physical vulnerability.
To address this gap, we introduce Graph Categorical Structured Variational
Autoencoder (GraphCSVAE), a novel probabilistic data-driven framework for
modeling physical vulnerability by integrating deep learning, graph
representation, and categorical probabilistic inference, using time-series
satellite-derived datasets and prior expert belief systems. We introduce a
weakly supervised first-order transition matrix that reflects the changes in
the spatiotemporal distribution of physical vulnerability in two
disaster-stricken and socioeconomically disadvantaged areas: (1) the
cyclone-impacted coastal Khurushkul community in Bangladesh and (2) the
mudslide-affected city of Freetown in Sierra Leone. Our work reveals
post-disaster regional dynamics in physical vulnerability, offering valuable
insights into localized spatiotemporal auditing and sustainable strategies for
post-disaster risk reduction.

</details>


### [182] [ARMA Block: A CNN-Based Autoregressive and Moving Average Module for Long-Term Time Series Forecasting](https://arxiv.org/abs/2509.10324)
*Myung Jin Kim,YeongHyeon Park,Il Dong Yun*

Main category: cs.LG

TL;DR: 基于ARIMA模型灵感的简单卷积模块，直接多步预测时间序列，在九个标准数据集上表现竞争力。


<details>
  <summary>Details</summary>
Motivation: 解决传统ARIMA模型需要迭代多步预测且难以扩展到多变量设置的问题，提出一种简单有效的解决方案。

Method: 设计包含两个卷积组件的模块：一个捕捉趋势（自回归），另一个精炼局部变化（移动平均），直接进行多步预测。

Result: 在九个标准数据集上达到竞争性的准确性，尤其在具有强烈趋势变化的数据集上表现优异，同时保持了结构简单性。

Conclusion: 该方法不仅在长期时间序列预测中表现竞争力，而且其模块内在编码绝对位置信息，有潜力作为轻量级的位置嵌入替代方案。

Abstract: This paper proposes a simple yet effective convolutional module for long-term
time series forecasting. The proposed block, inspired by the Auto-Regressive
Integrated Moving Average (ARIMA) model, consists of two convolutional
components: one for capturing the trend (autoregression) and the other for
refining local variations (moving average). Unlike conventional ARIMA, which
requires iterative multi-step forecasting, the block directly performs
multi-step forecasting, making it easily extendable to multivariate settings.
Experiments on nine widely used benchmark datasets demonstrate that our method
ARMA achieves competitive accuracy, particularly on datasets exhibiting strong
trend variations, while maintaining architectural simplicity. Furthermore,
analysis shows that the block inherently encodes absolute positional
information, suggesting its potential as a lightweight replacement for
positional embeddings in sequential models.

</details>


### [183] [Physics-informed sensor coverage through structure preserving machine learning](https://arxiv.org/abs/2509.10363)
*Benjamin David Shaffer,Brooks Kinch,Joseph Klobusicky,M. Ani Hsieh,Nathaniel Trask*

Main category: cs.LG

TL;DR: 基于条件神经维纳形式(CNWF)的数字双生框架，通过结构保持的运算符学习实现港海运输系统的适应性源定位，结合Lloyd算法进行传感器部署优化。


<details>
  <summary>Details</summary>
Motivation: 解决复杂几何环境下源定位问题，结合物理约束与实时数据同化，提高定位精度和物理可实现性。

Method: 使用CNWF构建数字双生，结合FEEC的数值保证和transformer运算符学习，通过条件注意机制识别约化基准和积分平衡方程，采用交错方案进行传感器部署优化。

Result: 在复杂几何环境下实现了更高精度的点源恢复，物理约束的引入提高了识别准确性，正则性作为定位的充分条件。

Conclusion: 结构保持提供了有效的归纳偏置，CNWF框架能够在保持数值稳定性和一致性的同时实现实时适应性源定位。

Abstract: We present a machine learning framework for adaptive source localization in
which agents use a structure-preserving digital twin of a coupled
hydrodynamic-transport system for real-time trajectory planning and data
assimilation. The twin is constructed with conditional neural Whitney forms
(CNWF), coupling the numerical guarantees of finite element exterior calculus
(FEEC) with transformer-based operator learning. The resulting model preserves
discrete conservation, and adapts in real time to streaming sensor data. It
employs a conditional attention mechanism to identify: a reduced Whitney-form
basis; reduced integral balance equations; and a source field, each compatible
with given sensor measurements. The induced reduced-order environmental model
retains the stability and consistency of standard finite-element simulation,
yielding a physically realizable, regular mapping from sensor data to the
source field. We propose a staggered scheme that alternates between evaluating
the digital twin and applying Lloyd's algorithm to guide sensor placement, with
analysis providing conditions for monotone improvement of a coverage
functional. Using the predicted source field as an importance function within
an optimal-recovery scheme, we demonstrate recovery of point sources under
continuity assumptions, highlighting the role of regularity as a sufficient
condition for localization. Experimental comparisons with physics-agnostic
transformer architectures show improved accuracy in complex geometries when
physical constraints are enforced, indicating that structure preservation
provides an effective inductive bias for source identification.

</details>


### [184] [A Discrepancy-Based Perspective on Dataset Condensation](https://arxiv.org/abs/2509.10367)
*Tong Chen,Raghavendra Selvan*

Main category: cs.LG

TL;DR: 数据集缩缠的统一框架，通过分布偏差量化数据分布距离，扩展了传统的演算法目标


<details>
  <summary>Details</summary>
Motivation: 将数据集缩缠任务从特定的演算法扩展到更一般化的定义，以包容更多目标如稳健性、隐私性等

Method: 提出基于分布偏差的统一框架，使用不同法则下的概率分布距离来量化数据集表示

Result: 建立了一个能够包容现有数据集缩缠方法的统一框架，并将其扩展到更广泛的应用场景

Conclusion: 该框架为数据集缩缠领域提供了更严谨的理论基础，并支持多样化的优化目标

Abstract: Given a dataset of finitely many elements $\mathcal{T} = \{\mathbf{x}_i\}_{i
= 1}^N$, the goal of dataset condensation (DC) is to construct a synthetic
dataset $\mathcal{S} = \{\tilde{\mathbf{x}}_j\}_{j = 1}^M$ which is
significantly smaller ($M \ll N$) such that a model trained from scratch on
$\mathcal{S}$ achieves comparable or even superior generalization performance
to a model trained on $\mathcal{T}$. Recent advances in DC reveal a close
connection to the problem of approximating the data distribution represented by
$\mathcal{T}$ with a reduced set of points. In this work, we present a unified
framework that encompasses existing DC methods and extend the task-specific
notion of DC to a more general and formal definition using notions of
discrepancy, which quantify the distance between probability distribution in
different regimes. Our framework broadens the objective of DC beyond
generalization, accommodating additional objectives such as robustness,
privacy, and other desirable properties.

</details>


### [185] [Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms](https://arxiv.org/abs/2509.10369)
*Gul Rukh Khattak,Konstantinos Patlatzoglou,Joseph Barker,Libor Pastika,Boroumand Zeidaabadi,Ahmed El-Medany,Hesham Aggour,Yixiu Liang,Antonio H. Ribeiro,Jeffrey Annis,Antonio Luiz Pinho Ribeiro,Junbo Ge,Daniel B. Kramer,Jonathan W. Waks,Evan Brittain,Nicholas Peters,Fu Siong Ng,Arunashis Sau*

Main category: cs.LG

TL;DR: 对比学习在自监督预训练中广泛应用，但受队列组成影响。CAPE模型在四大洲多人群ECG数据上预训练，发现预训练队列的人口统计和健康状况影响下游性能。多中心多样化队列提高分布内精度但降低分布外泛化性，为此提出IDB策略增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索对比学习在自监督预训练中对队列组成的依赖性，特别是人口统计学特征、健康状况和人群多样性如何影响下游预测任务的性能表现。

Method: 提出CAPE基础模型，在四大洲(n=5,203,352)的多样化人群心电图数据上进行预训练，系统评估预训练队列特征对下游任务的影响，并开发In-Distribution Batch (IDB)策略来保持队列内一致性。

Result: 发现下游性能取决于预训练队列的分布特性，包括人口统计和健康状况。多中心多样化队列提高分布内精度但降低分布外泛化能力，会编码队列特异性伪影。IDB策略能增强分布外鲁棒性。

Conclusion: 这项工作为开发临床公平和可泛化的基础模型提供了重要见解，强调了预训练队列组成对模型性能的关键影响，并提出了改进泛化能力的方法。

Abstract: Contrastive learning is a widely adopted self-supervised pretraining
strategy, yet its dependence on cohort composition remains underexplored. We
present Contrasting by Patient Augmented Electrocardiograms (CAPE) foundation
model and pretrain on four cohorts (n = 5,203,352), from diverse populations
across three continents (North America, South America, Asia). We systematically
assess how cohort demographics, health status, and population diversity
influence the downstream performance for prediction tasks also including two
additional cohorts from another continent (Europe). We find that downstream
performance depends on the distributional properties of the pretraining cohort,
including demographics and health status. Moreover, while pretraining with a
multi-centre, demographically diverse cohort improves in-distribution accuracy,
it reduces out-of-distribution (OOD) generalisation of our contrastive approach
by encoding cohort-specific artifacts. To address this, we propose the
In-Distribution Batch (IDB) strategy, which preserves intra-cohort consistency
during pretraining and enhances OOD robustness. This work provides important
insights for developing clinically fair and generalisable foundation models.

</details>


### [186] [Flow Straight and Fast in Hilbert Space: Functional Rectified Flow](https://arxiv.org/abs/2509.10384)
*Jianxin Zhang,Clayton Scott*

Main category: cs.LG

TL;DR: 本文提出了无限维希尔伯特空间中的整流流函数形式化方法，建立了基于连续性方程叠加原理的理论框架，并扩展到函数流匹配和概率流ODE，实验证明性能优于现有函数生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有有限维欧几里得空间中的生成模型已有函数化推广，但整流流在无限维空间的扩展尚未探索，需要建立严格的函数化理论框架。

Method: 基于无限维空间中连续性方程的叠加原理，建立整流流的函数形式化，并将其扩展到函数流匹配和函数概率流ODE，作为整流流的非线性推广。

Result: 实验证明该方法相比现有函数生成模型具有更优越的性能，同时移除了现有函数流匹配理论中的限制性测度理论假设。

Conclusion: 成功建立了无限维希尔伯特空间中整流流的严格函数化理论框架，为函数生成模型提供了新的理论基础和实践方法，性能优于现有方法。

Abstract: Many generative models originally developed in finite-dimensional Euclidean
space have functional generalizations in infinite-dimensional settings.
However, the extension of rectified flow to infinite-dimensional spaces remains
unexplored. In this work, we establish a rigorous functional formulation of
rectified flow in an infinite-dimensional Hilbert space. Our approach builds
upon the superposition principle for continuity equations in an
infinite-dimensional space. We further show that this framework extends
naturally to functional flow matching and functional probability flow ODEs,
interpreting them as nonlinear generalizations of rectified flow. Notably, our
extension to functional flow matching removes the restrictive measure-theoretic
assumptions in the existing theory of \citet{kerrigan2024functional}.
Furthermore, we demonstrate experimentally that our method achieves superior
performance compared to existing functional generative models.

</details>


### [187] [Vendi Information Gain for Active Learning and its Application to Ecology](https://arxiv.org/abs/2509.10390)
*Quan Nguyen,Adji Bousso Dieng*

Main category: cs.LG

TL;DR: 提出Vendi信息增益(VIG)主动学习策略，通过考虑数据集范围的预测不确定性来选择最具信息量和多样性的图像进行标注，在Snapshot Serengeti数据集上仅用10%标签就达到接近全监督的准确率。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱生物多样性监测中，物种识别因标注资源有限成为主要瓶颈。传统主动学习方法只关注单个预测的不确定性，而忽略了整个数据集范围的不确定性。

Method: 引入Vendi信息增益(VIG)主动学习策略，选择能最大程度降低数据集范围预测不确定性的图像，同时考虑信息量和多样性。

Result: 在Snapshot Serengeti数据集上，VIG仅使用不到10%的标签就实现了接近全监督的预测准确率，在各项指标和批次大小上都优于标准基线方法，并在特征空间中收集到更多样化的数据。

Conclusion: VIG方法在数据有限的环境中具有广泛适用性，对生物多样性监测具有重要价值，能够显著减少标注成本同时保持高准确率。

Abstract: While monitoring biodiversity through camera traps has become an important
endeavor for ecological research, identifying species in the captured image
data remains a major bottleneck due to limited labeling resources. Active
learning -- a machine learning paradigm that selects the most informative data
to label and train a predictive model -- offers a promising solution, but
typically focuses on uncertainty in the individual predictions without
considering uncertainty across the entire dataset. We introduce a new active
learning policy, Vendi information gain (VIG), that selects images based on
their impact on dataset-wide prediction uncertainty, capturing both
informativeness and diversity. Applied to the Snapshot Serengeti dataset, VIG
achieves impressive predictive accuracy close to full supervision using less
than 10% of the labels. It consistently outperforms standard baselines across
metrics and batch sizes, collecting more diverse data in the feature space. VIG
has broad applicability beyond ecology, and our results highlight its value for
biodiversity monitoring in data-limited environments.

</details>


### [188] [Inpainting-Guided Policy Optimization for Diffusion Large Language Models](https://arxiv.org/abs/2509.10396)
*Siyan Zhao,Mengchen Liu,Jing Huang,Miao Liu,Chenyu Wang,Bo Liu,Yuandong Tian,Guan Pang,Sean Bell,Aditya Grover,Feiyu Chen*

Main category: cs.LG

TL;DR: IGPO是一种新的RL框架，利用掩码扩散LLM的修复能力指导探索，通过插入部分真实推理轨迹来提升样本效率，在数学推理任务上取得SOTA结果


<details>
  <summary>Details</summary>
Motivation: 解决RL对齐LLM时的探索挑战：稀疏奖励信号和样本浪费问题，利用dLLM独特的修复能力来指导探索

Method: 提出IGPO框架，在在线采样时策略性地插入部分真实推理轨迹，结合监督微调合成重写的简洁轨迹，并采用基于熵的过滤等技术

Result: 在GSM8K、Math500和AMC三个数学基准测试中取得显著提升，为全注意力掩码dLLM实现了新的最先进结果

Conclusion: IGPO成功地将dLLM的修复能力与RL算法设计相结合，有效解决了探索效率问题，为dLLM的强化学习对齐提供了新思路

Abstract: Masked diffusion large language models (dLLMs) are emerging as promising
alternatives to autoregressive LLMs, offering competitive performance while
supporting unique generation capabilities such as inpainting. We explore how
inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with
reinforcement learning faces an exploration challenge: sparse reward signals
and sample waste when models fail to discover correct solutions. While this
inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their
inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided
Policy Optimization), an RL framework that strategically inserts partial
ground-truth reasoning traces during online sampling. Unlike providing full
solutions, inpainting steers exploration toward promising trajectory spaces
while preserving self-generated reasoning, bridging supervised fine-tuning and
reinforcement learning. We apply IGPO to group-based optimization methods such
as GRPO, where exploration failures cause zero advantages and gradients. IGPO
restores meaningful gradients while improving sample efficiency. We also
propose supervised fine-tuning on synthetically rewritten concise traces that
better align with dLLM generation patterns. With additional techniques
including entropy-based filtering, our training recipe yields substantial gains
across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new
state-of-the-art results for full-attention masked dLLMs.

</details>


### [189] [Multipole Semantic Attention: A Fast Approximation of Softmax Attention for Pretraining](https://arxiv.org/abs/2509.10406)
*Rupert Mitchell,Kristian Kersting*

Main category: cs.LG

TL;DR: MuSe是一种高效的softmax注意力近似方法，结合语义聚类和多极展开技术，通过分层两阶段注意力机制降低Transformer的二次计算复杂度


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在长序列处理中注意力机制的二次计算复杂度问题，同时保持注意力对查询和键空间的不对称处理特性

Method: 在学习的表示空间中分别对查询和键进行聚类，使用包含单极子和偶极子校正的多极展开近似，支持分层块分解的因果注意力计算

Result: 在8k上下文长度下比CUDNN Flash Attention快3倍，相对平方误差低于20%；在16k上下文的端到端预训练中实现12.2%运行时间减少，仅损失0.36%性能

Conclusion: 多极展开近似为高效Transformer预训练提供了可行方案，能够显著降低计算复杂度同时保持模型性能

Abstract: We present Multipole Semantic Attention (MuSe), an efficient approximation of
softmax attention that combines semantic clustering with multipole expansions
from computational physics. Our method addresses the quadratic computational
complexity of transformers in the context length by clustering queries and keys
separately in their learned representation spaces, enabling a hierarchical
two-stage attention mechanism. Unlike prior clustering approaches that group
only keys or use unified clustering, we maintain separate clusterings that
respect attention's asymmetric treatment of these spaces. We augment
centroid-based (monopole) approximations with dipole corrections that capture
directional variance within clusters, preserving richer information during
training. The method operates as a drop-in replacement for standard attention,
requiring only hyperparameter specification without architectural
modifications. Our approach achieves $\mathcal{O}(NCD)$ complexity for acausal
attention with $C$ clusters and $\mathcal{O}(NCD \log N)$ for causal attention.
On isolated attention layers, we demonstrate $3\times$ speedup over CUDNN Flash
Attention at 8k context length, with relative squared errors below 20%. For
causal attention, we develop a hierarchical block decomposition that combines
exact local computation with efficient long-range approximation. In end-to-end
pretraining of a 30M parameter model on book-length texts with 16k context, we
achieve 12.2% runtime reduction with only 0.36% loss degradation, establishing
the viability of multipole approximations for efficient transformer
pretraining.

</details>


### [190] [Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining](https://arxiv.org/abs/2509.10419)
*Francesco Vitale,Tommaso Zoppi,Francesco Flammini,Nicola Mazzocca*

Main category: cs.LG

TL;DR: 使用过程挖掘和机器学习进行铁路系统运行时控制流异常检测和定位


<details>
  <summary>Details</summary>
Motivation: 随着铁路系统复杂性和关键性增加，需要提高系统韧性来应对设计时未知的残余故障、系统环境变化和网络威胁

Method: 采用过程挖掘从执行轨迹中学习系统实际控制流，进行在线一致性检查，并使用无监督机器学习进行异常定位

Result: 在ERTMS/ETCS L2的RBC切换场景中测试，显示出高精度、高效性和可解释性的异常检测和定位能力

Conclusion: 该方法能有效增强铁路控制系统的运行时韧性，为关键基础设施提供可靠的异常监测解决方案

Abstract: Ensuring the resilience of computer-based railways is increasingly crucial to
account for uncertainties and changes due to the growing complexity and
criticality of those systems. Although their software relies on strict
verification and validation processes following well-established best-practices
and certification standards, anomalies can still occur at run-time due to
residual faults, system and environmental modifications that were unknown at
design-time, or other emergent cyber-threat scenarios. This paper explores
run-time control-flow anomaly detection using process mining to enhance the
resilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European
Train Control System Level 2). Process mining allows learning the actual
control flow of the system from its execution traces, thus enabling run-time
monitoring through online conformance checking. In addition, anomaly
localization is performed through unsupervised machine learning to link
relevant deviations to critical system components. We test our approach on a
reference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its
capability to detect and localize anomalies with high accuracy, efficiency, and
explainability.

</details>


### [191] [Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration](https://arxiv.org/abs/2509.10439)
*Ahmed Khaled,Satyen Kale,Arthur Douillard,Chi Jin,Rob Fergus,Manzil Zaheer*

Main category: cs.LG

TL;DR: 本文研究了Local SGD中外部优化器的作用，证明了新的收敛保证，发现调整外部学习率可以在优化误差和随机梯度噪声方差之间进行权衡，并弥补内部学习率的不良调整。理论表明外部学习率有时应大于1，使用动量可以改善收敛率，加速外部优化器能提高通信轮次的收敛效率。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习需要大规模批处理、分布式数据和并行计算硬件，通信成为主要瓶颈。Local SGD能减少通信开销，但现有研究主要关注本地优化过程的超参数，对外部优化器及其超参数的选择不够明确。

Method: 通过理论分析证明Local SGD的收敛保证，研究外部学习率的作用机制，扩展到使用动量的情况，分析外部优化器的加速效果，并提出数据依赖的分析方法。使用标准语言模型和各种外部优化器进行综合实验验证。

Result: 理论证明调整外部学习率可以在优化误差和随机梯度噪声方差之间权衡，并能补偿内部学习率的不良调整。动量调整的外部学习率有类似作用，加速外部优化器能改善通信轮次的收敛率。实验验证了理论发现。

Conclusion: 外部优化器在Local SGD中起着关键作用，适当调整外部学习率（有时需要大于1）和使用动量能显著改善算法性能，外部加速优化器能提高通信效率，为分布式机器学习提供了重要的理论指导。

Abstract: Modern machine learning often requires training with large batch size,
distributed data, and massively parallel compute hardware (like mobile and
other edge devices or distributed data centers). Communication becomes a major
bottleneck in such settings but methods like Local Stochastic Gradient Descent
(Local SGD) show great promise in reducing this additional communication
overhead. Local SGD consists of three parts: a local optimization process, an
aggregation mechanism, and an outer optimizer that uses the aggregated updates
from the nodes to produce a new model. While there exists an extensive
literature on understanding the impact of hyperparameters in the local
optimization process, the choice of outer optimizer and its hyperparameters is
less clear. We study the role of the outer optimizer in Local SGD, and prove
new convergence guarantees for the algorithm. In particular, we show that
tuning the outer learning rate allows us to (a) trade off between optimization
error and stochastic gradient noise variance, and (b) make up for ill-tuning of
the inner learning rate. Our theory suggests that the outer learning rate
should sometimes be set to values greater than $1$. We extend our results to
settings where we use momentum in the outer optimizer, and we show a similar
role for the momentum-adjusted outer learning rate. We also study acceleration
in the outer optimizer and show that it improves the convergence rate as a
function of the number of communication rounds, improving upon the convergence
rate of prior algorithms that apply acceleration locally. Finally, we also
introduce a novel data-dependent analysis of Local SGD that yields further
insights on outer learning rate tuning. We conduct comprehensive experiments
with standard language models and various outer optimizers to validate our
theory.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [192] [Off Policy Lyapunov Stability in Reinforcement Learning](https://arxiv.org/abs/2509.09863)
*Sarvan Gill,Daniela Constantinescu*

Main category: eess.SY

TL;DR: 提出一种离策略学习Lyapunov函数的方法，结合SAC和PPO算法，提供数据高效稳定性保证


<details>
  <summary>Details</summary>
Motivation: 传统强化学习缺乏稳定性保证，现有基于Lyapunov函数的方法由于采用同策略学习导致样本效率低下

Method: 开发离策略Lyapunov函数学习方法，并将其整合到Soft Actor Critic和Proximal Policy Optimization算法中

Result: 在倒立摆和四旋翼飞行器仿真中，使用所提离策略Lyapunov函数的算法性能得到显著提升

Conclusion: 离策略Lyapunov函数学习方法能够有效提高强化学习算法的数据效率和稳定性

Abstract: Traditional reinforcement learning lacks the ability to provide stability
guarantees. More recent algorithms learn Lyapunov functions alongside the
control policies to ensure stable learning. However, the current self-learned
Lyapunov functions are sample inefficient due to their on-policy nature. This
paper introduces a method for learning Lyapunov functions off-policy and
incorporates the proposed off-policy Lyapunov function into the Soft Actor
Critic and Proximal Policy Optimization algorithms to provide them with a data
efficient stability certificate. Simulations of an inverted pendulum and a
quadrotor illustrate the improved performance of the two algorithms when
endowed with the proposed off-policy Lyapunov function.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [193] [HypoGeneAgent: A Hypothesis Language Agent for Gene-Set Cluster Resolution Selection Using Perturb-seq Datasets](https://arxiv.org/abs/2509.09740)
*Ying Yuan,Xing-Yue Monica Ge,Aaron Archer Waterman,Tommaso Biancalani,David Richmond,Yogesh Pandit,Avtar Singh,Russell Littman,Jin Liu,Jan-Christian Huetter,Vladimir Ermakov*

Main category: q-bio.QM

TL;DR: HYPOGENEAGENT是一个基于大语言模型的框架，将细胞聚类注释转化为可量化优化的任务，通过计算簇内一致性和簇间分离度来选择最佳聚类分辨率。


<details>
  <summary>Details</summary>
Motivation: 传统单细胞研究中聚类分辨率选择和功能注释都依赖主观启发式方法和专家经验，缺乏客观量化标准。

Method: 使用LLM作为基因集分析师生成GO假设和置信度评分，然后通过句子嵌入模型计算簇内一致性（高相似度）和簇间分离度（低相似度），结合生成分辨率评分。

Result: 在K562 CRISPRi Perturb-seq数据集测试中，该方法选择的聚类粒度比传统指标（如轮廓系数、模块度）更符合已知通路。

Conclusion: LLM代理可作为聚类分辨率和功能注释的客观裁决者，为实现单细胞多组学研究中的全自动、上下文感知解释管道铺平道路。

Abstract: Large-scale single-cell and Perturb-seq investigations routinely involve
clustering cells and subsequently annotating each cluster with Gene-Ontology
(GO) terms to elucidate the underlying biological programs. However, both
stages, resolution selection and functional annotation, are inherently
subjective, relying on heuristics and expert curation. We present
HYPOGENEAGENT, a large language model (LLM)-driven framework, transforming
cluster annotation into a quantitatively optimizable task. Initially, an LLM
functioning as a gene-set analyst analyzes the content of each gene program or
perturbation module and generates a ranked list of GO-based hypotheses,
accompanied by calibrated confidence scores. Subsequently, we embed every
predicted description with a sentence-embedding model, compute pair-wise cosine
similarities, and let the agent referee panel score (i) the internal
consistency of the predictions, high average similarity within the same
cluster, termed intra-cluster agreement (ii) their external distinctiveness,
low similarity between clusters, termed inter-cluster separation. These two
quantities are combined to produce an agent-derived resolution score, which is
maximized when clusters exhibit simultaneous coherence and mutual exclusivity.
When applied to a public K562 CRISPRi Perturb-seq dataset as a preliminary
test, our Resolution Score selects clustering granularities that exhibit
alignment with known pathway compared to classical metrics such silhouette
score, modularity score for gene functional enrichment summary. These findings
establish LLM agents as objective adjudicators of cluster resolution and
functional annotation, thereby paving the way for fully automated,
context-aware interpretation pipelines in single-cell multi-omics studies.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [194] [Generative Engine Optimization: How to Dominate AI Search](https://arxiv.org/abs/2509.08919)
*Mahe Chen,Xiaoxuan Wang,Kaiwen Chen,Nick Koudas*

Main category: cs.IR

TL;DR: 本文通过大规模实验对比了AI搜索和传统网络搜索的信息来源偏向，发现AI搜索系统存在对权威第三方源的系统偏向，并提出了生成式引擎优化(GEO)战略框架


<details>
  <summary>Details</summary>
Motivation: 生成式AI搜索引擎的快速普及改变了信息检索方式，从传统的排名列表转向综合性答案，这对SEO实践构成挑战，需要新的优化范式

Method: 进行大规模、受控实验，涵盖多个垂直领域、语言和查询重写，定量分析AI搜索和谷歌搜索在信息来源方面的关键差异

Result: AI搜索存在系统性偏向，更偏好权威第三方源，而非品牌自有内容和社交内容，这与谷歌的平衡混合形成明显对比；同时不同AI搜索服务在域名多样性、新鲜度、跨语言稳定性和语言敏感性方面存在显著差异

Conclusion: 提出了GEO战略议程，包括：(1)为机器扫描和证明设计内容，(2)占领权威媒体建立AI认知的权威性，(3)采用引擎特定和语言敏感策略，(4)克服大品牌偏见以支持小众玩家，为在生成式搜索环境中获得可见性提供了基础性分析和战略框架

Abstract: The rapid adoption of generative AI-powered search engines like ChatGPT,
Perplexity, and Gemini is fundamentally reshaping information retrieval, moving
from traditional ranked lists to synthesized, citation-backed answers. This
shift challenges established Search Engine Optimization (SEO) practices and
necessitates a new paradigm, which we term Generative Engine Optimization
(GEO).
  This paper presents a comprehensive comparative analysis of AI Search and
traditional web search (Google). Through a series of large-scale, controlled
experiments across multiple verticals, languages, and query paraphrases, we
quantify critical differences in how these systems source information. Our key
findings reveal that AI Search exhibit a systematic and overwhelming bias
towards Earned media (third-party, authoritative sources) over Brand-owned and
Social content, a stark contrast to Google's more balanced mix. We further
demonstrate that AI Search services differ significantly from each other in
their domain diversity, freshness, cross-language stability, and sensitivity to
phrasing.
  Based on these empirical results, we formulate a strategic GEO agenda. We
provide actionable guidance for practitioners, emphasizing the critical need
to: (1) engineer content for machine scannability and justification, (2)
dominate earned media to build AI-perceived authority, (3) adopt
engine-specific and language-aware strategies, and (4) overcome the inherent
"big brand bias" for niche players. Our work provides the foundational
empirical analysis and a strategic framework for achieving visibility in the
new generative search landscape.

</details>


### [195] [DB3 Team's Solution For Meta KDD Cup' 25](https://arxiv.org/abs/2509.09681)
*Yikuan Xia,Jiazun Chen,Yirui Zhan,Suifeng Zhao,Weipeng Jiang,Chaorui Zhang,Wei Han,Bo Bai,Jun Gao*

Main category: cs.IR

TL;DR: db3团队在KDD Cup'25 Meta CRAG-MM挑战赛中获胜的解决方案，通过多模态检索管道和LLM幻觉控制技术，在三个任务中分别获得第2、第2和第1名


<details>
  <summary>Details</summary>
Motivation: 解决CRAG-MM挑战赛中的多模态、多轮问答基准问题，特别是处理第一人称视角的自我中心查询挑战

Method: 开发了包含图像索引知识图谱、网络资源和多轮对话的领域特定检索管道，以及采用SFT、DPO和RL的先进拒绝训练方法进行幻觉控制

Result: 在Task 1和Task 2中获得第2名，在Task 3中获得第1名，最终赢得自我中心查询卓越大奖

Conclusion: 该综合框架通过专门的检索管道和统一的LLM调优方法，成功解决了多模态多轮问答中的幻觉问题，特别是在处理第一人称视角挑战方面表现出色

Abstract: This paper presents the db3 team's winning solution for the Meta CRAG-MM
Challenge 2025 at KDD Cup'25. Addressing the challenge's unique multi-modal,
multi-turn question answering benchmark (CRAG-MM), we developed a comprehensive
framework that integrates tailored retrieval pipelines for different tasks with
a unified LLM-tuning approach for hallucination control. Our solution features
(1) domain-specific retrieval pipelines handling image-indexed knowledge
graphs, web sources, and multi-turn conversations; and (2) advanced refusal
training using SFT, DPO, and RL. The system achieved 2nd place in Task 1, 2nd
place in Task 2, and 1st place in Task 3, securing the grand prize for
excellence in ego-centric queries through superior handling of first-person
perspective challenges.

</details>


### [196] [Forecasting Clicks in Digital Advertising: Multimodal Inputs and Interpretable Outputs](https://arxiv.org/abs/2509.09683)
*Briti Gangopadhyay,Zhao Wang,Shingo Takamatsu*

Main category: cs.IR

TL;DR: 提出多模态预测框架，结合点击数据和文本日志进行广告点击量预测，使用强化学习提升文本理解能力，在准确性和可解释性方面优于基线方法


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型仅依赖数值数据，忽略了文本元素中的丰富上下文信息（如关键词更新），需要开发能够融合多模态信息并生成可解释预测的方法

Method: 多模态预测框架，结合点击数据和文本日志，使用强化学习来提升文本信息理解和多模态融合能力，生成人类可解释的解释和数值预测

Result: 在大规模行业数据集上的实验表明，该方法在准确性和推理质量方面均优于基线方法

Conclusion: 提出的多模态框架成功整合了文本和数值信息，通过强化学习增强了模型的理解能力，为数字广告点击量预测提供了更准确和可解释的解决方案

Abstract: Forecasting click volume is a key task in digital advertising, influencing
both revenue and campaign strategy. Traditional time series models rely solely
on numerical data, often overlooking rich contextual information embedded in
textual elements, such as keyword updates. We present a multimodal forecasting
framework that combines click data with textual logs from real-world ad
campaigns and generates human-interpretable explanations alongside numeric
predictions. Reinforcement learning is used to improve comprehension of textual
information and enhance fusion of modalities. Experiments on a large-scale
industry dataset show that our method outperforms baselines in both accuracy
and reasoning quality.

</details>


### [197] [Text-to-SQL Oriented to the Process Mining Domain: A PT-EN Dataset for Query Translation](https://arxiv.org/abs/2509.09684)
*Bruno Yui Yamate,Thais Rodrigues Neubauer,Marcelo Fantinato,Sarajane Marques Peres*

Main category: cs.IR

TL;DR: 本文介绍了text-2-SQL-4-PM，一个用于流程挖掘领域文本到SQL任务的双语（葡萄牙语-英语）基准数据集，包含1,655个自然语言语句和205个SQL语句，并通过GPT-3.5 Turbo验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 促进自然语言查询数据库，提高非SQL专家用户的可访问性和专家用户的生产力，特别针对流程挖掘领域的独特挑战。

Method: 采用专家手动整理、专业翻译和详细标注过程，包括人工生成的释义，构建双语数据集并进行基线研究。

Result: 数据集支持文本到SQL实现的评估，展示了在流程挖掘领域的应用可行性，并为语义解析等NLP任务提供更广泛的适用性。

Conclusion: text-2-SQL-4-PM是一个专门针对流程挖掘领域的高质量双语基准数据集，有效支持文本到SQL任务的研究和应用评估。

Abstract: This paper introduces text-2-SQL-4-PM, a bilingual (Portuguese-English)
benchmark dataset designed for the text-to-SQL task in the process mining
domain. Text-to-SQL conversion facilitates natural language querying of
databases, increasing accessibility for users without SQL expertise and
productivity for those that are experts. The text-2-SQL-4-PM dataset is
customized to address the unique challenges of process mining, including
specialized vocabularies and single-table relational structures derived from
event logs. The dataset comprises 1,655 natural language utterances, including
human-generated paraphrases, 205 SQL statements, and ten qualifiers. Methods
include manual curation by experts, professional translations, and a detailed
annotation process to enable nuanced analyses of task complexity. Additionally,
a baseline study using GPT-3.5 Turbo demonstrates the feasibility and utility
of the dataset for text-to-SQL applications. The results show that
text-2-SQL-4-PM supports evaluation of text-to-SQL implementations, offering
broader applicability for semantic parsing and other natural language
processing tasks.

</details>


### [198] [TalkPlayData 2: An Agentic Synthetic Data Pipeline for Multimodal Conversational Music Recommendation](https://arxiv.org/abs/2509.09685)
*Keunwoo Choi,Seungheon Doh,Juhan Nam*

Main category: cs.IR

TL;DR: TalkPlayData 2是一个通过多智能体LLM对话生成的合成多模态音乐推荐数据集，包含音频和图像信息，用于训练生成式音乐推荐模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决多模态对话音乐推荐数据稀缺的问题，通过智能体数据管道生成高质量的合成数据集。

Method: 创建多个具有不同角色和专门提示的LLM智能体，让Listener LLM和Recsys LLM进行对话并记录聊天数据，每个对话都基于微调的对话目标，所有LLM都支持音频和图像多模态输入。

Result: 在LLM-as-a-judge和主观评估实验中，TalkPlayData 2在训练生成式音乐推荐模型的各个方面都达到了预期目标。

Conclusion: 该工作成功开发了一个开源的多模态音乐推荐合成数据集，为生成式推荐模型训练提供了高质量数据资源。

Abstract: We present TalkPlayData 2, a synthetic dataset for multimodal conversational
music recommendation generated by an agentic data pipeline. In TalkPlayData 2
pipeline, multiple large language model (LLM) agents are created under various
roles with specialized prompts and access to different parts of information,
and the chat data is acquired by logging the conversation between the Listener
LLM and the Recsys LLM. To cover various conversation scenarios, for each
conversation, the Listener LLM is conditioned on a finetuned conversation goal.
Finally, all the LLMs are multimodal with audio and images, allowing a
simulation of multimodal recommendation and conversation. In the LLM-as-a-judge
and subjective evaluation experiments, TalkPlayData 2 achieved the proposed
goal in various aspects related to training a generative recommendation model
for music. TalkPlayData 2 and its generation code are open-sourced at
https://talkpl.ai/talkplaydata2.html.

</details>


### [199] [GeoGPT.RAG Technical Report](https://arxiv.org/abs/2509.09686)
*Fei Huang,Fan Wu,Zeqing Zhang,Qihao Wang,Long Zhang,Grant Michael Boquet,Hongyang Chen*

Main category: cs.IR

TL;DR: GeoGPT是一个开源的地球科学大语言模型系统，通过检索增强生成(RAG)技术整合专业地学知识库，提供准确的地学领域回答，并支持用户上传个性化知识库。


<details>
  <summary>Details</summary>
Motivation: 为了提升地球科学领域大语言模型的领域专业能力，解决通用模型在地学专业知识上的不足，需要构建专门的地学知识增强系统。

Method: 采用检索增强生成(RAG)技术，集成专门构建的GeoGPT地学知识库；对嵌入模型和排序模型进行微调以优化检索质量；支持用户上传个性化出版物列表构建专属知识库。

Result: 系统能够生成准确、上下文相关的地学领域回答；检索质量和领域对齐显著提升；提供了开源的核心RAG组件GeoEmbedding和GeoReranker。

Conclusion: GeoGPT通过RAG技术和专门的地学知识库，有效提升了地学领域大语言模型的专业能力，体现了对开放科学和社区驱动发展的承诺，为全球地学研究者提供了强大的AI工具。

Abstract: GeoGPT is an open large language model system built to advance research in
the geosciences. To enhance its domain-specific capabilities, we integrated
Retrieval Augmented Generation(RAG), which augments model outputs with relevant
information retrieved from an external knowledge source. GeoGPT uses RAG to
draw from the GeoGPT Library, a specialized corpus curated for geoscientific
content, enabling it to generate accurate, context-specific answers. Users can
also create personalized knowledge bases by uploading their own publication
lists, allowing GeoGPT to retrieve and respond using user-provided materials.
To further improve retrieval quality and domain alignment, we fine-tuned both
the embedding model and a ranking model that scores retrieved passages by
relevance to the query. These enhancements optimize RAG for geoscience
applications and significantly improve the system's ability to deliver precise
and trustworthy outputs. GeoGPT reflects a strong commitment to open science
through its emphasis on collaboration, transparency, and community driven
development. As part of this commitment, we have open-sourced two core RAG
components-GeoEmbedding and GeoReranker-to support geoscientists, researchers,
and professionals worldwide with powerful, accessible AI tools.

</details>


### [200] [AI-Powered Assistant for Long-Term Access to RHIC Knowledge](https://arxiv.org/abs/2509.09688)
*Mohammad Atif,Vincent Garonne,Eric Lancon,Jerome Lauret,Alexandr Prozorov,Michal Vranovsky*

Main category: cs.IR

TL;DR: RHIC数据与分析保存计划开发了一个基于大语言模型的AI助手系统，通过自然语言访问文档、工作流和软件，旨在支持重离子碰撞实验数据的可重现性、教育和未来发现。


<details>
  <summary>Details</summary>
Motivation: 随着RHIC运行25年结束，保存其海量数据（约1EB）和嵌入的科学知识成为关键优先事项，需要确保这些科学遗产的长期可访问性和可用性。

Method: 基于大语言模型，采用检索增强生成和模型上下文协议，索引RHIC实验的结构化和非结构化内容，实现领域适应的交互方式。

Result: 成功部署了AI助手系统，报告了计算性能、多实验集成进展，以及为可持续和可解释的长期AI访问设计的架构特性。

Conclusion: 现代AI/ML工具能够显著提升科学遗产数据的可用性和可发现性，为大型科学实验的数据保存和知识传承提供了有效解决方案。

Abstract: As the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National
Laboratory concludes 25 years of operation, preserving not only its vast data
holdings ($\sim$1 ExaByte) but also the embedded scientific knowledge becomes a
critical priority. The RHIC Data and Analysis Preservation Plan (DAPP)
introduces an AI-powered assistant system that provides natural language access
to documentation, workflows, and software, with the aim of supporting
reproducibility, education, and future discovery. Built upon Large Language
Models using Retrieval-Augmented Generation and the Model Context Protocol,
this assistant indexes structured and unstructured content from RHIC
experiments and enables domain-adapted interaction. We report on the
deployment, computational performance, ongoing multi-experiment integration,
and architectural features designed for a sustainable and explainable long-term
AI access. Our experience illustrates how modern AI/ML tools can transform the
usability and discoverability of scientific legacy data.

</details>


### [201] [Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors](https://arxiv.org/abs/2509.09689)
*Himanshu Thakur,Eshani Agrawal,Smruthi Mukund*

Main category: cs.IR

TL;DR: 使用冻结的大型语言模型提取文本用户表示，并通过微调的小型语言模型构建高效的用户行为代理，在推荐系统中实现离线指标与实际性能的桥梁。


<details>
  <summary>Details</summary>
Motivation: 解决传统推荐模型在模拟复杂随机用户行为时的挑战，特别是需要有效处理大规模表格交互数据、克服预训练模型的归纳偏差，并为数百万用户实现可扩展的解决方案。

Method: 采用冻结的大型语言模型提取鲁棒的文本用户表示，使用微调的小型语言模型构建成本效益高的用户代理，并为用户群体训练多个低秩适配器（persona），在可扩展性和性能之间取得最佳平衡。

Result: 实验提供了令人信服的经验证据，证明该方法开发出的用户代理能够有效弥合推荐系统离线指标与实际性能之间的差距。

Conclusion: 该方法通过创新的架构设计，成功解决了用户行为模拟中的关键挑战，为推荐系统的性能评估提供了更可靠的解决方案。

Abstract: A long-standing challenge in developing accurate recommendation models is
simulating user behavior, mainly due to the complex and stochastic nature of
user interactions. Towards this, one promising line of work has been the use of
Large Language Models (LLMs) for simulating user behavior. However, aligning
these general-purpose large pre-trained models with user preferences
necessitates: (i) effectively and continously parsing large-scale tabular
user-item interaction data, (ii) overcoming pre-training-induced inductive
biases to accurately learn user specific knowledge, and (iii) achieving the
former two at scale for millions of users. While most previous works have
focused on complex methods to prompt an LLM or fine-tune it on tabular
interaction datasets, our approach shifts the focus to extracting robust
textual user representations using a frozen LLM and simulating cost-effective,
resource-efficient user agents powered by fine-tuned Small Language Models
(SLMs). Further, we showcase a method for training multiple low-rank adapters
for groups of users or \textit{persona}, striking an optimal balance between
scalability and performance of user behavior agents. Our experiments provide
compelling empirical evidence of the efficacy of our methods, demonstrating
that user agents developed using our approach have the potential to bridge the
gap between offline metrics and real-world performance of recommender systems.

</details>


### [202] [Wave-Based Semantic Memory with Resonance-Based Retrieval: A Phase-Aware Alternative to Vector Embedding Stores](https://arxiv.org/abs/2509.09691)
*Aleksandr Listopad*

Main category: cs.IR

TL;DR: 波动基于语义记忆系统，通过波动模型和共振基于检索，充分保留频幅和相位信息，提高了语义相似性的表达能力和稳健性


<details>
  <summary>Details</summary>
Motivation: 传统的向量基于记忆系统依赖于余弦或内积相似性，虽然计算高效，但本质上对相位不敏感，无法充分抓取对于意义表达至关重要的共振现象

Method: 提出波动基于语义记忆框架，将知识模型化为波动模式ψ(x) = A(x)e^{iφ(x)}，通过共振基于干涉进行检索，保留频幅和相位信息

Result: 共振基于检索在相位移动、否定和组合查询等情况下实现了更高的辨别能力，ResonanceDB实现了千万级模式的可扩展性和毫秒级延迟

Conclusion: 波动基于记忆可以作为向量库的可行替代方案，适用于AGI导向的推理和知识表达

Abstract: Conventional vector-based memory systems rely on cosine or inner product
similarity within real-valued embedding spaces. While computationally
efficient, such approaches are inherently phase-insensitive and limited in
their ability to capture resonance phenomena crucial for meaning
representation. We propose Wave-Based Semantic Memory, a novel framework that
models knowledge as wave patterns $\psi(x) = A(x) e^{i\phi(x)}$ and retrieves
it through resonance-based interference. This approach preserves both amplitude
and phase information, enabling more expressive and robust semantic similarity.
We demonstrate that resonance-based retrieval achieves higher discriminative
power in cases where vector methods fail, including phase shifts, negations,
and compositional queries. Our implementation, ResonanceDB, shows scalability
to millions of patterns with millisecond latency, positioning wave-based memory
as a viable alternative to vector stores for AGI-oriented reasoning and
knowledge representation.

</details>


### [203] [Powering Job Search at Scale: LLM-Enhanced Query Understanding in Job Matching Systems](https://arxiv.org/abs/2509.09690)
*Ping Liu,Jianqiang Shen,Qianqi Shen,Chunnan Yao,Kevin Kao,Dan Xu,Rajat Arora,Baofen Zheng,Caleb Johnson,Liangjie Hong,Jingwei Wu,Wenjing Zhang*

Main category: cs.IR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Query understanding is essential in modern relevance systems, where user
queries are often short, ambiguous, and highly context-dependent. Traditional
approaches often rely on multiple task-specific Named Entity Recognition models
to extract structured facets as seen in job search applications. However, this
fragmented architecture is brittle, expensive to maintain, and slow to adapt to
evolving taxonomies and language patterns. In this paper, we introduce a
unified query understanding framework powered by a Large Language Model (LLM),
designed to address these limitations. Our approach jointly models the user
query and contextual signals such as profile attributes to generate structured
interpretations that drive more accurate and personalized recommendations. The
framework improves relevance quality in online A/B testing while significantly
reducing system complexity and operational overhead. The results demonstrate
that our solution provides a scalable and adaptable foundation for query
understanding in dynamic web applications.

</details>


### [204] [Model-agnostic post-hoc explainability for recommender systems](https://arxiv.org/abs/2509.10245)
*Irina Arévalo,Jose L Salmeron*

Main category: cs.IR

TL;DR: 这篇论文提出了一种模型无关的删除诊断方法，用于评估推荐系统中用户或项目的影响力，以提高系统的可解释性和透明度。


<details>
  <summary>Details</summary>
Motivation: 虽然复杂的特征嵌入和深度学习算法能够提供精细的推荐，但它们常常会降低系统的可解释性和透明度。需要一种方法来量化指定观测数据对推荐系统的影响。

Method: 研究者开发了一种系统性的删除诊断方法，通过比较模型与删除特定用户或项目后的相似模型的性能，来量化该观测数据对推荐器的影响。方法在神经协同过滤(NCF)和奇异值分解(SVD)两种不同的推荐模型上进行了应用和验证。

Result: 在MovieLens和Amazon Reviews数据集上的实验结果显示，该方法能够提供对模型行为的深入见解，并验证了该方法在不同推荐范式下的普遍性。

Conclusion: 该研究提出的删除诊断方法是一种模型无关的工具，能够有效地量化用户或项目对推荐系统的影响，显著提高了推荐系统的可解释性和透明度，为推荐系统的可解释性研究做出了贡献。

Abstract: Recommender systems often benefit from complex feature embeddings and deep
learning algorithms, which deliver sophisticated recommendations that enhance
user experience, engagement, and revenue. However, these methods frequently
reduce the interpretability and transparency of the system. In this research,
we develop a systematic application, adaptation, and evaluation of deletion
diagnostics in the recommender setting. The method compares the performance of
a model to that of a similar model trained without a specific user or item,
allowing us to quantify how that observation influences the recommender, either
positively or negatively. To demonstrate its model-agnostic nature, the
proposal is applied to both Neural Collaborative Filtering (NCF), a widely used
deep learning-based recommender, and Singular Value Decomposition (SVD), a
classical collaborative filtering technique. Experiments on the MovieLens and
Amazon Reviews datasets provide insights into model behavior and highlight the
generality of the approach across different recommendation paradigms.

</details>


### [205] [Diversified recommendations of cultural activities with personalized determinantal point processes](https://arxiv.org/abs/2509.10392)
*Carole Ibrahim,Hiba Bederina,Daniel Cuesta,Laurent Montier,Cyrille Delabre,Jill-Jênn Vie*

Main category: cs.IR

TL;DR: 通过个性化行列点过程(DPP)来实现推荐系统的多样性推荐，在保持相关性的同时提升多样性，不影响核心业务指标


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统在提升多样性时对核心业务指标的负面影响问题，扩宽用户文化实践

Method: 采用个性化行列点过程(DPP)抽样方法，使用质量-多样性分解矩阵来加强用户偏好的权重

Result: 通过离线和在线指标评估了相关性与多样性之间的交换弊示，并为实际应用提供了建议

Conclusion: 个性化DPP方法能够有效平衡推荐的相关性和多样性，为生产环境中的多样性推荐提供了可行方案

Abstract: While optimizing recommendation systems for user engagement is a
well-established practice, effectively diversifying recommendations without
negatively impacting core business metrics remains a significant industry
challenge. In line with our initiative to broaden our audience's cultural
practices, this study investigates using personalized Determinantal Point
Processes (DPPs) to sample diverse and relevant recommendations. We rely on a
well-known quality-diversity decomposition of the similarity kernel to give
more weight to user preferences. In this paper, we present our implementations
of the personalized DPP sampling, evaluate the trade-offs between relevance and
diversity through both offline and online metrics, and give insights for
practitioners on their use in a production environment. For the sake of
reproducibility, we release the full code for our platform and experiments on
GitHub.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [206] [Chord: Chain of Rendering Decomposition for PBR Material Estimation from Generated Texture Images](https://arxiv.org/abs/2509.09952)
*Zhi Ying,Boxiang Rong,Jingyu Wang,Maoyuan Xu*

Main category: cs.GR

TL;DR: 提出了一种新颖的两阶段生成-估计框架，用于高质量PBR材质生成，通过微调扩散模型生成可平铺纹理，再通过链式分解方案预测SVBRDF通道，实现了高效、高质量且用户可控的材质生成。


<details>
  <summary>Details</summary>
Motivation: 传统材质创建和重建需要艺术家大量时间和专业知识，现有基于视觉基础模型的方法在质量、灵活性和用户控制方面存在不足，需要更好的解决方案。

Method: 两阶段框架：1)生成阶段使用微调扩散模型合成符合用户输入的着色可平铺纹理；2)估计阶段采用链式分解方案，通过单步图像条件扩散模型顺序预测SVBRDF通道。

Result: 方法在质量和性能上优于现有材质生成和估计方法，对生成纹理和真实照片都表现出强鲁棒性，支持文本到材质、图像到材质、结构引导生成和材质编辑等多种应用。

Conclusion: 该框架提供了高效、高质量且用户可控的PBR材质生成解决方案，在多个应用场景中展现出优异的灵活性和实用性。

Abstract: Material creation and reconstruction are crucial for appearance modeling but
traditionally require significant time and expertise from artists. While recent
methods leverage visual foundation models to synthesize PBR materials from
user-provided inputs, they often fall short in quality, flexibility, and user
control. We propose a novel two-stage generate-and-estimate framework for PBR
material generation. In the generation stage, a fine-tuned diffusion model
synthesizes shaded, tileable texture images aligned with user input. In the
estimation stage, we introduce a chained decomposition scheme that sequentially
predicts SVBRDF channels by passing previously extracted representation as
input into a single-step image-conditional diffusion model. Our method is
efficient, high quality, and enables flexible user control. We evaluate our
approach against existing material generation and estimation methods,
demonstrating superior performance. Our material estimation method shows strong
robustness on both generated textures and in-the-wild photographs. Furthermore,
we highlight the flexibility of our framework across diverse applications,
including text-to-material, image-to-material, structure-guided generation, and
material editing.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [207] [Matrix-free Neural Preconditioner for the Dirac Operator in Lattice Gauge Theory](https://arxiv.org/abs/2509.10378)
*Yixuan Sun,Srinivas Eswar,Yin Lin,William Detmold,Phiala Shanahan,Xiaoye Li,Yang Liu,Prasanna Balaprakash*

Main category: hep-lat

TL;DR: 提出基于算子学习的框架来构建线性映射作为有效预处理器，用于加速格点QCD中的共轭梯度求解器，无需显式矩阵，能降低条件数并减少迭代次数。


<details>
  <summary>Details</summary>
Motivation: 格点量子色动力学中求解稀疏但病态的Hermitian正定线性系统需要迭代方法如共轭梯度法，计算成本高。现有多网格预处理器构建困难且计算开销大。

Method: 利用算子学习技术构建线性映射作为预处理器，不依赖原始线性系统或预处理器中的显式矩阵，实现高效模型训练和在CG求解器中的应用。

Result: 在Schwinger模型U(1)规范理论中，该预处理器方案有效降低了线性系统的条件数，在相关参数范围内将收敛所需迭代次数减少约一半，并展示了零样本学习能力。

Conclusion: 该框架通过学习依赖于晶格结构的通用映射，能够为不同尺寸规范场配置构建的Dirac算子提供有效的预处理器，展示了算子学习在加速科学计算中的潜力。

Abstract: Linear systems arise in generating samples and in calculating observables in
lattice quantum chromodynamics~(QCD). Solving the Hermitian positive definite
systems, which are sparse but ill-conditioned, involves using iterative
methods, such as Conjugate Gradient (CG), which are time-consuming and
computationally expensive. Preconditioners can effectively accelerate this
process, with the state-of-the-art being multigrid preconditioners. However,
constructing useful preconditioners can be challenging, adding additional
computational overhead, especially in large linear systems. We propose a
framework, leveraging operator learning techniques, to construct linear maps as
effective preconditioners. The method in this work does not rely on explicit
matrices from either the original linear systems or the produced
preconditioners, allowing efficient model training and application in the CG
solver. In the context of the Schwinger model U(1) gauge theory in 1+1
spacetime dimensions with two degenerate-mass fermions), this preconditioning
scheme effectively decreases the condition number of the linear systems and
approximately halves the number of iterations required for convergence in
relevant parameter ranges. We further demonstrate the framework learns a
general mapping dependent on the lattice structure which leads to zero-shot
learning ability for the Dirac operators constructed from gauge field
configurations of different sizes.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [208] [SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints](https://arxiv.org/abs/2509.09853)
*Zhiyu Fan,Kirill Vasilevski,Dayi Lin,Boyuan Chen,Yihao Chen,Zhiqing Zhong,Jie M. Zhang,Pinjia He,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: SWE-Effi：一种新的多维度评估指标，用于重新评估AI系统在软件工程任务中的整体效能，综合考虑准确性和资源消耗（token和时间）的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有AI软件工程排行榜（如SWE-bench）仅关注解决方案准确性，忽视了资源受限环境下的效能问题。任何AI系统不仅需要正确，还必须具有成本效益。

Method: 引入SWE-Effi指标集，在SWE-bench基准的子集上重新评估流行的AI问题解决系统，从多维度（准确率、token消耗、时间消耗）进行效能评分。

Result: 发现AI系统效能不仅取决于框架本身，还取决于与基础模型的整合程度；识别出"token滚雪球"效应和"昂贵失败"模式；观察到token预算和时间预算下的效能权衡。

Conclusion: 需要综合考虑准确性和资源消耗的多维度评估体系；模型整合质量对效能至关重要；资源效率问题限制了实际部署和RL训练的可扩展性。

Abstract: The advancement of large language models (LLMs) and code agents has
demonstrated significant potential to assist software engineering (SWE) tasks,
such as autonomous issue resolution and feature addition. Existing AI for
software engineering leaderboards (e.g., SWE-bench) focus solely on solution
accuracy, ignoring the crucial factor of effectiveness in a
resource-constrained world. This is a universal problem that also exists beyond
software engineering tasks: any AI system should be more than correct - it must
also be cost-effective. To address this gap, we introduce SWE-Effi, a set of
new metrics to re-evaluate AI systems in terms of holistic effectiveness
scores. We define effectiveness as the balance between the accuracy of outcome
(e.g., issue resolve rate) and the resources consumed (e.g., token and time).
In this paper, we specifically focus on the software engineering scenario by
re-ranking popular AI systems for issue resolution on a subset of the SWE-bench
benchmark using our new multi-dimensional metrics. We found that AI system's
effectiveness depends not just on the scaffold itself, but on how well it
integrates with the base model, which is key to achieving strong performance in
a resource-efficient manner. We also identified systematic challenges such as
the "token snowball" effect and, more significantly, a pattern of "expensive
failures". In these cases, agents consume excessive resources while stuck on
unsolvable tasks - an issue that not only limits practical deployment but also
drives up the cost of failed rollouts during RL training. Lastly, we observed a
clear trade-off between effectiveness under the token budget and effectiveness
under the time budget, which plays a crucial role in managing project budgets
and enabling scalable reinforcement learning, where fast responses are
essential.

</details>


### [209] [From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem](https://arxiv.org/abs/2509.09873)
*James Jewitt,Hao Li,Bram Adams,Gopi Krishnan Rajbahadur,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 首次对Hugging Face平台上的数据集、模型和开源软件进行全面许可证审计，发现35.5%的模型到应用转换通过重新授权消除了限制性许可条款，并提供了可扩展的规则引擎来检测许可冲突。


<details>
  <summary>Details</summary>
Motivation: 开源AI生态系统中隐藏的许可证冲突带来严重法律和道德风险，但缺乏数据驱动的理解来识别这些冲突的频率、来源和影响群体。

Method: 对Hugging Face平台的36.4万个数据集、1.6万个模型以及14万个GitHub项目进行结构化审计，开发了一个可扩展的规则引擎来编码近200个SPDX和模型特定许可条款。

Result: 实证分析显示了系统性的许可证违规行为，其35.5%的模型到应用转换通过重新授权消除了限制性条款。规则引擎可以解决86.4%的软件应用中的许可证冲突。

Conclusion: 许可证遵循是开源AI中的关键治理挑战，本研究为实现自动化的、大规模的AI知觉合规性提供了数据和工具支持。

Abstract: Hidden license conflicts in the open-source AI ecosystem pose serious legal
and ethical risks, exposing organizations to potential litigation and users to
undisclosed risk. However, the field lacks a data-driven understanding of how
frequently these conflicts occur, where they originate, and which communities
are most affected. We present the first end-to-end audit of licenses for
datasets and models on Hugging Face, as well as their downstream integration
into open-source software applications, covering 364 thousand datasets, 1.6
million models, and 140 thousand GitHub projects. Our empirical analysis
reveals systemic non-compliance in which 35.5% of model-to-application
transitions eliminate restrictive license clauses by relicensing under
permissive terms. In addition, we prototype an extensible rule engine that
encodes almost 200 SPDX and model-specific clauses for detecting license
conflicts, which can solve 86.4% of license conflicts in software applications.
To support future research, we release our dataset and the prototype engine.
Our study highlights license compliance as a critical governance challenge in
open-source AI and provides both the data and tools necessary to enable
automated, AI-aware compliance at scale.

</details>


### [210] [WALL: A Web Application for Automated Quality Assurance using Large Language Models](https://arxiv.org/abs/2509.09918)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: WALL是一个集成SonarQube和大型语言模型的Web应用，通过三个模块自动化代码问题检测、修复和评估，实验证明能显著减少人工工作量并降低成本。


<details>
  <summary>Details</summary>
Motivation: 随着软件项目复杂度增加，代码问题数量和种类大幅增长，需要高效的自动化工具来检测、解决和评估代码问题。

Method: 开发WALL应用，集成SonarQube和GPT-3.5 Turbo/GPT-4o等LLM，包含问题提取工具、代码问题修订器和代码比较工具三个模块。

Result: 在563个文件7599个问题上进行实验，证明WALL能有效减少人工工作量，混合使用成本效益型和先进LLM可显著降低成本并提高修订率。

Conclusion: WALL展示了自动化代码质量管理的有效性，未来工作将集成开源LLM并消除人工干预，实现完全自动化。

Abstract: As software projects become increasingly complex, the volume and variety of
issues in code files have grown substantially. Addressing this challenge
requires efficient issue detection, resolution, and evaluation tools. This
paper presents WALL, a web application that integrates SonarQube and large
language models (LLMs) such as GPT-3.5 Turbo and GPT-4o to automate these
tasks. WALL comprises three modules: an issue extraction tool, code issues
reviser, and code comparison tool. Together, they enable a seamless pipeline
for detecting software issues, generating automated code revisions, and
evaluating the accuracy of revisions. Our experiments, conducted on 563 files
with over 7,599 issues, demonstrate WALL's effectiveness in reducing human
effort while maintaining high-quality revisions. Results show that employing a
hybrid approach of cost-effective and advanced LLMs can significantly lower
costs and improve revision rates. Future work aims to enhance WALL's
capabilities by integrating open-source LLMs and eliminating human
intervention, paving the way for fully automated code quality management.

</details>


### [211] [Targeted Test Selection Approach in Continuous Integration](https://arxiv.org/abs/2509.10279)
*Pavel Plyusnin,Aleksey Antonov,Vasilii Ermakov,Aleksandr Khaybriev,Margarita Kikot,Ilseyar Alimova,Stanislav Moiseev*

Main category: cs.SE

TL;DR: T-TS是一种基于机器学习的工业测试选择方法，通过词袋表示代码变更文件，无需覆盖率映射，能选择15%的测试用例，减少5.9倍执行时间，检测95%以上的测试失败。


<details>
  <summary>Details</summary>
Motivation: 随着代码库扩展和测试套件增长，在高频代码提交环境下，高效管理测试过程变得日益困难，需要更智能的测试选择方法。

Method: 提出Targeted Test Selection (T-TS)机器学习方法，使用变更文件的词袋表示，整合跨文件特征和其他预测特征，避免使用覆盖率映射。

Result: 在生产环境中部署评估，T-TS仅选择15%的测试用例，执行时间减少5.9倍，流水线加速5.6倍，检测超过95%的测试失败。

Conclusion: T-TS在工业环境中表现出色，提供了高效的测试选择解决方案，实现已公开可用以支持进一步研究和实际应用。

Abstract: In modern software development change-based testing plays a crucial role.
However, as codebases expand and test suites grow, efficiently managing the
testing process becomes increasingly challenging, especially given the high
frequency of daily code commits. We propose Targeted Test Selection (T-TS), a
machine learning approach for industrial test selection. Our key innovation is
a data representation that represent commits as Bags-of-Words of changed files,
incorporates cross-file and additional predictive features, and notably avoids
the use of coverage maps. Deployed in production, T-TS was comprehensively
evaluated against industry standards and recent methods using both internal and
public datasets, measuring time efficiency and fault detection. On live
industrial data, T-TS selects only 15% of tests, reduces execution time by
$5.9\times$, accelerates the pipeline by $5.6\times$, and detects over 95% of
test failures. The implementation is publicly available to support further
research and practical adoption.

</details>


### [212] [Generating Energy-Efficient Code via Large-Language Models -- Where are we now?](https://arxiv.org/abs/2509.10099)
*Radu Apsan,Vincenzo Stoico,Michel Albonico,Rudra Dhar,Karthik Vaidhyanathan,Ivano Malavolta*

Main category: cs.SE

TL;DR: 大语言模型生成的Python代码在能源效率方面对比人类开发者和绿色软件专家的表现不一，绿色软件专家的代码在所有硬件平台都更节能


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型生成的Python代码的能源效率，与人类开发者和绿色软件专家的代码进行对比

Method: 使用6个普遍的LLM模型和4种提示技巧，在EvoEval基准的9个编码问题上测试363个解决方案，并在3种不同硬件平台（服务器、PC和Raspberry Pi）上测量能消耗

Result: 人类解决方案在服务器上节能16%，在Raspberry Pi上节能3%，而LLM在PC上比人类开发者节能25%。绿色软件专家的代码在所有硬件平台上都比所有LLM节能至30%

Conclusion: 虽然LLM显示了较好的代码生成能力，但没有任何LLM生成的代码比经验丰富的绿色软件开发者更节能，说明人类专业知识在开发能源效率高的Python代码方面仍不可或缺

Abstract: Context. The rise of Large Language Models (LLMs) has led to their widespread
adoption in development pipelines. Goal. We empirically assess the energy
efficiency of Python code generated by LLMs against human-written code and code
developed by a Green software expert. Method. We test 363 solutions to 9 coding
problems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting
techniques, and comparing them to human-developed solutions. Energy consumption
is measured on three different hardware platforms: a server, a PC, and a
Raspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16%
more energy-efficient on the server and 3% on the Raspberry Pi, while LLMs
outperform human developers by 25% on the PC. Prompting does not consistently
lead to energy savings, where the most energy-efficient prompts vary by
hardware platform. The code developed by a Green software expert is
consistently more energy-efficient by at least 17% to 30% against all LLMs on
all hardware platforms. Conclusions. Even though LLMs exhibit relatively good
code generation capabilities, no LLM-generated code was more energy-efficient
than that of an experienced Green software developer, suggesting that as of
today there is still a great need of human expertise for developing
energy-efficient Python code.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [213] [LLM-Based Instance-Driven Heuristic Bias In the Context of a Biased Random Key Genetic Algorithm](https://arxiv.org/abs/2509.09707)
*Camilo Chacón Sartori,Martín Isla Pino,Pedro Pinacho-Davidson,Christian Blum*

Main category: cs.NE

TL;DR: 提出了一种将大语言模型与BRKGA算法结合的新框架，通过人机协作设计实例特定指标，生成定制化启发式偏置来指导算法搜索，在复杂组合优化问题上取得显著改进


<details>
  <summary>Details</summary>
Motivation: 现有方法大多利用LLMs生成代码来创建或改进特定启发式，但往往忽略了单个问题实例的结构特性，需要更有效的实例驱动方法

Method: 引入人机协作流程共同设计计算高效指标，LLM分析这些实例特定指标生成定制启发式偏置，指导BRKGA算法向有希望的搜索空间区域探索

Result: 在1,050个不同复杂度实例上的实验表明，最佳混合方法BRKGA+Llama-4-Maverick相比基线取得统计显著改进，特别是在最复杂实例上表现突出

Conclusion: 利用LLM生成先验的实例驱动启发式偏置是增强复杂优化领域中元启发式算法的有效方法

Abstract: Integrating Large Language Models (LLMs) within metaheuristics opens a novel
path for solving complex combinatorial optimization problems. While most
existing approaches leverage LLMs for code generation to create or refine
specific heuristics, they often overlook the structural properties of
individual problem instances. In this work, we introduce a novel framework that
integrates LLMs with a Biased Random-Key Genetic Algorithm (BRKGA) to solve the
NP-hard Longest Run Subsequence problem. Our approach extends the
instance-driven heuristic bias paradigm by introducing a human-LLM
collaborative process to co-design and implement a set of computationally
efficient metrics. The LLM analyzes these instance-specific metrics to generate
a tailored heuristic bias, which steers the BRKGA toward promising areas of the
search space. We conduct a comprehensive experimental evaluation, including
rigorous statistical tests, convergence and behavioral analyses, and targeted
ablation studies, comparing our method against a standard BRKGA baseline across
1,050 generated instances of varying complexity. Results show that our
top-performing hybrid, BRKGA+Llama-4-Maverick, achieves statistically
significant improvements over the baseline, particularly on the most complex
instances. Our findings confirm that leveraging an LLM to produce an a priori,
instance-driven heuristic bias is a valuable approach for enhancing
metaheuristics in complex optimization domains.

</details>


### [214] [Predictive Spike Timing Enables Distributed Shortest Path Computation in Spiking Neural Networks](https://arxiv.org/abs/2509.10077)
*Simen Storesund,Kristian Valset Aars,Robin Dietrich,Nicolai Waniek*

Main category: cs.NE

TL;DR: 提出了一种基于脉冲时序的生物学合理最短路径算法，通过局部脉冲消息传递和时序压缩机制实现最短路径计算，无需全局状态或梯度更新


<details>
  <summary>Details</summary>
Motivation: 现有图算法需要全局状态和生物学上不可信的操作，而强化学习方法依赖缓慢的梯度更新，与生物系统的快速行为适应不一致

Method: 利用脉冲时序巧合识别最优路径节点，通过抑制-兴奋消息对的早期接收来减少响应延迟，实现从目标到源的时间压缩传播

Result: 算法收敛并能发现所有最短路径，在随机空间网络上通过仿真验证了纯时序机制的有效性

Conclusion: 展示了短期时序动态如何单独计算最短路径，为理解生物网络如何通过局部计算解决复杂问题提供了新见解，对计算神经科学、AI和神经形态系统有重要意义

Abstract: Efficient planning and sequence selection are central to intelligence, yet
current approaches remain largely incompatible with biological computation.
Classical graph algorithms like Dijkstra's or A* require global state and
biologically implausible operations such as backtracing, while reinforcement
learning methods rely on slow gradient-based policy updates that appear
inconsistent with rapid behavioral adaptation observed in natural systems.
  We propose a biologically plausible algorithm for shortest-path computation
that operates through local spike-based message-passing with realistic
processing delays. The algorithm exploits spike-timing coincidences to identify
nodes on optimal paths: Neurons that receive inhibitory-excitatory message
pairs earlier than predicted reduce their response delays, creating a temporal
compression that propagates backwards from target to source. Through analytical
proof and simulations on random spatial networks, we demonstrate that the
algorithm converges and discovers all shortest paths using purely timing-based
mechanisms. By showing how short-term timing dynamics alone can compute
shortest paths, this work provides new insights into how biological networks
might solve complex computational problems through purely local computation and
relative spike-time prediction. These findings open new directions for
understanding distributed computation in biological and artificial systems,
with possible implications for computational neuroscience, AI, reinforcement
learning, and neuromorphic systems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [215] [Sparse Polyak: an adaptive step size rule for high-dimensional M-estimation](https://arxiv.org/abs/2509.09802)
*Tianqi Qiao,Marie Maros*

Main category: math.OC

TL;DR: Sparse Polyak是一种改进的自适应步长方法，专门针对高维统计估计问题设计，通过估计受限Lipschitz平滑常数来解决标准Polyak步长在高维场景下的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 在高维统计估计问题中，当问题维度远大于样本量时，标准的Polyak自适应步长性能显著下降，即使问题条件良好，也需要更多迭代次数才能达到最优统计精度。这是因为高维情况下传统的Lipschitz平滑常数估计不再有效。

Method: 提出Sparse Polyak方法，通过修改步长来估计受限Lipschitz平滑常数（即仅在问题相关方向上的平滑性），而不是估计全局的Lipschitz常数。

Result: 理论分析和数值实验均表明，Sparse Polyak在高维设置下相比标准Polyak步长具有更好的性能表现。

Conclusion: Sparse Polyak通过针对高维问题的特殊设计，有效解决了标准自适应步长方法在高维统计估计中的局限性，为高维优化问题提供了更有效的解决方案。

Abstract: We propose and study Sparse Polyak, a variant of Polyak's adaptive step size,
designed to solve high-dimensional statistical estimation problems where the
problem dimension is allowed to grow much faster than the sample size. In such
settings, the standard Polyak step size performs poorly, requiring an
increasing number of iterations to achieve optimal statistical precision-even
when, the problem remains well conditioned and/or the achievable precision
itself does not degrade with problem size. We trace this limitation to a
mismatch in how smoothness is measured: in high dimensions, it is no longer
effective to estimate the Lipschitz smoothness constant. Instead, it is more
appropriate to estimate the smoothness restricted to specific directions
relevant to the problem (restricted Lipschitz smoothness constant). Sparse
Polyak overcomes this issue by modifying the step size to estimate the
restricted Lipschitz smoothness constant. We support our approach with both
theoretical analysis and numerical experiments, demonstrating its improved
performance.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [216] [HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario](https://arxiv.org/abs/2509.10096)
*Saeed Saadatnejad,Reyhaneh Hosseininejad,Jose Barreiros,Katherine M. Tsui,Alexandre Alahi*

Main category: cs.RO

TL;DR: 提出了HHI-Assist数据集和基于Transformer的条件去噪扩散模型，用于预测物理交互场景中的人体运动，提高辅助机器人的安全性和响应能力


<details>
  <summary>Details</summary>
Motivation: 劳动力短缺和人口老龄化需要辅助机器人，但机器人需要准确的人体运动预测来实现安全响应，而物理交互中的耦合动力学复杂性使这成为挑战

Method: 收集人类-人类交互辅助任务的动作捕捉数据集(HHI-Assist)，并开发基于条件Transformer的去噪扩散模型来预测交互代理的姿态

Result: 模型有效捕捉了护理者和被护理者之间的耦合动力学，相比基线有改进，并在未见场景中表现出强泛化能力

Conclusion: 通过推进交互感知的运动预测和引入新数据集，这项工作有潜力显著增强机器人辅助策略，数据集和代码已开源

Abstract: The increasing labor shortage and aging population underline the need for
assistive robots to support human care recipients. To enable safe and
responsive assistance, robots require accurate human motion prediction in
physical interaction scenarios. However, this remains a challenging task due to
the variability of assistive settings and the complexity of coupled dynamics in
physical interactions. In this work, we address these challenges through two
key contributions: (1) HHI-Assist, a dataset comprising motion capture clips of
human-human interactions in assistive tasks; and (2) a conditional
Transformer-based denoising diffusion model for predicting the poses of
interacting agents. Our model effectively captures the coupled dynamics between
caregivers and care receivers, demonstrating improvements over baselines and
strong generalization to unseen scenarios. By advancing interaction-aware
motion prediction and introducing a new dataset, our work has the potential to
significantly enhance robotic assistance policies. The dataset and code are
available at: https://sites.google.com/view/hhi-assist/home

</details>


### [217] [GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation](https://arxiv.org/abs/2509.10454)
*Hang Yin,Haoyu Wei,Xiuwei Xu,Wenxuan Guo,Jie Zhou,Jiwen Lu*

Main category: cs.RO

TL;DR: 这篇论文提出了一种无需训练的视觉-语言导航框架，通过将导航指令分解为显式空间约束并采用图约束优化方法，实现了在连续环境中的零样本适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本VLN方法主要为离散环境设计，或需要在连续模拟器环境中进行无监督训练，这使得它们在真实场景中的普适性和部署性面临挑战。

Method: 将导航指导形式化为图约束优化问题，通过解构指令为显式空间约束。构建覆盖VLN指令中所有空间关系类型的空间约束库，将人类指令分解为有向无环图结构，利用约束求解器解决图约束优化问题。

Result: 在标准测试集上进行了大量实验，显示了成功率和导航效率方面相比最先进的零样本VLN方法有显著提升。真实世界实验证明了该框架能够有效地渐适到新环境和指令集。

Conclusion: 该无需训练的框架通过约束驱动的方式解码空间语义，实现了对未见环境的零样本适应，为更健壮和自主的导航框架铭定了基础。

Abstract: In this paper, we propose a training-free framework for vision-and-language
navigation (VLN). Existing zero-shot VLN methods are mainly designed for
discrete environments or involve unsupervised training in continuous simulator
environments, which makes it challenging to generalize and deploy them in
real-world scenarios. To achieve a training-free framework in continuous
environments, our framework formulates navigation guidance as graph constraint
optimization by decomposing instructions into explicit spatial constraints. The
constraint-driven paradigm decodes spatial semantics through constraint
solving, enabling zero-shot adaptation to unseen environments. Specifically, we
construct a spatial constraint library covering all types of spatial
relationship mentioned in VLN instructions. The human instruction is decomposed
into a directed acyclic graph, with waypoint nodes, object nodes and edges,
which are used as queries to retrieve the library to build the graph
constraints. The graph constraint optimization is solved by the constraint
solver to determine the positions of waypoints, obtaining the robot's
navigation path and final goal. To handle cases of no solution or multiple
solutions, we construct a navigation tree and the backtracking mechanism.
Extensive experiments on standard benchmarks demonstrate significant
improvements in success rate and navigation efficiency compared to
state-of-the-art zero-shot VLN methods. We further conduct real-world
experiments to show that our framework can effectively generalize to new
environments and instruction sets, paving the way for a more robust and
autonomous navigation framework.

</details>


### [218] [Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision](https://arxiv.org/abs/2509.09893)
*Hanbit Oh,Masaki Murooka,Tomohiro Motoda,Ryoichi Nakajo,Yukiyasu Domae*

Main category: cs.RO

TL;DR: SART框架通过单次人类演示和自主轨迹增强，实现了高效安全的模仿学习，大幅减少了人工干预和碰撞风险。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习需要大量演示数据或随机探索，存在安全性差、碰撞频繁、需要人工重置环境等问题，特别是在狭小空间任务中。

Method: SART分为两个阶段：1）单次人类教学，提供一次演示并标注关键路径点的精度边界；2）机器人自主增强，在边界内生成多样化的无碰撞轨迹并与原始演示连接。

Result: 在仿真和真实世界操作任务中，SART相比仅使用人类收集演示的策略获得了显著更高的成功率。

Conclusion: SART框架能够从单次演示中学习策略，通过安全的自主数据增强最小化人工干预，同时确保安全性，提高了数据收集效率。

Abstract: Imitation learning is a promising paradigm for training robot agents;
however, standard approaches typically require substantial data acquisition --
via numerous demonstrations or random exploration -- to ensure reliable
performance. Although exploration reduces human effort, it lacks safety
guarantees and often results in frequent collisions -- particularly in
clearance-limited tasks (e.g., peg-in-hole) -- thereby, necessitating manual
environmental resets and imposing additional human burden. This study proposes
Self-Augmented Robot Trajectory (SART), a framework that enables policy
learning from a single human demonstration, while safely expanding the dataset
through autonomous augmentation. SART consists of two stages: (1) human
teaching only once, where a single demonstration is provided and precision
boundaries -- represented as spheres around key waypoints -- are annotated,
followed by one environment reset; (2) robot self-augmentation, where the robot
generates diverse, collision-free trajectories within these boundaries and
reconnects to the original demonstration. This design improves the data
collection efficiency by minimizing human effort while ensuring safety.
Extensive evaluations in simulation and real-world manipulation tasks show that
SART achieves substantially higher success rates than policies trained solely
on human-collected demonstrations. Video results available at
https://sites.google.com/view/sart-il .

</details>


### [219] [TwinTac: A Wide-Range, Highly Sensitive Tactile Sensor with Real-to-Sim Digital Twin Sensor Model](https://arxiv.org/abs/2509.10063)
*Xiyan Huang,Zhe Xu,Chenxi Xiao*

Main category: cs.RO

TL;DR: TwinTac系统通过结合物理触觉传感器和其数字孪生模型，解决了触觉感知在机器人技能学习中的仿真缺失问题，实现了高质量的数据生成和跨域学习


<details>
  <summary>Details</summary>
Motivation: 机器人技能学习中缺乏触觉传感器的仿真模型，阻碍了基于触觉感知的有效策略开发，需要弥合这一差距

Method: 设计高灵敏度物理触觉传感器，采用真实到仿真的方法开发数字孪生模型，通过收集同步跨域数据训练神经网络将仿真数据映射到真实传感器响应

Result: 物理传感器灵敏度得到表征，数字孪生模型能够一致地复制物理传感器输出，仿真数据能有效增强真实数据，提高物体分类任务的准确性

Conclusion: TwinTac系统有潜力弥合跨域学习任务的差距，为触觉感知驱动的机器人技能学习提供了有效的仿真解决方案

Abstract: Robot skill acquisition processes driven by reinforcement learning often rely
on simulations to efficiently generate large-scale interaction data. However,
the absence of simulation models for tactile sensors has hindered the use of
tactile sensing in such skill learning processes, limiting the development of
effective policies driven by tactile perception. To bridge this gap, we present
TwinTac, a system that combines the design of a physical tactile sensor with
its digital twin model. Our hardware sensor is designed for high sensitivity
and a wide measurement range, enabling high quality sensing data essential for
object interaction tasks. Building upon the hardware sensor, we develop the
digital twin model using a real-to-sim approach. This involves collecting
synchronized cross-domain data, including finite element method results and the
physical sensor's outputs, and then training neural networks to map simulated
data to real sensor responses. Through experimental evaluation, we
characterized the sensitivity of the physical sensor and demonstrated the
consistency of the digital twin in replicating the physical sensor's output.
Furthermore, by conducting an object classification task, we showed that
simulation data generated by our digital twin sensor can effectively augment
real-world data, leading to improved accuracy. These results highlight
TwinTac's potential to bridge the gap in cross-domain learning tasks.

</details>


### [220] [Robot guide with multi-agent control and automatic scenario generation with LLM](https://arxiv.org/abs/2509.10317)
*Elizaveta D. Moskovskaya,Anton D. Moscowsky*

Main category: cs.RO

TL;DR: 开发了一种结合多智能体资源管理和基于大语言模型的自动行为场景生成的混合控制架构，用于人形导游机器人，解决了传统系统手动配置、灵活性差和行为不自然的问题。


<details>
  <summary>Details</summary>
Motivation: 传统导游机器人系统依赖手动调整行为场景，存在配置繁琐、灵活性低和机器人行为不够自然等局限性，需要更自动化和自然的控制方法。

Method: 采用两阶段生成过程：首先生成风格化叙述文本，然后整合非语言动作标签；使用多智能体系统协调并行动作执行和冲突解决，并在主要操作完成后维持默认行为。

Result: 试验结果表明，该方法在自动化和社会化机器人控制系统扩展方面具有潜力，能够实现更自然的机器人行为。

Conclusion: 提出的混合控制架构成功克服了传统系统的局限性，为社交机器人控制系统的自动化和规模化提供了有效解决方案。

Abstract: The work describes the development of a hybrid control architecture for an
anthropomorphic tour guide robot, combining a multi-agent resource management
system with automatic behavior scenario generation based on large language
models. The proposed approach aims to overcome the limitations of traditional
systems, which rely on manual tuning of behavior scenarios. These limitations
include manual configuration, low flexibility, and lack of naturalness in robot
behavior. The process of preparing tour scenarios is implemented through a
two-stage generation: first, a stylized narrative is created, then non-verbal
action tags are integrated into the text. The multi-agent system ensures
coordination and conflict resolution during the execution of parallel actions,
as well as maintaining default behavior after the completion of main
operations, contributing to more natural robot behavior. The results obtained
from the trial demonstrate the potential of the proposed approach for
automating and scaling social robot control systems.

</details>


### [221] [Efficient Learning-Based Control of a Legged Robot in Lunar Gravity](https://arxiv.org/abs/2509.10128)
*Philip Arm,Oliver Fischer,Joseph Church,Adrian Fuhrer,Hendrik Kolvenbach,Marco Hutter*

Main category: cs.RO

TL;DR: 提出基于强化学习的足式机器人控制方法，通过重力缩放功率优化奖励函数，在月球重力到超地球重力环境下实现高效节能的运动控制


<details>
  <summary>Details</summary>
Motivation: 行星探测机器人面临严格的功率和热预算限制，需要能够在多种重力环境下高效运行的节能控制方法

Method: 使用强化学习开发重力缩放功率优化的奖励函数，设计恒力弹簧卸载系统进行月球重力下的真实实验验证

Result: 在地球重力下功率消耗23.4W（比基线提升23%），在月球重力下12.2W（比基线降低36%），成功在多种重力环境下验证控制器的可扩展性

Conclusion: 该方法为足式机器人提供了跨多重力级别开发功率高效运动控制器的可扩展解决方案

Abstract: Legged robots are promising candidates for exploring challenging areas on
low-gravity bodies such as the Moon, Mars, or asteroids, thanks to their
advanced mobility on unstructured terrain. However, as planetary robots' power
and thermal budgets are highly restricted, these robots need energy-efficient
control approaches that easily transfer to multiple gravity environments. In
this work, we introduce a reinforcement learning-based control approach for
legged robots with gravity-scaled power-optimized reward functions. We use our
approach to develop and validate a locomotion controller and a base pose
controller in gravity environments from lunar gravity (1.62 m/s2) to a
hypothetical super-Earth (19.62 m/s2). Our approach successfully scales across
these gravity levels for locomotion and base pose control with the
gravity-scaled reward functions. The power-optimized locomotion controller
reached a power consumption for locomotion of 23.4 W in Earth gravity on a
15.65 kg robot at 0.4 m/s, a 23 % improvement over the baseline policy.
Additionally, we designed a constant-force spring offload system that allowed
us to conduct real-world experiments on legged locomotion in lunar gravity. In
lunar gravity, the power-optimized control policy reached 12.2 W, 36 % less
than a baseline controller which is not optimized for power efficiency. Our
method provides a scalable approach to developing power-efficient locomotion
controllers for legged robots across multiple gravity levels.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [222] [Engineering Spatial and Molecular Features from Cellular Niches to Inform Predictions of Inflammatory Bowel Disease](https://arxiv.org/abs/2509.09923)
*Myles Joshua Toledo Tan,Maria Kapetanaki,Panayiotis V. Benos*

Main category: q-bio.GN

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Differentiating between the two main subtypes of Inflammatory Bowel Disease
(IBD): Crohns disease (CD) and ulcerative colitis (UC) is a persistent clinical
challenge due to overlapping presentations. This study introduces a novel
computational framework that employs spatial transcriptomics (ST) to create an
explainable machine learning model for IBD classification. We analyzed ST data
from the colonic mucosa of healthy controls (HC), UC, and CD patients. Using
Non-negative Matrix Factorization (NMF), we first identified four recurring
cellular niches, representing distinct functional microenvironments within the
tissue. From these niches, we systematically engineered 44 features capturing
three key aspects of tissue pathology: niche composition, neighborhood
enrichment, and niche-gene signals. A multilayer perceptron (MLP) classifier
trained on these features achieved an accuracy of 0.774 +/- 0.161 for the more
challenging three-class problem (HC, UC, and CD) and 0.916 +/- 0.118 in the
two-class problem of distinguishing IBD from healthy tissue. Crucially, model
explainability analysis revealed that disruptions in the spatial organization
of niches were the strongest predictors of general inflammation, while the
classification between UC and CD relied on specific niche-gene expression
signatures. This work provides a robust, proof-of-concept pipeline that
transforms descriptive spatial data into an accurate and explainable predictive
tool, offering not only a potential new diagnostic paradigm but also deeper
insights into the distinct biological mechanisms that drive IBD subtypes.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [223] [Vibe Check: Understanding the Effects of LLM-Based Conversational Agents' Personality and Alignment on User Perceptions in Goal-Oriented Tasks](https://arxiv.org/abs/2509.09870)
*Hasibur Rahman,Smit Desai*

Main category: cs.HC

TL;DR: 这篇论文研究了语言模型会话机器人的人格表达程度和用户-机器人人格匹配对用户感知的影响，发现中等表达程度效果最佳，人格匹配能进一步提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型让会话机器人能够表达不同人格，需要研究这种设计如何影响用户感知，特别是在目标导向任务中的效果。

Method: 采用组间实验设计(N=150)，让参与者与具有低、中、高人格表达程度的旅行规划机器人交互，通过新颖的Trait Modulation Keys框架控制人格特征。

Result: 结果显示倒U垂关系：中等表达程度在智力、享受度、人种化、采用意向、信任和喜爱度方面获得最积极评价，显著超过两个极端。外向性和情绪稳定性是最关键的人格特征。

Conclusion: 人格表达程度和战略性人格匹配是会话机器人设计的最佳目标，为LLM基础的会话机器人提供了重要设计启示。

Abstract: Large language models (LLMs) enable conversational agents (CAs) to express
distinctive personalities, raising new questions about how such designs shape
user perceptions. This study investigates how personality expression levels and
user-agent personality alignment influence perceptions in goal-oriented tasks.
In a between-subjects experiment (N=150), participants completed travel
planning with CAs exhibiting low, medium, or high expression across the Big
Five traits, controlled via our novel Trait Modulation Keys framework. Results
revealed an inverted-U relationship: medium expression produced the most
positive evaluations across Intelligence, Enjoyment, Anthropomorphism,
Intention to Adopt, Trust, and Likeability, significantly outperforming both
extremes. Personality alignment further enhanced outcomes, with Extraversion
and Emotional Stability emerging as the most influential traits. Cluster
analysis identified three distinct compatibility profiles, with "Well-Aligned"
users reporting substantially positive perceptions. These findings demonstrate
that personality expression and strategic trait alignment constitute optimal
design targets for CA personality, offering design implications as LLM-based
CAs become increasingly prevalent.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [224] [DCHO: A Decomposition-Composition Framework for Predicting Higher-Order Brain Connectivity to Enhance Diverse Downstream Applications](https://arxiv.org/abs/2509.09696)
*Weibin Li,Wendu Li,Quanying Liu*

Main category: q-bio.NC

TL;DR: DCHO是一个用于建模和预测高阶脑连接动态演化的统一框架，通过分解-组合策略将预测任务转化为HOBC推断和潜在轨迹预测两个子问题，在多个神经影像数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的高阶脑连接分析主要关注静态分析，限制了其在动态预测任务中的应用。需要一种能够建模和预测HOBC时间演化的方法。

Method: 采用分解-组合框架，包含双视图编码器提取多尺度拓扑特征，潜在组合学习器捕获高级HOBC信息，以及潜在空间预测损失来增强时间轨迹建模。

Result: 在多个神经影像数据集上的实验表明，DCHO在非预测任务（状态分类）和预测任务（脑动力学预测）中都取得了优越性能，显著优于现有方法。

Conclusion: DCHO提供了一个有效的统一框架来建模和预测高阶脑连接的动态演化，为脑连接分析开辟了新的研究方向。

Abstract: Higher-order brain connectivity (HOBC), which captures interactions among
three or more brain regions, provides richer organizational information than
traditional pairwise functional connectivity (FC). Recent studies have begun to
infer latent HOBC from noninvasive imaging data, but they mainly focus on
static analyses, limiting their applicability in dynamic prediction tasks. To
address this gap, we propose DCHO, a unified approach for modeling and
forecasting the temporal evolution of HOBC based on a Decomposition-Composition
framework, which is applicable to both non-predictive tasks (state
classification) and predictive tasks (brain dynamics forecasting). DCHO adopts
a decomposition-composition strategy that reformulates the prediction task into
two manageable subproblems: HOBC inference and latent trajectory prediction. In
the inference stage, we propose a dual-view encoder to extract multiscale
topological features and a latent combinatorial learner to capture high-level
HOBC information. In the forecasting stage, we introduce a latent-space
prediction loss to enhance the modeling of temporal trajectories. Extensive
experiments on multiple neuroimaging datasets demonstrate that DCHO achieves
superior performance in both non-predictive tasks (state classification) and
predictive tasks (brain dynamics forecasting), significantly outperforming
existing methods.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [225] [Openness in AI and downstream governance: A global value chain approach](https://arxiv.org/abs/2509.10220)
*Christopher Foster*

Main category: cs.CY

TL;DR: 本文探讨AI开放性的经济影响，认为开放性作为独特的跨企业关系形式，可能在全球AI技术领导权竞争中产生潜在溢出效应，尽管仍需警惕大型科技公司的权力集中。


<details>
  <summary>Details</summary>
Motivation: AI快速发展导致权力和价值集中在少数大型科技公司手中，而开放性AI（开源模型、数据集和工具链）的兴起提出了重要问题：开放性资源是否能支持技术转移和追赶能力，挑战AI行业权力集中。

Method: 将AI开放性概念化为独特的跨企业关系类型，运用价值链分析方法，考察基础AI公司在价值链中的"外包"资本主义动态，以及下游AI采用可能出现的治理和控制类型。

Result: 构建了一个将基础AI与下游价值链联系起来的分析框架，扩展了先前对AI价值链的映射，深化了对AI作为生产性部门的理解。

Conclusion: 尽管仍需批判领先AI公司的权力，但AI开放性可能在全球技术领导权竞争中产生潜在溢出效应，为技术转移和追赶提供可能性。

Abstract: The rise of AI has been rapid, becoming a leading sector for investment and
promising disruptive impacts across the economy. Within the critical analysis
of the economic impacts, AI has been aligned to the critical literature on data
power and platform capitalism - further concentrating power and value capture
amongst a small number of "big tech" leaders.
  The equally rapid rise of openness in AI (here taken to be claims made by AI
firms about openness, "open source" and free provision) signals an interesting
development. It highlights an emerging ecosystem of open AI models, datasets
and toolchains, involving massive capital investment. It poses questions as to
whether open resources can support technological transfer and the ability for
catch-up, even in the face of AI industry power.
  This work seeks to add conceptual clarity to these debates by conceptualising
openness in AI as a unique type of interfirm relation and therefore amenable to
value chain analysis. This approach then allows consideration of the capitalist
dynamics of "outsourcing" of foundational firms in value chains, and
consequently the types of governance and control that might emerge downstream
as AI is adopted. This work, therefore, extends previous mapping of AI value
chains to build a framework which links foundational AI with downstream value
chains.
  Overall, this work extends our understanding of AI as a productive sector.
While the work remains critical of the power of leading AI firms, openness in
AI may lead to potential spillovers stemming from the intense competition for
global technological leadership in AI.

</details>


### [226] [We Need a New Ethics for a World of AI Agents](https://arxiv.org/abs/2509.10289)
*Iason Gabriel,Geoff Keeling,Arianna Manzini,James Evans*

Main category: cs.CY

TL;DR: 人工智能代理的普及带来安全、人机关系和社会协调的新挑战，需要科学家、工程师和政策制定者加强关注


<details>
  <summary>Details</summary>
Motivation: 讨论AI代理普及化世界的深层次影响，提醒社会各界重视相关挑战

Method: 通过理论分析和问题探讨，识别关键挑战领域

Result: 确定了人类与AI代理之间以及代理之间交互的核心安全问题

Conclusion: 心并推进广泛有益的AI代理相互作用需要多方协作和主动应对

Abstract: The deployment of capable AI agents raises fresh questions about safety,
human-machine relationships and social coordination. We argue for greater
engagement by scientists, scholars, engineers and policymakers with the
implications of a world increasingly populated by AI agents. We explore key
challenges that must be addressed to ensure that interactions between humans
and agents, and among agents themselves, remain broadly beneficial.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [227] [An Information-Theoretic Framework for Credit Risk Modeling: Unifying Industry Practice with Statistical Theory for Fair and Interpretable Scorecards](https://arxiv.org/abs/2509.09855)
*Agus Sudjianto,Denis Burakov*

Main category: stat.ML

TL;DR: 本文建立了信息理论框架，统一了信用风险建模中的WoE、IV和PSI指标，证明它们都是经典信息散度的实例，并首次推导出标准误差用于假设检验和概率公平约束。


<details>
  <summary>Details</summary>
Motivation: 信用风险建模广泛使用WoE、IV和PSI等指标，但这些指标的理论基础相互脱节，缺乏统一的统计基础。

Method: 通过delta方法应用于WoE变换，推导IV和PSI的标准误差；使用深度为1的XGBoost树桩进行自动分箱；比较三种编码策略并采用混合整数规划寻找帕累托有效解。

Result: 所有方法都达到相当的预测性能（AUC 0.82-0.84），证明基于信息理论的分箱方法比编码选择更重要；能够量化性能-公平前沿的不确定性。

Conclusion: 该框架为广泛使用的信用风险指标提供了首个严格的统计基础，同时为受监管环境中平衡准确性和公平性提供了原则性工具。

Abstract: Credit risk modeling relies extensively on Weight of Evidence (WoE) and
Information Value (IV) for feature engineering, and Population Stability Index
(PSI) for drift monitoring, yet their theoretical foundations remain
disconnected. We establish a unified information-theoretic framework revealing
these industry-standard metrics as instances of classical information
divergences. Specifically, we prove that IV exactly equals PSI (Jeffreys
divergence) computed between good and bad credit outcomes over identical bins.
Through the delta method applied to WoE transformations, we derive standard
errors for IV and PSI, enabling formal hypothesis testing and probabilistic
fairness constraints for the first time. We formalize credit modeling's
inherent performance-fairness trade-off as maximizing IV for predictive power
while minimizing IV for protected attributes. Using automated binning with
depth-1 XGBoost stumps, we compare three encoding strategies: logistic
regression with one-hot encoding, WoE transformation, and constrained XGBoost.
All methods achieve comparable predictive performance (AUC 0.82-0.84),
demonstrating that principled, information-theoretic binning outweighs encoding
choice. Mixed-integer programming traces Pareto-efficient solutions along the
performance-fairness frontier with uncertainty quantification. This framework
bridges theory and practice, providing the first rigorous statistical
foundation for widely-used credit risk metrics while offering principled tools
for balancing accuracy and fairness in regulated environments.

</details>


### [228] [Repulsive Monte Carlo on the sphere for the sliced Wasserstein distance](https://arxiv.org/abs/2509.10166)
*Vladimir Petrovic,Rémi Bardenet,Agnès Desolneux*

Main category: stat.ML

TL;DR: 本文研究在高维球面上使用蒙特卡洛方法计算函数积分的问题，特别关注切片Wasserstein距离的计算。提出使用排斥性节点(负相关)的积分方法，分析UnifOrtho估计器的方差特性，并给出不同维度下的计算建议。


<details>
  <summary>Details</summary>
Motivation: 切片Wasserstein距离在机器学习中作为Wasserstein距离的替代或独立距离度量具有重要价值，但现有数值积分方法存在改进空间。本文旨在探索使用排斥性节点(负相关)的积分方法，以降低方差并提高计算效率。

Method: 从确定性点过程(DPPs)和排斥点过程文献中提取积分方法，分析UnifOrtho正交蒙特卡洛估计器的方差特性，并进行数值基准测试比较不同积分方法的性能。

Result: 发现UnifOrtho估计器在高维情况下对切片Wasserstein距离估计效果显著，DPP-based积分方法仅在准蒙特卡洛有效时表现良好，排斥性积分方法总体上显示中等程度的方差降低。

Conclusion: 推荐在低维情况下使用随机化准蒙特卡洛方法，在高维情况下使用UnifOrtho方法。DPP-based积分方法适用范围有限，排斥性积分方法需要更多理论工作来提高鲁棒性。

Abstract: In this paper, we consider the problem of computing the integral of a
function on the unit sphere, in any dimension, using Monte Carlo methods.
Although the methods we present are general, our guiding thread is the sliced
Wasserstein distance between two measures on $\mathbb{R}^d$, which is precisely
an integral on the $d$-dimensional sphere. The sliced Wasserstein distance (SW)
has gained momentum in machine learning either as a proxy to the less
computationally tractable Wasserstein distance, or as a distance in its own
right, due in particular to its built-in alleviation of the curse of
dimensionality. There has been recent numerical benchmarks of quadratures for
the sliced Wasserstein, and our viewpoint differs in that we concentrate on
quadratures where the nodes are repulsive, i.e. negatively dependent. Indeed,
negative dependence can bring variance reduction when the quadrature is adapted
to the integration task. Our first contribution is to extract and motivate
quadratures from the recent literature on determinantal point processes (DPPs)
and repelled point processes, as well as repulsive quadratures from the
literature specific to the sliced Wasserstein distance. We then numerically
benchmark these quadratures. Moreover, we analyze the variance of the UnifOrtho
estimator, an orthogonal Monte Carlo estimator. Our analysis sheds light on
UnifOrtho's success for the estimation of the sliced Wasserstein in large
dimensions, as well as counterexamples from the literature. Our final
recommendation for the computation of the sliced Wasserstein distance is to use
randomized quasi-Monte Carlo in low dimensions and \emph{UnifOrtho} in large
dimensions. DPP-based quadratures only shine when quasi-Monte Carlo also does,
while repelled quadratures show moderate variance reduction in general, but
more theoretical effort is needed to make them robust.

</details>


### [229] [Why does your graph neural network fail on some graphs? Insights from exact generalisation error](https://arxiv.org/abs/2509.10337)
*Nil Ayday,Mahalakshmi Sabanayagam,Debarghya Ghoshdastidar*

Main category: stat.ML

TL;DR: 该论文通过信号处理视角推导了图神经网络在转导固定设计设置下的精确泛化误差，揭示了只有节点特征与图结构对齐的信息才有助于泛化，并量化了同配性对泛化的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管GNN在图结构数据学习中广泛应用，但其成功或失败的理论理解仍然不足。现有研究主要关注架构限制（如过平滑和过挤压），但无法解释GNN如何提取有意义的表示或为何相似架构间性能差异巨大，这涉及泛化能力的问题。

Method: 采用信号处理视角，将GNN解释为通过图结构对节点特征进行操作的图滤波器。聚焦线性GNN同时允许图滤波器中的非线性，推导了包括卷积、PageRank和注意力模型在内的广泛GNN类型的精确泛化误差。

Result: 推导出了首个针对多种GNN的精确泛化误差，发现只有节点特征与图结构对齐的信息对泛化有贡献。量化了同配性对泛化的具体影响，提供了模型选择的理论指导。

Conclusion: 该工作建立了一个解释GNN何时以及为何能有效利用结构和特征信息的理论框架，为理解GNN泛化机制和实际模型选择提供了重要见解。

Abstract: Graph Neural Networks (GNNs) are widely used in learning on graph-structured
data, yet a principled understanding of why they succeed or fail remains
elusive. While prior works have examined architectural limitations such as
over-smoothing and over-squashing, these do not explain what enables GNNs to
extract meaningful representations or why performance varies drastically
between similar architectures. These questions are related to the role of
generalisation: the ability of a model to make accurate predictions on
unlabelled data. Although several works have derived generalisation error
bounds for GNNs, these are typically loose, restricted to a single
architecture, and offer limited insight into what governs generalisation in
practice. In this work, we take a different approach by deriving the exact
generalisation error for GNNs in a transductive fixed-design setting through
the lens of signal processing. From this viewpoint, GNNs can be interpreted as
graph filter operators that act on node features via the graph structure. By
focusing on linear GNNs while allowing non-linearity in the graph filters, we
derive the first exact generalisation error for a broad range of GNNs,
including convolutional, PageRank-based, and attention-based models. The exact
characterisation of the generalisation error reveals that only the aligned
information between node features and graph structure contributes to
generalisation. Furthermore, we quantify the effect of homophily on
generalisation. Our work provides a framework that explains when and why GNNs
can effectively leverage structural and feature information, offering practical
guidance for model selection.

</details>


### [230] [Differentially Private Decentralized Dataset Synthesis Through Randomized Mixing with Correlated Noise](https://arxiv.org/abs/2509.10385)
*Utsab Saha,Tanvir Muntakim Tonoy,Hafiz Imtiaz*

Main category: stat.ML

TL;DR: 提出CAPE辅助的联邦DP-CDA算法，通过客户端间生成反相关噪声来改善联邦学习中的隐私-效用权衡，在MNIST和FashionMNIST数据集上实现了接近集中式设置的性能。


<details>
  <summary>Details</summary>
Motivation: 解决DP-CDA在联邦学习环境中面临的挑战：客户端数据量有限导致本地计算敏感度增加，需要注入更多噪声来保证差分隐私，从而导致效用显著下降。

Method: 将CAPE协议集成到联邦DP-CDA框架中，允许客户端生成联合分布的反相关噪声，在聚合时相互抵消，同时保持个体级别的隐私保护。

Result: 在MNIST和FashionMNIST数据集上的大量实验表明，该方法在某些参数范围内可以达到与集中式对应方法相当的效用，同时保持严格的差分隐私保证。

Conclusion: CAPE辅助的联邦DP-CDA算法显著改善了联邦设置中的隐私-效用权衡，为分布式环境下的差分隐私数据合成提供了有效解决方案。

Abstract: In this work, we explore differentially private synthetic data generation in
a decentralized-data setting by building on the recently proposed
Differentially Private Class-Centric Data Aggregation (DP-CDA). DP-CDA
synthesizes data in a centralized setting by mixing multiple randomly-selected
samples from the same class and injecting carefully calibrated Gaussian noise,
ensuring ({\epsilon}, {\delta})-differential privacy. When deployed in a
decentralized or federated setting, where each client holds only a small
partition of the data, DP-CDA faces new challenges. The limited sample size per
client increases the sensitivity of local computations, requiring higher noise
injection to maintain the differential privacy guarantee. This, in turn, leads
to a noticeable degradation in the utility compared to the centralized setting.
To mitigate this issue, we integrate the Correlation-Assisted Private
Estimation (CAPE) protocol into the federated DP-CDA framework and propose CAPE
Assisted Federated DP-CDA algorithm. CAPE enables limited collaboration among
the clients by allowing them to generate jointly distributed (anti-correlated)
noise that cancels out in aggregate, while preserving privacy at the individual
level. This technique significantly improves the privacy-utility trade-off in
the federated setting. Extensive experiments on MNIST and FashionMNIST datasets
demonstrate that the proposed CAPE Assisted Federated DP-CDA approach can
achieve utility comparable to its centralized counterpart under some parameter
regime, while maintaining rigorous differential privacy guarantees.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [231] [Machine-learning competition to grade EEG background patterns in newborns with hypoxic-ischaemic encephalopathy](https://arxiv.org/abs/2509.09695)
*Fabio Magarelli,Geraldine B. Boylan,Saeed Montazeri,Feargal O'Sullivan,Dominic Lightbody,Minoo Ashoori,Tamara Skoric Ceranic,John M. O'Toole*

Main category: eess.SP

TL;DR: 通过机器学习竞赛开发新生儿脑电图背景模式严重程度分类模型，深度学习模型在未见数据上表现更好，强调大规模多样性数据集的重要性


<details>
  <summary>Details</summary>
Motivation: 解决新生儿脑功能监测中精准模型开发靠的高质量注释数据短缺问题，通过竞赛形式激发研究人员利用专业数据集进行协作学习

Method: 编译包含353小时102名新生儿的多中心EEG数据集，分为训练、测试和验证集，建立网页竞赛平台开发EEG背景模式严重程度分类模型

Result: 虽然特征基模型在测试集上排名第一，但深度学习模型在验证集上表现更好；所有模型在验证集上性能都显著下降

Conclusion: 强调了模型在未见数据上普遍化的挑战，强调大规模多样性数据集对于稳健模型开发的重要性，开放访问数据和协作开发有助于加快临床决策支持工具发展

Abstract: Machine learning (ML) has the potential to support and improve expert
performance in monitoring the brain function of at-risk newborns. Developing
accurate and reliable ML models depends on access to high-quality, annotated
data, a resource in short supply. ML competitions address this need by
providing researchers access to expertly annotated datasets, fostering shared
learning through direct model comparisons, and leveraging the benefits of
crowdsourcing diverse expertise. We compiled a retrospective dataset containing
353 hours of EEG from 102 individual newborns from a multi-centre study. The
data was fully anonymised and divided into training, testing, and held-out
validation datasets. EEGs were graded for the severity of abnormal background
patterns. Next, we created a web-based competition platform and hosted a
machine learning competition to develop ML models for classifying the severity
of EEG background patterns in newborns. After the competition closed, the top 4
performing models were evaluated offline on a separate held-out validation
dataset. Although a feature-based model ranked first on the testing dataset,
deep learning models generalised better on the validation sets. All methods had
a significant decline in validation performance compared to the testing
performance. This highlights the challenges for model generalisation on unseen
data, emphasising the need for held-out validation datasets in ML studies with
neonatal EEG. The study underscores the importance of training ML models on
large and diverse datasets to ensure robust generalisation. The competition's
outcome demonstrates the potential for open-access data and collaborative ML
development to foster a collaborative research environment and expedite the
development of clinical decision-support tools for neonatal neuromonitoring.

</details>


### [232] [FetalSleepNet: A Transfer Learning Framework with Spectral Equalisation Domain Adaptation for Fetal Sleep Stage Classification](https://arxiv.org/abs/2509.10082)
*Weitao Tang,Johann Vargas-Calixto,Nasim Katebi,Nhi Tran,Sharmony B. Kelly,Gari D. Clifford,Robert Galinsky,Faezeh Marzbanrad*

Main category: eess.SP

TL;DR: FetalSleepNet是首个用于胎儿羊脑电图睡眠分期分类的深度学习框架，通过迁移学习和频谱均衡技术，在胎儿EEG睡眠分期中达到86.6%的准确率。


<details>
  <summary>Details</summary>
Motivation: 胎儿脑电图获取复杂且人工解释困难，准确的睡眠分期有助于早期检测与妊娠并发症相关的异常脑成熟。

Method: 使用轻量级深度神经网络，通过从成人EEG的迁移学习训练胎儿羊EEG数据，采用频谱均衡域适应策略减少跨域不匹配。

Result: 完全微调结合频谱均衡获得最佳性能（准确率86.6%，宏观F1分数62.5），优于基线模型。

Conclusion: FetalSleepNet是首个专门用于胎儿EEG自动睡眠分期的深度学习框架，其轻量级设计适合低功耗实时可穿戴胎儿监测系统部署。

Abstract: Introduction: This study presents FetalSleepNet, the first published deep
learning approach to classifying sleep states from the ovine
electroencephalogram (EEG). Fetal EEG is complex to acquire and difficult and
laborious to interpret consistently. However, accurate sleep stage
classification may aid in the early detection of abnormal brain maturation
associated with pregnancy complications (e.g. hypoxia or intrauterine growth
restriction).
  Methods: EEG electrodes were secured onto the ovine dura over the parietal
cortices of 24 late gestation fetal sheep. A lightweight deep neural network
originally developed for adult EEG sleep staging was trained on the ovine EEG
using transfer learning from adult EEG. A spectral equalisation-based domain
adaptation strategy was used to reduce cross-domain mismatch.
  Results: We demonstrated that while direct transfer performed poorly, full
fine tuning combined with spectral equalisation achieved the best overall
performance (accuracy: 86.6 percent, macro F1-score: 62.5), outperforming
baseline models.
  Conclusions: To the best of our knowledge, FetalSleepNet is the first deep
learning framework specifically developed for automated sleep staging from the
fetal EEG. Beyond the laboratory, the EEG-based sleep stage classifier
functions as a label engine, enabling large scale weak/semi supervised labeling
and distillation to facilitate training on less invasive signals that can be
acquired in the clinic, such as Doppler Ultrasound or electrocardiogram data.
FetalSleepNet's lightweight design makes it well suited for deployment in low
power, real time, and wearable fetal monitoring systems.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [233] [RFSeek and Ye Shall Find](https://arxiv.org/abs/2509.10216)
*Noga H. Rotman,Tiago Ferreira,Hila Peleg,Mark Silberstein,Alexandra Silva*

Main category: cs.NI

TL;DR: RFSeek是一个基于LLM的交互工具，能够从RFC文档中自动提取协议逻辑的可视化摘要，生成可追溯来源的图表，帮助理解网络协议规范。


<details>
  <summary>Details</summary>
Motivation: RFC文档冗长且基于文本，阻碍了对协议操作的精确理解，需要更直观的可视化工具来提升协议理解效率。

Method: 利用大型语言模型(LLMs)自动生成可追溯来源的可探索图表，提取官方状态机和文本中隐藏的逻辑，支持用户定制化可视化。

Result: RFSeek不仅重建了RFC中的现有图表，还发现了文本中描述但图表中缺失的重要逻辑节点和边，并为复杂协议如QUIC生成了新的可视化图表。

Conclusion: 结合LLMs和形式化、用户定制化可视化的摘要可视化方法，为增强协议理解和支持稳健实现提供了有前景的方向。

Abstract: Requests for Comments (RFCs) are extensive specification documents for
network protocols, but their prose-based format and their considerable length
often impede precise operational understanding. We present RFSeek, an
interactive tool that automatically extracts visual summaries of protocol logic
from RFCs. RFSeek leverages large language models (LLMs) to generate
provenance-linked, explorable diagrams, surfacing both official state machines
and additional logic found only in the RFC text. Compared to existing RFC
visualizations, RFSeek's visual summaries are more transparent and easier to
audit against their textual source. We showcase the tool's potential through a
series of use cases, including guided knowledge extraction and semantic
diffing, applied to protocols such as TCP, QUIC, PPTP, and DCCP.
  In practice, RFSeek not only reconstructs the RFC diagrams included in some
specifications, but, more interestingly, also uncovers important logic such as
nodes or edges described in the text but missing from those diagrams. RFSeek
further derives new visualization diagrams for complex RFCs, with QUIC as a
representative case. Our approach, which we term \emph{Summary Visualization},
highlights a promising direction: combining LLMs with formal, user-customized
visualizations to enhance protocol comprehension and support robust
implementations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [234] [Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective](https://arxiv.org/abs/2509.10371)
*Seokjin Go,Joongun Park,Spandan More,Hanjiang Wu,Irene Wang,Aaron Jezghani,Tushar Krishna,Divya Mahajan*

Main category: cs.DC

TL;DR: 本文对大规模多GPU系统中LLM训练进行了全面性能分析，发现性能不仅取决于硬件扩展，还受硬件拓扑、并行策略和模型执行的复杂交互影响，提出了改进未来LLM系统可扩展性和可靠性的建议。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速扩展，训练工作负载已远超单节点分析极限，需要深入理解这些模型在大规模多GPU系统中的行为特征。

Method: 使用NVIDIA H100/H200和AMD MI250 GPU，分析密集和稀疏模型在不同并行策略（张量、流水线、数据和专家并行）下的硬件利用率、功耗和热行为，评估激活重计算和计算-通信重叠等优化效果。

Result: 研究发现：扩展硬件容量不是性能的唯一决定因素；在通信受限场景中，较少但内存更大的scale-up系统可能优于scale-out系统；某些并行组合会导致带宽利用不足；过大的微批次会引发突发执行和峰值功耗问题。

Conclusion: 训练性能由硬件、系统拓扑和模型执行之间的复杂交互共同决定，需要精心调优配置才能获得最佳性能，为未来LLM系统的设计和优化提供了重要指导。

Abstract: The rapid scaling of Large Language Models (LLMs) has pushed training
workloads far beyond the limits of single-node analysis, demanding a deeper
understanding of how these models behave across large-scale, multi-GPU systems.
In this paper, we present a comprehensive characterization of LLM training
across diverse real-world workloads and hardware platforms, including NVIDIA
H100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various
parallelism strategies -- tensor, pipeline, data, and expert -- and evaluate
their effects on hardware utilization, power consumption, and thermal behavior.
We further evaluate the effectiveness of optimizations such as activation
recomputation and compute-communication overlap. Our findings show that
performance is not determined solely by scaling hardware capacity. Scale-up
systems with fewer, higher-memory GPUs can outperform scale-out systems in
communication-bound regimes, but only under carefully tuned configurations; in
other cases, scale-out deployments achieve superior throughput. We also show
that certain parallelism combinations, such as tensor with pipeline, lead to
bandwidth underutilization due to inefficient data chunking, while increasing
microbatch sizes beyond a certain point induces bursty execution and peak power
excursions that worsen thermal throttling. These insights reveal how training
performance is shaped by complex interactions between hardware, system
topology, and model execution. We conclude by offering recommendations for
system and hardware design to improve the scalability and reliability of future
LLM systems and workloads. The source code of this project is available at
https://github.com/sitar-lab/CharLLM-PPT.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [235] [Automated Tuning for Diffusion Inverse Problem Solvers without Generative Prior Retraining](https://arxiv.org/abs/2509.09880)
*Yaşar Utku Alçalar,Junno Yun,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: ZADS是一种零样本自适应扩散采样方法，通过测试时优化自适应调整保真度权重，无需重新训练扩散先验，在加速MRI重建中优于传统压缩感知和现有扩散方法。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在解决逆问题（如加速MRI重建）时，性能严重依赖于精心调整的保真度权重，特别是在快速采样计划下。现有方法通常依赖启发式或固定权重，无法适应不同的测量条件和不规则时间步计划。

Method: 提出Zero-shot Adaptive Diffusion Sampling (ZADS)，将去噪过程视为固定的展开采样器，仅使用欠采样测量以自监督方式优化保真度权重，无需重新训练扩散先验。

Result: 在fastMRI膝盖数据集上的实验表明，ZADS始终优于传统压缩感知和最近的基于扩散的方法，能够在不同噪声计划和采集设置下提供高保真重建。

Conclusion: ZADS通过测试时自适应权重调整，有效解决了扩散模型在逆问题中的权重敏感性问题，为加速MRI重建提供了更鲁棒和通用的解决方案。

Abstract: Diffusion/score-based models have recently emerged as powerful generative
priors for solving inverse problems, including accelerated MRI reconstruction.
While their flexibility allows decoupling the measurement model from the
learned prior, their performance heavily depends on carefully tuned data
fidelity weights, especially under fast sampling schedules with few denoising
steps. Existing approaches often rely on heuristics or fixed weights, which
fail to generalize across varying measurement conditions and irregular timestep
schedules. In this work, we propose Zero-shot Adaptive Diffusion Sampling
(ZADS), a test-time optimization method that adaptively tunes fidelity weights
across arbitrary noise schedules without requiring retraining of the diffusion
prior. ZADS treats the denoising process as a fixed unrolled sampler and
optimizes fidelity weights in a self-supervised manner using only undersampled
measurements. Experiments on the fastMRI knee dataset demonstrate that ZADS
consistently outperforms both traditional compressed sensing and recent
diffusion-based methods, showcasing its ability to deliver high-fidelity
reconstructions across varying noise schedules and acquisition settings.

</details>


### [236] [Drone-Based Multispectral Imaging and Deep Learning for Timely Detection of Branched Broomrape in Tomato Farms](https://arxiv.org/abs/2509.09972)
*Mohammadreza Narimani,Alireza Pourreza,Ali Moghimi,Mohsen Mesgaran,Parastoo Farajpoor,Hamid Jafarbiglu*

Main category: eess.IV

TL;DR: 使用无人机多光谱影像和LSTM深度学习网络，结合SMOTE技术处理类别不平衡，实现了对番茄寄生植物分枝列当的早期检测，最高准确率达88.37%。


<details>
  <summary>Details</summary>
Motivation: 分枝列当对加州番茄产业构成严重威胁，传统检测方法困难且化学防治成本高、效果差、环境不友好，需要开发早期精准检测技术。

Method: 在已知感染区域，基于生长度日确定5个关键生长阶段，使用无人机多光谱影像提取番茄冠层反射率，结合LSTM网络和SMOTE技术处理数据不平衡问题。

Result: 在897生长度日时检测准确率79.09%，召回率70.36%；整合所有生长阶段并采用SMOTE后，准确率提升至88.37%，召回率达95.37%。

Conclusion: 时序多光谱分析和LSTM网络在早期列当检测方面具有强大潜力，无人机多光谱传感与深度学习结合可为精准农业提供有力工具，但需要更多实际数据支持实际部署。

Abstract: This study addresses the escalating threat of branched broomrape (Phelipanche
ramosa) to California's tomato industry, which supplies over 90 percent of U.S.
processing tomatoes. The parasite's largely underground life cycle makes early
detection difficult, while conventional chemical controls are costly,
environmentally harmful, and often ineffective. To address this, we combined
drone-based multispectral imagery with Long Short-Term Memory (LSTM) deep
learning networks, using the Synthetic Minority Over-sampling Technique (SMOTE)
to handle class imbalance. Research was conducted on a known broomrape-infested
tomato farm in Woodland, Yolo County, CA, across five key growth stages
determined by growing degree days (GDD). Multispectral images were processed to
isolate tomato canopy reflectance. At 897 GDD, broomrape could be detected with
79.09 percent overall accuracy and 70.36 percent recall without integrating
later stages. Incorporating sequential growth stages with LSTM improved
detection substantially. The best-performing scenario, which integrated all
growth stages with SMOTE augmentation, achieved 88.37 percent overall accuracy
and 95.37 percent recall. These results demonstrate the strong potential of
temporal multispectral analysis and LSTM networks for early broomrape
detection. While further real-world data collection is needed for practical
deployment, this study shows that UAV-based multispectral sensing coupled with
deep learning could provide a powerful precision agriculture tool to reduce
losses and improve sustainability in tomato production.

</details>


### [237] [Polarization Denoising and Demosaicking: Dataset and Baseline Method](https://arxiv.org/abs/2509.10098)
*Muhamad Daniel Ariff Bin Abdul Rahman,Yusuke Monno,Masayuki Tanaka,Masatoshi Okutomi*

Main category: eess.IV

TL;DR: 本文提出了一种用于偏振去噪和马赛克的新方法和数据集，为分层焦面偏振仪提供了可重现的基准方法。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏适宜的评估数据集和坚实的基准方法，偏振去噪与马赛克联合任务的研究较少。本文旨在解决这一问题。

Method: 采用先去噪后马赛克的方法，基于彻底的信号处理组件构建可重现的方法。创建了包含40个真实场景和三种噪声水平的数据集。

Result: 实验结果表明，该方法在图像重建性能方面显示出比其他替代方法更高的效果，提供了坚实的基准。

Conclusion: 本文提供的数据集和方法为偏振去噪与马赛克联合任务建立了可靠的基准，有助于推动该领域的进一步研究。

Abstract: A division-of-focal-plane (DoFP) polarimeter enables us to acquire images
with multiple polarization orientations in one shot and thus it is valuable for
many applications using polarimetric information. The image processing pipeline
for a DoFP polarimeter entails two crucial tasks: denoising and demosaicking.
While polarization demosaicking for a noise-free case has increasingly been
studied, the research for the joint task of polarization denoising and
demosaicking is scarce due to the lack of a suitable evaluation dataset and a
solid baseline method. In this paper, we propose a novel dataset and method for
polarization denoising and demosaicking. Our dataset contains 40 real-world
scenes and three noise-level conditions, consisting of pairs of noisy mosaic
inputs and noise-free full images. Our method takes a
denoising-then-demosaicking approach based on well-accepted signal processing
components to offer a reproducible method. Experimental results demonstrate
that our method exhibits higher image reconstruction performance than other
alternative methods, offering a solid baseline.

</details>


### [238] [Multi-pathology Chest X-ray Classification with Rejection Mechanisms](https://arxiv.org/abs/2509.10348)
*Yehudit Aperstein,Amit Tzahar,Alon Gottlib,Tal Verber,Ravit Shagan Damti,Alexander Apartsin*

Main category: eess.IV

TL;DR: 该研究提出了一个基于DenseNet-121的不确定性感知框架，通过熵拒绝和置信区间拒绝两种选择性预测机制，在胸片多标签分类中提高模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学影像任务中存在过度自信风险，特别是在需要同时检测多种共存病理的胸片多标签分类中，这在高风险医疗场景中构成重大风险。

Method: 使用DenseNet-121作为骨干网络，集成两种选择性预测机制：熵基拒绝和置信区间基拒绝。采用分位数校准程序调整拒绝阈值，支持全局和类别特定策略。在三个大型公共数据集上进行实验验证。

Result: 选择性拒绝改善了诊断准确性和覆盖率之间的权衡，熵基拒绝在所有病理中获得了最高的平均AUC。实验证明该方法能有效识别不确定预测并交由临床专家处理。

Conclusion: 该研究支持将选择性预测整合到AI辅助诊断工作流中，为深度学习在临床环境中更安全、不确定性感知的部署提供了实用步骤。

Abstract: Overconfidence in deep learning models poses a significant risk in
high-stakes medical imaging tasks, particularly in multi-label classification
of chest X-rays, where multiple co-occurring pathologies must be detected
simultaneously. This study introduces an uncertainty-aware framework for chest
X-ray diagnosis based on a DenseNet-121 backbone, enhanced with two selective
prediction mechanisms: entropy-based rejection and confidence interval-based
rejection. Both methods enable the model to abstain from uncertain predictions,
improving reliability by deferring ambiguous cases to clinical experts. A
quantile-based calibration procedure is employed to tune rejection thresholds
using either global or class-specific strategies. Experiments conducted on
three large public datasets (PadChest, NIH ChestX-ray14, and MIMIC-CXR)
demonstrate that selective rejection improves the trade-off between diagnostic
accuracy and coverage, with entropy-based rejection yielding the highest
average AUC across all pathologies. These results support the integration of
selective prediction into AI-assisted diagnostic workflows, providing a
practical step toward safer, uncertainty-aware deployment of deep learning in
clinical settings.

</details>


### [239] [Accelerating 3D Photoacoustic Computed Tomography with End-to-End Physics-Aware Neural Operators](https://arxiv.org/abs/2509.09894)
*Jiayun Wang,Yousuf Aborahama,Arya Khokhar,Yang Zhang,Chuwei Wang,Karteekeya Sastry,Julius Berner,Yilin Luo,Boris Bonev,Zongyi Li,Kamyar Azizzadenesheli,Lihong V. Wang,Anima Anandkumar*

Main category: eess.IV

TL;DR: Pano是一个端到端的物理感知神经网络模型，能够从稀疏传感器测量直接学习逆声学映射，实现高质量的三维光声计算机断层扫描重建，显著降低硬件要求和采集时间。


<details>
  <summary>Details</summary>
Motivation: 当前三维PACT系统需要密集的传感器阵列和长时间采集，限制了临床转化。需要开发能够从稀疏采样中重建高质量图像的方法。

Method: 采用球面离散-连续卷积保持半球形传感器几何结构，融入亥姆霍兹方程约束确保物理一致性，实现分辨率无关的端到端学习。

Result: 在模拟和真实实验数据中均能重建高质量图像，即使在显著减少传感器数量和有限角度采集配置下也能保持一致的性能。

Conclusion: Pano为3D PACT的临床转化提供了实用途径，大幅降低硬件需求而不影响重建质量，实现了实时体积成像能力。

Abstract: Photoacoustic computed tomography (PACT) combines optical contrast with
ultrasonic resolution, achieving deep-tissue imaging beyond the optical
diffusion limit. While three-dimensional PACT systems enable high-resolution
volumetric imaging for applications spanning transcranial to breast imaging,
current implementations require dense transducer arrays and prolonged
acquisition times, limiting clinical translation. We introduce Pano (PACT
imaging neural operator), an end-to-end physics-aware model that directly
learns the inverse acoustic mapping from sensor measurements to volumetric
reconstructions. Unlike existing approaches (e.g. universal back-projection
algorithm), Pano learns both physics and data priors while also being agnostic
to the input data resolution. Pano employs spherical discrete-continuous
convolutions to preserve hemispherical sensor geometry, incorporates Helmholtz
equation constraints to ensure physical consistency and operates
resolutionindependently across varying sensor configurations. We demonstrate
the robustness and efficiency of Pano in reconstructing high-quality images
from both simulated and real experimental data, achieving consistent
performance even with significantly reduced transducer counts and limited-angle
acquisition configurations. The framework maintains reconstruction fidelity
across diverse sparse sampling patterns while enabling real-time volumetric
imaging capabilities. This advancement establishes a practical pathway for
making 3D PACT more accessible and feasible for both preclinical research and
clinical applications, substantially reducing hardware requirements without
compromising image reconstruction quality.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [240] [Tackling One Health Risks: How Large Language Models are leveraged for Risk Negotiation and Consensus-building](https://arxiv.org/abs/2509.09906)
*Alexandra Fetsch,Iurii Savvateev,Racem Ben Romdhane,Martin Wiedmann,Artemiy Dimov,Maciej Durkalec,Josef Teichmann,Jakob Zinsstag,Konstantinos Koutsoumanis,Andreja Rajkovic,Jason Mann,Mauro Tonolla,Monika Ehling-Schulz,Matthias Filter,Sophia Johler*

Main category: cs.MA

TL;DR: 提出基于大语言模型的AI辅助谈判框架，用于复杂风险分析和跨部门协商，通过模拟谈判和语义分析解决信息过载问题


<details>
  <summary>Details</summary>
Motivation: 传统风险分析框架简化复杂性导致信息孤岛，需要整体性策略来平衡不同利益相关者的竞争利益，但受限于时间、信息量和视角整合的复杂性

Method: 开发AI辅助谈判框架，整合大语言模型和自主代理到以谈判为中心的风险分析工作流中，支持利益相关者模拟谈判、建模动态、预测妥协和评估方案影响

Result: 在两个真实场景（生物农药审慎使用和野生动物种群控制）中进行概念验证，证明该框架能有效缓解信息过载，增强时间约束下的决策过程

Conclusion: AI辅助谈判有潜力解决当前跨部门参与工具缺乏的问题，开源web设计适合资源有限的广泛用户群体定制开发

Abstract: Key global challenges of our times are characterized by complex
interdependencies and can only be effectively addressed through an integrated,
participatory effort. Conventional risk analysis frameworks often reduce
complexity to ensure manageability, creating silos that hinder comprehensive
solutions. A fundamental shift towards holistic strategies is essential to
enable effective negotiations between different sectors and to balance the
competing interests of stakeholders. However, achieving this balance is often
hindered by limited time, vast amounts of information, and the complexity of
integrating diverse perspectives. This study presents an AI-assisted
negotiation framework that incorporates large language models (LLMs) and
AI-based autonomous agents into a negotiation-centered risk analysis workflow.
The framework enables stakeholders to simulate negotiations, systematically
model dynamics, anticipate compromises, and evaluate solution impacts. By
leveraging LLMs' semantic analysis capabilities we could mitigate information
overload and augment decision-making process under time constraints.
Proof-of-concept implementations were conducted in two real-world scenarios:
(i) prudent use of a biopesticide, and (ii) targeted wild animal population
control. Our work demonstrates the potential of AI-assisted negotiation to
address the current lack of tools for cross-sectoral engagement. Importantly,
the solution's open source, web based design, suits for application by a
broader audience with limited resources and enables users to tailor and develop
it for their own needs.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [241] [Whisper Has an Internal Word Aligner](https://arxiv.org/abs/2509.09987)
*Sung-Lin Yeh,Yen Meng,Hao Tang*

Main category: eess.AS

TL;DR: 通过分析Whisper模型的注意力头，发现特定头能够捐锐字符级对齐信息，提出一种无监督的词级时间戳提取方法，在严格容差范围内较以往方法更准确。


<details>
  <summary>Details</summary>
Motivation: 当前对于从强大语音识别器获取准确词级时间戳的需求日益增长，但现有方法需要额外训练或性能不佳，评估标准也较为松懈，通常允许超过200毫秒的容差。

Method: 通过识别Whisper模型中能够捐锐准确词对齐的特定注意力头，并发现使用字符比使用字符片能产生更细粒度和准确的对齐。基于这些发现，提出了一种无监督的方法：通过过滤注意力头同时使用字符对Whisper进行教师强制来提取词对齐。

Result: 该方法不需要训练，且在20毫秒到100毫秒的严格容差范围内，生成的词对齐比以往工作更加准确。

Conclusion: 本研究发现了Whisper模型中特定注意力头的对齐特性，并基于此提出了一种无监督的高准确度词级时间戳提取方法，为语音识别中的精确对齐问题提供了有效解决方案。

Abstract: There is an increasing interest in obtaining accurate word-level timestamps
from strong automatic speech recognizers, in particular Whisper. Existing
approaches either require additional training or are simply not competitive.
The evaluation in prior work is also relatively loose, typically using a
tolerance of more than 200 ms. In this work, we discover attention heads in
Whisper that capture accurate word alignments and are distinctively different
from those that do not. Moreover, we find that using characters produces finer
and more accurate alignments than using wordpieces. Based on these findings, we
propose an unsupervised approach to extracting word alignments by filtering
attention heads while teacher forcing Whisper with characters. Our approach not
only does not require training but also produces word alignments that are more
accurate than prior work under a stricter tolerance between 20 ms and 100 ms.

</details>


### [242] [Unified Learnable 2D Convolutional Feature Extraction for ASR](https://arxiv.org/abs/2509.10031)
*Peter Vieting,Benedikt Hilmes,Ralf Schlüter,Hermann Ney*

Main category: eess.AS

TL;DR: 这篇论文提出了一种通用的神经网络前端结构，通过2D卷积网络实现了高效的语音特征提取，在计算资源有限的场景下保持了与现有方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 目前的神经前端技术仍大量受到经典方法的影响，虽然这种归纳偏见有助于系统设计，但研究者希望开发更通用的前端结构，并统一前端架构。

Method: 采用2D卷积网络构建通用前端，通过系统性实验渐进地减少现有技术的影响，实现了统一的前端架构设计。

Result: 该通用2D卷积前端具有高参数效率，适合计算资源有限的场景，且性能与现有的监督学习特征提取器相当。

Conclusion: 这种通用统一的前端接口方法不仅可行，而且能够在保持高效的同时实现与现有方法相当的性能表现。

Abstract: Neural front-ends represent a promising approach to feature extraction for
automatic speech recognition (ASR) systems as they enable to learn specifically
tailored features for different tasks. Yet, many of the existing techniques
remain heavily influenced by classical methods. While this inductive bias may
ease the system design, our work aims to develop a more generic front-end for
feature extraction. Furthermore, we seek to unify the front-end architecture
contrasting with existing approaches that apply a composition of several layer
topologies originating from different sources. The experiments systematically
show how to reduce the influence of existing techniques to achieve a generic
front-end. The resulting 2D convolutional front-end is parameter-efficient and
suitable for a scenario with limited computational resources unlike large
models pre-trained on unlabeled audio. The results demonstrate that this
generic unified approach is not only feasible but also matches the performance
of existing supervised learnable feature extractors.

</details>


### [243] [Error Analysis in a Modular Meeting Transcription System](https://arxiv.org/abs/2509.10143)
*Peter Vieting,Simon Berger,Thilo von Neumann,Christoph Boeddeker,Ralf Schlüter,Reinhold Haeb-Umbach*

Main category: eess.AS

TL;DR: 本文分析了语音分离中的泄漏问题，发现存在显著的跨通道泄漏，但语音活动检测能大部分忽略这些泄漏，且先进的分割方法能大幅缩小与oracle分割的性能差距。


<details>
  <summary>Details</summary>
Motivation: 会议转录领域存在性能限制，本文通过分析语音分离中的泄漏问题来探索性能提升的潜力。

Method: 扩展了之前的泄漏分析框架，加入了时间局部性敏感性。比较了不同的分割方法，包括能量基础的VAD和先进的语音分割方法。

Result: 发现在主讲者活动区域存在显著的跨通道泄漏，但VAD大部分忽略了这些泄漏。先进的分割方法能将与oracle分割的性能差距缩小三分之一。在LibriCSS数据集上达到了独立训练的最高性能。

Conclusion: 语音分离中的泄漏问题实际影响有限，而先进的语音分割技术是提升会议转录性能的关键因素。

Abstract: Meeting transcription is a field of high relevance and remarkable progress in
recent years. Still, challenges remain that limit its performance. In this
work, we extend a previously proposed framework for analyzing leakage in speech
separation with proper sensitivity to temporal locality. We show that there is
significant leakage to the cross channel in areas where only the primary
speaker is active. At the same time, the results demonstrate that this does not
affect the final performance much as these leaked parts are largely ignored by
the voice activity detection (VAD). Furthermore, different segmentations are
compared showing that advanced diarization approaches are able to reduce the
gap to oracle segmentation by a third compared to a simple energy-based VAD. We
additionally reveal what factors contribute to the remaining difference. The
results represent state-of-the-art performance on LibriCSS among systems that
train the recognition module on LibriSpeech data only.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [244] [Reinforcement learning for spin torque oscillator tasks](https://arxiv.org/abs/2509.10057)
*Jakub Mojsiejuk,Sławomir Ziętek,Witold Skowroński*

Main category: physics.app-ph

TL;DR: 使用强化学习实现自旋电子振荡器的自动频率同步，通过修改基础任务提升收敛性和能效


<details>
  <summary>Details</summary>
Motivation: 解决自旋电子振荡器(STO)的自动频率同步问题，传统方法可能效率不高，希望通过强化学习实现更有效的同步控制

Method: 使用宏自旋Landau-Lifschitz-Gilbert-Slonczewski方程数值模拟STO，训练两种类型的强化学习代理在固定步数内实现目标频率同步

Result: 在模拟环境中展示了同步收敛性和能量效率的改进，证明该方法可以有效提升STO同步性能

Conclusion: 强化学习方法能够有效实现自旋电子振荡器的频率同步，且通过任务修改可以进一步提升同步过程的收敛速度和能效表现

Abstract: We address the problem of automatic synchronisation of the spintronic
oscillator (STO) by means of reinforcement learning (RL). A numerical solution
of the macrospin Landau-Lifschitz-Gilbert-Slonczewski equation is used to
simulate the STO and we train the two types of RL agents to synchronise with a
target frequency within a fixed number of steps. We explore modifications to
this base task and show an improvement in both convergence and energy
efficiency of the synchronisation that can be easily achieved in the simulated
environment.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [245] [Standards in the Preparation of Biomedical Research Metadata: A Bridge2AI Perspective](https://arxiv.org/abs/2509.10432)
*Harry Caufield,Satrajit Ghosh,Sek Wong Kong,Jillian Parker,Nathan Sheffield,Bhavesh Patel,Andrew Williams,Timothy Clark,Monica C. Munoz-Torres*

Main category: q-bio.OT

TL;DR: 本文描述了Bridge2AI联盟如何通过元数据标准化和创建来促进生物医学数据集的AI准备度，包括FAIR原则、数据来源、可解释性等关键标准。


<details>
  <summary>Details</summary>
Motivation: 为了确保生物医学数据集能够最优化且合乎道德地用于AI和机器学习方法，需要定义AI准备度的具体标准和元数据要求。

Method: 通过评估Bridge2AI大挖战项目中元数据创建和标准化的状况，提供指南建议，识别空白区和改进领域。

Result: 形成了促进AI准备度的元数据创建实践经验，为新项目提供有价值的指导，确保数据集具备FAIR性、可追溯性、可解释性等关键特征。

Conclusion: 元数据标准化对于实现生物医学数据集的AI准备度至关重要，Bridge2AI的经验为领域内的其他项目提供了可质实施的最佳实践。

Abstract: AI-readiness describes the degree to which data may be optimally and
ethically used for subsequent AI and Machine Learning (AI/ML) methods, where
those methods may involve some combination of model training, data
classification, and ethical, explainable prediction. The Bridge2AI consortium
has defined the particular criteria a biomedical dataset may possess to render
it AI-ready: in brief, a dataset's readiness is related to its FAIRness,
provenance, degree of characterization, explainability, sustainability, and
computability, in addition to its accompaniment with documentation about
ethical data practices.
  To ensure AI-readiness and to clarify data structure and relationships within
Bridge2AI's Grand Challenges (GCs), particular types of metadata are necessary.
The GCs within the Bridge2AI initiative include four data-generating projects
focusing on generating AI/ML-ready datasets to tackle complex biomedical and
behavioral research problems. These projects develop standardized, multimodal
data, tools, and training resources to support AI integration, while addressing
ethical data practices. Examples include using voice as a biomarker, building
interpretable genomic tools, modeling disease trajectories with diverse
multimodal data, and mapping cellular and molecular health indicators across
the human body.
  This report assesses the state of metadata creation and standardization in
the Bridge2AI GCs, provides guidelines where required, and identifies gaps and
areas for improvement across the program. New projects, including those outside
the Bridge2AI consortium, would benefit from what we have learned about
creating metadata as part of efforts to promote AI readiness.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [246] [VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions](https://arxiv.org/abs/2509.09716)
*Jun Zhan,Mingyang Han,Yuxuan Xie,Chen Wang,Dong Zhang,Kexin Huang,Haoxiang Shi,DongXiao Wang,Tengtao Song,Qinyuan Cheng,Shimin Li,Jun Song,Xipeng Qiu,Bo Zheng*

Main category: cs.SD

TL;DR: 提出了语音风格适应(VSA)新任务，研究口语模型根据语音指令调整说话风格的能力，并发布了双语基准VStyle和评估框架LALM as a Judge。


<details>
  <summary>Details</summary>
Motivation: 当前口语模型主要关注语义准确性和指令跟随，但根据语音指令调整说话风格的能力研究不足，需要建立系统性的评估基准。

Method: 提出VSA任务，构建双语基准VStyle覆盖4类语音生成，开发LALM as a Judge框架进行渐进式评估（文本忠实度、风格遵循度、自然度）。

Result: 实验表明当前商业系统和开源SLM在可控风格适应方面存在明显局限性，验证了该任务的新颖性和挑战性。

Conclusion: 通过发布VStyle数据集和评估工具包，为推进以人为中心的口语交互研究提供基础，当前模型在风格适应方面仍需改进。

Abstract: Spoken language models (SLMs) have emerged as a unified paradigm for speech
understanding and generation, enabling natural human machine interaction.
However, while most progress has focused on semantic accuracy and instruction
following, the ability of SLMs to adapt their speaking style based on spoken
instructions has received limited attention. We introduce Voice Style
Adaptation (VSA), a new task that examines whether SLMs can modify their
speaking style, such as timbre, prosody, or persona following natural language
spoken commands. To study this task, we present VStyle, a bilingual (Chinese &
English) benchmark covering four categories of speech generation: acoustic
attributes, natural language instruction, role play, and implicit empathy. We
also introduce the Large Audio Language Model as a Judge (LALM as a Judge)
framework, which progressively evaluates outputs along textual faithfulness,
style adherence, and naturalness, ensuring reproducible and objective
assessment. Experiments on commercial systems and open source SLMs demonstrate
that current models face clear limitations in controllable style adaptation,
highlighting both the novelty and challenge of this task. By releasing VStyle
and its evaluation toolkit, we aim to provide the community with a foundation
for advancing human centered spoken interaction. The dataset and code are
publicly available at
\href{https://junzhan2000.github.io/VStyle.github.io/}{project's homepage}.

</details>


### [247] [Testing chatbots on the creation of encoders for audio conditioned image generation](https://arxiv.org/abs/2509.09717)
*Jorge E. León,Miguel Carrasco*

Main category: cs.SD

TL;DR: 这篇论文探索了使用现代聊天机器人设计音频编码器来替代Stable Diffusion中的CLIP文本编码器，以实现从音频直接生成图像的可能性。虽然所有聊天机器都能生成有效的模型设计，但无一能够达到满意的结果，显示了这些模型在专业编码任务上的差距。


<details>
  <summary>Details</summary>
Motivation: 虽然现代生成式图像模型主要依靠文本编码器来转换语义概念，但有明确证据表明音频也可以作为输入来使用。通过让聊天机器器设计音频编码器，探索是否能够替代文本编码器实现从音频直接生成图像。

Method: 使用5个公开聊天机器设计神经网络结构作为音频编码器，在超过200万个上下文相关的音频-图像-文本观测数据上训练每个有效的编码器，并使用多种指标在验证集和测试集上评估，同时进行生成图像的定性分析。

Result: 虽然几乎所有聊天机器都能生成有效的模型设计，但没有任何一个能够达到满意的结果。Gemini音频编码器在数量指标上表现最好，而Grok音频编码器生成了更一致的图像（特别是与文本编码器组合使用时）。

Conclusion: 研究发现聊天机器存在共享的架构偏见，并强调了在这些模型未来版本中需要弥补的编码沟涞。建议未来研究重点关注更专业化的任务，以充分测试聊天机器的创造力和推理能力。

Abstract: On one hand, recent advances in chatbots has led to a rising popularity in
using these models for coding tasks. On the other hand, modern generative image
models primarily rely on text encoders to translate semantic concepts into
visual representations, even when there is clear evidence that audio can be
employed as input as well. Given the previous, in this work, we explore whether
state-of-the-art conversational agents can design effective audio encoders to
replace the CLIP text encoder from Stable Diffusion 1.5, enabling image
synthesis directly from sound. We prompted five publicly available chatbots to
propose neural architectures to work as these audio encoders, with a set of
well-explained shared conditions. Each valid suggested encoder was trained on
over two million context related audio-image-text observations, and evaluated
on held-out validation and test sets using various metrics, together with a
qualitative analysis of their generated images. Although almost all chatbots
generated valid model designs, none achieved satisfactory results, indicating
that their audio embeddings failed to align reliably with those of the original
text encoder. Among the proposals, the Gemini audio encoder showed the best
quantitative metrics, while the Grok audio encoder produced more coherent
images (particularly, when paired with the text encoder). Our findings reveal a
shared architectural bias across chatbots and underscore the remaining coding
gap that needs to be bridged in future versions of these models. We also
created a public demo so everyone could study and try out these audio encoders.
Finally, we propose research questions that should be tackled in the future,
and encourage other researchers to perform more focused and highly specialized
tasks like this one, so the respective chatbots cannot make use of well-known
solutions and their creativity/reasoning is fully tested.

</details>


### [248] [SoilSound: Smartphone-based Soil Moisture Estimation](https://arxiv.org/abs/2509.09823)
*Yixuan Gao,Tanvir Ahmed,Shuang He,Zhongqi Cheng,Rajalakshmi Nandakumar*

Main category: cs.SD

TL;DR: SoilSound是一种基于智能手机的声学传感系统，利用内置扬声器和麦克风通过垂直扫描机制测量土壤湿度，无需校准且不干扰土壤，平均绝对误差为2.39%。


<details>
  <summary>Details</summary>
Motivation: 现有土壤湿度监测方法需要侵入式探头或专用设备，限制了公众使用。需要一种普及、易获取的解决方案。

Method: 利用手机扬声器向土壤发送声学啁啾信号，通过垂直扫描记录反射声波，基于表面粗糙度效应的声学反射模型，使用卷积神经网络进行设备端湿度估计。

Result: 在10个不同地点测试，平均绝对误差为2.39%，能够准确追踪15.9%到34.0%的土壤湿度范围，适用于多种土壤类型和环境。

Conclusion: SoilSound无需校准或干扰土壤即可实现准确的土壤湿度监测，为家庭园丁、城市农民和资源有限地区的农业社区提供了普及的监测解决方案。

Abstract: Soil moisture monitoring is essential for agriculture and environmental
management, yet existing methods require either invasive probes disturbing the
soil or specialized equipment, limiting access to the public. We present
SoilSound, an ubiquitous accessible smartphone-based acoustic sensing system
that can measure soil moisture without disturbing the soil. We leverage the
built-in speaker and microphone to perform a vertical scan mechanism to
accurately measure moisture without any calibration. Unlike existing work that
use transmissive properties, we propose an alternate model for acoustic
reflections in soil based on the surface roughness effect to enable moisture
sensing without disturbing the soil. The system works by sending acoustic
chirps towards the soil and recording the reflections during a vertical scan,
which are then processed and fed to a convolutional neural network for
on-device soil moisture estimation with negligible computational, memory, or
power overhead. We evaluated the system by training with curated soils in boxes
in the lab and testing in the outdoor fields and show that SoilSound achieves a
mean absolute error (MAE) of 2.39% across 10 different locations. Overall, the
evaluation shows that SoilSound can accurately track soil moisture levels
ranging from 15.9% to 34.0% across multiple soil types, environments, and
users; without requiring any calibration or disturbing the soil, enabling
widespread moisture monitoring for home gardeners, urban farmers, citizen
scientists, and agricultural communities in resource-limited settings.

</details>


### [249] [CoDiCodec: Unifying Continuous and Discrete Compressed Representations of Audio](https://arxiv.org/abs/2509.09836)
*Marco Pasini,Stefan Lattner,George Fazekas*

Main category: cs.SD

TL;DR: CoDiCodec是一种新颖的音频自编码器，通过有限标量量化和FSQ-dropout技术，同时生成连续嵌入和离散标记，在11Hz和2.38kbps速率下提供高质量的音频压缩，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有音频自编码器需要在连续嵌入和离散标记之间做出选择，且在高压缩比下保持音频保真度仍具挑战性。需要一种能够同时支持两种表示形式的统一方法。

Method: 采用有限标量量化(FSQ)和新型FSQ-dropout技术，通过单一一致性损失进行端到端训练，支持自回归解码和并行解码策略。

Result: 在相似比特率下，CoDiCodec在重建音频质量方面优于现有的连续和离散自编码器，并行解码策略实现了更优的音频质量和更快的解码速度。

Conclusion: CoDiCodec为音频压缩提供了统一的方法，弥合了连续和离散生成建模范式之间的差距，为下游生成任务提供了前所未有的灵活性。

Abstract: Efficiently representing audio signals in a compressed latent space is
critical for latent generative modelling. However, existing autoencoders often
force a choice between continuous embeddings and discrete tokens. Furthermore,
achieving high compression ratios while maintaining audio fidelity remains a
challenge. We introduce CoDiCodec, a novel audio autoencoder that overcomes
these limitations by both efficiently encoding global features via summary
embeddings, and by producing both compressed continuous embeddings at ~ 11 Hz
and discrete tokens at a rate of 2.38 kbps from the same trained model,
offering unprecedented flexibility for different downstream generative tasks.
This is achieved through Finite Scalar Quantization (FSQ) and a novel
FSQ-dropout technique, and does not require additional loss terms beyond the
single consistency loss used for end-to-end training. CoDiCodec supports both
autoregressive decoding and a novel parallel decoding strategy, with the latter
achieving superior audio quality and faster decoding. CoDiCodec outperforms
existing continuous and discrete autoencoders at similar bitrates in terms of
reconstruction audio quality. Our work enables a unified approach to audio
compression, bridging the gap between continuous and discrete generative
modelling paradigms.

</details>


### [250] [Prototypical Contrastive Learning For Improved Few-Shot Audio Classification](https://arxiv.org/abs/2509.10074)
*Christos Sgouropoulos,Christos Nikou,Stefanos Vlachos,Vasileios Theiou,Christos Foukanelis,Theodoros Giannakopoulos*

Main category: cs.SD

TL;DR: 本文提出了一种将监督对比损失融入原型小样本学习的音频分类方法，使用角度损失和自注意力机制，在MetaAudio基准上取得了最先进的性能


<details>
  <summary>Details</summary>
Motivation: 音频领域的小样本学习相对未被充分探索，现有研究主要集中在图像领域，需要探索音频分类中的小样本学习方法

Method: 将监督对比损失整合到原型小样本训练中，使用角度损失替代标准对比损失，采用SpecAugment数据增强和自注意力机制来统一增强输入的嵌入表示

Result: 在MetaAudio基准测试的5-way 5-shot设置中取得了最先进的性能

Conclusion: 提出的方法通过整合监督对比损失和角度损失，有效提升了音频小样本分类的性能，证明了该方法在音频领域的有效性

Abstract: Few-shot learning has emerged as a powerful paradigm for training models with
limited labeled data, addressing challenges in scenarios where large-scale
annotation is impractical. While extensive research has been conducted in the
image domain, few-shot learning in audio classification remains relatively
underexplored. In this work, we investigate the effect of integrating
supervised contrastive loss into prototypical few shot training for audio
classification. In detail, we demonstrate that angular loss further improves
the performance compared to the standard contrastive loss. Our method leverages
SpecAugment followed by a self-attention mechanism to encapsulate diverse
information of augmented input versions into one unified embedding. We evaluate
our approach on MetaAudio, a benchmark including five datasets with predefined
splits, standardized preprocessing, and a comprehensive set of few-shot
learning models for comparison. The proposed approach achieves state-of-the-art
performance in a 5-way, 5-shot setting.

</details>


### [251] [Improving Audio Event Recognition with Consistency Regularization](https://arxiv.org/abs/2509.10391)
*Shanmuka Sadhu,Weiran Wang*

Main category: cs.SD

TL;DR: 本文提出将一致性正则化(CR)应用于音频事件识别，在AudioSet数据集上验证了其有效性，并在有监督和半监督设置下均取得了性能提升


<details>
  <summary>Details</summary>
Motivation: 一致性正则化在自动语音识别中已显示出优势，但尚未在音频事件识别中得到充分探索。本文旨在验证CR在音频事件识别任务中的有效性，特别是在不同规模训练集和半监督场景下的表现

Method: 使用一致性正则化技术，通过增强数据的不同视图来强制模型预测一致性。进行了广泛的消融研究，包括小规模(~20k)和大规模(~1.8M)有监督训练集，以及使用更强增强和多重增强策略。还扩展到了半监督设置，使用2万标注样本和180万未标注样本

Result: CR在有监督基线(已大量使用数据增强)基础上带来了持续改进。对于小训练集，使用更强增强和多重增强的CR带来了额外收益。在半监督设置中，相比小训练集的最佳模型获得了性能提升

Conclusion: 一致性正则化在音频事件识别中具有显著效果，能够有效提升模型性能，特别是在数据有限的情况下。该方法在有监督和半监督学习场景下都表现出良好的泛化能力

Abstract: Consistency regularization (CR), which enforces agreement between model
predictions on augmented views, has found recent benefits in automatic speech
recognition [1]. In this paper, we propose the use of consistency
regularization for audio event recognition, and demonstrate its effectiveness
on AudioSet. With extensive ablation studies for both small ($\sim$20k) and
large ($\sim$1.8M) supervised training sets, we show that CR brings consistent
improvement over supervised baselines which already heavily utilize data
augmentation, and CR using stronger augmentation and multiple augmentations
leads to additional gain for the small training set. Furthermore, we extend the
use of CR into the semi-supervised setup with 20K labeled samples and 1.8M
unlabeled samples, and obtain performance improvement over our best model
trained on the small set.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [252] [Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks](https://arxiv.org/abs/2509.09706)
*Taniya Gidatkar,Oluwaseun Ajao,Matthew Shardlow*

Main category: cs.CR

TL;DR: 本研究评估了Flan-T5、BERT和RoBERTa-Base等大型语言模型对抗对抗攻击的韧性，发现RoBERTa-Base和FlanT5表现出色（攻击成功率0%），而BERT-Base存在显著漏洞（TextFooler攻击成功率93.75%）。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在对抗攻击下的安全性和韧性，识别现有防护机制的优缺点，为开发更有效的防御策略提供依据。

Method: 使用TextFooler和BERTAttack系统设计对抗测试，对Flan-T5、BERT和RoBERTa-Base模型进行攻击实验。

Result: RoBERTa-Base和FlanT5表现出卓越韧性，攻击成功率为0%；BERT-Base准确率从48%降至3%，TextFooler攻击成功率达93.75%。

Conclusion: 某些LLMs已具备有效防御机制但计算成本高，研究为开发更高效防御策略提供了实践建议，有助于提升LLM安全性。

Abstract: This study evaluates the resilience of large language models (LLMs) against
adversarial attacks, specifically focusing on Flan-T5, BERT, and RoBERTa-Base.
Using systematically designed adversarial tests through TextFooler and
BERTAttack, we found significant variations in model robustness. RoBERTa-Base
and FlanT5 demonstrated remarkable resilience, maintaining accuracy even when
subjected to sophisticated attacks, with attack success rates of 0%. In
contrast. BERT-Base showed considerable vulnerability, with TextFooler
achieving a 93.75% success rate in reducing model accuracy from 48% to just 3%.
Our research reveals that while certain LLMs have developed effective defensive
mechanisms, these safeguards often require substantial computational resources.
This study contributes to the understanding of LLM security by identifying
existing strengths and weaknesses in current safeguarding approaches and
proposes practical recommendations for developing more efficient and effective
defensive strategies.

</details>


### [253] [ZORRO: Zero-Knowledge Robustness and Privacy for Split Learning (Full Version)](https://arxiv.org/abs/2509.09787)
*Nojan Sheybani,Alessandro Pegoraro,Jonathan Knauer,Phillip Rieger,Elissa Mollakuqe,Farinaz Koushanfar,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: ZORRO是一个基于零知识证明的分割学习防御方案，通过客户端验证确保本地训练模型的良性性质，有效防御恶意客户端注入后门的攻击。


<details>
  <summary>Details</summary>
Motivation: 分割学习在资源受限环境中处理敏感数据时很有效，但分布式特性使恶意客户端可以通过投毒中间梯度来注入后门。现有防御方案主要关注服务器端保护且引入额外开销，客户端防御面临如何强制恶意客户端正确执行防御算法的挑战。

Method: 提出ZORRO方案，应用交互式零知识证明(ZKPs)让客户端证明其正确执行了客户端防御算法，通过模型分区的频率表示进行深度检查，确保每个客户端向前传递良性检查点。

Result: 在广泛评估中，ZORRO将攻击成功率降低到6%以下，即使对于客户端存储100万个参数的模型，开销也小于10秒。

Conclusion: ZORRO提供了一种私有、可验证且鲁棒的分割学习防御方案，通过零知识证明有效解决了客户端防御的执行验证问题。

Abstract: Split Learning (SL) is a distributed learning approach that enables
resource-constrained clients to collaboratively train deep neural networks
(DNNs) by offloading most layers to a central server while keeping in- and
output layers on the client-side. This setup enables SL to leverage server
computation capacities without sharing data, making it highly effective in
resource-constrained environments dealing with sensitive data. However, the
distributed nature enables malicious clients to manipulate the training
process. By sending poisoned intermediate gradients, they can inject backdoors
into the shared DNN. Existing defenses are limited by often focusing on
server-side protection and introducing additional overhead for the server. A
significant challenge for client-side defenses is enforcing malicious clients
to correctly execute the defense algorithm.
  We present ZORRO, a private, verifiable, and robust SL defense scheme.
Through our novel design and application of interactive zero-knowledge proofs
(ZKPs), clients prove their correct execution of a client-located defense
algorithm, resulting in proofs of computational integrity attesting to the
benign nature of locally trained DNN portions. Leveraging the frequency
representation of model partitions enables ZORRO to conduct an in-depth
inspection of the locally trained models in an untrusted environment, ensuring
that each client forwards a benign checkpoint to its succeeding client. In our
extensive evaluation, covering different model architectures as well as various
attack strategies and data scenarios, we show ZORRO's effectiveness, as it
reduces the attack success rate to less than 6\% while causing even for models
storing \numprint{1000000} parameters on the client-side an overhead of less
than 10 seconds.

</details>


### [254] [SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation with Security-Aware Group Relative Policy Optimization](https://arxiv.org/abs/2509.09942)
*Lei Yu,Jingyuan Zhang,Xin Wang,Jiajia Ma,Li Yang,Fengjun Zhang*

Main category: cs.CR

TL;DR: SmartCoder-R1是一个基于Qwen2.5-Coder-7B的新型框架，通过持续预训练、长链思维监督微调和安全感知强化学习，实现了安全可解释的智能合约生成，在多项指标上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在智能合约生成中的两个关键问题：缺乏透明推理过程的黑盒特性，以及生成代码存在严重安全漏洞的问题。智能合约管理高价值资产，漏洞可能导致灾难性财务损失。

Method: 1) 持续预训练(CPT)进行模型专业化；2) 在7,998个专家验证的推理-代码样本上进行长链思维监督微调(L-CoT SFT)；3) 使用安全感知组相对策略优化(S-GRPO)进行强化学习，优化编译成功、安全合规和格式正确的加权奖励信号。

Result: 在756个真实世界函数的基准测试中，SmartCoder-R1在17个基线模型中达到最先进性能：ComPass 87.70%、VulRate 8.60%、SafeAval 80.16%、FuncRate 53.84%、FullRate 50.53%（比最强基线DeepSeek-R1提升45.79%）。生成推理在人工评估中也表现优异：功能性82.7%、安全性85.3%、清晰度90.7%。

Conclusion: SmartCoder-R1成功解决了LLM在智能合约生成中的黑盒和安全漏洞问题，通过结合专业化训练、人类安全分析模拟和安全感知强化学习，实现了安全可解释的代码生成，为高价值资产管理提供了可靠解决方案。

Abstract: Smart contracts automate the management of high-value assets, where
vulnerabilities can lead to catastrophic financial losses. This challenge is
amplified in Large Language Models (LLMs) by two interconnected failures: they
operate as unauditable "black boxes" lacking a transparent reasoning process,
and consequently, generate code riddled with critical security vulnerabilities.
To address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a
novel framework for secure and explainable smart contract generation. It begins
with Continual Pre-training (CPT) to specialize the model. We then apply Long
Chain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated
reasoning-and-code samples to train the model to emulate human security
analysis. Finally, to directly mitigate vulnerabilities, we employ
Security-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement
learning phase that refines the generation policy by optimizing a weighted
reward signal for compilation success, security compliance, and format
correctness. Evaluated against 17 baselines on a benchmark of 756 real-world
functions, SmartCoder-R1 establishes a new state of the art, achieving top
performance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a
SafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This
FullRate marks a 45.79% relative improvement over the strongest baseline,
DeepSeek-R1. Crucially, its generated reasoning also excels in human
evaluations, achieving high-quality ratings for Functionality (82.7%), Security
(85.3%), and Clarity (90.7%).

</details>


### [255] [Securing LLM-Generated Embedded Firmware through AI Agent-Driven Validation and Patching](https://arxiv.org/abs/2509.09970)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.CR

TL;DR: 提出结合LLM生成固件与自动化安全验证的三阶段方法，通过虚拟化环境迭代优化，显著提升固件安全性和实时性能


<details>
  <summary>Details</summary>
Motivation: LLM生成嵌入式固件时存在安全漏洞和实时性能不达标的问题，需要系统化的方法来确保生成代码的安全性和可靠性

Method: 三阶段方法：1)使用结构化提示词让GPT-4等模型生成固件 2)在QEMU虚拟化环境中部署测试 3)通过模糊测试、静态分析和运行时监控检测漏洞，AI代理协作进行迭代修复

Result: 漏洞修复率92.4%(提升37.3%)，威胁模型符合率95.8%，安全覆盖指数0.87，最坏执行时间8.6ms，抖动195μs

Conclusion: 该方法有效提升了LLM生成固件的安全性和实时性能，为未来研究提供了开源数据集

Abstract: Large Language Models (LLMs) show promise in generating firmware for embedded
systems, but often introduce security flaws and fail to meet real-time
performance constraints. This paper proposes a three-phase methodology that
combines LLM-based firmware generation with automated security validation and
iterative refinement in a virtualized environment. Using structured prompts,
models like GPT-4 generate firmware for networking and control tasks, deployed
on FreeRTOS via QEMU. These implementations are tested using fuzzing, static
analysis, and runtime monitoring to detect vulnerabilities such as buffer
overflows (CWE-120), race conditions (CWE-362), and denial-of-service threats
(CWE-400). Specialized AI agents for Threat Detection, Performance
Optimization, and Compliance Verification collaborate to improve detection and
remediation. Identified issues are categorized using CWE, then used to prompt
targeted LLM-generated patches in an iterative loop. Experiments show a 92.4\%
Vulnerability Remediation Rate (37.3\% improvement), 95.8\% Threat Model
Compliance, and 0.87 Security Coverage Index. Real-time metrics include 8.6ms
worst-case execution time and 195{\mu}s jitter. This process enhances firmware
security and performance while contributing an open-source dataset for future
research.

</details>


### [256] [Investigating Feature Attribution for 5G Network Intrusion Detection](https://arxiv.org/abs/2509.10206)
*Federica Uccello,Simin Nadjm-Tehrani*

Main category: cs.CR

TL;DR: 本文比较了SHAP和VoTE-XAI两种可解释AI方法在5G网络安全中的表现，发现VoTE-XAI在稀疏性、稳定性和效率方面优于SHAP，特别是在高维特征场景下。


<details>
  <summary>Details</summary>
Motivation: 随着5G网络在关键应用中的普及，需要从恶意活动检测转向能够提供可靠判决的系统。理解机器学习模型的安全警报对于实现可操作的事件响应编排至关重要，XAI技术通过提供警报原因的解释来增强信任。

Method: 研究比较了两种XAI方法：基于统计特征关联的SHAP和基于逻辑解释的VoTE-XAI。通过分析XGBoost模型在三种不同5G通信攻击用例中生成的警报解释，使用稀疏性、稳定性和效率三个指标进行评估。

Result: 在92个特征的5G网络中，VoTE-XAI对ICMPFlood DoS攻击仅识别出6个重要特征，而SHAP识别出20多个。VoTE-XAI的效率显著更高，在高维设置（478个特征）下单个解释生成时间小于0.002秒。SHAP选择的所有重要特征都被VoTE-XAI覆盖。

Conclusion: 基于逻辑解释的VoTE-XAI方法在5G网络安全警报解释中表现出更好的稀疏性、稳定性和效率，为未来通信系统的可解释AI应用提供了有价值的见解。

Abstract: With the rise of fifth-generation (5G) networks in critical applications, it
is urgent to move from detection of malicious activity to systems capable of
providing a reliable verdict suitable for mitigation. In this regard,
understanding and interpreting machine learning (ML) models' security alerts is
crucial for enabling actionable incident response orchestration. Explainable
Artificial Intelligence (XAI) techniques are expected to enhance trust by
providing insights into why alerts are raised. A dominant approach
statistically associates feature sets that can be correlated to a given alert.
This paper starts by questioning whether such attribution is relevant for
future generation communication systems, and investigates its merits in
comparison with an approach based on logical explanations. We extensively study
two methods, SHAP and VoTE-XAI, by analyzing their interpretations of alerts
generated by an XGBoost model in three different use cases with several 5G
communication attacks. We identify three metrics for assessing explanations:
sparsity, how concise they are; stability, how consistent they are across
samples from the same attack type; and efficiency, how fast an explanation is
generated. As an example, in a 5G network with 92 features, 6 were deemed
important by VoTE-XAI for a Denial of Service (DoS) variant, ICMPFlood, while
SHAP identified over 20. More importantly, we found a significant divergence
between features selected by SHAP and VoTE-XAI. However, none of the top-ranked
features selected by SHAP were missed by VoTE-XAI. When it comes to efficiency
of providing interpretations, we found that VoTE-XAI is significantly more
responsive, e.g. it provides a single explanation in under 0.002 seconds, in a
high-dimensional setting (478 features).

</details>
