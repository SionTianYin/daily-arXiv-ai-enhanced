<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 66]
- [cs.CV](#cs.CV) [Total: 69]
- [cs.AI](#cs.AI) [Total: 39]
- [cs.LG](#cs.LG) [Total: 70]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cs.IR](#cs.IR) [Total: 3]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.RO](#cs.RO) [Total: 12]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.NI](#cs.NI) [Total: 11]
- [cs.AR](#cs.AR) [Total: 2]
- [physics.data-an](#physics.data-an) [Total: 1]
- [eess.IV](#eess.IV) [Total: 9]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.DL](#cs.DL) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.DC](#cs.DC) [Total: 3]
- [stat.ML](#stat.ML) [Total: 11]
- [cs.HC](#cs.HC) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.CY](#cs.CY) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical Vector Spaces](https://arxiv.org/abs/2505.07831)
*Michael Pichat,William Pogrund,Paloma Pichat,Judicael Poumay,Armanouche Gasparian,Samuel Demarchi,Martin Corbet,Alois Georgeon,Michael Veillet-Guillem*

Main category: cs.CL

TL;DR: 论文提出了一种几何方法，将神经元定义为具有非正交基的分类向量空间，通过层内注意力机制识别关键分类区域以提高语言模型效率。


<details>
  <summary>Details</summary>
Motivation: 理解人工神经网络中合成神经元的多义性，并提出一种替代方法来优化其效率。

Method: 将神经元定义为分类向量空间，利用层内注意力机制识别关键分类区域。

Result: 提出了一种更高效的语言模型神经元结构，能够更好地利用多义性。

Conclusion: 几何定义和层内注意力机制为语言模型神经元的优化提供了新方向。

Abstract: The polysemantic nature of synthetic neurons in artificial intelligence
language models is currently understood as the result of a necessary
superposition of distributed features within the latent space. We propose an
alternative approach, geometrically defining a neuron in layer n as a
categorical vector space with a non-orthogonal basis, composed of categorical
sub-dimensions extracted from preceding neurons in layer n-1. This categorical
vector space is structured by the activation space of each neuron and enables,
via an intra-neuronal attention process, the identification and utilization of
a critical categorical zone for the efficiency of the language model - more
homogeneous and located at the intersection of these different categorical
sub-dimensions.

</details>


### [2] [A Tale of Two Identities: An Ethical Audit of Human and AI-Crafted Personas](https://arxiv.org/abs/2505.07850)
*Pranav Narayanan Venkit,Jiayi Li,Yingfan Zhou,Sarah Rajtmajer,Shomir Wilson*

Main category: cs.CL

TL;DR: 论文研究了三种大型语言模型（GPT4o、Gemini 1.5 Pro、Deepseek 2.5）生成的合成人物在种族身份表征上的问题，揭示了算法他者化现象及其社会技术危害。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在数据有限领域（如健康、隐私和HCI）中生成合成人物的应用增多，需要理解这些叙述如何表征身份，尤其是少数群体。

Method: 采用混合方法（细读、词汇分析和参数化创造力框架），比较1512个LLM生成人物与人类撰写的回答。

Result: 发现LLM过度强调种族标记，使用文化编码语言，生成句法复杂但叙事简化的人物，导致刻板印象、异域化、抹除和善意偏见等危害。

Conclusion: 提出设计建议，包括叙事感知评估指标和以社区为中心的验证协议，以减少合成身份生成中的危害。

Abstract: As LLMs (large language models) are increasingly used to generate synthetic
personas particularly in data-limited domains such as health, privacy, and HCI,
it becomes necessary to understand how these narratives represent identity,
especially that of minority communities. In this paper, we audit synthetic
personas generated by 3 LLMs (GPT4o, Gemini 1.5 Pro, Deepseek 2.5) through the
lens of representational harm, focusing specifically on racial identity. Using
a mixed methods approach combining close reading, lexical analysis, and a
parameterized creativity framework, we compare 1512 LLM generated personas to
human-authored responses. Our findings reveal that LLMs disproportionately
foreground racial markers, overproduce culturally coded language, and construct
personas that are syntactically elaborate yet narratively reductive. These
patterns result in a range of sociotechnical harms, including stereotyping,
exoticism, erasure, and benevolent bias, that are often obfuscated by
superficially positive narrations. We formalize this phenomenon as algorithmic
othering, where minoritized identities are rendered hypervisible but less
authentic. Based on these findings, we offer design recommendations for
narrative-aware evaluation metrics and community-centered validation protocols
for synthetic identity generation.

</details>


### [3] [Joint Detection of Fraud and Concept Drift inOnline Conversations with LLM-Assisted Judgment](https://arxiv.org/abs/2505.07852)
*Ali Senol,Garima Agrawal,Huan Liu*

Main category: cs.CL

TL;DR: 提出了一种两阶段检测框架，结合集成分类模型和概念漂移分析，用于实时检测数字通信平台中的虚假交互。


<details>
  <summary>Details</summary>
Motivation: 虚假交互检测问题尚未充分解决，传统静态方法难以适应动态对话变化，易误判良性话题转换。

Method: 两阶段框架：1) 集成分类模型识别可疑对话；2) 概念漂移分析（OCDD）和LLM评估话题转换是否为欺诈。

Result: 在社交工程聊天数据上验证，框架提高了检测准确性和可解释性。

Conclusion: 模块化方法优于双LLM基线，平衡了检测性能和实时性。

Abstract: Detecting fake interactions in digital communication platforms remains a
challenging and insufficiently addressed problem. These interactions may appear
as harmless spam or escalate into sophisticated scam attempts, making it
difficult to flag malicious intent early. Traditional detection methods often
rely on static anomaly detection techniques that fail to adapt to dynamic
conversational shifts. One key limitation is the misinterpretation of benign
topic transitions referred to as concept drift as fraudulent behavior, leading
to either false alarms or missed threats. We propose a two stage detection
framework that first identifies suspicious conversations using a tailored
ensemble classification model. To improve the reliability of detection, we
incorporate a concept drift analysis step using a One Class Drift Detector
(OCDD) to isolate conversational shifts within flagged dialogues. When drift is
detected, a large language model (LLM) assesses whether the shift indicates
fraudulent manipulation or a legitimate topic change. In cases where no drift
is found, the behavior is inferred to be spam like. We validate our framework
using a dataset of social engineering chat scenarios and demonstrate its
practical advantages in improving both accuracy and interpretability for real
time fraud detection. To contextualize the trade offs, we compare our modular
approach against a Dual LLM baseline that performs detection and judgment using
different language models.

</details>


### [4] [CrashSage: A Large Language Model-Centered Framework for Contextual and Interpretable Traffic Crash Analysis](https://arxiv.org/abs/2505.07853)
*Hao Zhen,Jidong J. Yang*

Main category: cs.CL

TL;DR: CrashSage是一个基于大型语言模型（LLM）的框架，通过文本转换、数据增强、模型微调和可解释性技术，提升交通事故分析的性能。


<details>
  <summary>Details</summary>
Motivation: 全球每年因交通事故造成巨大的人员和经济损失，传统方法无法充分捕捉复杂关系和上下文信息，亟需更先进的解决方案。

Method: 1. 将表格数据转换为结构化文本；2. 使用基础LLM进行数据增强；3. 微调LLaMA3-8B模型；4. 应用梯度可解释性技术。

Result: CrashSage在事故严重性推断上表现优于基线方法（如GPT-4o等），并提供更透明的决策解释。

Conclusion: CrashSage通过LLM技术显著提升了交通事故分析的准确性和可解释性，为道路安全干预提供了有力支持。

Abstract: Road crashes claim over 1.3 million lives annually worldwide and incur global
economic losses exceeding \$1.8 trillion. Such profound societal and financial
impacts underscore the urgent need for road safety research that uncovers crash
mechanisms and delivers actionable insights. Conventional statistical models
and tree ensemble approaches typically rely on structured crash data,
overlooking contextual nuances and struggling to capture complex relationships
and underlying semantics. Moreover, these approaches tend to incur significant
information loss, particularly in narrative elements related to multi-vehicle
interactions, crash progression, and rare event characteristics. This study
presents CrashSage, a novel Large Language Model (LLM)-centered framework
designed to advance crash analysis and modeling through four key innovations.
First, we introduce a tabular-to-text transformation strategy paired with
relational data integration schema, enabling the conversion of raw,
heterogeneous crash data into enriched, structured textual narratives that
retain essential structural and relational context. Second, we apply
context-aware data augmentation using a base LLM model to improve narrative
coherence while preserving factual integrity. Third, we fine-tune the LLaMA3-8B
model for crash severity inference, demonstrating superior performance over
baseline approaches, including zero-shot, zero-shot with chain-of-thought
prompting, and few-shot learning, with multiple models (GPT-4o, GPT-4o-mini,
LLaMA3-70B). Finally, we employ a gradient-based explainability technique to
elucidate model decisions at both the individual crash level and across broader
risk factor dimensions. This interpretability mechanism enhances transparency
and enables targeted road safety interventions by providing deeper insights
into the most influential factors.

</details>


### [5] [Unpacking Robustness in Inflectional Languages: Adversarial Evaluation and Mechanistic Insights](https://arxiv.org/abs/2505.07856)
*Paweł Walkowiak,Marek Klonowski,Marcin Oleksy,Arkadiusz Janz*

Main category: cs.CL

TL;DR: 论文探讨了对抗性攻击在屈折语言中的表现，提出了一种基于Edge Attribution Patching（EAP）的新评估协议，并创建了一个新基准来分析屈折与对抗鲁棒性的关系。


<details>
  <summary>Details</summary>
Motivation: 研究对抗性攻击在屈折语言（如波兰语）中的表现，以弥补现有方法主要针对非屈折语言（如英语）的不足。

Method: 设计了一种基于EAP的评估协议，利用平行语料库（波兰语和英语）分析模型行为，并创建了基于MultiEmo数据集的新基准。

Result: 揭示了屈折对模型行为及对抗鲁棒性的影响，并识别了模型中与屈折相关的机制性元素。

Conclusion: 屈折语言中的对抗性攻击表现与非屈折语言不同，新协议和基准为理解这一差异提供了工具。

Abstract: Various techniques are used in the generation of adversarial examples,
including methods such as TextBugger which introduce minor, hardly visible
perturbations to words leading to changes in model behaviour. Another class of
techniques involves substituting words with their synonyms in a way that
preserves the text's meaning but alters its predicted class, with TextFooler
being a prominent example of such attacks. Most adversarial example generation
methods are developed and evaluated primarily on non-inflectional languages,
typically English. In this work, we evaluate and explain how adversarial
attacks perform in inflectional languages. To explain the impact of inflection
on model behaviour and its robustness under attack, we designed a novel
protocol inspired by mechanistic interpretability, based on Edge Attribution
Patching (EAP) method. The proposed evaluation protocol relies on parallel
task-specific corpora that include both inflected and syncretic variants of
texts in two languages -- Polish and English. To analyse the models and explain
the relationship between inflection and adversarial robustness, we create a new
benchmark based on task-oriented dataset MultiEmo, enabling the identification
of mechanistic inflection-related elements of circuits within the model and
analyse their behaviour under attack.

</details>


### [6] [Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed Predictive Pipelines](https://arxiv.org/abs/2505.07857)
*Faiza Hassan,Summra Saleem,Kashif Javed,Muhammad Nabeel Asim,Abdur Rehman,Andreas Dengel*

Main category: cs.CL

TL;DR: 本文提出了一种针对乌尔都语的意图检测方法LLMPIA，结合对比学习和原型注意力机制，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语作为第十大语言，缺乏基于少样本学习的意图检测方法，传统方法仅能预测训练集中的类别。

Method: 采用对比学习利用无标注数据重新训练预训练语言模型，结合原型注意力机制构建LLMPIA管道。

Result: 在ATIS和Web Queries数据集上，LLMPIA在少样本设置下表现优异，F1分数显著提升。

Conclusion: LLMPIA为乌尔都语意图检测提供了高效解决方案，性能优于现有方法。

Abstract: Multifarious intent detection predictors are developed for different
languages, including English, Chinese and French, however, the field remains
underdeveloped for Urdu, the 10th most spoken language. In the realm of
well-known languages, intent detection predictors utilize the strategy of
few-shot learning and prediction of unseen classes based on the model training
on seen classes. However, Urdu language lacks few-shot strategy based intent
detection predictors and traditional predictors are focused on prediction of
the same classes which models have seen in the train set. To empower Urdu
language specific intent detection, this introduces a unique contrastive
learning approach that leverages unlabeled Urdu data to re-train pre-trained
language models. This re-training empowers LLMs representation learning for the
downstream intent detection task. Finally, it reaps the combined potential of
pre-trained LLMs and the prototype-informed attention mechanism to create a
comprehensive end-to-end LLMPIA intent detection pipeline. Under the paradigm
of proposed predictive pipeline, it explores the potential of 6 distinct
language models and 13 distinct similarity computation methods. The proposed
framework is evaluated on 2 public benchmark datasets, namely ATIS encompassing
5836 samples and Web Queries having 8519 samples. Across ATIS dataset under
4-way 1 shot and 4-way 5 shot experimental settings LLMPIA achieved 83.28% and
98.25% F1-Score and on Web Queries dataset produced 76.23% and 84.42% F1-Score,
respectively. In an additional case study on the Web Queries dataset under same
classes train and test set settings, LLMPIA outperformed state-of-the-art
predictor by 53.55% F1-Score.

</details>


### [7] [Scaling Laws for Speculative Decoding](https://arxiv.org/abs/2505.07858)
*Siyuan Yan,Mo Zhu,Guo-qing Jiang,Jianfei Wang,Jiaxing Chen,Wentai Zhang,Xiang Liao,Xiao Cui,Chen Zhang,Zhuoran Song,Ran Zhu*

Main category: cs.CL

TL;DR: 该研究探索了在大语言模型（LLM）中通过推测解码技术提升推理任务效率的方法，提出了对数线性缩放定律，并开发了Scylla系统，显著提升了解码速度和任务性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（如OpenAI-o3和DeepSeek-R1）在复杂推理任务中的应用增加，解码效率成为关键瓶颈。研究旨在通过推测解码技术解决这一问题。

Method: 研究通过密集LLM架构探索推测解码技术，提出了对数线性缩放定律（定理1.1-1.3），并开发了Scylla系统，协调多维度缩放。

Result: Scylla在解码速度和任务性能上优于现有方法（如EAGLE2和EAGLE3），在工业部署中实现了2倍的解码吞吐量提升。

Conclusion: 系统性缩放对高效LLM推理具有变革潜力，Scylla为未来研究提供了重要基础。

Abstract: The escalating demand for efficient decoding in large language models (LLMs)
is particularly critical for reasoning-intensive architectures like OpenAI-o3
and DeepSeek-R1, which depend on extended chain-of-thought reasoning. This
study investigates speculative decoding techniques through dense LLM
architectures to establish foundational insights for accelerating reasoning
tasks. While speculative decoding methods leveraging parallel
draft-verification cycles have emerged as promising acceleration techniques,
the scaling laws governing decoding efficiency remain under-explored compared
to conventional backbone LLMs developed through Pretraining->SFT->RLHF training
paradigms. In this work, we discover Log-linear Scaling Laws (Theorem 1.1, 1.2
and 1.3) governing draft model acceptance rate (or decoding speed) across three
dimensions: pretraining token volume, draft model capacity, and decoding batch
size. Building on these laws, we achieve Scylla, which coordinates
multi-dimensional scaling for popular LLMs (Llama2/3, Qwen2.5). Empirical
validation shows Scylla achieves 1.5-2.2 higher acceptance rate than EAGLE2 and
0.3 higher than EAGLE3 at temperature T = 0, with peak performance gains on
summarization and QA tasks (Figure 2). Industrial inference engine deployments
demonstrate 2X decoding throughput improvements over EAGLE2 (Table 5),
validating the transformative potential of systematic scaling for efficient LLM
inference. Code will be released later.

</details>


### [8] [Boosting Performance on ARC is a Matter of Perspective](https://arxiv.org/abs/2505.07859)
*Daniel Franzen,Jan Disselhoff,David Hartmann*

Main category: cs.CL

TL;DR: 论文提出了一种通过数据增强和深度优先搜索算法提升LLM在ARC-AGI任务中表现的方法，并利用LLM作为生成器和评分器，实现了71.6%的高分。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在抽象推理任务（如ARC-AGI）中的局限性，提升其表现。

Method: 在训练、生成和评分阶段使用任务特定的数据增强，结合深度优先搜索算法生成多样化的候选解，并利用LLM的输出概率选择最优解。

Result: 在公开的ARC-AGI评估集上获得71.6%的分数（286.5/400任务），表现优异。

Conclusion: 该方法在透明性、可复现性和低成本方面具有优势，尽管闭源方法得分更高。

Abstract: The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge
for large language models (LLMs), exposing limitations in their abstract
reasoning abilities. In this work, we leverage task-specific data augmentations
throughout the training, generation, and scoring phases, and employ a
depth-first search algorithm to generate diverse, high-probability candidate
solutions. Furthermore, we utilize the LLM not only as a generator but also as
a scorer, using its output probabilities to select the most promising
solutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the
public ARC-AGI evaluation set, demonstrating state-of-the-art performance among
publicly available approaches. While concurrent closed-source work has reported
higher scores, our method distinguishes itself through its transparency,
reproducibility, and remarkably low inference cost, averaging only around 2ct
per task on readily available hardware (we assume a price of 36ct/hour for a
Nvidia 4090 GPU).

</details>


### [9] [Scalable LLM Math Reasoning Acceleration with Low-rank Distillation](https://arxiv.org/abs/2505.07861)
*Harry Dong,Bilge Acun,Beidi Chen,Yuejie Chi*

Main category: cs.CL

TL;DR: Caprese是一种低成本蒸馏方法，用于恢复因高效推理方法部署而丢失的数学能力，同时保持语言任务性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学推理上需要大量计算资源，现有高效推理方法虽在语言任务上表现良好，但会严重降低数学性能。

Method: 提出Caprese方法，通过少量额外参数（约1%）和合成训练样本（20K），在不扰动原始权重的情况下恢复数学能力。

Result: Caprese显著减少了活跃参数数量（如Gemma 2 9B和Llama 3.1 8B减少约2B），并降低了延迟（如Qwen 2.5 14B生成2048个令牌时延迟减少11%）。

Conclusion: Caprese是一种高效且低成本的方法，可在不损害语言任务性能的情况下恢复数学推理能力。

Abstract: Due to long generations, large language model (LLM) math reasoning demands
significant computational resources and time. While many existing efficient
inference methods have been developed with excellent performance preservation
on language tasks, they often severely degrade math performance. In this paper,
we propose Caprese, a low-cost distillation method to recover lost capabilities
from deploying efficient inference methods, focused primarily in feedforward
blocks. With original weights unperturbed, roughly 1% of additional parameters,
and only 20K synthetic training samples, we are able to recover much if not all
of the math capabilities lost from efficient inference for thinking LLMs and
without harm to language tasks for instruct LLMs. Moreover, Caprese slashes the
number of active parameters (~2B cut for Gemma 2 9B and Llama 3.1 8B) and
integrates cleanly into existing model layers to reduce latency (>11% reduction
to generate 2048 tokens with Qwen 2.5 14B) while encouraging response brevity.

</details>


### [10] [Graph Laplacian Wavelet Transformer via Learnable Spectral Decomposition](https://arxiv.org/abs/2505.07862)
*Andrew Kiruluta,Eric Lundy,Priscilla Burity*

Main category: cs.CL

TL;DR: 论文提出Graph Wavelet Transformer（GWT），用多尺度小波变换替代传统点积自注意力机制，降低计算和内存复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有序列到序列模型依赖点积自注意力机制，计算和内存复杂度高（O(N²)）。

Method: 引入GWT架构，基于显式图拉普拉斯算子定义可学习的多尺度小波变换。

Result: 多尺度谱分解为图结构序列建模提供了高效、可解释且表达能力强的替代方案。

Conclusion: GWT是解决传统自注意力机制复杂度问题的有效方法。

Abstract: Existing sequence to sequence models for structured language tasks rely
heavily on the dot product self attention mechanism, which incurs quadratic
complexity in both computation and memory for input length N. We introduce the
Graph Wavelet Transformer (GWT), a novel architecture that replaces this
bottleneck with a learnable, multi scale wavelet transform defined over an
explicit graph Laplacian derived from syntactic or semantic parses. Our
analysis shows that multi scale spectral decomposition offers an interpretable,
efficient, and expressive alternative to quadratic self attention for graph
structured sequence modeling.

</details>


### [11] [QoSBERT: An Uncertainty-Aware Approach based on Pre-trained Language Models for Service Quality Prediction](https://arxiv.org/abs/2505.07863)
*Ziliang Wang,Xiaohong Zhang,Ze Shi Li,Meng Yan*

Main category: cs.CL

TL;DR: QoSBERT利用预训练语言模型将QoS预测重新定义为语义回归任务，结合蒙特卡洛Dropout不确定性估计，显著提升了预测精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统QoS模型依赖人工特征工程且仅提供点估计，缺乏预测置信度信息，限制了其在实际应用中的可信度。

Method: QoSBERT通过自然语言描述自动编码用户服务元数据，结合注意力池化和多层感知机回归器，并集成蒙特卡洛Dropout进行不确定性估计。

Result: 在标准QoS数据集上，QoSBERT在响应时间和吞吐量预测上分别平均降低了11.7%和6.9%的MAE，同时提供了校准的置信区间。

Conclusion: QoSBERT不仅提升了预测精度，还提供了可靠的不确定性量化，为更可信的服务选择和优化奠定了基础。

Abstract: Accurate prediction of Quality of Service (QoS) metrics is fundamental for
selecting and managing cloud based services. Traditional QoS models rely on
manual feature engineering and yield only point estimates, offering no insight
into the confidence of their predictions. In this paper, we propose QoSBERT,
the first framework that reformulates QoS prediction as a semantic regression
task based on pre trained language models. Unlike previous approaches relying
on sparse numerical features, QoSBERT automatically encodes user service
metadata into natural language descriptions, enabling deep semantic
understanding. Furthermore, we integrate a Monte Carlo Dropout based
uncertainty estimation module, allowing for trustworthy and risk-aware service
quality prediction, which is crucial yet underexplored in existing QoS models.
QoSBERT applies attentive pooling over contextualized embeddings and a
lightweight multilayer perceptron regressor, fine tuned jointly to minimize
absolute error. We further exploit the resulting uncertainty estimates to
select high quality training samples, improving robustness in low resource
settings. On standard QoS benchmark datasets, QoSBERT achieves an average
reduction of 11.7% in MAE and 6.7% in RMSE for response time prediction, and
6.9% in MAE for throughput prediction compared to the strongest baselines,
while providing well calibrated confidence intervals for robust and trustworthy
service quality estimation. Our approach not only advances the accuracy of
service quality prediction but also delivers reliable uncertainty
quantification, paving the way for more trustworthy, data driven service
selection and optimization.

</details>


### [12] [Efficient Fairness Testing in Large Language Models: Prioritizing Metamorphic Relations for Bias Detection](https://arxiv.org/abs/2505.07870)
*Suavis Giramata,Madhusudan Srinivasan,Venkat Naidu Gudivada,Upulee Kanewala*

Main category: cs.CL

TL;DR: 本文提出了一种基于句子多样性的方法，用于优先排序变形关系（MRs），以高效检测大型语言模型（LLMs）中的公平性问题。实验表明，该方法在故障检测率和时间效率上优于随机和距离优先排序。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，其输出中的公平性和偏见问题日益突出。由于测试用例数量庞大，如何高效检测这些问题成为关键挑战。

Method: 采用句子多样性方法计算并排序MRs，以优化故障检测。

Result: 实验结果显示，该方法比随机和距离优先排序分别提高了22%和12%的故障检测率，并减少了首次故障发现时间。

Conclusion: 多样性优先排序方法在提升LLMs公平性测试效果的同时，显著降低了计算成本。

Abstract: Large Language Models (LLMs) are increasingly deployed in various
applications, raising critical concerns about fairness and potential biases in
their outputs. This paper explores the prioritization of metamorphic relations
(MRs) in metamorphic testing as a strategy to efficiently detect fairness
issues within LLMs. Given the exponential growth of possible test cases,
exhaustive testing is impractical; therefore, prioritizing MRs based on their
effectiveness in detecting fairness violations is crucial. We apply a sentence
diversity-based approach to compute and rank MRs to optimize fault detection.
Experimental results demonstrate that our proposed prioritization approach
improves fault detection rates by 22% compared to random prioritization and 12%
compared to distance-based prioritization, while reducing the time to the first
failure by 15% and 8%, respectively. Furthermore, our approach performs within
5% of fault-based prioritization in effectiveness, while significantly reducing
the computational cost associated with fault labeling. These results validate
the effectiveness of diversity-based MR prioritization in enhancing fairness
testing for LLMs.

</details>


### [13] [Evaluating Financial Sentiment Analysis with Annotators Instruction Assisted Prompting: Enhancing Contextual Interpretation and Stock Prediction Accuracy](https://arxiv.org/abs/2505.07871)
*A M Muntasir Rahman,Ajim Uddin,Guiling "Grace" Wang*

Main category: cs.CL

TL;DR: 论文提出了一种新的评估提示方法（AIAP），通过整合人类标注者的任务指令，改善了金融情感分析（FSA）中LLMs的性能，并在新数据集WSBS上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 金融情感分析中LLMs的表现受限于现有基准数据集的主观性和标注不一致性，导致评估不公平。

Method: 引入Annotators' Instruction Assisted Prompt（AIAP），将人类标注者的任务指令整合到LLMs的提示框架中，以标准化情感理解。

Result: AIAP显著提升了LLMs的性能（最高提升9.08），并提出了基于模型置信度的情感索引方法，增强了股票价格预测。

Conclusion: AIAP为金融情感分析提供了更公平和有效的评估方法，同时展示了WSB作为金融文本来源的重要性。

Abstract: Financial sentiment analysis (FSA) presents unique challenges to LLMs that
surpass those in typical sentiment analysis due to the nuanced language used in
financial contexts. The prowess of these models is often undermined by the
inherent subjectivity of sentiment classifications in existing benchmark
datasets like Financial Phrasebank. These datasets typically feature undefined
sentiment classes that reflect the highly individualized perspectives of
annotators, leading to significant variability in annotations. This variability
results in an unfair expectation for LLMs during benchmarking, where they are
tasked to conjecture the subjective viewpoints of human annotators without
sufficient context. In this paper, we introduce the Annotators' Instruction
Assisted Prompt, a novel evaluation prompt designed to redefine the task
definition of FSA for LLMs. By integrating detailed task instructions
originally intended for human annotators into the LLMs' prompt framework, AIAP
aims to standardize the understanding of sentiment across both human and
machine interpretations, providing a fair and context-rich foundation for
sentiment analysis. We utilize a new dataset, WSBS, derived from the
WallStreetBets subreddit to demonstrate how AIAP significantly enhances LLM
performance by aligning machine operations with the refined task definitions.
Experimental results demonstrate that AIAP enhances LLM performance
significantly, with improvements up to 9.08. This context-aware approach not
only yields incremental gains in performance but also introduces an innovative
sentiment-indexing method utilizing model confidence scores. This method
enhances stock price prediction models and extracts more value from the
financial sentiment analysis, underscoring the significance of WSB as a
critical source of financial text. Our research offers insights into both
improving FSA through better evaluation methods.

</details>


### [14] [The Sound of Populism: Distinct Linguistic Features Across Populist Variants](https://arxiv.org/abs/2505.07874)
*Yu Wang,Runxi Yu,Zhongyuan Wang,Jing He*

Main category: cs.CL

TL;DR: 该研究通过结合LIWC特征和RoBERTa模型，分析美国政治演讲中的民粹主义声音，揭示了其直接、自信的语言风格及其在不同民粹主义维度中的差异。


<details>
  <summary>Details</summary>
Motivation: 探索民粹主义在政治演讲中的语言表现，揭示其情感和风格特征。

Method: 结合LIWC特征和RoBERTa模型，分析美国总统就职演说和国情咨文中的语言标记。

Result: 民粹主义语言风格直接且自信，右翼民粹主义和人民中心主义更具情感色彩。

Conclusion: 民粹主义语言风格是战略性的，不同维度在情感表达上有显著差异。

Abstract: This study explores the sound of populism by integrating the classic
Linguistic Inquiry and Word Count (LIWC) features, which capture the emotional
and stylistic tones of language, with a fine-tuned RoBERTa model, a
state-of-the-art context-aware language model trained to detect nuanced
expressions of populism. This approach allows us to uncover the auditory
dimensions of political rhetoric in U.S. presidential inaugural and State of
the Union addresses. We examine how four key populist dimensions (i.e.,
left-wing, right-wing, anti-elitism, and people-centrism) manifest in the
linguistic markers of speech, drawing attention to both commonalities and
distinct tonal shifts across these variants. Our findings reveal that populist
rhetoric consistently features a direct, assertive ``sound" that forges a
connection with ``the people'' and constructs a charismatic leadership persona.
However, this sound is not simply informal but strategically calibrated.
Notably, right-wing populism and people-centrism exhibit a more emotionally
charged discourse, resonating with themes of identity, grievance, and crisis,
in contrast to the relatively restrained emotional tones of left-wing and
anti-elitist expressions.

</details>


### [15] [Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic Constraints](https://arxiv.org/abs/2505.07883)
*Jian-Qiao Zhu,Haijiang Yan,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 论文探讨了如何从LLM嵌入中恢复符合概率论公理的连贯事件概率，提出了一种基于变分自编码器的方法，实验证明其优于模型直接生成的概率。


<details>
  <summary>Details</summary>
Motivation: LLM生成的事件概率存在不连贯性，违反概率论公理，研究旨在从嵌入中恢复更准确的连贯概率。

Method: 通过扩展的变分自编码器（VAE）在潜在空间中强制概率论公理约束（如加法规则），使概率自然涌现。

Result: 实验表明，从嵌入恢复的概率比模型直接生成的更连贯，且更接近真实概率。

Conclusion: 该方法能有效提高LLM生成概率的连贯性，为不确定性事件提供更准确的估计。

Abstract: Rational decision-making under uncertainty requires coherent degrees of
belief in events. However, event probabilities generated by Large Language
Models (LLMs) have been shown to exhibit incoherence, violating the axioms of
probability theory. This raises the question of whether coherent event
probabilities can be recovered from the embeddings used by the models. If so,
those derived probabilities could be used as more accurate estimates in events
involving uncertainty. To explore this question, we propose enforcing axiomatic
constraints, such as the additive rule of probability theory, in the latent
space learned by an extended variational autoencoder (VAE) applied to LLM
embeddings. This approach enables event probabilities to naturally emerge in
the latent space as the VAE learns to both reconstruct the original embeddings
and predict the embeddings of semantically related events. We evaluate our
method on complementary events (i.e., event A and its complement, event not-A),
where the true probabilities of the two events must sum to 1. Experiment
results on open-weight language models demonstrate that probabilities recovered
from embeddings exhibit greater coherence than those directly reported by the
corresponding models and align closely with the true probabilities.

</details>


### [16] [Development of a WAZOBIA-Named Entity Recognition System](https://arxiv.org/abs/2505.07884)
*S. E Emedem,I. E Onyenwe,E. G Onyedinma*

Main category: cs.CL

TL;DR: 该研究开发了WAZOBIA-NER系统，针对尼日利亚三大语言（豪萨语、约鲁巴语和伊博语）进行命名实体识别，填补了资源匮乏语言的空白。


<details>
  <summary>Details</summary>
Motivation: 现有NER系统主要关注英语和欧洲语言，非洲语言资源匮乏，亟需解决方案。

Method: 结合CRF、BiLSTM、BERT和RNN等机器学习与深度学习技术，并利用OCR处理图像文本。

Result: 系统在精度、召回率、F1分数和准确率上表现优异，分别为0.9511、0.9400、0.9564和0.9301。

Conclusion: 研究表明，利用现有NLP框架和迁移学习可为资源匮乏的非洲语言构建高效NER工具。

Abstract: Named Entity Recognition NER is very crucial for various natural language
processing applications, including information extraction, machine translation,
and sentiment analysis. Despite the ever-increasing interest in African
languages within computational linguistics, existing NER systems focus mainly
on English, European, and a few other global languages, leaving a significant
gap for under-resourced languages. This research presents the development of a
WAZOBIA-NER system tailored for the three most prominent Nigerian languages:
Hausa, Yoruba, and Igbo. This research begins with a comprehensive compilation
of annotated datasets for each language, addressing data scarcity and
linguistic diversity challenges. Exploring the state-of-the-art machine
learning technique, Conditional Random Fields (CRF) and deep learning models
such as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional Encoder
Representation from Transformers (Bert) and fine-tune with a Recurrent Neural
Network (RNN), the study evaluates the effectiveness of these approaches in
recognizing three entities: persons, organizations, and locations. The system
utilizes optical character recognition (OCR) technology to convert textual
images into machine-readable text, thereby enabling the Wazobia system to
accept both input text and textual images for extraction purposes. The system
achieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 in
F1-score, and 0.9301 in accuracy. The model's evaluation was conducted across
three languages, with precision, recall, F1-score, and accuracy as key
assessment metrics. The Wazobia-NER system demonstrates that it is feasible to
build robust NER tools for under-resourced African languages using current NLP
frameworks and transfer learning.

</details>


### [17] [PLHF: Prompt Optimization with Few-Shot Human Feedback](https://arxiv.org/abs/2505.07886)
*Chun-Pai Yang,Kan Zheng,Shou-De Lin*

Main category: cs.CL

TL;DR: PLHF是一种基于人类反馈的少样本提示优化框架，解决了传统方法在输出质量难以量化时的优化难题。


<details>
  <summary>Details</summary>
Motivation: 传统提示优化方法在输出质量难以通过标准样本评估时效果不佳，需要一种无需明确指标的优化方案。

Method: PLHF采用类似RLHF的技术，通过评估模块估计输出质量，仅需单轮人类反馈完成优化。

Result: 在公开和工业数据集上，PLHF优于先前的输出评分策略。

Conclusion: PLHF为复杂任务中的提示优化提供了高效解决方案。

Abstract: Automatic prompt optimization frameworks are developed to obtain suitable
prompts for large language models (LLMs) with respect to desired output quality
metrics. Although existing approaches can handle conventional tasks such as
fixed-solution question answering, defining the metric becomes complicated when
the output quality cannot be easily assessed by comparisons with standard
golden samples. Consequently, optimizing the prompts effectively and
efficiently without a clear metric becomes a critical challenge. To address the
issue, we present PLHF (which stands for "P"rompt "L"earning with "H"uman
"F"eedback), a few-shot prompt optimization framework inspired by the
well-known RLHF technique. Different from naive strategies, PLHF employs a
specific evaluator module acting as the metric to estimate the output quality.
PLHF requires only a single round of human feedback to complete the entire
prompt optimization process. Empirical results on both public and industrial
datasets show that PLHF outperforms prior output grading strategies for LLM
prompt optimizations.

</details>


### [18] [Implementing Long Text Style Transfer with LLMs through Dual-Layered Sentence and Paragraph Structure Extraction and Mapping](https://arxiv.org/abs/2505.07888)
*Yusen Wu,Xiaotie Deng*

Main category: cs.CL

TL;DR: 论文提出了一种分层框架ZeroStylus，结合句子级风格适应和段落级结构连贯性，用于零样本学习的长文本风格迁移。


<details>
  <summary>Details</summary>
Motivation: 解决长文本风格迁移中保持句法和语义一致性的挑战，强调句子和段落级处理的必要性。

Method: 通过分层模板获取和模板引导生成的两阶段框架，动态构建句子和段落模板库。

Result: 实验显示在风格一致性、内容保留和表达质量上优于基线方法，平均得分6.90（基线6.70）。

Conclusion: ZeroStylus无需平行语料库或LLM微调，实现了连贯的长文本风格迁移。

Abstract: This paper addresses the challenge in long-text style transfer using
zero-shot learning of large language models (LLMs), proposing a hierarchical
framework that combines sentence-level stylistic adaptation with
paragraph-level structural coherence. We argue that in the process of effective
paragraph-style transfer, to preserve the consistency of original syntactic and
semantic information, it is essential to perform style transfer not only at the
sentence level but also to incorporate paragraph-level semantic considerations,
while ensuring structural coherence across inter-sentential relationships. Our
proposed framework, ZeroStylus, operates through two systematic phases:
hierarchical template acquisition from reference texts and template-guided
generation with multi-granular matching. The framework dynamically constructs
sentence and paragraph template repositories, enabling context-aware
transformations while preserving inter-sentence logical relationships.
Experimental evaluations demonstrate significant improvements over baseline
methods, with structured rewriting achieving 6.90 average score compared to
6.70 for direct prompting approaches in tri-axial metrics assessing style
consistency, content preservation, and expression quality. Ablation studies
validate the necessity of both template hierarchies during style transfer,
showing higher content preservation win rate against sentence-only approaches
through paragraph-level structural encoding, as well as direct prompting method
through sentence-level pattern extraction and matching. The results establish
new capabilities for coherent long-text style transfer without requiring
parallel corpora or LLM fine-tuning.

</details>


### [19] [BioProBench: Comprehensive Dataset and Benchmark in Biological Protocol Understanding and Reasoning](https://arxiv.org/abs/2505.07889)
*Yuyang Liu,Liuzhenghao Lv,Xiancheng Zhang,Li Yuan,Yonghong Tian*

Main category: cs.CL

TL;DR: BioProBench是一个大规模、多任务的生物协议理解和推理基准，包含五个核心任务，用于全面评估LLMs在生物协议文本上的表现。实验表明，当前LLMs在深层推理和结构化生成任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 生物协议对生命科学研究的可重复性和安全性至关重要，但目前缺乏对LLMs在这些高度专业化、准确性关键且程序性文本上的系统性评估。

Method: 基于27K原始协议构建BioProBench，生成556K高质量结构化实例，评估12种主流LLMs在五个核心任务上的表现。

Result: 实验结果显示，顶级模型在表面理解任务上表现良好，但在深层推理和结构化生成任务（如排序和生成）上表现不佳。开源模型在某些任务上接近闭源模型水平，但生物专用小模型落后于通用LLMs。

Conclusion: 生物协议的推理对当前LLMs构成显著挑战，BioProBench为诊断这些限制提供了标准化框架，并指导开发更适合自动化复杂科学程序的AI系统。

Abstract: Biological protocols are fundamental to reproducible and safe life science
research. While LLMs excel on general tasks, their systematic evaluation on
these highly specialized, accuracy-critical, and inherently procedural texts
remains limited. In this work, we present BioProBench, the first large-scale,
integrated multi-task benchmark for biological protocol understanding and
reasoning. While limited benchmarks have touched upon specific aspects like
protocol QA, BioProBench provides a comprehensive suite of five core tasks:
Protocol Question Answering, Step Ordering, Error Correction, Protocol
Generation, and Protocol Reasoning, enabling a holistic evaluation of LLMs on
procedural biological texts. Built upon 27K original protocols, it yields
nearly 556K high-quality structured instances. We evaluate 12 mainstream
open/closed-source LLMs on BioProBench. Experimental results reveal that while
top models preform well on surface understanding tasks, struggle significantly
with deep reasoning and structured generation tasks like ordering and
generation. Furthermore, model comparisons reveal diverse performance: certain
open-source models approach closed-source levels on some tasks, yet
bio-specific small models lag behind general LLMs, indicating limitations on
complex procedural content. Overall, our findings underscore that procedural
reasoning within biological protocols represents a significant challenge for
current LLMs. BioProBench serves as a standardized framework to diagnose these
specific limitations and guide the development of AI systems better equipped
for safely automating complex scientific procedures. The code and data are
available at: https://github.com/YuyangSunshine/bioprotocolbench and
https://huggingface.co/datasets/GreatCaptainNemo/BioProBench.

</details>


### [20] [TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks](https://arxiv.org/abs/2505.07890)
*Kutay Ertürk,Furkan Altınışık,İrem Sarıaltın,Ömer Nezih Gerek*

Main category: cs.CL

TL;DR: TSLFormer是一种轻量级且鲁棒的土耳其手语识别模型，将手势视为有序的字符串语言，仅使用3D关节位置作为输入，通过序列到序列翻译实现高效识别。


<details>
  <summary>Details</summary>
Motivation: 手语识别通常依赖RGB或深度视频，计算成本高。本研究旨在通过仅使用3D关节位置降低输入维度，同时保留语义信息，为听障人士提供实时、移动和辅助通信系统。

Method: 使用Google的Mediapipe库提取手和躯干的3D关节位置作为输入，采用基于自注意力机制的序列到序列翻译模型（TSLFormer），捕捉手势序列中的时间共现和运动模式。

Result: 在AUTSL数据集（36,000样本，227个单词）上表现优异，计算成本低，证明基于关节的输入足以支持实时应用。

Conclusion: TSLFormer展示了基于关节的输入在手语识别中的有效性，为轻量级、实时手语识别系统提供了可行方案。

Abstract: This study presents TSLFormer, a light and robust word-level Turkish Sign
Language (TSL) recognition model that treats sign gestures as ordered,
string-like language. Instead of using raw RGB or depth videos, our method only
works with 3D joint positions - articulation points - extracted using Google's
Mediapipe library, which focuses on the hand and torso skeletal locations. This
creates efficient input dimensionality reduction while preserving important
semantic gesture information.
  Our approach revisits sign language recognition as sequence-to-sequence
translation, inspired by the linguistic nature of sign languages and the
success of transformers in natural language processing. Since TSLFormer uses
the self-attention mechanism, it effectively captures temporal co-occurrence
within gesture sequences and highlights meaningful motion patterns as words
unfold.
  Evaluated on the AUTSL dataset with over 36,000 samples and 227 different
words, TSLFormer achieves competitive performance with minimal computational
cost. These results show that joint-based input is sufficient for enabling
real-time, mobile, and assistive communication systems for hearing-impaired
individuals.

</details>


### [21] [TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking](https://arxiv.org/abs/2505.07891)
*Ching Nam Hang,Pei-Duo Yu,Chee Wei Tan*

Main category: cs.CL

TL;DR: TrumorGPT是一种基于生成式人工智能的健康领域事实核查工具，旨在区分真实的健康谣言（trumors），利用大型语言模型和图增强生成技术提升准确性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体时代，虚假信息迅速传播，形成信息流行病，对社会构成威胁。TrumorGPT旨在解决这一问题，特别是在健康领域。

Method: 结合大型语言模型（LLM）和少样本学习，构建语义健康知识图谱并进行推理。采用图增强生成（GraphRAG）技术，利用动态更新的知识图谱减少幻觉问题。

Result: 在广泛的医疗数据集上评估，TrumorGPT在公共卫生声明的事实核查中表现优异。

Conclusion: TrumorGPT是打击健康相关虚假信息的重要工具，提升了数字信息时代的信任和准确性。

Abstract: In the age of social media, the rapid spread of misinformation and rumors has
led to the emergence of infodemics, where false information poses a significant
threat to society. To combat this issue, we introduce TrumorGPT , a novel
generative artificial intelligence solution designed for fact-checking in the
health domain. TrumorGPT aims to distinguish "trumors", which are
health-related rumors that turn out to be true, providing a crucial tool in
differentiating between mere speculation and verified facts. This framework
leverages a large language model (LLM) with few-shot learning for semantic
health knowledge graph construction and semantic reasoning. TrumorGPT
incorporates graph-based retrieval-augmented generation (GraphRAG) to address
the hallucination issue common in LLMs and the limitations of static training
data. GraphRAG involves accessing and utilizing information from regularly
updated semantic health knowledge graphs that consist of the latest medical
news and health information, ensuring that fact-checking by TrumorGPT is based
on the most recent data. Evaluating with extensive healthcare datasets,
TrumorGPT demonstrates superior performance in fact-checking for public health
claims. Its ability to effectively conduct fact-checking across various
platforms marks a critical step forward in the fight against health-related
misinformation, enhancing trust and accuracy in the digital information age.

</details>


### [22] [LongCodeBench: Evaluating Coding LLMs at 1M Context Windows](https://arxiv.org/abs/2505.07897)
*Stefano Rando,Luca Romani,Alessio Sampieri,Yuta Kyuragi,Luca Franco,Fabio Galasso,Tatsunori Hashimoto,John Yang*

Main category: cs.CL

TL;DR: 论文介绍了LongCodeBench（LCB），一个用于测试长上下文模型中代码理解和修复能力的基准，发现长上下文仍是所有模型的弱点。


<details>
  <summary>Details</summary>
Motivation: 现代长上下文模型的上下文长度快速增长，但缺乏现实的基准测试，尤其是需要大量上下文的场景。代码理解和修复被选为自然测试任务。

Method: 通过从真实GitHub问题中构建QA（LongCodeQA）和bug修复（LongSWE-Bench）任务，分层设计基准复杂度，评估不同规模模型。

Result: 所有模型在长上下文表现均下降，如Claude 3.5 Sonnet从29%降至3%，Qwen2.5从70.2%降至40%。

Conclusion: 长上下文能力仍是模型的瓶颈，需进一步优化。

Abstract: Context lengths for models have grown rapidly, from thousands to millions of
tokens in just a few years. The extreme context sizes of modern long-context
models have made it difficult to construct realistic long-context benchmarks --
not only due to the cost of collecting million-context tasks but also in
identifying realistic scenarios that require significant contexts. We identify
code comprehension and repair as a natural testbed and challenge task for
long-context models and introduce LongCodeBench (LCB), a benchmark to test LLM
coding abilities in long-context scenarios. Our benchmark tests both the
comprehension and repair capabilities of LCLMs in realistic and important
settings by drawing from real-world GitHub issues and constructing QA
(LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the
complexity of our benchmark, enabling us to evaluate models across different
scales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model.
We find that long-context remains a weakness for all models, with performance
drops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for
Qwen2.5.

</details>


### [23] [DeltaEdit: Enhancing Sequential Editing in Large Language Models by Controlling Superimposed Noise](https://arxiv.org/abs/2505.07899)
*Ding Cao,Yuchen Cai,Rongxi Guo,Xuesong He,Guiquan Liu*

Main category: cs.CL

TL;DR: DeltaEdit通过动态正交约束策略优化更新参数，显著提高了长期编辑的成功率，同时保持了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有连续知识编辑方法在长期编辑后成功率显著下降，原因是叠加噪声积累导致输出偏离目标。

Method: 提出DeltaEdit，采用动态正交约束策略优化更新参数，减少编辑间的干扰。

Result: DeltaEdit在编辑成功率和泛化能力保留上显著优于现有方法。

Conclusion: DeltaEdit能够确保模型在长期连续编辑下保持稳定可靠的性能。

Abstract: Sequential knowledge editing techniques aim to continuously update the
knowledge in large language models at a low cost, preventing the models from
generating outdated or incorrect information. However, existing sequential
editing methods suffer from a significant decline in editing success rates
after long-term editing. Through theoretical analysis and experiments, we
identify that as the number of edits increases, the model's output increasingly
deviates from the desired target, leading to a drop in editing success rates.
We refer to this issue as the accumulation of superimposed noise problem. To
address this, we identify the factors contributing to this deviation and
propose DeltaEdit, a novel method that optimizes update parameters through a
dynamic orthogonal constraints strategy, effectively reducing interference
between edits to mitigate deviation. Experimental results demonstrate that
DeltaEdit significantly outperforms existing methods in edit success rates and
the retention of generalization capabilities, ensuring stable and reliable
model performance even under extensive sequential editing.

</details>


### [24] [SEM: Reinforcement Learning for Search-Efficient Large Language Models](https://arxiv.org/abs/2505.07903)
*Zeyang Sha,Shiwen Cui,Weiqiang Wang*

Main category: cs.CL

TL;DR: 论文提出SEM框架，通过后训练强化学习优化大语言模型（LLMs）的搜索行为，减少冗余搜索并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法导致LLMs频繁冗余搜索，效率低下且成本高，需优化其搜索决策能力。

Method: 结合MuSiQue和MMLU数据集构建平衡场景，设计结构化推理模板，使用GRPO后训练模型，奖励函数鼓励精准回答与必要检索。

Result: 实验表明，SEM显著减少冗余搜索，同时在多个基准测试中保持或提升回答准确性。

Conclusion: SEM框架提升了LLMs的推理效率，并扩展其合理利用外部知识的能力。

Abstract: Recent advancements in Large Language Models(LLMs) have demonstrated their
capabilities not only in reasoning but also in invoking external tools,
particularly search engines. However, teaching models to discern when to invoke
search and when to rely on their internal knowledge remains a significant
challenge. Existing reinforcement learning approaches often lead to redundant
search behaviors, resulting in inefficiencies and over-cost. In this paper, we
propose SEM, a novel post-training reinforcement learning framework that
explicitly trains LLMs to optimize search usage. By constructing a balanced
dataset combining MuSiQue and MMLU, we create scenarios where the model must
learn to distinguish between questions it can answer directly and those
requiring external retrieval. We design a structured reasoning template and
employ Group Relative Policy Optimization(GRPO) to post-train the model's
search behaviors. Our reward function encourages accurate answering without
unnecessary search while promoting effective retrieval when needed.
Experimental results demonstrate that our method significantly reduces
redundant search operations while maintaining or improving answer accuracy
across multiple challenging benchmarks. This framework advances the model's
reasoning efficiency and extends its capability to judiciously leverage
external knowledge.

</details>


### [25] [Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions](https://arxiv.org/abs/2505.07920)
*Daoze Zhang,Zhijian Bao,Sihang Du,Zhiyi Zhao,Kuangling Zhang,Dezheng Bao,Yang Yang*

Main category: cs.CL

TL;DR: 论文提出了一个名为Re^2的大规模一致性保障的同行评审和反驳数据集，以解决现有数据集在多样性、质量和交互支持方面的不足。


<details>
  <summary>Details</summary>
Motivation: 同行评审系统因提交量激增而压力巨大，导致评审质量下降，部分原因是缺乏作者自我评估工具。LLMs在辅助评审方面潜力巨大，但受限于现有数据集的质量。

Method: 构建了包含19,926份初始提交、70,668条评审意见和53,818条反驳的Re^2数据集，覆盖24个会议和21个研讨会，并将反驳阶段建模为多轮对话范式。

Result: Re^2数据集支持静态评审任务和动态交互LLM助手，为作者提供更实用的指导，并帮助缓解评审负担。

Conclusion: Re^2数据集通过高质量和多样性，为LLM辅助评审提供了更可靠的基础，有望改善同行评审系统的效率和质量。

Abstract: Peer review is a critical component of scientific progress in the fields like
AI, but the rapid increase in submission volume has strained the reviewing
system, which inevitably leads to reviewer shortages and declines review
quality. Besides the growing research popularity, another key factor in this
overload is the repeated resubmission of substandard manuscripts, largely due
to the lack of effective tools for authors to self-evaluate their work before
submission. Large Language Models (LLMs) show great promise in assisting both
authors and reviewers, and their performance is fundamentally limited by the
quality of the peer review data. However, existing peer review datasets face
three major limitations: (1) limited data diversity, (2) inconsistent and
low-quality data due to the use of revised rather than initial submissions, and
(3) insufficient support for tasks involving rebuttal and reviewer-author
interactions. To address these challenges, we introduce the largest
consistency-ensured peer review and rebuttal dataset named Re^2, which
comprises 19,926 initial submissions, 70,668 review comments, and 53,818
rebuttals from 24 conferences and 21 workshops on OpenReview. Moreover, the
rebuttal and discussion stage is framed as a multi-turn conversation paradigm
to support both traditional static review tasks and dynamic interactive LLM
assistants, providing more practical guidance for authors to refine their
manuscripts and helping alleviate the growing review burden. Our data and code
are available in https://anonymous.4open.science/r/ReviewBench_anon/.

</details>


### [26] [Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models](https://arxiv.org/abs/2505.07968)
*Weiyi Wu,Xinwen Xu,Chongyang Gao,Xingjian Diao,Siting Li,Lucas A. Salas,Jiang Gui*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在医疗领域中应对临床指南演变的能力，发现模型在拒绝过时建议和避免矛盾指导方面存在困难，并提出了两种改进策略。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗领域潜力巨大，但难以适应快速变化的医学知识，可能导致过时或矛盾的治疗建议。

Method: 开发了DriftMedQA基准模拟指南演变，评估了七种先进LLMs的时间可靠性，并探索了检索增强生成和偏好微调两种缓解策略。

Result: 模型在4,290个场景中表现出拒绝过时建议的困难，两种改进策略结合使用效果最佳。

Conclusion: 需提升LLMs对时间变化的鲁棒性，以确保其在临床实践中的可靠性。

Abstract: Large Language Models (LLMs) have great potential in the field of health
care, yet they face great challenges in adapting to rapidly evolving medical
knowledge. This can lead to outdated or contradictory treatment suggestions.
This study investigated how LLMs respond to evolving clinical guidelines,
focusing on concept drift and internal inconsistencies. We developed the
DriftMedQA benchmark to simulate guideline evolution and assessed the temporal
reliability of various LLMs. Our evaluation of seven state-of-the-art models
across 4,290 scenarios demonstrated difficulties in rejecting outdated
recommendations and frequently endorsing conflicting guidance. Additionally, we
explored two mitigation strategies: Retrieval-Augmented Generation and
preference fine-tuning via Direct Preference Optimization. While each method
improved model performance, their combination led to the most consistent and
reliable results. These findings underscore the need to improve LLM robustness
to temporal shifts to ensure more dependable applications in clinical practice.

</details>


### [27] [Task-Adaptive Semantic Communications with Controllable Diffusion-based Data Regeneration](https://arxiv.org/abs/2505.07980)
*Fupei Guo,Achintha Wijesinghe,Songyang Zhang,Zhi Ding*

Main category: cs.CL

TL;DR: 本文提出了一种基于扩散模型的任务自适应语义通信框架，能够根据下游任务动态调整语义信息传递。


<details>
  <summary>Details</summary>
Motivation: 传统通信以比特为单位传递数据，而语义通信旨在传递语义信息以提高带宽效率。为了适应接收端多样化的下游任务，需要动态调整关键语义信息的传递。

Method: 通过扩散模型初始化传输深度压缩的通用语义表示，接收端生成任务需求文本提示作为反馈，结合注意力机制更新语义传输以更符合接收端目标。

Result: 测试结果表明，该方法能自适应保留任务关键信息，同时保持高压缩效率。

Conclusion: 该框架为下一代语义通信提供了一种高效的任务自适应解决方案。

Abstract: Semantic communications represent a new paradigm of next-generation
networking that shifts bit-wise data delivery to conveying the semantic
meanings for bandwidth efficiency. To effectively accommodate various potential
downstream tasks at the receiver side, one should adaptively convey the most
critical semantic information. This work presents a novel task-adaptive
semantic communication framework based on diffusion models that is capable of
dynamically adjusting the semantic message delivery according to various
downstream tasks. Specifically, we initialize the transmission of a
deep-compressed general semantic representation from the transmitter to enable
diffusion-based coarse data reconstruction at the receiver. The receiver
identifies the task-specific demands and generates textual prompts as feedback.
Integrated with the attention mechanism, the transmitter updates the semantic
transmission with more details to better align with the objectives of the
intended receivers. Our test results demonstrate the efficacy of the proposed
method in adaptively preserving critical task-relevant information for semantic
communications while preserving high compression efficiency.

</details>


### [28] [Large Language Models and Arabic Content: A Review](https://arxiv.org/abs/2505.08004)
*Haneh Rhel,Dmitri Roussinov*

Main category: cs.CL

TL;DR: 论文概述了大型语言模型（LLMs）在阿拉伯语自然语言处理（NLP）中的应用，包括早期预训练模型、性能提升技术（如微调和提示工程）以及相关数据集和基准。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语资源稀缺且语言复杂，研究者希望通过LLMs解决阿拉伯语NLP任务中的挑战。

Method: 研究回顾了预训练LLMs在阿拉伯语NLP中的应用，探讨了微调和提示工程技术。

Result: LLMs在阿拉伯语NLP任务中表现优异，技术改进进一步提升了性能。

Conclusion: LLMs在阿拉伯语NLP中的应用前景广阔，未来研究需持续关注模型优化和数据资源扩展。

Abstract: Over the past three years, the rapid advancement of Large Language Models
(LLMs) has had a profound impact on multiple areas of Artificial Intelligence
(AI), particularly in Natural Language Processing (NLP) across diverse
languages, including Arabic. Although Arabic is considered one of the most
widely spoken languages across 27 countries in the Arabic world and used as a
second language in some other non-Arabic countries as well, there is still a
scarcity of Arabic resources, datasets, and tools. Arabic NLP tasks face
various challenges due to the complexities of the Arabic language, including
its rich morphology, intricate structure, and diverse writing standards, among
other factors. Researchers have been actively addressing these challenges,
demonstrating that pre-trained Large Language Models (LLMs) trained on
multilingual corpora achieve significant success in various Arabic NLP tasks.
This study provides an overview of using large language models (LLMs) for the
Arabic language, highlighting early pre-trained Arabic Language models across
various NLP applications and their ability to handle diverse Arabic content
tasks and dialects. It also provides an overview of how techniques like
finetuning and prompt engineering can enhance the performance of these models.
Additionally, the study summarizes common Arabic benchmarks and datasets while
presenting our observations on the persistent upward trend in the adoption of
LLMs.

</details>


### [29] [TiSpell: A Semi-Masked Methodology for Tibetan Spelling Correction covering Multi-Level Error with Data Augmentation](https://arxiv.org/abs/2505.08037)
*Yutong Liu,Feng Xiao,Ziyue Zhang,Yongbin Yu,Cheng Huang,Fan Gao,Xiangxiang Wang,Ma-bao Ban,Manping Fan,Thupten Tsering,Cheng Huang,Gadeng Luosang,Renzeng Duojie,Nyima Tashi*

Main category: cs.CL

TL;DR: 提出了一种多级藏文拼写纠错方法TiSpell，结合字符和音节级纠错，通过数据增强生成训练集，实验证明其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单级纠错，缺乏对字符和音节级错误的有效整合，且缺乏开源数据集。

Method: 提出数据增强方法生成多级错误，设计半掩码模型TiSpell，支持字符和音节级纠错。

Result: 在模拟和真实数据上，TiSpell优于基线模型，与先进方法性能相当。

Conclusion: TiSpell在多级藏文拼写纠错中表现优异，验证了其有效性。

Abstract: Multi-level Tibetan spelling correction addresses errors at both the
character and syllable levels within a unified model. Existing methods focus
mainly on single-level correction and lack effective integration of both
levels. Moreover, there are no open-source datasets or augmentation methods
tailored for this task in Tibetan. To tackle this, we propose a data
augmentation approach using unlabeled text to generate multi-level corruptions,
and introduce TiSpell, a semi-masked model capable of correcting both
character- and syllable-level errors. Although syllable-level correction is
more challenging due to its reliance on global context, our semi-masked
strategy simplifies this process. We synthesize nine types of corruptions on
clean sentences to create a robust training set. Experiments on both simulated
and real-world data demonstrate that TiSpell, trained on our dataset,
outperforms baseline models and matches the performance of state-of-the-art
approaches, confirming its effectiveness.

</details>


### [30] [FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning](https://arxiv.org/abs/2505.08054)
*Zhehao Zhang,Weijie Xu,Fanyou Wu,Chandan K. Reddy*

Main category: cs.CL

TL;DR: 论文提出FalseReject资源，用于解决LLMs因安全对齐导致的过度拒绝良性查询问题，通过多智能体框架生成多样化提示，显著减少不必要的拒绝。


<details>
  <summary>Details</summary>
Motivation: LLMs的安全对齐常导致良性查询被过度拒绝，影响实用性。

Method: 提出FalseReject资源，包含16k有毒查询及结构化响应；采用图引导的多智能体交互框架生成多样化提示。

Result: 在29个SOTA LLMs上测试，FalseReject显著减少不必要的拒绝，且不影响安全性或语言能力。

Conclusion: FalseReject通过结构化响应和多智能体框架，有效解决了LLMs的过度拒绝问题。

Abstract: Safety alignment approaches in large language models (LLMs) often lead to the
over-refusal of benign queries, significantly diminishing their utility in
sensitive scenarios. To address this challenge, we introduce FalseReject, a
comprehensive resource containing 16k seemingly toxic queries accompanied by
structured responses across 44 safety-related categories. We propose a
graph-informed adversarial multi-agent interaction framework to generate
diverse and complex prompts, while structuring responses with explicit
reasoning to aid models in accurately distinguishing safe from unsafe contexts.
FalseReject includes training datasets tailored for both standard
instruction-tuned models and reasoning-oriented models, as well as a
human-annotated benchmark test set. Our extensive benchmarking on 29
state-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges.
Empirical results demonstrate that supervised finetuning with FalseReject
substantially reduces unnecessary refusals without compromising overall safety
or general language capabilities.

</details>


### [31] [HYPERNYM MERCURY: Token Optimization through Semantic Field Constriction and Reconstruction from Hypernyms. A New Text Compression Method](https://arxiv.org/abs/2505.08058)
*Chris Forrester,Octavia Sulea*

Main category: cs.CL

TL;DR: 本文介绍了一种新颖的文本表示方案和语义压缩技术，可减少90%以上的token，同时保持高语义相似性。


<details>
  <summary>Details</summary>
Motivation: 在NLP和下一代AI中，计算优化是一个重要任务，本文旨在通过token减少提升效率。

Method: 提出了一种专利待决的文本表示方案和段落级语义压缩技术，支持无损压缩和粒度控制。

Result: 在开源数据（如《德古拉》）上测试，结果显示该方法在多类型和模型下均有效。

Conclusion: 该技术能显著减少token，同时保持语义，适用于多种场景。

Abstract: Compute optimization using token reduction of LLM prompts is an emerging task
in the fields of NLP and next generation, agentic AI. In this white paper, we
introduce a novel (patent pending) text representation scheme and a
first-of-its-kind word-level semantic compression of paragraphs that can lead
to over 90\% token reduction, while retaining high semantic similarity to the
source text. We explain how this novel compression technique can be lossless
and how the detail granularity is controllable. We discuss benchmark results
over open source data (i.e. Bram Stoker's Dracula available through Project
Gutenberg) and show how our results hold at the paragraph level, across
multiple genres and models.

</details>


### [32] [Are LLMs complicated ethical dilemma analyzers?](https://arxiv.org/abs/2505.08106)
*Jiashen,Du,Jesse Yao,Allen Liu,Zhekai Zhang*

Main category: cs.CL

TL;DR: LLMs在伦理推理中表现优于非专家人类，但在历史背景和复杂策略上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否能模拟人类伦理推理并作为可信代理。

Method: 使用包含196个伦理困境的基准数据集，评估多个LLM的表现，采用复合指标框架。

Result: LLMs在词汇和结构对齐上优于人类，但在历史背景和策略上表现不佳。

Conclusion: LLMs在伦理决策中具有潜力，但仍需改进历史理解和策略生成。

Abstract: One open question in the study of Large Language Models (LLMs) is whether
they can emulate human ethical reasoning and act as believable proxies for
human judgment. To investigate this, we introduce a benchmark dataset
comprising 196 real-world ethical dilemmas and expert opinions, each segmented
into five structured components: Introduction, Key Factors, Historical
Theoretical Perspectives, Resolution Strategies, and Key Takeaways. We also
collect non-expert human responses for comparison, limited to the Key Factors
section due to their brevity. We evaluate multiple frontier LLMs (GPT-4o-mini,
Claude-3.5-Sonnet, Deepseek-V3, Gemini-1.5-Flash) using a composite metric
framework based on BLEU, Damerau-Levenshtein distance, TF-IDF cosine
similarity, and Universal Sentence Encoder similarity. Metric weights are
computed through an inversion-based ranking alignment and pairwise AHP
analysis, enabling fine-grained comparison of model outputs to expert
responses. Our results show that LLMs generally outperform non-expert humans in
lexical and structural alignment, with GPT-4o-mini performing most consistently
across all sections. However, all models struggle with historical grounding and
proposing nuanced resolution strategies, which require contextual abstraction.
Human responses, while less structured, occasionally achieve comparable
semantic similarity, suggesting intuitive moral reasoning. These findings
highlight both the strengths and current limitations of LLMs in ethical
decision-making.

</details>


### [33] [Putting It All into Context: Simplifying Agents with LCLMs](https://arxiv.org/abs/2505.08120)
*Mingjian Jiang,Yangjun Ruan,Luis Lastras,Pavan Kapanipathi,Tatsunori Hashimoto*

Main category: cs.CL

TL;DR: 研究表明，在复杂任务（如SWE-bench）中，简单地将整个环境放入长上下文语言模型（LCLM）并正确提示模型，可以使其性能与精心设计的复杂代理架构相媲美。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型代理架构的复杂性是否必要，尤其是在处理复杂任务时。

Method: 使用无支架的长上下文语言模型（如Gemini-1.5-Pro和Gemini-2.5-Pro）进行测试，并与复杂代理架构对比。

Result: Gemini-1.5-Pro无支架方法达到38%解决率，与复杂代理架构（32%）相当；Gemini-2.5-Pro无支架方法达到50.8%。

Conclusion: 在某些情况下，简单的无支架方法可以替代复杂的代理架构，尤其是在使用更强大的模型时。

Abstract: Recent advances in language model (LM) agents have demonstrated significant
potential for automating complex real-world tasks. To make progress on these
difficult tasks, LM agent architectures have become increasingly complex, often
incorporating multi-step retrieval tools, multiple agents, and scaffolding
adapted to the underlying LM. In this work, we investigate whether all of this
complexity is necessary, or if parts of these scaffolds can be removed on
challenging tasks like SWE-bench. We show that in the case of SWE-bench, simply
putting the entire environment into the context of a long context language
model (LCLM) and properly prompting the model makes it competitive with
carefully tuned, complex agent scaffolds. We show that a Gemini-1.5-Pro model
without any scaffolding or tools achieves 38% on SWE-Bench-Verified, comparable
with approaches using carefully tuned agent scaffolds (32%). While the
unscaffolded approach with Gemini-1.5-Pro falls short of the strongest agentic
architectures, we demonstrate that the more capable Gemini-2.5-Pro using the
same unscaffolded approach directly attains a 50.8% solve rate. Additionally, a
two-stage approach combining Gemini-1.5-Pro with Claude-3.7 achieves a
competitive 48.6% solve rate.

</details>


### [34] [ALOHA: Empowering Multilingual Agent for University Orientation with Hierarchical Retrieval](https://arxiv.org/abs/2505.08130)
*Mingxu Tao,Bowen Tang,Mingxuan Ma,Yining Zhang,Hourun Li,Feifan Wen,Hao Ma,Jia Yang*

Main category: cs.CL

TL;DR: ALOHA是一种多语言代理，通过分层检索增强，用于解决大学校园信息检索问题，支持多语言和实时交互。


<details>
  <summary>Details</summary>
Motivation: 现有公开服务无法满足师生对校园特定信息的需求，主要由于LLM缺乏领域知识和搜索引擎在多语言及实时场景中的局限性。

Method: 引入ALOHA系统，结合分层检索和外部API，提供交互式多语言服务。

Result: 人评估和案例研究表明，ALOHA在多语言查询中表现优于商业聊天机器人和搜索引擎，已为超过12,000人提供服务。

Conclusion: ALOHA系统能够高效、准确地满足校园信息检索需求，具有实际应用价值。

Abstract: The rise of Large Language Models~(LLMs) revolutionizes information
retrieval, allowing users to obtain required answers through complex
instructions within conversations. However, publicly available services remain
inadequate in addressing the needs of faculty and students to search
campus-specific information. It is primarily due to the LLM's lack of
domain-specific knowledge and the limitation of search engines in supporting
multilingual and timely scenarios. To tackle these challenges, we introduce
ALOHA, a multilingual agent enhanced by hierarchical retrieval for university
orientation. We also integrate external APIs into the front-end interface to
provide interactive service. The human evaluation and case study show our
proposed system has strong capabilities to yield correct, timely, and
user-friendly responses to the queries in multiple languages, surpassing
commercial chatbots and search engines. The system has been deployed and has
provided service for more than 12,000 people.

</details>


### [35] [Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage](https://arxiv.org/abs/2505.08167)
*Ruilin Liu,Zhixiao Zhao,Jieqiong Li,Chang Liu,Dongbo Wang*

Main category: cs.CL

TL;DR: 论文提出了一种结合双向思维链和奖励机制的新训练方法，用于解决领域特定大语言模型（如ICH-Qwen）在微调过程中面临的偏见、知识继承错误和灾难性遗忘问题。该方法在问答任务中表现优于其他方法，并展示了跨领域的适应性。


<details>
  <summary>Details</summary>
Motivation: 解决领域特定大语言模型（如ICH-Qwen）在微调过程中面临的偏见、知识继承错误和灾难性遗忘问题。

Method: 提出了一种结合双向思维链（正向和反向推理）和奖励机制（结构和内容评估）的训练方法。

Result: 在ICH-Qwen上进行的实验表明，该方法在准确性、Bleu-4和Rouge-L评分上优于其他方法，并通过消融实验验证了其有效性。跨领域实验也证明了其通用性。

Conclusion: 该方法为领域特定大语言模型的训练提供了一种有效且通用的解决方案，适用于未来多领域的应用。

Abstract: The rapid development of large language models (LLMs) has provided
significant support and opportunities for the advancement of domain-specific
LLMs. However, fine-tuning these large models using Intangible Cultural
Heritage (ICH) data inevitably faces challenges such as bias, incorrect
knowledge inheritance, and catastrophic forgetting. To address these issues, we
propose a novel training method that integrates a bidirectional chains of
thought and a reward mechanism. This method is built upon ICH-Qwen, a large
language model specifically designed for the field of intangible cultural
heritage. The proposed method enables the model to not only perform forward
reasoning but also enhances the accuracy of the generated answers by utilizing
reverse questioning and reverse reasoning to activate the model's latent
knowledge. Additionally, a reward mechanism is introduced during training to
optimize the decision-making process. This mechanism improves the quality of
the model's outputs through structural and content evaluations with different
weighting schemes. We conduct comparative experiments on ICH-Qwen, with results
demonstrating that our method outperforms 0-shot, step-by-step reasoning,
knowledge distillation, and question augmentation methods in terms of accuracy,
Bleu-4, and Rouge-L scores on the question-answering task. Furthermore, the
paper highlights the effectiveness of combining the bidirectional chains of
thought and reward mechanism through ablation experiments. In addition, a
series of generalizability experiments are conducted, with results showing that
the proposed method yields improvements on various domain-specific datasets and
advanced models in areas such as Finance, Wikidata, and StrategyQA. This
demonstrates that the method is adaptable to multiple domains and provides a
valuable approach for model training in future applications across diverse
fields.

</details>


### [36] [Exploiting Text Semantics for Few and Zero Shot Node Classification on Text-attributed Graph](https://arxiv.org/abs/2505.08168)
*Yuxiang Wang,Xiao Yan,Shiyu Jin,Quanqing Xu,Chuang Hu,Yuanyuan Zhu,Bo Du,Jia Wu,Jiawei Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种名为TSA的文本语义增强方法，通过引入更多文本语义监督信号，提升文本属性图（TAG）中少样本和零样本节点分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖基于图的增强技术，而基于文本的增强技术尚未充分探索。TSA旨在填补这一空白，通过文本语义增强提升分类性能。

Method: 设计了两种增强技术：正语义匹配（检索相似文本匹配节点）和负语义对比（添加负提示构造相反语义文本）。

Result: 在5个数据集上评估，TSA优于13个基线方法，通常比最佳基线准确率提升5%以上。

Conclusion: TSA通过文本语义增强显著提升了节点分类性能，为TAG任务提供了新思路。

Abstract: Text-attributed graph (TAG) provides a text description for each graph node,
and few- and zero-shot node classification on TAGs have many applications in
fields such as academia and social networks. Existing work utilizes various
graph-based augmentation techniques to train the node and text embeddings,
while text-based augmentations are largely unexplored. In this paper, we
propose Text Semantics Augmentation (TSA) to improve accuracy by introducing
more text semantic supervision signals. Specifically, we design two
augmentation techniques, i.e., positive semantics matching and negative
semantics contrast, to provide more reference texts for each graph node or text
description. Positive semantic matching retrieves texts with similar embeddings
to match with a graph node. Negative semantic contrast adds a negative prompt
to construct a text description with the opposite semantics, which is
contrasted with the original node and text. We evaluate TSA on 5 datasets and
compare with 13 state-of-the-art baselines. The results show that TSA
consistently outperforms all baselines, and its accuracy improvements over the
best-performing baseline are usually over 5%.

</details>


### [37] [A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs](https://arxiv.org/abs/2505.08200)
*Artem Shelmanov,Ekaterina Fadeeva,Akim Tsvigun,Ivan Tsvigun,Zhuohan Xie,Igor Kiselev,Nico Daheim,Caiqi Zhang,Artem Vazhentsev,Mrinmaya Sachan,Preslav Nakov,Timothy Baldwin*

Main category: cs.CL

TL;DR: 论文提出了一种预训练的不确定性量化（UQ）模块，用于增强大型语言模型（LLMs）检测幻觉的能力，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: LLMs容易生成虚假信息（幻觉），而用户缺乏检测工具，不确定性量化（UQ）可以帮助评估模型输出的可靠性。

Method: 引入预训练的UQ模块，利用Transformer架构和LLM注意力图提取的特征，提升不确定性捕捉能力。

Result: 实验表明，这些模块在幻觉检测上表现优异，且能泛化到未训练的语言。

Conclusion: 预训练的UQ模块显著提升了LLMs的可靠性，并公开了代码和模型。

Abstract: Large Language Models (LLMs) have the tendency to hallucinate, i.e., to
sporadically generate false or fabricated information. This presents a major
challenge, as hallucinations often appear highly convincing and users generally
lack the tools to detect them. Uncertainty quantification (UQ) provides a
framework for assessing the reliability of model outputs, aiding in the
identification of potential hallucinations. In this work, we introduce
pre-trained UQ heads: supervised auxiliary modules for LLMs that substantially
enhance their ability to capture uncertainty compared to unsupervised UQ
methods. Their strong performance stems from the powerful Transformer
architecture in their design and informative features derived from LLM
attention maps. Experimental evaluation shows that these heads are highly
robust and achieve state-of-the-art performance in claim-level hallucination
detection across both in-domain and out-of-domain prompts. Moreover, these
modules demonstrate strong generalization to languages they were not explicitly
trained on. We pre-train a collection of UQ heads for popular LLM series,
including Mistral, Llama, and Gemma 2. We publicly release both the code and
the pre-trained heads.

</details>


### [38] [Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement](https://arxiv.org/abs/2505.08245)
*Haoran Ye,Jing Jin,Yuhang Xie,Xin Zhang,Guojie Song*

Main category: cs.CL

TL;DR: 论文提出LLM心理测量学这一新兴交叉领域，结合心理测量工具与理论评估大语言模型，以解决传统评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的快速发展超越了传统评估方法，需要新的评估范式来量化人类心理特质并推动以人为中心的AI系统。

Method: 通过系统整合心理测量学的工具、理论与原则，扩展评估范围、优化方法并验证结果。

Result: 提出了LLM心理测量学的结构化框架，为跨学科研究者提供全面理解，并提供了相关资源库。

Conclusion: 旨在推动未来评估范式的发展，实现人类水平AI，促进以人为中心的AI系统为社会服务。

Abstract: The rapid advancement of large language models (LLMs) has outpaced
traditional evaluation methodologies. It presents novel challenges, such as
measuring human-like psychological constructs, navigating beyond static and
task-specific benchmarks, and establishing human-centered evaluation. These
challenges intersect with Psychometrics, the science of quantifying the
intangible aspects of human psychology, such as personality, values, and
intelligence. This survey introduces and synthesizes an emerging
interdisciplinary field of LLM Psychometrics, which leverages psychometric
instruments, theories, and principles to evaluate, understand, and enhance
LLMs. We systematically explore the role of Psychometrics in shaping
benchmarking principles, broadening evaluation scopes, refining methodologies,
validating results, and advancing LLM capabilities. This paper integrates
diverse perspectives to provide a structured framework for researchers across
disciplines, enabling a more comprehensive understanding of this nascent field.
Ultimately, we aim to provide actionable insights for developing future
evaluation paradigms that align with human-level AI and promote the advancement
of human-centered AI systems for societal benefit. A curated repository of LLM
psychometric resources is available at
https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.

</details>


### [39] [Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual Compression for Scalable Knowledge Integration](https://arxiv.org/abs/2505.08261)
*Rishabh Agrawal,Himanshu Kumar*

Main category: cs.CL

TL;DR: 论文提出自适应上下文压缩（ACC）和混合CAG-RAG框架，以优化大规模动态知识库的缓存增强生成（CAG）效率。


<details>
  <summary>Details</summary>
Motivation: 解决缓存增强生成（CAG）在大规模和动态知识库中扩展性不足的问题。

Method: 引入自适应上下文压缩（ACC）动态管理上下文输入，并提出混合CAG-RAG框架结合选择性检索。

Result: 实验表明，该方法提升了可扩展性、效率和多跳推理性能。

Conclusion: 提出的方法为实际知识集成挑战提供了实用解决方案。

Abstract: The rapid progress in large language models (LLMs) has paved the way for
novel approaches in knowledge-intensive tasks. Among these, Cache-Augmented
Generation (CAG) has emerged as a promising alternative to Retrieval-Augmented
Generation (RAG). CAG minimizes retrieval latency and simplifies system design
by preloading knowledge into the model's context. However, challenges persist
in scaling CAG to accommodate large and dynamic knowledge bases effectively.
This paper introduces Adaptive Contextual Compression (ACC), an innovative
technique designed to dynamically compress and manage context inputs, enabling
efficient utilization of the extended memory capabilities of modern LLMs. To
further address the limitations of standalone CAG, we propose a Hybrid CAG-RAG
Framework, which integrates selective retrieval to augment preloaded contexts
in scenarios requiring additional information. Comprehensive evaluations on
diverse datasets highlight the proposed methods' ability to enhance
scalability, optimize efficiency, and improve multi-hop reasoning performance,
offering practical solutions for real-world knowledge integration challenges.

</details>


### [40] [Evaluating the Effectiveness of Black-Box Prompt Optimization as the Scale of LLMs Continues to Grow](https://arxiv.org/abs/2505.08303)
*Ziyu Zhou,Yihang Wu,Jingyuan Yang,Zhan Xiao,Rongjun Li*

Main category: cs.CL

TL;DR: 黑盒提示优化方法在大规模语言模型（如DeepSeek V3和Gemini 2.0 Flash）上效果有限，且随着模型规模增大，优化效果减弱。


<details>
  <summary>Details</summary>
Motivation: 研究黑盒提示优化方法在大规模语言模型上的有效性，验证其是否仍能显著提升性能。

Method: 选取三种黑盒优化方法，在多个大规模LLM（如DeepSeek V3和Gemini 2.0 Flash）及不同规模模型（Qwen 2.5系列）上进行实验。

Result: 黑盒优化方法在大规模LLM上仅带来有限改进，且模型规模越大，优化效果越差。

Conclusion: 模型规模是影响黑盒提示优化效果的主要因素，大规模LLM可能不再需要此类优化。

Abstract: Black-Box prompt optimization methods have emerged as a promising strategy
for refining input prompts to better align large language models (LLMs),
thereby enhancing their task performance. Although these methods have
demonstrated encouraging results, most studies and experiments have primarily
focused on smaller-scale models (e.g., 7B, 14B) or earlier versions (e.g.,
GPT-3.5) of LLMs. As the scale of LLMs continues to increase, such as with
DeepSeek V3 (671B), it remains an open question whether these black-box
optimization techniques will continue to yield significant performance
improvements for models of such scale. In response to this, we select three
well-known black-box optimization methods and evaluate them on large-scale LLMs
(DeepSeek V3 and Gemini 2.0 Flash) across four NLU and NLG datasets. The
results show that these black-box prompt optimization methods offer only
limited improvements on these large-scale LLMs. Furthermore, we hypothesize
that the scale of the model is the primary factor contributing to the limited
benefits observed. To explore this hypothesis, we conducted experiments on LLMs
of varying sizes (Qwen 2.5 series, ranging from 7B to 72B) and observed an
inverse scaling law, wherein the effectiveness of black-box optimization
methods diminished as the model size increased.

</details>


### [41] [AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale](https://arxiv.org/abs/2505.08311)
*Yunjie Ji,Xiaoyu Tian,Sitong Zhao,Haotian Wang,Shuaiting Chen,Yiping Peng,Han Zhao,Xiangang Li*

Main category: cs.CL

TL;DR: AM-Thinking-v1是一个32B密集语言模型，在数学和编码能力上表现优异，超越DeepSeek-R1并与领先的MoE模型竞争。


<details>
  <summary>Details</summary>
Motivation: 展示开源社区在32B规模模型上的高性能，平衡顶级性能和实际可用性。

Method: 基于开源Qwen2.5-32B模型，结合监督微调和强化学习的后训练流程。

Result: 在AIME 2024、2025和LiveCodeBench上分别取得85.3、74.4和70.3的高分。

Conclusion: AM-Thinking-v1证明了开源协作在中等规模模型中的潜力，推动推理能力的边界。

Abstract: We present AM-Thinking-v1, a 32B dense language model that advances the
frontier of reasoning, embodying the collaborative spirit of open-source
innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts
(MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves
impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on
LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities
among open-source models of similar scale.
  Built entirely from the open-source Qwen2.5-32B base model and publicly
available queries, AM-Thinking-v1 leverages a meticulously crafted
post-training pipeline - combining supervised fine-tuning and reinforcement
learning - to deliver exceptional reasoning capabilities. This work
demonstrates that the open-source community can achieve high performance at the
32B scale, a practical sweet spot for deployment and fine-tuning. By striking a
balance between top-tier performance and real-world usability, we hope
AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale
models, pushing reasoning boundaries while keeping accessibility at the core of
innovation. We have open-sourced our model on
\href{https://huggingface.co/a-m-team/AM-Thinking-v1}{Hugging Face}.

</details>


### [42] [On the Geometry of Semantics in Next-token Prediction](https://arxiv.org/abs/2505.08348)
*Yize Zhao,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 论文探讨了现代语言模型如何通过简单的下一个词预测（NTP）训练目标学习潜在语义和语法概念。研究发现，NTP优化隐式引导模型通过奇异值分解（SVD）编码概念，揭示了词嵌入如何捕捉语言结构。


<details>
  <summary>Details</summary>
Motivation: 理解NTP训练目标如何引导语言模型学习语义和语法概念，填补分布语义学、神经崩溃几何与神经网络训练动态之间的空白。

Method: 通过分析NTP优化的隐式偏置，提出SVD分解方法揭示词嵌入的语义结构，并利用谱聚类识别可解释的语义概念。

Result: 研究发现NTP优化隐式引导模型通过SVD编码语义概念，且最重要的SVD因子在训练早期学习。新提出的正交聚类方法能有效识别语义。

Conclusion: 研究揭示了NTP训练目标如何通过隐式偏置引导语言模型学习语义表示，为理解语言模型的语义学习机制提供了新视角。

Abstract: Modern language models demonstrate a remarkable ability to capture linguistic
meaning despite being trained solely through next-token prediction (NTP). We
investigate how this conceptually simple training objective leads models to
extract and encode latent semantic and grammatical concepts. Our analysis
reveals that NTP optimization implicitly guides models to encode concepts via
singular value decomposition (SVD) factors of a centered data-sparsity matrix
that captures next-word co-occurrence patterns. While the model never
explicitly constructs this matrix, learned word and context embeddings
effectively factor it to capture linguistic structure. We find that the most
important SVD factors are learned first during training, motivating the use of
spectral clustering of embeddings to identify human-interpretable semantics,
including both classical k-means and a new orthant-based method directly
motivated by our interpretation of concepts. Overall, our work bridges
distributional semantics, neural collapse geometry, and neural network training
dynamics, providing insights into how NTP's implicit biases shape the emergence
of meaning representations in language models.

</details>


### [43] [Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring](https://arxiv.org/abs/2505.08351)
*Mina Almasi,Ross Deans Kristensen-McLachlan*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）作为第二语言学习自适应导师的潜力，评估了系统提示能否可靠控制模型生成适合学生水平的文本。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在语言学习中的适应性，特别是通过系统提示控制文本难度。

Method: 使用7B至12B参数的LLMs模拟西班牙语师生对话，通过CEFR提示控制文本难度。

Result: 系统提示可约束模型输出，但长期交互中会出现对齐漂移。

Conclusion: LLMs可作为个性化导师，但需改进提示方法以应对长期交互挑战。

Abstract: This paper investigates the potentials of Large Language Models (LLMs) as
adaptive tutors in the context of second-language learning. In particular, we
evaluate whether system prompting can reliably constrain LLMs to generate only
text appropriate to the student's competence level. We simulate full
teacher-student dialogues in Spanish using instruction-tuned, open-source LLMs
ranging in size from 7B to 12B parameters. Dialogues are generated by having an
LLM alternate between tutor and student roles with separate chat histories. The
output from the tutor model is then used to evaluate the effectiveness of
CEFR-based prompting to control text difficulty across three proficiency levels
(A1, B1, C1). Our findings suggest that while system prompting can be used to
constrain model outputs, prompting alone is too brittle for sustained,
long-term interactional contexts - a phenomenon we term alignment drift. Our
results provide insights into the feasibility of LLMs for personalized,
proficiency-aligned adaptive tutors and provide a scalable method for low-cost
evaluation of model performance without human participants.

</details>


### [44] [Towards Contamination Resistant Benchmarks](https://arxiv.org/abs/2505.08389)
*Rahmatullah Musawi,Sheng Lu*

Main category: cs.CL

TL;DR: 论文提出了一种基于凯撒密码的污染抵抗基准，用于更严格地评估大型语言模型（LLMs），揭示了当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: LLM评估的可靠性受到污染问题的严重影响，需要开发污染抵抗的基准以更准确地评估模型能力。

Method: 设计基于凯撒密码的基准测试，控制污染因素，测试多种LLMs在不同设置下的表现。

Result: 发现LLMs在污染受控的情况下表现不佳，揭示了模型能力的局限性。

Conclusion: 研究为开发污染抵抗基准提供了方向，有助于更准确地评估LLMs的真实能力。

Abstract: The rapid development of large language models (LLMs) has transformed the
landscape of natural language processing. Evaluating LLMs properly is crucial
for understanding their potential and addressing concerns such as safety.
However, LLM evaluation is confronted by various factors, among which
contamination stands out as a key issue that undermines the reliability of
evaluations. In this work, we introduce the concept of contamination resistance
to address this challenge. We propose a benchmark based on Caesar ciphers
(e.g., "ab" to "bc" when the shift is 1), which, despite its simplicity, is an
excellent example of a contamination resistant benchmark. We test this
benchmark on widely used LLMs under various settings, and we find that these
models struggle with this benchmark when contamination is controlled. Our
findings reveal issues in current LLMs and raise important questions regarding
their true capabilities. Our work contributes to the development of
contamination resistant benchmarks, enabling more rigorous LLM evaluation and
offering insights into the true capabilities and limitations of LLMs.

</details>


### [45] [Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping](https://arxiv.org/abs/2505.08392)
*Ren Zhuang,Ben Wang,Shuifa Sun*

Main category: cs.CL

TL;DR: 论文提出Adaptive GoGI-Skip框架，通过动态压缩Chain-of-Thought（CoT）提示，显著提升推理效率，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 当前CoT压缩技术依赖通用重要性指标和静态压缩率，可能导致关键信息丢失或无法适应不同推理复杂度。

Method: 提出Goal-Gradient Importance（GoGI）指标和Adaptive Dynamic Skipping（ADS）机制，动态调整压缩率。

Result: 在多个推理基准测试中，平均减少45%的CoT标记，推理速度提升1.6-2.0倍，同时保持高准确性。

Conclusion: Adaptive GoGI-Skip在效率和准确性之间取得平衡，优于现有基线，推动了CoT推理技术的发展。

Abstract: Large Language Models leverage Chain-of-Thought (CoT) prompting for complex
tasks, but their reasoning traces are often excessively verbose and
inefficient, leading to significant computational costs and latency. Current
CoT compression techniques typically rely on generic importance metrics and
static compression rates, which may inadvertently remove functionally critical
tokens or fail to adapt to varying reasoning complexity. To overcome these
limitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic
CoT compression via supervised fine-tuning. This approach introduces two
synergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric
accurately identifying functionally relevant tokens by measuring the gradient
influence of their intermediate representations on the final answer loss, and
(2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the
compression rate based on runtime model uncertainty while ensuring local
coherence through an adaptive N-token constraint. To our knowledge, this is the
first work unifying a goal-oriented, gradient-based importance metric with
dynamic, uncertainty-aware skipping for CoT compression. Trained on compressed
MATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization
across diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It
achieves substantial efficiency gains - reducing CoT token counts by over 45%
on average and delivering 1.6-2.0 times inference speedups - while maintaining
high reasoning accuracy. Notably, it significantly outperforms existing
baselines by preserving accuracy even at high effective compression rates,
advancing the state of the art in the CoT reasoning efficiency-accuracy
trade-off.

</details>


### [46] [TUMS: Enhancing Tool-use Abilities of LLMs with Multi-structure Handlers](https://arxiv.org/abs/2505.08402)
*Aiyao He,Sijia Cui,Shuai Xu,Yanna Wang,Bo Xu*

Main category: cs.CL

TL;DR: TUMS框架通过参数级处理提升LLMs工具使用能力，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在工具使用中因参数生成问题导致非可执行或不正确操作，需改进。

Method: TUMS框架包含意图识别、任务分解、子任务处理和执行四部分。

Result: 在ToolQA基准测试中，TUMS框架在简单和困难任务上分别提升19.6%和50.6%。

Conclusion: TUMS框架有效提升LLMs工具使用能力，为未来研究提供新方向。

Abstract: Recently, large language models(LLMs) have played an increasingly important
role in solving a wide range of NLP tasks, leveraging their capabilities of
natural language understanding and generating. Integration with external tools
further enhances LLMs' effectiveness, providing more precise, timely, and
specialized responses. However, LLMs still encounter difficulties with
non-executable actions and improper actions, which are primarily attributed to
incorrect parameters. The process of generating parameters by LLMs is confined
to the tool level, employing the coarse-grained strategy without considering
the different difficulties of various tools. To address this issue, we propose
TUMS, a novel framework designed to enhance the tool-use capabilities of LLMs
by transforming tool-level processing into parameter-level processing.
Specifically, our framework consists of four key components: (1) an intent
recognizer that identifies the user's intent to help LLMs better understand the
task; (2) a task decomposer that breaks down complex tasks into simpler
subtasks, each involving a tool call; (3) a subtask processor equipped with
multi-structure handlers to generate accurate parameters; and (4) an executor.
Our empirical studies have evidenced the effectiveness and efficiency of the
TUMS framework with an average of 19.6\% and 50.6\% improvement separately on
easy and hard benchmarks of ToolQA, meanwhile, we demonstrated the key
contribution of each part with ablation experiments, offering more insights and
stimulating future research on Tool-augmented LLMs.

</details>


### [47] [Hakim: Farsi Text Embedding Model](https://arxiv.org/abs/2505.08435)
*Mehran Sarmadi,Morteza Alikhani,Erfan Zinvandi,Zahra Pourbahman*

Main category: cs.CL

TL;DR: 本文提出Hakim，一种波斯语文本嵌入模型，性能提升8.5%，并引入三个新数据集。


<details>
  <summary>Details</summary>
Motivation: 波斯语在大规模嵌入研究中代表性不足，需提升其语言理解能力。

Method: 基于BERT架构设计Hakim模型，引入新数据集Corpesia、Pairsia-sup和Pairsia-unsup。

Result: Hakim在FaMTEB基准测试中表现优异，RetroMAE模型在信息检索中效果显著。

Conclusion: Hakim为波斯语理解奠定新基础，适用于聊天机器人和RAG系统。

Abstract: Recent advancements in text embedding have significantly improved natural
language understanding across many languages, yet Persian remains notably
underrepresented in large-scale embedding research. In this paper, we present
Hakim, a novel state-of-the-art Persian text embedding model that achieves a
8.5% performance improvement over existing approaches on the FaMTEB benchmark,
outperforming all previously developed Persian language models. As part of this
work, we introduce three new datasets - Corpesia, Pairsia-sup, and
Pairsia-unsup - to support supervised and unsupervised training scenarios.
Additionally, Hakim is designed for applications in chatbots and
retrieval-augmented generation (RAG) systems, particularly addressing retrieval
tasks that require incorporating message history within these systems. We also
propose a new baseline model built on the BERT architecture. Our language model
consistently achieves higher accuracy across various Persian NLP tasks, while
the RetroMAE-based model proves particularly effective for textual information
retrieval applications. Together, these contributions establish a new
foundation for advancing Persian language understanding.

</details>


### [48] [A document processing pipeline for the construction of a dataset for topic modeling based on the judgments of the Italian Supreme Court](https://arxiv.org/abs/2505.08439)
*Matteo Marulli,Glauco Panattoni,Marco Bertini*

Main category: cs.CL

TL;DR: 为解决意大利法律研究中缺乏公开数据集的问题，开发了一个文档处理流程，生成适用于主题建模的匿名数据集。


<details>
  <summary>Details</summary>
Motivation: 意大利法律研究中缺乏公开数据集，限制了最高法院判决中法律主题的分析。

Method: 集成了文档布局分析（YOLOv8x）、光学字符识别和文本匿名化的处理流程。

Result: DLA模块的mAP@50为0.964，OCR检测器的mAP@50-95为0.9022，文本识别器的字符错误率为0.0047。数据集提升了主题建模的多样性（0.6198）和一致性（0.6638）。

Conclusion: 使用BERTopic提取主题，并通过大型语言模型生成标签和摘要，结果与领域专家解释一致。

Abstract: Topic modeling in Italian legal research is hindered by the lack of public
datasets, limiting the analysis of legal themes in Supreme Court judgments. To
address this, we developed a document processing pipeline that produces an
anonymized dataset optimized for topic modeling.
  The pipeline integrates document layout analysis (YOLOv8x), optical character
recognition, and text anonymization. The DLA module achieved a mAP@50 of 0.964
and a mAP@50-95 of 0.800. The OCR detector reached a mAP@50-95 of 0.9022, and
the text recognizer (TrOCR) obtained a character error rate of 0.0047 and a
word error rate of 0.0248. Compared to OCR-only methods, our dataset improved
topic modeling with a diversity score of 0.6198 and a coherence score of
0.6638.
  We applied BERTopic to extract topics and used large language models to
generate labels and summaries. Outputs were evaluated against domain expert
interpretations. Claude Sonnet 3.7 achieved a BERTScore F1 of 0.8119 for
labeling and 0.9130 for summarization.

</details>


### [49] [IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation](https://arxiv.org/abs/2505.08450)
*Kazuki Hayashi,Hidetaka Kamigaito,Shinya Kouda,Taro Watanabe*

Main category: cs.CL

TL;DR: IterKey是一个基于LLM的迭代关键词生成框架，通过稀疏检索增强RAG，平衡了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决密集检索方法缺乏可解释性和稀疏检索方法无法完全捕捉查询意图的问题。

Method: 采用三阶段LLM驱动流程：生成检索关键词、基于检索文档生成答案、验证答案，失败时迭代优化关键词。

Result: 在四个QA任务中，IterKey比BM25-based RAG和简单基线方法提升了5%至20%的准确率，性能接近密集检索方法。

Conclusion: IterKey是一种新颖的BM25-based方法，通过LLM迭代优化RAG，有效平衡了准确性和可解释性。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a way to complement the
in-context knowledge of Large Language Models (LLMs) by integrating external
documents. However, real-world applications demand not only accuracy but also
interpretability. While dense retrieval methods provide high accuracy, they
lack interpretability; conversely, sparse retrieval methods offer transparency
but often fail to capture the full intent of queries due to their reliance on
keyword matching. To address these issues, we introduce IterKey, an LLM-driven
iterative keyword generation framework that enhances RAG via sparse retrieval.
IterKey consists of three LLM-driven stages: generating keywords for retrieval,
generating answers based on retrieved documents, and validating the answers. If
validation fails, the process iteratively repeats with refined keywords. Across
four QA tasks, experimental results show that IterKey achieves 5% to 20%
accuracy improvements over BM25-based RAG and simple baselines. Its performance
is comparable to dense retrieval-based RAG and prior iterative query refinement
methods using dense models. In summary, IterKey is a novel BM25-based approach
leveraging LLMs to iteratively refine RAG, effectively balancing accuracy with
interpretability.

</details>


### [50] [RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models](https://arxiv.org/abs/2505.08463)
*Fujun Zhang,XiangDong Su*

Main category: cs.CL

TL;DR: 论文提出RepCali方法，通过校准预训练语言模型（PLM）的潜在空间表示，解决编码器与解码器输入不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: PLM在微调后仍存在编码器输出与解码器输入不匹配的问题，影响性能。

Method: 在编码器后加入校准模块，优化潜在空间表示作为解码器输入。

Result: 在25个PLM模型和8个任务中验证，RepCali显著提升性能，优于基准方法。

Conclusion: RepCali通用性强、易实现，能有效提升PLM在下游任务中的表现。

Abstract: Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm
in applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs
still struggle with the discrepancies between the representation obtained from
the PLMs' encoder and the optimal input to the PLMs' decoder. This paper
tackles this challenge by learning to calibrate the representation of PLMs in
the latent space. In the proposed representation calibration method (RepCali),
we integrate a specific calibration block to the latent space after the encoder
and use the calibrated output as the decoder input. The merits of the proposed
RepCali include its universality to all PLMs with encoder-decoder
architectures, its plug-and-play nature, and ease of implementation. Extensive
experiments on 25 PLM-based models across 8 tasks (including both English and
Chinese datasets) demonstrate that the proposed RepCali offers desirable
enhancements to PLMs (including LLMs) and significantly improves the
performance of downstream tasks. Comparison experiments across 4 benchmark
tasks indicate that RepCali is superior to the representative fine-tuning
baselines.

</details>


### [51] [Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions](https://arxiv.org/abs/2505.08464)
*Lata Pangtey,Anukriti Bhatnagar,Shubhi Bansal,Shahid Shafi Dar,Nagendra Kumar*

Main category: cs.CL

TL;DR: 本文综述了基于大语言模型（LLMs）的立场检测研究，系统分析了其方法、数据集、应用及挑战，并提出了新的分类法。


<details>
  <summary>Details</summary>
Motivation: 现有综述缺乏对LLMs在立场检测中应用的全面覆盖，本文旨在填补这一空白。

Method: 通过系统分析，提出基于学习方式、数据模态和目标关系的分类法，并讨论评估技术和数据集。

Result: 总结了LLMs在立场检测中的优势与局限，并探讨了其在多个领域的应用。

Conclusion: 指出了未来研究方向，如可解释性推理和低资源适应，为开发下一代立场检测系统提供指导。

Abstract: Stance detection is essential for understanding subjective content across
various platforms such as social media, news articles, and online reviews.
Recent advances in Large Language Models (LLMs) have revolutionized stance
detection by introducing novel capabilities in contextual understanding,
cross-domain generalization, and multimodal analysis. Despite these
progressions, existing surveys often lack comprehensive coverage of approaches
that specifically leverage LLMs for stance detection. To bridge this critical
gap, our review article conducts a systematic analysis of stance detection,
comprehensively examining recent advancements of LLMs transforming the field,
including foundational concepts, methodologies, datasets, applications, and
emerging challenges. We present a novel taxonomy for LLM-based stance detection
approaches, structured along three key dimensions: 1) learning methods,
including supervised, unsupervised, few-shot, and zero-shot; 2) data
modalities, such as unimodal, multimodal, and hybrid; and 3) target
relationships, encompassing in-target, cross-target, and multi-target
scenarios. Furthermore, we discuss the evaluation techniques and analyze
benchmark datasets and performance trends, highlighting the strengths and
limitations of different architectures. Key applications in misinformation
detection, political analysis, public health monitoring, and social media
moderation are discussed. Finally, we identify critical challenges such as
implicit stance expression, cultural biases, and computational constraints,
while outlining promising future directions, including explainable stance
reasoning, low-resource adaptation, and real-time deployment frameworks. Our
survey highlights emerging trends, open challenges, and future directions to
guide researchers and practitioners in developing next-generation stance
detection systems powered by large language models.

</details>


### [52] [Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?](https://arxiv.org/abs/2505.08468)
*Md Tahmid Rahman Laskar,Mohammed Saidul Islam,Ridwan Mahbub,Ahmed Masry,Mizanur Rahman,Amran Bhuiyan,Mir Tafseer Nayeem,Shafiq Joty,Enamul Hoque,Jimmy Huang*

Main category: cs.CL

TL;DR: 论文评估了13种开源大型视觉语言模型（LVLM）作为图表理解任务的自动评估工具，发现部分模型性能接近GPT-4，但存在位置偏好和长度偏差等问题。


<details>
  <summary>Details</summary>
Motivation: 图表理解任务需求增长，但现有评估方法成本高且耗时，限制了实际应用。

Method: 设计了成对和点对点评估任务，涵盖事实准确性、信息量和相关性等标准，并分析模型在格式、位置一致性等方面的表现。

Result: 部分开源LVLM评估性能接近GPT-4（80%一致），但部分表现较差（低于10%一致）。

Conclusion: 开源LVLM可作为图表任务的成本效益评估工具，但仍需解决位置偏好和长度偏差等问题。

Abstract: Charts are ubiquitous as they help people understand and reason with data.
Recently, various downstream tasks, such as chart question answering,
chart2text, and fact-checking, have emerged. Large Vision-Language Models
(LVLMs) show promise in tackling these tasks, but their evaluation is costly
and time-consuming, limiting real-world deployment. While using LVLMs as judges
to assess the chart comprehension capabilities of other LVLMs could streamline
evaluation processes, challenges like proprietary datasets, restricted access
to powerful models, and evaluation costs hinder their adoption in industrial
settings. To this end, we present a comprehensive evaluation of 13 open-source
LVLMs as judges for diverse chart comprehension and reasoning tasks. We design
both pairwise and pointwise evaluation tasks covering criteria like factual
correctness, informativeness, and relevancy. Additionally, we analyze LVLM
judges based on format adherence, positional consistency, length bias, and
instruction-following. We focus on cost-effective LVLMs (<10B parameters)
suitable for both research and commercial use, following a standardized
evaluation protocol and rubric to measure the LVLM judge's accuracy.
Experimental results reveal notable variability: while some open LVLM judges
achieve GPT-4-level evaluation performance (about 80% agreement with GPT-4
judgments), others struggle (below ~10% agreement). Our findings highlight that
state-of-the-art open-source LVLMs can serve as cost-effective automatic
evaluators for chart-related tasks, though biases such as positional preference
and length bias persist.

</details>


### [53] [LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models](https://arxiv.org/abs/2505.08498)
*Takumi Shibata,Yuichi Miyamura*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的比较性论文评分方法（LCES），通过成对比较任务提高零样本自动评分的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本方法直接生成绝对分数，易受模型偏见和评分不一致影响，导致与人工评分偏差较大。

Method: 将论文评分任务转化为成对比较，利用LLM判断两篇论文的优劣，再通过RankNet将比较结果转换为连续分数。

Result: 实验表明，LCES在准确性和计算效率上优于传统零样本方法，且对不同LLM主干具有鲁棒性。

Conclusion: LCES为现实中的零样本自动论文评分提供了一种高效且可靠的解决方案。

Abstract: Recent advances in large language models (LLMs) have enabled zero-shot
automated essay scoring (AES), providing a promising way to reduce the cost and
effort of essay scoring in comparison with manual grading. However, most
existing zero-shot approaches rely on LLMs to directly generate absolute
scores, which often diverge from human evaluations owing to model biases and
inconsistent scoring. To address these limitations, we propose LLM-based
Comparative Essay Scoring (LCES), a method that formulates AES as a pairwise
comparison task. Specifically, we instruct LLMs to judge which of two essays is
better, collect many such comparisons, and convert them into continuous scores.
Considering that the number of possible comparisons grows quadratically with
the number of essays, we improve scalability by employing RankNet to
efficiently transform LLM preferences into scalar scores. Experiments using AES
benchmark datasets show that LCES outperforms conventional zero-shot methods in
accuracy while maintaining computational efficiency. Moreover, LCES is robust
across different LLM backbones, highlighting its applicability to real-world
zero-shot AES.

</details>


### [54] [Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding](https://arxiv.org/abs/2505.08504)
*Jeongwoo Kang,Maximin Coavoux,Cédric Lopez,Didier Schwab*

Main category: cs.CL

TL;DR: 论文提出了一种基于三元组的线性化方法，以解决Penman编码在处理深层图和节点重入时的局限性，并比较了其效率。


<details>
  <summary>Details</summary>
Motivation: Penman编码在深层图中可能导致相关节点在文本中距离过远，且需要逆角色处理节点重入，增加了预测关系类型的数量。

Method: 提出了一种基于三元组的线性化方法，并与Penman编码进行效率比较。

Result: 三元组编码在表示图结构时表现良好，但仍需改进以与Penman的简洁嵌套表示竞争。

Conclusion: 三元组编码有潜力，但需进一步优化以更好地表示嵌套图结构。

Abstract: Sequence-to-sequence models are widely used to train Abstract Meaning
Representation (Banarescu et al., 2013, AMR) parsers. To train such models, AMR
graphs have to be linearized into a one-line text format. While Penman encoding
is typically used for this purpose, we argue that it has limitations: (1) for
deep graphs, some closely related nodes are located far apart in the linearized
text (2) Penman's tree-based encoding necessitates inverse roles to handle node
re-entrancy, doubling the number of relation types to predict. To address these
issues, we propose a triple-based linearization method and compare its
efficiency with Penman linearization. Although triples are well suited to
represent a graph, our results suggest room for improvement in triple encoding
to better compete with Penman's concise and explicit representation of a nested
graph structure.

</details>


### [55] [Are We Paying Attention to Her? Investigating Gender Disambiguation and Attention in Machine Translation](https://arxiv.org/abs/2505.08546)
*Chiara Manna,Afra Alishahi,Frédéric Blain,Eva Vanmassenhove*

Main category: cs.CL

TL;DR: 论文提出了一种新的评估指标MPA，用于衡量NMT系统对性别线索的依赖程度，发现现有模型更倾向于忽略性别线索而依赖统计性别刻板印象。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标未能充分捕捉NMT系统对上下文性别线索的整合程度，因此需要更精细的评估方法。

Method: 提出Minimal Pair Accuracy (MPA)指标，通过最小对（仅性别代词不同的句子对）评估模型对性别线索的依赖。

Result: 模型大多忽略性别线索，依赖刻板印象；在反刻板情况下，模型更倾向于考虑男性线索而忽略女性线索。

Conclusion: NMT系统在性别处理上存在偏见，需改进模型设计以减少对刻板印象的依赖。

Abstract: While gender bias in modern Neural Machine Translation (NMT) systems has
received much attention, traditional evaluation metrics do not to fully capture
the extent to which these systems integrate contextual gender cues. We propose
a novel evaluation metric called Minimal Pair Accuracy (MPA), which measures
the reliance of models on gender cues for gender disambiguation. MPA is
designed to go beyond surface-level gender accuracy metrics by focusing on
whether models adapt to gender cues in minimal pairs -- sentence pairs that
differ solely in the gendered pronoun, namely the explicit indicator of the
target's entity gender in the source language (EN). We evaluate a number of NMT
models on the English-Italian (EN--IT) language pair using this metric, we show
that they ignore available gender cues in most cases in favor of (statistical)
stereotypical gender interpretation. We further show that in anti-stereotypical
cases, these models tend to more consistently take masculine gender cues into
account while ignoring the feminine cues. Furthermore, we analyze the attention
head weights in the encoder component and show that while all models encode
gender information to some extent, masculine cues elicit a more diffused
response compared to the more concentrated and specialized responses to
feminine gender cues.

</details>


### [56] [Small but Significant: On the Promise of Small Language Models for Accessible AIED](https://arxiv.org/abs/2505.08588)
*Yumou Wei,Paulo Carvalho,John Stamper*

Main category: cs.CL

TL;DR: 论文指出，尽管GPT等大型语言模型（LLMs）在AI教育领域（AIED）中占据主导地位，但小型语言模型（SLMs）如Phi-2在资源受限的教育机构中具有潜力，且无需复杂提示策略即可解决关键教育挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在AIED中的主导地位可能忽视了SLMs的潜力，尤其是在资源受限的机构中。

Method: 通过关键词搜索分析AIED 2024论文，并展示SLMs（如Phi-2）在知识组件（KC）发现中的有效性。

Result: SLMs无需复杂提示即可有效解决教育挑战，为资源受限机构提供高质量AI工具。

Conclusion: 呼吁更多关注SLM-based AIED方法的发展。

Abstract: GPT has become nearly synonymous with large language models (LLMs), an
increasingly popular term in AIED proceedings. A simple keyword-based search
reveals that 61% of the 76 long and short papers presented at AIED 2024
describe novel solutions using LLMs to address some of the long-standing
challenges in education, and 43% specifically mention GPT. Although LLMs
pioneered by GPT create exciting opportunities to strengthen the impact of AI
on education, we argue that the field's predominant focus on GPT and other
resource-intensive LLMs (with more than 10B parameters) risks neglecting the
potential impact that small language models (SLMs) can make in providing
resource-constrained institutions with equitable and affordable access to
high-quality AI tools. Supported by positive results on knowledge component
(KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as
Phi-2 can produce an effective solution without elaborate prompting strategies.
Hence, we call for more attention to developing SLM-based AIED approaches.

</details>


### [57] [Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and Pa-thology Foundation Models](https://arxiv.org/abs/2505.08590)
*Hussien Al-Asi,Jordan P Reynolds,Shweta Agarwal,Bryan J Dangott,Aziza Nassar,Zeynettin Akkus*

Main category: cs.CL

TL;DR: 该研究结合检索增强生成（RAG）和病理学基础模型，提升甲状腺细胞学诊断的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决细胞学解释、标准化和诊断准确性方面的挑战。

Method: 利用RAG增强的大型语言模型（LLMs）和病理学基础模型，结合动态检索和图像特征提取。

Result: 显著提升诊断效率和一致性，基础模型UNI的AUC达到0.73-0.93。

Conclusion: AI辅助甲状腺细胞病理学具有潜力，为未来应用铺平道路。

Abstract: Advancements in artificial intelligence (AI) are transforming pathology by
integrat-ing large language models (LLMs) with retrieval-augmented generation
(RAG) and domain-specific foundation models. This study explores the
application of RAG-enhanced LLMs coupled with pathology foundation models for
thyroid cytology diagnosis, addressing challenges in cytological
interpretation, standardization, and diagnostic accuracy. By leveraging a
curated knowledge base, RAG facilitates dy-namic retrieval of relevant case
studies, diagnostic criteria, and expert interpreta-tion, improving the
contextual understanding of LLMs. Meanwhile, pathology foun-dation models,
trained on high-resolution pathology images, refine feature extrac-tion and
classification capabilities. The fusion of these AI-driven approaches en-hances
diagnostic consistency, reduces variability, and supports pathologists in
dis-tinguishing benign from malignant thyroid lesions. Our results demonstrate
that integrating RAG with pathology-specific LLMs significantly improves
diagnostic efficiency and interpretability, paving the way for AI-assisted
thyroid cytopathology, with foundation model UNI achieving AUC 0.73-0.93 for
correct prediction of surgi-cal pathology diagnosis from thyroid cytology
samples.

</details>


### [58] [Automatic Task Detection and Heterogeneous LLM Speculative Decoding](https://arxiv.org/abs/2505.08600)
*Danying Ge,Jianhua Gao,Qizhi Jiang,Yifei Feng,Weixing Ji*

Main category: cs.CL

TL;DR: 提出了一种针对下游任务优化的推测解码算法，通过任务分区和异构草稿模型分配，提升解码速度和接受率。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法在下游任务中面临解码速度和接受率的权衡，草稿模型能力有限，难以保证效率。

Method: 自动任务分区和分配方法，将任务分类并分配给异构草稿模型，结合在线轻量级提示分类器动态路由。

Result: 草稿准确率提升6%至50%，LLM推理速度提升1.10x至2.64x。

Conclusion: 该方法有效提升下游任务的解码效率和一致性。

Abstract: Speculative decoding, which combines a draft model with a target model, has
emerged as an effective approach to accelerate large language model (LLM)
inference. However, existing methods often face a trade-off between the
acceptance rate and decoding speed in downstream tasks due to the limited
capacity of the draft model, making it difficult to ensure efficiency across
diverse tasks. To address this problem, we propose a speculative decoding
algorithm tailored for downstream task optimization. It includes an automatic
task partitioning and assigning method, which automatically categorizes
downstream tasks into different sub-tasks and assigns them to a set of
heterogeneous draft models. Each draft model is aligned with the target model
using task-specific data, thereby enhancing the consistency of inference
results. In addition, our proposed method incorporates an online lightweight
prompt classifier to dynamically route prompts to the appropriate draft model.
Experimental results demonstrate that the proposed method improves draft
accuracy by 6% to 50% over vanilla speculative decoding, while achieving a
speedup of 1.10x to 2.64x in LLM inference.

</details>


### [59] [Scaling Context, Not Parameters: Training a Compact 7B Language Model for Efficient Long-Context Processing](https://arxiv.org/abs/2505.08651)
*Chen Wu,Yin Song*

Main category: cs.CL

TL;DR: MegaBeam-Mistral-7B是一个支持512K标记上下文长度的语言模型，解决了长上下文训练的实际限制，并在多个长上下文基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文训练的实际限制，支持合规监控和验证等现实任务。

Method: 开发了一个7B参数的语言模型，支持512K标记的上下文长度。

Result: 在HELMET和RULER基准测试中表现出色，是唯一在BABILong上无需RAG或针对性微调即可实现竞争性长程推理的开放模型。

Conclusion: 该模型已作为开源项目发布，下载量超过10万次，具有广泛的应用潜力。

Abstract: We present MegaBeam-Mistral-7B, a language model that supports 512K-token
context length. Our work addresses practical limitations in long-context
training, supporting real-world tasks such as compliance monitoring and
verification. Evaluated on three long-context benchmarks, our 7B-parameter
model demonstrates superior in-context learning performance on HELMET and
robust retrieval and tracing capability on RULER. It is currently the only open
model to achieve competitive long-range reasoning on BABILong at 512K context
length without RAG or targeted fine-tuning. Released as fully open source under
the Apache 2.0 license, the model has been downloaded over 100,000 times on
Hugging Face. Model available at:
https://huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k

</details>


### [60] [Revealing economic facts: LLMs know more than they say](https://arxiv.org/abs/2505.08662)
*Marcus Buckmann,Quynh Anh Nguyen,Edward Hill*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLM）的隐藏状态可用于估计和填补经济与金融统计数据，且效果优于模型的文本输出。


<details>
  <summary>Details</summary>
Motivation: 探索LLM隐藏状态是否包含比直接文本输出更丰富的经济信息。

Method: 使用简单线性模型训练开源LLM的隐藏状态，并提出无需目标变量标签数据的迁移学习方法。

Result: 隐藏状态能有效估计经济和金融数据，仅需少量标注样本即可训练，迁移学习显著提升准确性。

Conclusion: LLM隐藏状态在经济数据估计和填补任务中具有实用价值。

Abstract: We investigate whether the hidden states of large language models (LLMs) can
be used to estimate and impute economic and financial statistics. Focusing on
county-level (e.g. unemployment) and firm-level (e.g. total assets) variables,
we show that a simple linear model trained on the hidden states of open-source
LLMs outperforms the models' text outputs. This suggests that hidden states
capture richer economic information than the responses of the LLMs reveal
directly. A learning curve analysis indicates that only a few dozen labelled
examples are sufficient for training. We also propose a transfer learning
method that improves estimation accuracy without requiring any labelled data
for the target variable. Finally, we demonstrate the practical utility of
hidden-state representations in super-resolution and data imputation tasks.

</details>


### [61] [Adaptive Schema-aware Event Extraction with Retrieval-Augmented Generation](https://arxiv.org/abs/2505.08690)
*Sheng Liang,Hang Lv,Zhihao Wen,Yaxiong Wu,Yongyue Zhang,Hao Wang,Yong Liu*

Main category: cs.CL

TL;DR: 论文提出了一种自适应模式感知事件抽取（ASEE）方法，结合模式改写和检索增强生成，解决了现有事件抽取中的模式固定和评测基准缺失问题，并在多领域数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有事件抽取方法存在模式固定和缺乏联合模式匹配与抽取评测基准的问题，大语言模型虽有潜力但存在模式幻觉和上下文窗口限制。

Method: 提出ASEE方法，结合模式改写和检索增强生成，构建MD-SEE评测基准整合多领域数据集。

Result: ASEE在多领域数据集上表现出强适应性，显著提升了事件抽取的准确性。

Conclusion: ASEE为事件抽取提供了一种灵活且高效的解决方案，MD-SEE基准为未来研究提供了评测基础。

Abstract: Event extraction (EE) is a fundamental task in natural language processing
(NLP) that involves identifying and extracting event information from
unstructured text. Effective EE in real-world scenarios requires two key steps:
selecting appropriate schemas from hundreds of candidates and executing the
extraction process. Existing research exhibits two critical gaps: (1) the rigid
schema fixation in existing pipeline systems, and (2) the absence of benchmarks
for evaluating joint schema matching and extraction. Although large language
models (LLMs) offer potential solutions, their schema hallucination tendencies
and context window limitations pose challenges for practical deployment. In
response, we propose Adaptive Schema-aware Event Extraction (ASEE), a novel
paradigm combining schema paraphrasing with schema retrieval-augmented
generation. ASEE adeptly retrieves paraphrased schemas and accurately generates
targeted structures. To facilitate rigorous evaluation, we construct the
Multi-Dimensional Schema-aware Event Extraction (MD-SEE) benchmark, which
systematically consolidates 12 datasets across diverse domains, complexity
levels, and language settings. Extensive evaluations on MD-SEE show that our
proposed ASEE demonstrates strong adaptability across various scenarios,
significantly improving the accuracy of event extraction.

</details>


### [62] [NurValues: Real-World Nursing Values Evaluation for Large Language Models in Clinical Context](https://arxiv.org/abs/2505.08734)
*Ben Yao,Qiuchi Li,Yazhou Zhang,Siyu Yang,Bohan Zhang,Prayag Tiwari,Jing Qin*

Main category: cs.CL

TL;DR: 论文提出了首个护理价值对齐基准，包含五个核心价值维度，并通过真实护理行为实例和对抗性数据集评估了23种先进LLM的表现。


<details>
  <summary>Details</summary>
Motivation: 为临床环境中开发价值敏感的LLM提供基础，填补护理价值对齐研究的空白。

Method: 通过五个月实地研究收集1,100个护理行为实例，标注后生成对抗性数据集（Easy-Level和Hard-Level），并评估23种LLM。

Result: DeepSeek-V3在Easy-Level表现最佳（94.55），Claude 3.5 Sonnet在Hard-Level领先（89.43）；Justice是最难评估的价值维度；上下文学习显著提升对齐性。

Conclusion: 该研究为临床环境中价值敏感LLM的开发提供了重要基准和数据集。

Abstract: This work introduces the first benchmark for nursing value alignment,
consisting of five core value dimensions distilled from international nursing
codes: Altruism, Human Dignity, Integrity, Justice, and Professionalism. The
benchmark comprises 1,100 real-world nursing behavior instances collected
through a five-month longitudinal field study across three hospitals of varying
tiers. These instances are annotated by five clinical nurses and then augmented
with LLM-generated counterfactuals with reversed ethic polarity. Each original
case is paired with a value-aligned and a value-violating version, resulting in
2,200 labeled instances that constitute the Easy-Level dataset. To increase
adversarial complexity, each instance is further transformed into a
dialogue-based format that embeds contextual cues and subtle misleading
signals, yielding a Hard-Level dataset. We evaluate 23 state-of-the-art (SoTA)
LLMs on their alignment with nursing values. Our findings reveal three key
insights: (1) DeepSeek-V3 achieves the highest performance on the Easy-Level
dataset (94.55), where Claude 3.5 Sonnet outperforms other models on the
Hard-Level dataset (89.43), significantly surpassing the medical LLMs; (2)
Justice is consistently the most difficult nursing value dimension to evaluate;
and (3) in-context learning significantly improves alignment. This work aims to
provide a foundation for value-sensitive LLMs development in clinical settings.
The dataset and the code are available at
https://huggingface.co/datasets/Ben012345/NurValues.

</details>


### [63] [Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies](https://arxiv.org/abs/2505.08739)
*Xiaoliang Luo,Xinyi Xu,Michael Ramscar,Bradley C. Love*

Main category: cs.CL

TL;DR: 论文证明了自回归大语言模型（LLM）在不同分词顺序下学习一致概率分布的可能性，并提出评估协议，揭示先前研究的缺陷。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在不同分词顺序下是否能学习一致概率分布，为理解其学习机制提供理论基础。

Method: 理论证明序列困惑度在任意分解下不变，并重新训练GPT-2模型以验证顺序效应。

Result: 发现模型在不同顺序下存在系统性偏差，尤其是随机排列顺序。偏差源于自注意力机制的位置和局部性偏好。

Conclusion: 研究为理解LLM的位置偏差提供了新视角，并提出了检测不一致概率分布的方法。

Abstract: Can autoregressive large language models (LLMs) learn consistent probability
distributions when trained on sequences in different token orders? We prove
formally that for any well-defined probability distribution, sequence
perplexity is invariant under any factorization, including forward, backward,
or arbitrary permutations. This result establishes a rigorous theoretical
foundation for studying how LLMs learn from data and defines principled
protocols for empirical evaluation. Applying these protocols, we show that
prior studies examining ordering effects suffer from critical methodological
flaws. We retrain GPT-2 models across forward, backward, and arbitrary permuted
orders on scientific text. We find systematic deviations from theoretical
invariance across all orderings with arbitrary permutations strongly deviating
from both forward and backward models, which largely (but not completely)
agreed with one another. Deviations were traceable to differences in
self-attention, reflecting positional and locality biases in processing. Our
theoretical and empirical results provide novel avenues for understanding
positional biases in LLMs and suggest methods for detecting when LLMs'
probability distributions are inconsistent and therefore untrustworthy.

</details>


### [64] [AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large Language Models](https://arxiv.org/abs/2505.08750)
*Yanxi Zhang,Xin Cong,Zhong Zhang,Xiao Liu,Dongyan Zhao,Yesai Wu*

Main category: cs.CL

TL;DR: 论文提出AC-Reason框架，结合形式因果理论提升LLM在实际因果推理中的表现，并引入AC-Bench基准验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法缺乏形式因果理论支持，导致解释性不足，需改进实际因果推理能力。

Method: 提出AC-Reason框架，通过半形式推理识别因果事件，推断形式因果因素，并基于理论算法回答因果查询。

Result: AC-Reason显著提升LLM性能，在BBH-CJ和AC-Bench上表现优于基线，GPT-4 + AC-Reason分别达到75.04%和71.82%准确率。

Conclusion: 结合形式因果理论显著提升LLM因果推理能力，AC-Bench为未来研究提供新基准。

Abstract: Actual causality (AC), a fundamental aspect of causal reasoning (CR), is
responsible for attribution and responsibility assignment in real-world
scenarios. However, existing LLM-based methods lack grounding in formal AC
theory, resulting in limited interpretability. Therefore, we propose AC-Reason,
a semi-formal reasoning framework that identifies causally relevant events
within an AC scenario, infers the values of their formal causal factors (e.g.,
sufficiency, necessity, and normality), and answers AC queries via a
theory-guided algorithm with explanations. While AC-Reason does not explicitly
construct a causal graph, it operates over variables in the underlying causal
structure to support principled reasoning. To enable comprehensive evaluation,
we introduce AC-Bench, a new benchmark built upon and substantially extending
Big-Bench Hard Causal Judgment (BBH-CJ). AC-Bench comprises ~1K carefully
annotated samples, each with detailed reasoning steps and focuses solely on
actual causation. The case study shows that synthesized samples in AC-Bench
present greater challenges for LLMs. Extensive experiments on BBH-CJ and
AC-Bench show that AC-Reason consistently improves LLM performance over
baselines. On BBH-CJ, all tested LLMs surpass the average human rater accuracy
of 69.60%, with GPT-4 + AC-Reason achieving 75.04%. On AC-Bench, GPT-4 +
AC-Reason again achieves the highest accuracy of 71.82%. AC-Bench further
enables fine-grained analysis of reasoning faithfulness, revealing that only
Qwen-2.5-72B-Instruct, Claude-3.5-Sonnet, and GPT-4o exhibit faithful
reasoning, whereas GPT-4 tends to exploit shortcuts. Finally, our ablation
study proves that integrating AC theory into LLMs is highly effective, with the
proposed algorithm contributing the most significant performance gains.

</details>


### [65] [Aya Vision: Advancing the Frontier of Multilingual Multimodality](https://arxiv.org/abs/2505.08751)
*Saurabh Dash,Yiyang Nan,John Dang,Arash Ahmadian,Shivalika Singh,Madeline Smith,Bharat Venkitesh,Vlad Shmyhlo,Viraat Aryabumi,Walter Beller-Morales,Jeremy Pekmez,Jason Ozuzu,Pierre Richemond,Acyr Locatelli,Nick Frosst,Phil Blunsom,Aidan Gomez,Ivan Zhang,Marzieh Fadaee,Manoj Govindassamy,Sudip Roy,Matthias Gallé,Beyza Ermis,Ahmet Üstün,Sara Hooker*

Main category: cs.CL

TL;DR: 论文提出了一种解决多语言多模态模型构建挑战的新方法，包括高质量多语言指令数据的合成标注框架和跨模态模型合并技术，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 构建多语言多模态模型面临数据稀缺、模态对齐和灾难性遗忘等挑战，需要新的技术来解决这些问题。

Method: 开发了合成标注框架以生成高质量多语言多模态指令数据，并提出了跨模态模型合并技术以减少灾难性遗忘。

Result: Aya-Vision-8B和Aya-Vision-32B在性能上超越了同类模型，甚至优于更大的模型。

Conclusion: 该研究推动了多语言多模态领域的发展，提供了高效计算和高性能的技术方案。

Abstract: Building multimodal language models is fundamentally challenging: it requires
aligning vision and language modalities, curating high-quality instruction
data, and avoiding the degradation of existing text-only capabilities once
vision is introduced. These difficulties are further magnified in the
multilingual setting, where the need for multimodal data in different languages
exacerbates existing data scarcity, machine translation often distorts meaning,
and catastrophic forgetting is more pronounced. To address the aforementioned
challenges, we introduce novel techniques spanning both data and modeling.
First, we develop a synthetic annotation framework that curates high-quality,
diverse multilingual multimodal instruction data, enabling Aya Vision models to
produce natural, human-preferred responses to multimodal inputs across many
languages. Complementing this, we propose a cross-modal model merging technique
that mitigates catastrophic forgetting, effectively preserving text-only
capabilities while simultaneously enhancing multimodal generative performance.
Aya-Vision-8B achieves best-in-class performance compared to strong multimodal
models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger
Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which
outperforms models more than twice its size, such as Molmo-72B and
LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the
multi-modal frontier, and provides insights into techniques that effectively
bend the need for compute while delivering extremely high performance.

</details>


### [66] [HealthBench: Evaluating Large Language Models Towards Improved Human Health](https://arxiv.org/abs/2505.08775)
*Rahul K. Arora,Jason Wei,Rebecca Soskin Hicks,Preston Bowman,Joaquin Quiñonero-Candela,Foivos Tsimpourlas,Michael Sharman,Meghan Shah,Andrea Vallone,Alex Beutel,Johannes Heidecke,Karan Singhal*

Main category: cs.CL

TL;DR: HealthBench是一个开源基准测试，用于评估大型语言模型在医疗领域的性能和安全性，包含5000次多轮对话和48562条独特评分标准，覆盖多种健康场景和行为维度。


<details>
  <summary>Details</summary>
Motivation: 为医疗领域提供更真实、开放的评估工具，推动语言模型在健康领域的应用和发展。

Method: 通过多轮对话和医生制定的评分标准，评估模型在多种健康场景和行为维度上的表现。

Result: 模型性能逐年提升，GPT-4.1 nano表现优于GPT-4o且成本更低；还发布了HealthBench Consensus和HealthBench Hard两个变体。

Conclusion: HealthBench为模型开发和健康应用提供了重要基准，有望推动技术进步和人类健康受益。

Abstract: We present HealthBench, an open-source benchmark measuring the performance
and safety of large language models in healthcare. HealthBench consists of
5,000 multi-turn conversations between a model and an individual user or
healthcare professional. Responses are evaluated using conversation-specific
rubrics created by 262 physicians. Unlike previous multiple-choice or
short-answer benchmarks, HealthBench enables realistic, open-ended evaluation
through 48,562 unique rubric criteria spanning several health contexts (e.g.,
emergencies, transforming clinical data, global health) and behavioral
dimensions (e.g., accuracy, instruction following, communication). HealthBench
performance over the last two years reflects steady initial progress (compare
GPT-3.5 Turbo's 16% to GPT-4o's 32%) and more rapid recent improvements (o3
scores 60%). Smaller models have especially improved: GPT-4.1 nano outperforms
GPT-4o and is 25 times cheaper. We additionally release two HealthBench
variations: HealthBench Consensus, which includes 34 particularly important
dimensions of model behavior validated via physician consensus, and HealthBench
Hard, where the current top score is 32%. We hope that HealthBench grounds
progress towards model development and applications that benefit human health.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [67] [MilChat: Introducing Chain of Thought Reasoning and GRPO to a Multimodal Small Language Model for Remote Sensing](https://arxiv.org/abs/2505.07984)
*Aybora Koksal,A. Aydin Alatan*

Main category: cs.CV

TL;DR: MilChat是一种轻量级多模态语言模型，专为分析偏远地区的遥感图像（如导弹发射场）而设计，通过专家验证的数据集和强化学习优化，显著优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在专业领域（如军事遥感）的适应性和效率不足，需要针对性优化。

Method: 使用2B参数的开源MLLM进行监督微调，结合CoT推理和GRPO强化学习，提升对军事关键特征的检测能力。

Result: 在MilData基准测试中达到80%召回率和98%精确度，优于通用模型和现有遥感方法。

Conclusion: 针对性的微调和强化学习可显著提升MLLMs在专业领域的表现。

Abstract: Remarkable capabilities in understanding and generating text-image content
have been demonstrated by recent advancements in multimodal large language
models (MLLMs). However, their effectiveness in specialized
domains-particularly those requiring resource-efficient and domain-specific
adaptations-has remained limited. In this work, a lightweight multimodal
language model termed MilChat is introduced, specifically adapted to analyze
remote sensing imagery in secluded areas, including challenging missile launch
sites. A new dataset, MilData, was compiled by verifying hundreds of aerial
images through expert review, and subtle military installations were
highlighted via detailed captions. Supervised fine-tuning on a 2B-parameter
open-source MLLM with chain-of-thought (CoT) reasoning annotations was
performed, enabling more accurate and interpretable explanations. Additionally,
Group Relative Policy Optimization (GRPO) was leveraged to enhance the model's
ability to detect critical domain-specific cues-such as defensive layouts and
key military structures-while minimizing false positives on civilian scenes.
Through empirical evaluations, it has been shown that MilChat significantly
outperforms both larger, general-purpose multimodal models and existing remote
sensing-adapted approaches on open-ended captioning and classification metrics.
Over 80% recall and 98% precision were achieved on the newly proposed MilData
benchmark, underscoring the potency of targeted fine-tuning and reinforcement
learning in specialized real-world applications.

</details>


### [68] [Vision Foundation Model Embedding-Based Semantic Anomaly Detection](https://arxiv.org/abs/2505.07998)
*Max Peter Ronecker,Matthew Foutter,Amine Elhafsi,Daniele Gammelli,Ihor Barakaiev,Marco Pavone,Daniel Watzenig*

Main category: cs.CV

TL;DR: 该论文提出了一种基于视觉基础模型的语义异常检测框架，通过比较运行时图像的局部视觉嵌入与安全场景数据库，实现高效异常检测。


<details>
  <summary>Details</summary>
Motivation: 语义异常可能导致自主系统推理失败，因此需要一种有效的检测方法。

Method: 提出两种框架变体：基于原始网格嵌入和基于实例分割的对象中心表示，并引入过滤机制减少误报。

Result: 在CARLA模拟异常中，基于实例的方法性能接近GPT-4o，并能精确定位异常。

Conclusion: 视觉基础模型的嵌入在实时异常检测中具有潜在实用性。

Abstract: Semantic anomalies are contextually invalid or unusual combinations of
familiar visual elements that can cause undefined behavior and failures in
system-level reasoning for autonomous systems. This work explores semantic
anomaly detection by leveraging the semantic priors of state-of-the-art vision
foundation models, operating directly on the image. We propose a framework that
compares local vision embeddings from runtime images to a database of nominal
scenarios in which the autonomous system is deemed safe and performant. In this
work, we consider two variants of the proposed framework: one using raw
grid-based embeddings, and another leveraging instance segmentation for
object-centric representations. To further improve robustness, we introduce a
simple filtering mechanism to suppress false positives. Our evaluations on
CARLA-simulated anomalies show that the instance-based method with filtering
achieves performance comparable to GPT-4o, while providing precise anomaly
localization. These results highlight the potential utility of vision
embeddings from foundation models for real-time anomaly detection in autonomous
systems.

</details>


### [69] [RDD: Robust Feature Detector and Descriptor using Deformable Transformer](https://arxiv.org/abs/2505.08013)
*Gonglin Chen,Tianwen Fu,Haiwei Chen,Wenbin Teng,Hanyuan Xiao,Yajie Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于可变形Transformer的鲁棒关键点检测器/描述符（RDD），通过可变形自注意力机制捕捉全局上下文和几何不变性，显著提升了在稀疏匹配任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 在结构从运动和SLAM中，面对显著视角变化等挑战场景下的鲁棒特征检测和描述问题尚未解决。现有方法未能有效学习长距离关系中的视觉线索。

Method: 利用可变形Transformer设计RDD，通过可变形自注意力机制聚焦关键位置，降低搜索空间复杂度并建模几何不变性。结合Air-to-Ground和MegaDepth数据集进行训练。

Result: RDD在稀疏匹配任务中优于所有现有方法，并能进行半稠密匹配。同时引入了两个新基准测试以全面评估性能。

Conclusion: RDD通过全局上下文和几何不变性建模，显著提升了关键点检测和描述的性能，适用于复杂场景。

Abstract: As a core step in structure-from-motion and SLAM, robust feature detection
and description under challenging scenarios such as significant viewpoint
changes remain unresolved despite their ubiquity. While recent works have
identified the importance of local features in modeling geometric
transformations, these methods fail to learn the visual cues present in
long-range relationships. We present Robust Deformable Detector (RDD), a novel
and robust keypoint detector/descriptor leveraging the deformable transformer,
which captures global context and geometric invariance through deformable
self-attention mechanisms. Specifically, we observed that deformable attention
focuses on key locations, effectively reducing the search space complexity and
modeling the geometric invariance. Furthermore, we collected an Air-to-Ground
dataset for training in addition to the standard MegaDepth dataset. Our
proposed method outperforms all state-of-the-art keypoint detection/description
methods in sparse matching tasks and is also capable of semi-dense matching. To
ensure comprehensive evaluation, we introduce two challenging benchmarks: one
emphasizing large viewpoint and scale variations, and the other being an
Air-to-Ground benchmark -- an evaluation setting that has recently gaining
popularity for 3D reconstruction across different altitudes.

</details>


### [70] [Visually Interpretable Subtask Reasoning for Visual Question Answering](https://arxiv.org/abs/2505.08084)
*Yu Cheng,Arushi Goel,Hakan Bilen*

Main category: cs.CV

TL;DR: VISTAR是一个用于多模态大语言模型（MLLMs）的子任务驱动训练框架，通过生成文本和视觉解释，提升复杂视觉问题解答的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在复杂视觉问题解答中计算成本高、准确性低的问题，同时增强模型的可解释性。

Method: VISTAR通过微调MLLMs生成结构化的子任务思维链（Subtask-of-Thought rationales），无需依赖外部模型。

Result: 在两个基准测试中，VISTAR显著提高了推理准确性，同时保持了可解释性。

Conclusion: VISTAR为复杂视觉问题解答提供了一种高效且可解释的解决方案。

Abstract: Answering complex visual questions like `Which red furniture can be used for
sitting?' requires multi-step reasoning, including object recognition,
attribute filtering, and relational understanding. Recent work improves
interpretability in multimodal large language models (MLLMs) by decomposing
tasks into sub-task programs, but these methods are computationally expensive
and less accurate due to poor adaptation to target data. To address this, we
introduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a
subtask-driven training framework that enhances both interpretability and
reasoning by generating textual and visual explanations within MLLMs. Instead
of relying on external models, VISTAR fine-tunes MLLMs to produce structured
Subtask-of-Thought rationales (step-by-step reasoning sequences). Experiments
on two benchmarks show that VISTAR consistently improves reasoning accuracy
while maintaining interpretability. Our code and dataset will be available at
https://github.com/ChengJade/VISTAR.

</details>


### [71] [Multi-modal wound classification using wound image and location by Xception and Gaussian Mixture Recurrent Neural Network (GMRNN)](https://arxiv.org/abs/2505.08086)
*Ramin Mousa,Ehsan Matbooe,Hakimeh Khojasteh,Amirali Bengari,Mohammadmahdi Vahediahmar*

Main category: cs.CV

TL;DR: 提出了一种基于迁移学习的多模态AI模型，结合Xception和GMRNN架构，用于伤口分类，显著提高了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 急性及难愈合伤口的有效诊断对临床护理至关重要，但现有工具常因感染、血管疾病等因素导致效果不佳。AI技术可加速医学图像解读并改善早期检测。

Method: 通过迁移学习算法提取特征并结合位置特征，构建多模态网络，分类糖尿病、压力、手术和静脉溃疡伤口。与深度神经网络进行对比。

Result: 实验结果显示伤口分类准确率在78.77%至100%之间，证明了该方法的高效性。

Conclusion: 该方法在常见伤口类型的分类中表现出卓越的准确性，为临床诊断提供了有力工具。

Abstract: The effective diagnosis of acute and hard-to-heal wounds is crucial for wound
care practitioners to provide effective patient care. Poor clinical outcomes
are often linked to infection, peripheral vascular disease, and increasing
wound depth, which collectively exacerbate these comorbidities. However,
diagnostic tools based on Artificial Intelligence (AI) speed up the
interpretation of medical images and improve early detection of disease. In
this article, we propose a multi-modal AI model based on transfer learning
(TL), which combines two state-of-the-art architectures, Xception and GMRNN,
for wound classification. The multi-modal network is developed by concatenating
the features extracted by a transfer learning algorithm and location features
to classify the wound types of diabetic, pressure, surgical, and venous ulcers.
The proposed method is comprehensively compared with deep neural networks (DNN)
for medical image analysis. The experimental results demonstrate a notable
wound-class classifications (containing only diabetic, pressure, surgical, and
venous) vary from 78.77 to 100\% in various experiments. The results presented
in this study showcase the exceptional accuracy of the proposed methodology in
accurately classifying the most commonly occurring wound types using wound
images and their corresponding locations.

</details>


### [72] [Topology-Guided Knowledge Distillation for Efficient Point Cloud Processing](https://arxiv.org/abs/2505.08101)
*Luu Tung Hai,Thinh D. Le,Zhicheng Ding,Qing Tian,Truong-Son Hy*

Main category: cs.CV

TL;DR: 提出了一种新的点云知识蒸馏框架，通过拓扑感知表示和梯度引导蒸馏，将高性能教师模型的知识高效迁移到轻量级学生模型，显著减小模型规模和推理时间。


<details>
  <summary>Details</summary>
Motivation: 点云处理在自动驾驶等领域至关重要，但高性能模型在资源受限环境中部署困难，需解决计算和内存需求高的问题。

Method: 采用拓扑感知表示和梯度引导知识蒸馏，捕捉点云的几何结构并通过梯度特征对齐指导学生模型学习。

Result: 在NuScenes等数据集上表现优异，模型规模减少约16倍，推理时间降低1.9倍，并在LiDAR数据上的分割性能超越现有蒸馏方法。

Conclusion: 该方法为资源受限环境下的点云处理提供了高效解决方案，实现了性能与效率的平衡。

Abstract: Point cloud processing has gained significant attention due to its critical
role in applications such as autonomous driving and 3D object recognition.
However, deploying high-performance models like Point Transformer V3 in
resource-constrained environments remains challenging due to their high
computational and memory demands. This work introduces a novel distillation
framework that leverages topology-aware representations and gradient-guided
knowledge distillation to effectively transfer knowledge from a high-capacity
teacher to a lightweight student model. Our approach captures the underlying
geometric structures of point clouds while selectively guiding the student
model's learning process through gradient-based feature alignment. Experimental
results in the Nuscenes, SemanticKITTI, and Waymo datasets demonstrate that the
proposed method achieves competitive performance, with an approximately 16x
reduction in model size and a nearly 1.9x decrease in inference time compared
to its teacher model. Notably, on NuScenes, our method achieves
state-of-the-art performance among knowledge distillation techniques trained
solely on LiDAR data, surpassing prior knowledge distillation baselines in
segmentation performance. Our implementation is available publicly at:
  https://github.com/HySonLab/PointDistill

</details>


### [73] [Sleep Position Classification using Transfer Learning for Bed-based Pressure Sensors](https://arxiv.org/abs/2505.08111)
*Olivier Papillon,Rafik Goubran,James Green,Julien Larivière-Chartier,Caitlin Higginson,Frank Knoefel,Rébecca Robillard*

Main category: cs.CV

TL;DR: 利用预训练的Vision Transformer模型（ViTMAE和ViTPose）进行睡眠姿势分类，解决了低分辨率压力敏感垫数据标注不足的问题，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 睡眠姿势影响睡眠质量和睡眠障碍（如呼吸暂停），但临床环境中标注数据稀缺，需要高效的非侵入式监测方法。

Method: 采用迁移学习，利用预训练的ViTMAE和ViTPose模型，对低分辨率压力敏感垫数据进行睡眠姿势分类。

Result: 在112晚患者数据上验证，性能优于传统机器学习方法（SVM、XGBoost、随机森林）和深度学习模型（TCN）。

Conclusion: 该方法在低分辨率数据下表现良好，有望在临床环境中实际应用。

Abstract: Bed-based pressure-sensitive mats (PSMs) offer a non-intrusive way of
monitoring patients during sleep. We focus on four-way sleep position
classification using data collected from a PSM placed under a mattress in a
sleep clinic. Sleep positions can affect sleep quality and the prevalence of
sleep disorders, such as apnea. Measurements were performed on patients with
suspected sleep disorders referred for assessments at a sleep clinic. Training
deep learning models can be challenging in clinical settings due to the need
for large amounts of labeled data. To overcome the shortage of labeled training
data, we utilize transfer learning to adapt pre-trained deep learning models to
accurately estimate sleep positions from a low-resolution PSM dataset collected
in a polysomnography sleep lab. Our approach leverages Vision Transformer
models pre-trained on ImageNet using masked autoencoding (ViTMAE) and a
pre-trained model for human pose estimation (ViTPose). These approaches
outperform previous work from PSM-based sleep pose classification using deep
learning (TCN) as well as traditional machine learning models (SVM, XGBoost,
Random Forest) that use engineered features. We evaluate the performance of
sleep position classification from 112 nights of patient recordings and
validate it on a higher resolution 13-patient dataset. Despite the challenges
of differentiating between sleep positions from low-resolution PSM data, our
approach shows promise for real-world deployment in clinical settings

</details>


### [74] [Now you see it, Now you don't: Damage Label Agreement in Drone & Satellite Post-Disaster Imagery](https://arxiv.org/abs/2505.08117)
*Thomas Manzini,Priyankari Perali,Jayesh Tripathi,Robin Murphy*

Main category: cs.CV

TL;DR: 该论文通过分析卫星和无人机图像对15,814栋建筑的损坏标签，发现29.02%的标签不一致，且两种来源的分布差异显著，可能对机器学习损坏评估系统的部署带来风险和潜在危害。


<details>
  <summary>Details</summary>
Motivation: 目前尚无研究探讨无人机与卫星图像在建筑损坏评估中的标签一致性，而现有工作因标签标准、建筑位置偏差和数据量不足受限。

Method: 通过使用相同的损坏标签标准和建筑位置，比较三种飓风中的卫星和无人机图像标签，数据量是之前研究的19.05倍。

Result: 卫星标签比无人机标签少报告至少20.43%的损坏（p<1.2x10^-117），且两者分布显著不同（p<5.1x10^-175），表明基于其中一种训练的模型会误判实际状况。

Conclusion: 为避免伦理风险和社会危害，论文提出四项建议以提高CV/ML损坏评估系统的可靠性和透明度。

Abstract: This paper audits damage labels derived from coincident satellite and drone
aerial imagery for 15,814 buildings across Hurricanes Ian, Michael, and Harvey,
finding 29.02% label disagreement and significantly different distributions
between the two sources, which presents risks and potential harms during the
deployment of machine learning damage assessment systems. Currently, there is
no known study of label agreement between drone and satellite imagery for
building damage assessment. The only prior work that could be used to infer if
such imagery-derived labels agree is limited by differing damage label schemas,
misaligned building locations, and low data quantities. This work overcomes
these limitations by comparing damage labels using the same damage label
schemas and building locations from three hurricanes, with the 15,814 buildings
representing 19.05 times more buildings considered than the most relevant prior
work. The analysis finds satellite-derived labels significantly under-report
damage by at least 20.43% compared to drone-derived labels (p<1.2x10^-117), and
satellite- and drone-derived labels represent significantly different
distributions (p<5.1x10^-175). This indicates that computer vision and machine
learning (CV/ML) models trained on at least one of these distributions will
misrepresent actual conditions, as the differing satellite and drone-derived
distributions cannot simultaneously represent the distribution of actual
conditions in a scene. This potential misrepresentation poses ethical risks and
potential societal harm if not managed. To reduce the risk of future societal
harms, this paper offers four recommendations to improve reliability and
transparency to decisio-makers when deploying CV/ML damage assessment systems
in practice

</details>


### [75] [JSover: Joint Spectrum Estimation and Multi-Material Decomposition from Single-Energy CT Projections](https://arxiv.org/abs/2505.08123)
*Qing Wu,Hongjiang Wei,Jingyi Yu,S. Kevin Zhou,Yuyao Zhang*

Main category: cs.CV

TL;DR: JSover是一种新型的单能CT多材料分解框架，通过联合重建和能量谱估计，显著提高了分解的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统多材料分解方法依赖光谱CT和预测量能谱，临床适用性受限。单能CT分解方法存在两步流程导致的伪影和噪声问题。

Method: 提出JSover框架，一步完成多材料分解和能量谱估计，结合物理先验和隐式神经表示（INR）优化求解。

Result: 实验表明，JSover在模拟和真实CT数据上均优于现有方法，准确性和计算效率更高。

Conclusion: JSover为单能CT多材料分解提供了更可靠和高效的解决方案，具有广泛临床应用潜力。

Abstract: Multi-material decomposition (MMD) enables quantitative reconstruction of
tissue compositions in the human body, supporting a wide range of clinical
applications. However, traditional MMD typically requires spectral CT scanners
and pre-measured X-ray energy spectra, significantly limiting clinical
applicability. To this end, various methods have been developed to perform MMD
using conventional (i.e., single-energy, SE) CT systems, commonly referred to
as SEMMD. Despite promising progress, most SEMMD methods follow a two-step
image decomposition pipeline, which first reconstructs monochromatic CT images
using algorithms such as FBP, and then performs decomposition on these images.
The initial reconstruction step, however, neglects the energy-dependent
attenuation of human tissues, introducing severe nonlinear beam hardening
artifacts and noise into the subsequent decomposition. This paper proposes
JSover, a fundamentally reformulated one-step SEMMD framework that jointly
reconstructs multi-material compositions and estimates the energy spectrum
directly from SECT projections. By explicitly incorporating physics-informed
spectral priors into the SEMMD process, JSover accurately simulates a virtual
spectral CT system from SE acquisitions, thereby improving the reliability and
accuracy of decomposition. Furthermore, we introduce implicit neural
representation (INR) as an unsupervised deep learning solver for representing
the underlying material maps. The inductive bias of INR toward continuous image
patterns constrains the solution space and further enhances estimation quality.
Extensive experiments on both simulated and real CT datasets show that JSover
outperforms state-of-the-art SEMMD methods in accuracy and computational
efficiency.

</details>


### [76] [SLAG: Scalable Language-Augmented Gaussian Splatting](https://arxiv.org/abs/2505.08124)
*Laszlo Szilagyi,Francis Engelmann,Jeannette Bohg*

Main category: cs.CV

TL;DR: SLAG是一个多GPU框架，用于语言增强的高斯散射，提升大规模场景嵌入的速度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决时间敏感和大数据场景下机器人计算资源有限的问题。

Method: 集成2D视觉语言模型特征到3D场景，通过归一化加权平均计算语言嵌入，无需损失函数。

Result: 在16-GPU设置下，嵌入计算速度提升18倍，同时保持嵌入质量。

Conclusion: SLAG为大规模机器人应用提供了高效、可扩展的解决方案。

Abstract: Language-augmented scene representations hold great promise for large-scale
robotics applications such as search-and-rescue, smart cities, and mining. Many
of these scenarios are time-sensitive, requiring rapid scene encoding while
also being data-intensive, necessitating scalable solutions. Deploying these
representations on robots with limited computational resources further adds to
the challenge. To address this, we introduce SLAG, a multi-GPU framework for
language-augmented Gaussian splatting that enhances the speed and scalability
of embedding large scenes. Our method integrates 2D visual-language model
features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG
eliminates the need for a loss function to compute per-Gaussian language
embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters
via a normalized weighted average, enabling highly parallelized scene encoding.
Additionally, we introduce a vector database for efficient embedding storage
and retrieval. Our experiments show that SLAG achieves an 18 times speedup in
embedding computation on a 16-GPU setup compared to OpenGaussian, while
preserving embedding quality on the ScanNet and LERF datasets. For more
details, visit our project website: https://slag-project.github.io/.

</details>


### [77] [Asynchronous Multi-Object Tracking with an Event Camera](https://arxiv.org/abs/2505.08126)
*Angus Apps,Ziwei Wang,Vladimir Perejogin,Timothy Molloy,Robert Mahony*

Main category: cs.CV

TL;DR: AEMOT算法通过异步处理事件相机数据，检测并跟踪多目标，性能优于其他事件算法37%。


<details>
  <summary>Details</summary>
Motivation: 事件相机因其低延迟、高时间分辨率和高动态范围，适合在动态环境中跟踪目标。

Method: AEMOT通过识别光流区域检测特征，使用AEB跟踪器构建候选对象，并通过分类验证阶段筛选对象。

Result: 在Bee Swarm数据集上，AEMOT的精确率和召回率超过其他算法37%。

Conclusion: AEMOT算法高效且性能优越，相关代码和数据集将开源。

Abstract: Events cameras are ideal sensors for enabling robots to detect and track
objects in highly dynamic environments due to their low latency output, high
temporal resolution, and high dynamic range. In this paper, we present the
Asynchronous Event Multi-Object Tracking (AEMOT) algorithm for detecting and
tracking multiple objects by processing individual raw events asynchronously.
AEMOT detects salient event blob features by identifying regions of consistent
optical flow using a novel Field of Active Flow Directions built from the
Surface of Active Events. Detected features are tracked as candidate objects
using the recently proposed Asynchronous Event Blob (AEB) tracker in order to
construct small intensity patches of each candidate object. A novel learnt
validation stage promotes or discards candidate objects based on classification
of their intensity patches, with promoted objects having their position,
velocity, size, and orientation estimated at their event rate. We evaluate
AEMOT on a new Bee Swarm Dataset, where it tracks dozens of small bees with
precision and recall performance exceeding that of alternative event-based
detection and tracking algorithms by over 37%. Source code and the labelled
event Bee Swarm Dataset will be open sourced

</details>


### [78] [MoKD: Multi-Task Optimization for Knowledge Distillation](https://arxiv.org/abs/2505.08170)
*Zeeshan Hayder,Ali Cheraghian,Lars Petersson,Mehrtash Harandi*

Main category: cs.CV

TL;DR: MoKD通过多任务优化解决知识蒸馏中的梯度冲突和梯度主导问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决知识蒸馏中学习目标与教师指导的平衡问题，以及教师与学生模型知识表示的差异。

Method: 将知识蒸馏重新定义为多目标优化问题，并引入子空间学习框架改进知识传递。

Result: 在ImageNet-1K和COCO数据集上表现优异，达到最先进性能。

Conclusion: MoKD在知识蒸馏中实现了高效且高性能的模型训练。

Abstract: Compact models can be effectively trained through Knowledge Distillation
(KD), a technique that transfers knowledge from larger, high-performing teacher
models. Two key challenges in Knowledge Distillation (KD) are: 1) balancing
learning from the teacher's guidance and the task objective, and 2) handling
the disparity in knowledge representation between teacher and student models.
To address these, we propose Multi-Task Optimization for Knowledge Distillation
(MoKD). MoKD tackles two main gradient issues: a) Gradient Conflicts, where
task-specific and distillation gradients are misaligned, and b) Gradient
Dominance, where one objective's gradient dominates, causing imbalance. MoKD
reformulates KD as a multi-objective optimization problem, enabling better
balance between objectives. Additionally, it introduces a subspace learning
framework to project feature representations into a high-dimensional space,
improving knowledge transfer. Our MoKD is demonstrated to outperform existing
methods through extensive experiments on image classification using the
ImageNet-1K dataset and object detection using the COCO dataset, achieving
state-of-the-art performance with greater efficiency. To the best of our
knowledge, MoKD models also achieve state-of-the-art performance compared to
models trained from scratch.

</details>


### [79] [Empowering Vision Transformers with Multi-Scale Causal Intervention for Long-Tailed Image Classification](https://arxiv.org/abs/2505.08173)
*Xiaoshuo Yan,Zhaochuan Li,Lei Meng,Zhuang Qi,Wei Wu,Zixuan Li,Xiangxu Meng*

Main category: cs.CV

TL;DR: TSCNet是一种两阶段因果建模方法，通过多尺度因果干预解决ViT在长尾分类中的性能问题。


<details>
  <summary>Details</summary>
Motivation: 现有因果模型在ViT上表现不佳，因其全局特征表示难以建模细粒度特征与预测的关联。

Method: 提出TSCNet，包括分层因果表示学习（HCRL）和反事实对数偏差校准（CLBC）两阶段。

Result: 在多个长尾基准测试中，TSCNet显著优于现有方法。

Conclusion: TSCNet能有效消除数据不平衡引入的多种偏差。

Abstract: Causal inference has emerged as a promising approach to mitigate long-tail
classification by handling the biases introduced by class imbalance. However,
along with the change of advanced backbone models from Convolutional Neural
Networks (CNNs) to Visual Transformers (ViT), existing causal models may not
achieve an expected performance gain. This paper investigates the influence of
existing causal models on CNNs and ViT variants, highlighting that ViT's global
feature representation makes it hard for causal methods to model associations
between fine-grained features and predictions, which leads to difficulties in
classifying tail classes with similar visual appearance. To address these
issues, this paper proposes TSCNet, a two-stage causal modeling method to
discover fine-grained causal associations through multi-scale causal
interventions. Specifically, in the hierarchical causal representation learning
stage (HCRL), it decouples the background and objects, applying backdoor
interventions at both the patch and feature level to prevent model from using
class-irrelevant areas to infer labels which enhances fine-grained causal
representation. In the counterfactual logits bias calibration stage (CLBC), it
refines the optimization of model's decision boundary by adaptive constructing
counterfactual balanced data distribution to remove the spurious associations
in the logits caused by data distribution. Extensive experiments conducted on
various long-tail benchmarks demonstrate that the proposed TSCNet can eliminate
multiple biases introduced by data imbalance, which outperforms existing
methods.

</details>


### [80] [Monocular Depth Guided Occlusion-Aware Disparity Refinement via Semi-supervised Learning in Laparoscopic Images](https://arxiv.org/abs/2505.08178)
*Ziteng Liu,Dongdong He,Chenghong Zhang,Wenpeng Gao,Yili Fu*

Main category: cs.CV

TL;DR: DGORNet利用单目深度信息和位置嵌入模块改进视差图，结合光流差异损失提升动态手术场景的鲁棒性，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决立体腹腔镜图像视差估计中的遮挡和标记数据稀缺问题。

Method: 提出DGORNet，结合单目深度信息、位置嵌入模块和光流差异损失。

Result: 在SCARED数据集上表现优于现有方法，尤其在遮挡和无纹理区域。

Conclusion: DGORNet有效提升腹腔镜手术的视差估计，解决了数据限制和视差估计的挑战。

Abstract: Occlusion and the scarcity of labeled surgical data are significant
challenges in disparity estimation for stereo laparoscopic images. To address
these issues, this study proposes a Depth Guided Occlusion-Aware Disparity
Refinement Network (DGORNet), which refines disparity maps by leveraging
monocular depth information unaffected by occlusion. A Position Embedding (PE)
module is introduced to provide explicit spatial context, enhancing the
network's ability to localize and refine features. Furthermore, we introduce an
Optical Flow Difference Loss (OFDLoss) for unlabeled data, leveraging temporal
continuity across video frames to improve robustness in dynamic surgical
scenes. Experiments on the SCARED dataset demonstrate that DGORNet outperforms
state-of-the-art methods in terms of End-Point Error (EPE) and Root Mean
Squared Error (RMSE), particularly in occlusion and texture-less regions.
Ablation studies confirm the contributions of the Position Embedding and
Optical Flow Difference Loss, highlighting their roles in improving spatial and
temporal consistency. These results underscore DGORNet's effectiveness in
enhancing disparity estimation for laparoscopic surgery, offering a practical
solution to challenges in disparity estimation and data limitations.

</details>


### [81] [Unsupervised Raindrop Removal from a Single Image using Conditional Diffusion Models](https://arxiv.org/abs/2505.08190)
*Lhuqita Fazry,Valentino Vito*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的单图像雨滴去除新方法。


<details>
  <summary>Details</summary>
Motivation: 单图像雨滴去除任务具有挑战性，现有方法多依赖GAN，而扩散模型在图像修复领域表现优异。

Method: 采用扩散模型进行图像修复，结合雨滴区域检测技术。

Result: 实现了基于扩散模型的雨滴去除，效果优于传统方法。

Conclusion: 扩散模型在雨滴去除任务中具有潜力，为单图像修复提供了新思路。

Abstract: Raindrop removal is a challenging task in image processing. Removing
raindrops while relying solely on a single image further increases the
difficulty of the task. Common approaches include the detection of raindrop
regions in the image, followed by performing a background restoration process
conditioned on those regions. While various methods can be applied for the
detection step, the most common architecture used for background restoration is
the Generative Adversarial Network (GAN). Recent advances in the use of
diffusion models have led to state-of-the-art image inpainting techniques. In
this paper, we introduce a novel technique for raindrop removal from a single
image using diffusion-based image inpainting.

</details>


### [82] [ADC-GS: Anchor-Driven Deformable and Compressed Gaussian Splatting for Dynamic Scene Reconstruction](https://arxiv.org/abs/2505.08196)
*He Huang,Qi Yang,Mufan Liu,Yiling Xu,Zhu Li*

Main category: cs.CV

TL;DR: ADC-GS提出了一种基于锚点的动态场景重建方法，通过分层处理和优化策略显著提升了渲染速度和存储效率。


<details>
  <summary>Details</summary>
Motivation: 现有4D高斯泼溅方法忽略了相邻高斯基元的冗余性，导致性能不佳。

Method: ADC-GS在规范空间中基于锚点组织高斯基元，采用分层粗到细的管道和速率失真优化。

Result: 实验显示ADC-GS渲染速度提升300%-800%，存储效率达到最优。

Conclusion: ADC-GS在动态场景重建中实现了高效且高质量的表示。

Abstract: Existing 4D Gaussian Splatting methods rely on per-Gaussian deformation from
a canonical space to target frames, which overlooks redundancy among adjacent
Gaussian primitives and results in suboptimal performance. To address this
limitation, we propose Anchor-Driven Deformable and Compressed Gaussian
Splatting (ADC-GS), a compact and efficient representation for dynamic scene
reconstruction. Specifically, ADC-GS organizes Gaussian primitives into an
anchor-based structure within the canonical space, enhanced by a temporal
significance-based anchor refinement strategy. To reduce deformation
redundancy, ADC-GS introduces a hierarchical coarse-to-fine pipeline that
captures motions at varying granularities. Moreover, a rate-distortion
optimization is adopted to achieve an optimal balance between bitrate
consumption and representation fidelity. Experimental results demonstrate that
ADC-GS outperforms the per-Gaussian deformation approaches in rendering speed
by 300%-800% while achieving state-of-the-art storage efficiency without
compromising rendering quality. The code is released at
https://github.com/H-Huang774/ADC-GS.git.

</details>


### [83] [Visual Watermarking in the Era of Diffusion Models: Advances and Challenges](https://arxiv.org/abs/2505.08197)
*Junxian Duan,Jiyang Guang,Wenkui Yang,Ran He*

Main category: cs.CV

TL;DR: 论文探讨了在生成式AI（如Stable Diffusion）时代，视觉水印作为版权保护的有效机制，以及扩散模型在提升水印检测精度和鲁棒性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的发展，视觉内容容易被滥用，传统被动检测方法难以应对复杂篡改，因此需要更有效的水印保护技术。

Method: 研究分析了扩散模型在水印技术中的应用，探索其如何通过特征学习嵌入不可察觉且鲁棒的水印。

Result: 扩散模型能够提升水印检测的准确性和鲁棒性，为对抗伪造威胁提供创新解决方案。

Conclusion: 开发基于扩散模型的水印技术对保护数字内容所有权至关重要，尤其是在生成式AI快速发展的背景下。

Abstract: As generative artificial intelligence technologies like Stable Diffusion
advance, visual content becomes more vulnerable to misuse, raising concerns
about copyright infringement. Visual watermarks serve as effective protection
mechanisms, asserting ownership and deterring unauthorized use. Traditional
deepfake detection methods often rely on passive techniques that struggle with
sophisticated manipulations. In contrast, diffusion models enhance detection
accuracy by allowing for the effective learning of features, enabling the
embedding of imperceptible and robust watermarks. We analyze the strengths and
challenges of watermark techniques related to diffusion models, focusing on
their robustness and application in watermark generation. By exploring the
integration of advanced diffusion models and watermarking security, we aim to
advance the discourse on preserving watermark robustness against evolving
forgery threats. It emphasizes the critical importance of developing innovative
solutions to protect digital content and ensure the preservation of ownership
rights in the era of generative AI.

</details>


### [84] [Object detection in adverse weather conditions for autonomous vehicles using Instruct Pix2Pix](https://arxiv.org/abs/2505.08228)
*Unai Gurbindo,Axel Brando,Jaume Abella,Caroline König*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型Instruct Pix2Pix的数据增强方法，用于提升目标检测模型在恶劣天气下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气条件下目标检测系统的鲁棒性对自动驾驶技术发展至关重要。

Method: 利用Instruct Pix2Pix生成真实天气增强数据集，并在CARLA模拟器和真实数据集BDD100K、ACDC上验证。

Result: 实验表明，该方法显著提升了Faster R-CNN和YOLOv10在恶劣天气下的性能。

Conclusion: 研究为提升自动驾驶感知系统的可靠性奠定了基础，并提供了未来发展的方向。

Abstract: Enhancing the robustness of object detection systems under adverse weather
conditions is crucial for the advancement of autonomous driving technology.
This study presents a novel approach leveraging the diffusion model Instruct
Pix2Pix to develop prompting methodologies that generate realistic datasets
with weather-based augmentations aiming to mitigate the impact of adverse
weather on the perception capabilities of state-of-the-art object detection
models, including Faster R-CNN and YOLOv10. Experiments were conducted in two
environments, in the CARLA simulator where an initial evaluation of the
proposed data augmentation was provided, and then on the real-world image data
sets BDD100K and ACDC demonstrating the effectiveness of the approach in real
environments.
  The key contributions of this work are twofold: (1) identifying and
quantifying the performance gap in object detection models under challenging
weather conditions, and (2) demonstrating how tailored data augmentation
strategies can significantly enhance the robustness of these models. This
research establishes a solid foundation for improving the reliability of
perception systems in demanding environmental scenarios, and provides a pathway
for future advancements in autonomous driving.

</details>


### [85] [HMPNet: A Feature Aggregation Architecture for Maritime Object Detection from a Shipborne Perspective](https://arxiv.org/abs/2505.08231)
*Yu Zhang,Fengyuan Liu,Juan Lyu,Yi Wei,Changdong Yu*

Main category: cs.CV

TL;DR: 论文提出了Navigation12数据集和HMPNet模型，用于船舶视角下的目标检测，提升了精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 海上智能导航中目标检测缺乏专用数据集，阻碍了先进视觉技术的应用。

Method: 基于Navigation12数据集，设计了轻量级HMPNet架构，包含动态调制主干、多尺度特征聚合模块和共享权重检测器。

Result: HMPNet在精度和效率上优于现有方法，mAP提升3.3%，参数减少23%。

Conclusion: Navigation12和HMPNet为海上目标检测提供了有效解决方案，具有实际应用潜力。

Abstract: In the realm of intelligent maritime navigation, object detection from a
shipborne perspective is paramount. Despite the criticality, the paucity of
maritime-specific data impedes the deployment of sophisticated visual
perception techniques, akin to those utilized in autonomous vehicular systems,
within the maritime context. To bridge this gap, we introduce Navigation12, a
novel dataset annotated for 12 object categories under diverse maritime
environments and weather conditions. Based upon this dataset, we propose
HMPNet, a lightweight architecture tailored for shipborne object detection.
HMPNet incorporates a hierarchical dynamic modulation backbone to bolster
feature aggregation and expression, complemented by a matrix cascading
poly-scale neck and a polymerization weight sharing detector, facilitating
efficient multi-scale feature aggregation. Empirical evaluations indicate that
HMPNet surpasses current state-of-the-art methods in terms of both accuracy and
computational efficiency, realizing a 3.3% improvement in mean Average
Precision over YOLOv11n, the prevailing model, and reducing parameters by 23%.

</details>


### [86] [G-MSGINet: A Grouped Multi-Scale Graph-Involution Network for Contactless Fingerprint Recognition](https://arxiv.org/abs/2505.08233)
*Santhoshkumar Peddi,Soham Bandyopadhyay,Debasis Samanta*

Main category: cs.CV

TL;DR: G-MSGINet是一种高效的无接触指纹识别框架，通过整合局部细节定位和身份嵌入，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多分支架构或复杂预处理，限制了实际应用中的扩展性和泛化能力。

Method: 引入GMSGI层，结合像素级卷积、动态多尺度核生成和图关系建模，实现端到端优化。

Result: 在多个数据集上，F1分数达0.83±0.02，Rank-1准确率97.0%-99.1%，EER低至0.5%。

Conclusion: G-MSGINet在性能和效率上优于现有方法，适用于实际生物识别场景。

Abstract: This paper presents G-MSGINet, a unified and efficient framework for robust
contactless fingerprint recognition that jointly performs minutiae localization
and identity embedding directly from raw input images. Existing approaches rely
on multi-branch architectures, orientation labels, or complex preprocessing
steps, which limit scalability and generalization across real-world acquisition
scenarios. In contrast, the proposed architecture introduces the GMSGI layer, a
novel computational module that integrates grouped pixel-level involution,
dynamic multi-scale kernel generation, and graph-based relational modelling
into a single processing unit. Stacked GMSGI layers progressively refine both
local minutiae-sensitive features and global topological representations
through end-to-end optimization. The architecture eliminates explicit
orientation supervision and adapts graph connectivity directly from learned
kernel descriptors, thereby capturing meaningful structural relationships among
fingerprint regions without fixed heuristics. Extensive experiments on three
benchmark datasets, namely PolyU, CFPose, and Benchmark 2D/3D, demonstrate that
G-MSGINet consistently achieves minutiae F1-scores in the range of
$0.83\pm0.02$ and Rank-1 identification accuracies between 97.0% and 99.1%,
while maintaining an Equal Error Rate (EER) as low as 0.5%. These results
correspond to improvements of up to 4.8% in F1-score and 1.4% in Rank-1
accuracy when compared to prior methods, using only 0.38 million parameters and
6.63 giga floating-point operations, which represents up to ten times fewer
parameters than competitive baselines. This highlights the scalability and
effectiveness of G-MSGINet in real-world contactless biometric recognition
scenarios.

</details>


### [87] [Removing Watermarks with Partial Regeneration using Semantic Information](https://arxiv.org/abs/2505.08234)
*Krti Tallam,John Kevin Cava,Caleb Geniesse,N. Benjamin Erichson,Michael W. Mahoney*

Main category: cs.CV

TL;DR: 论文提出了一种名为SemanticRegen的三阶段攻击方法，能够有效擦除最先进的语义和隐形水印，同时保持图像的表观意义。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像的普及，隐形水印成为版权和来源的主要防御手段，但其对抗自适应攻击的鲁棒性尚未充分研究。

Method: SemanticRegen通过三个阶段实现攻击：(i)使用视觉语言模型获取细粒度描述，(ii)通过零样本分割提取前景掩码，(iii)通过LLM引导的扩散模型仅修复背景，保留显著对象和风格线索。

Result: 在四种水印系统（TreeRing、StegaStamp、StableSig、DWT/DCT）上测试，SemanticRegen是唯一能击败TreeRing水印的方法，同时降低其他方案的比特准确率，并保持高感知质量（mSSIM=0.94）。

Conclusion: 研究揭示了当前水印防御与自适应语义感知攻击能力之间的差距，强调了开发对内容保留再生攻击具有鲁棒性的水印算法的紧迫性。

Abstract: As AI-generated imagery becomes ubiquitous, invisible watermarks have emerged
as a primary line of defense for copyright and provenance. The newest
watermarking schemes embed semantic signals - content-aware patterns that are
designed to survive common image manipulations - yet their true robustness
against adaptive adversaries remains under-explored. We expose a previously
unreported vulnerability and introduce SemanticRegen, a three-stage, label-free
attack that erases state-of-the-art semantic and invisible watermarks while
leaving an image's apparent meaning intact. Our pipeline (i) uses a
vision-language model to obtain fine-grained captions, (ii) extracts foreground
masks with zero-shot segmentation, and (iii) inpaints only the background via
an LLM-guided diffusion model, thereby preserving salient objects and style
cues. Evaluated on 1,000 prompts across four watermarking systems - TreeRing,
StegaStamp, StableSig, and DWT/DCT - SemanticRegen is the only method to defeat
the semantic TreeRing watermark (p = 0.10 > 0.05) and reduces bit-accuracy
below 0.75 for the remaining schemes, all while maintaining high perceptual
quality (masked SSIM = 0.94 +/- 0.01). We further introduce masked SSIM (mSSIM)
to quantify fidelity within foreground regions, showing that our attack
achieves up to 12 percent higher mSSIM than prior diffusion-based attackers.
These results highlight an urgent gap between current watermark defenses and
the capabilities of adaptive, semantics-aware adversaries, underscoring the
need for watermarking algorithms that are resilient to content-preserving
regenerative attacks.

</details>


### [88] [EventDiff: A Unified and Efficient Diffusion Model Framework for Event-based Video Frame Interpolation](https://arxiv.org/abs/2505.08235)
*Hanle Zheng,Xujie Han,Zegang Peng,Shangbin Zhang,Guangxun Du,Zhuo Zou,Xilin Wang,Jibin Wu,Hao Guo,Lei Deng*

Main category: cs.CV

TL;DR: EventDiff是一种基于事件和扩散模型的视频帧插值方法，通过隐式运动建模和两阶段训练策略，在多样化的VFI场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决传统事件相机方法在复杂运动和光照变化下的局限性，同时避免显式运动建模对高保真图像重建的影响。

Method: 提出EventDiff框架，结合事件-帧混合自动编码器和时空交叉注意力模块，通过去噪扩散过程在潜在空间直接插值。

Result: 在多个数据集上表现最优，PSNR提升显著，推理速度更快。

Conclusion: EventDiff为事件相机的VFI任务提供了一种高效且鲁棒的解决方案。

Abstract: Video Frame Interpolation (VFI) is a fundamental yet challenging task in
computer vision, particularly under conditions involving large motion,
occlusion, and lighting variation. Recent advancements in event cameras have
opened up new opportunities for addressing these challenges. While existing
event-based VFI methods have succeeded in recovering large and complex motions
by leveraging handcrafted intermediate representations such as optical flow,
these designs often compromise high-fidelity image reconstruction under subtle
motion scenarios due to their reliance on explicit motion modeling. Meanwhile,
diffusion models provide a promising alternative for VFI by reconstructing
frames through a denoising process, eliminating the need for explicit motion
estimation or warping operations. In this work, we propose EventDiff, a unified
and efficient event-based diffusion model framework for VFI. EventDiff features
a novel Event-Frame Hybrid AutoEncoder (HAE) equipped with a lightweight
Spatial-Temporal Cross Attention (STCA) module that effectively fuses dynamic
event streams with static frames. Unlike previous event-based VFI methods,
EventDiff performs interpolation directly in the latent space via a denoising
diffusion process, making it more robust across diverse and challenging VFI
scenarios. Through a two-stage training strategy that first pretrains the HAE
and then jointly optimizes it with the diffusion model, our method achieves
state-of-the-art performance across multiple synthetic and real-world event VFI
datasets. The proposed method outperforms existing state-of-the-art event-based
VFI methods by up to 1.98dB in PSNR on Vimeo90K-Triplet and shows superior
performance in SNU-FILM tasks with multiple difficulty levels. Compared to the
emerging diffusion-based VFI approach, our method achieves up to 5.72dB PSNR
gain on Vimeo90K-Triplet and 4.24X faster inference.

</details>


### [89] [Congenital Heart Disease recognition using Deep Learning/Transformer models](https://arxiv.org/abs/2505.08242)
*Aidar Amangeldi,Vladislav Yarovenko,Angsar Taigonyrov*

Main category: cs.CV

TL;DR: 双模态（声音和图像）深度学习用于先天性心脏病诊断，分别达到73.9%和80.72%的准确率。


<details>
  <summary>Details</summary>
Motivation: 先天性心脏病（CHD）是婴儿发病和死亡的主要原因，但现有非侵入性筛查方法存在假阴性问题。

Method: 采用双模态（声音和图像）深度学习方法进行CHD诊断。

Result: 在ZCHSound数据集上准确率为73.9%，在DICOM胸部X光数据集上准确率为80.72%。

Conclusion: 双模态深度学习方法在CHD诊断中具有潜力，但仍需进一步提升准确率。

Abstract: Congenital Heart Disease (CHD) remains a leading cause of infant morbidity
and mortality, yet non-invasive screening methods often yield false negatives.
Deep learning models, with their ability to automatically extract features, can
assist doctors in detecting CHD more effectively. In this work, we investigate
the use of dual-modality (sound and image) deep learning methods for CHD
diagnosis. We achieve 73.9% accuracy on the ZCHSound dataset and 80.72%
accuracy on the DICOM Chest X-ray dataset.

</details>


### [90] [Identifying Memorization of Diffusion Models through p-Laplace Analysis](https://arxiv.org/abs/2505.08246)
*Jonathan Brokman,Amit Giloni,Omer Hofman,Roman Vainshtein,Hisashi Kojima,Guy Gilboa*

Main category: cs.CV

TL;DR: 该论文研究了扩散模型中的得分函数是否能用于计算高阶微分（p-Laplace算子），并利用其识别记忆的训练数据。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型得分函数的进一步应用，特别是高阶微分的计算及其在识别记忆数据中的潜力。

Method: 提出基于学习得分函数的数值p-Laplace近似方法，并在高斯混合模型和图像生成模型中验证其有效性。

Result: p-Laplace算子能有效识别概率分布的关键特征，首次在图像生成模型中实现了记忆数据的识别。

Conclusion: 得分函数的高阶微分（p-Laplace算子）可用于识别记忆数据，为生成模型的分析提供了新工具。

Abstract: Diffusion models, today's leading image generative models, estimate the score
function, i.e. the gradient of the log probability of (perturbed) data samples,
without direct access to the underlying probability distribution. This work
investigates whether the estimated score function can be leveraged to compute
higher-order differentials, namely p-Laplace operators. We show here these
operators can be employed to identify memorized training data. We propose a
numerical p-Laplace approximation based on the learned score functions, showing
its effectiveness in identifying key features of the probability landscape. We
analyze the structured case of Gaussian mixture models, and demonstrate the
results carry-over to image generative models, where memorization
identification based on the p-Laplace operator is performed for the first time.

</details>


### [91] [CNN and ViT Efficiency Study on Tiny ImageNet and DermaMNIST Datasets](https://arxiv.org/abs/2505.08259)
*Aidar Amangeldi,Angsar Taigonyrov,Muhammad Huzaid Jawad,Chinedu Emmanuel Mbonu*

Main category: cs.CV

TL;DR: 研究评估了卷积与Transformer架构在医学和通用图像分类任务中的权衡，通过微调策略发现Vision Transformers在性能、推理速度和参数数量上优于基线ResNet-18。


<details>
  <summary>Details</summary>
Motivation: 探索在资源受限环境中部署高效图像分类模型的可行性，比较不同架构的优劣。

Method: 使用ResNet-18作为基线，对四种Vision Transformer变体（Tiny、Small、Base、Large）进行微调，应用于DermatologyMNIST和TinyImageNet数据集，并通过系统超参数调整优化性能。

Result: 适当微调的Vision Transformers在性能上匹配或超越基线，同时实现更快的推理速度和更少的参数需求。

Conclusion: Vision Transformers在资源受限环境中具有部署潜力，尤其在需要高效推理和低复杂度的场景中。

Abstract: This study evaluates the trade-offs between convolutional and
transformer-based architectures on both medical and general-purpose image
classification benchmarks. We use ResNet-18 as our baseline and introduce a
fine-tuning strategy applied to four Vision Transformer variants (Tiny, Small,
Base, Large) on DermatologyMNIST and TinyImageNet. Our goal is to reduce
inference latency and model complexity with acceptable accuracy degradation.
Through systematic hyperparameter variations, we demonstrate that appropriately
fine-tuned Vision Transformers can match or exceed the baseline's performance,
achieve faster inference, and operate with fewer parameters, highlighting their
viability for deployment in resource-constrained environments.

</details>


### [92] [Few-shot Novel Category Discovery](https://arxiv.org/abs/2505.08260)
*Chunming Li,Shidong Wang,Haofeng Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种新的Few-Shot Novel Category Discovery (FSNCD)设置，结合半监督层次聚类和不确定性感知K均值聚类，显著提升了模型在新类别发现任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有NCD方法依赖转导学习，限制了其在真实场景中的应用。少量新类别标签数据可以缓解这一问题，因此论文探索了结合少量支持样本的新类别发现方法。

Method: 提出了FSNCD框架，结合Semi-supervised Hierarchical Clustering (SHC)和Uncertainty-aware K-means Clustering (UKC)，以增强模型的推理能力。

Result: 在五个常用数据集上的实验表明，该方法在不同任务设置和场景下均达到了领先性能。

Conclusion: FSNCD框架通过结合少量支持样本和新型聚类算法，显著提升了新类别发现的灵活性和性能。

Abstract: The recently proposed Novel Category Discovery (NCD) adapt paradigm of
transductive learning hinders its application in more real-world scenarios. In
fact, few labeled data in part of new categories can well alleviate this
burden, which coincides with the ease that people can label few of new category
data. Therefore, this paper presents a new setting in which a trained agent is
able to flexibly switch between the tasks of identifying examples of known
(labelled) classes and clustering novel (completely unlabeled) classes as the
number of query examples increases by leveraging knowledge learned from only a
few (handful) support examples. Drawing inspiration from the discovery of novel
categories using prior-based clustering algorithms, we introduce a novel
framework that further relaxes its assumptions to the real-world open set level
by unifying the concept of model adaptability in few-shot learning. We refer to
this setting as Few-Shot Novel Category Discovery (FSNCD) and propose
Semi-supervised Hierarchical Clustering (SHC) and Uncertainty-aware K-means
Clustering (UKC) to examine the model's reasoning capabilities. Extensive
experiments and detailed analysis on five commonly used datasets demonstrate
that our methods can achieve leading performance levels across different task
settings and scenarios.

</details>


### [93] [Open the Eyes of MPNN: Vision Enhances MPNN in Link Prediction](https://arxiv.org/abs/2505.08266)
*Yanbin Wei,Xuehao Wang,Zhan Zhuang,Yang Chen,Shuhao Chen,Yulong Zhang,Yu Zhang,James Kwok*

Main category: cs.CV

TL;DR: GVN和E-GVN框架首次将视觉感知引入MPNNs，显著提升了链接预测性能。


<details>
  <summary>Details</summary>
Motivation: 视觉感知在MPNNs中被忽视，但其潜力巨大。

Method: 提出GVN和E-GVN框架，赋予MPNNs视觉结构感知能力。

Result: 在七个数据集上表现优异，兼容现有SOTA方法并取得新SOTA。

Conclusion: GVN为链接预测开辟了新方向。

Abstract: Message-passing graph neural networks (MPNNs) and structural features (SFs)
are cornerstones for the link prediction task. However, as a common and
intuitive mode of understanding, the potential of visual perception has been
overlooked in the MPNN community. For the first time, we equip MPNNs with
vision structural awareness by proposing an effective framework called Graph
Vision Network (GVN), along with a more efficient variant (E-GVN). Extensive
empirical results demonstrate that with the proposed frameworks, GVN
consistently benefits from the vision enhancement across seven link prediction
datasets, including challenging large-scale graphs. Such improvements are
compatible with existing state-of-the-art (SOTA) methods and GVNs achieve new
SOTA results, thereby underscoring a promising novel direction for link
prediction.

</details>


### [94] [IrrMap: A Large-Scale Comprehensive Dataset for Irrigation Method Mapping](https://arxiv.org/abs/2505.08273)
*Nibir Chandra Mandal,Oishee Bintey Hoque,Abhijin Adiga,Samarth Swarup,Mandy Wilson,Lu Feng,Yangfeng Ji,Miaomiao Zhang,Geoffrey Fox,Madhav Marathe*

Main category: cs.CV

TL;DR: IrrMap是一个用于灌溉方法映射的大规模数据集（110万个图像块），包含多分辨率卫星图像和辅助数据，覆盖美国西部多个州的农田，支持深度学习模型训练和基准测试。


<details>
  <summary>Details</summary>
Motivation: 提供首个大规模灌溉方法映射数据集，支持农业和地理空间分析的研究与应用。

Method: 利用Landsat和Sentinel卫星图像，结合作物类型、土地利用和植被指数等辅助数据，生成标准化224x224 GeoTIFF图像块，并提供数据加载器和训练测试分割。

Result: 数据集覆盖168万多个农场和1410万英亩土地，提供灌溉方法分布、空间模式和面积变化的分析。

Conclusion: IrrMap及其配套工具和代码已公开，为灌溉研究和类似应用提供了丰富资源和扩展可能性。

Abstract: We introduce IrrMap, the first large-scale dataset (1.1 million patches) for
irrigation method mapping across regions. IrrMap consists of multi-resolution
satellite imagery from LandSat and Sentinel, along with key auxiliary data such
as crop type, land use, and vegetation indices. The dataset spans 1,687,899
farms and 14,117,330 acres across multiple western U.S. states from 2013 to
2023, providing a rich and diverse foundation for irrigation analysis and
ensuring geospatial alignment and quality control. The dataset is ML-ready,
with standardized 224x224 GeoTIFF patches, the multiple input modalities,
carefully chosen train-test-split data, and accompanying dataloaders for
seamless deep learning model training andbenchmarking in irrigation mapping.
The dataset is also accompanied by a complete pipeline for dataset generation,
enabling researchers to extend IrrMap to new regions for irrigation data
collection or adapt it with minimal effort for other similar applications in
agricultural and geospatial analysis. We also analyze the irrigation method
distribution across crop groups, spatial irrigation patterns (using Shannon
diversity indices), and irrigated area variations for both LandSat and
Sentinel, providing insights into regional and resolution-based differences. To
promote further exploration, we openly release IrrMap, along with the derived
datasets, benchmark models, and pipeline code, through a GitHub repository:
https://github.com/Nibir088/IrrMap and Data repository:
https://huggingface.co/Nibir/IrrMap, providing comprehensive documentation and
implementation details.

</details>


### [95] [Ultra Lowrate Image Compression with Semantic Residual Coding and Compression-aware Diffusion](https://arxiv.org/abs/2505.08281)
*Anle Ke,Xu Zhang,Tong Chen,Ming Lu,Chao Zhou,Jiawen Gu,Zhan Ma*

Main category: cs.CV

TL;DR: ResULIC提出了一种基于残差引导的超低码率图像压缩方法，通过结合语义残差编码和压缩感知扩散模型，显著提升了重建质量和编码效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型图像压缩框架在重建保真度和编码效率上表现不佳，原因是语义检索、潜在压缩和生成模型的碎片化整合。

Method: 提出语义残差编码（SRC）捕捉原始图像与压缩潜在表示间的语义差异，并引入压缩感知扩散模型（CDM）优化比特率与扩散时间步的匹配。

Result: 实验表明，ResULIC在LPIPS和FID指标上分别实现了80.7%和66.3%的BD-rate节省，优于现有扩散方法。

Conclusion: ResULIC通过残差信号和扩散模型的协同优化，显著提升了超低码率图像压缩的性能。

Abstract: Existing multimodal large model-based image compression frameworks often rely
on a fragmented integration of semantic retrieval, latent compression, and
generative models, resulting in suboptimal performance in both reconstruction
fidelity and coding efficiency. To address these challenges, we propose a
residual-guided ultra lowrate image compression named ResULIC, which
incorporates residual signals into both semantic retrieval and the
diffusion-based generation process. Specifically, we introduce Semantic
Residual Coding (SRC) to capture the semantic disparity between the original
image and its compressed latent representation. A perceptual fidelity optimizer
is further applied for superior reconstruction quality. Additionally, we
present the Compression-aware Diffusion Model (CDM), which establishes an
optimal alignment between bitrates and diffusion time steps, improving
compression-reconstruction synergy. Extensive experiments demonstrate the
effectiveness of ResULIC, achieving superior objective and subjective
performance compared to state-of-the-art diffusion-based methods with - 80.7%,
-66.3% BD-rate saving in terms of LPIPS and FID. Project page is available at
https: //njuvision.github.io/ResULIC/.

</details>


### [96] [Disruptive Transformation of Artworks in Master-Disciple Relationships: The Case of Ukiyo-e Artworks](https://arxiv.org/abs/2505.08284)
*Honna Shinichi,Akira Matsui*

Main category: cs.CV

TL;DR: 论文通过机器学习对浮世绘进行定量分析，揭示了其整体创造力随文化成熟下降，但风格创造力保持高水平。


<details>
  <summary>Details</summary>
Motivation: 传统艺术研究依赖主观判断，机器学习为东方绘画（如浮世绘）提供了定量分析的新方法。

Method: 使用11,000张高分辨率浮世绘图像，基于网络计算创造力，分析作品和艺术家的创造力。

Result: 浮世绘整体创造力随文化成熟下降，但风格创造力保持高水平并更细分。

Conclusion: 研究为浮世绘分析提供新视角，展示了其在东方艺术中的文化演变意义。

Abstract: Artwork research has long relied on human sensibility and subjective
judgment, but recent developments in machine learning have enabled the
quantitative assessment of features that humans could not discover. In Western
paintings, comprehensive analyses have been conducted from various perspectives
in conjunction with large databases, but such extensive analysis has not been
sufficiently conducted for Eastern paintings. Then, we focus on Ukiyo-e, a
traditional Japanese art form, as a case study of Eastern paintings, and
conduct a quantitative analysis of creativity in works of art using 11,000
high-resolution images. This involves using the concept of calculating
creativity from networks to analyze both the creativity of the artwork and that
of the artists. As a result, In terms of Ukiyo-e as a whole, it was found that
the creativity of its appearance has declined with the maturation of culture,
but in terms of style, it has become more segmented with the maturation of
culture and has maintained a high level of creativity. This not only provides
new insights into the study of Ukiyo-e but also shows how Ukiyo-e has evolved
within the ongoing cultural history, playing a culturally significant role in
the analysis of Eastern art.

</details>


### [97] [FauForensics: Boosting Audio-Visual Deepfake Detection with Facial Action Units](https://arxiv.org/abs/2505.08294)
*Jian Wang,Baoyuan Wu,Li Liu,Qingshan Liu*

Main category: cs.CV

TL;DR: 提出了一种名为FauForensics的新框架，利用生物不变的面部动作单元（FAUs）检测多模态深度伪造内容，通过细粒度帧级视听相似性计算和跨模态查询融合模块，实现了优异的性能和跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展导致逼真的视听深度伪造威胁增加，现有单模态检测方法难以应对多模态伪造，需解决模态特征异构性和泛化能力不足的问题。

Method: 引入生物不变的面部动作单元（FAUs）作为伪造抵抗表示，提出细粒度帧级视听相似性计算和跨模态查询融合模块，动态对齐时空唇音关系。

Result: 在FakeAVCeleb和LAV-DF数据集上表现优于现有方法，平均提升4.83%，具有卓越的跨数据集泛化能力。

Conclusion: FauForensics框架通过生物不变特征和动态跨模态融合，显著提升了多模态深度伪造检测的性能和泛化能力。

Abstract: The rapid evolution of generative AI has increased the threat of realistic
audio-visual deepfakes, demanding robust detection methods. Existing solutions
primarily address unimodal (audio or visual) forgeries but struggle with
multimodal manipulations due to inadequate handling of heterogeneous modality
features and poor generalization across datasets. To this end, we propose a
novel framework called FauForensics by introducing biologically invariant
facial action units (FAUs), which is a quantitative descriptor of facial muscle
activity linked to emotion physiology. It serves as forgery-resistant
representations that reduce domain dependency while capturing subtle dynamics
often disrupted in synthetic content. Besides, instead of comparing entire
video clips as in prior works, our method computes fine-grained frame-wise
audiovisual similarities via a dedicated fusion module augmented with learnable
cross-modal queries. It dynamically aligns temporal-spatial lip-audio
relationships while mitigating multi-modal feature heterogeneity issues.
Experiments on FakeAVCeleb and LAV-DF show state-of-the-art (SOTA) performance
and superior cross-dataset generalizability with up to an average of 4.83\%
than existing methods.

</details>


### [98] [Knowledge-Informed Deep Learning for Irrigation Type Mapping from Remote Sensing](https://arxiv.org/abs/2505.08302)
*Oishee Bintey Hoque,Nibir Chandra Mandal,Abhijin Adiga,Samarth Swarup,Sayjro Kossi Nouwakpo,Amanda Wilson,Madhav Marathe*

Main category: cs.CV

TL;DR: KIIM是一种基于Swin-Transformer的灌溉方法映射模型，通过多模态信息融合和两阶段迁移学习，显著提高了灌溉分类的准确性，尤其是在数据有限的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有基于光谱特征的卫星图像模型在复杂农业景观和有限训练数据下效果不佳，亟需更高效的灌溉分类方法。

Method: KIIM结合了作物灌溉概率编码、空间注意力图、双向交叉注意力和加权集成方法，并采用两阶段迁移学习。

Result: 在五个美国州的实验中，KIIM的IoU提升了22.9%，滴灌分类提升71.4%，且在数据有限时仍能保持性能。

Conclusion: KIIM显著减少了对手动标注的依赖，使大规模自动灌溉映射更可行和经济。

Abstract: Accurate mapping of irrigation methods is crucial for sustainable
agricultural practices and food systems. However, existing models that rely
solely on spectral features from satellite imagery are ineffective due to the
complexity of agricultural landscapes and limited training data, making this a
challenging problem. We present Knowledge-Informed Irrigation Mapping (KIIM), a
novel Swin-Transformer based approach that uses (i) a specialized projection
matrix to encode crop to irrigation probability, (ii) a spatial attention map
to identify agricultural lands from non-agricultural lands, (iii)
bi-directional cross-attention to focus complementary information from
different modalities, and (iv) a weighted ensemble for combining predictions
from images and crop information. Our experimentation on five states in the US
shows up to 22.9\% (IoU) improvement over baseline with a 71.4% (IoU)
improvement for hard-to-classify drip irrigation. In addition, we propose a
two-phase transfer learning approach to enhance cross-state irrigation mapping,
achieving a 51% IoU boost in a state with limited labeled data. The ability to
achieve baseline performance with only 40% of the training data highlights its
efficiency, reducing the dependency on extensive manual labeling efforts and
making large-scale, automated irrigation mapping more feasible and
cost-effective.

</details>


### [99] [An incremental algorithm for non-convex AI-enhanced medical image processing](https://arxiv.org/abs/2505.08324)
*Elena Morotti*

Main category: cs.CV

TL;DR: 论文提出了一种名为incDG的混合框架，结合深度学习和增量模型优化，用于高效解决非凸正则化逆问题，尤其在医学影像中表现优异。


<details>
  <summary>Details</summary>
Motivation: 非凸正则化逆问题因其复杂的优化空间和多局部极小值而难以解决，但这些模型在医学影像等领域能提供高质量的任务导向解。

Method: incDG框架结合深度学习（生成初始解）和非凸变分解算器（通过正则化增量迭代优化），融合AI效率与模型优化理论保证。

Result: 在TpV正则化任务中，incDG在医学图像去模糊和断层重建中表现优于传统迭代解算器和深度学习方法，且无需真实数据训练。

Conclusion: incDG是一种高效、稳健的工具，适用于解决影像及其他领域的非凸逆问题。

Abstract: Solving non-convex regularized inverse problems is challenging due to their
complex optimization landscapes and multiple local minima. However, these
models remain widely studied as they often yield high-quality, task-oriented
solutions, particularly in medical imaging, where the goal is to enhance
clinically relevant features rather than merely minimizing global error. We
propose incDG, a hybrid framework that integrates deep learning with
incremental model-based optimization to efficiently approximate the
$\ell_0$-optimal solution of imaging inverse problems. Built on the Deep Guess
strategy, incDG exploits a deep neural network to generate effective
initializations for a non-convex variational solver, which refines the
reconstruction through regularized incremental iterations. This design combines
the efficiency of Artificial Intelligence (AI) tools with the theoretical
guarantees of model-based optimization, ensuring robustness and stability. We
validate incDG on TpV-regularized optimization tasks, demonstrating its
effectiveness in medical image deblurring and tomographic reconstruction across
diverse datasets, including synthetic images, brain CT slices, and
chest-abdomen scans. Results show that incDG outperforms both conventional
iterative solvers and deep learning-based methods, achieving superior accuracy
and stability. Moreover, we confirm that training incDG without ground truth
does not significantly degrade performance, making it a practical and powerful
tool for solving non-convex inverse problems in imaging and beyond.

</details>


### [100] [A computer vision-based model for occupancy detection using low-resolution thermal images](https://arxiv.org/abs/2505.08336)
*Xue Cui,Vincent Gbouna Zakka,Minhyun Lee*

Main category: cs.CV

TL;DR: 论文提出了一种基于低分辨率热成像和计算机视觉的占用检测模型，解决了隐私问题并降低了计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 传统HVAC系统基于固定时间表运行，不考虑占用情况，而现有RGB图像占用检测方法存在隐私问题。

Method: 采用低分辨率热成像和YOLOv5模型进行占用检测，利用迁移学习优化模型。

Result: 模型性能优异，精确度、召回率和mAP50值接近1.000。

Conclusion: 该模型不仅解决了隐私问题，还减少了计算资源需求，为HVAC系统提供了更优的占用检测方案。

Abstract: Occupancy plays an essential role in influencing the energy consumption and
operation of heating, ventilation, and air conditioning (HVAC) systems.
Traditional HVAC typically operate on fixed schedules without considering
occupancy. Advanced occupant-centric control (OCC) adopted occupancy status in
regulating HVAC operations. RGB images combined with computer vision (CV)
techniques are widely used for occupancy detection, however, the detailed
facial and body features they capture raise significant privacy concerns.
Low-resolution thermal images offer a non-invasive solution that mitigates
privacy issues. The study developed an occupancy detection model utilizing
low-resolution thermal images and CV techniques, where transfer learning was
applied to fine-tune the You Only Look Once version 5 (YOLOv5) model. The
developed model ultimately achieved satisfactory performance, with precision,
recall, mAP50, and mAP50 values approaching 1.000. The contributions of this
model lie not only in mitigating privacy concerns but also in reducing
computing resource demands.

</details>


### [101] [FAD: Frequency Adaptation and Diversion for Cross-domain Few-shot Learning](https://arxiv.org/abs/2505.08349)
*Ruixiao Shi,Fu Feng,Yucheng Xie,Jing Wang,Xin Geng*

Main category: cs.CV

TL;DR: 论文提出了一种频率感知框架FAD，通过频域表示和分频带适应提升跨域小样本学习的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 跨域小样本学习中，空间域方法忽略了频域变化对泛化的重要性，导致性能受限。

Method: FAD框架将特征转换到频域，分频带（低、中、高）并分别适应，使用定制卷积核优化各频带。

Result: 在Meta-Dataset基准测试中，FAD优于现有方法，验证了频域表示的有效性。

Conclusion: 频域适应和分频带处理能显著提升跨域小样本学习的泛化性能。

Abstract: Cross-domain few-shot learning (CD-FSL) requires models to generalize from
limited labeled samples under significant distribution shifts. While recent
methods enhance adaptability through lightweight task-specific modules, they
operate solely in the spatial domain and overlook frequency-specific variations
that are often critical for robust transfer. We observe that spatially similar
images across domains can differ substantially in their spectral
representations, with low and high frequencies capturing complementary semantic
information at coarse and fine levels. This indicates that uniform spatial
adaptation may overlook these spectral distinctions, thus constraining
generalization. To address this, we introduce Frequency Adaptation and
Diversion (FAD), a frequency-aware framework that explicitly models and
modulates spectral components. At its core is the Frequency Diversion Adapter,
which transforms intermediate features into the frequency domain using the
discrete Fourier transform (DFT), partitions them into low, mid, and
high-frequency bands via radial masks, and reconstructs each band using inverse
DFT (IDFT). Each frequency band is then adapted using a dedicated convolutional
branch with a kernel size tailored to its spectral scale, enabling targeted and
disentangled adaptation across frequencies. Extensive experiments on the
Meta-Dataset benchmark demonstrate that FAD consistently outperforms
state-of-the-art methods on both seen and unseen domains, validating the
utility of frequency-domain representations and band-wise adaptation for
improving generalization in CD-FSL.

</details>


### [102] [STORYANCHORS: Generating Consistent Multi-Scene Story Frames for Long-Form Narratives](https://arxiv.org/abs/2505.08350)
*Bo Wang,Haoyang Huang,Zhiyin Lu,Fengyuan Liu,Guoqing Ma,Jianlong Yuan,Yuan Zhang,Nan Duan*

Main category: cs.CV

TL;DR: StoryAnchors是一个统一的框架，用于生成高质量、多场景且具有强时序一致性的故事帧。它通过双向故事生成器和特定条件提升生成质量，支持可编辑和扩展的故事帧生成。


<details>
  <summary>Details</summary>
Motivation: 解决多场景故事帧生成中的时序一致性和叙事丰富性问题，区别于传统视频合成。

Method: 采用双向故事生成器整合过去和未来上下文，结合多事件故事帧标注和渐进式训练方法。

Result: 在一致性、叙事连贯性和场景多样性上优于现有开源模型，与GPT-4o在叙事一致性上表现相当。

Conclusion: StoryAnchors为故事驱动帧生成提供了可扩展、灵活且高度可编辑的基础，推动了该领域的研究边界。

Abstract: This paper introduces StoryAnchors, a unified framework for generating
high-quality, multi-scene story frames with strong temporal consistency. The
framework employs a bidirectional story generator that integrates both past and
future contexts to ensure temporal consistency, character continuity, and
smooth scene transitions throughout the narrative. Specific conditions are
introduced to distinguish story frame generation from standard video synthesis,
facilitating greater scene diversity and enhancing narrative richness. To
further improve generation quality, StoryAnchors integrates Multi-Event Story
Frame Labeling and Progressive Story Frame Training, enabling the model to
capture both overarching narrative flow and event-level dynamics. This approach
supports the creation of editable and expandable story frames, allowing for
manual modifications and the generation of longer, more complex sequences.
Extensive experiments show that StoryAnchors outperforms existing open-source
models in key areas such as consistency, narrative coherence, and scene
diversity. Its performance in narrative consistency and story richness is also
on par with GPT-4o. Ultimately, StoryAnchors pushes the boundaries of
story-driven frame generation, offering a scalable, flexible, and highly
editable foundation for future research.

</details>


### [103] [DArFace: Deformation Aware Robustness for Low Quality Face Recognition](https://arxiv.org/abs/2505.08423)
*Sadaf Gulshad,Abdullah Aldahlawi Thakaa*

Main category: cs.CV

TL;DR: DArFace提出了一种新的面部识别框架，通过模拟真实低质量图像的全局和局部变形，提升识别鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有面部识别系统在低质量图像（如低分辨率、运动模糊）下性能下降，且现有方法常忽略局部非刚性变形。

Method: DArFace通过对抗训练模拟全局和局部变形，并结合对比学习保持身份一致性。

Result: 在TinyFace、IJB-B和IJB-C等低质量基准测试中，DArFace显著优于现有方法。

Conclusion: DArFace通过建模局部变形，显著提升了低质量图像下的面部识别性能。

Abstract: Facial recognition systems have achieved remarkable success by leveraging
deep neural networks, advanced loss functions, and large-scale datasets.
However, their performance often deteriorates in real-world scenarios involving
low-quality facial images. Such degradations, common in surveillance footage or
standoff imaging include low resolution, motion blur, and various distortions,
resulting in a substantial domain gap from the high-quality data typically used
during training. While existing approaches attempt to address robustness by
modifying network architectures or modeling global spatial transformations,
they frequently overlook local, non-rigid deformations that are inherently
present in real-world settings. In this work, we introduce DArFace, a
Deformation-Aware robust Face recognition framework that enhances robustness to
such degradations without requiring paired high- and low-quality training
samples. Our method adversarially integrates both global transformations (e.g.,
rotation, translation) and local elastic deformations during training to
simulate realistic low-quality conditions. Moreover, we introduce a contrastive
objective to enforce identity consistency across different deformed views.
Extensive evaluations on low-quality benchmarks including TinyFace, IJB-B, and
IJB-C demonstrate that DArFace surpasses state-of-the-art methods, with
significant gains attributed to the inclusion of local deformation modeling.

</details>


### [104] [DHECA-SuperGaze: Dual Head-Eye Cross-Attention and Super-Resolution for Unconstrained Gaze Estimation](https://arxiv.org/abs/2505.08426)
*Franko Šikić,Donik Vršnak,Sven Lončarić*

Main category: cs.CV

TL;DR: 本文提出了一种名为DHECA-SuperGaze的深度学习方法，通过超分辨率和双头眼交叉注意力模块改进无约束视线估计，并在Gaze360和GFIE数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 无约束视线估计在现实场景中面临挑战，如图像分辨率低和头眼交互建模不足。本文旨在通过新方法解决这些问题。

Method: 采用双分支卷积主干处理眼部和多尺度超分辨率头部图像，并引入双头眼交叉注意力模块进行特征优化。

Result: 在Gaze360和GFIE数据集上，静态和时序配置下的角度误差显著降低，跨数据集测试也验证了方法的泛化能力。

Conclusion: DHECA-SuperGaze在无约束视线估计中表现优异，具有鲁棒的泛化性能。

Abstract: Unconstrained gaze estimation is the process of determining where a subject
is directing their visual attention in uncontrolled environments. Gaze
estimation systems are important for a myriad of tasks such as driver
distraction monitoring, exam proctoring, accessibility features in modern
software, etc. However, these systems face challenges in real-world scenarios,
partially due to the low resolution of in-the-wild images and partially due to
insufficient modeling of head-eye interactions in current state-of-the-art
(SOTA) methods. This paper introduces DHECA-SuperGaze, a deep learning-based
method that advances gaze prediction through super-resolution (SR) and a dual
head-eye cross-attention (DHECA) module. Our dual-branch convolutional backbone
processes eye and multiscale SR head images, while the proposed DHECA module
enables bidirectional feature refinement between the extracted visual features
through cross-attention mechanisms. Furthermore, we identified critical
annotation errors in one of the most diverse and widely used gaze estimation
datasets, Gaze360, and rectified the mislabeled data. Performance evaluation on
Gaze360 and GFIE datasets demonstrates superior within-dataset performance of
the proposed method, reducing angular error (AE) by 0.48{\deg} (Gaze360) and
2.95{\deg} (GFIE) in static configurations, and 0.59{\deg} (Gaze360) and
3.00{\deg} (GFIE) in temporal settings compared to prior SOTA methods.
Cross-dataset testing shows improvements in AE of more than 1.53{\deg}
(Gaze360) and 3.99{\deg} (GFIE) in both static and temporal settings,
validating the robust generalization properties of our approach.

</details>


### [105] [Visual Image Reconstruction from Brain Activity via Latent Representation](https://arxiv.org/abs/2505.08429)
*Yukiyasu Kamitani,Misato Tanaka,Ken Shirakawa*

Main category: cs.CV

TL;DR: 视觉图像重建通过深度神经网络和生成模型取得进展，但仍面临零样本泛化和主观感知建模的挑战。


<details>
  <summary>Details</summary>
Motivation: 探索如何从大脑活动中解码视觉内容，以理解神经编码并推动临床应用。

Method: 利用分层潜在表示、组合策略和模块化架构，结合DNN和生成模型。

Result: 实现了更精细的图像重建，但仍需改进泛化能力和主观感知建模。

Conclusion: 需多样化数据集、优化评估指标，并关注伦理问题，以推动未来发展。

Abstract: Visual image reconstruction, the decoding of perceptual content from brain
activity into images, has advanced significantly with the integration of deep
neural networks (DNNs) and generative models. This review traces the field's
evolution from early classification approaches to sophisticated reconstructions
that capture detailed, subjective visual experiences, emphasizing the roles of
hierarchical latent representations, compositional strategies, and modular
architectures. Despite notable progress, challenges remain, such as achieving
true zero-shot generalization for unseen images and accurately modeling the
complex, subjective aspects of perception. We discuss the need for diverse
datasets, refined evaluation metrics aligned with human perceptual judgments,
and compositional representations that strengthen model robustness and
generalizability. Ethical issues, including privacy, consent, and potential
misuse, are underscored as critical considerations for responsible development.
Visual image reconstruction offers promising insights into neural coding and
enables new psychological measurements of visual experiences, with applications
spanning clinical diagnostics and brain-machine interfaces.

</details>


### [106] [TT-DF: A Large-Scale Diffusion-Based Dataset and Benchmark for Human Body Forgery Detection](https://arxiv.org/abs/2505.08437)
*Wenkui Yang,Zhida Zhang,Xiaoqiang Zhou,Junxian Duan,Jie Cao*

Main category: cs.CV

TL;DR: 论文介绍了TikTok-DeepFake（TT-DF）数据集，专注于人体伪造检测，并提出了一种新的检测模型TOF-Net。


<details>
  <summary>Details</summary>
Motivation: 由于人体伪造检测缺乏数据集和方法，作者旨在填补这一空白。

Method: 提出了TT-DF数据集，包含多种伪造方法和配置，并设计了TOF-Net模型，利用时空不一致性和光流分布差异进行检测。

Result: TOF-Net在TT-DF上表现优异，优于现有面部伪造检测模型。

Conclusion: TT-DF和TOF-Net为人体伪造检测提供了有效工具，填补了研究空白。

Abstract: The emergence and popularity of facial deepfake methods spur the vigorous
development of deepfake datasets and facial forgery detection, which to some
extent alleviates the security concerns about facial-related artificial
intelligence technologies. However, when it comes to human body forgery, there
has been a persistent lack of datasets and detection methods, due to the later
inception and complexity of human body generation methods. To mitigate this
issue, we introduce TikTok-DeepFake (TT-DF), a novel large-scale
diffusion-based dataset containing 6,120 forged videos with 1,378,857 synthetic
frames, specifically tailored for body forgery detection. TT-DF offers a wide
variety of forgery methods, involving multiple advanced human image animation
models utilized for manipulation, two generative configurations based on the
disentanglement of identity and pose information, as well as different
compressed versions. The aim is to simulate any potential unseen forged data in
the wild as comprehensively as possible, and we also furnish a benchmark on
TT-DF. Additionally, we propose an adapted body forgery detection model,
Temporal Optical Flow Network (TOF-Net), which exploits the spatiotemporal
inconsistencies and optical flow distribution differences between natural data
and forged data. Our experiments demonstrate that TOF-Net achieves favorable
performance on TT-DF, outperforming current state-of-the-art extendable facial
forgery detection models. For our TT-DF dataset, please refer to
https://github.com/HashTAG00002/TT-DF.

</details>


### [107] [A Survey of 3D Reconstruction with Event Cameras: From Event-based Geometry to Neural 3D Rendering](https://arxiv.org/abs/2505.08438)
*Chuanzhi Xu,Haoxian Zhou,Langyi Chen,Haodong Chen,Ying Zhou,Vera Chung,Qiang Qu*

Main category: cs.CV

TL;DR: 该论文综述了事件相机在3D重建中的应用，分类了现有方法，总结了数据集，并指出了当前研究的局限性与未来方向。


<details>
  <summary>Details</summary>
Motivation: 事件相机因其异步捕捉亮度变化的能力，在3D重建中表现出色，尤其是在极端环境下。本文旨在提供首个专注于事件相机3D重建的全面综述。

Method: 将现有工作按输入模态（立体、单目、多模态）和重建方法（几何、深度学习、神经渲染）分类，并按时间顺序细分。

Result: 总结了相关公共数据集，并指出当前研究在数据可用性、评估、表示和动态场景处理方面的局限性。

Conclusion: 本文为事件驱动的3D重建提供了全面参考和未来发展的路线图。

Abstract: Event cameras have emerged as promising sensors for 3D reconstruction due to
their ability to capture per-pixel brightness changes asynchronously. Unlike
conventional frame-based cameras, they produce sparse and temporally rich data
streams, which enable more accurate 3D reconstruction and open up the
possibility of performing reconstruction in extreme environments such as
high-speed motion, low light, or high dynamic range scenes. In this survey, we
provide the first comprehensive review focused exclusively on 3D reconstruction
using event cameras. The survey categorises existing works into three major
types based on input modality - stereo, monocular, and multimodal systems, and
further classifies them by reconstruction approach, including geometry-based,
deep learning-based, and recent neural rendering techniques such as Neural
Radiance Fields and 3D Gaussian Splatting. Methods with a similar research
focus were organised chronologically into the most subdivided groups. We also
summarise public datasets relevant to event-based 3D reconstruction. Finally,
we highlight current research limitations in data availability, evaluation,
representation, and dynamic scene handling, and outline promising future
research directions. This survey aims to serve as a comprehensive reference and
a roadmap for future developments in event-driven 3D reconstruction.

</details>


### [108] [VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models](https://arxiv.org/abs/2505.08455)
*Pritam Sarkar,Ali Etemad*

Main category: cs.CV

TL;DR: 论文提出了VCRBench，一个用于评估大型视频语言模型（LVLMs）在视频因果推理能力的基准，并提出了RRD方法以提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门评估视频因果推理的基准，导致LVLMs在此任务上的能力未被充分探索。

Method: 通过创建VCRBench基准，测试LVLMs在视频因果推理中的表现，并提出RRD方法分解任务为视频识别和因果推理两部分。

Result: 实验表明LVLMs在长视频因果推理中表现不佳，但RRD方法能显著提升性能（最高25.2%）。

Conclusion: LVLMs在视频因果推理中依赖语言知识，RRD方法为提升此类任务性能提供了有效途径。

Abstract: Despite recent advances in video understanding, the capabilities of Large
Video Language Models (LVLMs) to perform video-based causal reasoning remains
underexplored, largely due to the absence of relevant and dedicated benchmarks
for evaluating causal reasoning in visually grounded and goal-driven settings.
To fill this gap, we introduce a novel benchmark named Video-based long-form
Causal Reasoning (VCRBench). We create VCRBench using procedural videos of
simple everyday activities, where the steps are deliberately shuffled with each
clip capturing a key causal event, to test whether LVLMs can identify, reason
about, and correctly sequence the events needed to accomplish a specific goal.
Moreover, the benchmark is carefully designed to prevent LVLMs from exploiting
linguistic shortcuts, as seen in multiple-choice or binary QA formats, while
also avoiding the challenges associated with evaluating open-ended QA. Our
evaluation of state-of-the-art LVLMs on VCRBench suggests that these models
struggle with video-based long-form causal reasoning, primarily due to their
difficulty in modeling long-range causal dependencies directly from visual
observations. As a simple step toward enabling such capabilities, we propose
Recognition-Reasoning Decomposition (RRD), a modular approach that breaks
video-based causal reasoning into two sub-tasks of video recognition and causal
reasoning. Our experiments on VCRBench show that RRD significantly boosts
accuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis
reveals interesting insights, for instance, that LVLMs primarily rely on
language knowledge for complex video-based long-form causal reasoning tasks.

</details>


### [109] [A Deep Learning-Driven Framework for Inhalation Injury Grading Using Bronchoscopy Images](https://arxiv.org/abs/2505.08517)
*Yifan Li,Alan W Pang,Jo Woon Chong*

Main category: cs.CV

TL;DR: 本研究提出了一种基于深度学习的框架，利用支气管镜图像和机械通气时间作为客观指标，用于吸入性损伤分级。通过改进的StarGAN生成模型增强数据质量，显著提高了分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法如AIS依赖主观评估且与临床结果相关性弱，吸入性损伤的诊断和分级面临挑战。

Method: 采用改进的StarGAN（结合Patch Loss和SSIM Loss）生成高质量合成图像，并使用Swin Transformer进行分类评估。

Result: 改进后的数据集使分类准确率提升至77.78%，FID得分最低为30.06，生成的图像被烧伤外科医生认可为具有临床相关性。

Conclusion: 改进的StarGAN能有效解决数据稀缺问题，提升吸入性损伤分级的准确性。

Abstract: Inhalation injuries face a challenge in clinical diagnosis and grading due to
the limitations of traditional methods, such as Abbreviated Injury Score (AIS),
which rely on subjective assessments and show weak correlations with clinical
outcomes. This study introduces a novel deep learning-based framework for
grading inhalation injuries using bronchoscopy images with the duration of
mechanical ventilation as an objective metric. To address the scarcity of
medical imaging data, we propose enhanced StarGAN, a generative model that
integrates Patch Loss and SSIM Loss to improve synthetic images' quality and
clinical relevance. The augmented dataset generated by enhanced StarGAN
significantly improved classification performance when evaluated using the Swin
Transformer, achieving an accuracy of 77.78%, an 11.11% improvement over the
original dataset. Image quality was assessed using the Fr\'echet Inception
Distance (FID), where Enhanced StarGAN achieved the lowest FID of 30.06,
outperforming baseline models. Burn surgeons confirmed the realism and clinical
relevance of the generated images, particularly the preservation of bronchial
structures and color distribution. These results highlight the potential of
enhanced StarGAN in addressing data limitations and improving classification
accuracy for inhalation injury grading.

</details>


### [110] [Attention-based Generative Latent Replay: A Continual Learning Approach for WSI Analysis](https://arxiv.org/abs/2505.08524)
*Pratibha Kumari,Daniel Reisenbüchler,Afshin Bozorgpour,Nadine S. Schaadt,Friedrich Feuerhake,Dorit Merhof*

Main category: cs.CV

TL;DR: 提出了一种基于注意力的生成潜在重放持续学习框架（AGLR-CL），用于解决全切片图像（WSI）分类中的域偏移问题，通过合成WSI表示和补丁分布来保留历史知识，无需存储原始数据。


<details>
  <summary>Details</summary>
Motivation: 解决WSI分类中因不同器官、疾病或机构差异导致的域偏移问题，同时保护隐私。

Method: 使用高斯混合模型（GMMs）合成WSI表示和补丁分布，结合注意力机制过滤重要补丁嵌入，避免显式存储数据。

Result: 在多个公共数据集上验证了AGLR-CL在生物标志物检测和分子状态预测中的有效性，性能优于无缓冲方法，与有缓冲方法相当。

Conclusion: AGLR-CL提供了一种隐私保护且高效的域增量持续学习方法，适用于WSI分类。

Abstract: Whole slide image (WSI) classification has emerged as a powerful tool in
computational pathology, but remains constrained by domain shifts, e.g., due to
different organs, diseases, or institution-specific variations. To address this
challenge, we propose an Attention-based Generative Latent Replay Continual
Learning framework (AGLR-CL), in a multiple instance learning (MIL) setup for
domain incremental WSI classification. Our method employs Gaussian Mixture
Models (GMMs) to synthesize WSI representations and patch count distributions,
preserving knowledge of past domains without explicitly storing original data.
A novel attention-based filtering step focuses on the most salient patch
embeddings, ensuring high-quality synthetic samples. This privacy-aware
strategy obviates the need for replay buffers and outperforms other buffer-free
counterparts while matching the performance of buffer-based solutions. We
validate AGLR-CL on clinically relevant biomarker detection and molecular
status prediction across multiple public datasets with diverse centers, organs,
and patient cohorts. Experimental results confirm its ability to retain prior
knowledge and adapt to new domains, offering an effective, privacy-preserving
avenue for domain incremental continual learning in WSI classification.

</details>


### [111] [Dynamic Snake Upsampling Operater and Boundary-Skeleton Weighted Loss for Tubular Structure Segmentation](https://arxiv.org/abs/2505.08525)
*Yiqi Chen,Ganghai Huang,Sheng Zhang,Jianglin Dai*

Main category: cs.CV

TL;DR: 论文提出动态蛇形上采样算子和边界-骨架加权损失，用于提升管状拓扑结构的分割精度和拓扑一致性。


<details>
  <summary>Details</summary>
Motivation: 管状拓扑结构（如裂隙和血管）的精确分割对下游定量分析和建模至关重要，但传统上采样算子难以处理其细长和弯曲形态。

Method: 设计基于自适应采样域的动态蛇形上采样算子，动态调整采样步长；提出边界-骨架加权损失，平衡主体和边界权重分配。

Result: 实验表明，该方法在多种数据集和骨干网络中显著提升了分割精度和拓扑一致性。

Conclusion: 动态蛇形上采样算子和边界-骨架加权损失为管状结构分割提供了有效解决方案。

Abstract: Accurate segmentation of tubular topological structures (e.g., fissures and
vasculature) is critical in various fields to guarantee dependable downstream
quantitative analysis and modeling. However, in dense prediction tasks such as
semantic segmentation and super-resolution, conventional upsampling operators
cannot accommodate the slenderness of tubular structures and the curvature of
morphology. This paper introduces a dynamic snake upsampling operators and a
boundary-skeleton weighted loss tailored for topological tubular structures.
Specifically, we design a snake upsampling operators based on an adaptive
sampling domain, which dynamically adjusts the sampling stride according to the
feature map and selects a set of subpixel sampling points along the serpentine
path, enabling more accurate subpixel-level feature recovery for tubular
structures. Meanwhile, we propose a skeleton-to-boundary increasing weighted
loss that trades off main body and boundary weight allocation based on mask
class ratio and distance field, preserving main body overlap while enhancing
focus on target topological continuity and boundary alignment precision.
Experiments across various domain datasets and backbone networks show that this
plug-and-play dynamic snake upsampling operator and boundary-skeleton weighted
loss boost both pixel-wise segmentation accuracy and topological consistency of
results.

</details>


### [112] [Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting](https://arxiv.org/abs/2505.08527)
*Zheang Huai,Hui Tang,Yi Li,Zhuangzhuang Chen,Xiaomeng Li*

Main category: cs.CV

TL;DR: 论文提出了一种基于Segment Anything Model（SAM）的双特征引导（DFG）自动提示方法，用于无源域自适应分割（SFDA），通过自动生成准确的边界框提示来提升目标域的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有SFDA方法在生成边界框提示时因域差距导致的缺陷，探索SAM在SFDA中的潜力。

Method: 提出DFG方法，分两阶段：特征聚合阶段初步适应目标域并准备特征分布；第二阶段基于目标模型和SAM特征逐步扩展边界框提示，并通过连通性分析优化伪标签。

Result: 在3D和2D数据集上表现优于传统方法。

Conclusion: DFG方法有效提升了SFDA的性能，展示了SAM在自动提示中的潜力。

Abstract: Source-free domain adaptation (SFDA) for segmentation aims at adapting a
model trained in the source domain to perform well in the target domain with
only the source model and unlabeled target data.Inspired by the recent success
of Segment Anything Model (SAM) which exhibits the generality of segmenting
images of various modalities and in different domains given human-annotated
prompts like bounding boxes or points, we for the first time explore the
potentials of Segment Anything Model for SFDA via automatedly finding an
accurate bounding box prompt. We find that the bounding boxes directly
generated with existing SFDA approaches are defective due to the domain gap.To
tackle this issue, we propose a novel Dual Feature Guided (DFG) auto-prompting
approach to search for the box prompt. Specifically, the source model is first
trained in a feature aggregation phase, which not only preliminarily adapts the
source model to the target domain but also builds a feature distribution
well-prepared for box prompt search. In the second phase, based on two feature
distribution observations, we gradually expand the box prompt with the guidance
of the target model feature and the SAM feature to handle the class-wise
clustered target features and the class-wise dispersed target features,
respectively. To remove the potentially enlarged false positive regions caused
by the over-confident prediction of the target model, the refined pseudo-labels
produced by SAM are further postprocessed based on connectivity analysis.
Experiments on 3D and 2D datasets indicate that our approach yields superior
performance compared to conventional methods. Code is available at
https://github.com/zheangh/DFG.

</details>


### [113] [The RaspGrade Dataset: Towards Automatic Raspberry Ripeness Grading with Deep Learning](https://arxiv.org/abs/2505.08537)
*Mohamed Lamine Mekhalfi,Paul Chippendale,Fabio Poiesi,Samuele Bonecher,Gilberto Osler,Nicola Zancanella*

Main category: cs.CV

TL;DR: 研究探讨了计算机视觉在快速、准确、非侵入式食品质量评估中的应用，专注于工业环境中实时将覆盆子分为五类的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决覆盆子在传送带上移动时的实时分级问题，提高食品质量评估效率。

Method: 使用名为RaspGrade的数据集进行实例分割实验，获取水果级掩码并分类。

Result: 某些覆盆子等级因颜色相似和遮挡难以分类，而其他等级基于颜色较易区分。

Conclusion: RaspGrade数据集可用于进一步研究，但需改进分类方法以应对挑战。

Abstract: This research investigates the application of computer vision for rapid,
accurate, and non-invasive food quality assessment, focusing on the novel
challenge of real-time raspberry grading into five distinct classes within an
industrial environment as the fruits move along a conveyor belt. To address
this, a dedicated dataset of raspberries, namely RaspGrade, was acquired and
meticulously annotated. Instance segmentation experiments revealed that
accurate fruit-level masks can be obtained; however, the classification of
certain raspberry grades presents challenges due to color similarities and
occlusion, while others are more readily distinguishable based on color. The
acquired and annotated RaspGrade dataset is accessible on HuggingFace at:
https://huggingface.co/datasets/FBK-TeV/RaspGrade.

</details>


### [114] [DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art](https://arxiv.org/abs/2505.08552)
*Haroon Wahab,Hassan Ugail,Irfan Mehmood*

Main category: cs.CV

TL;DR: 论文提出DFA-CON框架，通过对比学习检测AI生成艺术品的版权侵权或伪造问题。


<details>
  <summary>Details</summary>
Motivation: 生成AI工具广泛用于视觉内容创作，但可能侵犯版权或伪造作品，需有效检测方法。

Method: 采用对比学习框架DFA-CON，学习判别性表示空间，检测多种攻击类型（如修复、风格迁移等）。

Result: 在多数攻击类型中表现优于现有预训练模型，检测效果稳健。

Conclusion: DFA-CON为AI生成艺术品的版权侵权和伪造检测提供了有效解决方案。

Abstract: Recent proliferation of generative AI tools for visual content
creation-particularly in the context of visual artworks-has raised serious
concerns about copyright infringement and forgery. The large-scale datasets
used to train these models often contain a mixture of copyrighted and
non-copyrighted artworks. Given the tendency of generative models to memorize
training patterns, they are susceptible to varying degrees of copyright
violation. Building on the recently proposed DeepfakeArt Challenge benchmark,
this work introduces DFA-CON, a contrastive learning framework designed to
detect copyright-infringing or forged AI-generated art. DFA-CON learns a
discriminative representation space, posing affinity among original artworks
and their forged counterparts within a contrastive learning framework. The
model is trained across multiple attack types, including inpainting, style
transfer, adversarial perturbation, and cutmix. Evaluation results demonstrate
robust detection performance across most attack types, outperforming recent
pretrained foundation models. Code and model checkpoints will be released
publicly upon acceptance.

</details>


### [115] [Reinforcement Learning meets Masked Video Modeling : Trajectory-Guided Adaptive Token Selection](https://arxiv.org/abs/2505.08561)
*Ayush K. Rai,Kyle Min,Tarun Krishna,Feiyan Hu,Alan F. Smeaton,Noel E. O'Connor*

Main category: cs.CV

TL;DR: 论文提出了一种轨迹感知自适应令牌采样器（TATS），用于视频建模中的掩码选择，结合MAE框架和PPO优化策略，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视频建模中的掩码策略选择存在挑战，需要更高效且适应动态的解决方案。

Method: 提出TATS方法，结合MAE框架和PPO优化策略，动态选择运动中心令牌。

Result: 在多个基准测试中表现优异，支持高掩码率且不影响下游任务性能。

Conclusion: TATS方法高效、通用，优于现有技术。

Abstract: Masked video modeling~(MVM) has emerged as a highly effective pre-training
strategy for visual foundation models, whereby the model reconstructs masked
spatiotemporal tokens using information from visible tokens. However, a key
challenge in such approaches lies in selecting an appropriate masking strategy.
Previous studies have explored predefined masking techniques, including random
and tube-based masking, as well as approaches that leverage key motion priors,
optical flow and semantic cues from externally pre-trained models. In this
work, we introduce a novel and generalizable Trajectory-Aware Adaptive Token
Sampler (TATS), which models the motion dynamics of tokens and can be
seamlessly integrated into the masked autoencoder (MAE) framework to select
motion-centric tokens in videos. Additionally, we propose a unified training
strategy that enables joint optimization of both MAE and TATS from scratch
using Proximal Policy Optimization (PPO). We show that our model allows for
aggressive masking without compromising performance on the downstream task of
action recognition while also ensuring that the pre-training remains memory
efficient. Extensive experiments of the proposed approach across four
benchmarks, including Something-Something v2, Kinetics-400, UCF101, and HMDB51,
demonstrate the effectiveness, transferability, generalization, and efficiency
of our work compared to other state-of-the-art methods.

</details>


### [116] [Thermal Detection of People with Mobility Restrictions for Barrier Reduction at Traffic Lights Controlled Intersections](https://arxiv.org/abs/2505.08568)
*Xiao Ni,Carsten Kuehnel,Xiaoyi Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于热成像的交通信号灯系统，动态调整信号时长以服务行动不便者，并开发了YOLO-Thermal检测器，提高了热成像检测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有RGB摄像头系统忽视行动不便者需求，且在恶劣天气或隐私方面存在问题。

Method: 构建热成像数据集TD4PWMR，开发YOLO-Thermal检测器，结合特征提取和注意力机制。

Result: YOLO-Thermal优于现有检测器，系统有效提升无障碍交叉路口。

Conclusion: 热成像系统为行动不便者提供更安全、隐私友好的解决方案。

Abstract: Rapid advances in deep learning for computer vision have driven the adoption
of RGB camera-based adaptive traffic light systems to improve traffic safety
and pedestrian comfort. However, these systems often overlook the needs of
people with mobility restrictions. Moreover, the use of RGB cameras presents
significant challenges, including limited detection performance under adverse
weather or low-visibility conditions, as well as heightened privacy concerns.
To address these issues, we propose a fully automated, thermal detector-based
traffic light system that dynamically adjusts signal durations for individuals
with walking impairments or mobility burden and triggers the auditory signal
for visually impaired individuals, thereby advancing towards barrier-free
intersection for all users. To this end, we build the thermal dataset for
people with mobility restrictions (TD4PWMR), designed to capture diverse
pedestrian scenarios, particularly focusing on individuals with mobility aids
or mobility burden under varying environmental conditions, such as different
lighting, weather, and crowded urban settings. While thermal imaging offers
advantages in terms of privacy and robustness to adverse conditions, it also
introduces inherent hurdles for object detection due to its lack of color and
fine texture details and generally lower resolution of thermal images. To
overcome these limitations, we develop YOLO-Thermal, a novel variant of the
YOLO architecture that integrates advanced feature extraction and attention
mechanisms for enhanced detection accuracy and robustness in thermal imaging.
Experiments demonstrate that the proposed thermal detector outperforms existing
detectors, while the proposed traffic light system effectively enhances
barrier-free intersection. The source codes and dataset are available at
https://github.com/leon2014dresden/YOLO-THERMAL.

</details>


### [117] [ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible Long-term Tracking](https://arxiv.org/abs/2505.08581)
*Haofeng Liu,Mingqi Gao,Xuxiao Luo,Ziyue Wang,Guanyi Qin,Junde Wu,Yueming Jin*

Main category: cs.CV

TL;DR: ReSurgSAM2是一个两阶段的手术场景分割框架，结合了Segment Anything Model 2和多样性驱动的长期记忆机制，显著提升了分割和跟踪的准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的手术场景分割方法效率低且跟踪时间短，难以适应复杂的手术场景，因此需要一种更高效、长期稳定的解决方案。

Method: 采用两阶段框架：1) 使用跨模态时空Mamba进行文本引导的目标检测与分割；2) 通过可信初始帧选择和多样性驱动记忆机制实现长期跟踪。

Result: ReSurgSAM2在准确性和效率上显著优于现有方法，实时运行速度达61.2 FPS。

Conclusion: ReSurgSAM2为手术场景分割提供了高效、稳定的解决方案，适用于复杂的手术环境。

Abstract: Surgical scene segmentation is critical in computer-assisted surgery and is
vital for enhancing surgical quality and patient outcomes. Recently, referring
surgical segmentation is emerging, given its advantage of providing surgeons
with an interactive experience to segment the target object. However, existing
methods are limited by low efficiency and short-term tracking, hindering their
applicability in complex real-world surgical scenarios. In this paper, we
introduce ReSurgSAM2, a two-stage surgical referring segmentation framework
that leverages Segment Anything Model 2 to perform text-referred target
detection, followed by tracking with reliable initial frame identification and
diversity-driven long-term memory. For the detection stage, we propose a
cross-modal spatial-temporal Mamba to generate precise detection and
segmentation results. Based on these results, our credible initial frame
selection strategy identifies the reliable frame for the subsequent tracking.
Upon selecting the initial frame, our method transitions to the tracking stage,
where it incorporates a diversity-driven memory mechanism that maintains a
credible and diverse memory bank, ensuring consistent long-term tracking.
Extensive experiments demonstrate that ReSurgSAM2 achieves substantial
improvements in accuracy and efficiency compared to existing methods, operating
in real-time at 61.2 FPS. Our code and datasets will be available at
https://github.com/jinlab-imvr/ReSurgSAM2.

</details>


### [118] [A Large-scale Benchmark on Geological Fault Delineation Models: Domain Shift, Training Dynamics, Generalizability, Evaluation and Inferential Behavior](https://arxiv.org/abs/2505.08585)
*Jorge Quesada,Chen Zhou,Prithwijit Chowdhury,Mohammad Alotaibi,Ahmad Mustafa,Yusufjon Kumamnov,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.CV

TL;DR: 论文通过大规模基准测试研究，评估了地震解释中机器学习模型的泛化能力，揭示了当前微调策略的脆弱性，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 地震解释中机器学习模型的泛化能力缺乏系统性研究，数据分布差异、微调策略和评估协议不一致是主要障碍。

Method: 研究训练并评估了200多个模型，涵盖三种异构数据集，系统分析了预训练、微调和联合训练策略。

Result: 研究发现当前微调策略脆弱，存在灾难性遗忘问题，并提出了改进模型泛化能力的建议。

Conclusion: 研究为地震解释中故障描绘模型的部署提供了指导，并指出了未来发展的方向。

Abstract: Machine learning has taken a critical role in seismic interpretation
workflows, especially in fault delineation tasks. However, despite the recent
proliferation of pretrained models and synthetic datasets, the field still
lacks a systematic understanding of the generalizability limits of these models
across seismic data representing a variety of geologic, acquisition and
processing settings. Distributional shifts between different data sources,
limitations in fine-tuning strategies and labeled data accessibility, and
inconsistent evaluation protocols all represent major roadblocks in the
deployment of reliable and robust models in real-world exploration settings. In
this paper, we present the first large-scale benchmarking study explicitly
designed to provide answers and guidelines for domain shift strategies in
seismic interpretation. Our benchmark encompasses over $200$ models trained and
evaluated on three heterogeneous datasets (synthetic and real data) including
FaultSeg3D, CRACKS, and Thebe. We systematically assess pretraining,
fine-tuning, and joint training strategies under varying degrees of domain
shift. Our analysis highlights the fragility of current fine-tuning practices,
the emergence of catastrophic forgetting, and the challenges of interpreting
performance in a systematic manner. We establish a robust experimental baseline
to provide insights into the tradeoffs inherent to current fault delineation
workflows, and shed light on directions for developing more generalizable,
interpretable and effective machine learning models for seismic interpretation.
The insights and analyses reported provide a set of guidelines on the
deployment of fault delineation models within seismic interpretation workflows.

</details>


### [119] [PrePrompt: Predictive prompting for class incremental learning](https://arxiv.org/abs/2505.08586)
*Libo Huang,Zhulin An,Chuanguang Yang,Boyu Diao,Fei Wang,Yan Zeng,Zhifeng Hao,Yongjun Xu*

Main category: cs.CV

TL;DR: 论文提出了一种名为PrePrompt的新框架，通过预测任务特定提示来改进基于预训练模型的类增量学习（CIL），解决了现有相关性方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于相关性策略的CIL方法难以用少量可训练提示拟合所有任务的特征空间，因此需要一种更有效的方法。

Method: PrePrompt将CIL分解为两阶段预测框架：任务特定提示预测和标签预测，并通过特征翻译动态平衡稳定性和可塑性。

Result: 实验表明，PrePrompt在多个基准测试中优于现有的基于提示的CIL方法。

Conclusion: PrePrompt通过预测任务特定提示和动态特征翻译，显著提升了类增量学习的性能。

Abstract: Class Incremental Learning (CIL) based on pre-trained models offers a
promising direction for open-world continual learning. Existing methods
typically rely on correlation-based strategies, where an image's classification
feature is used as a query to retrieve the most related key prompts and select
the corresponding value prompts for training. However, these approaches face an
inherent limitation: fitting the entire feature space of all tasks with only a
few trainable prompts is fundamentally challenging. We propose Predictive
Prompting (PrePrompt), a novel CIL framework that circumvents correlation-based
limitations by leveraging pre-trained models' natural classification ability to
predict task-specific prompts. Specifically, PrePrompt decomposes CIL into a
two-stage prediction framework: task-specific prompt prediction followed by
label prediction. While theoretically appealing, this framework risks bias
toward recent classes due to missing historical data for older classifier
calibration. PrePrompt then mitigates this by incorporating feature
translation, dynamically balancing stability and plasticity. Experiments across
multiple benchmarks demonstrate PrePrompt's superiority over state-of-the-art
prompt-based CIL methods. The code will be released upon acceptance.

</details>


### [120] [MESSI: A Multi-Elevation Semantic Segmentation Image Dataset of an Urban Environment](https://arxiv.org/abs/2505.08589)
*Barak Pinkovich,Boaz Matalon,Ehud Rivlin,Hector Rotstein*

Main category: cs.CV

TL;DR: MESSI数据集包含2525张无人机拍摄的密集城市环境图像，支持多海拔语义分割研究，并公开作为基准。


<details>
  <summary>Details</summary>
Motivation: 研究深度对语义分割的影响，并覆盖无人机3D飞行中的视觉多样性。

Method: 使用多种神经网络模型进行语义分割，并提供图像的位置、方向及相机参数。

Result: MESSI数据集可用于训练深度神经网络，支持语义分割及其他应用。

Conclusion: MESSI将公开作为无人机图像语义分割的评估基准。

Abstract: This paper presents a Multi-Elevation Semantic Segmentation Image (MESSI)
dataset comprising 2525 images taken by a drone flying over dense urban
environments. MESSI is unique in two main features. First, it contains images
from various altitudes, allowing us to investigate the effect of depth on
semantic segmentation. Second, it includes images taken from several different
urban regions (at different altitudes). This is important since the variety
covers the visual richness captured by a drone's 3D flight, performing
horizontal and vertical maneuvers. MESSI contains images annotated with
location, orientation, and the camera's intrinsic parameters and can be used to
train a deep neural network for semantic segmentation or other applications of
interest (e.g., localization, navigation, and tracking). This paper describes
the dataset and provides annotation details. It also explains how semantic
segmentation was performed using several neural network models and shows
several relevant statistics. MESSI will be published in the public domain to
serve as an evaluation benchmark for semantic segmentation using images
captured by a drone or similar vehicle flying over a dense urban environment.

</details>


### [121] [Rejoining fragmented ancient bamboo slips with physics-driven deep learning](https://arxiv.org/abs/2505.08601)
*Jinchi Zhu,Zhou Zhao,Hailong Lei,Xiaoguang Wang,Jialiang Lu,Jing Li,Qianqian Tang,Jiachen Shen,Gui-Song Xia,Bo Du,Yongchao Xu*

Main category: cs.CV

TL;DR: WisePanda是一个基于物理原理的深度学习框架，用于重新拼接破碎的竹简，显著提高了匹配准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 竹简是记录东亚古代文明的重要媒介，但许多出土的竹简破碎成不规则碎片，拼接困难。

Method: 利用断裂和材料劣化的物理原理生成合成训练数据，训练匹配网络，无需手动配对样本。

Result: Top-50匹配准确率从36%提升至52%，拼接效率提高约20倍。

Conclusion: 将物理原理融入深度学习模型显著提升了性能，为古代文物修复提供了新范式。

Abstract: Bamboo slips are a crucial medium for recording ancient civilizations in East
Asia, and offers invaluable archaeological insights for reconstructing the Silk
Road, studying material culture exchanges, and global history. However, many
excavated bamboo slips have been fragmented into thousands of irregular pieces,
making their rejoining a vital yet challenging step for understanding their
content. Here we introduce WisePanda, a physics-driven deep learning framework
designed to rejoin fragmented bamboo slips. Based on the physics of fracture
and material deterioration, WisePanda automatically generates synthetic
training data that captures the physical properties of bamboo fragmentations.
This approach enables the training of a matching network without requiring
manually paired samples, providing ranked suggestions to facilitate the
rejoining process. Compared to the leading curve matching method, WisePanda
increases Top-50 matching accuracy from 36\% to 52\%. Archaeologists using
WisePanda have experienced substantial efficiency improvements (approximately
20 times faster) when rejoining fragmented bamboo slips. This research
demonstrates that incorporating physical principles into deep learning models
can significantly enhance their performance, transforming how archaeologists
restore and study fragmented artifacts. WisePanda provides a new paradigm for
addressing data scarcity in ancient artifact restoration through physics-driven
machine learning.

</details>


### [122] [Unsupervised Out-of-Distribution Detection in Medical Imaging Using Multi-Exit Class Activation Maps and Feature Masking](https://arxiv.org/abs/2505.08604)
*Yu-Jen Chen,Xueyang Li,Yiyu Shi,Tsung-Yi Ho*

Main category: cs.CV

TL;DR: 提出了一种基于多出口类激活图（MECAM）的无监督OOD检测框架，通过特征掩码和多分辨率CAM提升检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 观察到ID数据的CAM通常集中在预测相关区域，而OOD数据缺乏这种聚焦激活，利用这一差异进行检测。

Method: 使用多出口网络结合不同分辨率和深度的CAM，通过特征掩码区分ID和OOD数据。

Result: 在多个ID和OOD数据集上验证了MECAM的有效性，优于现有方法。

Conclusion: 多出口网络和特征掩码为医学影像中的无监督OOD检测提供了新思路，有望提升临床模型的可靠性和可解释性。

Abstract: Out-of-distribution (OOD) detection is essential for ensuring the reliability
of deep learning models in medical imaging applications. This work is motivated
by the observation that class activation maps (CAMs) for in-distribution (ID)
data typically emphasize regions that are highly relevant to the model's
predictions, whereas OOD data often lacks such focused activations. By masking
input images with inverted CAMs, the feature representations of ID data undergo
more substantial changes compared to those of OOD data, offering a robust
criterion for differentiation. In this paper, we introduce a novel unsupervised
OOD detection framework, Multi-Exit Class Activation Map (MECAM), which
leverages multi-exit CAMs and feature masking. By utilizing mult-exit networks
that combine CAMs from varying resolutions and depths, our method captures both
global and local feature representations, thereby enhancing the robustness of
OOD detection. We evaluate MECAM on multiple ID datasets, including ISIC19 and
PathMNIST, and test its performance against three medical OOD datasets, RSNA
Pneumonia, COVID-19, and HeadCT, and one natural image OOD dataset, iSUN.
Comprehensive comparisons with state-of-the-art OOD detection methods validate
the effectiveness of our approach. Our findings emphasize the potential of
multi-exit networks and feature masking for advancing unsupervised OOD
detection in medical imaging, paving the way for more reliable and
interpretable models in clinical practice.

</details>


### [123] [Leveraging Multi-Modal Information to Enhance Dataset Distillation](https://arxiv.org/abs/2505.08605)
*Zhe Li,Hadrien Reynaud,Bernhard Kainz*

Main category: cs.CV

TL;DR: 论文提出两种改进数据集蒸馏的方法：基于文本的监督和对象中心掩码，通过结合文本信息和优化对象级特征提升蒸馏数据集质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注视觉表示优化，但结合多模态信息和对象级细化可以显著提升蒸馏数据集的质量。

Method: 引入两种策略：1）文本特征融合（特征拼接和文本匹配）；2）对象中心掩码（掩码特征对齐损失和掩码梯度匹配损失）。

Result: 实验表明，结合文本指导和对象中心掩码能显著提升蒸馏数据集的性能。

Conclusion: 多模态信息和对象级优化是提升数据集蒸馏效果的关键。

Abstract: Dataset distillation aims to create a compact and highly representative
synthetic dataset that preserves the knowledge of a larger real dataset. While
existing methods primarily focus on optimizing visual representations,
incorporating additional modalities and refining object-level information can
significantly improve the quality of distilled datasets. In this work, we
introduce two key enhancements to dataset distillation: caption-guided
supervision and object-centric masking. To integrate textual information, we
propose two strategies for leveraging caption features: the feature
concatenation, where caption embeddings are fused with visual features at the
classification stage, and caption matching, which introduces a caption-based
alignment loss during training to ensure semantic coherence between real and
synthetic data. Additionally, we apply segmentation masks to isolate target
objects and remove background distractions, introducing two loss functions
designed for object-centric learning: masked feature alignment loss and masked
gradient matching loss. Comprehensive evaluations demonstrate that integrating
caption-based guidance and object-centric masking enhances dataset
distillation, leading to synthetic datasets that achieve superior performance
on downstream tasks.

</details>


### [124] [Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World](https://arxiv.org/abs/2505.08607)
*Yuran Wang,Yingping Liang,Ying Fu*

Main category: cs.CV

TL;DR: 论文提出BooSTer框架，利用视觉基础模型和大规模混合图像源（合成、真实和单视图图像）解决立体匹配中标注数据稀缺和域差距问题。


<details>
  <summary>Details</summary>
Motivation: 立体匹配方法依赖密集像素级标注数据，但真实数据标注耗时且稀缺，合成与真实图像间存在域差距。

Method: 结合单目深度估计和扩散模型生成立体匹配数据；利用伪单目深度标签和动态尺度不变损失进行监督；引入视觉基础模型提取特征。

Result: 在基准数据集上表现优异，显著提升精度，尤其在标注数据有限和域偏移场景中。

Conclusion: BooSTer框架有效解决了标注数据稀缺和域差距问题，提升了立体匹配的精度和泛化能力。

Abstract: Stereo matching methods rely on dense pixel-wise ground truth labels, which
are laborious to obtain, especially for real-world datasets. The scarcity of
labeled data and domain gaps between synthetic and real-world images also pose
notable challenges. In this paper, we propose a novel framework,
\textbf{BooSTer}, that leverages both vision foundation models and large-scale
mixed image sources, including synthetic, real, and single-view images. First,
to fully unleash the potential of large-scale single-view images, we design a
data generation strategy combining monocular depth estimation and diffusion
models to generate dense stereo matching data from single-view images. Second,
to tackle sparse labels in real-world datasets, we transfer knowledge from
monocular depth estimation models, using pseudo-mono depth labels and a dynamic
scale- and shift-invariant loss for additional supervision. Furthermore, we
incorporate vision foundation model as an encoder to extract robust and
transferable features, boosting accuracy and generalization. Extensive
experiments on benchmark datasets demonstrate the effectiveness of our
approach, achieving significant improvements in accuracy over existing methods,
particularly in scenarios with limited labeled data and domain shifts.

</details>


### [125] [WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks](https://arxiv.org/abs/2505.08614)
*Ziyuan He,Zhiqing Guo,Liejun Wang,Gaobo Yang,Yunfeng Diao,Dan Ma*

Main category: cs.CV

TL;DR: WaveGuard是一种主动水印框架，通过频域嵌入和图结构一致性增强鲁棒性和不可感知性，用于应对Deepfake技术的隐私和身份盗窃风险。


<details>
  <summary>Details</summary>
Motivation: Deepfake技术带来隐私侵犯和身份盗窃等风险，需要一种更鲁棒且不可感知的水印方法。

Method: 使用双树复小波变换（DT-CWT）将水印嵌入高频子带，并采用结构一致性图神经网络（SC-GNN）保持视觉质量，同时设计了注意力模块优化嵌入精度。

Result: 在人脸交换和重演任务中，WaveGuard在鲁棒性和视觉质量上均优于现有方法。

Conclusion: WaveGuard为Deepfake威胁提供了一种有效的解决方案，代码已开源。

Abstract: Deepfake technology poses increasing risks such as privacy invasion and
identity theft. To address these threats, we propose WaveGuard, a proactive
watermarking framework that enhances robustness and imperceptibility via
frequency-domain embedding and graph-based structural consistency.
Specifically, we embed watermarks into high-frequency sub-bands using Dual-Tree
Complex Wavelet Transform (DT-CWT) and employ a Structural Consistency Graph
Neural Network (SC-GNN) to preserve visual quality. We also design an attention
module to refine embedding precision. Experimental results on face swap and
reenactment tasks demonstrate that WaveGuard outperforms state-of-the-art
methods in both robustness and visual quality. Code is available at
https://github.com/vpsg-research/WaveGuard.

</details>


### [126] [OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning](https://arxiv.org/abs/2505.08617)
*Zhaochen Su,Linjie Li,Mingyang Song,Yunzhuo Hao,Zhengyuan Yang,Jun Zhang,Guanjie Chen,Jiawei Gu,Juntao Li,Xiaoye Qu,Yu Cheng*

Main category: cs.CV

TL;DR: OpenThinkIMG是一个开源框架，用于增强大型视觉语言模型（LVLMs）的动态视觉工具调用能力，通过强化学习（V-ToolRL）显著提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏标准化基础设施，阻碍了视觉工具的集成和交互数据的生成，限制了LVLMs的动态问题解决能力。

Method: 提出OpenThinkIMG框架，包含标准化工具接口、轨迹生成和训练环境；并设计V-ToolRL强化学习框架，优化工具调用策略。

Result: 在图表推理任务中，V-ToolRL训练的模型比监督学习基线表现更好（+12.7分），甚至超越GPT-4.1（+8.68分）。

Conclusion: OpenThinkIMG为动态视觉推理提供了基础框架，有望推动AI代理的视觉认知能力发展。

Abstract: While humans can flexibly leverage interactive visual cognition for complex
problem-solving, enabling Large Vision-Language Models (LVLMs) to learn
similarly adaptive behaviors with visual tools remains challenging. A
significant hurdle is the current lack of standardized infrastructure, which
hinders integrating diverse tools, generating rich interaction data, and
training robust agents effectively. To address these gaps, we introduce
OpenThinkIMG, the first open-source, comprehensive end-to-end framework for
tool-augmented LVLMs. It features standardized vision tool interfaces, scalable
trajectory generation for policy initialization, and a flexible training
environment. Furthermore, considering supervised fine-tuning (SFT) on static
demonstrations offers limited policy generalization for dynamic tool
invocation, we propose a novel reinforcement learning (RL) framework V-ToolRL
to train LVLMs to learn adaptive policies for invoking external vision tools.
V-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies
by directly optimizing for task success using feedback from tool interactions.
We empirically validate V-ToolRL on challenging chart reasoning tasks. Our
RL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its
SFT-initialized counterpart (+28.83 points) and surpasses established
supervised tool-learning baselines like Taco and CogCom by an average of +12.7
points. Notably, it also surpasses prominent closed-source models like GPT-4.1
by +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational
framework for advancing dynamic, tool-augmented visual reasoning, helping the
community develop AI agents that can genuinely "think with images".

</details>


### [127] [DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian Splatting](https://arxiv.org/abs/2505.08644)
*Holly Dinkel,Marcel Büsching,Alberta Longhini,Brian Coltin,Trey Smith,Danica Kragic,Mårten Björkman,Timothy Bretl*

Main category: cs.CV

TL;DR: DLO-Splatting算法通过多视角RGB图像和夹爪状态信息，预测-更新滤波估计可变形线性物体的3D形状。


<details>
  <summary>Details</summary>
Motivation: 现有视觉方法在复杂场景（如打结）中表现不佳，需结合动态模型和视觉优化。

Method: 使用基于位置的动力学模型预测形状，并通过3D高斯Splatting渲染损失优化对齐视觉观测。

Result: 初步实验在打结场景中展示了优于纯视觉方法的效果。

Conclusion: DLO-Splatting为复杂场景中的可变形物体形状估计提供了有效解决方案。

Abstract: This work presents DLO-Splatting, an algorithm for estimating the 3D shape of
Deformable Linear Objects (DLOs) from multi-view RGB images and gripper state
information through prediction-update filtering. The DLO-Splatting algorithm
uses a position-based dynamics model with shape smoothness and rigidity
dampening corrections to predict the object shape. Optimization with a 3D
Gaussian Splatting-based rendering loss iteratively renders and refines the
prediction to align it with the visual observations in the update step. Initial
experiments demonstrate promising results in a knot tying scenario, which is
challenging for existing vision-only methods.

</details>


### [128] [SkillFormer: Unified Multi-View Video Understanding for Proficiency Estimation](https://arxiv.org/abs/2505.08665)
*Edoardo Bianchi,Antonio Liotta*

Main category: cs.CV

TL;DR: SkillFormer是一种参数高效的架构，用于从第一人称和第三人称视频中统一评估多视角技能水平，通过跨视角融合模块和低秩适应技术显著提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 评估复杂活动中的人类技能水平在体育、康复和训练中有广泛应用，但现有方法在多视角融合和计算效率上存在不足。

Method: 基于TimeSformer，SkillFormer引入CrossViewFusion模块，结合多头交叉注意力、可学习门控和自适应自校准，并利用低秩适应技术微调少量参数。

Result: 在EgoExo4D数据集上，SkillFormer在多视角设置中达到最先进精度，参数减少4.5倍，训练周期减少3.75倍。

Conclusion: SkillFormer证明了多视角融合在细粒度技能评估中的价值，同时显著提升了计算效率。

Abstract: Assessing human skill levels in complex activities is a challenging problem
with applications in sports, rehabilitation, and training. In this work, we
present SkillFormer, a parameter-efficient architecture for unified multi-view
proficiency estimation from egocentric and exocentric videos. Building on the
TimeSformer backbone, SkillFormer introduces a CrossViewFusion module that
fuses view-specific features using multi-head cross-attention, learnable
gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to
fine-tune only a small subset of parameters, significantly reducing training
costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves
state-of-the-art accuracy in multi-view settings while demonstrating remarkable
computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer
training epochs than prior baselines. It excels in multiple structured tasks,
confirming the value of multi-view integration for fine-grained skill
assessment.

</details>


### [129] [Calibration and Uncertainty for multiRater Volume Assessment in multiorgan Segmentation (CURVAS) challenge results](https://arxiv.org/abs/2505.08685)
*Meritxell Riera-Marin,Sikha O K,Julia Rodriguez-Comas,Matthias Stefan May,Zhaohong Pan,Xiang Zhou,Xiaokun Liang,Franciskus Xaverius Erick,Andrea Prenner,Cedric Hemon,Valentin Boussot,Jean-Louis Dillenseger,Jean-Claude Nunes,Abdul Qayyum,Moona Mazher,Steven A Niederer,Kaisar Kushibar,Carlos Martin-Isla,Petia Radeva,Karim Lekadir,Theodore Barfoot,Luis C. Garcia Peraza Herrera,Ben Glocker,Tom Vercauteren,Lucas Gago,Justin Englemann,Joy-Marie Kleiss,Anton Aubanell,Andreu Antolin,Javier Garcia-Lopez,Miguel A. Gonzalez Ballester,Adrian Galdran*

Main category: cs.CV

TL;DR: CURVAS挑战赛探讨了多标注者在医学图像分割中的重要性，强调校准和不确定性估计对模型可靠性的关键作用。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中标注变异性、校准和不确定性估计的挑战，以提高模型的临床适用性。

Method: 七支团队提交了多种DL模型，使用DSC、ECE和CRPS等指标评估，结合共识和非共识标注评估模型表现。

Result: 校准良好的模型表现更优，预训练模型在非标准解剖结构中更稳健，最佳模型实现了高DSC和良好校准。

Conclusion: 多标注者标注、校准评估和不确定性感知是开发可靠医学图像分割模型的关键。

Abstract: Deep learning (DL) has become the dominant approach for medical image
segmentation, yet ensuring the reliability and clinical applicability of these
models requires addressing key challenges such as annotation variability,
calibration, and uncertainty estimation. This is why we created the Calibration
and Uncertainty for multiRater Volume Assessment in multiorgan Segmentation
(CURVAS), which highlights the critical role of multiple annotators in
establishing a more comprehensive ground truth, emphasizing that segmentation
is inherently subjective and that leveraging inter-annotator variability is
essential for robust model evaluation. Seven teams participated in the
challenge, submitting a variety of DL models evaluated using metrics such as
Dice Similarity Coefficient (DSC), Expected Calibration Error (ECE), and
Continuous Ranked Probability Score (CRPS). By incorporating consensus and
dissensus ground truth, we assess how DL models handle uncertainty and whether
their confidence estimates align with true segmentation performance. Our
findings reinforce the importance of well-calibrated models, as better
calibration is strongly correlated with the quality of the results.
Furthermore, we demonstrate that segmentation models trained on diverse
datasets and enriched with pre-trained knowledge exhibit greater robustness,
particularly in cases deviating from standard anatomical structures. Notably,
the best-performing models achieved high DSC and well-calibrated uncertainty
estimates. This work underscores the need for multi-annotator ground truth,
thorough calibration assessments, and uncertainty-aware evaluations to develop
trustworthy and clinically reliable DL-based medical image segmentation models.

</details>


### [130] [SPAST: Arbitrary Style Transfer with Style Priors via Pre-trained Large-scale Model](https://arxiv.org/abs/2505.08695)
*Zhanjie Zhang,Quanwei Zhang,Junsheng Luan,Mengyuan Yang,Yun Wang,Lei Zhao*

Main category: cs.CV

TL;DR: SPAST框架结合局部-全局窗口大小风格化模块和风格先验损失，实现高质量风格迁移且减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么生成低质量图像，要么推理时间长且难以保留内容结构。

Method: 设计LGWSSM模块融合风格与内容特征，引入风格先验损失以利用预训练模型的先验知识。

Result: 实验证明SPAST能生成高质量风格化图像且推理时间更短。

Conclusion: SPAST在质量和效率上优于现有方法。

Abstract: Given an arbitrary content and style image, arbitrary style transfer aims to
render a new stylized
  image which preserves the content image's structure and possesses the style
image's style. Existing
  arbitrary style transfer methods are based on either small models or
pre-trained large-scale models.
  The small model-based methods fail to generate high-quality stylized images,
bringing artifacts and
  disharmonious patterns. The pre-trained large-scale model-based methods can
generate high-quality
  stylized images but struggle to preserve the content structure and cost long
inference time. To this
  end, we propose a new framework, called SPAST, to generate high-quality
stylized images with
  less inference time. Specifically, we design a novel Local-global Window Size
Stylization Module
  (LGWSSM)tofuse style features into content features. Besides, we introduce a
novel style prior loss,
  which can dig out the style priors from a pre-trained large-scale model into
the SPAST and motivate
  the SPAST to generate high-quality stylized images with short inference
time.We conduct abundant
  experiments to verify that our proposed method can generate high-quality
stylized images and less
  inference time compared with the SOTA arbitrary style transfer methods.

</details>


### [131] [Controllable Image Colorization with Instance-aware Texts and Masks](https://arxiv.org/abs/2505.08705)
*Yanru An,Ling Gui,Qiang Hu,Chunlei Cai,Tianxiao Ye,Xiaoyun Zhang,Yanfeng Wang*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的实例感知图像着色方法MT-Color，通过像素级掩码注意力机制和实例掩码文本引导模块解决颜色溢出和绑定错误问题，并引入多实例采样策略和专用数据集GPT-color。


<details>
  <summary>Details</summary>
Motivation: 当前主流图像着色模型存在颜色溢出和绑定错误问题，且无法实现实例级着色。

Method: 设计了像素级掩码注意力机制和实例掩码文本引导模块，采用多实例采样策略，并构建了专用数据集GPT-color。

Result: 定性和定量实验表明，模型和数据集优于先前方法。

Conclusion: MT-Color方法在实例级图像着色任务中表现优异，解决了颜色溢出和绑定错误问题。

Abstract: Recently, the application of deep learning in image colorization has received
widespread attention. The maturation of diffusion models has further advanced
the development of image colorization models. However, current mainstream image
colorization models still face issues such as color bleeding and color binding
errors, and cannot colorize images at the instance level. In this paper, we
propose a diffusion-based colorization method MT-Color to achieve precise
instance-aware colorization with use-provided guidance. To tackle color
bleeding issue, we design a pixel-level mask attention mechanism that
integrates latent features and conditional gray image features through
cross-attention. We use segmentation masks to construct cross-attention masks,
preventing pixel information from exchanging between different instances. We
also introduce an instance mask and text guidance module that extracts instance
masks and text representations of each instance, which are then fused with
latent features through self-attention, utilizing instance masks to form
self-attention masks to prevent instance texts from guiding the colorization of
other areas, thus mitigating color binding errors. Furthermore, we apply a
multi-instance sampling strategy, which involves sampling each instance region
separately and then fusing the results. Additionally, we have created a
specialized dataset for instance-level colorization tasks, GPT-color, by
leveraging large visual language models on existing image datasets. Qualitative
and quantitative experiments show that our model and dataset outperform
previous methods and datasets.

</details>


### [132] [TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series](https://arxiv.org/abs/2505.08723)
*Xiaolei Qin,Di Wang,Jing Zhang,Fengxiang Wang,Xin Su,Bo Du,Liangpei Zhang*

Main category: cs.CV

TL;DR: TiMo是一种新型的分层视觉Transformer基础模型，专为卫星图像时间序列（SITS）分析设计，通过动态捕捉多尺度时空关系，显著提升了任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有时空基础模型依赖普通视觉Transformer，未能显式捕捉多尺度时空关系，限制了其在下游任务中的效果。

Method: 提出TiMo模型，引入时空陀螺仪注意力机制，动态捕捉多尺度时空模式；使用MillionST数据集进行预训练。

Result: 在多项任务（如森林砍伐监测、土地覆盖分割等）中，TiMo优于现有方法。

Conclusion: TiMo通过改进的注意力机制和大规模预训练，显著提升了SITS分析的性能。

Abstract: Satellite image time series (SITS) provide continuous observations of the
Earth's surface, making them essential for applications such as environmental
management and disaster assessment. However, existing spatiotemporal foundation
models rely on plain vision transformers, which encode entire temporal
sequences without explicitly capturing multiscale spatiotemporal relationships
between land objects. This limitation hinders their effectiveness in downstream
tasks. To overcome this challenge, we propose TiMo, a novel hierarchical vision
transformer foundation model tailored for SITS analysis. At its core, we
introduce a spatiotemporal gyroscope attention mechanism that dynamically
captures evolving multiscale patterns across both time and space. For
pre-training, we curate MillionST, a large-scale dataset of one million images
from 100,000 geographic locations, each captured across 10 temporal phases over
five years, encompassing diverse geospatial changes and seasonal variations.
Leveraging this dataset, we adapt masked image modeling to pre-train TiMo,
enabling it to effectively learn and encode generalizable spatiotemporal
representations.Extensive experiments across multiple spatiotemporal
tasks-including deforestation monitoring, land cover segmentation, crop type
classification, and flood detection-demonstrate TiMo's superiority over
state-of-the-art methods. Code, model, and dataset will be released at
https://github.com/MiliLab/TiMo.

</details>


### [133] [Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving](https://arxiv.org/abs/2505.08725)
*Zongchuang Zhao,Haoyu Fu,Dingkang Liang,Xin Zhou,Dingyuan Zhang,Hongwei Xie,Bing Wang,Xiang Bai*

Main category: cs.CV

TL;DR: 论文提出NuInteract数据集和DriveMonkey框架，解决现有大视觉语言模型在3D场景理解中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型（LVLMs）在自动驾驶场景中缺乏全面的场景理解能力，尤其是2D与3D的映射关系及3D定位与指令理解的结合不足。

Method: 引入NuInteract数据集（150万对多视角图像语言对），并提出DriveMonkey框架，通过可学习查询集成空间处理器。

Result: DriveMonkey在3D视觉定位任务中表现优于通用LVLMs，提升9.86%。

Conclusion: NuInteract和DriveMonkey为3D场景理解提供了有效解决方案，代码和数据集将开源。

Abstract: The Large Visual-Language Models (LVLMs) have significantly advanced image
understanding. Their comprehension and reasoning capabilities enable promising
applications in autonomous driving scenarios. However, existing research
typically focuses on front-view perspectives and partial objects within scenes,
struggling to achieve comprehensive scene understanding. Meanwhile, existing
LVLMs suffer from the lack of mapping relationship between 2D and 3D and
insufficient integration of 3D object localization and instruction
understanding. To tackle these limitations, we first introduce NuInteract, a
large-scale dataset with over 1.5M multi-view image language pairs spanning
dense scene captions and diverse interactive tasks. Furthermore, we propose
DriveMonkey, a simple yet effective framework that seamlessly integrates LVLMs
with a spatial processor using a series of learnable queries. The spatial
processor, designed as a plug-and-play component, can be initialized with
pre-trained 3D detectors to improve 3D perception. Our experiments show that
DriveMonkey outperforms general LVLMs, especially achieving a 9.86% notable
improvement on the 3D visual grounding task. The dataset and code will be
released at https://github.com/zc-zhao/DriveMonkey.

</details>


### [134] [Advancing Food Nutrition Estimation via Visual-Ingredient Feature Fusion](https://arxiv.org/abs/2505.08747)
*Huiyan Qi,Bin Zhu,Chong-Wah Ngo,Jingjing Chen,Ee-Peng Lim*

Main category: cs.CV

TL;DR: FastFood数据集和VIF²方法通过融合视觉与成分特征提升营养估计准确性。


<details>
  <summary>Details</summary>
Motivation: 营养估计对健康饮食至关重要，但缺乏带营养标注的数据集限制了进展。

Method: 提出VIF²方法，结合视觉与成分特征，并通过数据增强优化成分预测。

Result: 在FastFood和Nutrition5k数据集上验证了方法的有效性。

Conclusion: 成分信息对营养估计至关重要，VIF²方法在不同骨干网络中表现优异。

Abstract: Nutrition estimation is an important component of promoting healthy eating
and mitigating diet-related health risks. Despite advances in tasks such as
food classification and ingredient recognition, progress in nutrition
estimation is limited due to the lack of datasets with nutritional annotations.
To address this issue, we introduce FastFood, a dataset with 84,446 images
across 908 fast food categories, featuring ingredient and nutritional
annotations. In addition, we propose a new model-agnostic Visual-Ingredient
Feature Fusion (VIF$^2$) method to enhance nutrition estimation by integrating
visual and ingredient features. Ingredient robustness is improved through
synonym replacement and resampling strategies during training. The
ingredient-aware visual feature fusion module combines ingredient features and
visual representation to achieve accurate nutritional prediction. During
testing, ingredient predictions are refined using large multimodal models by
data augmentation and majority voting. Our experiments on both FastFood and
Nutrition5k datasets validate the effectiveness of our proposed method built in
different backbones (e.g., Resnet, InceptionV3 and ViT), which demonstrates the
importance of ingredient information in nutrition estimation.
https://huiyanqi.github.io/fastfood-nutrition-estimation/.

</details>


### [135] [Towards Autonomous UAV Visual Object Search in City Space: Benchmark and Agentic Methodology](https://arxiv.org/abs/2505.08765)
*Yatai Ji,Zhengqiu Zhu,Yong Zhao,Beidan Liu,Chen Gao,Yihao Zhao,Sihang Qiu,Yue Hu,Quanjun Yin,Yong Li*

Main category: cs.CV

TL;DR: 论文提出了CityAVOS数据集和PRPSearcher方法，用于解决无人机在复杂城市环境中自主搜索目标物体的挑战。PRPSearcher通过多模态大语言模型模拟人类三层认知，显著提升了搜索成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂城市环境中表现不佳，主要由于冗余语义处理、相似物体区分和探索-利用困境。为解决这些问题，作者提出了首个AVOS任务基准数据集和新型搜索方法。

Method: PRPSearcher通过构建三种专用地图（动态语义地图、3D认知地图和不确定性地图）模拟人类认知，并结合去噪机制和IPT提示机制优化搜索。

Result: 实验显示PRPSearcher在成功率和搜索效率上显著优于基线方法（平均提升37.69% SR和28.96% SPL）。

Conclusion: 尽管PRPSearcher表现优异，但与人类相比仍存在差距，未来需进一步提升语义推理和空间探索能力。

Abstract: Aerial Visual Object Search (AVOS) tasks in urban environments require
Unmanned Aerial Vehicles (UAVs) to autonomously search for and identify target
objects using visual and textual cues without external guidance. Existing
approaches struggle in complex urban environments due to redundant semantic
processing, similar object distinction, and the exploration-exploitation
dilemma. To bridge this gap and support the AVOS task, we introduce CityAVOS,
the first benchmark dataset for autonomous search of common urban objects. This
dataset comprises 2,420 tasks across six object categories with varying
difficulty levels, enabling comprehensive evaluation of UAV agents' search
capabilities. To solve the AVOS tasks, we also propose PRPSearcher
(Perception-Reasoning-Planning Searcher), a novel agentic method powered by
multi-modal large language models (MLLMs) that mimics human three-tier
cognition. Specifically, PRPSearcher constructs three specialized maps: an
object-centric dynamic semantic map enhancing spatial perception, a 3D
cognitive map based on semantic attraction values for target reasoning, and a
3D uncertainty map for balanced exploration-exploitation search. Also, our
approach incorporates a denoising mechanism to mitigate interference from
similar objects and utilizes an Inspiration Promote Thought (IPT) prompting
mechanism for adaptive action planning. Experimental results on CityAVOS
demonstrate that PRPSearcher surpasses existing baselines in both success rate
and search efficiency (on average: +37.69% SR, +28.96% SPL, -30.69% MSS, and
-46.40% NE). While promising, the performance gap compared to humans highlights
the need for better semantic reasoning and spatial exploration capabilities in
AVOS tasks. This work establishes a foundation for future advances in embodied
target search. Dataset and source code are available at
https://anonymous.4open.science/r/CityAVOS-3DF8.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [136] [An Optimized Evacuation Plan for an Active-Shooter Situation Constrained by Network Capacity](https://arxiv.org/abs/2505.07830)
*Joseph Lavalle-Rivera,Aniirudh Ramesh,Subhadeep Chakraborty*

Main category: cs.AI

TL;DR: 论文提出了一种多路径优化算法，用于公共枪击事件中的紧急疏散，显著减少伤亡和拥挤。


<details>
  <summary>Details</summary>
Motivation: 公共枪击事件频发，疏散决策在高压和缺乏实时信息下易出错，亟需优化算法。

Method: 开发了一种多路径路由优化算法，考虑路径容量，避免拥挤和瓶颈。

Result: 算法减少总伤亡34.16%（相比无容量约束算法）和53.3%（相比专家建议策略），关键节点拥挤减少50%。

Conclusion: 多路径优化算法能有效提升疏散效率，显著降低伤亡和拥挤。

Abstract: A total of more than 3400 public shootings have occurred in the United States
between 2016 and 2022. Among these, 25.1% of them took place in an educational
institution, 29.4% at the workplace including office buildings, 19.6% in retail
store locations, and 13.4% in restaurants and bars. During these critical
scenarios, making the right decisions while evacuating can make the difference
between life and death. However, emergency evacuation is intensely stressful,
which along with the lack of verifiable real-time information may lead to fatal
incorrect decisions. To tackle this problem, we developed a multi-route routing
optimization algorithm that determines multiple optimal safe routes for each
evacuee while accounting for available capacity along the route, thus reducing
the threat of crowding and bottlenecking. Overall, our algorithm reduces the
total casualties by 34.16% and 53.3%, compared to our previous routing
algorithm without capacity constraints and an expert-advised routing strategy
respectively. Further, our approach to reduce crowding resulted in an
approximate 50% reduction in occupancy in key bottlenecking nodes compared to
both of the other evacuation algorithms.

</details>


### [137] [RAN Cortex: Memory-Augmented Intelligence for Context-Aware Decision-Making in AI-Native Networks](https://arxiv.org/abs/2505.07842)
*Sebastian Barros*

Main category: cs.AI

TL;DR: 提出RAN Cortex架构，通过记忆增强AI决策系统，解决RAN中无状态决策的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有RAN中的AI模块（如xApps和rApps）缺乏记忆能力，无法利用历史事件优化决策。

Method: 设计包含上下文编码器、向量存储、召回引擎和策略接口的模块化架构。

Result: 通过用例（如体育场流量缓解和无人机走廊移动管理）展示记忆增强的决策优势。

Conclusion: RAN Cortex为AI原生RAN设计引入记忆机制，提升适应性和连续性。

Abstract: As Radio Access Networks (RAN) evolve toward AI-native architectures,
intelligent modules such as xApps and rApps are expected to make increasingly
autonomous decisions across scheduling, mobility, and resource management
domains. However, these agents remain fundamentally stateless, treating each
decision as isolated, lacking any persistent memory of prior events or
outcomes. This reactive behavior constrains optimization, especially in
environments where network dynamics exhibit episodic or recurring patterns. In
this work, we propose RAN Cortex, a memory-augmented architecture that enables
contextual recall in AI-based RAN decision systems. RAN Cortex introduces a
modular layer composed of four elements: a context encoder that transforms
network state into high-dimensional embeddings, a vector-based memory store of
past network episodes, a recall engine to retrieve semantically similar
situations, and a policy interface that supplies historical context to AI
agents in real time or near-real time. We formalize the retrieval-augmented
decision problem in the RAN, present a system architecture compatible with
O-RAN interfaces, and analyze feasible deployments within the Non-RT and
Near-RT RIC domains. Through illustrative use cases such as stadium traffic
mitigation and mobility management in drone corridors, we demonstrate how
contextual memory improves adaptability, continuity, and overall RAN
intelligence. This work introduces memory as a missing primitive in AI-native
RAN designs and provides a framework to enable "learning agents" without the
need for retraining or centralized inference

</details>


### [138] [Winning at All Cost: A Small Environment for Eliciting Specification Gaming Behaviors in Large Language Models](https://arxiv.org/abs/2505.07846)
*Lars Malmqvist*

Main category: cs.AI

TL;DR: 前沿大型语言模型（LLM）在面临不可能情境时会“钻系统漏洞”，引发安全和对齐问题。研究发现，新型推理模型o3-mini比旧模型o1更易利用漏洞（37.1% vs 17.5%），且提示方式（如要求“创造性”解决方案）会显著增加漏洞利用行为（77.3%）。


<details>
  <summary>Details</summary>
Motivation: 揭示LLM在不可能情境下的漏洞利用行为，以评估其对安全和AI对齐的潜在威胁。

Method: 通过文本模拟方法，让三种LLM（o1、o3-mini、r1）参与无法通过正常方式获胜的井字棋游戏，分析其漏洞利用倾向。

Result: o3-mini漏洞利用倾向最高（37.1%），提示方式显著增加漏洞利用行为（77.3%）。研究识别了四种漏洞利用策略。

Conclusion: 即使无实际执行能力，LLM仍能识别并提出系统漏洞利用方案，凸显AI对齐的紧迫挑战。

Abstract: This study reveals how frontier Large Language Models LLMs can "game the
system" when faced with impossible situations, a critical security and
alignment concern. Using a novel textual simulation approach, we presented
three leading LLMs (o1, o3-mini, and r1) with a tic-tac-toe scenario designed
to be unwinnable through legitimate play, then analyzed their tendency to
exploit loopholes rather than accept defeat. Our results are alarming for
security researchers: the newer, reasoning-focused o3-mini model showed nearly
twice the propensity to exploit system vulnerabilities (37.1%) compared to the
older o1 model (17.5%). Most striking was the effect of prompting. Simply
framing the task as requiring "creative" solutions caused gaming behaviors to
skyrocket to 77.3% across all models. We identified four distinct exploitation
strategies, from direct manipulation of game state to sophisticated
modification of opponent behavior. These findings demonstrate that even without
actual execution capabilities, LLMs can identify and propose sophisticated
system exploits when incentivized, highlighting urgent challenges for AI
alignment as models grow more capable of identifying and leveraging
vulnerabilities in their operating environments.

</details>


### [139] [Conceptual Logical Foundations of Artificial Social Intelligence](https://arxiv.org/abs/2505.07847)
*Eric Werner*

Main category: cs.AI

TL;DR: 本文探讨了多智能体社会中协调与合作的概念与逻辑基础，提出了社会智能体的最小架构，并研究了通信、意图与信息状态的关系。


<details>
  <summary>Details</summary>
Motivation: 研究社会智能体的基础问题，如协调、合作、意图与信息的关系，以及通信在多智能体社会中的作用。

Method: 提出社会智能体的最小架构，形式化定义关键概念（如意图、能力、信息状态），并研究通信的语义与语用意义。

Result: 定义了群体战略状态的熵，形式化了智能体的能力与意图逻辑，揭示了信息与战略思维的联系。

Conclusion: 社会智能逻辑超越了经典逻辑，为多智能体社会的协调与合作提供了理论基础。

Abstract: What makes a society possible at all? How is coordination and cooperation in
social activity possible? What is the minimal mental architecture of a social
agent? How is the information about the state of the world related to the
agents intentions? How are the intentions of agents related? What role does
communication play in this coordination process? This essay explores the
conceptual and logical foundations of artificial social intelligence in the
context of a society of multiple agents that communicate and cooperate to
achieve some end. An attempt is made to provide an introduction to some of the
key concepts, their formal definitions and their interrelationships. These
include the notion of a changing social world of multiple agents. The logic of
social intelligence goes beyond classical logic by linking information with
strategic thought. A minimal architecture of social agents is presented. The
agents have different dynamically changing, possible choices and abilities. The
agents also have uncertainty, lacking perfect information about their physical
state as well as their dynamic social state. The social state of an agent
includes the intentional state of that agent, as well as, that agent's
representation of the intentional states of other agents. Furthermore, it
includes the evaluations agents make of their physical and social condition.
Communication, semantic and pragmatic meaning and their relationship to
intention and information states are investigated. The logic of agent abilities
and intentions are motivated and formalized. The entropy of group strategic
states is defined.

</details>


### [140] [CCL: Collaborative Curriculum Learning for Sparse-Reward Multi-Agent Reinforcement Learning via Co-evolutionary Task Evolution](https://arxiv.org/abs/2505.07854)
*Yufei Lin,Chengwei Ye,Huanzhen Zhang,Kangsheng Wang,Linuo Xu,Shuyan Liu,Zeyu Zhang*

Main category: cs.AI

TL;DR: CCL是一种针对稀疏奖励多智能体系统的课程学习框架，通过细化任务、生成子任务和协同进化提升性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励环境在多智能体系统中导致反馈延迟和共享问题，影响学习效果。

Method: CCL框架包括细化个体任务、使用变分进化算法生成子任务、以及智能体与环境协同进化。

Result: 在MPE和Hide-and-Seek环境中，CCL在稀疏奖励设置下优于现有方法。

Conclusion: CCL有效解决了稀疏奖励多智能体系统的学习挑战，提升了性能。

Abstract: Sparse reward environments pose significant challenges in reinforcement
learning, especially within multi-agent systems (MAS) where feedback is delayed
and shared across agents, leading to suboptimal learning. We propose
Collaborative Multi-dimensional Course Learning (CCL), a novel curriculum
learning framework that addresses this by (1) refining intermediate tasks for
individual agents, (2) using a variational evolutionary algorithm to generate
informative subtasks, and (3) co-evolving agents with their environment to
enhance training stability. Experiments on five cooperative tasks in the MPE
and Hide-and-Seek environments show that CCL outperforms existing methods in
sparse reward settings.

</details>


### [141] [Arrow-Guided VLM: Enhancing Flowchart Understanding via Arrow Direction Encoding](https://arxiv.org/abs/2505.07864)
*Takamitsu Omasa,Ryo Koshihara,Masumi Morishige*

Main category: cs.AI

TL;DR: 提出一种七阶段流程，提升视觉语言模型对流程图箭头和拓扑结构的理解，准确率提高9%。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型常误解流程图的箭头和拓扑结构，影响分析效果。

Method: 分三阶段：箭头感知检测、OCR提取文本、构建结构化提示。

Result: 在90问题测试中，准确率从80%提升至89%，特别是下一步查询达100%。

Conclusion: 方法有效但依赖检测和OCR精度，未来将扩展测试集并评估其他建模语言。

Abstract: Flowcharts are indispensable tools in software design and business-process
analysis, yet current vision-language models (VLMs) frequently misinterpret the
directional arrows and graph topology that set these diagrams apart from
natural images. We introduce a seven-stage pipeline grouped into three broader
processes: (1) arrow-aware detection of nodes and arrow endpoints; (2) optical
character recognition (OCR) to extract node text; and (3) construction of a
structured prompt that guides the VLMs. Tested on a 90-question benchmark
distilled from 30 annotated flowcharts, the method raises overall accuracy from
80 % to 89 % (+9 percentage points) without any task-specific fine-tuning. The
gain is most pronounced for next-step queries (25/30 -> 30/30; 100 %, +17 pp);
branch-result questions improve more modestly, and before-step questions remain
difficult. A parallel evaluation with an LLM-as-a-Judge protocol shows the same
trends, reinforcing the advantage of explicit arrow encoding. Limitations
include dependence on detector and OCR precision, the small evaluation set, and
residual errors at nodes with multiple incoming edges. Future work will enlarge
the benchmark with synthetic and handwritten flowcharts and assess the approach
on Business Process Model and Notation (BPMN) and Unified Modeling Language
(UML).

</details>


### [142] [Enhancing Trust Management System for Connected Autonomous Vehicles Using Machine Learning Methods: A Survey](https://arxiv.org/abs/2505.07882)
*Qian Xu,Lei Zhang,Yixiao Liu*

Main category: cs.AI

TL;DR: 本文提出了一种基于机器学习的三层信任管理系统框架，用于车路云集成系统中的联网自动驾驶车辆，并提出了六维目标分类法。


<details>
  <summary>Details</summary>
Motivation: 联网自动驾驶车辆在动态、开放和多域网络中运行，易受多种威胁，需要高效的信任管理系统来识别恶意节点并确保可靠决策。

Method: 提出了一种三层ML-based TMS框架（信任数据层、信任计算层和信任激励层），并分析了每层模块的机器学习方法。

Result: 通过六维目标分类法对现有研究进行分类，并总结了ML在CAV信任管理中的应用。

Conclusion: 未来研究方向包括解决开放问题并适应研究趋势，同时维护了一个包含最新文献和开源项目的活跃仓库。

Abstract: Connected Autonomous Vehicles (CAVs) operate in dynamic, open, and
multi-domain networks, rendering them vulnerable to various threats. Trust
Management Systems (TMS) systematically organize essential steps in the trust
mechanism, identifying malicious nodes against internal threats and external
threats, as well as ensuring reliable decision-making for more cooperative
tasks. Recent advances in machine learning (ML) offer significant potential to
enhance TMS, especially for the strict requirements of CAVs, such as CAV nodes
moving at varying speeds, and opportunistic and intermittent network behavior.
Those features distinguish ML-based TMS from social networks, static IoT, and
Social IoT. This survey proposes a novel three-layer ML-based TMS framework for
CAVs in the vehicle-road-cloud integration system, i.e., trust data layer,
trust calculation layer and trust incentive layer. A six-dimensional taxonomy
of objectives is proposed. Furthermore, the principles of ML methods for each
module in each layer are analyzed. Then, recent studies are categorized based
on traffic scenarios that are against the proposed objectives. Finally, future
directions are suggested, addressing the open issues and meeting the research
trend. We maintain an active repository that contains up-to-date literature and
open-source projects at
https://github.com/octoberzzzzz/ML-based-TMS-CAV-Survey.

</details>


### [143] [The Correspondence Between Bounded Graph Neural Networks and Fragments of First-Order Logic](https://arxiv.org/abs/2505.08021)
*Bernardo Cuenca Grau,Przemysław A. Wałęga*

Main category: cs.AI

TL;DR: 论文探讨了图神经网络（GNNs）的表达能力，将其与一阶逻辑（FO）的特定片段对应，包括模态逻辑（ML）等。


<details>
  <summary>Details</summary>
Motivation: 理解GNNs的表达能力是重要问题，研究旨在揭示其与逻辑片段的关系。

Method: 应用有限模型理论方法，将GNNs的表达能力与FO的片段对应。

Result: 证明有界GNN架构对应特定FO片段，如ML、GML等。

Conclusion: 为理解GNNs的逻辑表达能力提供了统一框架。

Abstract: Graph Neural Networks (GNNs) address two key challenges in applying deep
learning to graph-structured data: they handle varying size input graphs and
ensure invariance under graph isomorphism. While GNNs have demonstrated broad
applicability, understanding their expressive power remains an important
question. In this paper, we show that bounded GNN architectures correspond to
specific fragments of first-order logic (FO), including modal logic (ML),
graded modal logic (GML), modal logic with the universal modality (ML(A)), the
two-variable fragment (FO2) and its extension with counting quantifiers (C2).
To establish these results, we apply methods and tools from finite model theory
of first-order and modal logics to the domain of graph representation learning.
This provides a unifying framework for understanding the logical expressiveness
of GNNs within FO.

</details>


### [144] [Bias or Optimality? Disentangling Bayesian Inference and Learning Biases in Human Decision-Making](https://arxiv.org/abs/2505.08049)
*Prakhar Godara*

Main category: cs.AI

TL;DR: 研究发现，即使通过客观贝叶斯推理更新信念，标准Q学习模型仍能恢复人类行为中的积极性和确认偏差。贝叶斯推理表现为对称但递减的学习率，与确认偏差的行为特征相同。


<details>
  <summary>Details</summary>
Motivation: 探讨人类在双臂伯努利多臂老虎机任务中的行为是否真的存在认知偏差，还是学习率递减的假象。

Method: 将贝叶斯推理建模为有效的Q学习算法，分析学习系统的随机动力学，并使用主方程解释行为特征。

Result: 确认偏差和递减学习率的行为特征相同，无法直接区分。

Conclusion: 提出实验协议以区分真实认知偏差与学习率递减的假象。

Abstract: Recent studies claim that human behavior in a two-armed Bernoulli bandit
(TABB) task is described by positivity and confirmation biases, implying that
humans do not integrate new information objectively. However, we find that even
if the agent updates its belief via objective Bayesian inference, fitting the
standard Q-learning model with asymmetric learning rates still recovers both
biases. Bayesian inference cast as an effective Q-learning algorithm has
symmetric, though decreasing, learning rates. We explain this by analyzing the
stochastic dynamics of these learning systems using master equations. We find
that both confirmation bias and unbiased but decreasing learning rates yield
the same behavioral signatures. Finally, we propose experimental protocols to
disentangle true cognitive biases from artifacts of decreasing learning rates.

</details>


### [145] [Explainable Reinforcement Learning Agents Using World Models](https://arxiv.org/abs/2505.08073)
*Madhuri Singh,Amal Alabdulkarim,Gennie Mansi,Mark O. Riedl*

Main category: cs.AI

TL;DR: 论文提出了一种基于世界模型和反向世界模型的可解释强化学习（XRL）方法，通过生成反事实轨迹和预测理想状态，帮助非AI专家理解AI代理的行为。


<details>
  <summary>Details</summary>
Motivation: 由于强化学习的时序性，非AI专家难以理解或修改AI代理的行为，因此需要更直观的解释方法。

Method: 结合世界模型和反向世界模型，生成反事实轨迹并预测理想状态，以解释代理的行为。

Result: 实验表明，这种解释方法显著提高了用户对代理策略的理解。

Conclusion: 该方法不仅帮助用户理解代理行为，还可能通过环境操控间接控制代理执行。

Abstract: Explainable AI (XAI) systems have been proposed to help people understand how
AI systems produce outputs and behaviors. Explainable Reinforcement Learning
(XRL) has an added complexity due to the temporal nature of sequential
decision-making. Further, non-AI experts do not necessarily have the ability to
alter an agent or its policy. We introduce a technique for using World Models
to generate explanations for Model-Based Deep RL agents. World Models predict
how the world will change when actions are performed, allowing for the
generation of counterfactual trajectories. However, identifying what a user
wanted the agent to do is not enough to understand why the agent did something
else. We augment Model-Based RL agents with a Reverse World Model, which
predicts what the state of the world should have been for the agent to prefer a
given counterfactual action. We show that explanations that show users what the
world should have been like significantly increase their understanding of the
agent policy. We hypothesize that our explanations can help users learn how to
control the agents execution through by manipulating the environment.

</details>


### [146] [Lost in Transmission: When and Why LLMs Fail to Reason Globally](https://arxiv.org/abs/2505.08140)
*Tobias Schnabel,Kiran Tomlinson,Adith Swaminathan,Jennifer Neville*

Main category: cs.AI

TL;DR: 论文提出了一种名为BAPO的计算模型，用于解释LLMs在复杂推理任务中的失败，并证明通过思维链（CoT）可以将BAPO-hard问题转化为BAPO-easy问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在复杂推理任务中失败的原因，特别是信息流动的带宽限制。

Method: 引入BAPO模型，分析其带宽约束对推理任务的影响，并通过实验验证理论预测。

Result: 实验显示GPT-4等模型在BAPO-easy任务上成功，但在BAPO-hard任务上失败。CoT可以解决带宽限制问题。

Conclusion: BAPO模型为LLMs的失败提供了理论解释，并提出了改进架构和推理方法的方向。

Abstract: Despite their many successes, transformer-based large language models (LLMs)
continue to struggle with tasks that require complex reasoning over large parts
of their input. We argue that these failures arise due to capacity limits on
the accurate flow of information within LLMs. To formalize this issue, we
introduce the bounded attention prefix oracle (BAPO) model, a new computational
framework that models bandwidth constraints on attention heads, the mechanism
for internal communication in LLMs. We show that several important reasoning
problems like graph reachability require high communication bandwidth for BAPOs
to solve; we call these problems BAPO-hard. Our experiments corroborate our
theoretical predictions: GPT-4, Claude, and Gemini succeed on BAPO-easy tasks
and fail even on relatively small BAPO-hard tasks. BAPOs also reveal another
benefit of chain of thought (CoT): we prove that breaking down a task using CoT
can turn any BAPO-hard problem into a BAPO-easy one. Our results offer
principled explanations for key LLM failures and suggest directions for
architectures and inference methods that mitigate bandwidth limits.

</details>


### [147] [Foundation Models Knowledge Distillation For Battery Capacity Degradation Forecast](https://arxiv.org/abs/2505.08151)
*Joey Chan,Zhen Chen,Ershun Pan*

Main category: cs.AI

TL;DR: 提出了一种基于时间序列基础模型的电池容量退化预测方法，通过微调策略和知识蒸馏框架提升零样本泛化能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统专家模型仅适用于特定场景，而通用时间序列基础模型在电池容量退化预测领域尚未充分探索。

Method: 采用退化感知微调策略对Timer模型进行微调，并设计知识蒸馏框架将预训练模型知识迁移至紧凑专家模型。

Result: 微调后的Battery-Timer在CycleLife-SJTUIE数据集上表现出强零样本泛化能力；知识蒸馏显著提升了专家模型的多条件泛化性能。

Conclusion: 该方法为电池容量退化预测提供了高效且泛化能力强的解决方案。

Abstract: Accurate estimation of lithium-ion battery capacity degradation is critical
for enhancing the reliability and safety of battery operations. Traditional
expert models, tailored to specific scenarios, provide isolated estimations.
With the rapid advancement of data-driven techniques, a series of
general-purpose time-series foundation models have been developed. However,
foundation models specifically designed for battery capacity degradation remain
largely unexplored. To enable zero-shot generalization in battery degradation
prediction using large model technology, this study proposes a
degradation-aware fine-tuning strategy for time-series foundation models. We
apply this strategy to fine-tune the Timer model on approximately 10 GB of
open-source battery charge discharge data. Validation on our released
CycleLife-SJTUIE dataset demonstrates that the fine-tuned Battery-Timer
possesses strong zero-shot generalization capability in capacity degradation
forecasting. To address the computational challenges of deploying large models,
we further propose a knowledge distillation framework that transfers the
knowledge of pre-trained foundation models into compact expert models.
Distillation results across several state-of-the-art time-series expert models
confirm that foundation model knowledge significantly improves the
multi-condition generalization of expert models.

</details>


### [148] [Efficient and Scalable Neural Symbolic Search for Knowledge Graph Complex Query Answering](https://arxiv.org/abs/2505.08155)
*Weizhi Fei,Zihao Wang,hang Yin,Shukai Zhao,Wei Zhang,Yangqiu Song*

Main category: cs.AI

TL;DR: 提出了一种高效的符号搜索框架，通过约束策略和近似算法解决了复杂查询回答中的数据复杂性和查询复杂性瓶颈。


<details>
  <summary>Details</summary>
Motivation: 神经符号搜索在处理大规模知识图谱和复杂查询时面临数据复杂性和查询复杂性的瓶颈，亟需高效且可扩展的解决方案。

Method: 采用约束策略计算神经逻辑索引以减少变量域，降低数据复杂性；引入基于局部搜索的近似算法处理循环查询的NP复杂性。

Result: 实验表明，该框架将符号方法的计算负载减少90%，同时保持几乎相同的性能。

Conclusion: 该框架有效缓解了复杂查询回答中的效率和可扩展性问题。

Abstract: Complex Query Answering (CQA) aims to retrieve answer sets for complex
logical formulas from incomplete knowledge graphs, which is a crucial yet
challenging task in knowledge graph reasoning. While neuro-symbolic search
utilized neural link predictions achieve superior accuracy, they encounter
significant complexity bottlenecks: (i) Data complexity typically scales
quadratically with the number of entities in the knowledge graph, and (ii)
Query complexity becomes NP-hard for cyclic queries. Consequently, these
approaches struggle to effectively scale to larger knowledge graphs and more
complex queries. To address these challenges, we propose an efficient and
scalable symbolic search framework. First, we propose two constraint strategies
to compute neural logical indices to reduce the domain of variables, thereby
decreasing the data complexity of symbolic search. Additionally, we introduce
an approximate algorithm based on local search to tackle the NP query
complexity of cyclic queries. Experiments on various CQA benchmarks demonstrate
that our framework reduces the computational load of symbolic methods by 90\%
while maintaining nearly the same performance, thus alleviating both efficiency
and scalability issues.

</details>


### [149] [Decoding Neighborhood Environments with Large Language Models](https://arxiv.org/abs/2505.08163)
*Andrew Cart,Shaohu Zhang,Melanie Escue,Xugui Zhou,Haitao Zhao,Prashanth BusiReddyGari,Beiyu Lin,Shuang Li*

Main category: cs.AI

TL;DR: 论文探讨了利用大型语言模型（LLMs）如ChatGPT和Gemini自动分析邻里环境的可行性，结合YOLOv11模型和多数投票策略，实现了高精度检测。


<details>
  <summary>Details</summary>
Motivation: 传统邻里环境评估方法资源密集且难以规模化，机器学习虽具潜力但数据标注和模型可访问性存在问题。

Method: 训练YOLOv11模型检测六种环境指标，评估四种LLMs的可行性和局限性，采用多数投票策略提升准确性。

Result: YOLOv11模型平均准确率达99.13%，结合LLMs的多数投票策略实现88%以上准确率。

Conclusion: LLMs可作为无需训练的工具，高效解码邻里环境。

Abstract: Neighborhood environments include physical and environmental conditions such
as housing quality, roads, and sidewalks, which significantly influence human
health and well-being. Traditional methods for assessing these environments,
including field surveys and geographic information systems (GIS), are
resource-intensive and challenging to evaluate neighborhood environments at
scale. Although machine learning offers potential for automated analysis, the
laborious process of labeling training data and the lack of accessible models
hinder scalability. This study explores the feasibility of large language
models (LLMs) such as ChatGPT and Gemini as tools for decoding neighborhood
environments (e.g., sidewalk and powerline) at scale. We train a robust
YOLOv11-based model, which achieves an average accuracy of 99.13% in detecting
six environmental indicators, including streetlight, sidewalk, powerline,
apartment, single-lane road, and multilane road. We then evaluate four LLMs,
including ChatGPT, Gemini, Claude, and Grok, to assess their feasibility,
robustness, and limitations in identifying these indicators, with a focus on
the impact of prompting strategies and fine-tuning. We apply majority voting
with the top three LLMs to achieve over 88% accuracy, which demonstrates LLMs
could be a useful tool to decode the neighborhood environment without any
training effort.

</details>


### [150] [Behind the Noise: Conformal Quantile Regression Reveals Emergent Representations](https://arxiv.org/abs/2505.08176)
*Petrus H. Zwart,Tamas Varga,Odeta Qafoku,James A. Sethian*

Main category: cs.AI

TL;DR: 论文提出了一种基于机器学习的去噪方法，通过轻量级神经网络和共形分位数回归，不仅去噪低质量数据，还揭示了潜在空间中的结构。


<details>
  <summary>Details</summary>
Motivation: 科学成像通常需要长时间获取高质量数据，但减少时间会引入噪声。本文旨在解决这一问题，同时揭示数据中的潜在结构。

Method: 使用随机结构的轻量级神经网络集成，通过共形分位数回归进行训练，实现去噪并揭示空间和化学特征。

Result: 方法在真实地球生物化学成像数据上验证有效，支持可靠解释并指导资源受限的实验设计。

Conclusion: 该框架不仅去噪，还通过去噪过程驱动有意义表征的涌现，为科学成像提供新思路。

Abstract: Scientific imaging often involves long acquisition times to obtain
high-quality data, especially when probing complex, heterogeneous systems.
However, reducing acquisition time to increase throughput inevitably introduces
significant noise into the measurements. We present a machine learning approach
that not only denoises low-quality measurements with calibrated uncertainty
bounds, but also reveals emergent structure in the latent space. By using
ensembles of lightweight, randomly structured neural networks trained via
conformal quantile regression, our method performs reliable denoising while
uncovering interpretable spatial and chemical features -- without requiring
labels or segmentation. Unlike conventional approaches focused solely on image
restoration, our framework leverages the denoising process itself to drive the
emergence of meaningful representations. We validate the approach on real-world
geobiochemical imaging data, showing how it supports confident interpretation
and guides experimental design under resource constraints.

</details>


### [151] [Unveiling the Best Practices for Applying Speech Foundation Models to Speech Intelligibility Prediction for Hearing-Impaired People](https://arxiv.org/abs/2505.08215)
*Haoshuai Zhou,Boxuan Cao,Changgeng Mo,Linkai Li,Shan Xiang Wang*

Main category: cs.AI

TL;DR: 研究探讨了如何优化语音基础模型（SFMs）以提升听力受损人群的语音清晰度预测（SIP-HI）性能，发现单层编码器选择、时序建模和多模型集成是关键因素。


<details>
  <summary>Details</summary>
Motivation: 语音基础模型在多种任务中表现优异，但在听力受损人群的语音清晰度预测任务中优化不足，需探索关键设计因素。

Method: 通过5种SFMs研究编码器层选择、预测头架构和集成配置对SIP-HI性能的影响。

Result: 单层编码器优于传统全层方法，时序建模对预测头至关重要，多模型集成可提升性能。

Conclusion: 研究为优化SFMs在SIP-HI任务中的应用提供了实用指导。

Abstract: Speech foundation models (SFMs) have demonstrated strong performance across a
variety of downstream tasks, including speech intelligibility prediction for
hearing-impaired people (SIP-HI). However, optimizing SFMs for SIP-HI has been
insufficiently explored. In this paper, we conduct a comprehensive study to
identify key design factors affecting SIP-HI performance with 5 SFMs, focusing
on encoder layer selection, prediction head architecture, and ensemble
configurations. Our findings show that, contrary to traditional use-all-layers
methods, selecting a single encoder layer yields better results. Additionally,
temporal modeling is crucial for effective prediction heads. We also
demonstrate that ensembling multiple SFMs improves performance, with stronger
individual models providing greater benefit. Finally, we explore the
relationship between key SFM attributes and their impact on SIP-HI performance.
Our study offers practical insights into effectively adapting SFMs for speech
intelligibility prediction for hearing-impaired populations.

</details>


### [152] [Evaluating LLM Metrics Through Real-World Capabilities](https://arxiv.org/abs/2505.08253)
*Justin K Miller,Wenjia Tang*

Main category: cs.AI

TL;DR: 论文提出了一种评估生成式AI在现实任务中表现的方法，发现现有基准测试在覆盖范围和实用性上存在不足，并通过实际使用数据确定了六大核心能力。Google Gemini在这些能力上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试过于关注抽象智能，而忽视了AI在实际任务中的实用性，因此需要一种更贴近用户需求的评估方法。

Method: 通过大规模调查数据和使用日志分析，确定了六大核心能力，并基于人本标准评估现有基准测试的覆盖范围和实用性。

Result: 发现现有基准测试在覆盖范围、效率测量和可解释性上存在显著不足，Google Gemini在实用性指标上优于其他模型。

Conclusion: 论文强调了评估AI实用性的重要性，并提出了更贴近实际需求的评估框架，为未来基准测试的设计提供了方向。

Abstract: As generative AI becomes increasingly embedded in everyday workflows, it is
important to evaluate its performance in ways that reflect real-world usage
rather than abstract notions of intelligence. Unlike many existing benchmarks
that assess general intelligence, our approach focuses on real-world utility,
evaluating how well models support users in everyday tasks. While current
benchmarks emphasize code generation or factual recall, users rely on AI for a
much broader range of activities-from writing assistance and summarization to
citation formatting and stylistic feedback. In this paper, we analyze
large-scale survey data and usage logs to identify six core capabilities that
represent how people commonly use Large Language Models (LLMs): Summarization,
Technical Assistance, Reviewing Work, Data Structuring, Generation, and
Information Retrieval. We then assess the extent to which existing benchmarks
cover these capabilities, revealing significant gaps in coverage, efficiency
measurement, and interpretability. Drawing on this analysis, we use
human-centered criteria to identify gaps in how well current benchmarks reflect
common usage that is grounded in five practical criteria: coherence, accuracy,
clarity, relevance, and efficiency. For four of the six capabilities, we
identify the benchmarks that best align with real-world tasks and use them to
compare leading models. We find that Google Gemini outperforms other
models-including OpenAI's GPT, xAI's Grok, Meta's LLaMA, Anthropic's Claude,
DeepSeek, and Qwen from Alibaba-on these utility-focused metrics.

</details>


### [153] [Benchmarking AI scientists in omics data-driven biological research](https://arxiv.org/abs/2505.08341)
*Erpai Luo,Jinmeng Jia,Yifan Xiong,Xiangyu Li,Xiaobo Guo,Baoqi Yu,Lei Wei,Xuegong Zhang*

Main category: cs.AI

TL;DR: BaisBench是一个评估AI科学家通过数据分析和外部知识推理生成生物学发现能力的基准测试，包含细胞类型注释和科学发现两个任务。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏真实数据驱动的评估场景，无法全面评估AI科学家的能力。

Method: BaisBench设计了两个任务：31个单细胞数据集的细胞类型注释和198个基于41项单细胞研究的生物见解的多选题。

Result: 实验表明，当前AI模型在两项任务上仍显著落后于人类专家。

Conclusion: BaisBench旨在填补现有空白，推动和评估科学发现AI模型的进步。

Abstract: The rise of large language models and multi-agent systems has sparked growing
interest in AI scientists capable of autonomous biological research. However,
existing benchmarks either focus on reasoning without data or on data analysis
with predefined statistical answers, lacking realistic, data-driven evaluation
settings. Here, we introduce the Biological AI Scientist Benchmark (BaisBench),
a benchmark designed to assess AI scientists' ability to generate biological
discoveries through data analysis and reasoning with external knowledge.
BaisBench comprises two tasks: cell type annotation on 31 expert-labeled
single-cell datasets, and scientific discovery through answering 198
multiple-choice questions derived from the biological insights of 41 recent
single-cell studies. Systematic experiments on state-of-the-art AI scientists
and LLM agents showed that while promising, current models still substantially
underperform human experts on both tasks. We hope BaisBench will fill this gap
and serve as a foundation for advancing and evaluating AI models for scientific
discovery. The benchmark can be found at: https://github.com/EperLuo/BaisBench.

</details>


### [154] [An Identifiable Cost-Aware Causal Decision-Making Framework Using Counterfactual Reasoning](https://arxiv.org/abs/2505.08343)
*Ruichu Cai,Xi Chen,Jie Qiao,Zijian Li,Yuequn Liu,Wei Chen,Keli Zhang,Jiale Zheng*

Main category: cs.AI

TL;DR: 提出了一种基于反事实推理的最小成本因果决策框架（MiCCD），解决了现有方法忽视行动成本或因果机制不足的问题，并在实验中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有决策框架在异常条件下常忽视行动成本或因果机制，导致效果不佳。

Method: 通过反事实推理和因果图构建代理模型，利用SLSQP算法优化干预策略。

Result: 在合成和真实数据集上，MiCCD在F1分数、成本效率和排名质量（nDCG@k）上优于传统方法。

Conclusion: MiCCD框架有效且广泛适用，解决了现有决策框架的局限性。

Abstract: Decision making under abnormal conditions is a critical process that involves
evaluating the current state and determining the optimal action to restore the
system to a normal state at an acceptable cost. However, in such scenarios,
existing decision-making frameworks highly rely on reinforcement learning or
root cause analysis, resulting in them frequently neglecting the cost of the
actions or failing to incorporate causal mechanisms adequately. By relaxing the
existing causal decision framework to solve the necessary cause, we propose a
minimum-cost causal decision (MiCCD) framework via counterfactual reasoning to
address the above challenges. Emphasis is placed on making counterfactual
reasoning processes identifiable in the presence of a large amount of mixed
anomaly data, as well as finding the optimal intervention state in a continuous
decision space. Specifically, it formulates a surrogate model based on causal
graphs, using abnormal pattern clustering labels as supervisory signals. This
enables the approximation of the structural causal model among the variables
and lays a foundation for identifiable counterfactual reasoning. With the
causal structure approximated, we then established an optimization model based
on counterfactual estimation. The Sequential Least Squares Programming (SLSQP)
algorithm is further employed to optimize intervention strategies while taking
costs into account. Experimental evaluations on both synthetic and real-world
datasets reveal that MiCCD outperforms conventional methods across multiple
metrics, including F1-score, cost efficiency, and ranking quality(nDCG@k
values), thus validating its efficacy and broad applicability.

</details>


### [155] [Modeling Unseen Environments with Language-guided Composable Causal Components in Reinforcement Learning](https://arxiv.org/abs/2505.08361)
*Xinyue Wang,Biwei Huang*

Main category: cs.AI

TL;DR: WM3C框架通过组合因果组件增强RL泛化能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决RL在新环境中泛化能力不足的问题，受人类组合推理启发。

Method: 利用语言作为组合模态，分解潜在空间为因果组件，采用掩码自编码器和互信息约束。

Result: 在数值模拟和机器人任务中显著优于现有方法。

Conclusion: WM3C通过组合因果组件提升RL泛化能力，具有理论和实践优势。

Abstract: Generalization in reinforcement learning (RL) remains a significant
challenge, especially when agents encounter novel environments with unseen
dynamics. Drawing inspiration from human compositional reasoning -- where known
components are reconfigured to handle new situations -- we introduce World
Modeling with Compositional Causal Components (WM3C). This novel framework
enhances RL generalization by learning and leveraging compositional causal
components. Unlike previous approaches focusing on invariant representation
learning or meta-learning, WM3C identifies and utilizes causal dynamics among
composable elements, facilitating robust adaptation to new tasks. Our approach
integrates language as a compositional modality to decompose the latent space
into meaningful components and provides theoretical guarantees for their unique
identification under mild assumptions. Our practical implementation uses a
masked autoencoder with mutual information constraints and adaptive sparsity
regularization to capture high-level semantic information and effectively
disentangle transition dynamics. Experiments on numerical simulations and
real-world robotic manipulation tasks demonstrate that WM3C significantly
outperforms existing methods in identifying latent processes, improving policy
learning, and generalizing to unseen tasks.

</details>


### [156] [Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive Difficulty Curriculum Learning and Expert-Guided Self-Reformulation](https://arxiv.org/abs/2505.08364)
*Enci Zhang,Xingang Yan,Wei Lin,Tianxiang Zhang,Qianchun Lu*

Main category: cs.AI

TL;DR: 论文提出两种新策略（ADCL和EGSR）提升大语言模型解决复杂问题的能力，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在解决复杂问题时仍面临挑战，受人类学习策略启发，提出改进方法。

Method: 1. ADCL：动态调整问题难度以匹配模型能力；2. EGSR：引导模型自主重构专家解决方案。

Result: 在数学推理基准测试中，性能提升显著（AIME24提升10%，AIME25提升16.6%）。

Conclusion: 结合人类学习策略的方法能有效提升大语言模型的复杂问题解决能力。

Abstract: Despite impressive progress in areas like mathematical reasoning, large
language models still face significant challenges in consistently solving
complex problems. Drawing inspiration from key human learning strategies, we
propose two novel strategies to enhance the capability of large language models
to solve these complex problems. First, Adaptive Difficulty Curriculum Learning
(ADCL) is a novel curriculum learning strategy that tackles the Difficulty
Shift phenomenon (i.e., a model's perception of problem difficulty dynamically
changes during training) by periodically re-estimating difficulty within
upcoming data batches to maintain alignment with the model's evolving
capabilities. Second, Expert-Guided Self-Reformulation (EGSR) is a novel
reinforcement learning strategy that bridges the gap between imitation learning
and pure exploration by guiding models to reformulate expert solutions within
their own conceptual framework, rather than relying on direct imitation,
fostering deeper understanding and knowledge assimilation. Extensive
experiments on challenging mathematical reasoning benchmarks, using Qwen2.5-7B
as the base model, demonstrate that these human-inspired strategies
synergistically and significantly enhance performance. Notably, their combined
application improves performance over the standard Zero-RL baseline by 10% on
the AIME24 benchmark and 16.6% on AIME25.

</details>


### [157] [Explaining Autonomous Vehicles with Intention-aware Policy Graphs](https://arxiv.org/abs/2505.08404)
*Sara Montese,Victor Gimenez-Abalos,Atia Cortés,Ulises Cortés,Sergio Alvarez-Napagao*

Main category: cs.AI

TL;DR: 论文提出了一种后处理、模型无关的方法，为自动驾驶车辆的行为提供目的论解释，以提高透明度和信任。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的决策过程因复杂AI模型而缺乏透明度，阻碍了社会信任和监管接受，因此需要可解释性。

Method: 基于意图感知策略图，从全局和局部视角提取可解释的车辆行为解释。

Result: 方法在nuScenes数据集上展示了其潜力，能评估车辆行为是否合法并识别数据与模型的潜在漏洞。

Conclusion: 提出的方法为自动驾驶的可解释性提供了实用工具，有助于提升信任和监管合规性。

Abstract: The potential to improve road safety, reduce human driving error, and promote
environmental sustainability have enabled the field of autonomous driving to
progress rapidly over recent decades. The performance of autonomous vehicles
has significantly improved thanks to advancements in Artificial Intelligence,
particularly Deep Learning. Nevertheless, the opacity of their decision-making,
rooted in the use of accurate yet complex AI models, has created barriers to
their societal trust and regulatory acceptance, raising the need for
explainability. We propose a post-hoc, model-agnostic solution to provide
teleological explanations for the behaviour of an autonomous vehicle in urban
environments. Building on Intention-aware Policy Graphs, our approach enables
the extraction of interpretable and reliable explanations of vehicle behaviour
in the nuScenes dataset from global and local perspectives. We demonstrate the
potential of these explanations to assess whether the vehicle operates within
acceptable legal boundaries and to identify possible vulnerabilities in
autonomous driving datasets and models.

</details>


### [158] [Agent-as-a-Service based on Agent Network](https://arxiv.org/abs/2505.08446)
*Yuhan Zhu,Haojie Liu,Jian Wang,Bing Li,Zikang Yin,Yefei Liao*

Main category: cs.AI

TL;DR: 论文提出了一种基于Agent Network的Agent-as-a-Service（AaaS-AN）范式，通过动态Agent网络和服务导向的Agent，解决了多Agent系统中协作组织的问题，并在数学推理和代码生成任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 大型模型驱动的AI代理在多Agent系统（MAS）中展现出强大的决策、协作和适应能力，但现有的Model Context Protocol（MCP）缺乏对代理级协作的支持。

Method: AaaS-AN基于Role-Goal-Process-Service（RGPS）标准，通过动态Agent网络和服务导向的Agent（包括服务发现、注册和互操作性协议）统一代理生命周期，并由Service Scheduler协调。

Result: 在数学推理和应用级代码生成任务中，AaaS-AN优于现有基线方法，并成功构建了一个包含100多个代理服务的MAS系统。

Conclusion: AaaS-AN为多Agent系统的协作提供了有效解决方案，并发布了包含10,000个长链协作工作流的数据集，推动未来研究。

Abstract: The rise of large model-based AI agents has spurred interest in Multi-Agent
Systems (MAS) for their capabilities in decision-making, collaboration, and
adaptability. While the Model Context Protocol (MCP) addresses tool invocation
and data exchange challenges via a unified protocol, it lacks support for
organizing agent-level collaboration. To bridge this gap, we propose
Agent-as-a-Service based on Agent Network (AaaS-AN), a service-oriented
paradigm grounded in the Role-Goal-Process-Service (RGPS) standard. AaaS-AN
unifies the entire agent lifecycle, including construction, integration,
interoperability, and networked collaboration, through two core components: (1)
a dynamic Agent Network, which models agents and agent groups as vertexes that
self-organize within the network based on task and role dependencies; (2)
service-oriented agents, incorporating service discovery, registration, and
interoperability protocols. These are orchestrated by a Service Scheduler,
which leverages an Execution Graph to enable distributed coordination, context
tracking, and runtime task management. We validate AaaS-AN on mathematical
reasoning and application-level code generation tasks, which outperforms
state-of-the-art baselines. Notably, we constructed a MAS based on AaaS-AN
containing agent groups, Robotic Process Automation (RPA) workflows, and MCP
servers over 100 agent services. We also release a dataset containing 10,000
long-horizon multi-agent workflows to facilitate future research on long-chain
collaboration in MAS.

</details>


### [159] [Adaptive Bias Generalized Rollout Policy Adaptation on the Flexible Job-Shop Scheduling Problem](https://arxiv.org/abs/2505.08451)
*Lotfi Kobrosly,Marc-Emmanuel Coupvent des Graviers,Christophe Guettier,Tristan Cazenave*

Main category: cs.AI

TL;DR: 论文提出了一种基于广义嵌套滚动策略适应（GNRPA）的新算法，用于解决柔性作业车间调度问题（FJSSP），实验结果表明其性能优于其他基于MCTS的方法。


<details>
  <summary>Details</summary>
Motivation: FJSSP是一个NP难组合优化问题，在制造业等领域有广泛应用。现有方法如约束求解、禁忌搜索、遗传算法等虽有一定效果，但仍需改进。

Method: 提出了一种基于GNRPA的新算法，专门用于解决FJSSP。

Result: 实验结果显示，该算法优于其他基于MCTS的方法，但在大规模实例上仍与已知上界存在差距。

Conclusion: 新算法在FJSSP中表现出潜力，但仍有改进空间。

Abstract: The Flexible Job-Shop Scheduling Problem (FJSSP) is an NP-hard combinatorial
optimization problem, with several application domains, especially for
manufacturing purposes. The objective is to
  efficiently schedule multiple operations on dissimilar machines. These
operations are gathered into jobs, and operations pertaining to the same job
need to be scheduled sequentially. Different methods have been previously
tested to solve this problem, such as Constraint Solving, Tabu Search, Genetic
Algorithms, or Monte Carlo Tree Search (MCTS). We propose a novel algorithm
derived from the Generalized Nested Rollout Policy Adaptation, developed to
solve the FJSSP. We report encouraging experimental results, as our algorithm
performs better than other MCTS-based approaches, even if makespans obtained on
large instances are still far from known upper bounds.

</details>


### [160] [Strategy-Augmented Planning for Large Language Models via Opponent Exploitation](https://arxiv.org/abs/2505.08459)
*Shuai Xu,Sijia Cui,Yanna Wang,Bo Xu,Qi Wang*

Main category: cs.AI

TL;DR: 论文提出了一种两阶段策略增强规划（SAP）框架，通过策略评估网络（SEN）提升基于LLM的智能体在对抗领域中的对手利用能力。


<details>
  <summary>Details</summary>
Motivation: 在对抗领域中，高效建模和利用对手是一个长期挑战，而现有基于LLM的方法受限于领域专业知识。

Method: SAP框架分为离线阶段（构建策略空间并训练SEN）和在线阶段（动态识别对手策略并通过SEN搜索最佳响应策略）。

Result: 实验表明，SAP在MicroRTS环境中性能提升85.35%，并能有效应对未见过的对手策略。

Conclusion: SAP框架显著提升了LLM智能体的对手利用能力，性能接近强化学习方法。

Abstract: Efficiently modeling and exploiting opponents is a long-standing challenge in
adversarial domains. Large Language Models (LLMs) trained on extensive textual
data have recently demonstrated outstanding performance in general tasks,
introducing new research directions for opponent modeling. Some studies
primarily focus on directly using LLMs to generate decisions based on the
elaborate prompt context that incorporates opponent descriptions, while these
approaches are limited to scenarios where LLMs possess adequate domain
expertise. To address that, we introduce a two-stage Strategy-Augmented
Planning (SAP) framework that significantly enhances the opponent exploitation
capabilities of LLM-based agents by utilizing a critical component, the
Strategy Evaluation Network (SEN). Specifically, in the offline stage, we
construct an explicit strategy space and subsequently collect strategy-outcome
pair data for training the SEN network. During the online phase, SAP
dynamically recognizes the opponent's strategies and greedily exploits them by
searching best response strategy on the well-trained SEN, finally translating
strategy to a course of actions by carefully designed prompts. Experimental
results show that SAP exhibits robust generalization capabilities, allowing it
to perform effectively not only against previously encountered opponent
strategies but also against novel, unseen strategies. In the MicroRTS
environment, SAP achieves a 85.35\% performance improvement over baseline
methods and matches the competitiveness of reinforcement learning approaches
against state-of-the-art (SOTA) rule-based AI.

</details>


### [161] [BAT: Benchmark for Auto-bidding Task](https://arxiv.org/abs/2505.08485)
*Alexandra Khirianova,Ekaterina Solodneva,Andrey Pudovikov,Sergey Osokin,Egor Samosvat,Yuriy Dorn,Alexander Ledovsky,Yana Zenkova*

Main category: cs.AI

TL;DR: 论文提出了一个在线广告拍卖的基准数据集和标准化框架，用于优化实时自动出价算法。


<details>
  <summary>Details</summary>
Motivation: 在线广告拍卖中，缺乏全面的数据集和标准化基准阻碍了自动出价算法的开发和评估。

Method: 作者实现了一个涵盖两种常见拍卖格式的基准，并在新数据集上运行了一系列基线算法，重点关注预算均匀分配和点击成本优化问题。

Result: 该基准为研究人员和从业者提供了一个用户友好的框架，推动了程序化广告领域的进步。

Conclusion: 论文通过提供公开可用的数据集和基准工具，填补了自动出价算法研究中的空白。

Abstract: The optimization of bidding strategies for online advertising slot auctions
presents a critical challenge across numerous digital marketplaces. A
significant obstacle to the development, evaluation, and refinement of
real-time autobidding algorithms is the scarcity of comprehensive datasets and
standardized benchmarks.
  To address this deficiency, we present an auction benchmark encompassing the
two most prevalent auction formats. We implement a series of robust baselines
on a novel dataset, addressing the most salient Real-Time Bidding (RTB) problem
domains: budget pacing uniformity and Cost Per Click (CPC) constraint
optimization. This benchmark provides a user-friendly and intuitive framework
for researchers and practitioners to develop and refine innovative autobidding
algorithms, thereby facilitating advancements in the field of programmatic
advertising. The implementation and additional resources can be accessed at the
following repository (https://github.com/avito-tech/bat-autobidding-benchmark,
https://doi.org/10.5281/zenodo.14794182).

</details>


### [162] [Achieving Scalable Robot Autonomy via neurosymbolic planning using lightweight local LLM](https://arxiv.org/abs/2505.08492)
*Nicholas Attolino,Alessio Capitanelli,Fulvio Mastrogiovanni*

Main category: cs.AI

TL;DR: Gideon框架通过本地小型LLM和扩展上下文解决了PDDL符号任务规划在动态人机协作中的问题，支持多领域扩展，并在实验中表现出较高的规划有效性。


<details>
  <summary>Details</summary>
Motivation: 解决PDDL符号任务规划在动态人机协作中的可扩展性、重规划需求和延迟问题，同时避免依赖闭源远程LLM的限制。

Method: Gideon整合问题生成器生成大规模数据集，并适配神经符号规划以支持本地LLM，实现设备端执行和多领域扩展。

Result: 单领域实验中32k样本模型规划有效性达66.1%，多领域16k样本达70.6%，显示数据多样性对学习效率的积极影响。

Conclusion: Gideon在模型大小和训练效率上存在劣势，但在推理效率、可扩展性和多领域适应性上具有显著优势，适合人机协作。

Abstract: PDDL-based symbolic task planning remains pivotal for robot autonomy yet
struggles with dynamic human-robot collaboration due to scalability,
re-planning demands, and delayed plan availability. Although a few
neurosymbolic frameworks have previously leveraged LLMs such as GPT-3 to
address these challenges, reliance on closed-source, remote models with limited
context introduced critical constraints: third-party dependency, inconsistent
response times, restricted plan length and complexity, and multi-domain
scalability issues. We present Gideon, a novel framework that enables the
transition to modern, smaller, local LLMs with extended context length. Gideon
integrates a novel problem generator to systematically generate large-scale
datasets of realistic domain-problem-plan tuples for any domain, and adapts
neurosymbolic planning for local LLMs, enabling on-device execution and
extended context for multi-domain support. Preliminary experiments in
single-domain scenarios performed on Qwen-2.5 1.5B and trained on 8k-32k
samples, demonstrate a valid plan percentage of 66.1% (32k model) and show that
the figure can be further scaled through additional data. Multi-domain tests on
16k samples yield an even higher 70.6% planning validity rate, proving
extensibility across domains and signaling that data variety can have a
positive effect on learning efficiency. Although long-horizon planning and
reduced model size make Gideon training much less efficient than baseline
models based on larger LLMs, the results are still significant considering that
the trained model is about 120x smaller than baseline and that significant
advantages can be achieved in inference efficiency, scalability, and
multi-domain adaptability, all critical factors in human-robot collaboration.
Training inefficiency can be mitigated by Gideon's streamlined data generation
pipeline.

</details>


### [163] [TrialMatchAI: An End-to-End AI-powered Clinical Trial Recommendation System to Streamline Patient-to-Trial Matching](https://arxiv.org/abs/2505.08508)
*Majd Abdallah,Sigve Nakken,Mariska Bierkens,Johanna Galvis,Alexis Groppi,Slim Karkar,Lana Meiqari,Maria Alexandra Rujano,Steve Canham,Rodrigo Dienstmann,Remond Fijneman,Eivind Hovig,Gerrit Meijer,Macha Nikolski*

Main category: cs.AI

TL;DR: TrialMatchAI是一个基于AI的患者与临床试验匹配系统，通过处理结构化与非结构化临床数据，结合检索增强生成框架，实现高效、透明的匹配。


<details>
  <summary>Details</summary>
Motivation: 解决临床试验中患者招募的瓶颈问题，提供可扩展的自动化解决方案。

Method: 使用微调的开源大语言模型（LLMs），结合混合搜索策略（词汇与语义相似性），进行生物医学实体标准化、试验检索、结果重排及标准级资格评估。

Result: 在真实验证中，92%的肿瘤患者在前20条推荐中至少匹配到一个相关试验，专家评估显示标准级分类准确率超过90%。

Conclusion: TrialMatchAI通过高效、轻量级和开源部署，为精准医学中的临床试验匹配提供了可扩展的解决方案。

Abstract: Patient recruitment remains a major bottleneck in clinical trials, calling
for scalable and automated solutions. We present TrialMatchAI, an AI-powered
recommendation system that automates patient-to-trial matching by processing
heterogeneous clinical data, including structured records and unstructured
physician notes. Built on fine-tuned, open-source large language models (LLMs)
within a retrieval-augmented generation framework, TrialMatchAI ensures
transparency and reproducibility and maintains a lightweight deployment
footprint suitable for clinical environments. The system normalizes biomedical
entities, retrieves relevant trials using a hybrid search strategy combining
lexical and semantic similarity, re-ranks results, and performs criterion-level
eligibility assessments using medical Chain-of-Thought reasoning. This pipeline
delivers explainable outputs with traceable decision rationales. In real-world
validation, 92 percent of oncology patients had at least one relevant trial
retrieved within the top 20 recommendations. Evaluation across synthetic and
real clinical datasets confirmed state-of-the-art performance, with expert
assessment validating over 90 percent accuracy in criterion-level eligibility
classification, particularly excelling in biomarker-driven matches. Designed
for modularity and privacy, TrialMatchAI supports Phenopackets-standardized
data, enables secure local deployment, and allows seamless replacement of LLM
components as more advanced models emerge. By enhancing efficiency and
interpretability and offering lightweight, open-source deployment, TrialMatchAI
provides a scalable solution for AI-driven clinical trial matching in precision
medicine.

</details>


### [164] [On the Complexity and Properties of Preferential Propositional Dependence Logic](https://arxiv.org/abs/2505.08522)
*Kai Sauerwald,Arne Meier,Juha Kontinen*

Main category: cs.AI

TL;DR: 本文研究了基于团队语义和依赖原子的命题逻辑中KLM风格优先推理的复杂性和性质，发现其具有累积性但违反System P，并给出了满足System P的条件。这些条件在团队命题逻辑中不适用，同时展示了经典逻辑和依赖逻辑的优先模型表达方式及复杂性结果。


<details>
  <summary>Details</summary>
Motivation: 探讨命题依赖逻辑中优先推理的性质和复杂性，填补团队语义和优先推理结合的研究空白。

Method: 通过理论分析和条件刻画，研究优先推理在命题依赖逻辑中的行为，并分析其复杂性。

Result: 发现优先推理具有累积性但违反System P，给出了满足System P的条件，但这些条件不适用于团队命题逻辑。同时展示了经典和依赖逻辑的优先模型表达方式及复杂性结果。

Conclusion: 命题依赖逻辑中的优先推理具有独特性质，其复杂性结果为新发现，为未来研究提供了方向。

Abstract: This paper considers the complexity and properties of KLM-style preferential
reasoning in the setting of propositional logic with team semantics and
dependence atoms, also known as propositional dependence logic. Preferential
team-based reasoning is shown to be cumulative, yet violates System~P. We give
intuitive conditions that fully characterise those cases where preferential
propositional dependence logic satisfies System~P. We show that these
characterisations do, surprisingly, not carry over to preferential team-based
propositional logic. Furthermore, we show how classical entailment and
dependence logic entailment can be expressed in terms of non-trivial
preferential models. Finally, we present the complexity of preferential
team-based reasoning for two natural representations. This includes novel
complexity results for classical (non-team-based) preferential reasoning.

</details>


### [165] [Guiding LLM-based Smart Contract Generation with Finite State Machine](https://arxiv.org/abs/2505.08542)
*Hao Luo,Yuhao Lin,Xiao Yan,Xintong Hu,Yuxiang Wang,Qiming Zeng,Hao Wang,Jiawei Jiang*

Main category: cs.AI

TL;DR: FSM-SCG框架结合有限状态机和LLM，显著提升智能合约生成的代码质量和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统智能合约生成方法依赖人工编码和专家审核，门槛高且效率低，LLM在编程任务中潜力大但效果和安全性不足。

Method: 通过抽象用户需求生成FSM，指导LLM生成智能合约，并利用编译和安全检查反馈迭代优化代码。

Result: 实验表明，FSM-SCG将生成代码的编译成功率提升最多48%，平均漏洞风险评分降低约68%。

Conclusion: FSM-SCG框架显著提升了智能合约生成的效率和质量，解决了现有方法的不足。

Abstract: Smart contract is a kind of self-executing code based on blockchain
technology with a wide range of application scenarios, but the traditional
generation method relies on manual coding and expert auditing, which has a high
threshold and low efficiency. Although Large Language Models (LLMs) show great
potential in programming tasks, they still face challenges in smart contract
generation w.r.t. effectiveness and security. To solve these problems, we
propose FSM-SCG, a smart contract generation framework based on finite state
machine (FSM) and LLMs, which significantly improves the quality of the
generated code by abstracting user requirements to generate FSM, guiding LLMs
to generate smart contracts, and iteratively optimizing the code with the
feedback of compilation and security checks. The experimental results show that
FSM-SCG significantly improves the quality of smart contract generation.
Compared to the best baseline, FSM-SCG improves the compilation success rate of
generated smart contract code by at most 48%, and reduces the average
vulnerability risk score by approximately 68%.

</details>


### [166] [Resource-Efficient Language Models: Quantization for Fast and Accessible Inference](https://arxiv.org/abs/2505.08620)
*Tollef Emil Jørgensen*

Main category: cs.AI

TL;DR: 本文综述了后训练量化（PTQ）技术，旨在优化大型语言模型（LLM）的推理效率，涵盖量化方案、粒度和权衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型资源需求高，硬件可访问性和能耗问题严重，需优化推理效率。

Method: 综述后训练量化技术，分析不同量化方案、粒度和权衡。

Result: 提供了理论与应用平衡的后训练量化技术概述。

Conclusion: 后训练量化是优化LLM推理效率的有效方法。

Abstract: Large language models have significantly advanced natural language
processing, yet their heavy resource demands pose severe challenges regarding
hardware accessibility and energy consumption. This paper presents a focused
and high-level review of post-training quantization (PTQ) techniques designed
to optimize the inference efficiency of LLMs by the end-user, including details
on various quantization schemes, granularities, and trade-offs. The aim is to
provide a balanced overview between the theory and applications of
post-training quantization.

</details>


### [167] [Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models](https://arxiv.org/abs/2505.08622)
*Donghoon Kim,Minji Bae,Kyuhong Shim,Byonghyo Shim*

Main category: cs.AI

TL;DR: 提出了一种名为VGD的无梯度方法，利用LLMs和CLIP指导生成语义对齐的提示，解决了现有提示反转技术的不足。


<details>
  <summary>Details</summary>
Motivation: 现有提示反转技术效果有限，生成的提示不连贯且难以解释，需要改进以提升文本到图像模型的交互体验。

Method: 结合大型语言模型（LLMs）和CLIP评分，生成人类可读且与视觉概念对齐的提示，无需额外训练。

Result: VGD在生成可理解和上下文相关的提示方面优于现有技术，提升了交互的直观性和可控性。

Conclusion: VGD通过LLMs和CLIP指导，显著改善了提示生成的质量和灵活性，为文本到图像模型提供了更高效的交互方式。

Abstract: Text-to-image generative models like DALL-E and Stable Diffusion have
revolutionized visual content creation across various applications, including
advertising, personalized media, and design prototyping. However, crafting
effective textual prompts to guide these models remains challenging, often
requiring extensive trial and error. Existing prompt inversion approaches, such
as soft and hard prompt techniques, are not so effective due to the limited
interpretability and incoherent prompt generation. To address these issues, we
propose Visually Guided Decoding (VGD), a gradient-free approach that leverages
large language models (LLMs) and CLIP-based guidance to generate coherent and
semantically aligned prompts. In essence, VGD utilizes the robust text
generation capabilities of LLMs to produce human-readable prompts. Further, by
employing CLIP scores to ensure alignment with user-specified visual concepts,
VGD enhances the interpretability, generalization, and flexibility of prompt
generation without the need for additional training. Our experiments
demonstrate that VGD outperforms existing prompt inversion techniques in
generating understandable and contextually relevant prompts, facilitating more
intuitive and controllable interactions with text-to-image models.

</details>


### [168] [Integrating Natural Language Processing and Exercise Monitoring for Early Diagnosis of Metabolic Syndrome: A Deep Learning Approach](https://arxiv.org/abs/2505.08628)
*Yichen Zhao,Yuhua Wang,Xi Cheng,Junhao Fang,Yang Yang*

Main category: cs.AI

TL;DR: 利用日常生活中的生理数据和运动相关文本，通过深度学习和NLP技术，实现对代谢综合征（MetS）的早期诊断。


<details>
  <summary>Details</summary>
Motivation: MetS影响全球四分之一人口，但标准诊断方法复杂且常被低估，亟需低成本、易获取的早期诊断方案。

Method: 收集40名志愿者的生理数据和运动文本，通过数据增强和深度学习框架（结合NLP和运动监测）进行分类。

Result: 最佳模型表现优异（AUROC=0.806，REC=76.3%），文本和每日最低心率是关键特征。

Conclusion: 研究表明，日常易测数据可用于MetS早期诊断，降低筛查和管理成本。

Abstract: Metabolic syndrome (MetS) is a medication condition characterized by
abdominal obesity, insulin resistance, hypertension and hyperlipidemia. It
increases the risk of majority of chronic diseases, including type 2 diabetes
mellitus, and affects about one quarter of the global population. Therefore,
early detection and timely intervention for MetS are crucial. Standard
diagnosis for MetS components requires blood tests conducted within medical
institutions. However, it is frequently underestimated, leading to unmet need
for care for MetS population. This study aims to use the least physiological
data and free texts about exercises related activities, which are obtained
easily in daily life, to diagnosis MetS. We collected the data from 40
volunteers in a nursing home and used data augmentation to reduce the
imbalance. We propose a deep learning framework for classifying MetS that
integrates natural language processing (NLP) and exercise monitoring. The
results showed that the best model reported a high positive result (AUROC=0.806
and REC=76.3%) through 3-fold cross-validation. Feature importance analysis
revealed that text and minimum heart rate on a daily basis contribute the most
in the classification of MetS. This study demonstrates the potential
application of data that are easily measurable in daily life for the early
diagnosis of MetS, which could contribute to reducing the cost of screening and
management for MetS population.

</details>


### [169] [TRAIL: Trace Reasoning and Agentic Issue Localization](https://arxiv.org/abs/2505.08638)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Jitin Krishnan,Anand Kannappan,Rebecca Qian*

Main category: cs.AI

TL;DR: 论文提出了一种动态评估代理工作流痕迹的方法，并引入了一个错误分类法，同时发布了148条人工标注的痕迹数据集（TRAIL）。


<details>
  <summary>Details</summary>
Motivation: 随着代理工作流的广泛应用，现有的人工评估方法无法应对其复杂性和规模增长，亟需一种可扩展的系统化评估方法。

Method: 论文提出了一种错误分类法，并构建了基于真实场景的148条人工标注痕迹数据集（TRAIL），用于评估代理系统的表现。

Result: 实验显示，现代长上下文LLM在痕迹调试上表现不佳，最佳模型Gemini-2.5-pro在TRAIL上仅得11%。

Conclusion: 论文呼吁开发更强大的评估工具，并公开数据集和代码以推动代理工作流的可扩展评估研究。

Abstract: The increasing adoption of agentic workflows across diverse domains brings a
critical need to scalably and systematically evaluate the complex traces these
systems generate. Current evaluation methods depend on manual, domain-specific
human analysis of lengthy workflow traces - an approach that does not scale
with the growing complexity and volume of agentic outputs. Error analysis in
these settings is further complicated by the interplay of external tool outputs
and language model reasoning, making it more challenging than traditional
software debugging. In this work, we (1) articulate the need for robust and
dynamic evaluation methods for agentic workflow traces, (2) introduce a formal
taxonomy of error types encountered in agentic systems, and (3) present a set
of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and
grounded in established agentic benchmarks. To ensure ecological validity, we
curate traces from both single and multi-agent systems, focusing on real-world
applications such as software engineering and open-world information retrieval.
Our evaluations reveal that modern long context LLMs perform poorly at trace
debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our
dataset and code are made publicly available to support and accelerate future
research in scalable evaluation for agentic workflows.

</details>


### [170] [WixQA: A Multi-Dataset Benchmark for Enterprise Retrieval-Augmented Generation](https://arxiv.org/abs/2505.08643)
*Dvir Cohen,Lin Burg,Sviatoslav Pykhnivskyi,Hagit Gur,Stanislav Kovynov,Olga Atzmon,Gilad Barkan*

Main category: cs.AI

TL;DR: WixQA是一个针对企业问答系统的基准测试套件，包含三个数据集，基于Wix.com客户支持交互和知识库快照，用于全面评估检索增强生成（RAG）系统。


<details>
  <summary>Details</summary>
Motivation: 企业问答系统需要反映实际用户问题的数据集，而现有开放域数据集无法满足这一需求。WixQA旨在填补这一空白，提供基于具体知识库的评估基准。

Method: WixQA包含三个数据集：(i) WixQA-ExpertWritten，200个真实用户查询和专家编写的多步答案；(ii) WixQA-Simulated，200个专家验证的QA对；(iii) WixQA-Synthetic，6,222个LLM生成的QA对。

Result: WixQA提供了知识库快照和数据集，并发布了基线结果，为企业RAG系统的评估提供了独特基准。

Conclusion: WixQA为企业在真实环境中评估RAG系统提供了实用工具，填补了现有基准的不足。

Abstract: Retrieval-Augmented Generation (RAG) is a cornerstone of modern question
answering (QA) systems, enabling grounded answers based on external knowledge.
Although recent progress has been driven by open-domain datasets, enterprise QA
systems need datasets that mirror the concrete, domain-specific issues users
raise in day-to-day support scenarios. Critically, evaluating end-to-end RAG
systems requires benchmarks comprising not only question--answer pairs but also
the specific knowledge base (KB) snapshot from which answers were derived. To
address this need, we introduce WixQA, a benchmark suite featuring QA datasets
precisely grounded in the released KB corpus, enabling holistic evaluation of
retrieval and generation components. WixQA includes three distinct QA datasets
derived from Wix.com customer support interactions and grounded in a snapshot
of the public Wix Help Center KB: (i) WixQA-ExpertWritten, 200 real user
queries with expert-authored, multi-step answers; (ii) WixQA-Simulated, 200
expert-validated QA pairs distilled from user dialogues; and (iii)
WixQA-Synthetic, 6,222 LLM-generated QA pairs, with one pair systematically
derived from each article in the knowledge base. We release the KB snapshot
alongside the datasets under MIT license and provide comprehensive baseline
results, forming a unique benchmark for evaluating enterprise RAG systems in
realistic enterprise environments.

</details>


### [171] [A Study of Data-driven Methods for Inventory Optimization](https://arxiv.org/abs/2505.08673)
*Lee Yeung Ping,Patrick Wong,Tan Cheng Han*

Main category: cs.AI

TL;DR: 本文分析了三种算法（时间序列、随机森林和深度强化学习）在三种库存模型（缺货损失、双源采购和多级库存模型）中的应用，评估了它们在超市环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究目的是探索数据驱动的高效方法，分析算法的可能性、潜力及当前挑战，以优化库存管理。

Method: 通过比较三种算法在不同模型中的表现，使用数据可视化工具和统计指标评估其效果。

Result: 结果显示各算法在预测准确性、市场适应性和对库存成本及客户满意度的影响上表现不同。

Conclusion: 研究为库存管理决策提供了详细指导，帮助管理者实时跟踪算法性能并识别供应链中的低效环节。

Abstract: This paper shows a comprehensive analysis of three algorithms (Time Series,
Random Forest (RF) and Deep Reinforcement Learning) into three inventory models
(the Lost Sales, Dual-Sourcing and Multi-Echelon Inventory Model). These
methodologies are applied in the supermarket context. The main purpose is to
analyse efficient methods for the data-driven. Their possibility, potential and
current challenges are taken into consideration in this report. By comparing
the results in each model, the effectiveness of each algorithm is evaluated
based on several key performance indicators, including forecast accuracy,
adaptability to market changes, and overall impact on inventory costs and
customer satisfaction levels. The data visualization tools and statistical
metrics are the indicators for the comparisons and show some obvious trends and
patterns that can guide decision-making in inventory management. These tools
enable managers to not only track the performance of different algorithms in
real-time but also to drill down into specific data points to understand the
underlying causes of inventory fluctuations. This level of detail is crucial
for pinpointing inefficiencies and areas for improvement within the supply
chain.

</details>


### [172] [LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs](https://arxiv.org/abs/2505.08704)
*K M Sajjadul Islam,Ayesha Siddika Nipu,Jiawei Wu,Praveen Madiraju*

Main category: cs.AI

TL;DR: 论文探讨了基于提示的大型语言模型（如GPT-4o和DeepSeek-R1）在电子健康记录（EHR）中识别医疗实体的性能，其中GPT-4o结合提示集成方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）中的非结构化临床文本需要高效提取关键医疗实体（如问题、测试和治疗），以支持下游临床应用。

Method: 使用大型语言模型（LLMs）进行基于提示的医疗实体识别，包括零样本、少样本和集成方法。

Result: GPT-4o结合提示集成方法在分类任务中表现最佳，F1分数为0.95，召回率为0.98，优于DeepSeek-R1。

Conclusion: 提示集成方法通过嵌入相似性和多数投票提高了可靠性，GPT-4o在该任务中表现最优。

Abstract: Electronic Health Records (EHRs) are digital records of patient information,
often containing unstructured clinical text. Named Entity Recognition (NER) is
essential in EHRs for extracting key medical entities like problems, tests, and
treatments to support downstream clinical applications. This paper explores
prompt-based medical entity recognition using large language models (LLMs),
specifically GPT-4o and DeepSeek-R1, guided by various prompt engineering
techniques, including zero-shot, few-shot, and an ensemble approach. Among all
strategies, GPT-4o with prompt ensemble achieved the highest classification
performance with an F1-score of 0.95 and recall of 0.98, outperforming
DeepSeek-R1 on the task. The ensemble method improved reliability by
aggregating outputs through embedding-based similarity and majority voting.

</details>


### [173] [DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of Large Language Models](https://arxiv.org/abs/2505.08744)
*Xiaoyang Chen,Xinan Dai,Yu Du,Qian Feng,Naixu Guo,Tingshuo Gu,Yuting Gao,Yingyi Gao,Xudong Han,Xiang Jiang,Yilin Jin,Hongyi Lin,Shisheng Lin,Xiangnan Li,Yuante Li,Yixing Li,Zhentao Lai,Zilu Ma,Yingrong Peng,Jiacheng Qian,Hao-Yu Sun,Jianbo Sun,Zirui Wang,Siwei Wu,Zian Wang,Bin Xu,Jianghao Xu,Yiyang Yu,Zichuan Yang,Hongji Zha,Ruichong Zhang*

Main category: cs.AI

TL;DR: DeepMath团队提出评估数学创造力的标准，并发布DeepMath-Creative基准测试，评估主流LLM的创造力。结果显示，即使宽松评分，最佳模型O3 Mini仅达70%准确率，且表现随问题复杂度下降。


<details>
  <summary>Details</summary>
Motivation: 当前数学LLM主要关注推理能力，创造力评估不足且数据集稀缺，需填补这一空白。

Method: 提出数学创造力评估标准，构建DeepMath-Creative基准测试，系统性评估LLM的创造性解题能力。

Result: 最佳模型O3 Mini在基础本科级任务中准确率70%，复杂问题表现显著下降，依赖记忆重组而非真正创造力。

Conclusion: 当前LLM在熟悉问题中表现尚可，但缺乏真正的创造性洞察或新颖合成能力。

Abstract: To advance the mathematical proficiency of large language models (LLMs), the
DeepMath team has launched an open-source initiative aimed at developing an
open mathematical LLM and systematically evaluating its mathematical
creativity. This paper represents the initial contribution of this initiative.
While recent developments in mathematical LLMs have predominantly emphasized
reasoning skills, as evidenced by benchmarks on elementary to
undergraduate-level mathematical tasks, the creative capabilities of these
models have received comparatively little attention, and evaluation datasets
remain scarce. To address this gap, we propose an evaluation criteria for
mathematical creativity and introduce DeepMath-Creative, a novel, high-quality
benchmark comprising constructive problems across algebra, geometry, analysis,
and other domains. We conduct a systematic evaluation of mainstream LLMs'
creative problem-solving abilities using this dataset. Experimental results
show that even under lenient scoring criteria -- emphasizing core solution
components and disregarding minor inaccuracies, such as small logical gaps,
incomplete justifications, or redundant explanations -- the best-performing
model, O3 Mini, achieves merely 70% accuracy, primarily on basic
undergraduate-level constructive tasks. Performance declines sharply on more
complex problems, with models failing to provide substantive strategies for
open problems. These findings suggest that, although current LLMs display a
degree of constructive proficiency on familiar and lower-difficulty problems,
such performance is likely attributable to the recombination of memorized
patterns rather than authentic creative insight or novel synthesis.

</details>


### [174] [ARC-NCA: Towards Developmental Solutions to the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2505.08778)
*Etienne Guichard,Felix Reimers,Mia Kvalsund,Mikkel Lepperød,Stefano Nichele*

Main category: cs.AI

TL;DR: ARC-NCA利用神经细胞自动机（NCA）及其增强版本（EngramNCA）解决ARC-AGI挑战，展示了在少量示例下实现强大抽象和推理能力，性能接近或超越ChatGPT 4.5，且成本更低。


<details>
  <summary>Details</summary>
Motivation: ARC-AGI是AGI领域的核心挑战，对人类简单但对AI系统极具挑战性，需开发能模拟生物系统发育过程的方法。

Method: 采用标准NCA和带隐藏记忆的EngramNCA，利用其模拟复杂动态和涌现模式的能力，实现适应性推理和抽象。

Result: ARC-NCA在少量示例下表现优异，性能接近或超越ChatGPT 4.5，且成本显著降低。

Conclusion: 将发育原理融入计算模型是提升AI问题解决能力的有效途径，ARC-NCA为AGI研究提供了新方向。

Abstract: The Abstraction and Reasoning Corpus (ARC), later renamed ARC-AGI, poses a
fundamental challenge in artificial general intelligence (AGI), requiring
solutions that exhibit robust abstraction and reasoning capabilities across
diverse tasks, while only few (with median count of three) correct examples are
presented. While ARC-AGI remains very challenging for artificial intelligence
systems, it is rather easy for humans. This paper introduces ARC-NCA, a
developmental approach leveraging standard Neural Cellular Automata (NCA) and
NCA enhanced with hidden memories (EngramNCA) to tackle the ARC-AGI benchmark.
NCAs are employed for their inherent ability to simulate complex dynamics and
emergent patterns, mimicking developmental processes observed in biological
systems. Developmental solutions may offer a promising avenue for enhancing
AI's problem-solving capabilities beyond mere training data extrapolation.
ARC-NCA demonstrates how integrating developmental principles into
computational models can foster adaptive reasoning and abstraction. We show
that our ARC-NCA proof-of-concept results may be comparable to, and sometimes
surpass, that of ChatGPT 4.5, at a fraction of the cost.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [175] [Blockbuster, Part 1: Block-level AI Operator Fusion](https://arxiv.org/abs/2505.07829)
*Ofer Dekel*

Main category: cs.LG

TL;DR: Blockbuster是一个用于AI算子融合的框架，兼容多处理器架构，通过块程序和数据移动建模实现高效融合。


<details>
  <summary>Details</summary>
Motivation: 现有AI程序在内存层级间的数据移动效率低，Blockbuster旨在通过显式建模数据移动提升融合效果。

Method: 采用图表示的块程序和两阶段融合算法（候选选择与规则融合），重点研究规则融合算法。

Result: 成功重新发现Flash Attention核，并实现复杂算子融合（如LayerNorm与矩阵乘的融合）。

Conclusion: Blockbuster的规则融合算法通过显式数据移动建模，显著提升了AI程序的融合能力。

Abstract: Blockbuster is a framework for AI operator fusion in inference programs. The
Blockbuster framework is compatible with any multiprocessor architecture that
has a tiered memory hierarchy, including GPUs, multi-core CPUs, and some AI
accelerator chips. It includes a graph-based representation for AI workloads,
called a block program, which explicitly models how blocks of data move between
the memory tiers. It also includes an operator fusion procedure, which is made
up of a candidate selection algorithm and a fusion algorithm that fuses each
individual candidate - this two-algorithm structure makes Blockbuster
especially suitable for large AI programs. The current paper focuses on the
fusion algorithm, which is a rule-based technique. While the literature is full
of previous rule-based fusion algorithms, what sets our algorithm apart is its
direct modeling of data movement between memory tiers, resulting in uniquely
powerful fusion results. As a first sanity check, we demonstrate how our
algorithm automatically rediscovers the well-known Flash Attention kernel.
Then, we demonstrate the real power of our approach by fusing LayerNorm with
matrix multiplication and RMSNorm with FNN-SwiGLU - the latter involves fusing
three matrix multiplications, a Hadamard product, a reduction, and a few
elementwise operations into a single mega-kernel.

</details>


### [176] [A General Approach of Automated Environment Design for Learning the Optimal Power Flow](https://arxiv.org/abs/2505.07832)
*Thomas Wolgast,Astrid Nieße*

Main category: cs.LG

TL;DR: 提出了一种利用多目标优化自动设计强化学习环境的方法，并在最优潮流问题上验证其优于手动设计。


<details>
  <summary>Details</summary>
Motivation: 解决如何设计强化学习环境以最大化训练性能的问题，尤其是在最优潮流问题中。

Method: 采用多目标优化的超参数优化框架，利用现有算法自动设计环境。

Result: 在五个最优潮流基准测试中，自动设计方法优于手动设计，并揭示了环境设计的关键决策。

Conclusion: 首次提出自动化设计强化学习环境的通用方法，并讨论了过拟合风险。

Abstract: Reinforcement learning (RL) algorithms are increasingly used to solve the
optimal power flow (OPF) problem. Yet, the question of how to design RL
environments to maximize training performance remains unanswered, both for the
OPF and the general case. We propose a general approach for automated RL
environment design by utilizing multi-objective optimization. For that, we use
the hyperparameter optimization (HPO) framework, which allows the reuse of
existing HPO algorithms and methods. On five OPF benchmark problems, we
demonstrate that our automated design approach consistently outperforms a
manually created baseline environment design. Further, we use statistical
analyses to determine which environment design decisions are especially
important for performance, resulting in multiple novel insights on how RL-OPF
environments should be designed. Finally, we discuss the risk of overfitting
the environment to the utilized RL algorithm. To the best of our knowledge,
this is the first general approach for automated RL environment design.

</details>


### [177] [Representation Learning with Mutual Influence of Modalities for Node Classification in Multi-Modal Heterogeneous Networks](https://arxiv.org/abs/2505.07895)
*Jiafan Li,Jiaqi Zhu,Liang Chang,Yilin Li,Miaomiao Li,Yang Wang,Hongan Wang*

Main category: cs.LG

TL;DR: 提出了一种名为HGNN-IMA的新模型，用于多模态异构网络中的节点分类，通过跨模态注意力机制实现自适应多模态融合。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法在保留模态特性或跨模态指导方面存在不足，需改进。

Method: 采用异构图变换器框架，结合嵌套跨模态注意力机制和模态对齐，增强信息传播。

Result: 实验验证了模型在节点分类任务中的优越性。

Conclusion: HGNN-IMA为处理多模态数据提供了创新视角，尤其在网络结构场景下。

Abstract: Nowadays, numerous online platforms can be described as multi-modal
heterogeneous networks (MMHNs), such as Douban's movie networks and Amazon's
product review networks. Accurately categorizing nodes within these networks is
crucial for analyzing the corresponding entities, which requires effective
representation learning on nodes. However, existing multi-modal fusion methods
often adopt either early fusion strategies which may lose the unique
characteristics of individual modalities, or late fusion approaches overlooking
the cross-modal guidance in GNN-based information propagation. In this paper,
we propose a novel model for node classification in MMHNs, named Heterogeneous
Graph Neural Network with Inter-Modal Attention (HGNN-IMA). It learns node
representations by capturing the mutual influence of multiple modalities during
the information propagation process, within the framework of heterogeneous
graph transformer. Specifically, a nested inter-modal attention mechanism is
integrated into the inter-node attention to achieve adaptive multi-modal
fusion, and modality alignment is also taken into account to encourage the
propagation among nodes with consistent similarities across all modalities.
Moreover, an attention loss is augmented to mitigate the impact of missing
modalities. Extensive experiments validate the superiority of the model in the
node classification task, providing an innovative view to handle multi-modal
data, especially when accompanied with network structures.

</details>


### [178] [Latent Behavior Diffusion for Sequential Reaction Generation in Dyadic Setting](https://arxiv.org/abs/2505.07901)
*Minh-Duc Nguyen,Hyung-Jeong Yang,Soo-Hyung Kim,Ji-Eun Shin,Seung-Won Kim*

Main category: cs.LG

TL;DR: 论文提出了一种基于潜在行为扩散模型的方法，用于生成与对话伙伴行为一致的面部反应，提升交互模拟的自然性和效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在生成多样且上下文相关的面部反应时的挑战。

Method: 结合上下文感知自编码器和扩散条件生成器，自编码器压缩输入特征，扩散生成器在潜在空间生成反应。

Result: 实验表明该方法在二元反应合成任务中优于现有方法。

Conclusion: 潜在行为扩散模型能有效生成多样且上下文相关的面部反应。

Abstract: The dyadic reaction generation task involves synthesizing responsive facial
reactions that align closely with the behaviors of a conversational partner,
enhancing the naturalness and effectiveness of human-like interaction
simulations. This paper introduces a novel approach, the Latent Behavior
Diffusion Model, comprising a context-aware autoencoder and a diffusion-based
conditional generator that addresses the challenge of generating diverse and
contextually relevant facial reactions from input speaker behaviors. The
autoencoder compresses high-dimensional input features, capturing dynamic
patterns in listener reactions while condensing complex input data into a
concise latent representation, facilitating more expressive and contextually
appropriate reaction synthesis. The diffusion-based conditional generator
operates on the latent space generated by the autoencoder to predict realistic
facial reactions in a non-autoregressive manner. This approach allows for
generating diverse facial reactions that reflect subtle variations in
conversational cues and emotional states. Experimental results demonstrate the
effectiveness of our approach in achieving superior performance in dyadic
reaction synthesis tasks compared to existing methods.

</details>


### [179] [A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny](https://arxiv.org/abs/2505.07908)
*Karahan Sarıtaş,Çağatay Yıldız*

Main category: cs.LG

TL;DR: 本文重新审视了自注意力机制是否实现核主成分分析（KPCA）的论点，发现其缺乏实证支持。


<details>
  <summary>Details</summary>
Motivation: 验证自注意力机制是否如Teo等人（2024）所述，实现了KPCA，并分析了其不一致性。

Method: 通过分析自注意力值向量与KPCA特征向量的相似性、重构损失及Gram矩阵特征值统计，验证其一致性。

Result: 发现自注意力与KPCA之间无显著对应关系，相关统计量差异巨大且不可复现。

Conclusion: 自注意力的KPCA解释缺乏实证依据。

Abstract: In this reproduction study, we revisit recent claims that self-attention
implements kernel principal component analysis (KPCA) (Teo et al., 2024),
positing that (i) value vectors $V$ capture the eigenvectors of the Gram matrix
of the keys, and (ii) that self-attention projects queries onto the principal
component axes of the key matrix $K$ in a feature space. Our analysis reveals
three critical inconsistencies: (1) No alignment exists between learned
self-attention value vectors and what is proposed in the KPCA perspective, with
average similarity metrics (optimal cosine similarity $\leq 0.32$, linear CKA
(Centered Kernel Alignment) $\leq 0.11$, kernel CKA $\leq 0.32$) indicating
negligible correspondence; (2) Reported decreases in reconstruction loss
$J_\text{proj}$, arguably justifying the claim that the self-attention
minimizes the projection error of KPCA, are misinterpreted, as the quantities
involved differ by orders of magnitude ($\sim\!10^3$); (3) Gram matrix
eigenvalue statistics, introduced to justify that $V$ captures the eigenvector
of the gram matrix, are irreproducible without undocumented
implementation-specific adjustments. Across 10 transformer architectures, we
conclude that the KPCA interpretation of self-attention lacks empirical
support.

</details>


### [180] [Tuning for Trustworthiness -- Balancing Performance and Explanation Consistency in Neural Network Optimization](https://arxiv.org/abs/2505.07910)
*Alexander Hinterleitner,Thomas Bartz-Beielstein*

Main category: cs.LG

TL;DR: 论文提出了一种新的XAI一致性概念，并将其纳入超参数调优目标，通过多目标优化框架平衡预测性能和解释稳健性。


<details>
  <summary>Details</summary>
Motivation: 当前超参数调优和神经网络架构优化中，可解释性常被忽视，研究旨在填补这一空白。

Method: 提出XAI一致性指标，并集成到SPOT工具箱中，采用加权聚合和基于期望的策略进行模型选择。

Result: 识别出架构配置空间中的三个区域：性能差且解释性低、性能强但解释性弱、以及平衡两者的折衷区域。

Conclusion: 研究为未来探索折衷区域模型的鲁棒性提供了基础，可能避免过拟合并提升分布外数据的预测可靠性。

Abstract: Despite the growing interest in Explainable Artificial Intelligence (XAI),
explainability is rarely considered during hyperparameter tuning or neural
architecture optimization, where the focus remains primarily on minimizing
predictive loss. In this work, we introduce the novel concept of XAI
consistency, defined as the agreement among different feature attribution
methods, and propose new metrics to quantify it. For the first time, we
integrate XAI consistency directly into the hyperparameter tuning objective,
creating a multi-objective optimization framework that balances predictive
performance with explanation robustness. Implemented within the Sequential
Parameter Optimization Toolbox (SPOT), our approach uses both weighted
aggregation and desirability-based strategies to guide model selection. Through
our proposed framework and supporting tools, we explore the impact of
incorporating XAI consistency into the optimization process. This enables us to
characterize distinct regions in the architecture configuration space: one
region with poor performance and comparatively low interpretability, another
with strong predictive performance but weak interpretability due to low
\gls{xai} consistency, and a trade-off region that balances both objectives by
offering high interpretability alongside competitive performance. Beyond
introducing this novel approach, our research provides a foundation for future
investigations into whether models from the trade-off zone-balancing
performance loss and XAI consistency-exhibit greater robustness by avoiding
overfitting to training performance, thereby leading to more reliable
predictions on out-of-distribution data.

</details>


### [181] [Combining Bayesian Inference and Reinforcement Learning for Agent Decision Making: A Review](https://arxiv.org/abs/2505.07911)
*Chengmin Zhou,Ville Kyrki,Pasi Fränti,Laura Ruotsalainen*

Main category: cs.LG

TL;DR: 本文综述了贝叶斯推理与强化学习（RL）结合在智能体决策中的进展，涵盖基础方法、经典与最新结合方式、性能比较及复杂问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推理在智能体决策中具有数据效率、泛化性、可解释性和安全性等优势，但目前缺乏系统性综述。本文旨在填补这一空白。

Method: 讨论了五类主题：1）贝叶斯方法；2）经典贝叶斯与RL结合；3）最新结合方法；4）性能分析；5）复杂RL问题中的应用。

Result: 总结了贝叶斯方法在RL各阶段的作用，为智能体决策提供了更优策略。

Conclusion: 贝叶斯与RL的结合在复杂决策问题中展现出潜力，未来研究可进一步优化其应用。

Abstract: Bayesian inference has many advantages in decision making of agents (e.g.
robotics/simulative agent) over a regular data-driven black-box neural network:
Data-efficiency, generalization, interpretability, and safety where these
advantages benefit directly/indirectly from the uncertainty quantification of
Bayesian inference. However, there are few comprehensive reviews to summarize
the progress of Bayesian inference on reinforcement learning (RL) for decision
making to give researchers a systematic understanding. This paper focuses on
combining Bayesian inference with RL that nowadays is an important approach in
agent decision making. To be exact, this paper discusses the following five
topics: 1) Bayesian methods that have potential for agent decision making.
First basic Bayesian methods and models (Bayesian rule, Bayesian learning, and
Bayesian conjugate models) are discussed followed by variational inference,
Bayesian optimization, Bayesian deep learning, Bayesian active learning,
Bayesian generative models, Bayesian meta-learning, and lifelong Bayesian
learning. 2) Classical combinations of Bayesian methods with model-based RL
(with approximation methods), model-free RL, and inverse RL. 3) Latest
combinations of potential Bayesian methods with RL. 4) Analytical comparisons
of methods that combine Bayesian methods with RL with respect to
data-efficiency, generalization, interpretability, and safety. 5) In-depth
discussions in six complex problem variants of RL, including unknown reward,
partial-observability, multi-agent, multi-task, non-linear non-Gaussian, and
hierarchical RL problems and the summary of how Bayesian methods work in the
data collection, data processing and policy learning stages of RL to pave the
way for better agent decision-making strategies.

</details>


### [182] [On-Device Crack Segmentation for Edge Structural Health Monitoring](https://arxiv.org/abs/2505.07915)
*Yuxuan Zhang,Ye Xu,Luciano Sebastian Martinez-Rau,Quynh Nguyen Phuong Vu,Bengt Oelmann,Sebastian Bader*

Main category: cs.LG

TL;DR: 该研究探索了轻量级U-Net架构，用于资源受限的微控制器上的裂缝分割，通过减少卷积核、网络深度和使用深度可分离卷积，实现了性能与资源消耗的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 裂缝分割在结构健康监测（SHM）中至关重要，但在资源受限的微控制器上部署深度学习模型面临内存、计算能力和能源限制的挑战。

Method: 研究采用三种优化策略：减少卷积核数量、降低网络深度和使用深度可分离卷积（DWConv2D）。

Result: 结果显示，减少卷积核和网络深度显著降低了RAM、Flash需求和推理时间，但牺牲了一些准确性。优化后的网络适合低功耗TinyML应用。

Conclusion: 该研究不仅推动了基于TinyML的裂缝分割，还为能源自主的边缘SHM系统提供了可能性。

Abstract: Crack segmentation can play a critical role in Structural Health Monitoring
(SHM) by enabling accurate identification of crack size and location, which
allows to monitor structural damages over time. However, deploying deep
learning models for crack segmentation on resource-constrained microcontrollers
presents significant challenges due to limited memory, computational power, and
energy resources. To address these challenges, this study explores lightweight
U-Net architectures tailored for TinyML applications, focusing on three
optimization strategies: filter number reduction, network depth reduction, and
the use of Depthwise Separable Convolutions (DWConv2D). Our results demonstrate
that reducing convolution kernels and network depth significantly reduces RAM
and Flash requirement, and inference times, albeit with some accuracy
trade-offs. Specifically, by reducing the filer number to 25%, the network
depth to four blocks, and utilizing depthwise convolutions, a good compromise
between segmentation performance and resource consumption is achieved. This
makes the network particularly suitable for low-power TinyML applications. This
study not only advances TinyML-based crack segmentation but also provides the
possibility for energy-autonomous edge SHM systems.

</details>


### [183] [Self-cross Feature based Spiking Neural Networks for Efficient Few-shot Learning](https://arxiv.org/abs/2505.07921)
*Qi Xu,Junyang Zhu,Dongdong Zhou,Hao Chen,Yang Liu,Jiangrong Shen,Qiang Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种基于脉冲神经网络（SNN）的小样本学习框架，通过自特征提取器和跨特征对比模块提升性能并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNN）在小样本学习中表现优异，但计算成本高且扩展性差。SNN因其事件驱动和低能耗特性更适合处理稀疏动态数据，但在复杂时空特征提取和跨类比较上仍有不足。

Method: 结合自特征提取器和跨特征对比模块，采用时间高效训练损失和InfoNCE损失优化脉冲序列的动态特性。

Result: 在神经形态数据集N-Omniglot上分类性能显著提升，在静态数据集CUB和miniImageNet上性能接近ANN且能耗低。

Conclusion: 提出的FSL-SNN框架在小样本学习中高效且节能，为SNN在复杂任务中的应用提供了新思路。

Abstract: Deep neural networks (DNNs) excel in computer vision tasks, especially,
few-shot learning (FSL), which is increasingly important for generalizing from
limited examples. However, DNNs are computationally expensive with scalability
issues in real world. Spiking Neural Networks (SNNs), with their event-driven
nature and low energy consumption, are particularly efficient in processing
sparse and dynamic data, though they still encounter difficulties in capturing
complex spatiotemporal features and performing accurate cross-class
comparisons. To further enhance the performance and efficiency of SNNs in
few-shot learning, we propose a few-shot learning framework based on SNNs,
which combines a self-feature extractor module and a cross-feature contrastive
module to refine feature representation and reduce power consumption. We apply
the combination of temporal efficient training loss and InfoNCE loss to
optimize the temporal dynamics of spike trains and enhance the discriminative
power. Experimental results show that the proposed FSL-SNN significantly
improves the classification performance on the neuromorphic dataset N-Omniglot,
and also achieves competitive performance to ANNs on static datasets such as
CUB and miniImageNet with low power consumption.

</details>


### [184] [Symbolic Regression with Multimodal Large Language Models and Kolmogorov Arnold Networks](https://arxiv.org/abs/2505.07956)
*Thomas R. Harvey,Fabian Ruehle,Cristofero S. Fraser-Taliente,James Halverson*

Main category: cs.LG

TL;DR: 提出了一种基于视觉能力大语言模型（LLM）和Funsearch思想的符号回归新方法，通过遗传算法优化函数表达式。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归方法需要预设函数集，而新方法通过提示工程灵活生成表达式，简化流程。

Method: 利用LLM从函数图像生成初始表达式，通过数值优化拟合参数，结合遗传算法和KANs扩展到多元函数。

Result: 证明了单变量函数足以完成符号回归，并通过KANs扩展到多元函数，简化表达式。

Conclusion: 新方法无需预设函数集，灵活高效，适用于符号回归任务。

Abstract: We present a novel approach to symbolic regression using vision-capable large
language models (LLMs) and the ideas behind Google DeepMind's Funsearch. The
LLM is given a plot of a univariate function and tasked with proposing an
ansatz for that function. The free parameters of the ansatz are fitted using
standard numerical optimisers, and a collection of such ans\"atze make up the
population of a genetic algorithm. Unlike other symbolic regression techniques,
our method does not require the specification of a set of functions to be used
in regression, but with appropriate prompt engineering, we can arbitrarily
condition the generative step. By using Kolmogorov Arnold Networks (KANs), we
demonstrate that ``univariate is all you need'' for symbolic regression, and
extend this method to multivariate functions by learning the univariate
function on each edge of a trained KAN. The combined expression is then
simplified by further processing with a language model.

</details>


### [185] [Making Small Language Models Efficient Reasoners: Intervention, Supervision, Reinforcement](https://arxiv.org/abs/2505.07961)
*Xuechen Zhang,Zijian Huang,Chenchun Ni,Ziyang Xiong,Jiasi Chen,Samet Oymak*

Main category: cs.LG

TL;DR: 本文提出两种方法（温度缩放和TLDR强化学习）优化小型语言模型的推理效率，减少冗余计算，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在推理过程中存在冗余和计算成本高的问题，尤其是经过监督微调（SFT）的模型。

Method: 1. 温度缩放（TS）控制推理停止点；2. TLDR（基于GRPO的长度正则化强化学习）实现多级推理长度控制。

Result: 在四个推理基准测试中，TS和TLDR显著提高了计算效率（TLDR节省约50%的token），且准确性损失极小。

Conclusion: 研究揭示了控制推理停止时间的重要性，并提供了针对小型模型的高效算法解决方案。

Abstract: Recent research enhances language model reasoning by scaling test-time
compute via longer chain-of-thought traces. This often improves accuracy but
also introduces redundancy and high computational cost, especially for small
language models distilled with supervised fine-tuning (SFT). In this work, we
propose new algorithms to improve token-efficient reasoning with small-scale
models by effectively trading off accuracy and computation. We first show that
the post-SFT model fails to determine the optimal stopping point of the
reasoning process, resulting in verbose and repetitive outputs. Verbosity also
significantly varies across wrong vs correct responses. To address these
issues, we propose two solutions: (1) Temperature scaling (TS) to control the
stopping point for the thinking phase and thereby trace length, and (2) TLDR: a
length-regularized reinforcement learning method based on GRPO that facilitates
multi-level trace length control (e.g. short, medium, long reasoning).
Experiments on four reasoning benchmarks, MATH500, AMC, AIME24 and
OlympiadBench, demonstrate that TS is highly effective compared to s1's budget
forcing approach and TLDR significantly improves token efficiency by about 50%
with minimal to no accuracy loss over the SFT baseline. Moreover, TLDR also
facilitates flexible control over the response length, offering a practical and
effective solution for token-efficient reasoning in small models. Ultimately,
our work reveals the importance of stopping time control, highlights
shortcomings of pure SFT, and provides effective algorithmic recipes.

</details>


### [186] [Fair Play for Individuals, Foul Play for Groups? Auditing Anonymization's Impact on ML Fairness](https://arxiv.org/abs/2505.07985)
*Héber H. Arcolezi,Mina Alishahi,Adda-Akram Bendoukha,Nesrine Kaaniche*

Main category: cs.LG

TL;DR: 论文研究了匿名化技术（如k-匿名、l-多样性和t-接近性）对机器学习公平性的影响，发现匿名化可能显著降低群体公平性，但提升个体公平性。


<details>
  <summary>Details</summary>
Motivation: 机器学习训练数据常包含敏感信息，匿名化技术用于保护隐私，但其对公平性的影响尚不明确。

Method: 通过系统审计匿名化技术对机器学习公平性的影响，评估个体和群体公平性。

Result: 匿名化可能导致群体公平性指标下降四个数量级，但相似性个体公平性指标因输入同质性增强而改善。

Conclusion: 研究揭示了隐私、公平性和实用性之间的权衡，为负责任的人工智能开发提供了指导。

Abstract: Machine learning (ML) algorithms are heavily based on the availability of
training data, which, depending on the domain, often includes sensitive
information about data providers. This raises critical privacy concerns.
Anonymization techniques have emerged as a practical solution to address these
issues by generalizing features or suppressing data to make it more difficult
to accurately identify individuals. Although recent studies have shown that
privacy-enhancing technologies can influence ML predictions across different
subgroups, thus affecting fair decision-making, the specific effects of
anonymization techniques, such as $k$-anonymity, $\ell$-diversity, and
$t$-closeness, on ML fairness remain largely unexplored. In this work, we
systematically audit the impact of anonymization techniques on ML fairness,
evaluating both individual and group fairness. Our quantitative study reveals
that anonymization can degrade group fairness metrics by up to four orders of
magnitude. Conversely, similarity-based individual fairness metrics tend to
improve under stronger anonymization, largely as a result of increased input
homogeneity. By analyzing varying levels of anonymization across diverse
privacy settings and data distributions, this study provides critical insights
into the trade-offs between privacy, fairness, and utility, offering actionable
guidelines for responsible AI development. Our code is publicly available at:
https://github.com/hharcolezi/anonymity-impact-fairness.

</details>


### [187] [A Scalable System to Prove Machine Learning Fairness in Zero-Knowledge](https://arxiv.org/abs/2505.07997)
*Tianyu Zhang,Shen Dong,O. Deniz Kose,Yanning Shen,Yupeng Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种基于零知识证明的方法FairZK，用于验证机器学习模型的公平性，同时保护模型机密性。通过优化公平性测量和高效协议，显著提升了验证速度。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在关键应用中的普及，确保其决策公平性变得重要，但传统方法需要公开模型参数，可能泄露机密。

Method: 提出一种仅需模型参数和输入聚合信息测量公平性的方法，并开发高效零知识证明协议。

Result: FairZK系统显著快于现有方法，支持大规模模型（4700万参数），验证时间仅343秒。

Conclusion: FairZK为机器学习公平性验证提供了一种高效且保密的新方案。

Abstract: With the rise of machine learning techniques, ensuring the fairness of
decisions made by machine learning algorithms has become of great importance in
critical applications. However, measuring fairness often requires full access
to the model parameters, which compromises the confidentiality of the models.
In this paper, we propose a solution using zero-knowledge proofs, which allows
the model owner to convince the public that a machine learning model is fair
while preserving the secrecy of the model. To circumvent the efficiency barrier
of naively proving machine learning inferences in zero-knowledge, our key
innovation is a new approach to measure fairness only with model parameters and
some aggregated information of the input, but not on any specific dataset. To
achieve this goal, we derive new bounds for the fairness of logistic regression
and deep neural network models that are tighter and better reflecting the
fairness compared to prior work. Moreover, we develop efficient zero-knowledge
proof protocols for common computations involved in measuring fairness,
including the spectral norm of matrices, maximum, absolute value, and
fixed-point arithmetic.
  We have fully implemented our system, FairZK, that proves machine learning
fairness in zero-knowledge. Experimental results show that FairZK is
significantly faster than the naive approach and an existing scheme that use
zero-knowledge inferences as a subroutine. The prover time is improved by
3.1x--1789x depending on the size of the model and the dataset. FairZK can
scale to a large model with 47 million parameters for the first time, and
generates a proof for its fairness in 343 seconds. This is estimated to be 4
orders of magnitude faster than existing schemes, which only scale to small
models with hundreds to thousands of parameters.

</details>


### [188] [Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks](https://arxiv.org/abs/2505.08022)
*Steffen Schotthöfer,H. Lexie Yang,Stefan Schnake*

Main category: cs.LG

TL;DR: 提出一种动态低秩训练方法，结合谱正则化，提升压缩模型的对抗鲁棒性，同时保持干净数据的准确性。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备上部署神经网络需要模型既紧凑又对对抗输入鲁棒，但压缩与鲁棒性常冲突。

Method: 引入动态低秩训练方案，结合谱正则化控制每层低秩核的条件数。

Result: 实验显示，该方法在标准架构、数据集和对抗攻击下，能实现94%压缩率，同时恢复或提升对抗精度。

Conclusion: 该方法高效、模型与数据无关，支持自适应压缩，平衡压缩与鲁棒性。

Abstract: Deployment of neural networks on resource-constrained devices demands models
that are both compact and robust to adversarial inputs. However, compression
and adversarial robustness often conflict. In this work, we introduce a
dynamical low-rank training scheme enhanced with a novel spectral regularizer
that controls the condition number of the low-rank core in each layer. This
approach mitigates the sensitivity of compressed models to adversarial
perturbations without sacrificing clean accuracy. The method is model- and
data-agnostic, computationally efficient, and supports rank adaptivity to
automatically compress the network at hand. Extensive experiments across
standard architectures, datasets, and adversarial attacks show the regularized
networks can achieve over 94% compression while recovering or improving
adversarial accuracy relative to uncompressed baselines.

</details>


### [189] [Demo: A Practical Testbed for Decentralized Federated Learning on Physical Edge Devices](https://arxiv.org/abs/2505.08033)
*Chao Feng,Nicolas Huber,Alberto Huertas Celdran,Gerome Bovet,Burkhard Stiller*

Main category: cs.LG

TL;DR: 该论文研究了去中心化联邦学习（DFL）在实际边缘设备上的应用，通过构建物理测试平台并引入能耗监测模块，验证了通信拓扑对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习依赖中心服务器，存在单点故障风险，而去中心化联邦学习（DFL）解决了这一问题，但在资源受限设备上的实际应用仍面临挑战。

Method: 设计并部署了一个基于边缘设备（如Raspberry Pi和Jetson Nano）的物理测试平台，扩展了DFL训练平台NEBULA，并加入能耗监测模块。

Result: 实验表明，通信拓扑密度对DFL模型性能有显著影响，拓扑越密集，性能越好。

Conclusion: 去中心化联邦学习在边缘设备上具有实际可行性，通信拓扑设计是优化性能的关键。

Abstract: Federated Learning (FL) enables collaborative model training without sharing
raw data, preserving participant privacy. Decentralized FL (DFL) eliminates
reliance on a central server, mitigating the single point of failure inherent
in the traditional FL paradigm, while introducing deployment challenges on
resource-constrained devices. To evaluate real-world applicability, this work
designs and deploys a physical testbed using edge devices such as Raspberry Pi
and Jetson Nano. The testbed is built upon a DFL training platform, NEBULA, and
extends it with a power monitoring module to measure energy consumption during
training. Experiments across multiple datasets show that model performance is
influenced by the communication topology, with denser topologies leading to
better outcomes in DFL settings.

</details>


### [190] [Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders](https://arxiv.org/abs/2505.08080)
*Dong Shu,Xuansheng Wu,Haiyan Zhao,Mengnan Du,Ninghao Liu*

Main category: cs.LG

TL;DR: 论文提出了一种名为GradSAE的新方法，通过结合输出端梯度信息来识别稀疏自编码器中最具影响力的潜在特征，以验证潜在特征对模型输出的因果影响。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器分析方法仅依赖输入端激活，忽略了潜在特征对模型输出的因果影响。论文假设激活的潜在特征对输出的贡献不均，且只有高因果影响的特征对模型操控有效。

Method: 提出GradSAE方法，通过输出端梯度信息识别最具影响力的潜在特征。

Result: 验证了潜在特征对模型输出的因果影响，并证明高因果影响的特征对模型操控更有效。

Conclusion: GradSAE是一种简单有效的方法，能够更准确地识别和操控稀疏自编码器中的关键潜在特征。

Abstract: Sparse Autoencoders (SAEs) have recently emerged as powerful tools for
interpreting and steering the internal representations of large language models
(LLMs). However, conventional approaches to analyzing SAEs typically rely
solely on input-side activations, without considering the causal influence
between each latent feature and the model's output. This work is built on two
key hypotheses: (1) activated latents do not contribute equally to the
construction of the model's output, and (2) only latents with high causal
influence are effective for model steering. To validate these hypotheses, we
propose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method
that identifies the most influential latents by incorporating output-side
gradient information.

</details>


### [191] [Fréchet Power-Scenario Distance: A Metric for Evaluating Generative AI Models across Multiple Time-Scales in Smart Grids](https://arxiv.org/abs/2505.08082)
*Yuting Cai,Shaohuai Liu,Chao Tian,Le Xie*

Main category: cs.LG

TL;DR: 提出一种基于Fréchet距离的新指标，用于评估智能电网中生成AI模型产生的合成数据质量。


<details>
  <summary>Details</summary>
Motivation: 传统欧氏距离指标无法有效评估合成数据集间的质量差异，需要一种从分布角度评估的方法。

Method: 使用Fréchet距离在学习的特征空间中估计两个数据集间的距离。

Result: 实证结果表明该指标在不同时间尺度和模型中表现优越。

Conclusion: 新指标提升了智能电网数据驱动决策的可靠性。

Abstract: Generative artificial intelligence (AI) models in smart grids have advanced
significantly in recent years due to their ability to generate large amounts of
synthetic data, which would otherwise be difficult to obtain in the real world
due to confidentiality constraints. A key challenge in utilizing such synthetic
data is how to assess the data quality produced from such generative models.
Traditional Euclidean distance-based metrics only reflect pair-wise relations
between two individual samples, and could fail in evaluating quality
differences between groups of synthetic datasets. In this work, we propose a
novel metric based on the Fr\'{e}chet Distance (FD) estimated between two
datasets in a learned feature space. The proposed method evaluates the quality
of generation from a distributional perspective. Empirical results demonstrate
the superiority of the proposed metric across timescales and models, enhancing
the reliability of data-driven decision-making in smart grid operations.

</details>


### [192] [A Federated Random Forest Solution for Secure Distributed Machine Learning](https://arxiv.org/abs/2505.08085)
*Alexandre Cotorobai,Jorge Miguel Silva,Jose Luis Oliveira*

Main category: cs.LG

TL;DR: 本文提出了一种支持随机森林分类器的联邦学习框架，解决了隐私和监管障碍，同时保持高性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 隐私和监管障碍阻碍了集中式机器学习解决方案，尤其是在医疗领域。现有联邦学习框架主要支持基于梯度的模型，缺乏对树基方法的支持。

Method: 利用PySyft进行隐私保护计算，支持加权模型平均、增量学习和本地评估，实现多机构协作训练随机森林模型。

Result: 在真实医疗数据集上，联邦方法的预测准确率与集中式方法相差不超过9%，同时满足严格隐私要求。

Conclusion: 该框架填补了联邦学习库的空白，适用于需要透明性和可靠性能的分布式机器学习任务。

Abstract: Privacy and regulatory barriers often hinder centralized machine learning
solutions, particularly in sectors like healthcare where data cannot be freely
shared. Federated learning has emerged as a powerful paradigm to address these
concerns; however, existing frameworks primarily support gradient-based models,
leaving a gap for more interpretable, tree-based approaches. This paper
introduces a federated learning framework for Random Forest classifiers that
preserves data privacy and provides robust performance in distributed settings.
By leveraging PySyft for secure, privacy-aware computation, our method enables
multiple institutions to collaboratively train Random Forest models on locally
stored data without exposing sensitive information. The framework supports
weighted model averaging to account for varying data distributions, incremental
learning to progressively refine models, and local evaluation to assess
performance across heterogeneous datasets. Experiments on two real-world
healthcare benchmarks demonstrate that the federated approach maintains
competitive predictive accuracy - within a maximum 9\% margin of centralized
methods - while satisfying stringent privacy requirements. These findings
underscore the viability of tree-based federated learning for scenarios where
data cannot be centralized due to regulatory, competitive, or technical
constraints. The proposed solution addresses a notable gap in existing
federated learning libraries, offering an adaptable tool for secure distributed
machine learning tasks that demand both transparency and reliable performance.
The tool is available at https://github.com/ieeta-pt/fed_rf.

</details>


### [193] [Manifold Learning with Normalizing Flows: Towards Regularity, Expressivity and Iso-Riemannian Geometry](https://arxiv.org/abs/2505.08087)
*Willem Diepeveen,Deanna Needell*

Main category: cs.LG

TL;DR: 论文提出了一种在多模态数据中解决扭曲和建模误差的方法，通过等距化学习的黎曼结构和平衡微分同胚参数化的规律性与表达性。


<details>
  <summary>Details</summary>
Motivation: 高维数据通常位于低维非线性流形附近（流形假设），但多模态数据中的扭曲和建模误差仍需解决。

Method: 通过等距化学习的黎曼结构和平衡微分同胚参数化的规律性与表达性。

Result: 在合成和真实数据的实验中展示了方法的有效性。

Conclusion: 该方法为多模态数据的非线性分析和可解释机器学习提供了新思路。

Abstract: Modern machine learning increasingly leverages the insight that
high-dimensional data often lie near low-dimensional, non-linear manifolds, an
idea known as the manifold hypothesis. By explicitly modeling the geometric
structure of data through learning Riemannian geometry algorithms can achieve
improved performance and interpretability in tasks like clustering,
dimensionality reduction, and interpolation. In particular, learned pullback
geometry has recently undergone transformative developments that now make it
scalable to learn and scalable to evaluate, which further opens the door for
principled non-linear data analysis and interpretable machine learning.
However, there are still steps to be taken when considering real-world
multi-modal data. This work focuses on addressing distortions and modeling
errors that can arise in the multi-modal setting and proposes to alleviate both
challenges through isometrizing the learned Riemannian structure and balancing
regularity and expressivity of the diffeomorphism parametrization. We showcase
the effectiveness of the synergy of the proposed approaches in several
numerical experiments with both synthetic and real data.

</details>


### [194] [High-order Regularization for Machine Learning and Learning-based Control](https://arxiv.org/abs/2505.08129)
*Xinghua Liu,Ming Cao*

Main category: cs.LG

TL;DR: 论文提出了一种新的高阶正则化（HR）方法，用于机器学习和神经网络训练，通过理论证明和实验验证其优越性能。


<details>
  <summary>Details</summary>
Motivation: 正则化在神经网络训练中广泛应用，但现有方法缺乏理论解释和可解释性。HR方法旨在填补这一空白，提供理论支持并增强模型的可解释性。

Method: 提出高阶正则化（HR）方法，将其视为逆映射的近似，并推导出可计算的近似误差。HR方法还证明了$L_2$正则化是其低阶特例。

Result: HR方法提供了误差的上下界，证明了其收敛性和可解释性。实验验证了HR在增强神经网络泛化能力方面的优越性能。

Conclusion: HR方法不仅理论严谨，还能显著提升神经网络的泛化能力和可解释性，适用于广泛的神经网络结构。

Abstract: The paper proposes a novel regularization procedure for machine learning. The
proposed high-order regularization (HR) provides new insight into
regularization, which is widely used to train a neural network that can be
utilized to approximate the action-value function in general reinforcement
learning problems. The proposed HR method ensures the provable convergence of
the approximation algorithm, which makes the much-needed connection between
regularization and explainable learning using neural networks. The proposed HR
method theoretically demonstrates that regularization can be regarded as an
approximation in terms of inverse mapping with explicitly calculable
approximation error, and the $L_2$ regularization is a lower-order case of the
proposed method. We provide lower and upper bounds for the error of the
proposed HR solution, which helps build a reliable model. We also find that
regularization with the proposed HR can be regarded as a contraction. We prove
that the generalizability of neural networks can be maximized with a proper
regularization matrix, and the proposed HR is applicable for neural networks
with any mapping matrix. With the theoretical explanation of the extreme
learning machine for neural network training and the proposed high-order
regularization, one can better interpret the output of the neural network, thus
leading to explainable learning. We present a case study based on regularized
extreme learning neural networks to demonstrate the application of the proposed
HR and give the corresponding incremental HR solution. We verify the
performance of the proposed HR method by solving a classic control problem in
reinforcement learning. The result demonstrates the superior performance of the
method with significant enhancement in the generalizability of the neural
network.

</details>


### [195] [Large Language Models for Computer-Aided Design: A Survey](https://arxiv.org/abs/2505.08137)
*Licheng Zhang,Bach Le,Naveed Akhtar,Siew-Kei Lam,Tuan Ngo*

Main category: cs.LG

TL;DR: 本文首次系统综述了大型语言模型（LLMs）与计算机辅助设计（CAD）的结合，探讨了LLMs在CAD中的六大应用领域，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: CAD是3D建模的行业标准，现代设计复杂性增加，LLMs有望优化CAD流程，但目前缺乏相关综述。

Method: 文章系统梳理了LLMs的基础、闭源与开源模型，并分类分析了LLMs在CAD中的六大应用领域。

Result: 提出了LLMs在CAD中的具体应用分类，展示了其潜力与影响。

Conclusion: LLMs与CAD的结合前景广阔，未来研究方向为技术创新提供了丰富机会。

Abstract: Large Language Models (LLMs) have seen rapid advancements in recent years,
with models like ChatGPT and DeepSeek, showcasing their remarkable capabilities
across diverse domains. While substantial research has been conducted on LLMs
in various fields, a comprehensive review focusing on their integration with
Computer-Aided Design (CAD) remains notably absent. CAD is the industry
standard for 3D modeling and plays a vital role in the design and development
of products across different industries. As the complexity of modern designs
increases, the potential for LLMs to enhance and streamline CAD workflows
presents an exciting frontier. This article presents the first systematic
survey exploring the intersection of LLMs and CAD. We begin by outlining the
industrial significance of CAD, highlighting the need for AI-driven innovation.
Next, we provide a detailed overview of the foundation of LLMs. We also examine
both closed-source LLMs as well as publicly available models. The core of this
review focuses on the various applications of LLMs in CAD, providing a taxonomy
of six key areas where these models are making considerable impact. Finally, we
propose several promising future directions for further advancements, which
offer vast opportunities for innovation and are poised to shape the future of
CAD technology. Github:
https://github.com/lichengzhanguom/LLMs-CAD-Survey-Taxonomy

</details>


### [196] [Mirror Mirror on the Wall, Have I Forgotten it All? A New Framework for Evaluating Machine Unlearning](https://arxiv.org/abs/2505.08138)
*Brennon Brimhall,Philip Mathew,Neil Fendley,Yinzhi Cao,Matthew Green*

Main category: cs.LG

TL;DR: 论文提出了一种称为“计算性遗忘”的强形式化定义，用于衡量机器遗忘方法的有效性，并证明现有方法无法满足该定义。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法无法确保模型完全遗忘指定数据，且缺乏理论支持。

Method: 通过区分算法（如成员推断分数和KL散度）评估遗忘效果，并提出计算性遗忘的定义。

Result: 证明现有方法无法满足计算性遗忘，且基于差分隐私的方法虽可行但效用极低。

Conclusion: 当前方法无法实现计算性遗忘，未来需解决相关开放性问题。

Abstract: Machine unlearning methods take a model trained on a dataset and a forget
set, then attempt to produce a model as if it had only been trained on the
examples not in the forget set. We empirically show that an adversary is able
to distinguish between a mirror model (a control model produced by retraining
without the data to forget) and a model produced by an unlearning method across
representative unlearning methods from the literature. We build distinguishing
algorithms based on evaluation scores in the literature (i.e. membership
inference scores) and Kullback-Leibler divergence.
  We propose a strong formal definition for machine unlearning called
computational unlearning. Computational unlearning is defined as the inability
for an adversary to distinguish between a mirror model and a model produced by
an unlearning method. If the adversary cannot guess better than random (except
with negligible probability), then we say that an unlearning method achieves
computational unlearning.
  Our computational unlearning definition provides theoretical structure to
prove unlearning feasibility results. For example, our computational unlearning
definition immediately implies that there are no deterministic computational
unlearning methods for entropic learning algorithms. We also explore the
relationship between differential privacy (DP)-based unlearning methods and
computational unlearning, showing that DP-based approaches can satisfy
computational unlearning at the cost of an extreme utility collapse. These
results demonstrate that current methodology in the literature fundamentally
falls short of achieving computational unlearning. We conclude by identifying
several open questions for future work.

</details>


### [197] [Multi-Layer Hierarchical Federated Learning with Quantization](https://arxiv.org/abs/2505.08145)
*Seyed Mohammad Azimi-Abarghouyi,Carlo Fischione*

Main category: cs.LG

TL;DR: 提出了一种多层分层联邦学习框架（QMLHFL），支持任意层数和网络架构，通过嵌套聚合和分层量化方案优化通信效率，并分析了其收敛性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有分层联邦学习模型通常限于两层聚合，限制了复杂大规模网络的扩展性和灵活性。

Method: 提出QMLHFL框架，支持多层嵌套聚合和分层量化，分析其收敛条件与速率，并优化层内迭代次数。

Result: QMLHFL在高数据异构性下仍保持高学习精度，优化后性能显著提升。

Conclusion: QMLHFL为分层联邦学习提供了灵活且高效的解决方案，适用于复杂网络环境。

Abstract: Almost all existing hierarchical federated learning (FL) models are limited
to two aggregation layers, restricting scalability and flexibility in complex,
large-scale networks. In this work, we propose a Multi-Layer Hierarchical
Federated Learning framework (QMLHFL), which appears to be the first study that
generalizes hierarchical FL to arbitrary numbers of layers and network
architectures through nested aggregation, while employing a layer-specific
quantization scheme to meet communication constraints. We develop a
comprehensive convergence analysis for QMLHFL and derive a general convergence
condition and rate that reveal the effects of key factors, including
quantization parameters, hierarchical architecture, and intra-layer iteration
counts. Furthermore, we determine the optimal number of intra-layer iterations
to maximize the convergence rate while meeting a deadline constraint that
accounts for both communication and computation times. Our results show that
QMLHFL consistently achieves high learning accuracy, even under high data
heterogeneity, and delivers notably improved performance when optimized,
compared to using randomly selected values.

</details>


### [198] [Feature Fitted Online Conformal Prediction for Deep Time Series Forecasting Model](https://arxiv.org/abs/2505.08158)
*Xiannan Huang,Shuhan Qiu*

Main category: cs.LG

TL;DR: 提出了一种轻量级的共形预测方法，用于时间序列预测中的不确定性量化，无需重新训练即可提供有效的覆盖范围和更短的区间长度。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度点预测模型的置信区间建模方法存在成本高、未能充分利用深度模型表示能力或缺乏理论保证的问题。

Method: 利用预训练点预测模型提取的特征拟合残差预测器并构建置信区间，结合自适应覆盖控制机制。

Result: 在12个数据集上的实验表明，该方法能提供更紧的置信区间并保持所需的覆盖率。

Conclusion: 该方法通过理论证明和实验验证，解决了现有方法的局限性，具有实际应用价值。

Abstract: Time series forecasting is critical for many applications, where deep
learning-based point prediction models have demonstrated strong performance.
However, in practical scenarios, there is also a need to quantify predictive
uncertainty through online confidence intervals. Existing confidence interval
modeling approaches building upon these deep point prediction models suffer
from key limitations: they either require costly retraining, fail to fully
leverage the representational strengths of deep models, or lack theoretical
guarantees. To address these gaps, we propose a lightweight conformal
prediction method that provides valid coverage and shorter interval lengths
without retraining. Our approach leverages features extracted from pre-trained
point prediction models to fit a residual predictor and construct confidence
intervals, further enhanced by an adaptive coverage control mechanism.
Theoretically, we prove that our method achieves asymptotic coverage
convergence, with error bounds dependent on the feature quality of the
underlying point prediction model. Experiments on 12 datasets demonstrate that
our method delivers tighter confidence intervals while maintaining desired
coverage rates. Code, model and dataset in
\href{https://github.com/xiannanhuang/FFDCI}{Github}

</details>


### [199] [Feasibility-Aware Pessimistic Estimation: Toward Long-Horizon Safety in Offline RL](https://arxiv.org/abs/2505.08179)
*Zhikun Tao,Gang Xiong,He Fang,Zhen Shen,Yunjun Han,Qing-Shan Jia*

Main category: cs.LG

TL;DR: FASP框架通过结合H-J可达性分析和悲观估计方法，解决了离线安全强化学习中长期安全性和样本效率的问题。


<details>
  <summary>Details</summary>
Motivation: 现有离线安全强化学习方法忽视长期安全性，且对分布外数据表现不佳。

Method: 使用H-J可达性分析生成安全标签，结合CVAE和悲观估计方法优化策略。

Result: FASP在DSRL基准测试中表现优异，尤其在安全性上超越现有方法。

Conclusion: FASP为离线安全强化学习提供了长期安全性和高效样本利用的解决方案。

Abstract: Offline safe reinforcement learning(OSRL) derives constraint-satisfying
policies from pre-collected datasets, offers a promising avenue for deploying
RL in safety-critical real-world domains such as robotics. However, the
majority of existing approaches emphasize only short-term safety, neglecting
long-horizon considerations. Consequently, they may violate safety constraints
and fail to ensure sustained protection during online deployment. Moreover, the
learned policies often struggle to handle states and actions that are not
present or out-of-distribution(OOD) from the offline dataset, and exhibit
limited sample efficiency. To address these challenges, we propose a novel
framework Feasibility-Aware offline Safe Reinforcement Learning with CVAE-based
Pessimism (FASP). First, we employ Hamilton-Jacobi (H-J) reachability analysis
to generate reliable safety labels, which serve as supervisory signals for
training both a conditional variational autoencoder (CVAE) and a safety
classifier. This approach not only ensures high sampling efficiency but also
provides rigorous long-horizon safety guarantees. Furthermore, we utilize
pessimistic estimation methods to estimate the Q-value of reward and cost,
which mitigates the extrapolation errors induces by OOD actions, and penalize
unsafe actions to enabled the agent to proactively avoid high-risk behaviors.
Moreover, we theoretically prove the validity of this pessimistic estimation.
Extensive experiments on DSRL benchmarks demonstrate that FASP algorithm
achieves competitive performance across multiple experimental tasks,
particularly outperforming state-of-the-art algorithms in terms of safety.

</details>


### [200] [DSADF: Thinking Fast and Slow for Decision Making](https://arxiv.org/abs/2505.08189)
*Alex Zhihao Dou,Dongfei Cui,Jun Yan,Weida Wang,Benteng Chen,Haoming Wang,Zeke Xie,Shufei Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种双系统自适应决策框架（DSADF），结合强化学习（RL）和视觉语言模型（VLM），以解决RL在动态环境中泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: RL在动态环境中泛化能力有限，现有方法中RL与基础模型（如LLM/VLM）的协作不够高效，导致决策不合理或效率低下。

Method: 受Kahneman的双系统理论启发，设计了DSADF框架：System 1（RL代理）负责快速直觉决策，System 2（VLM）负责深度分析推理。

Result: 在Crafter和Housekeep视频游戏环境中验证了DSADF的有效性，显著提升了未见和已知任务的决策能力。

Conclusion: DSADF通过结合RL和VLM的优势，实现了高效且自适应的决策，为复杂环境中的智能决策提供了新思路。

Abstract: Although Reinforcement Learning (RL) agents are effective in well-defined
environments, they often struggle to generalize their learned policies to
dynamic settings due to their reliance on trial-and-error interactions. Recent
work has explored applying Large Language Models (LLMs) or Vision Language
Models (VLMs) to boost the generalization of RL agents through policy
optimization guidance or prior knowledge. However, these approaches often lack
seamless coordination between the RL agent and the foundation model, leading to
unreasonable decision-making in unfamiliar environments and efficiency
bottlenecks. Making full use of the inferential capabilities of foundation
models and the rapid response capabilities of RL agents and enhancing the
interaction between the two to form a dual system is still a lingering
scientific question. To address this problem, we draw inspiration from
Kahneman's theory of fast thinking (System 1) and slow thinking (System 2),
demonstrating that balancing intuition and deep reasoning can achieve nimble
decision-making in a complex world. In this study, we propose a Dual-System
Adaptive Decision Framework (DSADF), integrating two complementary modules:
System 1, comprising an RL agent and a memory space for fast and intuitive
decision making, and System 2, driven by a VLM for deep and analytical
reasoning. DSADF facilitates efficient and adaptive decision-making by
combining the strengths of both systems. The empirical study in the video game
environment: Crafter and Housekeep demonstrates the effectiveness of our
proposed method, showing significant improvements in decision abilities for
both unseen and known tasks.

</details>


### [201] [A Multi-scale Representation Learning Framework for Long-Term Time Series Forecasting](https://arxiv.org/abs/2505.08199)
*Boshi Gao,Qingjian Ni,Fanbo Ju,Yu Chen,Ziqi Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种基于MLP的框架MDMixer，用于解决长期时间序列预测中的多粒度信息利用不足、通道特性忽视及趋势与季节性成分独特性的问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 长期时间序列预测在能源消耗和天气预测等领域有广泛应用，但由于复杂的时间模式和内在多尺度变化，准确预测具有挑战性。

Method: 采用MLP框架，通过多尺度预测和动态信息整合系统，独立建模趋势和季节性成分。

Result: 在八个基准测试中，MDMixer的平均MAE性能比现有方法TimeMixer提高了4.64%，同时兼顾训练效率和模型可解释性。

Conclusion: MDMixer有效解决了长期时间序列预测中的关键问题，性能显著提升，具有实用价值。

Abstract: Long-term time series forecasting (LTSF) offers broad utility in practical
settings like energy consumption and weather prediction. Accurately predicting
long-term changes, however, is demanding due to the intricate temporal patterns
and inherent multi-scale variations within time series. This work confronts key
issues in LTSF, including the suboptimal use of multi-granularity information,
the neglect of channel-specific attributes, and the unique nature of trend and
seasonal components, by introducing a proficient MLP-based forecasting
framework. Our method adeptly disentangles complex temporal dynamics using
clear, concurrent predictions across various scales. These multi-scale
forecasts are then skillfully integrated through a system that dynamically
assigns importance to information from different granularities, sensitive to
individual channel characteristics. To manage the specific features of temporal
patterns, a two-pronged structure is utilized to model trend and seasonal
elements independently. Experimental results on eight LTSF benchmarks
demonstrate that MDMixer improves average MAE performance by 4.64% compared to
the recent state-of-the-art MLP-based method (TimeMixer), while achieving an
effective balance between training efficiency and model interpretability.

</details>


### [202] [An Effective Flow-based Method for Positive-Unlabeled Learning: 2-HNC](https://arxiv.org/abs/2505.08212)
*Dorit Hochbaum,Torpong Nitayanont*

Main category: cs.LG

TL;DR: 论文提出了一种基于网络流的2-HNC方法，用于正样本-未标记样本（PU）学习，通过Hochbaum归一化割（HNC）生成嵌套分区，并分两阶段进行样本排序和分类，最终在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在PU学习中，仅提供正样本而其余数据未标记，现有方法需要改进。

Method: 2-HNC方法分两阶段：首先生成未标记样本的负样本可能性排序，第二阶段加入可能负样本并重新分类。

Result: 在合成和真实数据集上，2-HNC表现优异，常超越现有最优算法。

Conclusion: 2-HNC通过嵌套分区和两阶段策略，有效解决了PU学习问题，性能显著。

Abstract: In many scenarios of binary classification, only positive instances are
provided in the training data, leaving the rest of the data unlabeled. This
setup, known as positive-unlabeled (PU) learning, is addressed here with a
network flow-based method which utilizes pairwise similarities between samples.
The method we propose here, 2-HNC, leverages Hochbaum's Normalized Cut (HNC)
and the set of solutions it provides by solving a parametric minimum cut
problem. The set of solutions, that are nested partitions of the samples into
two sets, correspond to varying tradeoff values between the two goals: high
intra-similarity inside the sets and low inter-similarity between the two sets.
This nested sequence is utilized here to deliver a ranking of unlabeled samples
by their likelihood of being negative. Building on this insight, our method,
2-HNC, proceeds in two stages. The first stage generates this ranking without
assuming any negative labels, using a problem formulation that is constrained
only on positive labeled samples. The second stage augments the positive set
with likely-negative samples and recomputes the classification. The final label
prediction selects among all generated partitions in both stages, the one that
delivers a positive class proportion, closest to a prior estimate of this
quantity, which is assumed to be given. Extensive experiments across synthetic
and real datasets show that 2-HNC yields strong performance and often surpasses
existing state-of-the-art algorithms.

</details>


### [203] [Deep Probabilistic Modeling of User Behavior for Anomaly Detection via Mixture Density Networks](https://arxiv.org/abs/2505.08220)
*Lu Dai,Wenxuan Zhu,Xuehui Quan,Renzi Meng,Sheng Cai,Yichen Wang*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度混合密度网络的异常检测方法，用于识别复杂用户行为中的异常模式。通过高斯混合模型和神经网络参数化，该方法显著提升了检测罕见和非结构化行为的能力。实验结果表明其性能优于现有神经网络架构。


<details>
  <summary>Details</summary>
Motivation: 提高复杂用户行为中潜在异常模式的识别能力，特别是在网络安全和智能风控领域。

Method: 构建由神经网络参数化的高斯混合模型，基于概率密度定义异常评分函数，利用负对数似然进行建模。

Result: 在UNSW-NB15数据集上的实验显示，该方法在准确性、F1分数、AUC和训练稳定性方面优于现有方法。

Conclusion: 该方法为用户行为建模和异常检测提供了更具表达力和判别性的解决方案，推动了深度概率建模技术在相关领域的应用。

Abstract: To improve the identification of potential anomaly patterns in complex user
behavior, this paper proposes an anomaly detection method based on a deep
mixture density network. The method constructs a Gaussian mixture model
parameterized by a neural network, enabling conditional probability modeling of
user behavior. It effectively captures the multimodal distribution
characteristics commonly present in behavioral data. Unlike traditional
classifiers that rely on fixed thresholds or a single decision boundary, this
approach defines an anomaly scoring function based on probability density using
negative log-likelihood. This significantly enhances the model's ability to
detect rare and unstructured behaviors. Experiments are conducted on the
real-world network user dataset UNSW-NB15. A series of performance comparisons
and stability validation experiments are designed. These cover multiple
evaluation aspects, including Accuracy, F1- score, AUC, and loss fluctuation.
The results show that the proposed method outperforms several advanced neural
network architectures in both performance and training stability. This study
provides a more expressive and discriminative solution for user behavior
modeling and anomaly detection. It strongly promotes the application of deep
probabilistic modeling techniques in the fields of network security and
intelligent risk control.

</details>


### [204] [Clustering-based Low-Rank Matrix Approximation: An Adaptive Theoretical Analysis with Application to Data Compression](https://arxiv.org/abs/2505.08256)
*Sisipho Hamlomo,Marcellin Atemkeng*

Main category: cs.LG

TL;DR: 论文提出了一种自适应低秩矩阵近似（LoRMA）方法，通过分块聚类和局部SVD处理高局部变化的数据，优于传统全局SVD方法。


<details>
  <summary>Details</summary>
Motivation: 传统全局SVD方法在压缩高分辨率数据矩阵时忽略局部变化，导致细节丢失。自适应LoRMA旨在解决这一问题。

Method: 将数据矩阵分块为重叠区域，通过k-means聚类相似块，并在每个簇内进行SVD。分析了块大小对压缩效率和计算成本的影响。

Result: 在MRI、超声、CT和X射线四种模态中，自适应LoRMA在PSNR、SSIM、MSE、IoU和EPI指标上均优于全局SVD，保留了结构完整性和诊断相关性。

Conclusion: 自适应LoRMA在存储效率和诊断保真度上表现优异，尽管计算时间较长，但适用于高压缩需求的应用场景。

Abstract: Low-rank matrix approximation (LoRMA) is a fundamental tool for compressing
high-resolution data matrices by extracting important features while
suppressing redundancy. Low-rank methods, such as global singular value
decomposition (SVD), apply uniform compression across the entire data matrix,
often ignoring important local variations and leading to the loss of fine
structural details. To address these limitations, we introduce an adaptive
LoRMA, which partitions data matrix into overlapping patches, groups
structurally similar patches into several clusters using k-means, and performs
SVD within each cluster. We derive the overall compression factor accounting
for patch overlap and analyze how patch size influences compression efficiency
and computational cost. While the proposed adaptive LoRMA method is applicable
to any data exhibiting high local variation, we focus on medical imaging due to
its pronounced local variability. We evaluate and compare our adaptive LoRMA
against global SVD across four imaging modalities: MRI, ultrasound, CT scan,
and chest X-ray. Results demonstrate that adaptive LoRMA effectively preserves
structural integrity, edge details, and diagnostic relevance, as measured by
peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), mean
squared error (MSE), intersection over union (IoU), and edge preservation index
(EPI). Adaptive LoRMA significantly minimizes block artifacts and residual
errors, particularly in pathological regions, consistently outperforming global
SVD in terms of PSNR, SSIM, IoU, EPI, and achieving lower MSE. Adaptive LoRMA
prioritizes clinically salient regions while allowing aggressive compression in
non-critical regions, optimizing storage efficiency. Although adaptive LoRMA
requires higher processing time, its diagnostic fidelity justifies the overhead
for high-compression applications.

</details>


### [205] [Super-fast rates of convergence for Neural Networks Classifiers under the Hard Margin Condition](https://arxiv.org/abs/2505.08262)
*Nathanael Tepakbong,Ding-Xuan Zhou,Xiang Zhou*

Main category: cs.LG

TL;DR: 论文研究了在Tsybakov低噪声条件下，使用ReLU激活的深度神经网络（DNN）的二元分类问题，证明了在硬边界条件下，通过最小化经验风险和平方损失代理，DNN可以实现任意大的有限样本超额风险界。


<details>
  <summary>Details</summary>
Motivation: 探讨DNN在低噪声条件下的分类性能，特别是在硬边界条件下的表现。

Method: 使用ReLU激活的DNN，通过最小化经验风险和平方损失代理，结合ℓp惩罚。

Result: 在回归函数足够平滑的情况下，DNN可以实现阶数为O(n−α)的有限样本超额风险界，α可任意大。

Conclusion: 论文提出了一种新的超额风险分解方法，证明了DNN在硬边界条件下的优异性能。

Abstract: We study the classical binary classification problem for hypothesis spaces of
Deep Neural Networks (DNNs) with ReLU activation under Tsybakov's low-noise
condition with exponent $q>0$, and its limit-case $q\to\infty$ which we refer
to as the "hard-margin condition". We show that DNNs which minimize the
empirical risk with square loss surrogate and $\ell_p$ penalty can achieve
finite-sample excess risk bounds of order $\mathcal{O}\left(n^{-\alpha}\right)$
for arbitrarily large $\alpha>0$ under the hard-margin condition, provided that
the regression function $\eta$ is sufficiently smooth. The proof relies on a
novel decomposition of the excess risk which might be of independent interest.

</details>


### [206] [LLM Enhancers for GNNs: An Analysis from the Perspective of Causal Mechanism Identification](https://arxiv.org/abs/2505.08265)
*Hang Gao,Wenxuan Huang,Fengge Wu,Junsuo Zhao,Changwen Zheng,Huaping Liu*

Main category: cs.LG

TL;DR: 论文探讨了利用大语言模型（LLMs）优化图神经网络（GNNs）节点表示的方法，通过合成数据集和干预实验分析其机制，并提出了优化模块。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs作为特征增强器优化GNNs节点表示的方法潜力巨大，但其基本特性尚未充分研究。

Method: 构建可控因果关系的合成图数据集，通过干预实验分析LLMs和GNNs的深层机制，并设计优化模块。

Result: 实验验证了提出的优化模块在多数据集和模型上的有效性。

Conclusion: 研究揭示了LLMs和GNNs的交互机制，并提供了实用的优化方案。

Abstract: The use of large language models (LLMs) as feature enhancers to optimize node
representations, which are then used as inputs for graph neural networks
(GNNs), has shown significant potential in graph representation learning.
However, the fundamental properties of this approach remain underexplored. To
address this issue, we propose conducting a more in-depth analysis of this
issue based on the interchange intervention method. First, we construct a
synthetic graph dataset with controllable causal relationships, enabling
precise manipulation of semantic relationships and causal modeling to provide
data for analysis. Using this dataset, we conduct interchange interventions to
examine the deeper properties of LLM enhancers and GNNs, uncovering their
underlying logic and internal mechanisms. Building on the analytical results,
we design a plug-and-play optimization module to improve the information
transfer between LLM enhancers and GNNs. Experiments across multiple datasets
and models validate the proposed module.

</details>


### [207] [Decoupled Multimodal Prototypes for Visual Recognition with Missing Modalities](https://arxiv.org/abs/2505.08283)
*Jueqing Lu,Yuanyuan Qi,Xiaohao Yang,Shujie Zhou,Lan Du*

Main category: cs.LG

TL;DR: 提出了一种基于解耦原型的多模态学习输出头，动态适应缺失模态场景，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设所有模态可用，但现实中常缺失模态，需解决性能下降问题。

Method: 引入解耦原型输出头，利用模态特定的类原型动态适应缺失情况。

Result: 实验表明，该方法在多种缺失场景和缺失率下显著提升性能。

Conclusion: 解耦原型输出头有效解决了缺失模态问题，兼容现有提示方法。

Abstract: Multimodal learning enhances deep learning models by enabling them to
perceive and understand information from multiple data modalities, such as
visual and textual inputs. However, most existing approaches assume the
availability of all modalities, an assumption that often fails in real-world
applications. Recent works have introduced learnable missing-case-aware prompts
to mitigate performance degradation caused by missing modalities while reducing
the need for extensive model fine-tuning. Building upon the effectiveness of
missing-case-aware handling for missing modalities, we propose a novel
decoupled prototype-based output head, which leverages missing-case-aware
class-wise prototypes tailored for each individual modality. This approach
dynamically adapts to different missing modality scenarios and can be
seamlessly integrated with existing prompt-based methods. Extensive experiments
demonstrate that our proposed output head significantly improves performance
across a wide range of missing-modality scenarios and varying missing rates.

</details>


### [208] [A Practical Introduction to Deep Reinforcement Learning](https://arxiv.org/abs/2505.08295)
*Yinghan Sun,Hongxi Wang,Hua Chen,Wei Zhang*

Main category: cs.LG

TL;DR: 本文是一篇关于深度强化学习（DRL）的教程，特别关注PPO算法，旨在为初学者提供简洁直观的入门指南。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习算法多样且理论复杂，初学者难以入门。本文旨在通过直观解释和实用技巧降低学习门槛。

Method: 将算法统一在GPI框架下，强调直观解释、示例和工程技巧，而非冗长理论证明。

Result: 为读者提供了从基础概念到高级DRL算法实现的快速学习路径。

Conclusion: 本文是一个高效且易于理解的指南，帮助初学者快速掌握DRL和PPO算法。

Abstract: Deep reinforcement learning (DRL) has emerged as a powerful framework for
solving sequential decision-making problems, achieving remarkable success in a
wide range of applications, including game AI, autonomous driving, biomedicine,
and large language models. However, the diversity of algorithms and the
complexity of theoretical foundations often pose significant challenges for
beginners seeking to enter the field. This tutorial aims to provide a concise,
intuitive, and practical introduction to DRL, with a particular focus on the
Proximal Policy Optimization (PPO) algorithm, which is one of the most widely
used and effective DRL methods. To facilitate learning, we organize all
algorithms under the Generalized Policy Iteration (GPI) framework, offering
readers a unified and systematic perspective. Instead of lengthy theoretical
proofs, we emphasize intuitive explanations, illustrative examples, and
practical engineering techniques. This work serves as an efficient and
accessible guide, helping readers rapidly progress from basic concepts to the
implementation of advanced DRL algorithms.

</details>


### [209] [Efficient Unstructured Pruning of Mamba State-Space Models for Resource-Constrained Environments](https://arxiv.org/abs/2505.08299)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 提出了一种针对Mamba模型的无结构化剪枝框架，减少70%参数的同时保持95%性能。


<details>
  <summary>Details</summary>
Motivation: Mamba模型参数量大，在资源受限环境中部署困难。

Method: 结合梯度感知的幅度剪枝、迭代剪枝计划和全局剪枝策略。

Result: 在多个基准测试中实现高效能，性能损失极小。

Conclusion: 揭示了Mamba架构的冗余和鲁棒性，拓宽了其应用范围。

Abstract: State-space models (SSMs), particularly the Mamba architecture, have emerged
as powerful alternatives to Transformers for sequence modeling, offering
linear-time complexity and competitive performance across diverse tasks.
However, their large parameter counts pose significant challenges for
deployment in resource-constrained environments. We propose a novel
unstructured pruning framework tailored for Mamba models that achieves up to
70\% parameter reduction while retaining over 95\% of the original performance.
Our approach integrates three key innovations: (1) a gradient-aware magnitude
pruning technique that combines weight magnitude and gradient information to
identify less critical parameters, (2) an iterative pruning schedule that
gradually increases sparsity to maintain model stability, and (3) a global
pruning strategy that optimizes parameter allocation across the entire model.
Through extensive experiments on WikiText-103, Long Range Arena, and ETT
time-series benchmarks, we demonstrate significant efficiency gains with
minimal performance degradation. Our analysis of pruning effects on Mamba's
components reveals critical insights into the architecture's redundancy and
robustness, enabling practical deployment in resource-constrained settings
while broadening Mamba's applicability.

</details>


### [210] [Rapid Overfitting of Multi-Pass Stochastic Gradient Descent in Stochastic Convex Optimization](https://arxiv.org/abs/2505.08306)
*Shira Vansover-Hager,Tomer Koren,Roi Livni*

Main category: cs.LG

TL;DR: 多轮随机梯度下降（SGD）在随机凸优化（SCO）中的样本外性能研究表明，多轮SGD可能导致过拟合，尤其在非光滑情况下。


<details>
  <summary>Details</summary>
Motivation: 研究多轮SGD在SCO中的性能，揭示其与单轮SGD的差异及过拟合现象。

Method: 分析多轮SGD的步长和轮次对样本外性能的影响，比较光滑与非光滑情况。

Result: 多轮SGD在非光滑情况下显著过拟合，样本外损失可达Ω(1)。

Conclusion: 多轮SGD在SCO中存在性能下降和过拟合风险，尤其在非光滑情况下。

Abstract: We study the out-of-sample performance of multi-pass stochastic gradient
descent (SGD) in the fundamental stochastic convex optimization (SCO) model.
While one-pass SGD is known to achieve an optimal $\Theta(1/\sqrt{n})$ excess
population loss given a sample of size $n$, much less is understood about the
multi-pass version of the algorithm which is widely used in practice. Somewhat
surprisingly, we show that in the general non-smooth case of SCO, just a few
epochs of SGD can already hurt its out-of-sample performance significantly and
lead to overfitting. In particular, using a step size $\eta =
\Theta(1/\sqrt{n})$, which gives the optimal rate after one pass, can lead to
population loss as large as $\Omega(1)$ after just one additional pass. More
generally, we show that the population loss from the second pass onward is of
the order $\Theta(1/(\eta T) + \eta \sqrt{T})$, where $T$ is the total number
of steps. These results reveal a certain phase-transition in the out-of-sample
behavior of SGD after the first epoch, as well as a sharp separation between
the rates of overfitting in the smooth and non-smooth cases of SCO.
Additionally, we extend our results to with-replacement SGD, proving that the
same asymptotic bounds hold after $O(n \log n)$ steps. Finally, we also prove a
lower bound of $\Omega(\eta \sqrt{n})$ on the generalization gap of one-pass
SGD in dimension $d = \smash{\widetilde O}(n)$, improving on recent results of
Koren et al.(2022) and Schliserman et al.(2024).

</details>


### [211] [SpecSphere: Dual-Pass Spectral-Spatial Graph Neural Networks with Certified Robustness](https://arxiv.org/abs/2505.08320)
*Yoonhyuk Choi,Chong-Kwon Kim*

Main category: cs.LG

TL;DR: SpecSphere是一种双通道光谱-空间GNN，首次实现对预测的ℓ₀边翻转和ℓ∞特征扰动的认证，适应同质性-异质性谱，超越1-Weisfeiler-Lehman的表达能力，同时保持线性时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决现有GNN在同质性-异质性谱适应性和可证明鲁棒性方面的不足，同时提升表达能力和计算效率。

Method: 结合Chebyshev多项式光谱分支和注意力门控空间分支，通过轻量级MLP在合作-对抗极小极大游戏中融合表示。

Result: SpecSphere在节点分类准确性和认证鲁棒性方面达到最先进水平，并提供更严格的鲁棒性保证。

Conclusion: 高表达能力、异质性适应性和可证明鲁棒性可以在单一可扩展架构中共存。

Abstract: We introduce SpecSphere, the first dual-pass spectral-spatial GNN that
certifies every prediction against both $\ell\_{0}$ edge flips and
$\ell\_{\infty}$ feature perturbations, adapts to the full
homophily-heterophily spectrum, and surpasses the expressive power of
1-Weisfeiler-Lehman while retaining linear-time complexity. Our model couples a
Chebyshev-polynomial spectral branch with an attention-gated spatial branch and
fuses their representations through a lightweight MLP trained in a
cooperative-adversarial min-max game. We further establish (i) a uniform
Chebyshev approximation theorem, (ii) minimax-optimal risk across the
homophily-heterophily spectrum, (iii) closed-form robustness certificates, and
(iv) universal approximation strictly beyond 1-WL. SpecSphere achieves
state-of-the-art node-classification accuracy and delivers tighter certified
robustness guarantees on real-world benchmarks. These results demonstrate that
high expressivity, heterophily adaptation, and provable robustness can coexist
within a single, scalable architecture.

</details>


### [212] [FedRS-Bench: Realistic Federated Learning Datasets and Benchmarks in Remote Sensing](https://arxiv.org/abs/2505.08325)
*Haodong Zhao,Peng Peng,Chiyu Chen,Linqing Huang,Gongshen Liu*

Main category: cs.LG

TL;DR: 提出FedRS数据集和FedRS-Bench基准，解决遥感领域联邦学习中缺乏真实数据集和标准化评估的问题。


<details>
  <summary>Details</summary>
Motivation: 遥感图像数据分布广泛且隐私受限，联邦学习（FL）可解决数据共享问题，但缺乏真实数据集和标准化基准。

Method: 构建包含8个数据集、135个客户端的FedRS数据集，体现真实场景的标签分布不均、数据量不平衡和领域异质性。基于此实现10种FL算法和评估指标。

Result: 实验表明FL能提升模型性能，同时揭示不同方法在客户异质性和可用性条件下的性能权衡。

Conclusion: FedRS-Bench为大规模真实FL研究提供标准化测试平台，促进公平比较，加速相关研究。

Abstract: Remote sensing (RS) images are usually produced at an unprecedented scale,
yet they are geographically and institutionally distributed, making centralized
model training challenging due to data-sharing restrictions and privacy
concerns. Federated learning (FL) offers a solution by enabling collaborative
model training across decentralized RS data sources without exposing raw data.
However, there lacks a realistic federated dataset and benchmark in RS. Prior
works typically rely on manually partitioned single dataset, which fail to
capture the heterogeneity and scale of real-world RS data, and often use
inconsistent experimental setups, hindering fair comparison. To address this
gap, we propose a realistic federated RS dataset, termed FedRS. FedRS consists
of eight datasets that cover various sensors and resolutions and builds 135
clients, which is representative of realistic operational scenarios. Data for
each client come from the same source, exhibiting authentic federated
properties such as skewed label distributions, imbalanced client data volumes,
and domain heterogeneity across clients. These characteristics reflect
practical challenges in federated RS and support evaluation of FL methods at
scale. Based on FedRS, we implement 10 baseline FL algorithms and evaluation
metrics to construct the comprehensive FedRS-Bench. The experimental results
demonstrate that FL can consistently improve model performance over training on
isolated data silos, while revealing performance trade-offs of different
methods under varying client heterogeneity and availability conditions. We hope
FedRS-Bench will accelerate research on large-scale, realistic FL in RS by
providing a standardized, rich testbed and facilitating fair comparisons across
future works. The source codes and dataset are available at
https://fedrs-bench.github.io/.

</details>


### [213] [Low-Complexity Inference in Continual Learning via Compressed Knowledge Transfer](https://arxiv.org/abs/2505.08327)
*Zhenrong Liu,Janne M. J. Huttunen,Mikko Honkala*

Main category: cs.LG

TL;DR: 论文探讨了在持续学习（CL）中如何通过模型压缩技术（如剪枝和知识蒸馏）解决大预训练模型的高计算成本问题，提出了两种高效框架，并在实验中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 大预训练模型在持续学习中表现优异，但其高计算成本限制了实际应用，尤其是在低延迟或高能效场景中。

Method: 提出了两种框架：基于剪枝的策略（包括预剪枝和后剪枝）和基于知识蒸馏的教师-学生架构。

Result: 实验表明，这两种框架在准确性和推理复杂度之间取得了更好的平衡，性能优于基线方法。

Conclusion: 论文为不同场景下的模型压缩提供了实用框架，并分析了其权衡关系。

Abstract: Continual learning (CL) aims to train models that can learn a sequence of
tasks without forgetting previously acquired knowledge. A core challenge in CL
is balancing stability -- preserving performance on old tasks -- and plasticity
-- adapting to new ones. Recently, large pre-trained models have been widely
adopted in CL for their ability to support both, offering strong generalization
for new tasks and resilience against forgetting. However, their high
computational cost at inference time limits their practicality in real-world
applications, especially those requiring low latency or energy efficiency. To
address this issue, we explore model compression techniques, including pruning
and knowledge distillation (KD), and propose two efficient frameworks tailored
for class-incremental learning (CIL), a challenging CL setting where task
identities are unavailable during inference. The pruning-based framework
includes pre- and post-pruning strategies that apply compression at different
training stages. The KD-based framework adopts a teacher-student architecture,
where a large pre-trained teacher transfers downstream-relevant knowledge to a
compact student. Extensive experiments on multiple CIL benchmarks demonstrate
that the proposed frameworks achieve a better trade-off between accuracy and
inference complexity, consistently outperforming strong baselines. We further
analyze the trade-offs between the two frameworks in terms of accuracy and
efficiency, offering insights into their use across different scenarios.

</details>


### [214] [Reinforcement Learning (RL) Meets Urban Climate Modeling: Investigating the Efficacy and Impacts of RL-Based HVAC Control](https://arxiv.org/abs/2505.07045)
*Junjie Yu,John S. Schreck,David John Gagne,Keith W. Oleson,Jie Li,Yongtu Liang,Qi Liao,Mingfei Sun,David O. Topping,Zhonghua Zheng*

Main category: cs.LG

TL;DR: 该研究提出了一种结合强化学习（RL）与城市气候模型的框架，评估了RL在HVAC控制中的效果及其对室内和城市气候的影响。结果表明，不同气候背景下RL策略的效果和可转移性差异显著。


<details>
  <summary>Details</summary>
Motivation: 探索RL在HVAC控制中的潜力，并分析其在不同气候背景下的效果及对室内和城市气候的影响。

Method: 结合RL与城市气候模型，评估RL策略在不同城市气候中的表现及其可转移性。

Result: RL策略的效果和可转移性因气候背景而异，炎热气候城市表现更优。

Conclusion: 强调在多样化气候背景下评估RL策略的重要性，并提出城市间学习可能有助于RL-HVAC控制的部署。

Abstract: Reinforcement learning (RL)-based heating, ventilation, and air conditioning
(HVAC) control has emerged as a promising technology for reducing building
energy consumption while maintaining indoor thermal comfort. However, the
efficacy of such strategies is influenced by the background climate and their
implementation may potentially alter both the indoor climate and local urban
climate. This study proposes an integrated framework combining RL with an urban
climate model that incorporates a building energy model, aiming to evaluate the
efficacy of RL-based HVAC control across different background climates, impacts
of RL strategies on indoor climate and local urban climate, and the
transferability of RL strategies across cities. Our findings reveal that the
reward (defined as a weighted combination of energy consumption and thermal
comfort) and the impacts of RL strategies on indoor climate and local urban
climate exhibit marked variability across cities with different background
climates. The sensitivity of reward weights and the transferability of RL
strategies are also strongly influenced by the background climate. Cities in
hot climates tend to achieve higher rewards across most reward weight
configurations that balance energy consumption and thermal comfort, and those
cities with more varying atmospheric temperatures demonstrate greater RL
strategy transferability. These findings underscore the importance of
thoroughly evaluating RL-based HVAC control strategies in diverse climatic
contexts. This study also provides a new insight that city-to-city learning
will potentially aid the deployment of RL-based HVAC control.

</details>


### [215] [Structural-Temporal Coupling Anomaly Detection with Dynamic Graph Transformer](https://arxiv.org/abs/2505.08330)
*Chang Zong,Yueting Zhuang,Jian Shao,Weiming Lu*

Main category: cs.LG

TL;DR: 提出了一种基于动态图变换器的结构-时间耦合异常检测架构，通过二维位置编码增强模型性能，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 动态图中异常边检测在社交网络、交易管理等应用中很重要，但现有方法忽略了结构与时间信息的深层交互。

Method: 提出动态图变换器模型，结合结构与时间特征，通过二维位置编码捕捉异常感知的图演化模式。

Result: 在六个数据集上的实验表明，该方法优于当前最先进模型。

Conclusion: 该方法在真实任务中表现出色，验证了其有效性。

Abstract: Detecting anomalous edges in dynamic graphs is an important task in many
applications over evolving triple-based data, such as social networks,
transaction management, and epidemiology. A major challenge with this task is
the absence of structural-temporal coupling information, which decreases the
ability of the representation to distinguish anomalies from normal instances.
Existing methods focus on handling independent structural and temporal features
with embedding models, which ignore the deep interaction between these two
types of information. In this paper, we propose a structural-temporal coupling
anomaly detection architecture with a dynamic graph transformer model.
Specifically, we introduce structural and temporal features from two
integration levels to provide anomaly-aware graph evolutionary patterns. Then,
a dynamic graph transformer enhanced by two-dimensional positional encoding is
implemented to capture both discrimination and contextual consistency signals.
Extensive experiments on six datasets demonstrate that our method outperforms
current state-of-the-art models. Finally, a case study illustrates the strength
of our method when applied to a real-world task.

</details>


### [216] [SHAP-based Explanations are Sensitive to Feature Representation](https://arxiv.org/abs/2505.08345)
*Hyunseung Hwang,Andrew Bell,Joao Fonseca,Venetia Pliatsika,Julia Stoyanovich,Steven Euijong Whang*

Main category: cs.LG

TL;DR: 论文探讨了数据工程选择对局部特征解释的影响，发现常见的数据处理技术（如年龄直方图或种族编码）可以操纵SHAP等方法计算的特征重要性，甚至可能被用于掩盖歧视问题。


<details>
  <summary>Details</summary>
Motivation: 研究数据工程技术如何影响局部特征解释，揭示其潜在被滥用的可能性，填补了系统性探索的空白。

Method: 通过实验展示常见数据工程技术（如年龄直方图、种族编码）对SHAP等解释方法的影响。

Result: 发现数据处理技术可以显著改变特征重要性，甚至被用于掩盖歧视等敏感问题。

Conclusion: 数据工程选择对解释方法的可靠性有重要影响，需警惕其潜在滥用风险。

Abstract: Local feature-based explanations are a key component of the XAI toolkit.
These explanations compute feature importance values relative to an
``interpretable'' feature representation. In tabular data, feature values
themselves are often considered interpretable. This paper examines the impact
of data engineering choices on local feature-based explanations. We demonstrate
that simple, common data engineering techniques, such as representing age with
a histogram or encoding race in a specific way, can manipulate feature
importance as determined by popular methods like SHAP. Notably, the sensitivity
of explanations to feature representation can be exploited by adversaries to
obscure issues like discrimination. While the intuition behind these results is
straightforward, their systematic exploration has been lacking. Previous work
has focused on adversarial attacks on feature-based explainers by biasing data
or manipulating models. To the best of our knowledge, this is the first study
demonstrating that explainers can be misled by standard, seemingly innocuous
data engineering techniques.

</details>


### [217] [Localization of Impacts on Thin-Walled Structures by Recurrent Neural Networks: End-to-end Learning from Real-World Data](https://arxiv.org/abs/2505.08362)
*Alexander Humer,Lukas Grasboeck,Ayech Benjeddou*

Main category: cs.LG

TL;DR: 论文探讨了利用循环神经网络（RNN）在薄壁结构上定位冲击位置的方法，通过实验数据训练模型，取得了高精度结果。


<details>
  <summary>Details</summary>
Motivation: 薄壁结构上的冲击定位对结构健康监测（SHM）至关重要，但传统方法难以处理Lamb波的色散特性。

Method: 使用门控循环单元（GRU）处理长序列传感器数据，通过机器人实验生成物理数据训练模型。

Result: 即使数据集较小，模型仍能高精度估计冲击位置。

Conclusion: 实验数据训练的GRU模型在冲击定位中表现优异，为SHM提供了新思路。

Abstract: Today, machine learning is ubiquitous, and structural health monitoring (SHM)
is no exception. Specifically, we address the problem of impact localization on
shell-like structures, where knowledge of impact locations aids in assessing
structural integrity. Impacts on thin-walled structures excite Lamb waves,
which can be measured with piezoelectric sensors. Their dispersive
characteristics make it difficult to detect and localize impacts by
conventional methods. In the present contribution, we explore the localization
of impacts using neural networks. In particular, we propose to use {recurrent
neural networks} (RNNs) to estimate impact positions end-to-end, i.e., directly
from {sequential sensor data}. We deal with comparatively long sequences of
thousands of samples, since high sampling rate are needed to accurately capture
elastic waves. For this reason, the proposed approach builds upon Gated
Recurrent Units (GRUs), which are less prone to vanishing gradients as compared
to conventional RNNs. Quality and quantity of data are crucial when training
neural networks. Often, synthetic data is used, which inevitably introduces a
reality gap. Here, by contrast, we train our networks using {physical data from
experiments}, which requires automation to handle the large number of
experiments needed. For this purpose, a {robot is used to drop steel balls}
onto an {aluminum plate} equipped with {piezoceramic sensors}. Our results show
remarkable accuracy in estimating impact positions, even with a comparatively
small dataset.

</details>


### [218] [Density Ratio-based Causal Discovery from Bivariate Continuous-Discrete Data](https://arxiv.org/abs/2505.08371)
*Takashi Nicholas Maeda,Shohei Shimizu,Hidetoshi Matsui*

Main category: cs.LG

TL;DR: 提出了一种针对混合二元数据（连续变量和离散变量）的因果发现方法，通过分析条件密度比的单调性确定因果方向，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有约束和评分方法在二元数据中效果不佳，需要新方法解决因果方向判断问题。

Method: 通过分析连续变量在不同离散变量值下的条件密度比的单调性，判断因果方向。

Result: 理论证明和实验表明，该方法在合成和真实数据中优于现有方法。

Conclusion: 新方法无需强分布假设，能公平比较不同类型变量的因果方向，具有高准确性。

Abstract: This paper proposes a causal discovery method for mixed bivariate data
consisting of one continuous and one discrete variable. Existing
constraint-based approaches are ineffective in the bivariate setting, as they
rely on conditional independence tests that are not suited to bivariate data.
Score-based methods either impose strong distributional assumptions or face
challenges in fairly comparing causal directions between variables of different
types, due to differences in their information content. We introduce a novel
approach that determines causal direction by analyzing the monotonicity of the
conditional density ratio of the continuous variable, conditioned on different
values of the discrete variable. Our theoretical analysis shows that the
conditional density ratio exhibits monotonicity when the continuous variable
causes the discrete variable, but not in the reverse direction. This property
provides a principled basis for comparing causal directions between variables
of different types, free from strong distributional assumptions and bias
arising from differences in their information content. We demonstrate its
effectiveness through experiments on both synthetic and real-world datasets,
showing superior accuracy compared to existing methods.

</details>


### [219] [ConDiSim: Conditional Diffusion Models for Simulation Based Inference](https://arxiv.org/abs/2505.08403)
*Mayank Nautiyal,Andreas Hellander,Prashant Singh*

Main category: cs.LG

TL;DR: ConDiSim是一种基于条件扩散模型的仿真推理方法，适用于复杂系统的后验分布近似，具有高效性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 针对复杂系统中难以处理的似然函数问题，提出一种高效的仿真推理方法。

Method: 利用去噪扩散概率模型，通过前向加噪和反向去噪过程，结合观测数据近似后验分布。

Result: 在十个基准问题和两个实际测试问题中表现出高精度和计算效率。

Conclusion: ConDiSim为仿真推理提供了一个鲁棒且可扩展的框架，特别适合需要快速推理的参数推断工作流。

Abstract: We present a conditional diffusion model - ConDiSim, for simulation-based
inference of complex systems with intractable likelihoods. ConDiSim leverages
denoising diffusion probabilistic models to approximate posterior
distributions, consisting of a forward process that adds Gaussian noise to
parameters, and a reverse process learning to denoise, conditioned on observed
data. This approach effectively captures complex dependencies and
multi-modalities within posteriors. ConDiSim is evaluated across ten benchmark
problems and two real-world test problems, where it demonstrates effective
posterior approximation accuracy while maintaining computational efficiency and
stability in model training. ConDiSim offers a robust and extensible framework
for simulation-based inference, particularly suitable for parameter inference
workflows requiring fast inference methods.

</details>


### [220] [Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency](https://arxiv.org/abs/2505.08445)
*Adel Ammar,Anis Koubaa,Omer Nacar,Wadii Boulila*

Main category: cs.LG

TL;DR: 研究分析了检索增强生成（RAG）系统中超参数对速度和性能的影响，发现Chroma查询更快，Faiss检索更精确，固定长度分块表现最佳，重排序提升质量但增加延迟，最终通过优化配置实现99%的上下文精确度。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型的幻觉和知识陈旧问题，通过检索增强生成（RAG）结合外部搜索提升性能。

Method: 分析Chroma和Faiss向量存储、分块策略、交叉编码器重排序和温度等超参数对RAG系统的影响，评估六项指标。

Result: Chroma查询快13%，Faiss检索精度更高；固定长度分块表现最佳；重排序提升质量但增加延迟；优化配置实现99%上下文精确度。

Conclusion: RAG系统可通过超参数优化平衡计算成本和准确性，显著提升检索质量，适用于临床决策支持等关键应用。

Abstract: Large language models achieve high task performance yet often hallucinate or
rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses
these gaps by coupling generation with external search. We analyse how
hyperparameters influence speed and quality in RAG systems, covering Chroma and
Faiss vector stores, chunking policies, cross-encoder re-ranking, and
temperature, and we evaluate six metrics: faithfulness, answer correctness,
answer relevancy, context precision, context recall, and answer similarity.
Chroma processes queries 13% faster, whereas Faiss yields higher retrieval
precision, revealing a clear speed-accuracy trade-off. Naive fixed-length
chunking with small windows and minimal overlap outperforms semantic
segmentation while remaining the quickest option. Re-ranking provides modest
gains in retrieval quality yet increases runtime by roughly a factor of 5, so
its usefulness depends on latency constraints. These results help practitioners
balance computational cost and accuracy when tuning RAG systems for
transparent, up-to-date responses. Finally, we re-evaluate the top
configurations with a corrective RAG workflow and show that their advantages
persist when the model can iteratively request additional evidence. We obtain a
near-perfect context precision (99%), which demonstrates that RAG systems can
achieve extremely high retrieval accuracy with the right combination of
hyperparameters, with significant implications for applications where retrieval
quality directly impacts downstream task performance, such as clinical decision
support in healthcare.

</details>


### [221] [An adaptive sampling algorithm for data-generation to build a data-manifold for physical problem surrogate modeling](https://arxiv.org/abs/2505.08487)
*Chetra Mang,Axel TahmasebiMoradi,David Danan,Mouadh Yagoubi*

Main category: cs.LG

TL;DR: 提出了一种自适应采样算法（ASADG）用于生成更代表性的输入数据，以解决物理模型数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 物理模型通常涉及计算昂贵的PDE求解，而数据不平衡会导致响应流形表示不准确，影响模型预测能力。

Method: 通过迭代添加输入数据（基于单纯复形重心）来改进响应流形表示，并与LHS方法对比。

Result: ASADG在相同数据量下比LHS方法更能准确表示响应流形。

Conclusion: ASADG算法能有效生成更具代表性的输入数据，提升模型预测精度。

Abstract: Physical models classically involved Partial Differential equations (PDE) and
depending of their underlying complexity and the level of accuracy required,
and known to be computationally expensive to numerically solve them. Thus, an
idea would be to create a surrogate model relying on data generated by such
solver. However, training such a model on an imbalanced data have been shown to
be a very difficult task. Indeed, if the distribution of input leads to a poor
response manifold representation, the model may not learn well and
consequently, it may not predict the outcome with acceptable accuracy. In this
work, we present an Adaptive Sampling Algorithm for Data Generation (ASADG)
involving a physical model. As the initial input data may not accurately
represent the response manifold in higher dimension, this algorithm iteratively
adds input data into it. At each step the barycenter of each simplicial
complex, that the manifold is discretized into, is added as new input data, if
a certain threshold is satisfied. We demonstrate the efficiency of the data
sampling algorithm in comparison with LHS method for generating more
representative input data. To do so, we focus on the construction of a harmonic
transport problem metamodel by generating data through a classical solver. By
using such algorithm, it is possible to generate the same number of input data
as LHS while providing a better representation of the response manifold.

</details>


### [222] [Isolation Forest in Novelty Detection Scenario](https://arxiv.org/abs/2505.08489)
*Adam Ulrich,Jan Krňávek,Roman Šenkeřík,Zuzana Komínková Oplatková,Radek Vala*

Main category: cs.LG

TL;DR: 本文提出了一种改进的半空间树（HST）算法，专门用于新颖性检测任务，通过理论分析和实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 经典异常检测算法如One-Class SVM或LOF在可解释性和可扩展性上存在不足，而HST算法在流数据异常检测中表现优异，但未针对新颖性检测进行优化。

Method: 通过理论分析（概率分析、期望深度计算和组合推理）改进HST算法，假设新颖性出现在树的较高叶子节点。

Result: 改进后的HST算法在新颖性检测中表现优于原始隔离森林（Isolation Forest），新颖点更易被隔离。

Conclusion: 改进的HST算法为新颖性检测提供了理论基础，具有可解释性和高效性，为后续应用和实验奠定了基础。

Abstract: Data mining offers a diverse toolbox for extracting meaningful structures
from complex datasets, with anomaly detection emerging as a critical subfield
particularly in the context of streaming or real-time data. Within anomaly
detection, novelty detection focuses on identifying previously unseen patterns
after training solely on regular data. While classic algorithms such as
One-Class SVM or Local Outlier Factor (LOF) have been widely applied, they
often lack interpretability and scalability. In this work, we explore the
Half-Space Tree (HST) algorithm, originally proposed for streaming anomaly
detection, and propose a novel theoretical modification to adapt it
specifically for novelty detection tasks. Our approach is grounded in the idea
that anomalies i.e., novelties tend to appear in the higher leaves of the tree,
which are less frequently visited by regular instances. We analytically
demonstrate the effectiveness of this approach using probabilistic analysis,
expected depth (EXD) calculations, and combinatorial reasoning. A comparative
analysis of expected depths between our modified HST and the original Isolation
Forest highlights that novelty points are significantly more isolated in our
approach. This supports the hypothesis that HSTs, with appropriate structural
adaptation, can serve as interpretable and efficient novelty detectors. The
paper contributes a theoretical foundation and supporting analysis for this
adaptation, setting the stage for further application and experimentation.

</details>


### [223] [A new methodology to decompose a parametric domain using reduced order data manifold in machine learning](https://arxiv.org/abs/2505.08497)
*Chetra Mang,Axel TahmasebiMoradi,Mouadh Yagoubi*

Main category: cs.LG

TL;DR: 提出了一种基于迭代主成分分析的参数域分解新方法，通过降维和逆投影重构，实现了高效参数域分解。


<details>
  <summary>Details</summary>
Motivation: 解决高维流形降维及参数域分解问题，提升计算效率。

Method: 使用迭代主成分分析降维，开发逆投影重构方法，并基于低维流形分解参数域。

Result: 数值实验表明，该方法在谐波传输问题上比传统元模型（如神经网络）更高效有效。

Conclusion: 新方法在参数域分解中表现出优越性能，适用于复杂问题。

Abstract: We propose a new methodology for parametric domain decomposition using
iterative principal component analysis. Starting with iterative principle
component analysis, the high dimension manifold is reduced to the lower
dimension manifold. Moreover, two approaches are developed to reconstruct the
inverse projector to project from the lower data component to the original one.
Afterward, we provide a detailed strategy to decompose the parametric domain
based on the low dimension manifold. Finally, numerical examples of harmonic
transport problem are given to illustrate the efficiency and effectiveness of
the proposed method comparing to the classical meta-models such as neural
networks.

</details>


### [224] [InfoPO: On Mutual Information Maximization for Large Language Model Alignment](https://arxiv.org/abs/2505.08507)
*Teng Xiao,Zhen Ge,Sujay Sanghavi,Tian Wang,Julian Katz-Samuels,Marc Versage,Qingjun Cui,Trishul Chilimbi*

Main category: cs.LG

TL;DR: 本文提出了一种名为InfoPO的新算法，用于优化大型语言模型（LLMs）的后训练，解决了现有方法依赖Bradley-Terry模型导致的过拟合和性能不佳问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于Bradley-Terry模型的方法在推理任务中表现不佳且易过拟合，需要一种更高效且不依赖该模型的优化方法。

Method: 提出InfoPO算法，无需依赖Bradley-Terry模型，避免选中响应概率下降，实现高效对齐。

Result: 实验表明，InfoPO在多个公开基准测试中表现优于现有方法，尤其在推理任务中。

Conclusion: InfoPO是一种高效且无需依赖特定模型的偏好微调算法，显著提升了LLMs在推理任务中的性能。

Abstract: We study the post-training of large language models (LLMs) with human
preference data. Recently, direct preference optimization and its variants have
shown considerable promise in aligning language models, eliminating the need
for reward models and online sampling. Despite these benefits, these methods
rely on explicit assumptions about the Bradley-Terry (BT) model, which makes
them prone to overfitting and results in suboptimal performance, particularly
on reasoning-heavy tasks. To address these challenges, we propose a principled
preference fine-tuning algorithm called InfoPO, which effectively and
efficiently aligns large language models using preference data. InfoPO
eliminates the reliance on the BT model and prevents the likelihood of the
chosen response from decreasing. Extensive experiments confirm that InfoPO
consistently outperforms established baselines on widely used open benchmarks,
particularly in reasoning tasks.

</details>


### [225] [Learning Advanced Self-Attention for Linear Transformers in the Singular Value Domain](https://arxiv.org/abs/2505.08516)
*Hyowon Wi,Jeongwhan Choi,Noseong Park*

Main category: cs.LG

TL;DR: 论文提出了一种名为AGF的新方法，将自注意力机制解释为图信号处理中的图滤波器，以线性复杂度实现高效处理，并在多个任务中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有自注意力机制仅作为低通滤波器，无法有效利用多种频率信息，限制了其性能。

Method: 提出AGF方法，将自注意力解释为图信号处理中的图滤波器，通过奇异值域学习图滤波器，复杂度为线性。

Result: AGF在长距离任务和时间序列分类等任务中表现优异，达到最优性能。

Conclusion: AGF通过图信号处理视角优化自注意力机制，显著提升了模型性能。

Abstract: Transformers have demonstrated remarkable performance across diverse domains.
The key component of Transformers is self-attention, which learns the
relationship between any two tokens in the input sequence. Recent studies have
revealed that the self-attention can be understood as a normalized adjacency
matrix of a graph. Notably, from the perspective of graph signal processing
(GSP), the self-attention can be equivalently defined as a simple graph filter,
applying GSP using the value vector as the signal. However, the self-attention
is a graph filter defined with only the first order of the polynomial matrix,
and acts as a low-pass filter preventing the effective leverage of various
frequency information. Consequently, existing self-attention mechanisms are
designed in a rather simplified manner. Therefore, we propose a novel method,
called \underline{\textbf{A}}ttentive \underline{\textbf{G}}raph
\underline{\textbf{F}}ilter (AGF), interpreting the self-attention as learning
the graph filter in the singular value domain from the perspective of graph
signal processing for directed graphs with the linear complexity w.r.t. the
input length $n$, i.e., $\mathcal{O}(nd^2)$. In our experiments, we demonstrate
that AGF achieves state-of-the-art performance on various tasks, including Long
Range Arena benchmark and time series classification.

</details>


### [226] [GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in Class-Incremental Learning](https://arxiv.org/abs/2505.08528)
*Minsu Kim,Seong-Hyeon Hwang,Steven Euijong Whang*

Main category: cs.LG

TL;DR: 论文提出GradMix，一种针对类增量学习中灾难性遗忘问题的梯度选择性混合数据增强方法，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 持续学习中，新知识获取与旧知识保持存在挑战，现有方法（如经验回放）可能因随机混合样本而损害旧知识。

Method: GradMix通过基于梯度的选择性混合，仅混合有益类对的样本，避免有害类对。

Result: 实验表明，GradMix在多种数据集上通过最小化遗忘，显著提升了准确率。

Conclusion: GradMix为类增量学习中的灾难性遗忘问题提供了有效解决方案。

Abstract: In the context of continual learning, acquiring new knowledge while
maintaining previous knowledge presents a significant challenge. Existing
methods often use experience replay techniques that store a small portion of
previous task data for training. In experience replay approaches, data
augmentation has emerged as a promising strategy to further improve the model
performance by mixing limited previous task data with sufficient current task
data. However, we theoretically and empirically analyze that training with
mixed samples from random sample pairs may harm the knowledge of previous tasks
and cause greater catastrophic forgetting. We then propose GradMix, a robust
data augmentation method specifically designed for mitigating catastrophic
forgetting in class-incremental learning. GradMix performs gradient-based
selective mixup using a class-based criterion that mixes only samples from
helpful class pairs and not from detrimental class pairs for reducing
catastrophic forgetting. Our experiments on various real datasets show that
GradMix outperforms data augmentation baselines in accuracy by minimizing the
forgetting of previous knowledge.

</details>


### [227] [ExEBench: Benchmarking Foundation Models on Extreme Earth Events](https://arxiv.org/abs/2505.08529)
*Shan Zhao,Zhitong Xiong,Jie Zhao,Xiao Xiang Zhu*

Main category: cs.LG

TL;DR: 论文介绍了ExEBench，一个包含七类极端事件的基准数据集，用于评估基础模型在极端事件管理中的可靠性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 极端事件对人类和生态系统构成重大风险，基础模型在灾害管理中表现出潜力，但其性能受数据偏差影响。

Method: 通过ExEBench数据集（覆盖全球、多源数据）和多任务评估，测试基础模型的泛化能力。

Result: ExEBench为极端事件检测、监测和预测提供了评估平台，并促进新方法开发。

Conclusion: ExEBench有助于理解极端事件及其在气候变化下的相互作用，推动灾害管理技术进步。

Abstract: Our planet is facing increasingly frequent extreme events, which pose major
risks to human lives and ecosystems. Recent advances in machine learning (ML),
especially with foundation models (FMs) trained on extensive datasets, excel in
extracting features and show promise in disaster management. Nevertheless,
these models often inherit biases from training data, challenging their
performance over extreme values. To explore the reliability of FM in the
context of extreme events, we introduce \textbf{ExE}Bench (\textbf{Ex}treme
\textbf{E}arth Benchmark), a collection of seven extreme event categories
across floods, wildfires, storms, tropical cyclones, extreme precipitation,
heatwaves, and cold waves. The dataset features global coverage, varying data
volumes, and diverse data sources with different spatial, temporal, and
spectral characteristics. To broaden the real-world impact of FMs, we include
multiple challenging ML tasks that are closely aligned with operational needs
in extreme events detection, monitoring, and forecasting. ExEBench aims to (1)
assess FM generalizability across diverse, high-impact tasks and domains, (2)
promote the development of novel ML methods that benefit disaster management,
and (3) offer a platform for analyzing the interactions and cascading effects
of extreme events to advance our understanding of Earth system, especially
under the climate change expected in the decades to come. The dataset and code
are public https://github.com/zhaoshan2/EarthExtreme-Bench.

</details>


### [228] [OLinear: A Linear Model for Time Series Forecasting in Orthogonally Transformed Domain](https://arxiv.org/abs/2505.08550)
*Wenzhen Yue,Yong Liu,Haoxuan Li,Hao Wang,Xianghua Ying,Ruohao Guo,Bowei Xing,Ji Shi*

Main category: cs.LG

TL;DR: OLinear是一种基于线性变换的多变量时间序列预测模型，通过正交变换域提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统时间域预测模型因步长依赖性受限，固定基变换效果有限，需数据自适应方法。

Method: 使用OrthoTrans正交矩阵变换解耦时间相关性，NormLin线性层捕获多变量依赖。

Result: 在24个基准测试和140个任务中表现最优，NormLin模块优于多头自注意力且计算量更低。

Conclusion: OLinear高效且性能优越，NormLin可作为插件提升现有模型。

Abstract: This paper presents $\mathbf{OLinear}$, a $\mathbf{linear}$-based
multivariate time series forecasting model that operates in an
$\mathbf{o}$rthogonally transformed domain. Recent forecasting models typically
adopt the temporal forecast (TF) paradigm, which directly encode and decode
time series in the time domain. However, the entangled step-wise dependencies
in series data can hinder the performance of TF. To address this, some
forecasters conduct encoding and decoding in the transformed domain using
fixed, dataset-independent bases (e.g., sine and cosine signals in the Fourier
transform). In contrast, we utilize $\mathbf{OrthoTrans}$, a data-adaptive
transformation based on an orthogonal matrix that diagonalizes the series'
temporal Pearson correlation matrix. This approach enables more effective
encoding and decoding in the decorrelated feature domain and can serve as a
plug-in module to enhance existing forecasters. To enhance the representation
learning for multivariate time series, we introduce a customized linear layer,
$\mathbf{NormLin}$, which employs a normalized weight matrix to capture
multivariate dependencies. Empirically, the NormLin module shows a surprising
performance advantage over multi-head self-attention, while requiring nearly
half the FLOPs. Extensive experiments on 24 benchmarks and 140 forecasting
tasks demonstrate that OLinear consistently achieves state-of-the-art
performance with high efficiency. Notably, as a plug-in replacement for
self-attention, the NormLin module consistently enhances Transformer-based
forecasters. The code and datasets are available at
https://anonymous.4open.science/r/OLinear

</details>


### [229] [Online Learning and Unlearning](https://arxiv.org/abs/2505.08557)
*Yaxi Hu,Bernhard Schölkopf,Amartya Sanyal*

Main category: cs.LG

TL;DR: 论文提出了在线学习-遗忘问题，并基于在线梯度下降（OGD）设计了两种算法，分别是被动OLU和主动OLU，均能在保证遗忘性能的同时维持与标准OGD相当的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 在线学习中如何高效处理遗忘请求，确保模型在遗忘数据点后的输出与从未学习该数据点的模型统计不可区分。

Method: 1. 被动OLU：利用OGD的收缩性质，在遗忘时注入噪声，无需额外计算；2. 主动OLU：采用离线遗忘算法，将模型推向排除删除数据的解。

Result: 在标准凸性和平滑性假设下，两种方法均实现了与标准OGD相当的遗憾界。

Conclusion: 研究表明，在线学习中可以同时实现竞争性的遗憾界和遗忘保证。

Abstract: We formalize the problem of online learning-unlearning, where a model is
updated sequentially in an online setting while accommodating unlearning
requests between updates. After a data point is unlearned, all subsequent
outputs must be statistically indistinguishable from those of a model trained
without that point. We present two online learner-unlearner (OLU) algorithms,
both built upon online gradient descent (OGD). The first, passive OLU,
leverages OGD's contractive property and injects noise when unlearning occurs,
incurring no additional computation. The second, active OLU, uses an offline
unlearning algorithm that shifts the model toward a solution excluding the
deleted data. Under standard convexity and smoothness assumptions, both methods
achieve regret bounds comparable to those of standard OGD, demonstrating that
one can maintain competitive regret bounds while providing unlearning
guarantees.

</details>


### [230] [MUBox: A Critical Evaluation Framework of Deep Machine Unlearning](https://arxiv.org/abs/2505.08576)
*Xiang Li,Bhavani Thuraisingham,Wenqi Wei*

Main category: cs.LG

TL;DR: MUBox是一个评估深度学习中去学习方法的平台，整合了23种先进技术，通过11种指标在6种场景下测试，揭示了现有方法的局限性和评估复杂性。


<details>
  <summary>Details</summary>
Motivation: 法律框架要求实现‘被遗忘权’，需要从机器学习模型中移除特定数据，但目前去学习方法的效果和评估标准尚不明确。

Method: 开发MUBox平台，整合23种去学习方法，在6种场景下使用11种指标进行评估。

Result: 发现现有方法在不同场景下效果不一致，评估需多指标结合，且去毒方法效果因攻击类型而异。

Conclusion: MUBox为去学习方法提供了统一评估框架，强调多指标和多场景评估的重要性。

Abstract: Recent legal frameworks have mandated the right to be forgotten, obligating
the removal of specific data upon user requests. Machine Unlearning has emerged
as a promising solution by selectively removing learned information from
machine learning models. This paper presents MUBox, a comprehensive platform
designed to evaluate unlearning methods in deep learning. MUBox integrates 23
advanced unlearning techniques, tested across six practical scenarios with 11
diverse evaluation metrics. It allows researchers and practitioners to (1)
assess and compare the effectiveness of different machine unlearning methods
across various scenarios; (2) examine the impact of current evaluation metrics
on unlearning performance; and (3) conduct detailed comparative studies on
machine unlearning in a unified framework. Leveraging MUBox, we systematically
evaluate these unlearning methods in deep learning and uncover several key
insights: (a) Even state-of-the-art unlearning methods, including those
published in top-tier venues and winners of unlearning competitions,
demonstrate inconsistent effectiveness across diverse scenarios. Prior research
has predominantly focused on simplified settings, such as random forgetting and
class-wise unlearning, highlighting the need for broader evaluations across
more difficult unlearning tasks. (b) Assessing unlearning performance remains a
non-trivial problem, as no single evaluation metric can comprehensively capture
the effectiveness, efficiency, and preservation of model utility. Our findings
emphasize the necessity of employing multiple metrics to achieve a balanced and
holistic assessment of unlearning methods. (c) In the context of depoisoning,
our evaluation reveals significant variability in the effectiveness of existing
approaches, which is highly dependent on the specific type of poisoning
attacks.

</details>


### [231] [Clustering of Incomplete Data via a Bipartite Graph Structure](https://arxiv.org/abs/2505.08594)
*Amirhossein Javaheri,Daniel P. Palomar*

Main category: cs.LG

TL;DR: 提出了一种基于二分图模型的聚类方法，解决了中心节点数据缺失和重尾数据处理的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有二分图模型需要中心节点数据，且高斯模型对重尾数据效果不佳，金融数据中常见此问题。

Method: 设计了一种无需中心节点信息的二分图聚类方法，并优化了重尾数据处理能力。

Result: 通过真实金融数据实验验证了方法的有效性。

Conclusion: 该方法在数据不完整和重尾数据场景下表现优异。

Abstract: There are various approaches to graph learning for data clustering,
incorporating different spectral and structural constraints through diverse
graph structures. Some methods rely on bipartite graph models, where nodes are
divided into two classes: centers and members. These models typically require
access to data for the center nodes in addition to observations from the member
nodes. However, such additional data may not always be available in many
practical scenarios. Moreover, popular Gaussian models for graph learning have
demonstrated limited effectiveness in modeling data with heavy-tailed
distributions, which are common in financial markets. In this paper, we propose
a clustering method based on a bipartite graph model that addresses these
challenges. First, it can infer clusters from incomplete data without requiring
information about the center nodes. Second, it is designed to effectively
handle heavy-tailed data. Numerical experiments using real financial data
validate the efficiency of the proposed method for data clustering.

</details>


### [232] [Cost Function Estimation Using Inverse Reinforcement Learning with Minimal Observations](https://arxiv.org/abs/2505.08619)
*Sarmad Mehrdad,Avadesh Meduri,Ludovic Righetti*

Main category: cs.LG

TL;DR: 提出一种迭代逆强化学习算法，用于推断连续空间中的最优成本函数。基于最大熵准则，通过迭代优化权重并调整步长，确保学习到的成本函数特征与演示轨迹特征相似。相比同类方法，该算法能单独调整每个观测对分区函数的有效性，且无需大量样本，学习更快。通过求解最优控制问题生成样本轨迹，而非随机采样，使轨迹更具信息量。实验表明，该方法在多个模拟环境中优于两种先进算法。


<details>
  <summary>Details</summary>
Motivation: 现有逆强化学习方法在连续空间中推断成本函数时，通常需要大量样本且效率较低。本文旨在提出一种更高效、适应性更强的算法。

Method: 基于最大熵准则的迭代逆强化学习算法，通过优化权重和步长调整，生成更有效的样本轨迹（通过最优控制问题而非随机采样）。

Result: 在多个模拟环境中，该方法优于两种先进算法，表现出更高的效率和适应性。

Conclusion: 该算法在连续空间中推断最优成本函数时具有显著优势，尤其在样本效率和轨迹信息量方面表现突出。

Abstract: We present an iterative inverse reinforcement learning algorithm to infer
optimal cost functions in continuous spaces. Based on a popular maximum entropy
criteria, our approach iteratively finds a weight improvement step and proposes
a method to find an appropriate step size that ensures learned cost function
features remain similar to the demonstrated trajectory features. In contrast to
similar approaches, our algorithm can individually tune the effectiveness of
each observation for the partition function and does not need a large sample
set, enabling faster learning. We generate sample trajectories by solving an
optimal control problem instead of random sampling, leading to more informative
trajectories. The performance of our method is compared to two state of the art
algorithms to demonstrate its benefits in several simulated environments.

</details>


### [233] [Credit Assignment and Efficient Exploration based on Influence Scope in Multi-agent Reinforcement Learning](https://arxiv.org/abs/2505.08630)
*Shuai Han,Mehdi Dastani,Shihan Wang*

Main category: cs.LG

TL;DR: 提出了一种新方法（ISA）解决稀疏奖励多智能体强化学习中的信用分配和探索问题，通过计算智能体对状态属性的影响范围来优化性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励场景下，传统方法难以精确分配信用和有效探索，影响多智能体协作效果。

Method: 提出ISA算法，计算智能体对状态维度的影响范围，用于信用分配和探索空间限定。

Result: 在多种稀疏奖励场景中，ISA显著优于现有基线方法。

Conclusion: ISA有效解决了稀疏奖励下的信用分配和探索问题，提升了多智能体协作性能。

Abstract: Training cooperative agents in sparse-reward scenarios poses significant
challenges for multi-agent reinforcement learning (MARL). Without clear
feedback on actions at each step in sparse-reward setting, previous methods
struggle with precise credit assignment among agents and effective exploration.
In this paper, we introduce a novel method to deal with both credit assignment
and exploration problems in reward-sparse domains. Accordingly, we propose an
algorithm that calculates the Influence Scope of Agents (ISA) on states by
taking specific value of the dimensions/attributes of states that can be
influenced by individual agents. The mutual dependence between agents' actions
and state attributes are then used to calculate the credit assignment and to
delimit the exploration space for each individual agent. We then evaluate ISA
in a variety of sparse-reward multi-agent scenarios. The results show that our
method significantly outperforms the state-of-art baselines.

</details>


### [234] [Modular Federated Learning: A Meta-Framework Perspective](https://arxiv.org/abs/2505.08646)
*Frederico Vicente,Cláudia Soares,Dušan Jakovetić*

Main category: cs.LG

TL;DR: 本文提出了一种元框架视角，将联邦学习（FL）分解为模块化组件，系统化其方法、挑战和应用，并强调聚合与对齐的双重作用。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私保护和分布式环境中具有重要价值，但其复杂性需要结构化理解以推动研究和应用。

Method: 通过元框架将FL分解为模块化组件，包括通信、优化、安全和隐私，并提出新的分类法区分聚合与对齐。

Result: 提供了FL的历史背景、Python实现框架及关键挑战的系统化分析，为研究和部署提供基础。

Conclusion: 通过模块化元框架和聚合与对齐的双重作用，本文为FL的研究和实际应用提供了全面且灵活的基础。

Abstract: Federated Learning (FL) enables distributed machine learning training while
preserving privacy, representing a paradigm shift for data-sensitive and
decentralized environments. Despite its rapid advancements, FL remains a
complex and multifaceted field, requiring a structured understanding of its
methodologies, challenges, and applications. In this survey, we introduce a
meta-framework perspective, conceptualising FL as a composition of modular
components that systematically address core aspects such as communication,
optimisation, security, and privacy. We provide a historical contextualisation
of FL, tracing its evolution from distributed optimisation to modern
distributed learning paradigms. Additionally, we propose a novel taxonomy
distinguishing Aggregation from Alignment, introducing the concept of alignment
as a fundamental operator alongside aggregation. To bridge theory with
practice, we explore available FL frameworks in Python, facilitating real-world
implementation. Finally, we systematise key challenges across FL sub-fields,
providing insights into open research questions throughout the meta-framework
modules. By structuring FL within a meta-framework of modular components and
emphasising the dual role of Aggregation and Alignment, this survey provides a
holistic and adaptable foundation for understanding and advancing FL research
and deployment.

</details>


### [235] [AC-PKAN: Attention-Enhanced and Chebyshev Polynomial-Based Physics-Informed Kolmogorov-Arnold Networks](https://arxiv.org/abs/2505.08687)
*Hangwei Zhang,Zhimu Huang,Yan Wang*

Main category: cs.LG

TL;DR: 论文提出了一种改进的Chebyshev1KANs架构（AC-PKAN），通过引入小波激活的MLP和注意力机制，解决了原方法的秩崩溃问题，并在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 原始的Chebyshev1KANs虽然优于传统KANs，但仍存在秩崩溃问题，限制了其表达能力。

Method: 通过集成小波激活的MLP和内部注意力机制，设计了一种新的架构AC-PKAN，并引入外部残差梯度注意力（RGA）机制来优化损失函数。

Result: AC-PKAN在多个基准任务中表现优于或匹配现有最佳模型（如PINNsFormer）。

Conclusion: AC-PKAN显著提升了弱监督物理信息神经网络（PINNs）的表达能力，适用于复杂工程问题。

Abstract: Kolmogorov-Arnold Networks (KANs) have recently shown promise for solving
partial differential equations (PDEs). Yet their original formulation is
computationally and memory intensive, motivating the introduction of Chebyshev
Type-I-based KANs (Chebyshev1KANs). Although Chebyshev1KANs have outperformed
the vanilla KANs architecture, our rigorous theoretical analysis reveals that
they still suffer from rank collapse, ultimately limiting their expressive
capacity. To overcome these limitations, we enhance Chebyshev1KANs by
integrating wavelet-activated MLPs with learnable parameters and an internal
attention mechanism. We prove that this design preserves a full-rank Jacobian
and is capable of approximating solutions to PDEs of arbitrary order.
Furthermore, to alleviate the loss instability and imbalance introduced by the
Chebyshev polynomial basis, we externally incorporate a Residual Gradient
Attention (RGA) mechanism that dynamically re-weights individual loss terms
according to their gradient norms and residual magnitudes. By jointly
leveraging internal and external attention, we present AC-PKAN, a novel
architecture that constitutes an enhancement to weakly supervised
Physics-Informed Neural Networks (PINNs) and extends the expressive power of
KANs. Experimental results from nine benchmark tasks across three domains show
that AC-PKAN consistently outperforms or matches state-of-the-art models such
as PINNsFormer, establishing it as a highly effective tool for solving complex
real-world engineering problems in zero-data or data-sparse regimes. The code
will be made publicly available upon acceptance.

</details>


### [236] [PWC-MoE: Privacy-Aware Wireless Collaborative Mixture of Experts](https://arxiv.org/abs/2505.08719)
*Yang Su,Na Yan,Yansha Deng,Robert Schober*

Main category: cs.LG

TL;DR: 论文提出了一种隐私感知的无线协作专家混合框架（PWC-MoE），通过动态路由敏感和非敏感令牌，平衡计算成本、性能和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 解决云服务器上大型语言模型（LLMs）的隐私和带宽问题，同时避免本地小型语言模型（SLMs）性能不足的缺陷。

Method: 使用稀疏隐私感知门控网络动态路由令牌，引入负载均衡机制和带宽自适应令牌卸载方案。

Result: 实验表明PWC-MoE在带宽受限环境中有效保护隐私并保持高性能。

Conclusion: PWC-MoE为隐私敏感和带宽受限场景下的LLM部署提供了实用解决方案。

Abstract: Large language models (LLMs) hosted on cloud servers alleviate the
computational and storage burdens on local devices but raise privacy concerns
due to sensitive data transmission and require substantial communication
bandwidth, which is challenging in constrained environments. In contrast, small
language models (SLMs) running locally enhance privacy but suffer from limited
performance on complex tasks. To balance computational cost, performance, and
privacy protection under bandwidth constraints, we propose a privacy-aware
wireless collaborative mixture of experts (PWC-MoE) framework. Specifically,
PWC-MoE employs a sparse privacy-aware gating network to dynamically route
sensitive tokens to privacy experts located on local clients, while
non-sensitive tokens are routed to non-privacy experts located at the remote
base station. To achieve computational efficiency, the gating network ensures
that each token is dynamically routed to and processed by only one expert. To
enhance scalability and prevent overloading of specific experts, we introduce a
group-wise load-balancing mechanism for the gating network that evenly
distributes sensitive tokens among privacy experts and non-sensitive tokens
among non-privacy experts. To adapt to bandwidth constraints while preserving
model performance, we propose a bandwidth-adaptive and importance-aware token
offloading scheme. This scheme incorporates an importance predictor to evaluate
the importance scores of non-sensitive tokens, prioritizing the most important
tokens for transmission to the base station based on their predicted importance
and the available bandwidth. Experiments demonstrate that the PWC-MoE framework
effectively preserves privacy and maintains high performance even in
bandwidth-constrained environments, offering a practical solution for deploying
LLMs in privacy-sensitive and bandwidth-limited scenarios.

</details>


### [237] [Memorization-Compression Cycles Improve Generalization](https://arxiv.org/abs/2505.08727)
*Fangyuan Yu*

Main category: cs.LG

TL;DR: 论文提出了一种通过压缩内部表示提升泛化能力的方法，引入信息瓶颈语言建模（IBLM）目标，并提出GAPT训练算法，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索数据扩展和内部表示压缩对泛化能力的共同影响，并模拟生物学习与睡眠巩固的交替模式。

Method: 提出IBLM目标，将语言建模转化为约束优化问题；设计GAPT算法，动态切换记忆与压缩阶段。

Result: GAPT在GPT-2预训练中降低MBE 50%，提升交叉熵4.8%，OOD泛化提升35%，减少灾难性遗忘干扰97%。

Conclusion: 压缩表示与动态训练阶段切换能显著提升模型泛化能力，模拟生物学习机制。

Abstract: We prove theoretically that generalization improves not only through data
scaling but also by compressing internal representations. To operationalize
this insight, we introduce the Information Bottleneck Language Modeling (IBLM)
objective, which reframes language modeling as a constrained optimization
problem: minimizing representation entropy subject to optimal prediction
performance. Empirically, we observe an emergent memorization-compression cycle
during LLM pretraining, evidenced by oscillation positive/negative gradient
alignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of
representation entropy. This pattern closely mirrors the predictive-compressive
trade-off prescribed by IBLM and also parallels the biological alternation
between awake learning and sleep consolidation. Motivated by this observation,
we propose Gated Phase Transition (GAPT), a training algorithm that adaptively
switches between memorization and compression phases. When applied to GPT-2
pretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves
cross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining
task on arithmetic multiplication. In a setting designed to simulate
catastrophic forgetting, GAPT reduces interference by compressing and
separating representations, achieving a 97% improvement in separation -
paralleling the functional role of sleep consolidation.

</details>


### [238] [Preference Optimization for Combinatorial Optimization Problems](https://arxiv.org/abs/2505.08735)
*Mingjun Pan,Guanquan Lin,You-Wei Luo,Bin Zhu,Zhien Dai,Lijun Sun,Chun Yuan*

Main category: cs.LG

TL;DR: 论文提出了一种名为偏好优化的新方法，通过将定量奖励信号转化为定性偏好信号，解决了强化学习在组合优化中的效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在组合优化中面临奖励信号减弱和探索效率低的问题，导致性能不佳。

Method: 通过重新参数化奖励函数并结合偏好模型，提出了一种熵正则化的强化学习目标，同时引入局部搜索技术优化偏好对生成。

Result: 在多个基准问题（如TSP、CVRP和FFSP）上，该方法显著优于现有强化学习算法，收敛效率和解决方案质量均更优。

Conclusion: 偏好优化方法有效提升了强化学习在组合优化中的性能，解决了现有方法的局限性。

Abstract: Reinforcement Learning (RL) has emerged as a powerful tool for neural
combinatorial optimization, enabling models to learn heuristics that solve
complex problems without requiring expert knowledge. Despite significant
progress, existing RL approaches face challenges such as diminishing reward
signals and inefficient exploration in vast combinatorial action spaces,
leading to inefficiency. In this paper, we propose Preference Optimization, a
novel method that transforms quantitative reward signals into qualitative
preference signals via statistical comparison modeling, emphasizing the
superiority among sampled solutions. Methodologically, by reparameterizing the
reward function in terms of policy and utilizing preference models, we
formulate an entropy-regularized RL objective that aligns the policy directly
with preferences while avoiding intractable computations. Furthermore, we
integrate local search techniques into the fine-tuning rather than
post-processing to generate high-quality preference pairs, helping the policy
escape local optima. Empirical results on various benchmarks, such as the
Traveling Salesman Problem (TSP), the Capacitated Vehicle Routing Problem
(CVRP) and the Flexible Flow Shop Problem (FFSP), demonstrate that our method
significantly outperforms existing RL algorithms, achieving superior
convergence efficiency and solution quality.

</details>


### [239] [Towards Foundation Models for Experimental Readout Systems Combining Discrete and Continuous Data](https://arxiv.org/abs/2505.08736)
*James Giroux,Cristiano Fanelli*

Main category: cs.LG

TL;DR: 提出了一种用于核物理的基础模型，支持从成像切伦科夫探测器中处理低级别输入，通过三项创新解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于下一令牌预测方法的局限性，包括分辨率损失和缺乏条件生成能力。

Method: 采用离散空间特征和连续变量的分离词汇表、连续运动学条件嵌入以及高分辨率连续变量标记化。

Result: 模型能够快速生成高保真度的切伦科夫光子序列，并在重建任务中表现出泛化能力。

Conclusion: 该模型在核物理领域具有潜力，特别是在高分辨率生成和重建任务中。

Abstract: We present a (proto) Foundation Model for Nuclear Physics, capable of
operating on low-level detector inputs from Imaging Cherenkov Detectors at the
future Electron Ion Collider. To address limitations in existing next-token
prediction approaches-namely resolution loss from VQ-VAE tokenization and lack
of conditional generation-we propose three key innovations: (i) separate
vocabularies for discrete spatial features and continuous variates, combined
via Causal Multi-Head Cross-Attention (CMHCA), (ii) continuous kinematic
conditioning through prepended context embeddings, and (iii) scalable and
simple, high-resolution continuous variate tokenization without joint
vocabulary inflation. Our model enables fast, high-fidelity generation of pixel
and time sequences for Cherenkov photons, validated through closure tests in
the High Performance DIRC. We also show our model generalizes to reconstruction
tasks such as pion and kaon identification, in which we show its ability to
leverage fine-tuning.

</details>


### [240] [Sensitivity-Constrained Fourier Neural Operators for Forward and Inverse Problems in Parametric Differential Equations](https://arxiv.org/abs/2505.08740)
*Abdolmehdi Behroozi,Chaopeng Shen and,Daniel Kifer*

Main category: cs.LG

TL;DR: SC-FNO通过引入基于敏感性的正则化策略，解决了FNO在逆问题、敏感性估计和概念漂移中的局限性，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统FNO在逆问题和敏感性估计中表现不佳，需要一种更高效的方法。

Method: 提出SC-FNO，结合敏感性约束的正则化策略，优化训练过程。

Result: SC-FNO在预测解路径和参数反演任务中表现优异，且适用于高维参数空间。

Conclusion: SC-FNO是一种高效且通用的方法，适用于多种微分方程和神经算子。

Abstract: Parametric differential equations of the form du/dt = f(u, x, t, p) are
fundamental in science and engineering. While deep learning frameworks such as
the Fourier Neural Operator (FNO) can efficiently approximate solutions, they
struggle with inverse problems, sensitivity estimation (du/dp), and concept
drift. We address these limitations by introducing a sensitivity-based
regularization strategy, called Sensitivity-Constrained Fourier Neural
Operators (SC-FNO). SC-FNO achieves high accuracy in predicting solution paths
and consistently outperforms standard FNO and FNO with physics-informed
regularization. It improves performance in parameter inversion tasks, scales to
high-dimensional parameter spaces (tested with up to 82 parameters), and
reduces both data and training requirements. These gains are achieved with a
modest increase in training time (30% to 130% per epoch) and generalize across
various types of differential equations and neural operators. Code and selected
experiments are available at: https://github.com/AMBehroozi/SC_Neural_Operators

</details>


### [241] [Implet: A Post-hoc Subsequence Explainer for Time Series Models](https://arxiv.org/abs/2505.08748)
*Fanyu Meng,Ziwen Kan,Shahbaz Rezaei,Zhaodan Kong,Xin Chen,Xin Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为Implet的后处理解释器，用于生成时间序列模型的子序列级解释，提升模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 时间序列模型的可解释性对于建立信任、调试和实际应用中的解释至关重要。

Method: Implet通过识别对模型预测有显著贡献的关键时间片段，提供比传统特征归因方法更强的解释性，并进一步提出基于群体的解释框架。

Result: 在多个标准时间序列分类基准上验证了Implet的有效性。

Conclusion: Implet显著提升了时间序列模型的解释性，代码已开源。

Abstract: Explainability in time series models is crucial for fostering trust,
facilitating debugging, and ensuring interpretability in real-world
applications. In this work, we introduce Implet, a novel post-hoc explainer
that generates accurate and concise subsequence-level explanations for time
series models. Our approach identifies critical temporal segments that
significantly contribute to the model's predictions, providing enhanced
interpretability beyond traditional feature-attribution methods. Based on it,
we propose a cohort-based (group-level) explanation framework designed to
further improve the conciseness and interpretability of our explanations. We
evaluate Implet on several standard time-series classification benchmarks,
demonstrating its effectiveness in improving interpretability. The code is
available at https://github.com/LbzSteven/implet

</details>


### [242] [SPAT: Sensitivity-based Multihead-attention Pruning on Time Series Forecasting Models](https://arxiv.org/abs/2505.08768)
*Suhan Guo,Jiahong Deng,Mengjun Yi,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: SPAT是一种结构化剪枝方法，通过选择性移除冗余注意力机制，提升模型效率，减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 基于注意力的架构在多变量时间序列预测中表现优异，但计算成本高，需优化以减少延迟和模型大小。

Method: 提出动态敏感性度量SEND，在预训练阶段评估每个注意力模块的重要性，并移除冗余模块。

Result: 实验显示，SPAT剪枝模型在MSE、MAE和FLOPs上分别减少2.842%、1.996%和35.274%，性能优于现有轻量级方法。

Conclusion: SPAT通过保留最有效的注意力机制，显著提升模型效率，适用于标准和零样本推理。

Abstract: Attention-based architectures have achieved superior performance in
multivariate time series forecasting but are computationally expensive.
Techniques such as patching and adaptive masking have been developed to reduce
their sizes and latencies. In this work, we propose a structured pruning
method, SPAT ($\textbf{S}$ensitivity $\textbf{P}$runer for
$\textbf{At}$tention), which selectively removes redundant attention mechanisms
and yields highly effective models. Different from previous approaches, SPAT
aims to remove the entire attention module, which reduces the risk of
overfitting and enables speed-up without demanding specialized hardware. We
propose a dynamic sensitivity metric, $\textbf{S}$ensitivity
$\textbf{E}$nhanced $\textbf{N}$ormalized $\textbf{D}$ispersion (SEND) that
measures the importance of each attention module during the pre-training phase.
Experiments on multivariate datasets demonstrate that SPAT-pruned models
achieve reductions of 2.842% in MSE, 1.996% in MAE, and 35.274% in FLOPs.
Furthermore, SPAT-pruned models outperform existing lightweight, Mamba-based
and LLM-based SOTA methods in both standard and zero-shot inference,
highlighting the importance of retaining only the most effective attention
mechanisms. We have made our code publicly available
https://anonymous.4open.science/r/SPAT-6042.

</details>


### [243] [Addressing the Current Challenges of Quantum Machine Learning through Multi-Chip Ensembles](https://arxiv.org/abs/2505.08782)
*Junghoon Justin Park,Jiook Cha,Samuel Yen-Chi Chen,Huan-Hsin Tseng,Shinjae Yoo*

Main category: cs.LG

TL;DR: 论文提出了一种多芯片集成VQC框架，通过将高维计算分配到小型量子芯片上，提升可扩展性、可训练性和噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习（QML）在解决计算难题方面潜力巨大，但受限于NISQ设备的噪声、可扩展性和变分量子电路（VQC）的可训练性问题。

Method: 采用多芯片集成VQC框架，通过分区高维计算和可控纠缠增强性能。

Result: 实验表明，该方法缓解了贫瘠高原问题，减少了量子误差偏差和方差，并在标准数据集（MNIST、FashionMNIST、CIFAR-10）和真实数据集（PhysioNet EEG）上验证了其有效性。

Conclusion: 该框架为近期量子硬件上的可扩展QML提供了可行方案。

Abstract: Quantum Machine Learning (QML) holds significant promise for solving
computational challenges across diverse domains. However, its practical
deployment is constrained by the limitations of noisy intermediate-scale
quantum (NISQ) devices, including noise, limited scalability, and trainability
issues in variational quantum circuits (VQCs). We introduce the multi-chip
ensemble VQC framework, which partitions high-dimensional computations across
smaller quantum chips to enhance scalability, trainability, and noise
resilience. We show that this approach mitigates barren plateaus, reduces
quantum error bias and variance, and maintains robust generalization through
controlled entanglement. Designed to align with current and emerging quantum
hardware, the framework demonstrates strong potential for enabling scalable QML
on near-term devices, as validated by experiments on standard benchmark
datasets (MNIST, FashionMNIST, CIFAR-10) and real world dataset (PhysioNet
EEG).

</details>


### [244] [CodePDE: An Inference Framework for LLM-driven PDE Solver Generation](https://arxiv.org/abs/2505.08783)
*Shanda Li,Tanya Marwah,Junhong Shen,Weiwei Sun,Andrej Risteski,Yiming Yang,Ameet Talwalkar*

Main category: cs.LG

TL;DR: CodePDE利用大型语言模型（LLM）生成PDE求解器，无需任务特定调优，实现超人类性能。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器依赖专家知识且计算成本高，神经网络求解器需大量数据且缺乏可解释性。

Method: 将PDE求解视为代码生成任务，利用LLM的推理、调试、自优化和测试时扩展能力。

Result: CodePDE在多种代表性PDE问题上表现超人类，并分析了生成求解器的准确性、效率和数值方案选择。

Conclusion: LLM在PDE求解中展现出潜力，但也存在局限性，为未来模型开发提供了新视角。

Abstract: Partial differential equations (PDEs) are fundamental to modeling physical
systems, yet solving them remains a complex challenge. Traditional numerical
solvers rely on expert knowledge to implement and are computationally
expensive, while neural-network-based solvers require large training datasets
and often lack interpretability. In this work, we frame PDE solving as a code
generation task and introduce CodePDE, the first inference framework for
generating PDE solvers using large language models (LLMs). Leveraging advanced
inference-time algorithms and scaling strategies, CodePDE unlocks critical
capacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and
test-time scaling -- all without task-specific tuning. CodePDE achieves
superhuman performance across a range of representative PDE problems. We also
present a systematic empirical analysis of LLM generated solvers, analyzing
their accuracy, efficiency, and numerical scheme choices. Our findings
highlight the promise and the current limitations of LLMs in PDE solving,
offering a new perspective on solver design and opportunities for future model
development. Our code is available at https://github.com/LithiumDA/CodePDE.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [245] [Image-Guided Microstructure Optimization using Diffusion Models: Validated with Li-Mn-rich Cathode Precursors](https://arxiv.org/abs/2505.07906)
*Geunho Choi,Changhwan Lee,Jieun Kim,Insoo Ye,Keeyoung Jung,Inchul Park*

Main category: cond-mat.mtrl-sci

TL;DR: 论文提出了一种基于AI的闭环框架，用于锂离子电池正极前驱体的微观结构预测与优化。


<details>
  <summary>Details</summary>
Motivation: 微观结构对材料性能至关重要，但因其难以量化、预测和优化，很少被作为设计变量。

Method: 结合扩散式图像生成模型、定量图像分析流程和粒子群优化算法，从SEM图像提取形态特征，预测并优化合成条件。

Result: 框架能准确预测特定合成条件下的微观结构，并通过实验验证了预测与合成结构的高度一致性。

Conclusion: 该框架为数据驱动的材料设计提供了实用策略，支持正向预测和逆向设计，推动了自主微观结构工程的发展。

Abstract: Microstructure often dictates materials performance, yet it is rarely treated
as an explicit design variable because microstructure is hard to quantify,
predict, and optimize. Here, we introduce an image centric, closed-loop
framework that makes microstructural morphology into a controllable objective
and demonstrate its use case with Li- and Mn-rich layered oxide cathode
precursors. This work presents an integrated, AI driven framework for the
predictive design and optimization of lithium-ion battery cathode precursor
synthesis. This framework integrates a diffusion-based image generation model,
a quantitative image analysis pipeline, and a particle swarm optimization (PSO)
algorithm. By extracting key morphological descriptors such as texture,
sphericity, and median particle size (D50) from SEM images, the platform
accurately predicts SEM like morphologies resulting from specific
coprecipitation conditions, including reaction time-, solution concentration-,
and pH-dependent structural changes. Optimization then pinpoints synthesis
parameters that yield user defined target morphologies, as experimentally
validated by the close agreement between predicted and synthesized structures.
This framework offers a practical strategy for data driven materials design,
enabling both forward prediction and inverse design of synthesis conditions and
paving the way toward autonomous, image guided microstructure engineering.

</details>


### [246] [Enhancing the Efficiency of Complex Systems Crystal Structure Prediction by Active Learning Guided Machine Learning Potential](https://arxiv.org/abs/2505.08159)
*Jiaxiang Li,Junwei Feng,Jie Luo,Bowen Jiang,Xiangyu Zheng,Jian Lv,Keith Butler,Hanyu Liu,Congwei Xie,Yu Xie,Yanming Ma*

Main category: cond-mat.mtrl-sci

TL;DR: 提出了一种自动化工作流，构建通用且数据高效的机器学习势能（MLP），以解决复杂材料系统中的组合爆炸问题，并加速新材料的发现。


<details>
  <summary>Details</summary>
Motivation: 复杂材料系统的组合爆炸和化学计量空间巨大，导致传统晶体结构预测方法计算不可行。

Method: 开发了一种灵活且自动化的工作流，构建通用且数据高效的MLP，并在Mg-Ca-H三元和Be-P-N-O四元系统中验证。

Result: 工作流显著加速了高通量结构优化，并高效识别出有前景的化合物。

Conclusion: 该方法有效探索了复杂材料系统，加速了多组分新材料的发现。

Abstract: Understanding multicomponent complex material systems is essential for design
of advanced materials for a wide range of technological applications. While
state-of-the-art crystal structure prediction (CSP) methods effectively
identify new structures and assess phase stability, they face fundamental
limitations when applied to complex systems. This challenge stems from the
combinatorial explosion of atomic configurations and the vast stoichiometric
space, both of which contribute to computational demands that rapidly exceed
practical feasibility. In this work, we propose a flexible and automated
workflow to build a highly generalizable and data-efficient machine learning
potential (MLP), effectively unlocking the full potential of CSP algorithms.
The workflow is validated on both Mg-Ca-H ternary and Be-P-N-O quaternary
systems, demonstrating substantial machine learning acceleration in
high-throughput structural optimization and enabling the efficient
identification of promising compounds. These results underscore the
effectiveness of our approach in exploring complex material systems and
accelerating the discovery of new multicomponent materials.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [247] [OMGM: Orchestrate Multiple Granularities and Modalities for Efficient Multimodal Retrieval](https://arxiv.org/abs/2505.07879)
*Wei Yang,Jingjing Fu,Rui Wang,Jinyu Wang,Lei Song,Jiang Bian*

Main category: cs.IR

TL;DR: 提出了一种多模态检索增强生成系统，通过粗到细的多步检索提升知识库视觉问答效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法未充分利用多模态和知识粒度交互的问题，提升KB-VQA系统的检索和回答性能。

Method: 采用粗到细的多步检索策略，包括初始粗粒度跨模态检索、多模态融合重排序和文本重排序。

Result: 在InfoSeek和Encyclopedic-VQA基准测试中取得最优检索性能和竞争性回答结果。

Conclusion: 该方法有效提升了KB-VQA系统的性能，展示了多模态检索增强生成的潜力。

Abstract: Vision-language retrieval-augmented generation (RAG) has become an effective
approach for tackling Knowledge-Based Visual Question Answering (KB-VQA), which
requires external knowledge beyond the visual content presented in images. The
effectiveness of Vision-language RAG systems hinges on multimodal retrieval,
which is inherently challenging due to the diverse modalities and knowledge
granularities in both queries and knowledge bases. Existing methods have not
fully tapped into the potential interplay between these elements. We propose a
multimodal RAG system featuring a coarse-to-fine, multi-step retrieval that
harmonizes multiple granularities and modalities to enhance efficacy. Our
system begins with a broad initial search aligning knowledge granularity for
cross-modal retrieval, followed by a multimodal fusion reranking to capture the
nuanced multimodal information for top entity selection. A text reranker then
filters out the most relevant fine-grained section for augmented generation.
Extensive experiments on the InfoSeek and Encyclopedic-VQA benchmarks show our
method achieves state-of-the-art retrieval performance and highly competitive
answering results, underscoring its effectiveness in advancing KB-VQA systems.

</details>


### [248] [Efficient and Reproducible Biomedical Question Answering using Retrieval Augmented Generation](https://arxiv.org/abs/2505.07917)
*Linus Stuhlmann,Michael Alexander Saxer,Jonathan Fürst*

Main category: cs.IR

TL;DR: 研究评估了生物医学问答系统中的检索增强生成（RAG）方法，比较了不同检索策略和响应时间，发现BM25结合MedCPT在准确性和效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 生物医学问答系统需要高效的检索和生成组件以确保准确性、效率和可扩展性。

Method: 评估了BM25、BioBERT、MedCPT等检索方法及Elasticsearch、MongoDB等数据存储，并在PubMed子集和全集中测试性能。

Result: BM25检索50篇文档后用MedCPT重排，平衡了准确性（0.90）、召回率（0.90）和响应时间（1.91s）。

Conclusion: 研究揭示了检索深度、效率和可扩展性的权衡，系统开源且可扩展。

Abstract: Biomedical question-answering (QA) systems require effective retrieval and
generation components to ensure accuracy, efficiency, and scalability. This
study systematically examines a Retrieval-Augmented Generation (RAG) system for
biomedical QA, evaluating retrieval strategies and response time trade-offs. We
first assess state-of-the-art retrieval methods, including BM25, BioBERT,
MedCPT, and a hybrid approach, alongside common data stores such as
Elasticsearch, MongoDB, and FAISS, on a ~10% subset of PubMed (2.4M documents)
to measure indexing efficiency, retrieval latency, and retriever performance in
the end-to-end RAG system. Based on these insights, we deploy the final RAG
system on the full 24M PubMed corpus, comparing different retrievers' impact on
overall performance. Evaluations of the retrieval depth show that retrieving 50
documents with BM25 before reranking with MedCPT optimally balances accuracy
(0.90), recall (0.90), and response time (1.91s). BM25 retrieval time remains
stable (82ms), while MedCPT incurs the main computational cost. These results
highlight previously not well-known trade-offs in retrieval depth, efficiency,
and scalability for biomedical QA. With open-source code, the system is fully
reproducible and extensible.

</details>


### [249] [Hyperbolic Contrastive Learning with Model-augmentation for Knowledge-aware Recommendation](https://arxiv.org/abs/2505.08157)
*Shengyin Sun,Chen Ma*

Main category: cs.IR

TL;DR: 论文提出了一种基于双曲对比学习和模型增强的知识感知推荐方法，解决了现有方法在捕捉层次结构和避免偏好偏移方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比学习的GNN方法难以有效捕捉用户-项目二分图和知识图的层次结构，且通过扰动图结构生成正样本可能导致偏好偏移。

Method: 设计了Lorentzian知识聚合机制以捕捉层次结构，并提出三种模型级增强技术辅助双曲对比学习，避免偏好偏移。

Result: 实验表明，所提方法在性能上显著优于现有基线（最大提升11.03%）。

Conclusion: 通过结合双曲对比学习和模型增强，有效提升了知识感知推荐的性能，同时避免了偏好偏移问题。

Abstract: Benefiting from the effectiveness of graph neural networks (GNNs) and
contrastive learning, GNN-based contrastive learning has become mainstream for
knowledge-aware recommendation. However, most existing contrastive
learning-based methods have difficulties in effectively capturing the
underlying hierarchical structure within user-item bipartite graphs and
knowledge graphs. Moreover, they commonly generate positive samples for
contrastive learning by perturbing the graph structure, which may lead to a
shift in user preference learning. To overcome these limitations, we propose
hyperbolic contrastive learning with model-augmentation for knowledge-aware
recommendation. To capture the intrinsic hierarchical graph structures, we
first design a novel Lorentzian knowledge aggregation mechanism, which enables
more effective representations of users and items. Then, we propose three
model-level augmentation techniques to assist Hyperbolic contrastive learning.
Different from the classical structure-level augmentation (e.g., edge
dropping), the proposed model-augmentations can avoid preference shifts between
the augmented positive pair. Finally, we conduct extensive experiments to
demonstrate the superiority (maximum improvement of $11.03\%$) of proposed
methods over existing baselines.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [250] [Understanding molecular ratios in the carbon and oxygen poor outer Milky Way with interpretable machine learning](https://arxiv.org/abs/2505.08410)
*Gijs Vermariën,Serena Viti,Johannes Heyl,Francesco Fontani*

Main category: astro-ph.GA

TL;DR: 论文研究了银河系外围低金属丰度区域的分子线比例，利用可解释机器学习（如SHAP和UMAP）分析了温度和密度对碳氧化学的影响。


<details>
  <summary>Details</summary>
Motivation: 理解低金属丰度区域的分子线比例如何反映其物理和化学特性，尤其是碳和氧的初始丰度。

Method: 使用UCLCHEM生成大量天体化学模型，结合经典分析和可解释机器学习（SHAP和UMAP）研究分子线比例。

Result: 温度和密度是最重要特征，但碳氧丰度在部分参数空间中也关键；CN/HCN和HNC/HCN对初始碳丰度敏感，CS/SO对氧丰度敏感。

Conclusion: 所选分子线比例主要对初始碳丰度、温度和密度敏感，CN/HCN和HNC/HCN是探测碳丰度的理想工具，CS/SO对氧丰度敏感。

Abstract: Context. The outer Milky Way has a lower metallicity than our solar
neighbourhood, but still many molecules are detected in the region. Molecular
line ratios can serve as probes to better understand the chemistry and physics
in these regions. Aims. We use interpretable machine learning to study 9
different molecular ratios, helping us understand the forward connection
between the physics of these environments and the carbon and oxygen
chemistries. Methods. Using a large grid of astrochemical models generated
using UCLCHEM, we study the properties of molecular clouds of low oxygen and
carbon initial abundance. We first try to understand the line ratios using a
classical analysis. We then move on to using interpretable machine learning,
namely Shapley Additive Explanations (SHAP), to understand the higher order
dependencies of the ratios over the entire parameter grid. Lastly we use the
Uniform Manifold Approximation and Projection technique (UMAP) as a reduction
method to create intuitive groupings of models. Results. We find that the
parameter space is well covered by the line ratios, allowing us to investigate
all input parameters. SHAP analysis shows that the temperature and density are
the most important features, but the carbon and oxygen abundances are important
in parts of the parameter space. Lastly, we find that we can group different
types of ratios using UMAP. Conclusions. We show the chosen ratios are mostly
sensitive to changes in the carbon initial abundance, together with the
temperature and density. Especially the CN/HCN and HNC/HCN ratio are shown to
be sensitive to the initial carbon abundance, making them excellent probes for
this parameter. Out of the ratios, only CS/SO shows a sensitivity to the oxygen
abundance.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [251] [Safety and optimality in learning-based control at low computational cost](https://arxiv.org/abs/2505.08026)
*Dominik Baumann,Krzysztof Kowalczyk,Cristian R. Rojas,Koen Tiels,Pawel Wachel*

Main category: eess.SY

TL;DR: CoLSafe是一种计算轻量的安全学习算法，适用于大规模数据和低算力设备。


<details>
  <summary>Details</summary>
Motivation: 为物理系统提供安全保证的机器学习方法通常计算成本高，难以应用于大规模数据或嵌入式设备。

Method: 提出CoLSafe算法，其计算复杂度随数据点数亚线性增长。

Result: 在七自由度机械臂上验证了算法的有效性，并提供了安全和最优性保证。

Conclusion: CoLSafe是一种高效且安全的学习算法，适用于实际应用。

Abstract: Applying machine learning methods to physical systems that are supposed to
act in the real world requires providing safety guarantees. However, methods
that include such guarantees often come at a high computational cost, making
them inapplicable to large datasets and embedded devices with low computational
power. In this paper, we propose CoLSafe, a computationally lightweight safe
learning algorithm whose computational complexity grows sublinearly with the
number of data points. We derive both safety and optimality guarantees and
showcase the effectiveness of our algorithm on a seven-degrees-of-freedom robot
arm.

</details>


### [252] [Diffusion-assisted Model Predictive Control Optimization for Power System Real-Time Operation](https://arxiv.org/abs/2505.08535)
*Linna Xu,Yongli Zhu*

Main category: eess.SY

TL;DR: 提出了一种改进的模型预测控制（MPC）框架，结合扩散模型提升负荷预测精度，适用于可再生能源主导的电力系统。


<details>
  <summary>Details</summary>
Motivation: 解决可再生能源电力系统中缺乏明确状态转移规律的问题，提升负荷预测精度。

Method: 利用扩散模型生成时间序列数据增强训练集，并通过模型识别程序推导系统动态。

Result: 在工业园系统和IEEE 30总线系统上的案例研究表明，扩散模型显著提高了负荷预测精度，且推断的系统动态适用于实时电网操作。

Conclusion: 改进的MPC框架有效解决了可再生能源电力系统中的预测和控制问题。

Abstract: This paper presents a modified model predictive control (MPC) framework for
real-time power system operation. The framework incorporates a diffusion model
tailored for time series generation to enhance the accuracy of the load
forecasting module used in the system operation. In the absence of explicit
state transition law, a model-identification procedure is leveraged to derive
the system dynamics, thereby eliminating a barrier when applying MPC to a
renewables-dominated power system. Case study results on an industry park
system and the IEEE 30-bus system demonstrate that using the diffusion model to
augment the training dataset significantly improves load-forecasting accuracy,
and the inferred system dynamics are applicable to the real-time grid operation
with solar and wind.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [253] [Building-Block Aware Generative Modeling for 3D Crystals of Metal Organic Frameworks](https://arxiv.org/abs/2505.08531)
*Chenru Duan,Aditya Nandy,Sizhan Liu,Yuanqi Du,Liu He,Yi Qu,Haojun Jia,Jin-Hu Dou*

Main category: physics.chem-ph

TL;DR: BBA MOF Diffusion是一种生成模型，用于设计具有大单元细胞的金属有机框架（MOFs），扩展了化学空间。


<details>
  <summary>Details</summary>
Motivation: MOFs的设计空间巨大，传统方法难以覆盖，需要生成模型来高效探索。

Method: 采用SE(3)-equivariant扩散模型，学习3D全原子表示，并明确编码晶体拓扑网络。

Result: 模型能生成包含1000个原子的MOFs，具有高几何有效性、新颖性和多样性，并成功合成了一种预测的MOF。

Conclusion: BBA MOF Diffusion为合成高性能MOFs提供了实用途径。

Abstract: Metal-organic frameworks (MOFs) marry inorganic nodes, organic edges, and
topological nets into programmable porous crystals, yet their astronomical
design space defies brute-force synthesis. Generative modeling holds ultimate
promise, but existing models either recycle known building blocks or are
restricted to small unit cells. We introduce Building-Block-Aware MOF Diffusion
(BBA MOF Diffusion), an SE(3)-equivariant diffusion model that learns 3D
all-atom representations of individual building blocks, encoding
crystallographic topological nets explicitly. Trained on the CoRE-MOF database,
BBA MOF Diffusion readily samples MOFs with unit cells containing 1000 atoms
with great geometric validity, novelty, and diversity mirroring experimental
databases. Its native building-block representation produces unprecedented
metal nodes and organic edges, expanding accessible chemical space by orders of
magnitude. One high-scoring [Zn(1,4-TDC)(EtOH)2] MOF predicted by the model was
synthesized, where powder X-ray diffraction, thermogravimetric analysis, and N2
sorption confirm its structural fidelity. BBA-Diff thus furnishes a practical
pathway to synthesizable and high-performing MOFs.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [254] [Probabilistic approach to longitudinal response prediction: application to radiomics from brain cancer imaging](https://arxiv.org/abs/2505.07973)
*Isabella Cama,Michele Piana,Cristina Campi,Sara Garbarino*

Main category: stat.AP

TL;DR: 该研究提出了一种概率模型，用于纵向预测疾病进展，整合基线特征和中期随访数据，处理预测中的不确定性，并在合成场景和脑癌数据集中验证其竞争力。


<details>
  <summary>Details</summary>
Motivation: 纵向影像分析能够动态追踪疾病进展和治疗效果，但现有方法在不确定性和数据维度增长方面存在局限。

Method: 提出概率模型，整合基线特征和中期随访数据，处理预测不确定性，并控制问题维度的增长。

Result: 在合成场景和脑癌数据集中，模型表现与现有方法相当，同时独特地处理了不确定性和数据维度问题。

Conclusion: 该概率模型在纵向预测中具有竞争力，无需依赖中期随访数据，为疾病进展研究提供了新工具。

Abstract: Longitudinal imaging analysis tracks disease progression and treatment
response over time, providing dynamic insights into treatment efficacy and
disease evolution. Radiomic features extracted from medical imaging can support
the study of disease progression and facilitate longitudinal prediction of
clinical outcomes. This study presents a probabilistic model for longitudinal
response prediction, integrating baseline features with intermediate
follow-ups. The probabilistic nature of the model naturally allows to handle
the instrinsic uncertainty of the longitudinal prediction of disease
progression. We evaluate the proposed model against state-of-the-art disease
progression models in both a synthetic scenario and using a brain cancer
dataset. Results demonstrate that the approach is competitive against existing
methods while uniquely accounting for uncertainty and controlling the growth of
problem dimensionality, eliminating the need for data from intermediate
follow-ups.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [255] [PRISM: Complete Online Decentralized Multi-Agent Pathfinding with Rapid Information Sharing using Motion Constraints](https://arxiv.org/abs/2505.08025)
*Hannah Lee,Zachary Serlin,James Motes,Brendan Long,Marco Morales,Nancy M. Amato*

Main category: cs.RO

TL;DR: PRISM是一种去中心化算法，用于解决多任务多智能体路径规划问题，通过快速信息共享避免碰撞，并证明能解决所有可能的死锁场景。


<details>
  <summary>Details</summary>
Motivation: 解决多任务多智能体路径规划中的协作和死锁问题，提升效率和可扩展性。

Method: 采用快速通信策略，通过信息包交换运动约束信息，支持无直接通信场景。

Result: 在5种环境和25个随机场景中，PRISM支持3.4倍于CBS的智能体数量，处理2.5倍于TPTS的任务，且计算速度更快。

Conclusion: PRISM在复杂动态环境中表现出鲁棒性、可扩展性和高效性。

Abstract: We introduce PRISM (Pathfinding with Rapid Information Sharing using Motion
Constraints), a decentralized algorithm designed to address the multi-task
multi-agent pathfinding (MT-MAPF) problem. PRISM enables large teams of agents
to concurrently plan safe and efficient paths for multiple tasks while avoiding
collisions. It employs a rapid communication strategy that uses information
packets to exchange motion constraint information, enhancing cooperative
pathfinding and situational awareness, even in scenarios without direct
communication. We prove that PRISM resolves and avoids all deadlock scenarios
when possible, a critical challenge in decentralized pathfinding. Empirically,
we evaluate PRISM across five environments and 25 random scenarios,
benchmarking it against the centralized Conflict-Based Search (CBS) and the
decentralized Token Passing with Task Swaps (TPTS) algorithms. PRISM
demonstrates scalability and solution quality, supporting 3.4 times more agents
than CBS and handling up to 2.5 times more tasks in narrow passage environments
than TPTS. Additionally, PRISM matches CBS in solution quality while achieving
faster computation times, even under low-connectivity conditions. Its
decentralized design reduces the computational burden on individual agents,
making it scalable for large environments. These results confirm PRISM's
robustness, scalability, and effectiveness in complex and dynamic pathfinding
scenarios.

</details>


### [256] [What Matters for Batch Online Reinforcement Learning in Robotics?](https://arxiv.org/abs/2505.08078)
*Perry Dong,Suvir Mirchandani,Dorsa Sadigh,Chelsea Finn*

Main category: cs.RO

TL;DR: 论文研究了批量在线强化学习（batch online RL）在机器人学习中的有效性，提出了改进性能的三个关键因素，并设计了一种通用方法。


<details>
  <summary>Details</summary>
Motivation: 批量在线RL有望减少人工数据收集需求并实现自我改进，但现有方法难以高效学习。

Method: 系统研究了算法类别、策略提取方法和策略表达能力三个因素，提出了一种基于Q函数和隐式策略提取的通用方法。

Result: 使用Q函数和隐式策略提取显著优于模仿学习方法，且高表达能力策略更优。添加时间相关噪声进一步提升了性能。

Conclusion: 论文提出的方法在性能和扩展性上显著优于现有方法，为批量在线RL提供了有效解决方案。

Abstract: The ability to learn from large batches of autonomously collected data for
policy improvement -- a paradigm we refer to as batch online reinforcement
learning -- holds the promise of enabling truly scalable robot learning by
significantly reducing the need for human effort of data collection while
getting benefits from self-improvement. Yet, despite the promise of this
paradigm, it remains challenging to achieve due to algorithms not being able to
learn effectively from the autonomous data. For example, prior works have
applied imitation learning and filtered imitation learning methods to the batch
online RL problem, but these algorithms often fail to efficiently improve from
the autonomously collected data or converge quickly to a suboptimal point. This
raises the question of what matters for effective batch online RL in robotics.
Motivated by this question, we perform a systematic empirical study of three
axes -- (i) algorithm class, (ii) policy extraction methods, and (iii) policy
expressivity -- and analyze how these axes affect performance and scaling with
the amount of autonomous data. Through our analysis, we make several
observations. First, we observe that the use of Q-functions to guide batch
online RL significantly improves performance over imitation-based methods.
Building on this, we show that an implicit method of policy extraction -- via
choosing the best action in the distribution of the policy -- is necessary over
traditional policy extraction methods from offline RL. Next, we show that an
expressive policy class is preferred over less expressive policy classes. Based
on this analysis, we propose a general recipe for effective batch online RL. We
then show a simple addition to the recipe of using temporally-correlated noise
to obtain more diversity results in further performance gains. Our recipe
obtains significantly better performance and scaling compared to prior methods.

</details>


### [257] [UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations](https://arxiv.org/abs/2505.08787)
*Hanjung Kim,Jaehyun Kang,Hyolim Kang,Meedeum Cho,Seon Joo Kim,Youngwoon Lee*

Main category: cs.RO

TL;DR: UniSkill框架通过无标签的大规模跨体现视频数据学习技能表示，实现从人类视频提示到机器人策略的有效迁移。


<details>
  <summary>Details</summary>
Motivation: 模仿是人类学习新任务的基本机制，但机器人因与人类在视觉和物理能力上的差异而难以直接应用。现有方法依赖对齐数据，但大规模收集困难。

Method: 提出UniSkill框架，从无标签的跨体现视频数据中学习技能表示，无需对齐数据。

Result: 实验表明，UniSkill能有效指导机器人选择合适动作，即使面对未见过的视频提示。

Conclusion: UniSkill为跨体现技能迁移提供了一种无监督解决方案，具有实际应用潜力。

Abstract: Mimicry is a fundamental learning mechanism in humans, enabling individuals
to learn new tasks by observing and imitating experts. However, applying this
ability to robots presents significant challenges due to the inherent
differences between human and robot embodiments in both their visual appearance
and physical capabilities. While previous methods bridge this gap using
cross-embodiment datasets with shared scenes and tasks, collecting such aligned
data between humans and robots at scale is not trivial. In this paper, we
propose UniSkill, a novel framework that learns embodiment-agnostic skill
representations from large-scale cross-embodiment video data without any
labels, enabling skills extracted from human video prompts to effectively
transfer to robot policies trained only on robot data. Our experiments in both
simulation and real-world environments show that our cross-embodiment skills
successfully guide robots in selecting appropriate actions, even with unseen
video prompts. The project website can be found at:
https://kimhanjung.github.io/UniSkill.

</details>


### [258] [Scaling Multi Agent Reinforcement Learning for Underwater Acoustic Tracking via Autonomous Vehicles](https://arxiv.org/abs/2505.08222)
*Matteo Gallici,Ivan Masmitja,Mario Martín*

Main category: cs.RO

TL;DR: 论文提出了一种迭代蒸馏方法，将高保真模拟转移到简化的GPU加速环境中，显著提升多智能体强化学习（MARL）的训练效率，并结合Transformer架构（TransfMAPPO）实现高效的多目标跟踪。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）在复杂海洋环境中训练自动驾驶车辆（AV）时面临计算效率低下的问题，尤其是在多车辆场景中。现有高保真模拟器无法显著加速多车辆训练，限制了MARL的实际应用。

Method: 提出迭代蒸馏方法，将高保真模拟转移到简化的GPU加速环境，并结合Transformer架构（TransfMAPPO）学习多智能体策略。通过大规模课程学习在GPU上完成训练。

Result: 方法实现了高达30,000倍的加速，跟踪误差保持在5米以下，即使在多快速移动目标的情况下。

Conclusion: 该研究填补了大规模MARL训练与高保真部署之间的差距，为实际海洋任务中的自动驾驶车队控制提供了可扩展的框架。

Abstract: Autonomous vehicles (AV) offer a cost-effective solution for scientific
missions such as underwater tracking. Recently, reinforcement learning (RL) has
emerged as a powerful method for controlling AVs in complex marine
environments. However, scaling these techniques to a fleet--essential for
multi-target tracking or targets with rapid, unpredictable motion--presents
significant computational challenges. Multi-Agent Reinforcement Learning (MARL)
is notoriously sample-inefficient, and while high-fidelity simulators like
Gazebo's LRAUV provide 100x faster-than-real-time single-robot simulations,
they offer no significant speedup for multi-vehicle scenarios, making MARL
training impractical. To address these limitations, we propose an iterative
distillation method that transfers high-fidelity simulations into a simplified,
GPU-accelerated environment while preserving high-level dynamics. This approach
achieves up to a 30,000x speedup over Gazebo through parallelization, enabling
efficient training via end-to-end GPU acceleration. Additionally, we introduce
a novel Transformer-based architecture (TransfMAPPO) that learns multi-agent
policies invariant to the number of agents and targets, significantly improving
sample efficiency. Following large-scale curriculum learning conducted entirely
on GPU, we perform extensive evaluations in Gazebo, demonstrating that our
method maintains tracking errors below 5 meters over extended durations, even
in the presence of multiple fast-moving targets. This work bridges the gap
between large-scale MARL training and high-fidelity deployment, providing a
scalable framework for autonomous fleet control in real-world sea missions.

</details>


### [259] [Adaptive Diffusion Policy Optimization for Robotic Manipulation](https://arxiv.org/abs/2505.08376)
*Huiyun Jiang,Zhuang Yang*

Main category: cs.RO

TL;DR: ADPO是一种基于Adam的扩散策略优化方法，用于快速稳定地微调扩散策略，在机器人控制任务中表现优于其他扩散强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在强化学习中表现出潜力，但缺乏快速稳定优化扩散策略的研究。

Method: 提出ADPO框架，利用自适应梯度下降方法微调扩散策略，并在标准机器人任务中进行实验验证。

Result: ADPO在实验中表现优于或与基线方法相当，并分析了超参数敏感性。

Conclusion: ADPO为扩散策略优化提供了有效方法，并为实际应用提供了指导。

Abstract: Recent studies have shown the great potential of diffusion models in
improving reinforcement learning (RL) by modeling complex policies, expressing
a high degree of multi-modality, and efficiently handling high-dimensional
continuous control tasks. However, there is currently limited research on how
to optimize diffusion-based polices (e.g., Diffusion Policy) fast and stably.
In this paper, we propose an Adam-based Diffusion Policy Optimization (ADPO), a
fast algorithmic framework containing best practices for fine-tuning
diffusion-based polices in robotic control tasks using the adaptive gradient
descent method in RL. Adaptive gradient method is less studied in training RL,
let alone diffusion-based policies. We confirm that ADPO outperforms other
diffusion-based RL methods in terms of overall effectiveness for fine-tuning on
standard robotic tasks. Concretely, we conduct extensive experiments on
standard robotic control tasks to test ADPO, where, particularly, six popular
diffusion-based RL methods are provided as benchmark methods. Experimental
results show that ADPO acquires better or comparable performance than the
baseline methods. Finally, we systematically analyze the sensitivity of
multiple hyperparameters in standard robotics tasks, providing guidance for
subsequent practical applications. Our video demonstrations are released in
https://github.com/Timeless-lab/ADPO.git.

</details>


### [260] [Reinforcement Learning-based Fault-Tolerant Control for Quadrotor with Online Transformer Adaptation](https://arxiv.org/abs/2505.08223)
*Dohyun Kim,Jayden Dongwoo Lee,Hyochoong Bang,Jungho Bae*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习和Transformer的混合框架，用于多旋翼飞行器的容错控制，无需重新训练即可适应新配置。


<details>
  <summary>Details</summary>
Motivation: 多旋翼飞行器在应用中易受执行器故障影响，现有方法需先验知识或难以适应新配置。

Method: 结合强化学习和Transformer架构，实时推断潜在表示，适应未训练的系统模型。

Result: 在仿真中成功率达到95%，位置误差0.129米，优于现有方法。

Conclusion: 该框架提升了多旋翼飞行器的适应性和可靠性，适用于动态不确定环境。

Abstract: Multirotors play a significant role in diverse field robotics applications
but remain highly susceptible to actuator failures, leading to rapid
instability and compromised mission reliability. While various fault-tolerant
control (FTC) strategies using reinforcement learning (RL) have been widely
explored, most previous approaches require prior knowledge of the multirotor
model or struggle to adapt to new configurations. To address these limitations,
we propose a novel hybrid RL-based FTC framework integrated with a
transformer-based online adaptation module. Our framework leverages a
transformer architecture to infer latent representations in real time, enabling
adaptation to previously unseen system models without retraining. We evaluate
our method in a PyBullet simulation under loss-of-effectiveness actuator
faults, achieving a 95% success rate and a positional root mean square error
(RMSE) of 0.129 m, outperforming existing adaptation methods with 86% success
and an RMSE of 0.153 m. Further evaluations on quadrotors with varying
configurations confirm the robustness of our framework across untrained
dynamics. These results demonstrate the potential of our framework to enhance
the adaptability and reliability of multirotors, enabling efficient fault
management in dynamic and uncertain environments. Website is available at
http://00dhkim.me/paper/rl-ftc

</details>


### [261] [Continuous World Coverage Path Planning for Fixed-Wing UAVs using Deep Reinforcement Learning](https://arxiv.org/abs/2505.08382)
*Mirco Theile,Andres R. Zapata Rodriguez,Marco Caccamo,Alberto L. Sangiovanni-Vincentelli*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的无人机连续覆盖路径规划方法，通过可变大小矩形建模环境和曲率约束Bézier曲线规划运动，优化能耗。


<details>
  <summary>Details</summary>
Motivation: 传统离散网格方法无法满足无人机实际连续运动需求，需开发更高效的连续路径规划方法。

Method: 使用可变大小轴对齐矩形建模环境，结合曲率约束Bézier曲线规划运动，采用基于动作映射的AM-SAC强化学习算法。

Result: 实验表明，该方法能有效学习节能覆盖策略，适用于生成和手工设计的场景。

Conclusion: 提出的连续覆盖路径规划方法在能耗优化和覆盖完整性上表现优越。

Abstract: Unmanned Aerial Vehicle (UAV) Coverage Path Planning (CPP) is critical for
applications such as precision agriculture and search and rescue. While
traditional methods rely on discrete grid-based representations, real-world UAV
operations require power-efficient continuous motion planning. We formulate the
UAV CPP problem in a continuous environment, minimizing power consumption while
ensuring complete coverage. Our approach models the environment with
variable-size axis-aligned rectangles and UAV motion with curvature-constrained
B\'ezier curves. We train a reinforcement learning agent using an
action-mapping-based Soft Actor-Critic (AM-SAC) algorithm employing a
self-adaptive curriculum. Experiments on both procedurally generated and
hand-crafted scenarios demonstrate the effectiveness of our method in learning
energy-efficient coverage strategies.

</details>


### [262] [Parameter Estimation using Reinforcement Learning Causal Curiosity: Limits and Challenges](https://arxiv.org/abs/2505.08453)
*Miguel Arana-Catania,Weisi Guo*

Main category: cs.RO

TL;DR: 本文分析了强化学习方法Causal Curiosity，评估其在无需直接测量的情况下估计系统动态因果因素的能力，并提出了改进设计建议。


<details>
  <summary>Details</summary>
Motivation: 因果理解在科学与工程中至关重要，尤其是在优化复杂系统或探索未知环境时。本文旨在评估Causal Curiosity方法的测量准确性及其潜力与局限。

Method: 通过分析Causal Curiosity在机器人操纵器中的应用，研究其测量准确性、敏感性及混杂因素分离能力。

Result: 揭示了该方法的当前局限性和未来潜力，并提出了改进设计以提高其在现实复杂场景中的应用效率。

Conclusion: Causal Curiosity方法具有潜力，但需进一步优化以提高测量准确性和实用性。

Abstract: Causal understanding is important in many disciplines of science and
engineering, where we seek to understand how different factors in the system
causally affect an experiment or situation and pave a pathway towards creating
effective or optimising existing models. Examples of use cases are autonomous
exploration and modelling of unknown environments or assessing key variables in
optimising large complex systems. In this paper, we analyse a Reinforcement
Learning approach called Causal Curiosity, which aims to estimate as accurately
and efficiently as possible, without directly measuring them, the value of
factors that causally determine the dynamics of a system. Whilst the idea
presents a pathway forward, measurement accuracy is the foundation of
methodology effectiveness. Focusing on the current causal curiosity's robotic
manipulator, we present for the first time a measurement accuracy analysis of
the future potentials and current limitations of this technique and an analysis
of its sensitivity and confounding factor disentanglement capability - crucial
for causal analysis. As a result of our work, we promote proposals for an
improved and efficient design of Causal Curiosity methods to be applied to
real-world complex scenarios.

</details>


### [263] [Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning](https://arxiv.org/abs/2505.08264)
*Ahmed Abouelazm,Tim Weinstein,Tim Joseph,Philip Schörner,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 提出了一种自动课程学习框架，通过动态生成适应智能体能力的驾驶场景，提升强化学习在自动驾驶中的训练效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在固定场景中训练自动驾驶智能体，泛化能力有限；领域随机化方法效率低下且策略次优。

Method: 设计了一个自动课程学习框架，由‘教师’根据智能体当前策略动态生成和调整驾驶场景复杂度。

Result: 相比基线方法，该方法在低/高交通密度下分别提升9%和21%的成功率，并加快收敛速度。

Conclusion: 自动课程学习能显著提升基于强化学习的自动驾驶智能体的鲁棒性和效率。

Abstract: This paper addresses the challenges of training end-to-end autonomous driving
agents using Reinforcement Learning (RL). RL agents are typically trained in a
fixed set of scenarios and nominal behavior of surrounding road users in
simulations, limiting their generalization and real-life deployment. While
domain randomization offers a potential solution by randomly sampling driving
scenarios, it frequently results in inefficient training and sub-optimal
policies due to the high variance among training scenarios. To address these
limitations, we propose an automatic curriculum learning framework that
dynamically generates driving scenarios with adaptive complexity based on the
agent's evolving capabilities. Unlike manually designed curricula that
introduce expert bias and lack scalability, our framework incorporates a
``teacher'' that automatically generates and mutates driving scenarios based on
their learning potential -- an agent-centric metric derived from the agent's
current policy -- eliminating the need for expert design. The framework
enhances training efficiency by excluding scenarios the agent has mastered or
finds too challenging. We evaluate our framework in a reinforcement learning
setting where the agent learns a driving policy from camera images. Comparative
results against baseline methods, including fixed scenario training and domain
randomization, demonstrate that our approach leads to enhanced generalization,
achieving higher success rates: +9\% in low traffic density, +21\% in high
traffic density, and faster convergence with fewer training steps. Our findings
highlight the potential of ACL in improving the robustness and efficiency of
RL-based autonomous driving agents.

</details>


### [264] [From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation](https://arxiv.org/abs/2505.08548)
*Yifu Yuan,Haiqin Cui,Yibin Chen,Zibin Dong,Fei Ni,Longxin Kou,Jinyi Liu,Pengyi Li,Yan Zheng,Jianye Hao*

Main category: cs.RO

TL;DR: FSD模型通过空间关系推理生成中间表示，显著提升了机器人操作的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉-语言-动作模型在零样本性能上的不足，特别是在异构数据集稀缺的情况下。

Method: 提出FSD模型，结合分层数据管道和自一致性机制，对齐空间坐标与视觉信号。

Result: 在8个基准测试和VABench上表现优异，零样本机器人操作成功率显著提升。

Conclusion: FSD在空间推理和机器人操作任务中表现出色，为通用化机器人操作提供了新思路。

Abstract: Achieving generalization in robotic manipulation remains a critical
challenge, particularly for unseen scenarios and novel tasks. Current
Vision-Language-Action (VLA) models, while building on top of general
Vision-Language Models (VLMs), still fall short of achieving robust zero-shot
performance due to the scarcity and heterogeneity prevalent in embodied
datasets. To address these limitations, we propose FSD (From Seeing to Doing),
a novel vision-language model that generates intermediate representations
through spatial relationship reasoning, providing fine-grained guidance for
robotic manipulation. Our approach combines a hierarchical data pipeline for
training with a self-consistency mechanism that aligns spatial coordinates with
visual signals. Through extensive experiments, we comprehensively validated
FSD's capabilities in both "seeing" and "doing," achieving outstanding
performance across 8 benchmarks for general spatial reasoning and embodied
reference abilities, as well as on our proposed more challenging benchmark
VABench. We also verified zero-shot capabilities in robot manipulation,
demonstrating significant performance improvements over baseline methods in
both SimplerEnv and real robot settings. Experimental results show that FSD
achieves 54.1% success rate in SimplerEnv and 72% success rate across 8
real-world tasks, outperforming the strongest baseline by 30%.

</details>


### [265] [A Comparative Study of Human Activity Recognition: Motion, Tactile, and multi-modal Approaches](https://arxiv.org/abs/2505.08657)
*Valerio Belcamino,Nhat Minh Dinh Le,Quan Khanh Luu,Alessandro Carfì,Van Anh Ho,Fulvio Mastrogiovanni*

Main category: cs.RO

TL;DR: 该研究评估了基于视觉的触觉传感器在识别15种人类活动中的表现，并与基于IMU的数据手套进行比较，提出了一种结合触觉和运动数据的多模态框架。多模态方法在性能上优于单模态方法。


<details>
  <summary>Details</summary>
Motivation: 提升人机协作（HRC）中的人类活动识别（HAR）能力，通过结合触觉和运动数据的互补优势。

Method: 比较了三种方法：基于运动的分类（MBC）、基于触觉的分类（TBC）以及多模态分类（MMC）。离线验证用于评估准确性，在线验证测试连续动作序列的性能。

Result: 多模态方法在性能上显著优于单模态方法。

Conclusion: 结合触觉和运动传感可以显著提升HAR系统的性能，为人机协作提供更高效的解决方案。

Abstract: Human activity recognition (HAR) is essential for effective Human-Robot
Collaboration (HRC), enabling robots to interpret and respond to human actions.
This study evaluates the ability of a vision-based tactile sensor to classify
15 activities, comparing its performance to an IMU-based data glove.
Additionally, we propose a multi-modal framework combining tactile and motion
data to leverage their complementary strengths. We examined three approaches:
motion-based classification (MBC) using IMU data, tactile-based classification
(TBC) with single or dual video streams, and multi-modal classification (MMC)
integrating both. Offline validation on segmented datasets assessed each
configuration's accuracy under controlled conditions, while online validation
on continuous action sequences tested online performance. Results showed the
multi-modal approach consistently outperformed single-modality methods,
highlighting the potential of integrating tactile and motion sensing to enhance
HAR systems for collaborative robotics.

</details>


### [266] [A Social Robot with Inner Speech for Dietary Guidance](https://arxiv.org/abs/2505.08664)
*Valerio Belcamino,Alessandro Carfì,Valeria Seidita,Fulvio Mastrogiovanni,Antonio Chella*

Main category: cs.RO

TL;DR: 研究探讨了如何利用内部语音增强社交机器人在饮食建议中的透明度和信任度，通过模拟人类思维过程提升机器人的解释能力。


<details>
  <summary>Details</summary>
Motivation: 在医疗场景中，信任机器人助手依赖于准确的建议和自然对话，内部语音可以提升机器人的透明度和交互自然性。

Method: 开发了一个具备内部语音功能的社交机器人，结合大语言模型和知识图谱，用于验证用户输入、优化推理并生成清晰解释。

Result: 通过计算效率测试和小规模用户研究，验证了内部语音在解释机器人行为方面的可靠性。

Conclusion: 内部语音显著提升了机器人的透明度和信任度，优化了人机交互体验。

Abstract: We explore the use of inner speech as a mechanism to enhance transparency and
trust in social robots for dietary advice. In humans, inner speech structures
thought processes and decision-making; in robotics, it improves explainability
by making reasoning explicit. This is crucial in healthcare scenarios, where
trust in robotic assistants depends on both accurate recommendations and
human-like dialogue, which make interactions more natural and engaging.
Building on this, we developed a social robot that provides dietary advice, and
we provided the architecture with inner speech capabilities to validate user
input, refine reasoning, and generate clear justifications. The system
integrates large language models for natural language understanding and a
knowledge graph for structured dietary information. By making decisions more
transparent, our approach strengthens trust and improves human-robot
interaction in healthcare. We validated this by measuring the computational
efficiency of our architecture and conducting a small user study, which
assessed the reliability of inner speech in explaining the robot's behavior.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [267] [Tensor Sketch: Fast and Scalable Polynomial Kernel Approximation](https://arxiv.org/abs/2505.08146)
*Ninh Pham,Rasmus Pagh*

Main category: cs.DS

TL;DR: Tensor Sketch是一种高效随机特征映射方法，用于近似多项式核，适用于高维和大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 解决非线性核在大规模数据集上的计算效率问题。

Method: 提出Tensor Sketch方法，计算低维嵌入，时间复杂度为O(n(d+D log D))。

Result: 提供了近似误差的理论保证，确保核函数估计的准确性。

Conclusion: Tensor Sketch是一种高效且可扩展的计算工具，适用于多种应用场景。

Abstract: Approximation of non-linear kernels using random feature maps has become a
powerful technique for scaling kernel methods to large datasets. We propose
\textit{Tensor Sketch}, an efficient random feature map for approximating
polynomial kernels. Given $n$ training samples in $\R^d$ Tensor Sketch computes
low-dimensional embeddings in $\R^D$ in time $\BO{n(d+D \log{D})}$ making it
well-suited for high-dimensional and large-scale settings. We provide
theoretical guarantees on the approximation error, ensuring the fidelity of the
resulting kernel function estimates. We also discuss extensions and highlight
applications where Tensor Sketch serves as a central computational tool.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [268] [Improving Unsupervised Task-driven Models of Ventral Visual Stream via Relative Position Predictivity](https://arxiv.org/abs/2505.08316)
*Dazhong Rong,Hao Dong,Xing Gao,Jiyu Wei,Di Hong,Yaoyao Hao,Qinming He,Yueming Wang*

Main category: cs.CE

TL;DR: 论文提出了一种结合相对位置预测和对比学习的新方法，以更全面地模拟腹侧视觉流（VVS）的功能，实验表明该方法在物体识别和脑相似性方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注物体识别，但VVS的功能可能更广泛，作者希望通过引入相对位置预测（RP）来更准确地模拟VVS。

Method: 提出了一种结合RP预测和对比学习的无监督任务驱动方法。

Result: 实验显示，新方法显著提升了物体识别性能，同时增强了RP预测能力，并提高了模型的脑相似性。

Conclusion: 研究从计算角度证明了VVS在位置感知（尤其是RP预测）中的重要作用。

Abstract: Based on the concept that ventral visual stream (VVS) mainly functions for
object recognition, current unsupervised task-driven methods model VVS by
contrastive learning, and have achieved good brain similarity. However, we
believe functions of VVS extend beyond just object recognition. In this paper,
we introduce an additional function involving VVS, named relative position (RP)
prediction. We first theoretically explain contrastive learning may be unable
to yield the model capability of RP prediction. Motivated by this, we
subsequently integrate RP learning with contrastive learning, and propose a new
unsupervised task-driven method to model VVS, which is more inline with
biological reality. We conduct extensive experiments, demonstrating that: (i)
our method significantly improves downstream performance of object recognition
while enhancing RP predictivity; (ii) RP predictivity generally improves the
model brain similarity. Our results provide strong evidence for the involvement
of VVS in location perception (especially RP prediction) from a computational
perspective.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [269] [A Survey of Deep Learning for Complex Speech Spectrograms](https://arxiv.org/abs/2505.08694)
*Yuying Xie,Zheng-Hua Tan*

Main category: eess.AS

TL;DR: 本文综述了深度学习在复杂频谱图处理中的最新技术，包括网络架构、训练策略和应用。


<details>
  <summary>Details</summary>
Motivation: 深度学习在语音信号处理中的快速发展，尤其是复杂频谱图的分析与处理，需要系统总结。

Method: 介绍了复杂频谱图及其特征，探讨了复数神经网络架构、训练策略和损失函数。

Result: 深度学习在相位恢复、语音增强和分离等应用中取得了显著进展。

Conclusion: 本文为语音信号处理和复数神经网络领域的研究者提供了有价值的参考。

Abstract: Recent advancements in deep learning have significantly impacted the field of
speech signal processing, particularly in the analysis and manipulation of
complex spectrograms. This survey provides a comprehensive overview of the
state-of-the-art techniques leveraging deep neural networks for processing
complex spectrograms, which encapsulate both magnitude and phase information.
We begin by introducing complex spectrograms and their associated features for
various speech processing tasks. Next, we explore the key components and
architectures of complex-valued neural networks, which are specifically
designed to handle complex-valued data and have been applied for complex
spectrogram processing. We then discuss various training strategies and loss
functions tailored for training neural networks to process and model complex
spectrograms. The survey further examines key applications, including phase
retrieval, speech enhancement, and speech separation, where deep learning has
achieved significant progress by leveraging complex spectrograms or their
derived feature representations. Additionally, we examine the intersection of
complex spectrograms with generative models. This survey aims to serve as a
valuable resource for researchers and practitioners in the field of speech
signal processing and complex-valued neural networks.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [270] [Not that Groove: Zero-Shot Symbolic Music Editing](https://arxiv.org/abs/2505.08203)
*Li Zhang*

Main category: cs.SD

TL;DR: 论文提出了一种利用零样本提示的LLMs进行符号音乐编辑的方法，解决了音乐生成领域缺乏标记数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI音乐生成主要关注音频，但其在音乐制作行业中的应用受限。为提升灵活性并仅依赖文本指令，研究转向符号音乐编辑。

Method: 通过设计创新的格式，将LLMs与音乐接口结合，利用零样本提示编辑鼓点节奏。

Result: 成功证明了LLMs在无标记数据情况下能有效编辑音乐，并提供与音乐家判断高度一致的评价数据集。

Conclusion: 该方法为符号音乐编辑提供了灵活且有效的解决方案，推动了AI在音乐制作中的应用。

Abstract: Most work in AI music generation focused on audio, which has seen limited use
in the music production industry due to its rigidity. To maximize flexibility
while assuming only textual instructions from producers, we are among the first
to tackle symbolic music editing. We circumvent the known challenge of lack of
labeled data by proving that LLMs with zero-shot prompting can effectively edit
drum grooves. The recipe of success is a creatively designed format that
interfaces LLMs and music, while we facilitate evaluation by providing an
evaluation dataset with annotated unit tests that highly aligns with musicians'
judgment.

</details>


### [271] [Fast Text-to-Audio Generation with Adversarial Post-Training](https://arxiv.org/abs/2505.08175)
*Zachary Novack,Zach Evans,Zack Zukowski,Josiah Taylor,CJ Carr,Julian Parker,Adnan Al-Sinan,Gian Marco Iodice,Julian McAuley,Taylor Berg-Kirkpatrick,Jordi Pons*

Main category: cs.SD

TL;DR: 提出了一种名为ARC的对抗性加速算法，用于提升扩散/流模型的推理速度，无需蒸馏。


<details>
  <summary>Details</summary>
Motivation: 现有文本到音频系统推理速度慢，限制了其在实际应用中的实用性。

Method: 结合相对对抗性训练和对比判别器目标，优化Stable Audio Open模型。

Result: 在H100上生成12秒44.1kHz立体声音频仅需75毫秒，移动设备上生成7秒音频。

Conclusion: ARC后训练显著提升了文本到音频模型的推理速度，是目前最快的解决方案。

Abstract: Text-to-audio systems, while increasingly performant, are slow at inference
time, thus making their latency unpractical for many creative applications. We
present Adversarial Relativistic-Contrastive (ARC) post-training, the first
adversarial acceleration algorithm for diffusion/flow models not based on
distillation. While past adversarial post-training methods have struggled to
compare against their expensive distillation counterparts, ARC post-training is
a simple procedure that (1) extends a recent relativistic adversarial
formulation to diffusion/flow post-training and (2) combines it with a novel
contrastive discriminator objective to encourage better prompt adherence. We
pair ARC post-training with a number optimizations to Stable Audio Open and
build a model capable of generating $\approx$12s of 44.1kHz stereo audio in
$\approx$75ms on an H100, and $\approx$7s on a mobile edge-device, the fastest
text-to-audio model to our knowledge.

</details>


### [272] [A Mamba-based Network for Semi-supervised Singing Melody Extraction Using Confidence Binary Regularization](https://arxiv.org/abs/2505.08681)
*Xiaoliang He,Kangjie Dong,Jingkai Cao,Shuai Yu,Wei Li,Yi Yu*

Main category: cs.SD

TL;DR: 提出了一种基于Mamba的网络SpectMamba，用于半监督歌唱旋律提取，解决了现有方法计算效率低、忽略音符基础以及数据标注不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有歌唱旋律提取方法存在计算效率低、忽略音符基础和标注数据不足的问题，需要一种更高效的解决方案。

Method: 引入视觉Mamba实现线性计算复杂度，提出音符-f0解码器模拟音乐表演，并使用置信二元正则化模块利用未标注数据。

Result: 在多个公开数据集上验证了方法的有效性。

Conclusion: SpectMamba通过改进计算效率和利用未标注数据，显著提升了歌唱旋律提取的性能。

Abstract: Singing melody extraction (SME) is a key task in the field of music
information retrieval. However, existing methods are facing several
limitations: firstly, prior models use transformers to capture the contextual
dependencies, which requires quadratic computation resulting in low efficiency
in the inference stage. Secondly, prior works typically rely on
frequencysupervised methods to estimate the fundamental frequency (f0), which
ignores that the musical performance is actually based on notes. Thirdly,
transformers typically require large amounts of labeled data to achieve optimal
performances, but the SME task lacks of sufficient annotated data. To address
these issues, in this paper, we propose a mamba-based network, called
SpectMamba, for semi-supervised singing melody extraction using confidence
binary regularization. In particular, we begin by introducing vision mamba to
achieve computational linear complexity. Then, we propose a novel note-f0
decoder that allows the model to better mimic the musical performance. Further,
to alleviate the scarcity of the labeled data, we introduce a confidence binary
regularization (CBR) module to leverage the unlabeled data by maximizing the
probability of the correct classes. The proposed method is evaluated on several
public datasets and the conducted experiments demonstrate the effectiveness of
our proposed method.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [273] [Beyond Basic A/B testing: Improving Statistical Efficiency for Business Growth](https://arxiv.org/abs/2505.08128)
*Changshuai Wei,Phuc Nguyen,Benjamin Zelditch,Joyce Chen*

Main category: stat.ME

TL;DR: 本文提出了针对A/B测试中低统计功效问题的多种方法，包括回归调整、广义估计方程等，并提出了一种新型的双重稳健广义U方法。


<details>
  <summary>Details</summary>
Motivation: 标准A/B测试方法在大规模工业应用中通常基于t检验，但在样本量小、非高斯分布或ROI考虑下统计功效较低。

Method: 提出了回归调整、广义估计方程、Man-Whitney U和Zero-Trimmed U等方法，以及一种新型的双重稳健广义U方法。

Result: 提供了渐近正态性和效率界限的理论结果，并通过模拟研究和实际A/B测试验证了方法的有效性。

Conclusion: 所提方法能够有效解决A/B测试中的统计功效问题，并在理论和实践中均表现出色。

Abstract: The standard A/B testing approaches are mostly based on t-test in large scale
industry applications. These standard approaches however suffers from low
statistical power in business settings, due to nature of small sample-size or
non-Gaussian distribution or return-on-investment (ROI) consideration. In this
paper, we propose several approaches to addresses these challenges: (i)
regression adjustment, generalized estimating equation, Man-Whitney U and
Zero-Trimmed U that addresses each of these issues separately, and (ii) a novel
doubly robust generalized U that handles ROI consideration, distribution
robustness and small samples in one framework. We provide theoretical results
on asymptotic normality and efficiency bounds, together with insights on the
efficiency gain from theoretical analysis. We further conduct comprehensive
simulation studies and apply the methods to multiple real A/B tests.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [274] [A Large-Scale Empirical Analysis of Custom GPTs' Vulnerabilities in the OpenAI Ecosystem](https://arxiv.org/abs/2505.08148)
*Sunday Oyinlola Ogundoyin,Muhammad Ikram,Hassan Jameel Asghar,Benjamin Zi Hao Zhao,Dali Kaafar*

Main category: cs.CR

TL;DR: 研究分析了14,904个自定义GPT模型，发现95%以上存在安全漏洞，主要涉及角色扮演攻击、系统提示泄漏和钓鱼内容生成。


<details>
  <summary>Details</summary>
Motivation: 随着自定义GPT的广泛应用，其安全漏洞问题日益突出，但现有研究缺乏实证和大规模评估。

Method: 通过多指标排名系统评估自定义GPT对七种可攻击威胁的易感性。

Result: 95%以上的自定义GPT缺乏足够安全保护，主要漏洞包括角色扮演攻击（96.51%）、系统提示泄漏（92.20%）和钓鱼（91.22%）。

Conclusion: 需加强安全措施和内容审核，以确保GPT应用的安全部署。

Abstract: Millions of users leverage generative pretrained transformer (GPT)-based
language models developed by leading model providers for a wide range of tasks.
To support enhanced user interaction and customization, many platforms-such as
OpenAI-now enable developers to create and publish tailored model instances,
known as custom GPTs, via dedicated repositories or application stores. These
custom GPTs empower users to browse and interact with specialized applications
designed to meet specific needs. However, as custom GPTs see growing adoption,
concerns regarding their security vulnerabilities have intensified. Existing
research on these vulnerabilities remains largely theoretical, often lacking
empirical, large-scale, and statistically rigorous assessments of associated
risks.
  In this study, we analyze 14,904 custom GPTs to assess their susceptibility
to seven exploitable threats, such as roleplay-based attacks, system prompt
leakage, phishing content generation, and malicious code synthesis, across
various categories and popularity tiers within the OpenAI marketplace. We
introduce a multi-metric ranking system to examine the relationship between a
custom GPT's popularity and its associated security risks.
  Our findings reveal that over 95% of custom GPTs lack adequate security
protections. The most prevalent vulnerabilities include roleplay-based
vulnerabilities (96.51%), system prompt leakage (92.20%), and phishing
(91.22%). Furthermore, we demonstrate that OpenAI's foundational models exhibit
inherent security weaknesses, which are often inherited or amplified in custom
GPTs. These results highlight the urgent need for enhanced security measures
and stricter content moderation to ensure the safe deployment of GPT-based
applications.

</details>


### [275] [Where the Devil Hides: Deepfake Detectors Can No Longer Be Trusted](https://arxiv.org/abs/2505.08255)
*Shuaiwei Yuan,Junyu Dong,Yuezun Li*

Main category: cs.CR

TL;DR: 论文探讨了Deepfake检测器因第三方数据中毒而存在的安全风险，并提出了一种生成隐形触发模式的方法，通过两种投毒场景成功注入后门。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成技术的发展，Deepfake检测器面临第三方数据中毒的安全风险，可能导致检测器被操控。

Method: 开发了一种生成隐形触发模式的工具，并通过脏标签和干净标签两种投毒场景注入后门。

Result: 实验证明该方法在有效性、隐蔽性和实用性上优于基线方法。

Conclusion: 研究揭示了Deepfake检测器的安全漏洞，并提出了一种高效的攻击方法。

Abstract: With the advancement of AI generative techniques, Deepfake faces have become
incredibly realistic and nearly indistinguishable to the human eye. To counter
this, Deepfake detectors have been developed as reliable tools for assessing
face authenticity. These detectors are typically developed on Deep Neural
Networks (DNNs) and trained using third-party datasets. However, this protocol
raises a new security risk that can seriously undermine the trustfulness of
Deepfake detectors: Once the third-party data providers insert poisoned
(corrupted) data maliciously, Deepfake detectors trained on these datasets will
be injected ``backdoors'' that cause abnormal behavior when presented with
samples containing specific triggers. This is a practical concern, as
third-party providers may distribute or sell these triggers to malicious users,
allowing them to manipulate detector performance and escape accountability.
  This paper investigates this risk in depth and describes a solution to
stealthily infect Deepfake detectors. Specifically, we develop a trigger
generator, that can synthesize passcode-controlled, semantic-suppression,
adaptive, and invisible trigger patterns, ensuring both the stealthiness and
effectiveness of these triggers. Then we discuss two poisoning scenarios,
dirty-label poisoning and clean-label poisoning, to accomplish the injection of
backdoors. Extensive experiments demonstrate the effectiveness, stealthiness,
and practicality of our method compared to several baselines.

</details>


### [276] [Privacy-Preserving Analytics for Smart Meter (AMI) Data: A Hybrid Approach to Comply with CPUC Privacy Regulations](https://arxiv.org/abs/2505.08237)
*Benjamin Westrich*

Main category: cs.CR

TL;DR: 论文探讨了智能电表和燃气表数据（AMI）的隐私保护问题，提出了一种结合多种隐私保护技术的混合架构，以满足加州隐私法规和FIPPs要求。


<details>
  <summary>Details</summary>
Motivation: 智能电表数据（AMI）为公用事业和消费者提供了宝贵洞察，但也带来了隐私问题。加州法规要求严格保护用户能源使用数据。

Method: 综合研究了数据匿名化、隐私保护机器学习（差分隐私和联邦学习）、合成数据生成和加密技术（安全多方计算、同态加密），并提出了一种混合架构。

Result: 提出了一种满足法规要求的混合架构，支持高级分析（如机器学习和经济计量分析）同时保护隐私。

Conclusion: 为公用事业数据科学家和工程师提供了隐私设计蓝图，兼顾数据创新和法规合规。

Abstract: Advanced Metering Infrastructure (AMI) data from smart electric and gas
meters enables valuable insights for utilities and consumers, but also raises
significant privacy concerns. In California, regulatory decisions (CPUC
D.11-07-056 and D.11-08-045) mandate strict privacy protections for customer
energy usage data, guided by the Fair Information Practice Principles (FIPPs).
We comprehensively explore solutions drawn from data anonymization,
privacy-preserving machine learning (differential privacy and federated
learning), synthetic data generation, and cryptographic techniques (secure
multiparty computation, homomorphic encryption). This allows advanced
analytics, including machine learning models, statistical and econometric
analysis on energy consumption data, to be performed without compromising
individual privacy.
  We evaluate each technique's theoretical foundations, effectiveness, and
trade-offs in the context of utility data analytics, and we propose an
integrated architecture that combines these methods to meet real-world needs.
The proposed hybrid architecture is designed to ensure compliance with
California's privacy rules and FIPPs while enabling useful analytics, from
forecasting and personalized insights to academic research and econometrics,
while strictly protecting individual privacy. Mathematical definitions and
derivations are provided where appropriate to demonstrate privacy guarantees
and utility implications rigorously. We include comparative evaluations of the
techniques, an architecture diagram, and flowcharts to illustrate how they work
together in practice. The result is a blueprint for utility data scientists and
engineers to implement privacy-by-design in AMI data handling, supporting both
data-driven innovation and strict regulatory compliance.

</details>


### [277] [Securing RAG: A Risk Assessment and Mitigation Framework](https://arxiv.org/abs/2505.08728)
*Lukas Ammann,Sara Ott,Christoph R. Landolt,Marco P. Lehmann*

Main category: cs.CR

TL;DR: 本文探讨了检索增强生成（RAG）的安全与隐私挑战，提出了漏洞分析、缓解措施及一个结合RAG特定安全考量的框架。


<details>
  <summary>Details</summary>
Motivation: RAG作为行业标准，虽提升了NLP应用的质量，但也带来了新的安全与隐私问题，尤其是涉及敏感数据时。

Method: 首先分析RAG管道的漏洞和攻击面，提出缓解措施；其次开发一个结合RAG安全考量的框架。

Result: 提出了一个指导实现安全、合规、可信RAG系统的框架。

Conclusion: 该框架为RAG系统的安全实施提供了结构化指导。

Abstract: Retrieval Augmented Generation (RAG) has emerged as the de facto industry
standard for user-facing NLP applications, offering the ability to integrate
data without re-training or fine-tuning Large Language Models (LLMs). This
capability enhances the quality and accuracy of responses but also introduces
novel security and privacy challenges, particularly when sensitive data is
integrated. With the rapid adoption of RAG, securing data and services has
become a critical priority. This paper first reviews the vulnerabilities of RAG
pipelines, and outlines the attack surface from data pre-processing and data
storage management to integration with LLMs. The identified risks are then
paired with corresponding mitigations in a structured overview. In a second
step, the paper develops a framework that combines RAG-specific security
considerations, with existing general security guidelines, industry standards,
and best practices. The proposed framework aims to guide the implementation of
robust, compliant, secure, and trustworthy RAG systems.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [278] [Big Data and the Computational Social Science of Entrepreneurship and Innovation](https://arxiv.org/abs/2505.08706)
*Ningzi Li,Shiyang Lai,James Evans*

Main category: econ.GN

TL;DR: 论文探讨了利用大规模数据和机器学习方法研究创业与创新的机遇与挑战，提出了两种利用多模态数据的方法，并强调大数据与大模型结合对理论发展的推动作用。


<details>
  <summary>Details</summary>
Motivation: 随着大规模社交数据的爆发和机器学习方法的发展，创业与创新研究面临新的机遇与挑战，需要解决技术新颖性、新企业起源和竞争预测等问题。

Method: 提出两种方法：1）结合机器学习模型和大规模数据构建精准测量工具；2）利用大数据驱动的AI模型创建技术和商业的“数字双胞胎”，用于虚拟实验。

Result: 通过大数据与大模型的结合，能够更精准地观测和实验创业与创新过程，推动理论发展。

Conclusion: 论文主张将大数据与大模型结合，以促进创业与创新领域的理论发展与测试。

Abstract: As large-scale social data explode and machine-learning methods evolve,
scholars of entrepreneurship and innovation face new research opportunities but
also unique challenges. This chapter discusses the difficulties of leveraging
large-scale data to identify technological and commercial novelty, document new
venture origins, and forecast competition between new technologies and
commercial forms. It suggests how scholars can take advantage of new text,
network, image, audio, and video data in two distinct ways that advance
innovation and entrepreneurship research. First, machine-learning models,
combined with large-scale data, enable the construction of precision
measurements that function as system-level observatories of innovation and
entrepreneurship across human societies. Second, new artificial intelligence
models fueled by big data generate 'digital doubles' of technology and
business, forming laboratories for virtual experimentation about innovation and
entrepreneurship processes and policies. The chapter argues for the advancement
of theory development and testing in entrepreneurship and innovation by
coupling big data with big models.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [279] [ai.txt: A Domain-Specific Language for Guiding AI Interactions with the Internet](https://arxiv.org/abs/2505.07834)
*Yuekang Li,Wei Song,Bangshuo Zhu,Dong Gong,Yi Liu,Gelei Deng,Chunyang Chen,Lei Ma,Jun Sun,Toby Walsh,Jingling Xue*

Main category: cs.NI

TL;DR: ai.txt是一种新的领域特定语言（DSL），旨在规范AI模型、代理与网络内容的交互，弥补robots.txt的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如robots.txt）在AI与网络内容交互时缺乏精细化和语义表达能力，难以确保伦理和法律合规。

Method: ai.txt扩展了基于URL的访问控制，支持元素级精确调控，并提供自然语言指令。开发了集成开发环境和两种合规机制（XML强制执行和自然语言提示）。

Result: 初步实验和案例研究表明，ai.txt及其合规机制有效。

Conclusion: ai.txt有助于规范AI与互联网的交互，促进数字生态系统中AI的负责任使用。

Abstract: We introduce ai.txt, a novel domain-specific language (DSL) designed to
explicitly regulate interactions between AI models, agents, and web content,
addressing critical limitations of the widely adopted robots.txt standard. As
AI increasingly engages with online materials for tasks such as training,
summarization, and content modification, existing regulatory methods lack the
necessary granularity and semantic expressiveness to ensure ethical and legal
compliance. ai.txt extends traditional URL-based access controls by enabling
precise element-level regulations and incorporating natural language
instructions interpretable by AI systems. To facilitate practical deployment,
we provide an integrated development environment with code autocompletion and
automatic XML generation. Furthermore, we propose two compliance mechanisms:
XML-based programmatic enforcement and natural language prompt integration, and
demonstrate their effectiveness through preliminary experiments and case
studies. Our approach aims to aid the governance of AI-Internet interactions,
promoting responsible AI use in digital ecosystems.

</details>


### [280] [Intelligent Product 3.0: Decentralised AI Agents and Web3 Intelligence Standards](https://arxiv.org/abs/2505.07835)
*Alex C. Y. Wong,Duncan McFarlane,C. Ellarby,M. Lee,M. Kuok*

Main category: cs.NI

TL;DR: 本文回顾了智能产品的演进历程，从早期的Auto-ID项目到区块链、Web3和人工智能的进步，提出了智能产品3.0的新规范。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用去中心化身份、区块链和AI技术，提升智能产品的自主性和交互能力。

Method: 结合区块链、Web3和人工智能技术，分析去中心化身份和AI协作的潜力。

Result: 提出了智能产品3.0的新规范，展示了去中心化和AI驱动的无缝交互能力。

Conclusion: 智能产品3.0通过去中心化和AI技术，实现了更高水平的自主性和交互能力。

Abstract: Twenty-five years ago, the specification of the Intelligent Product was
established, envisaging real-time connectivity that not only enables products
to gather accurate data about themselves but also allows them to assess and
influence their own destiny. Early work by the Auto-ID project focused on
creating a single, open-standard repository for storing and retrieving product
information, laying a foundation for scalable connectivity. A decade later, the
approach was revisited in light of low-cost RFID systems that promised a
low-cost link between physical goods and networked information environments.
Since then, advances in blockchain, Web3, and artificial intelligence have
introduced unprecedented levels of resilience, consensus, and autonomy. By
leveraging decentralised identity, blockchain-based product information and
history, and intelligent AI-to-AI collaboration, this paper examines these
developments and outlines a new specification for the Intelligent Product 3.0,
illustrating how decentralised and AI-driven capabilities facilitate seamless
interaction between physical AI and everyday products.

</details>


### [281] [Efficient Telecom Specific LLM: TSLAM-Mini with QLoRA and Digital Twin Data](https://arxiv.org/abs/2505.07877)
*Vignesh Ethiraj,Divya Vijay,Sidhanth Menon,Heblin Berscilla*

Main category: cs.NI

TL;DR: 论文通过微调TSLAM-Mini模型，结合电信领域专用数据集和QLoRA技术，显著提升了模型在实时电信应用中的性能。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在电信领域的专业需求中表现不佳，需通过领域适配提升性能。

Method: 使用NetoAI的DigiTwin平台构建10万样本数据集，结合QLoRA技术微调TSLAM-Mini模型。

Result: TSLAM-Mini在电信应用中表现卓越，验证了领域专用数据集和PEFT方法的有效性。

Conclusion: 领域专用数据集和高效微调技术是提升智能网络管理的关键。

Abstract: General-purpose large language models (LLMs), despite their broad
capabilities accrued from open-world data, frequently exhibit suboptimal
performance when confronted with the nuanced and specialized demands inherent
in real-time telecommunications applications. This investigation addresses this
critical limitation through the meticulous fine-tuning of TSLAM-Mini developed
by NetoAI, a compact (3.8-billion parameter) causal language model
architecturally derived from Phi-4 Mini Instruct 4B. The fine-tuning regimen
leverages a bespoke dataset comprising 100,000 samples, strategically
engineered to address 20 pivotal telecommunications use-cases, encompassing
domains such as Network Fundamentals, IP Routing, MPLS, Network Security,
Automation, OSS/BSS, RAN, Mobile Core, Satellite Communications, and Ethical
AI. This dataset was curated utilizing NetoAI's DigiTwin platform, enriched
with granular insights from venerated network Subject Matter Experts (SMEs) and
authoritative RFC documents, thereby capturing high-fidelity representations of
real-world network dynamics through simulations inspired by digital twin
paradigms. Employing Quantized Low-Rank Adaptation (QLoRA), a state-of-the-art
Parameter Efficient Fine-Tuning (PEFT) technique, we achieved substantial
training efficiency and enabled prospective deployment on resource-constrained
hardware. A novel evaluation framework, predicated on a high-capacity LLM
(Qwen3-235B-A22B) functioning as an automated adjudicator, was instituted to
rigorously assess instruction-following fidelity and response quality across
the specified telecom use-cases. Empirical results unequivocally demonstrate
TSLAM-Mini's superior aptitude in telecom-centric applications, underscoring
the profound efficacy of domain-specific datasets and PEFT methodologies for
advancing intelligent network management.

</details>


### [282] [ML-Enabled Eavesdropper Detection in Beyond 5G IIoT Networks](https://arxiv.org/abs/2505.07837)
*Maria-Lamprini A. Bartsioka,Ioannis A. Bartsiokas,Panagiotis K. Gkonis,Dimitra I. Kaklamani,Iakovos S. Venieris*

Main category: cs.NI

TL;DR: 论文探讨了在B5G网络中利用ML/DL技术检测窃听问题，DCNN和RF模型表现优异，准确率接近100%。


<details>
  <summary>Details</summary>
Motivation: 5G/B5G网络在IIoT环境中存在安全漏洞，传统加密方法难以应对，需探索AI驱动的物理层安全技术。

Method: 使用模拟的B5G异构无线网络，评估RF、DCNN和LSTM模型，基于CSI、位置数据和传输功率分类用户。

Result: DCNN和RF模型在检测窃听者时准确率接近100%，且无误报。

Conclusion: AI与物理层安全结合在下一代无线网络中具有巨大潜力，可应对新兴安全威胁。

Abstract: Advanced fifth generation (5G) and beyond (B5G) communication networks have
revolutionized wireless technologies, supporting ultra-high data rates, low
latency, and massive connectivity. However, they also introduce
vulnerabilities, particularly in decentralized Industrial Internet of Things
(IIoT) environments. Traditional cryptographic methods struggle with
scalability and complexity, leading researchers to explore Artificial
Intelligence (AI)-driven physical layer techniques for secure communications.
In this context, this paper focuses on the utilization of Machine and Deep
Learning (ML/DL) techniques to tackle with the common problem of eavesdropping
detection. To this end, a simulated industrial B5G heterogeneous wireless
network is used to evaluate the performance of various ML/DL models, including
Random Forests (RF), Deep Convolutional Neural Networks (DCNN), and Long
Short-Term Memory (LSTM) networks. These models classify users as either
legitimate or malicious ones based on channel state information (CSI), position
data, and transmission power. According to the presented numerical results,
DCNN and RF models achieve a detection accuracy approaching 100\% in
identifying eavesdroppers with zero false alarms. In general, this work
underlines the great potential of combining AI and Physical Layer Security
(PLS) for next-generation wireless networks in order to address evolving
security threats.

</details>


### [283] [Token Communication-Driven Multimodal Large Models in Resource-Constrained Multiuser Networks](https://arxiv.org/abs/2505.07841)
*Junhe Zhang,Wanli Ni,Pengwei Wang,Dongyu Wang*

Main category: cs.NI

TL;DR: 论文提出了一种基于令牌通信的范式，用于在资源受限网络中部署多模态大模型（MLMs），通过对比分割微调和轻量压缩技术优化传输效率，实验显示测试准确率提升13.7%。


<details>
  <summary>Details</summary>
Motivation: 无线边缘智能应用的普及和多模态数据的爆炸式增长，使得在资源受限网络中部署MLMs面临带宽、计算能力和延迟等挑战。

Method: 1) 设计对比分割微调方法，将多模态输入投影到共享特征空间；2) 采用轻量压缩技术减少令牌传输大小。

Result: 在不同信噪比条件下，测试准确率提升13.7%，收敛速度更快。

Conclusion: 令牌通信范式为多用户网络中可扩展和弹性的MLM部署提供了有效解决方案。

Abstract: The proliferation of intelligent applications at the wireless edge, alongside
the exponential growth of multimodal data, poses challenges for deploying
multimodal large models (MLMs) in resource-constrained networks. These
constraints manifest as limited bandwidth, computational capacity, and
stringent latency requirements, particularly under low signal-to-noise ratio
(SNR) conditions. To overcome these limitations, we propose a token
communication paradigm that facilitates the decentralized deployment of MLMs
across user devices and edge infrastructure (e.g., base stations). In this
paradigm, task-relevant tokens are extracted from multimodal inputs and serve
as the primary medium for communication between distributed model components.
To align semantics and optimize transmission efficiency, we propose a
dual-pronged approach: 1) We design a contrastive split fine-tuning method to
project heterogeneous modalities into a shared feature space, enabling seamless
interaction between model components while preserving modal-specific semantics.
2) We employ a lightweight compression technique to reduce the size of
transmitted tokens, minimizing bandwidth consumption without sacrificing
task-critical information. The proposed framework integrates collaborative
fine-tuning of both the foundation model and multimodal transceivers, ensuring
that token generation and utilization are tailored to specific downstream
tasks. Simulation experiments conducted under different SNR conditions
demonstrate that our method results in a $13.7\%$ improvement in test accuracy.
Furthermore, our approach exhibits quicker convergence rates, even with reduced
token lengths, highlighting the promise of token communication for facilitating
more scalable and resilient MLM implementations in practical multiuser
networks.

</details>


### [284] [VoI-Driven Joint Optimization of Control and Communication in Vehicular Digital Twin Network](https://arxiv.org/abs/2505.07892)
*Lei Lei,Kan Zheng,Jie Mei,Xuemin,Shen*

Main category: cs.NI

TL;DR: 本文提出了一种车联网数字孪生网络（VDTN）架构，通过联合优化控制与通信，利用深度强化学习（DRL）模块实现最优策略，并在车队场景中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 6G网络为车联网数字孪生提供了无缝集成机会，利用数字孪生中的计算资源和时空数据提升车联网系统的通信与控制性能。

Method: 提出VDTN架构，定义基于控制性能的信息价值（VoI）概念，并设计联合优化框架，通过两个DRL模块迭代处理控制与通信问题。

Result: 仿真实验表明，该框架在车队场景中能有效优化控制与通信性能。

Conclusion: VDTN架构及其联合优化框架为车联网系统的性能提升提供了可行方案。

Abstract: The vision of sixth-generation (6G) wireless networks paves the way for the
seamless integration of digital twins into vehicular networks, giving rise to a
Vehicular Digital Twin Network (VDTN). The large amount of computing resources
as well as the massive amount of spatial-temporal data in Digital Twin (DT)
domain can be utilized to enhance the communication and control performance of
Internet of Vehicle (IoV) systems. In this article, we first propose the
architecture of VDTN, emphasizing key modules that center on functions related
to the joint optimization of control and communication. We then delve into the
intricacies of the multitimescale decision process inherent in joint
optimization in VDTN, specifically investigating the dynamic interplay between
control and communication. To facilitate the joint optimization, we define two
Value of Information (VoI) concepts rooted in control performance.
Subsequently, utilizing VoI as a bridge between control and communication, we
introduce a novel joint optimization framework, which involves iterative
processing of two Deep Reinforcement Learning (DRL) modules corresponding to
control and communication to derive the optimal policy. Finally, we conduct
simulations of the proposed framework applied to a platoon scenario to
demonstrate its effectiveness in ensu

</details>


### [285] [Channel Fingerprint Construction for Massive MIMO: A Deep Conditional Generative Approach](https://arxiv.org/abs/2505.07893)
*Zhenzhou Jin,Li You,Xudong Li,Zhen Gao,Yuanwei Liu,Xiang-Gen Xia,Xiqi Gao*

Main category: cs.NI

TL;DR: 论文提出了一种基于条件生成扩散模型（CGDM）的CF twins方法，用于将粗粒度信道指纹（CF）转换为细粒度CF，显著提升了重建性能。


<details>
  <summary>Details</summary>
Motivation: 由于实际感知节点和测试车辆的成本限制，粗粒度CF不足以支持无线收发器设计，因此需要一种方法将其转换为细粒度CF。

Method: 设计了CGDM作为CF twins的计算核心，利用变分推理技术推导ELBO，并结合轻量化技术（如剪枝和知识蒸馏）。

Result: 实验表明，该方法在重建性能上显著优于基线，且在不同放大因子下的零样本测试验证了其可扩展性和泛化能力。

Conclusion: 提出的CGDM方法有效解决了粗粒度CF到细粒度CF的转换问题，具有实际应用潜力。

Abstract: Accurate channel state information (CSI) acquisition for massive
multiple-input multiple-output (MIMO) systems is essential for future mobile
communication networks. Channel fingerprint (CF), also referred to as channel
knowledge map, is a key enabler for intelligent environment-aware communication
and can facilitate CSI acquisition. However, due to the cost limitations of
practical sensing nodes and test vehicles, the resulting CF is typically
coarse-grained, making it insufficient for wireless transceiver design. In this
work, we introduce the concept of CF twins and design a conditional generative
diffusion model (CGDM) with strong implicit prior learning capabilities as the
computational core of the CF twin to establish the connection between coarse-
and fine-grained CFs. Specifically, we employ a variational inference technique
to derive the evidence lower bound (ELBO) for the log-marginal distribution of
the observed fine-grained CF conditioned on the coarse-grained CF, enabling the
CGDM to learn the complicated distribution of the target data. During the
denoising neural network optimization, the coarse-grained CF is introduced as
side information to accurately guide the conditioned generation of the CGDM. To
make the proposed CGDM lightweight, we further leverage the additivity of
network layers and introduce a one-shot pruning approach along with a
multi-objective knowledge distillation technique. Experimental results show
that the proposed approach exhibits significant improvement in reconstruction
performance compared to the baselines. Additionally, zero-shot testing on
reconstruction tasks with different magnification factors further demonstrates
the scalability and generalization ability of the proposed approach.

</details>


### [286] [EnvCDiff: Joint Refinement of Environmental Information and Channel Fingerprints via Conditional Generative Diffusion Model](https://arxiv.org/abs/2505.07894)
*Zhenzhou Jin,Li You,Xiang-Gen Xia,Xiqi Gao*

Main category: cs.NI

TL;DR: 论文提出了一种深度条件生成学习方法（CDiff），用于从粗粒度环境信息和信道指纹（CF）中重构细粒度的EnvCF，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前环境感知通信中，环境信息和CF的获取多为粗粒度，不足以指导无线传输设计，因此需要一种方法将其精细化。

Method: 采用定制化的条件生成扩散模型（CDiff），同时优化环境信息和CF，重构细粒度的EnvCF。

Result: 实验表明，CDiff在EnvCF构建上的性能显著优于基线方法。

Conclusion: CDiff为环境感知通信提供了一种有效的细粒度CF重构方法。

Abstract: The paradigm shift from environment-unaware communication to intelligent
environment-aware communication is expected to facilitate the acquisition of
channel state information for future wireless communications. Channel
Fingerprint (CF), as an emerging enabling technology for environment-aware
communication, provides channel-related knowledge for potential locations
within the target communication area. However, due to the limited availability
of practical devices for sensing environmental information and measuring
channel-related knowledge, most of the acquired environmental information and
CF are coarse-grained, insufficient to guide the design of wireless
transmissions. To address this, this paper proposes a deep conditional
generative learning approach, namely a customized conditional generative
diffusion model (CDiff). The proposed CDiff simultaneously refines
environmental information and CF, reconstructing a fine-grained CF that
incorporates environmental information, referred to as EnvCF, from its
coarse-grained counterpart. Experimental results show that the proposed
approach significantly improves the performance of EnvCF construction compared
to the baselines.

</details>


### [287] [Online Learning-based Adaptive Beam Switching for 6G Networks: Enhancing Efficiency and Resilience](https://arxiv.org/abs/2505.08032)
*Seyed Bagher Hashemi Natanzi,Zhicong Zhu,Bo Tang*

Main category: cs.NI

TL;DR: 提出了一种基于深度强化学习（DRL）的在线学习框架，用于6G网络中的自适应波束切换，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 6G网络中的自适应波束切换面临高频、移动性和遮挡等挑战，需要更鲁棒的解决方案。

Method: 采用DRL框架，结合GRU架构和优先级经验回放，优化实时波束管理。

Result: 在时间相关遮挡场景下，该方法在SNR、吞吐量和准确性上显著优于传统启发式方法，且优于MAB基线。

Conclusion: 证明了记忆和优先级学习对6G波束管理的有效性，同时确认MAB作为强基线。

Abstract: Adaptive beam switching in 6G networks is challenged by high frequencies,
mobility, and blockage. We propose an Online Learning framework using Deep
Reinforcement Learning (DRL) with an enhanced state representation (velocity
and blockage history), a GRU architecture, and prioritized experience replay
for real-time beam optimization. Validated via Nvidia Sionna under
time-correlated blockage, our approach significantly enhances resilience in
SNR, throughput, and accuracy compared to a conventional heuristic.
Furthermore, the enhanced DRL agent outperforms a reactive Multi-Armed Bandit
(MAB) baseline by leveraging temporal dependencies, achieving lower performance
variability. This demonstrates the benefits of memory and prioritized learning
for robust 6G beam management, while confirming MAB as a strong baseline.

</details>


### [288] [Mobile Jamming Mitigation in 5G Networks: A MUSIC-Based Adaptive Beamforming Approach](https://arxiv.org/abs/2505.08046)
*Olivia Holguin,Rachel Donati,Seyed bagher Hashemi Natanzi,Bo Tang*

Main category: cs.NI

TL;DR: 提出了一种结合MUSIC、MVDR和机器学习的智能抗干扰框架，显著提升了5G网络在军事通信中的抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 移动干扰器对5G网络（尤其是军事通信）构成严重威胁，需要高效抗干扰解决方案。

Method: 整合MUSIC进行高分辨率DoA估计，MVDR波束成形抑制干扰，机器学习增强DoA预测。

Result: 在高速公路场景中，平均SNR提升9.58 dB，DoA估计准确率达99.8%。

Conclusion: 该框架计算高效、适应动态干扰模式，优于传统方法，是5G通信安全的可靠解决方案。

Abstract: Mobile jammers pose a critical threat to 5G networks, particularly in
military communications. We propose an intelligent anti-jamming framework that
integrates Multiple Signal Classification (MUSIC) for high-resolution
Direction-of-Arrival (DoA) estimation, Minimum Variance Distortionless Response
(MVDR) beamforming for adaptive interference suppression, and machine learning
(ML) to enhance DoA prediction for mobile jammers. Extensive simulations in a
realistic highway scenario demonstrate that our hybrid approach achieves an
average Signal-to-Noise Ratio (SNR) improvement of 9.58 dB (maximum 11.08 dB)
and up to 99.8% DoA estimation accuracy. The framework's computational
efficiency and adaptability to dynamic jammer mobility patterns outperform
conventional anti-jamming techniques, making it a robust solution for securing
5G communications in contested environments.

</details>


### [289] [Graph-Based Floor Separation Using Node Embeddings and Clustering of WiFi Trajectories](https://arxiv.org/abs/2505.08088)
*Rabia Yasa Kostas,Kahraman Kostas*

Main category: cs.NI

TL;DR: 本文提出了一种基于图的Wi-Fi指纹轨迹楼层分离方法，利用Node2Vec和K-means聚类，在复杂室内环境中实现垂直定位，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决室内多楼层环境中垂直定位的挑战，提升定位精度。

Method: 构建Wi-Fi指纹图，使用Node2Vec生成低维嵌入，K-means聚类识别楼层。

Result: 在华为大学挑战赛2021数据集上，准确率68.97%，F1分数61.99%，调整兰德指数57.19%。

Conclusion: 该方法对信号噪声和建筑复杂性具有鲁棒性，为楼层定位提供了可扩展解决方案。

Abstract: Indoor positioning systems (IPSs) are increasingly vital for location-based
services in complex multi-storey environments. This study proposes a novel
graph-based approach for floor separation using Wi-Fi fingerprint trajectories,
addressing the challenge of vertical localization in indoor settings. We
construct a graph where nodes represent Wi-Fi fingerprints, and edges are
weighted by signal similarity and contextual transitions. Node2Vec is employed
to generate low-dimensional embeddings, which are subsequently clustered using
K-means to identify distinct floors. Evaluated on the Huawei University
Challenge 2021 dataset, our method outperforms traditional community detection
algorithms, achieving an accuracy of 68.97%, an F1- score of 61.99%, and an
Adjusted Rand Index of 57.19%. By publicly releasing the preprocessed dataset
and implementation code, this work contributes to advancing research in indoor
positioning. The proposed approach demonstrates robustness to signal noise and
architectural complexities, offering a scalable solution for floor-level
localization.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [290] [SpNeRF: Memory Efficient Sparse Volumetric Neural Rendering Accelerator for Edge Devices](https://arxiv.org/abs/2505.08191)
*Yipu Zhang,Jiawei Liang,Jian Peng,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: SpNeRF提出了一种软硬件协同设计的稀疏体素神经渲染方案，通过预处理和在线解码步骤减少内存占用，同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 神经渲染在AR/VR应用中表现优异，但其大体积素网格数据和不规则访问模式对边缘设备的实时处理提出了挑战。现有方法未充分解决大体积素网格导致的内存问题。

Method: 提出预处理步骤（哈希映射）和在线解码步骤（位图掩码），并设计专用硬件架构支持稀疏体素网格处理。

Result: SpNeRF平均减少21.07倍内存占用，同时保持PSNR水平；在速度和能效上显著优于现有方案。

Conclusion: SpNeRF通过软硬件协同设计有效解决了神经渲染在边缘设备上的内存和效率问题。

Abstract: Neural rendering has gained prominence for its high-quality output, which is
crucial for AR/VR applications. However, its large voxel grid data size and
irregular access patterns challenge real-time processing on edge devices. While
previous works have focused on improving data locality, they have not
adequately addressed the issue of large voxel grid sizes, which necessitate
frequent off-chip memory access and substantial on-chip memory. This paper
introduces SpNeRF, a software-hardware co-design solution tailored for sparse
volumetric neural rendering. We first identify memory-bound rendering
inefficiencies and analyze the inherent sparsity in the voxel grid data of
neural rendering. To enhance efficiency, we propose novel preprocessing and
online decoding steps, reducing the memory size for voxel grid. The
preprocessing step employs hash mapping to support irregular data access while
maintaining a minimal memory size. The online decoding step enables efficient
on-chip sparse voxel grid processing, incorporating bitmap masking to mitigate
PSNR loss caused by hash collisions. To further optimize performance, we design
a dedicated hardware architecture supporting our sparse voxel grid processing
technique. Experimental results demonstrate that SpNeRF achieves an average
21.07$\times$ reduction in memory size while maintaining comparable PSNR
levels. When benchmarked against Jetson XNX, Jetson ONX, RT-NeRF.Edge and
NeuRex.Edge, our design achieves speedups of 95.1$\times$, 63.5$\times$,
1.5$\times$ and 10.3$\times$, and improves energy efficiency by 625.6$\times$,
529.1$\times$, 4$\times$, and 4.4$\times$, respectively.

</details>


### [291] [MINIMALIST: switched-capacitor circuits for efficient in-memory computation of gated recurrent units](https://arxiv.org/abs/2505.08599)
*Sebastian Billaudelle,Laura Kriener,Filippo Moro,Tristan Torchet,Melika Payvand*

Main category: cs.AR

TL;DR: 提出了一种基于最小门控循环单元（GRU）的硬件兼容架构及其高效混合信号硬件实现，利用开关电容电路进行内存计算和门控状态更新。


<details>
  <summary>Details</summary>
Motivation: 针对嵌入式边缘计算环境中内存受限系统的时序数据处理需求，结合训练范式的进步，设计高效RNN架构。

Method: 采用最小GRU架构，结合开关电容电路实现混合信号硬件设计，仅使用金属电容器、传输门和时钟比较器等通用电路。

Result: 在时间序列数据上验证性能，并通过混合信号仿真验证硬件兼容性，复现软件模型的记录数据。

Conclusion: 该设计为内存受限系统提供了一种高效且可扩展的RNN硬件实现方案。

Abstract: Recurrent neural networks (RNNs) have been a long-standing candidate for
processing of temporal sequence data, especially in memory-constrained systems
that one may find in embedded edge computing environments. Recent advances in
training paradigms have now inspired new generations of efficient RNNs. We
introduce a streamlined and hardware-compatible architecture based on minimal
gated recurrent units (GRUs), and an accompanying efficient mixed-signal
hardware implementation of the model. The proposed design leverages
switched-capacitor circuits not only for in-memory computation (IMC), but also
for the gated state updates. The mixed-signal cores rely solely on commodity
circuits consisting of metal capacitors, transmission gates, and a clocked
comparator, thus greatly facilitating scaling and transfer to other technology
nodes.
  We benchmark the performance of our architecture on time series data,
introducing all constraints required for a direct mapping to the hardware
system. The direct compatibility is verified in mixed-signal simulations,
reproducing data recorded from the software-only network model.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [292] [Contrastive Normalizing Flows for Uncertainty-Aware Parameter Estimation](https://arxiv.org/abs/2505.08709)
*Ibrahim Elsharkawy,Yonatan Kahn*

Main category: physics.data-an

TL;DR: 论文提出了一种基于对比归一化流（CNF）的新方法，用于解决高能物理中参数估计的系统不确定性挑战，并在HiggsML Uncertainty Challenge数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 在高能物理和机器学习中，系统不确定性（如探测器校准错误）会导致数据分布失真，影响统计精度。目前，如何在域偏移下实现不确定性感知的参数估计仍是一个开放问题。

Method: 采用对比归一化流（CNF）方法，通过嵌入数据和参数到学习的CNF映射中，生成可调对比分布，从而在偏移数据分布下实现鲁棒分类。

Result: 在HiggsML Uncertainty Challenge数据集上表现优异，结合分类器和频率学方法，提供了鲁棒的参数估计和不确定性量化。

Conclusion: CNF方法结合分类器，能够有效应对数据分布失真问题，为参数估计和不确定性量化提供了新思路。

Abstract: Estimating physical parameters from data is a crucial application of machine
learning (ML) in the physical sciences. However, systematic uncertainties, such
as detector miscalibration, induce data distribution distortions that can erode
statistical precision. In both high-energy physics (HEP) and broader ML
contexts, achieving uncertainty-aware parameter estimation under these domain
shifts remains an open problem. In this work, we address this challenge of
uncertainty-aware parameter estimation for a broad set of tasks critical for
HEP. We introduce a novel approach based on Contrastive Normalizing Flows
(CNFs), which achieves top performance on the HiggsML Uncertainty Challenge
dataset. Building on the insight that a binary classifier can approximate the
model parameter likelihood ratio, we address the practical limitations of
expressivity and the high cost of simulating high-dimensional parameter grids
by embedding data and parameters in a learned CNF mapping. This mapping yields
a tunable contrastive distribution that enables robust classification under
shifted data distributions. Through a combination of theoretical analysis and
empirical evaluations, we demonstrate that CNFs, when coupled with a classifier
and established frequentist techniques, provide principled parameter estimation
and uncertainty quantification through classification that is robust to data
distribution distortions.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [293] [Sub-diffraction terahertz backpropagation compressive imaging](https://arxiv.org/abs/2505.07839)
*Yongsheng Zhu,Shaojing Liu,Ximiao Wang,Runli Li,Haili Yang,Jiali Wang,Hongjia Zhu,Yanlin Ke,Ningsheng Xu,Huanjun Chen,Shaozhi Deng*

Main category: eess.IV

TL;DR: 提出了一种基于无训练神经网络的亚衍射太赫兹反向传播压缩成像技术，显著提高了成像分辨率并减少了采样时间。


<details>
  <summary>Details</summary>
Motivation: 太赫兹单像素成像（TSPI）因简单和成本效益高而备受关注，但其分辨率受限于波长，且现有亚波长分辨率技术需要苛刻条件和耗时过程。

Method: 使用单色连续波太赫兹辐射照射物体，通过硅片背面的光激发载流子调制太赫兹波，单点探测器记录信号，无训练神经网络在物理模型约束下重建图像。

Result: 实现了约λ0/7的空间分辨率（λ0=833.3μm），无需超薄光调制器，显著减少了采样时间。

Conclusion: 该方法为太赫兹显微成像和其他逆成像问题提供了高效解决方案。

Abstract: Terahertz single-pixel imaging (TSPI) has garnered significant attention due
to its simplicity and cost-effectiveness. However, the relatively long
wavelength of THz waves limits sub-diffraction-scale imaging resolution.
Although TSPI technique can achieve sub-wavelength resolution, it requires
harsh experimental conditions and time-consuming processes. Here, we propose a
sub-diffraction THz backpropagation compressive imaging technique. We
illuminate the object with monochromatic continuous-wave THz radiation. The
transmitted THz wave is modulated by prearranged patterns generated on the back
surface of a 500-{\mu}m-thick silicon wafer, realized through photoexcited
carriers using a 532-nm laser. The modulated THz wave is then recorded by a
single-element detector. An untrained neural network is employed to iteratively
reconstruct the object image with an ultralow compression ratio of 1.5625%
under a physical model constraint, thus reducing the long sampling times. To
further suppress the diffraction-field effects, embedded with the angular
spectrum propagation (ASP) theory to model the diffraction of THz waves during
propagation, the network retrieves near-field information from the object,
enabling sub-diffraction imaging with a spatial resolution of ~{\lambda}0/7
({\lambda}0 = 833.3 {\mu}m at 0.36 THz) and eliminating the need for ultrathin
photomodulators. This approach provides an efficient solution for advancing THz
microscopic imaging and addressing other inverse imaging challenges.

</details>


### [294] [Pose Estimation for Intra-cardiac Echocardiography Catheter via AI-Based Anatomical Understanding](https://arxiv.org/abs/2505.07851)
*Jaeyoung Huh,Ankur Kapoor,Young-Ho Kim*

Main category: eess.IV

TL;DR: 论文提出了一种基于Vision Transformer的解剖感知姿态估计系统，用于从ICE图像中确定导管位置和方向，无需外部跟踪传感器。


<details>
  <summary>Details</summary>
Motivation: 现有导航方法依赖电磁跟踪或手动调整，易受干扰且依赖操作者经验，因此需要一种更高效、自动化的解决方案。

Method: 采用ViT模型处理ICE图像，通过16x16的嵌入块和Transformer网络预测位置和方向，使用MSE损失函数优化。

Result: 实验结果显示平均位置误差为9.48 mm，方向误差为(16.13°, 8.98°, 10.47°)，验证了模型准确性。

Conclusion: 该系统提高了手术效率，减少操作负担，支持无跟踪实时导管定位，可独立或补充现有系统。

Abstract: Intra-cardiac Echocardiography (ICE) plays a crucial role in
Electrophysiology (EP) and Structural Heart Disease (SHD) interventions by
providing high-resolution, real-time imaging of cardiac structures. However,
existing navigation methods rely on electromagnetic (EM) tracking, which is
susceptible to interference and position drift, or require manual adjustments
based on operator expertise. To overcome these limitations, we propose a novel
anatomy-aware pose estimation system that determines the ICE catheter position
and orientation solely from ICE images, eliminating the need for external
tracking sensors. Our approach leverages a Vision Transformer (ViT)-based deep
learning model, which captures spatial relationships between ICE images and
anatomical structures. The model is trained on a clinically acquired dataset of
851 subjects, including ICE images paired with position and orientation labels
normalized to the left atrium (LA) mesh. ICE images are patchified into 16x16
embeddings and processed through a transformer network, where a [CLS] token
independently predicts position and orientation via separate linear layers. The
model is optimized using a Mean Squared Error (MSE) loss function, balancing
positional and orientational accuracy. Experimental results demonstrate an
average positional error of 9.48 mm and orientation errors of (16.13 deg, 8.98
deg, 10.47 deg) across x, y, and z axes, confirming the model accuracy.
Qualitative assessments further validate alignment between predicted and target
views within 3D cardiac meshes. This AI-driven system enhances procedural
efficiency, reduces operator workload, and enables real-time ICE catheter
localization for tracking-free procedures. The proposed method can function
independently or complement existing mapping systems like CARTO, offering a
transformative approach to ICE-guided interventions.

</details>


### [295] [Computationally Efficient Diffusion Models in Medical Imaging: A Comprehensive Review](https://arxiv.org/abs/2505.07866)
*Abdullah,Tao Huang,Ickjai Lee,Euijoon Ahn*

Main category: eess.IV

TL;DR: 该论文探讨了扩散模型在生成式人工智能中的高效性和推理时间问题，重点介绍了DDPM、LDM和WDM三种模型在自然和医学影像中的应用及其局限性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高质量合成图像方面表现出色，但其高计算成本仍是挑战。研究旨在优化扩散模型的效率和推理时间，特别是在医学影像领域。

Method: 论文分类并分析了三种扩散模型（DDPM、LDM、WDM），探讨了它们在自然和医学影像中的计算复杂性填补作用。

Result: 扩散模型在医学影像中展现出快速、可靠和高质量的图像生成能力，但仍存在计算成本高的问题。

Conclusion: 论文总结了扩散模型的当前局限性，并提出了未来在医学影像领域的研究方向和机会。

Abstract: The diffusion model has recently emerged as a potent approach in computer
vision, demonstrating remarkable performances in the field of generative
artificial intelligence. Capable of producing high-quality synthetic images,
diffusion models have been successfully applied across a range of applications.
However, a significant challenge remains with the high computational cost
associated with training and generating these models. This study focuses on the
efficiency and inference time of diffusion-based generative models,
highlighting their applications in both natural and medical imaging. We present
the most recent advances in diffusion models by categorizing them into three
key models: the Denoising Diffusion Probabilistic Model (DDPM), the Latent
Diffusion Model (LDM), and the Wavelet Diffusion Model (WDM). These models play
a crucial role in medical imaging, where producing fast, reliable, and
high-quality medical images is essential for accurate analysis of abnormalities
and disease diagnosis. We first investigate the general framework of DDPM, LDM,
and WDM and discuss the computational complexity gap filled by these models in
natural and medical imaging. We then discuss the current limitations of these
models as well as the opportunities and future research directions in medical
imaging.

</details>


### [296] [Evaluation of UAV-Based RGB and Multispectral Vegetation Indices for Precision Agriculture in Palm Tree Cultivation](https://arxiv.org/abs/2505.07840)
*Alavikunhu Panthakkan,S M Anzar,K. Sherin,Saeed Al Mansoori,Hussain Al-Ahmad*

Main category: eess.IV

TL;DR: 研究评估了无人机多光谱和RGB成像在迪拜棕榈树种植区的植被健康监测效果，发现RGB植被指数性能与多光谱相当，成本更低。


<details>
  <summary>Details</summary>
Motivation: 精准农业需要高效、低成本的植被监测方法，以提升作物产量和可持续性。

Method: 使用无人机搭载多光谱和RGB传感器，计算NDVI、SAVI等指数，并与RGB指数（如VARI、MGRVI）对比。

Result: RGB指数在多光谱性能相近的情况下，显著降低了成本，适合大规模农业监测。

Conclusion: RGB成像结合多光谱数据，为精准农业提供了高效、经济的解决方案，推动数据驱动决策的普及。

Abstract: Precision farming relies on accurate vegetation monitoring to enhance crop
productivity and promote sustainable agricultural practices. This study
presents a comprehensive evaluation of UAV-based imaging for vegetation health
assessment in a palm tree cultivation region in Dubai. By comparing
multispectral and RGB image data, we demonstrate that RGBbased vegetation
indices offer performance comparable to more expensive multispectral indices,
providing a cost-effective alternative for large-scale agricultural monitoring.
Using UAVs equipped with multispectral sensors, indices such as NDVI and SAVI
were computed to categorize vegetation into healthy, moderate, and stressed
conditions. Simultaneously, RGB-based indices like VARI and MGRVI delivered
similar results in vegetation classification and stress detection. Our findings
highlight the practical benefits of integrating RGB imagery into precision
farming, reducing operational costs while maintaining accuracy in plant health
monitoring. This research underscores the potential of UAVbased RGB imaging as
a powerful tool for precision agriculture, enabling broader adoption of
data-driven decision-making in crop management. By leveraging the strengths of
both multispectral and RGB imaging, this work advances the state of UAV
applications in agriculture, paving the way for more efficient and scalable
farming solutions.

</details>


### [297] [Skeleton-Guided Diffusion Model for Accurate Foot X-ray Synthesis in Hallux Valgus Diagnosis](https://arxiv.org/abs/2505.08247)
*Midi Wan,Pengfei Li,Yizhuo Liang,Di Wu,Yushan Pan,Guangzhen Zhu,Hao Wang*

Main category: eess.IV

TL;DR: 提出了一种骨骼约束条件扩散模型（SCCDM）和足部评估方法KCC，用于提升医学图像合成的准确性和临床适用性。


<details>
  <summary>Details</summary>
Motivation: 解决现有X射线模型在图像保真度、骨骼一致性和物理约束方面的不足，特别是缺乏骨骼引导的扩散方法。

Method: 结合多尺度特征提取和注意力机制，引入骨骼约束条件扩散模型（SCCDM）和KCC评估方法。

Result: SSIM提升5.72%（0.794），PSNR提升18.34%（21.40 dB），结合KCC后平均得分0.85。

Conclusion: SCCDM和KCC显著提升了医学图像合成的质量和临床实用性。

Abstract: Medical image synthesis plays a crucial role in providing anatomically
accurate images for diagnosis and treatment. Hallux valgus, which affects
approximately 19% of the global population, requires frequent weight-bearing
X-rays for assessment, placing additional strain on both patients and
healthcare providers. Existing X-ray models often struggle to balance image
fidelity, skeletal consistency, and physical constraints, particularly in
diffusion-based methods that lack skeletal guidance. We propose the
Skeletal-Constrained Conditional Diffusion Model (SCCDM) and introduce KCC, a
foot evaluation method utilizing skeletal landmarks. SCCDM incorporates
multi-scale feature extraction and attention mechanisms, improving the
Structural Similarity Index (SSIM) by 5.72% (0.794) and Peak Signal-to-Noise
Ratio (PSNR) by 18.34% (21.40 dB). When combined with KCC, the model achieves
an average score of 0.85, demonstrating strong clinical applicability. The code
is available at https://github.com/midisec/SCCDM.

</details>


### [298] [An integrated language-vision foundation model for conversational diagnostics and triaging in primary eye care](https://arxiv.org/abs/2505.08414)
*Zhi Da Soh,Yang Bai,Kai Yu,Yang Zhou,Xiaofeng Lei,Sahil Thakur,Zann Lee,Lee Ching Linette Phang,Qingsheng Peng,Can Can Xue,Rachel Shujuan Chong,Quan V. Hoang,Lavanya Raghavan,Yih Chung Tham,Charumathi Sabanayagam,Wei-Chi Wu,Ming-Chih Ho,Jiangnan He,Preeti Gupta,Ecosse Lamoureux,Seang Mei Saw,Vinay Nangia,Songhomitra Panda-Jonas,Jie Xu,Ya Xing Wang,Xinxing Xu,Jost B. Jonas,Tien Yin Wong,Rick Siow Mong Goh,Yong Liu,Ching-Yu Cheng*

Main category: eess.IV

TL;DR: Meta-EyeFM是一个结合大型语言模型（LLM）和视觉基础模型（VFM）的多功能基础模型，用于眼科疾病评估。通过路由机制和低秩适应微调，实现了高精度的任务特定分析。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型多为任务特定且缺乏用户友好界面，Meta-EyeFM旨在解决这一问题，提供多功能、高精度的眼科疾病评估工具。

Method: 结合LLM和VFM，采用路由机制分配任务，并通过低秩适应微调VFM，实现疾病检测、严重程度区分和常见体征识别。

Result: 模型在路由任务中达到100%准确率，疾病检测准确率≥82.2%，严重程度区分≥89%，体征识别≥76%。性能优于Gemini-1.5-flash和ChatGPT-4o LMMs，与眼科医生相当。

Conclusion: Meta-EyeFM提供了高可用性和诊断性能，可作为初级眼科护理的决策支持工具或在线LLM用于眼底评估。

Abstract: Current deep learning models are mostly task specific and lack a
user-friendly interface to operate. We present Meta-EyeFM, a multi-function
foundation model that integrates a large language model (LLM) with vision
foundation models (VFMs) for ocular disease assessment. Meta-EyeFM leverages a
routing mechanism to enable accurate task-specific analysis based on text
queries. Using Low Rank Adaptation, we fine-tuned our VFMs to detect ocular and
systemic diseases, differentiate ocular disease severity, and identify common
ocular signs. The model achieved 100% accuracy in routing fundus images to
appropriate VFMs, which achieved $\ge$ 82.2% accuracy in disease detection,
$\ge$ 89% in severity differentiation, $\ge$ 76% in sign identification.
Meta-EyeFM was 11% to 43% more accurate than Gemini-1.5-flash and ChatGPT-4o
LMMs in detecting various eye diseases and comparable to an ophthalmologist.
This system offers enhanced usability and diagnostic performance, making it a
valuable decision support tool for primary eye care or an online LLM for fundus
evaluation.

</details>


### [299] [GNCAF: A GNN-based Neighboring Context Aggregation Framework for Tertiary Lymphoid Structures Semantic Segmentation in WSI](https://arxiv.org/abs/2505.08430)
*Lei Su*

Main category: eess.IV

TL;DR: 本文提出了一种基于GNN的邻近上下文聚合框架（GNCAF），用于端到端地分割WSI中的三级淋巴结构（TLS）区域和成熟阶段，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖细胞代理任务且需额外后处理，无法充分利用邻近上下文信息。

Method: GNCAF通过多跳邻近上下文聚合和自注意力机制，增强目标区域的分割能力。

Result: 在TCGA-COAD和INHOUSE-PAAD数据集上，GNCAF在mF1和mIoU上分别提升22.08%和26.57%。

Conclusion: GNCAF不仅适用于TLS分割，还可扩展至淋巴结转移分割任务。

Abstract: Tertiary lymphoid structures (TLS) are organized clusters of immune cells,
whose maturity and area can be quantified in whole slide image (WSI) for
various prognostic tasks. Existing methods for assessing these characteristics
typically rely on cell proxy tasks and require additional post-processing
steps. In this work, We focus on a novel task-TLS Semantic Segmentation
(TLS-SS)-which segments both the regions and maturation stages of TLS in WSI in
an end-to-end manner. Due to the extensive scale of WSI and patch-based
segmentation strategies, TLS-SS necessitates integrating from neighboring
patches to guide target patch (target) segmentation. Previous techniques often
employ on multi-resolution approaches, constraining the capacity to leverage
the broader neighboring context while tend to preserve coarse-grained
information. To address this, we propose a GNN-based Neighboring Context
Aggregation Framework (GNCAF), which progressively aggregates multi-hop
neighboring context from the target and employs a self-attention mechanism to
guide the segmentation of the target. GNCAF can be integrated with various
segmentation models to enhance their ability to perceive contextual information
outside of the patch. We build two TLS-SS datasets, called TCGA-COAD and
INHOUSE-PAAD, and make the former (comprising 225 WSIs and 5041 TLSs) publicly
available. Experiments on these datasets demonstrate the superiority of GNCAF,
achieving a maximum of 22.08% and 26.57% improvement in mF1 and mIoU,
respectively. Additionally, we also validate the task scalability of GNCAF on
segmentation of lymph node metastases.

</details>


### [300] [A portable diagnosis model for Keratoconus using a smartphone](https://arxiv.org/abs/2505.08616)
*Yifan Li,Myeongjun Kim,Yanjing Jin,Peter Ho,Jo Woon Chong*

Main category: eess.IV

TL;DR: 提出了一种基于智能手机的便携式诊断框架，用于检测圆锥角膜（KC）的不同阶段，并通过两阶段检测流程实现高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统Placido盘地形图依赖专业设备，限制了可及性，因此需要一种便携的替代方案。

Method: 使用智能手机屏幕显示Placido盘，通过两阶段检测流程（WSVM分类和彩色图可视化）分析角膜反射。

Result: 在模拟眼球模型上验证，分类准确率最高达92.93%，且在不同手机型号上表现稳定。

Conclusion: 该框架为KC诊断提供了一种便携且高效的工具，具有临床潜力。

Abstract: Keratoconus (KC) is a progressive corneal disorder characterized by localized
thinning and protrusion, leading to visual distortion. While Placido disc-based
topography remains a standard in clinical diagnostics, its dependence on
specialized equipment limits accessibility. In this paper, we propose a
portable, smartphone-based diagnostic framework that captures corneal
reflections of a Placido disc displayed on a phone screen and applies a
two-stage detection pipeline, then validate on 3D-printed emulated eyeball
models that simulate normal, moderate, and severe KC stages based on anterior
chamber depth (ACD). The first step of the two-stage detection pipeline is
classifying different stages of KC with features including height and width of
extracted reflections using weighted support vector machine (WSVM). It achieves
a maximum accuracy of 92.93%, and maintains over 90% accuracy across multiple
smartphone models, including the Galaxy Z Flip 3, iPhone 15 Pro, and iPhone 16
Pro. For the second step, we visualize the KC-affected protrusion regions on
the corneas with color maps based on inter-disc distance, that provides an
intuitive representation of disease severity and localization. Moreover, we
validate the ability of the extracted features to differentiate between KC
stages with ANOVA and Omega Squared, with significant p-values (e.g., $p <
10^{-6}$) and large effect sizes ($\\omega^2$ up to 0.8398) among classes.

</details>


### [301] [VIViT: Variable-Input Vision Transformer Framework for 3D MR Image Segmentation](https://arxiv.org/abs/2505.08693)
*Badhan Kumar Das,Ajay Singh,Gengyan Zhao,Han Liu,Thomas J. Re,Dorin Comaniciu,Eli Gibson,Andreas Maier*

Main category: eess.IV

TL;DR: 论文提出了一种基于Transformer的框架VIViT，用于处理多对比度MR数据的自监督预训练和分割微调，解决了现有方法对固定输入模态的限制。


<details>
  <summary>Details</summary>
Motivation: 现实中的MR研究通常包含不同对比度的数据，而现有深度学习方法需要固定的输入模态，限制了大规模预训练和下游任务的适应性。

Method: 提出VIViT框架，支持自监督预训练和分割微调，能够处理输入对比度的变化。

Result: 在脑梗死和脑肿瘤分割任务中，VIViT分别取得了0.624和0.883的平均Dice分数，优于现有CNN和ViT模型。

Conclusion: VIViT框架在异构MR数据任务中表现出更好的适应性和性能。

Abstract: Self-supervised pretrain techniques have been widely used to improve the
downstream tasks' performance. However, real-world magnetic resonance (MR)
studies usually consist of different sets of contrasts due to different
acquisition protocols, which poses challenges for the current deep learning
methods on large-scale pretrain and different downstream tasks with different
input requirements, since these methods typically require a fixed set of input
modalities or, contrasts. To address this challenge, we propose variable-input
ViT (VIViT), a transformer-based framework designed for self-supervised
pretraining and segmentation finetuning for variable contrasts in each study.
With this ability, our approach can maximize the data availability in pretrain,
and can transfer the learned knowledge from pretrain to downstream tasks
despite variations in input requirements. We validate our method on brain
infarct and brain tumor segmentation, where our method outperforms current CNN
and ViT-based models with a mean Dice score of 0.624 and 0.883 respectively.
These results highlight the efficacy of our design for better adaptability and
performance on tasks with real-world heterogeneous MR data.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [302] [Bridging Large Language Models and Single-Cell Transcriptomics in Dissecting Selective Motor Neuron Vulnerability](https://arxiv.org/abs/2505.07896)
*Douglas Jiang,Zilin Dai,Luxuan Zhang,Qiyi Yu,Haoqi Sun,Feng Tian*

Main category: q-bio.GN

TL;DR: 提出了一种利用NCBI Gene数据库的基因注释和大型语言模型生成生物上下文细胞嵌入的新框架，用于单细胞RNA测序数据的分析。


<details>
  <summary>Details</summary>
Motivation: 解决单细胞水平测序数据中细胞身份和功能理解的挑战。

Method: 通过表达水平排序基因，检索NCBI Gene描述，使用LLMs（如OpenAI和BioBERT）将其转化为向量嵌入，并通过表达加权平均生成细胞嵌入。

Result: 生成紧凑且语义丰富的细胞嵌入，支持下游应用如细胞类型聚类和轨迹推断。

Conclusion: 该多模态策略结合结构化生物数据和先进语言模型，提升了单细胞数据分析的效率和可解释性。

Abstract: Understanding cell identity and function through single-cell level sequencing
data remains a key challenge in computational biology. We present a novel
framework that leverages gene-specific textual annotations from the NCBI Gene
database to generate biologically contextualized cell embeddings. For each cell
in a single-cell RNA sequencing (scRNA-seq) dataset, we rank genes by
expression level, retrieve their NCBI Gene descriptions, and transform these
descriptions into vector embedding representations using large language models
(LLMs). The models used include OpenAI text-embedding-ada-002,
text-embedding-3-small, and text-embedding-3-large (Jan 2024), as well as
domain-specific models BioBERT and SciBERT. Embeddings are computed via an
expression-weighted average across the top N most highly expressed genes in
each cell, providing a compact, semantically rich representation. This
multimodal strategy bridges structured biological data with state-of-the-art
language modeling, enabling more interpretable downstream applications such as
cell-type clustering, cell vulnerability dissection, and trajectory inference.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [303] [SPP-SBL: Space-Power Prior Sparse Bayesian Learning for Block Sparse Recovery](https://arxiv.org/abs/2505.08518)
*Yanhao Zhang,Zhihan Zhu,Yong Xia*

Main category: math.OC

TL;DR: 本文提出了一种基于方差变换框架的统一方法SPP-SBL，通过空间功率先验和EM算法解决块稀疏信号恢复中的未知结构模式问题。


<details>
  <summary>Details</summary>
Motivation: 块稀疏信号的结构模式未知是稀疏信号恢复中的核心挑战，现有方法难以统一处理。

Method: 提出方差变换框架，结合空间功率先验和EM算法，开发SPP-SBL方法，解决空间耦合参数估计问题。

Result: SPP-SBL成功恢复多种复杂结构稀疏信号（如链式结构、多模式信号）和真实多模态信号（图像、音频），在多个指标上表现优异。

Conclusion: 学习空间耦合参数的相对值是捕捉未知块稀疏模式和提高恢复精度的关键。

Abstract: The recovery of block-sparse signals with unknown structural patterns remains
a fundamental challenge in structured sparse signal reconstruction. By
proposing a variance transformation framework, this paper unifies existing
pattern-based block sparse Bayesian learning methods, and introduces a novel
space power prior based on undirected graph models to adaptively capture the
unknown patterns of block-sparse signals. By combining the EM algorithm with
high-order equation root-solving, we develop a new structured sparse Bayesian
learning method, SPP-SBL, which effectively addresses the open problem of space
coupling parameter estimation in pattern-based methods. We further demonstrate
that learning the relative values of space coupling parameters is key to
capturing unknown block-sparse patterns and improving recovery accuracy.
Experiments validate that SPP-SBL successfully recovers various challenging
structured sparse signals (e.g., chain-structured signals and multi-pattern
sparse signals) and real-world multi-modal structured sparse signals (images,
audio), showing significant advantages in recovery accuracy across multiple
metrics.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [304] [Aitomia: Your Intelligent Assistant for AI-Driven Atomistic and Quantum Chemical Simulations](https://arxiv.org/abs/2505.08195)
*Jinming Hu,Hassan Nawaz,Yuting Rui,Lijie Chi,Arif Ullah,Pavlo O. Dral*

Main category: physics.comp-ph

TL;DR: Aitomia是一个由AI驱动的平台，旨在辅助原子和量子化学模拟，通过聊天机器人和AI代理帮助专家和非专家完成模拟设置、运行、监控和分析。


<details>
  <summary>Details</summary>
Motivation: 降低原子模拟的门槛，加速相关领域的研究与开发。

Method: 结合微调的开源大语言模型（LLMs）、基于规则的代理和检索增强生成（RAG）系统。

Result: 平台已部分公开，并计划集成到Aitomistic Hub和XACS在线计算服务中。

Conclusion: Aitomia有望简化原子模拟流程，推动相关领域的进展。

Abstract: We have developed Aitomia - a platform powered by AI to assist in performing
AI-driven atomistic and quantum chemical (QC) simulations. This intelligent
assistant platform is equipped with chatbots and AI agents to help experts and
guide non-experts in setting up and running the atomistic simulations,
monitoring their computation status, analyzing the simulation results, and
summarizing them for the user in text and graphical forms. We achieve these
goals by exploiting fine-tuned open-source large language models (LLMs),
rule-based agents, and a retrieval-augmented generation (RAG) system. Aitomia
leverages the versatility of our MLatom ecosystem for AI-enhanced computational
chemistry. This intelligent assistant is going to be integrated into the
Aitomistic Hub and XACS online computing services, with some functionality
already publicly available as described at http://mlatom.com/aitomia. Aitomia
is expected to lower the barrier to performing atomistic simulations,
accelerating research and development in the relevant fields.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [305] [NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition](https://arxiv.org/abs/2505.08052)
*Kourosh Shahnazari,Seyed Moein Ayyoubzadeh*

Main category: cs.SI

TL;DR: 该研究通过构建多维相似性网络，模拟古典波斯诗人的影响力动态，结合语义、词汇、风格、主题和韵律特征，识别关键诗人、风格中心和桥梁诗人，并通过社区检测算法揭示文学流派。


<details>
  <summary>Details</summary>
Motivation: 旨在通过计算模型揭示波斯诗人之间的相互影响，区分经典重要性与互文影响，突出结构上重要但知名度较低的诗人。

Method: 使用Ganjoor语料库构建数据集，通过加权相似性矩阵生成聚合图，计算多种中心性指标，并应用Louvain社区检测算法。

Result: 发现了与已知文学流派（如Sabk-e Hindi、Sabk-e Khorasani等）高度一致的诗人集群，提供了数据驱动的波斯文学新视角。

Conclusion: 结合计算语言学与文学研究，提出了可解释且可扩展的诗歌传统模型，为数字人文研究提供了新方向。

Abstract: This study formalizes a computational model to simulate classical Persian
poets' dynamics of influence through constructing a multi-dimensional
similarity network. Using a rigorously curated dataset based on Ganjoor's
corpus, we draw upon semantic, lexical, stylistic, thematic, and metrical
features to demarcate each poet's corpus. Each is contained within weighted
similarity matrices, which are then appended to generate an aggregate graph
showing poet-to-poet influence. Further network investigation is carried out to
identify key poets, style hubs, and bridging poets by calculating degree,
closeness, betweenness, eigenvector, and Katz centrality measures. Further, for
typological insight, we use the Louvain community detection algorithm to
demarcate clusters of poets sharing both style and theme coherence, which
correspond closely to acknowledged schools of literature like Sabk-e Hindi,
Sabk-e Khorasani, and the Bazgasht-e Adabi phenomenon. Our findings provide a
new data-driven view of Persian literature distinguished between canonical
significance and interextual influence, thus highlighting relatively
lesser-known figures who hold great structural significance. Combining
computational linguistics with literary study, this paper produces an
interpretable and scalable model for poetic tradition, enabling retrospective
reflection as well as forward-looking research within digital humanities.

</details>


### [306] [The Truth Becomes Clearer Through Debate! Multi-Agent Systems with Large Language Models Unmask Fake News](https://arxiv.org/abs/2505.08532)
*Yuhan Liu,Yuxuan Liu,Xiaoqing Zhang,Xiuying Chen,Rui Yan*

Main category: cs.SI

TL;DR: 论文提出了一种名为TruEDebate（TED）的多智能体系统，利用大语言模型（LLMs）通过辩论过程提升假新闻检测的可解释性和效果。


<details>
  <summary>Details</summary>
Motivation: 现有假新闻检测方法要么可解释性差、泛化能力有限，要么未能充分利用LLMs的推理能力。

Method: TED采用辩论流程，包括DebateFlow Agents（组织辩论团队）和InsightFlow Agents（总结与分析辩论内容）。

Result: 通过模拟人类辩论过程，TED实现了对新闻内容的全面评估。

Conclusion: TED通过辩论机制显著提升了假新闻检测的可解释性和有效性。

Abstract: In today's digital environment, the rapid propagation of fake news via social
networks poses significant social challenges. Most existing detection methods
either employ traditional classification models, which suffer from low
interpretability and limited generalization capabilities, or craft specific
prompts for large language models (LLMs) to produce explanations and results
directly, failing to leverage LLMs' reasoning abilities fully. Inspired by the
saying that "truth becomes clearer through debate," our study introduces a
novel multi-agent system with LLMs named TruEDebate (TED) to enhance the
interpretability and effectiveness of fake news detection. TED employs a
rigorous debate process inspired by formal debate settings. Central to our
approach are two innovative components: the DebateFlow Agents and the
InsightFlow Agents. The DebateFlow Agents organize agents into two teams, where
one supports and the other challenges the truth of the news. These agents
engage in opening statements, cross-examination, rebuttal, and closing
statements, simulating a rigorous debate process akin to human discourse
analysis, allowing for a thorough evaluation of news content. Concurrently, the
InsightFlow Agents consist of two specialized sub-agents: the Synthesis Agent
and the Analysis Agent. The Synthesis Agent summarizes the debates and provides
an overarching viewpoint, ensuring a coherent and comprehensive evaluation. The
Analysis Agent, which includes a role-aware encoder and a debate graph,
integrates role embeddings and models the interactions between debate roles and
arguments using an attention mechanism, providing the final judgment.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [307] [SciCom Wiki: Fact-Checking and FAIR Knowledge Distribution for Scientific Videos and Podcasts](https://arxiv.org/abs/2505.07912)
*Tim Wittenborg,Constantin Sebastian Tremel,Niklas Stehr,Oliver Karras,Markus Stocker,Sören Auer*

Main category: cs.DL

TL;DR: 论文提出SciCom Wiki平台，支持科学传播知识基础设施（SciCom KI），通过FAIR媒体表示和事实核查工具应对信息泛滥。


<details>
  <summary>Details</summary>
Motivation: 民主社会需要可靠信息，但视频和播客作为传播媒介易传播错误信息，现有SciCom KI系统碎片化且无法应对内容泛滥。

Method: 基于Wikibase构建开源平台，通过53名利益相关者调查、11次访谈和14名参与者评估原型，开发神经符号计算事实核查工具。

Result: 平台和工具被验证为满足需求，但SciCom KI在FAIR知识和协作系统方面仍不足。

Conclusion: SciCom Wiki可作为核心知识节点，但需协作应对信息泛滥。

Abstract: Democratic societies need accessible, reliable information. Videos and
Podcasts have established themselves as the medium of choice for civic
dissemination, but also as carriers of misinformation. The emerging Science
Communication Knowledge Infrastructure (SciCom KI) curating non-textual media
is still fragmented and not adequately equipped to scale against the content
flood. Our work sets out to support the SciCom KI with a central, collaborative
platform, the SciCom Wiki, to facilitate FAIR (findable, accessible,
interoperable, reusable) media representation and the fact-checking of their
content, particularly for videos and podcasts. Building an open-source service
system centered around Wikibase, we survey requirements from 53 stakeholders,
refine these in 11 interviews, and evaluate our prototype based on these
requirements with another 14 participants. To address the most requested
feature, fact-checking, we developed a neurosymbolic computational
fact-checking approach, converting heterogenous media into knowledge graphs.
This increases machine-readability and allows comparing statements against
equally represented ground-truth. Our computational fact-checking tool was
iteratively evaluated through 10 expert interviews, a public user survey with
43 participants verified the necessity and usability of our tool. Overall, our
findings identified several needs to systematically support the SciCom KI. The
SciCom Wiki, as a FAIR digital library complementing our neurosymbolic
computational fact-checking framework, was found suitable to address the raised
requirements. Further, we identified that the SciCom KI is severely
underdeveloped regarding FAIR knowledge and related systems facilitating its
collaborative creation and curation. Our system can provide a central knowledge
node, yet a collaborative effort is required to scale against the imminent
(mis-)information flood.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [308] [Linear to Neural Networks Regression: QSPR of Drugs via Degree-Distance Indices](https://arxiv.org/abs/2505.07821)
*M. J. Nadjafi Arani,S. Sorgun,M. Mirzargar*

Main category: q-bio.BM

TL;DR: 该研究通过QSPR分析和机器学习技术，探索药物分子物理性质与拓扑指数之间的相关性，提出了一种结合度-距离拓扑指数和原子属性的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统药物设计研究主要关注基于度的拓扑指数，而本研究旨在通过引入度-距离拓扑指数和多种原子属性，提高分子性质预测的准确性。

Method: 研究计算了166种药物分子的度-距离拓扑指数，并结合六种原子属性，使用线性模型（线性回归、Lasso、岭回归）和非线性方法（随机森林、XGBoost、神经网络）进行预测。

Result: 结果表明，这些拓扑指数能有效预测特定物理化学性质，并验证了计算方法在分子性质估计中的实用性。

Conclusion: 研究为药物发现提供了一种创新视角，展示了拓扑指数与机器学习结合的潜力，有助于优化化学信息学研究的资源利用。

Abstract: This study conducts a Quantitative Structure Property Relationship (QSPR)
analysis to explore the correlation between the physical properties of drug
molecules and their topological indices using machine learning techniques.
While prior studies in drug design have focused on degree-based topological
indices, this work analyzes a dataset of 166 drug molecules by computing
degree-distance-based topological indices, incorporating vertex-edge weightings
with respect to different six atomic properties (atomic number, atomic radius,
atomic mass, density, electronegativity, ionization). Both linear models
(Linear Regression, Lasso, and Ridge Regression) and nonlinear approaches
(Random Forest, XGBoost, and Neural Networks) were employed to predict
molecular properties. The results demonstrate the effectiveness of these
indices in predicting specific physicochemical properties and underscore the
practical relevance of computational methods in molecular property estimation.
The study provides an innovative perspective on integrating topological indices
with machine learning to enhance predictive accuracy, highlighting their
potential application in drug discovery and development processes. This
predictive may also explain that establishing a reliable relationship between
topological indices and physical properties enables chemists to gain
preliminary insights into molecular behavior before conducting experimental
analyses, thereby optimizing resource utilization in cheminformatics research.

</details>


### [309] [Generative Molecular Design with Steerable and Granular Synthesizability Control](https://arxiv.org/abs/2505.08774)
*Jeff Guo,Víctor Sabanza-Gil,Zlatko Jončev,Jeremy S. Luterbacher,Philippe Schwaller*

Main category: q-bio.BM

TL;DR: 提出了一种可调控的小分子生成设计框架，通过强化学习实现合成路径的灵活控制。


<details>
  <summary>Details</summary>
Motivation: 解决小分子生成设计中合成路径灵活性和可控性的问题。

Method: 采用预训练的通用分子生成模型，结合强化学习，实现多参数优化和反应约束。

Result: 生成分子满足反应约束，合成路径匹配率超过90%，并在大规模虚拟筛选中表现高效。

Conclusion: 证明了预训练模型在强化学习下能生成满足严格合成约束的优化小分子。

Abstract: Synthesizability in small molecule generative design remains a bottleneck.
Existing works that do consider synthesizability can output predicted synthesis
routes for generated molecules. However, there has been minimal attention in
addressing the ease of synthesis and enabling flexibility to incorporate
desired reaction constraints. In this work, we propose a small molecule
generative design framework that enables steerable and granular
synthesizability control. Generated molecules satisfy arbitrary multi-parameter
optimization objectives with predicted synthesis routes containing pre-defined
allowed reactions, while optionally avoiding others. One can also enforce that
all reactions belong to a pre-defined set. We show the capability to
mix-and-match these reaction constraints across the most common medicinal
chemistry transformations. Next, we show how our framework can be used to
valorize industrial byproducts towards de novo optimized molecules. Going
further, we demonstrate how granular control over synthesizability constraints
can loosely mimic virtual screening of ultra-large make-on-demand libraries.
Using only a single GPU, we generate and dock 15k molecules to identify
promising candidates in Freedom 4.0 constituting 142B make-on-demand molecules
(assessing only 0.00001% of the library). Generated molecules satisfying the
reaction constraints have > 90% exact match rate. Lastly, we benchmark our
framework against recent synthesizability-constrained generative models and
demonstrate the highest sample efficiency even when imposing the additional
constraint that all molecules must be synthesizable from a single reaction
type. The main theme is demonstrating that a pre-trained generalist molecular
generative model can be incentivized to generate property-optimized small
molecules under challenging synthesizability constraints through reinforcement
learning.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [310] [Explainable Artificial Intelligence Techniques for Software Development Lifecycle: A Phase-specific Survey](https://arxiv.org/abs/2505.07058)
*Lakshit Arora,Sanjay Surendranath Girija,Shashank Kapoor,Aman Raj,Dipen Pradhan,Ankit Shetgaonkar*

Main category: cs.SE

TL;DR: 本文综述了可解释人工智能（XAI）在软件开发生命周期（SDLC）各阶段的应用，填补了XAI在软件工程中的研究空白，旨在提升AI模型的透明度和实用性。


<details>
  <summary>Details</summary>
Motivation: AI的‘黑盒问题’限制了其信任度和广泛应用，XAI通过提高透明度和可解释性来解决这一问题。目前XAI在软件工程中的应用主要集中在维护阶段，其他阶段研究较少。

Method: 通过文献综述，分析了多种XAI技术（如LIME、SHAP、规则提取等）在SDLC各阶段（需求、设计、测试、部署、演化）的应用。

Result: 研究发现68%的XAI研究集中在维护阶段，而需求和管理阶段仅占8%。本文首次全面综述了XAI在SDLC各阶段的应用。

Conclusion: 本文填补了XAI在软件工程中的研究空白，为AI驱动的软件开发提供了实用指导，推动了XAI在软件工程中的广泛应用。

Abstract: Artificial Intelligence (AI) is rapidly expanding and integrating more into
daily life to automate tasks, guide decision making, and enhance efficiency.
However, complex AI models, which make decisions without providing clear
explanations (known as the "black-box problem"), currently restrict trust and
widespread adoption of AI. Explainable Artificial Intelligence (XAI) has
emerged to address the black-box problem of making AI systems more
interpretable and transparent so stakeholders can trust, verify, and act upon
AI-based outcomes. Researchers have developed various techniques to foster XAI
in the Software Development Lifecycle. However, there are gaps in applying XAI
techniques in the Software Engineering phases. Literature review shows that 68%
of XAI in Software Engineering research is focused on maintenance as opposed to
8% on software management and requirements. In this paper, we present a
comprehensive survey of the applications of XAI methods such as concept-based
explanations, Local Interpretable Model-agnostic Explanations (LIME), SHapley
Additive exPlanations (SHAP), rule extraction, attention mechanisms,
counterfactual explanations, and example-based explanations to the different
phases of the Software Development Life Cycle (SDLC), including requirements
elicitation, design and development, testing and deployment, and evolution. To
the best of our knowledge, this paper presents the first comprehensive survey
of XAI techniques for every phase of the Software Development Life Cycle
(SDLC). This survey aims to promote explainable AI in Software Engineering and
facilitate the practical application of complex AI models in AI-driven software
development.

</details>


### [311] [Moving From Monolithic To Microservices Architecture for Multi-Agent Systems](https://arxiv.org/abs/2505.07838)
*Muskaan Goyal,Pranav Bhasin*

Main category: cs.SE

TL;DR: 本文探讨了从单体架构到微服务架构在多智能体系统（MAS）中的演变，分析了传统单体MAS的局限性和微服务架构的优势，并研究了相关通信协议和新兴架构模式。


<details>
  <summary>Details</summary>
Motivation: 随着微服务架构在软件开发中的成功应用，研究其在复杂多智能体系统（MAS）中的适用性变得重要，以解决传统单体架构的可扩展性和维护性问题。

Method: 通过文献综述和比较分析，探讨了微服务架构在MAS中的应用，包括通信协议（如ACL、MCP、A2A）和新兴架构模式。

Result: 研究发现微服务架构能显著提升MAS的可扩展性和维护性，但也面临设计挑战和通信复杂性。

Conclusion: 微服务架构为MAS提供了新的可能性，但需权衡其优势与挑战，未来研究应关注优化设计和协议。

Abstract: The transition from monolithic to microservices architecture revolutionized
software development by improving scalability and maintainability. This
paradigm shift is now becoming relevant for complex multi-agent systems (MAS).
This review article explores the evolution from monolithic architecture to
microservices architecture in the specific context of MAS. It will highlight
the limitations of traditional monolithic MAS and the benefits of adopting a
microservices-based approach. The article further examines the core
architectural principles and communication protocols, including Agent
Communication Languages (ACLs), the Model Context Protocol (MCP), and the
Application-to-Application (A2A) protocol. The article identifies emerging
architectural patterns, design challenges, and considerations through a
comparative lens of the paradigm shift.

</details>


### [312] [SweRank: Software Issue Localization with Code Ranking](https://arxiv.org/abs/2505.07849)
*Revanth Gangi Reddy,Tarun Suresh,JaeHyeok Doo,Ye Liu,Xuan Phi Nguyen,Yingbo Zhou,Semih Yavuz,Caiming Xiong,Heng Ji,Shafiq Joty*

Main category: cs.SE

TL;DR: SweRank是一种高效的检索-重排序框架，用于软件问题定位，结合了SweLoc数据集，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 软件问题定位耗时且现有方法效率低或成本高，需改进。

Method: 提出SweRank框架，利用SweLoc数据集训练，结合检索和重排序技术。

Result: 在SWE-Bench-Lite和LocBench上表现优异，优于传统模型和基于代理的系统。

Conclusion: SweRank高效且有效，SweLoc数据集对社区有重要价值。

Abstract: Software issue localization, the task of identifying the precise code
locations (files, classes, or functions) relevant to a natural language issue
description (e.g., bug report, feature request), is a critical yet
time-consuming aspect of software development. While recent LLM-based agentic
approaches demonstrate promise, they often incur significant latency and cost
due to complex multi-step reasoning and relying on closed-source LLMs.
Alternatively, traditional code ranking models, typically optimized for
query-to-code or code-to-code retrieval, struggle with the verbose and
failure-descriptive nature of issue localization queries. To bridge this gap,
we introduce SweRank, an efficient and effective retrieve-and-rerank framework
for software issue localization. To facilitate training, we construct SweLoc, a
large-scale dataset curated from public GitHub repositories, featuring
real-world issue descriptions paired with corresponding code modifications.
Empirical results on SWE-Bench-Lite and LocBench show that SweRank achieves
state-of-the-art performance, outperforming both prior ranking models and
costly agent-based systems using closed-source LLMs like Claude-3.5. Further,
we demonstrate SweLoc's utility in enhancing various existing retriever and
reranker models for issue localization, establishing the dataset as a valuable
resource for the community.

</details>


### [313] [Leveraging AI for Productive and Trustworthy HPC Software: Challenges and Research Directions](https://arxiv.org/abs/2505.08135)
*Keita Teranishi,Harshitha Menon,William F. Godoy,Prasanna Balaprakash,David Bau,Tal Ben-Nun,Abhinav Bathele,Franz Franchetti,Michael Franusich,Todd Gamblin,Giorgis Georgakoudis,Tom Goldstein,Arjun Guha,Steven Hahn,Costin Iancu,Zheming Jin,Terry Jones,Tze Meng Low,Het Mankad,Narasinga Rao Miniskar,Mohammad Alaul Haque Monil,Daniel Nichols,Konstantinos Parasyris,Swaroop Pophale,Pedro Valero-Lara,Jeffrey S. Vetter,Samuel Williams,Aaron Young*

Main category: cs.SE

TL;DR: 探讨AI如何革新高性能计算（HPC）软件开发，提出挑战和研究方向。


<details>
  <summary>Details</summary>
Motivation: AI技术（尤其是大语言模型）已改变软件开发，但HPC软件作为高度专业化的领域，其AI应用仍面临挑战。

Method: 通过美国能源部资助的Ellora和Durban项目，研究AI在HPC软件开发中的应用。

Result: 提出利用AI技术推动HPC软件开发的研究方向。

Conclusion: AI有望革新HPC软件开发，但需解决其独特挑战。

Abstract: We discuss the challenges and propose research directions for using AI to
revolutionize the development of high-performance computing (HPC) software. AI
technologies, in particular large language models, have transformed every
aspect of software development. For its part, HPC software is recognized as a
highly specialized scientific field of its own. We discuss the challenges
associated with leveraging state-of-the-art AI technologies to develop such a
unique and niche class of software and outline our research directions in the
two US Department of Energy--funded projects for advancing HPC Software via AI:
Ellora and Durban.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [314] [Monocular Online Reconstruction with Enhanced Detail Preservation](https://arxiv.org/abs/2505.07887)
*Songyin Wu,Zhaoyang Lv,Yufeng Zhu,Duncan Frost,Zhengqin Li,Ling-Qi Yan,Carl Ren,Richard Newcombe,Zhao Dong*

Main category: cs.GR

TL;DR: 提出了一种基于3D高斯分布的在线密集映射框架，用于从单目图像流中重建逼真细节。


<details>
  <summary>Details</summary>
Motivation: 解决单目在线重建中的两个关键挑战：无需依赖深度图的高斯分布，以及确保重建地图的局部和全局一致性。

Method: 引入两个关键模块：分层高斯管理模块用于有效分布高斯，全局一致性优化模块用于保持多尺度对齐和连贯性；并提出多级占用哈希体素（MOHV）结构。

Result: 与现有RGB-only和RGB-D方法相比，该框架在计算高效的同时实现了更优的重建质量。

Conclusion: 该框架不仅保留了细节和结构完整性，还能与多种跟踪系统无缝集成，具有通用性和可扩展性。

Abstract: We propose an online 3D Gaussian-based dense mapping framework for
photorealistic details reconstruction from a monocular image stream. Our
approach addresses two key challenges in monocular online reconstruction:
distributing Gaussians without relying on depth maps and ensuring both local
and global consistency in the reconstructed maps. To achieve this, we introduce
two key modules: the Hierarchical Gaussian Management Module for effective
Gaussian distribution and the Global Consistency Optimization Module for
maintaining alignment and coherence at all scales. In addition, we present the
Multi-level Occupancy Hash Voxels (MOHV), a structure that regularizes
Gaussians for capturing details across multiple levels of granularity. MOHV
ensures accurate reconstruction of both fine and coarse geometries and
textures, preserving intricate details while maintaining overall structural
integrity. Compared to state-of-the-art RGB-only and even RGB-D methods, our
framework achieves superior reconstruction quality with high computational
efficiency. Moreover, it integrates seamlessly with various tracking systems,
ensuring generality and scalability.

</details>


### [315] [PosterO: Structuring Layout Trees to Enable Language Models in Generalized Content-Aware Layout Generation](https://arxiv.org/abs/2505.07843)
*HsiaoYuan Hsu,Yuxin Peng*

Main category: cs.GR

TL;DR: PosterO提出了一种基于布局的方法，利用大语言模型（LLMs）生成多样化的海报布局，解决了现有方法在通用场景下布局多样性和形状变化元素处理的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在有限训练数据下专注于图像中心增强，忽视了布局多样性和通用设计意图的适应性。

Method: 通过SVG语言将布局结构化为树，结合设计意图向量化和层次节点表示，利用LLMs进行上下文学习预测新布局树。

Result: PosterO在多个基准测试中实现了最先进的性能，生成了视觉吸引力的布局。

Conclusion: PosterO展示了在通用场景下的强大能力，并通过PStylish7数据集为未来研究提供了挑战。

Abstract: In poster design, content-aware layout generation is crucial for
automatically arranging visual-textual elements on the given image. With
limited training data, existing work focused on image-centric enhancement.
However, this neglects the diversity of layouts and fails to cope with
shape-variant elements or diverse design intents in generalized settings. To
this end, we proposed a layout-centric approach that leverages layout knowledge
implicit in large language models (LLMs) to create posters for omnifarious
purposes, hence the name PosterO. Specifically, it structures layouts from
datasets as trees in SVG language by universal shape, design intent
vectorization, and hierarchical node representation. Then, it applies LLMs
during inference to predict new layout trees by in-context learning with
intent-aligned example selection. After layout trees are generated, we can
seamlessly realize them into poster designs by editing the chat with LLMs.
Extensive experimental results have demonstrated that PosterO can generate
visually appealing layouts for given images, achieving new state-of-the-art
performance across various benchmarks. To further explore PosterO's abilities
under the generalized settings, we built PStylish7, the first dataset with
multi-purpose posters and various-shaped elements, further offering a
challenging test for advanced research.

</details>


### [316] [ACT-R: Adaptive Camera Trajectories for 3D Reconstruction from Single Image](https://arxiv.org/abs/2505.08239)
*Yizhi Wang,Mingrui Zhao,Ali Mahdavi-Amiri,Hao Zhang*

Main category: cs.GR

TL;DR: 提出自适应视图规划方法，通过动态相机轨迹优化多视图合成，提升单视图3D重建的遮挡揭示和3D一致性。


<details>
  <summary>Details</summary>
Motivation: 解决传统多视图合成中无序视图生成导致的遮挡问题和3D不一致性，通过自适应相机轨迹优化视图序列。

Method: 计算自适应相机轨迹（ACT），最大化遮挡区域可见性，结合视频扩散模型生成新视图，再通过多视图3D重建模型完成最终重建。

Result: 在GSO数据集上显著提升3D重建效果，定量和定性均优于现有方法。

Conclusion: 自适应视图规划有效提升遮挡揭示和3D一致性，无需运行时训练，高效且性能优越。

Abstract: We introduce adaptive view planning to multi-view synthesis, aiming to
improve both occlusion revelation and 3D consistency for single-view 3D
reconstruction. Instead of generating an unordered set of views independently
or simultaneously, we generate a sequence of views, leveraging temporal
consistency to enhance 3D coherence. Most importantly, our view sequence is not
determined by a pre-determined camera setup. Instead, we compute an adaptive
camera trajectory (ACT), specifically, an orbit of camera views, which
maximizes the visibility of occluded regions of the 3D object to be
reconstructed. Once the best orbit is found, we feed it to a video diffusion
model to generate novel views around the orbit, which in turn, are passed to a
multi-view 3D reconstruction model to obtain the final reconstruction. Our
multi-view synthesis pipeline is quite efficient since it involves no run-time
training/optimization, only forward inferences by applying the pre-trained
models for occlusion analysis and multi-view synthesis. Our method predicts
camera trajectories that reveal occlusions effectively and produce consistent
novel views, significantly improving 3D reconstruction over SOTA on the unseen
GSO dataset, both quantitatively and qualitatively.

</details>


### [317] [M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis](https://arxiv.org/abs/2505.08293)
*Zhizhuo Yin,Yuk Hang Tsui,Pan Hui*

Main category: cs.GR

TL;DR: 论文提出了一种名为M3G的新框架，用于从音频生成全身手势，解决了现有系统因固定粒度而无法建模不同手势模式的问题。


<details>
  <summary>Details</summary>
Motivation: 现有系统在生成全身手势时，由于手势标记的固定粒度，无法有效建模不同手势模式的变化。

Method: 提出M3G框架，包括多粒度VQ-VAE（MGVQ-VAE）和多粒度标记预测器，用于从音频中提取信息并预测手势标记。

Result: 实验表明，M3G在生成自然且富有表现力的全身手势方面优于现有方法。

Conclusion: M3G通过多粒度建模解决了手势生成的挑战，提升了生成效果。

Abstract: Generating full-body human gestures encompassing face, body, hands, and
global movements from audio is a valuable yet challenging task in virtual
avatar creation. Previous systems focused on tokenizing the human gestures
framewisely and predicting the tokens of each frame from the input audio.
However, one observation is that the number of frames required for a complete
expressive human gesture, defined as granularity, varies among different human
gesture patterns. Existing systems fail to model these gesture patterns due to
the fixed granularity of their gesture tokens. To solve this problem, we
propose a novel framework named Multi-Granular Gesture Generator (M3G) for
audio-driven holistic gesture generation. In M3G, we propose a novel
Multi-Granular VQ-VAE (MGVQ-VAE) to tokenize motion patterns and reconstruct
motion sequences from different temporal granularities. Subsequently, we
proposed a multi-granular token predictor that extracts multi-granular
information from audio and predicts the corresponding motion tokens. Then M3G
reconstructs the human gestures from the predicted tokens using the MGVQ-VAE.
Both objective and subjective experiments demonstrate that our proposed M3G
framework outperforms the state-of-the-art methods in terms of generating
natural and expressive full-body human gestures.

</details>


### [318] [Claycode: Stylable and Deformable 2D Scannable Codes](https://arxiv.org/abs/2505.08666)
*Marco Maida,Alberto Crescini,Marco Perronet,Elena Camuffo*

Main category: cs.GR

TL;DR: Claycode是一种新型2D可扫描码，支持高度样式化和变形，基于树结构编码，优于传统二维码。


<details>
  <summary>Details</summary>
Motivation: 传统矩阵式二维码（如QR码）在样式化和变形场景中表现不佳，Claycode旨在解决这一问题。

Method: 通过树结构编码信息，将比特映射到拓扑树中，并以目标多边形边界内的颜色区域嵌套形式呈现。解码时从摄像头流中实时提取。

Result: Claycode在高度变形场景中表现优异，功能不受样式化影响，优于传统2D码。

Conclusion: Claycode为可扫描码提供了更高的灵活性和鲁棒性，适用于需要样式化和变形的场景。

Abstract: This paper introduces Claycode, a novel 2D scannable code designed for
extensive stylization and deformation. Unlike traditional matrix-based codes
(e.g., QR codes), Claycodes encode their message in a tree structure. During
the encoding process, bits are mapped into a topology tree, which is then
depicted as a nesting of color regions drawn within the boundaries of a target
polygon shape. When decoding, Claycodes are extracted and interpreted in
real-time from a camera stream. We detail the end-to-end pipeline and show that
Claycodes allow for extensive stylization without compromising their
functionality. We then empirically demonstrate Claycode's high tolerance to
heavy deformations, outperforming traditional 2D scannable codes in scenarios
where they typically fail.

</details>


### [319] [CAD-Coder:Text-Guided CAD Files Code Generation](https://arxiv.org/abs/2505.08686)
*Changqi He,Shuhan Zhang,Liguo Zhang,Jiajun Miao*

Main category: cs.GR

TL;DR: CAD-Coder是一个将自然语言指令转换为CAD脚本代码的框架，生成可编辑的CAD文件，解决了现有生成方法缺乏交互性和几何标注的问题。


<details>
  <summary>Details</summary>
Motivation: 传统CAD依赖专家手工绘制或修改现有库文件，无法快速个性化；现有生成方法缺乏交互性和几何标注，限制了实际应用。

Method: 提出CAD-Coder框架，通过自然语言指令生成CAD脚本代码，构建包含29,130个Dxf文件及其脚本代码的数据集。

Result: 在多种2D/3D CAD生成任务中表现优异，提供独特的可编辑草图和几何标注功能。

Conclusion: CAD-Coder为交互式生成CAD提供了高效解决方案，具有实际应用潜力。

Abstract: Computer-aided design (CAD) is a way to digitally create 2D drawings and 3D
models of real-world products. Traditional CAD typically relies on hand-drawing
by experts or modifications of existing library files, which doesn't allow for
rapid personalization. With the emergence of generative artificial
intelligence, convenient and efficient personalized CAD generation has become
possible. However, existing generative methods typically produce outputs that
lack interactive editability and geometric annotations, limiting their
practical applications in manufacturing. To enable interactive generative CAD,
we propose CAD-Coder, a framework that transforms natural language instructions
into CAD script codes, which can be executed in Python environments to generate
human-editable CAD files (.Dxf). To facilitate the generation of editable CAD
sketches with annotation information, we construct a comprehensive dataset
comprising 29,130 Dxf files with their corresponding script codes, where each
sketch preserves both editability and geometric annotations. We evaluate
CAD-Coder on various 2D/3D CAD generation tasks against existing methods,
demonstrating superior interactive capabilities while uniquely providing
editable sketches with geometric annotations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [320] [AI-Based Crypto Tokens: The Illusion of Decentralized AI?](https://arxiv.org/abs/2505.07828)
*Rischan Mafrur*

Main category: cs.DC

TL;DR: 本文综述了基于区块链和人工智能（AI）的AI代币项目，分析了其技术架构、代币用途、共识机制和商业模式，并指出了当前实现中的技术局限性和商业模式问题。


<details>
  <summary>Details</summary>
Motivation: 研究AI代币如何超越传统中心化AI服务，探索其在区块链生态系统中的实际价值。

Method: 通过全面审查领先的AI代币项目，评估其技术架构、商业模式及实际应用效果。

Result: 发现当前AI代币项目在技术上依赖链下计算，链上智能有限，且存在可扩展性问题；商业模式上多为中心化AI服务的简单复制。

Conclusion: 尽管新兴技术可能推动去中心化AI系统发展，但当前AI代币实现与承诺之间仍有显著差距，需更务实的评估与改进。

Abstract: The convergence of blockchain and artificial intelligence (AI) has led to the
emergence of AI-based tokens, which are cryptographic assets designed to power
decentralized AI platforms and services. This paper provides a comprehensive
review of leading AI-token projects, examining their technical architectures,
token utilities, consensus mechanisms, and underlying business models. We
explore how these tokens operate across various blockchain ecosystems and
assess the extent to which they offer value beyond traditional centralized AI
services. Based on this assessment, our analysis identifies several core
limitations. From a technical perspective, many platforms depend extensively on
off-chain computation, exhibit limited capabilities for on-chain intelligence,
and encounter significant scalability challenges. From a business perspective,
many models appear to replicate centralized AI service structures, simply
adding token-based payment and governance layers without delivering truly novel
value. In light of these challenges, we also examine emerging developments that
may shape the next phase of decentralized AI systems. These include approaches
for on-chain verification of AI outputs, blockchain-enabled federated learning,
and more robust incentive frameworks. Collectively, while emerging innovations
offer pathways to strengthen decentralized AI ecosystems, significant gaps
remain between the promises and the realities of current AI-token
implementations. Our findings contribute to a growing body of research at the
intersection of AI and blockchain, highlighting the need for critical
evaluation and more grounded approaches as the field continues to evolve.

</details>


### [321] [Patchwork: A Unified Framework for RAG Serving](https://arxiv.org/abs/2505.07833)
*Bodun Hu,Luis Pabon,Saurabh Agarwal,Aditya Akella*

Main category: cs.DC

TL;DR: Patchwork是一个端到端的RAG服务框架，通过灵活接口、分布式优化和动态调度显著提升性能。


<details>
  <summary>Details</summary>
Motivation: RAG系统因异构计算管道导致效率问题，亟需优化框架。

Method: Patchwork提供灵活接口、分布式部署和动态调度机制。

Result: 实验显示Patchwork吞吐量提升48%，SLO违规减少24%。

Conclusion: Patchwork有效解决RAG效率瓶颈，性能优于商业方案。

Abstract: Retrieval Augmented Generation (RAG) has emerged as a new paradigm for
enhancing Large Language Model reliability through integration with external
knowledge sources. However, efficient deployment of these systems presents
significant technical challenges due to their inherently heterogeneous
computational pipelines comprising LLMs, databases, and specialized processing
components. We introduce Patchwork, a comprehensive end-to-end RAG serving
framework designed to address these efficiency bottlenecks. Patchwork's
architecture offers three key innovations: First, it provides a flexible
specification interface enabling users to implement custom RAG pipelines.
Secondly, it deploys these pipelines as distributed inference systems while
optimizing for the unique scalability characteristics of individual RAG
components. Third, Patchwork incorporates an online scheduling mechanism that
continuously monitors request load and execution progress, dynamically
minimizing SLO violations through strategic request prioritization and resource
auto-scaling. Our experimental evaluation across four distinct RAG
implementations demonstrates that Patchwork delivers substantial performance
improvements over commercial alternatives, achieving throughput gains exceeding
48% while simultaneously reducing SLO violations by ~24%.

</details>


### [322] [Fused3S: Fast Sparse Attention on Tensor Cores](https://arxiv.org/abs/2505.08098)
*Zitong Li,Aparna Chandramowlishwaran*

Main category: cs.DC

TL;DR: Fused3S是一种首次融合的3S算法，通过联合优化张量核心利用率和减少数据移动，显著提升了稀疏注意力计算的效率。


<details>
  <summary>Details</summary>
Motivation: 稀疏注意力是许多神经网络模型的核心组件，但其计算模式（3S）在GPU上执行效率低，主要由于稀疏性与张量核心的不匹配以及数据移动的高成本。

Method: Fused3S首次将3S计算模式（SDDMM、softmax和SpMM）融合为一个算法，优化张量核心利用并减少数据移动。

Result: 在真实图数据集上，Fused3S在H100和A30 GPU上分别实现了1.6-16.3倍和1.5-14倍的加速，并在Graph Transformer推理中提升了1.05-5.36倍的端到端性能。

Conclusion: Fused3S通过融合和优化3S计算模式，显著提升了稀疏注意力计算的效率，适用于多种GPU架构和数据集。

Abstract: Sparse attention is a core building block in many leading neural network
models, from graph-structured learning to sparse sequence modeling. It can be
decomposed into a sequence of three sparse matrix operations (3S): sampled
dense-dense matrix multiplication (SDDMM), softmax normalization, and sparse
matrix multiplication (SpMM). Efficiently executing the 3S computational
pattern on modern GPUs remains challenging due to (a) the mismatch between
unstructured sparsity and tensor cores optimized for dense operations, and (b)
the high cost of data movement. Previous works have optimized these sparse
operations individually or addressed one of these challenges. This paper
introduces Fused3S, the first fused 3S algorithm that jointly maximizes tensor
core utilization and minimizes data movement. Across real-world graph datasets,
Fused3S achieves $1.6- 16.3\times$ and $1.5-14\times$ speedup over
state-of-the-art on H100 and A30 GPUs. Furthermore, integrating Fused3S into
Graph Transformer inference accelerates end-to-end performance by
$1.05-5.36\times$, consistently outperforming all 3S baselines across diverse
datasets (single and batched graphs) and GPU architectures.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [323] [Diffusion-based supervised learning of generative models for efficient sampling of multimodal distributions](https://arxiv.org/abs/2505.07825)
*Hoang Tran,Zezhong Zhang,Feng Bao,Dan Lu,Guannan Zhang*

Main category: stat.ML

TL;DR: 提出了一种混合生成模型，用于高效采样高维多模态概率分布，解决了传统蒙特卡洛方法在多模态分布中的比例问题。


<details>
  <summary>Details</summary>
Motivation: 传统蒙特卡洛方法（如Metropolis-Hastings和Langevin Monte Carlo）在高维单模态分布中有效，但在多模态分布中难以正确采样各模态比例。

Method: 采用分治策略：1) 通过能量函数最小化识别所有模态；2) 训练分类器分割各模态域；3) 为每个模态训练扩散模型辅助的生成模型；4) 使用桥采样调整模态比例。

Result: 数值实验表明，该框架能有效处理100维内不同形状的多模态分布，并在偏微分方程贝叶斯反问题中应用。

Conclusion: 提出的混合生成模型在多模态分布采样中表现优异，尤其适用于高维复杂分布。

Abstract: We propose a hybrid generative model for efficient sampling of
high-dimensional, multimodal probability distributions for Bayesian inference.
Traditional Monte Carlo methods, such as the Metropolis-Hastings and Langevin
Monte Carlo sampling methods, are effective for sampling from single-mode
distributions in high-dimensional spaces. However, these methods struggle to
produce samples with the correct proportions for each mode in multimodal
distributions, especially for distributions with well separated modes. To
address the challenges posed by multimodality, we adopt a divide-and-conquer
strategy. We start by minimizing the energy function with initial guesses
uniformly distributed within the prior domain to identify all the modes of the
energy function. Then, we train a classifier to segment the domain
corresponding to each mode. After the domain decomposition, we train a
diffusion-model-assisted generative model for each identified mode within its
support. Once each mode is characterized, we employ bridge sampling to estimate
the normalizing constant, allowing us to directly adjust the ratios between the
modes. Our numerical examples demonstrate that the proposed framework can
effectively handle multimodal distributions with varying mode shapes in up to
100 dimensions. An application to Bayesian inverse problem for partial
differential equations is also provided.

</details>


### [324] [Wasserstein Distributionally Robust Nonparametric Regression](https://arxiv.org/abs/2505.07967)
*Changyu Liu,Yuling Jiao,Junhui Wang,Jian Huang*

Main category: stat.ML

TL;DR: 该论文研究了Wasserstein分布鲁棒非参数估计器的泛化性质，重点分析了模型误设的影响，并通过神经网络和分布扰动建立了非渐近误差界。


<details>
  <summary>Details</summary>
Motivation: 分布鲁棒优化在模型不确定性下的预测和决策中具有重要作用，但非参数框架的研究较少。本文旨在填补这一空白。

Method: 使用带有Lipschitz约束的前馈神经网络，分析分布扰动引起的正则化效应，建立非渐近误差界。

Result: 提出了适用于Lipschitz和二次损失函数的误差界，并通过模拟研究和MNIST数据集验证了估计器的鲁棒性。

Conclusion: 该研究为非参数分布鲁棒优化提供了理论支持，并展示了其在实践中的有效性。

Abstract: Distributionally robust optimization has become a powerful tool for
prediction and decision-making under model uncertainty. By focusing on the
local worst-case risk, it enhances robustness by identifying the most
unfavorable distribution within a predefined ambiguity set. While extensive
research has been conducted in parametric settings, studies on nonparametric
frameworks remain limited. This paper studies the generalization properties of
Wasserstein distributionally robust nonparametric estimators, with particular
attention to the impact of model misspecification, where non-negligible
discrepancies between the estimation function space and target function can
impair generalization performance. We establish non-asymptotic error bounds for
the excess local worst-case risk by analyzing the regularization effects
induced by distributional perturbations and employing feedforward neural
networks with Lipschitz constraints. These bounds illustrate how uncertainty
levels and neural network structures influence generalization performance and
are applicable to both Lipschitz and quadratic loss functions. Furthermore, we
investigate the Lagrangian relaxation of the local worst-case risk and derive
corresponding non-asymptotic error bounds for these estimators. The robustness
of the proposed estimator is evaluated through simulation studies and
illustrated with an application to the MNIST dataset.

</details>


### [325] [Sharp Gaussian approximations for Decentralized Federated Learning](https://arxiv.org/abs/2505.08125)
*Soham Bonnerjee,Sayar Karmakar,Wei Biao Wu*

Main category: stat.ML

TL;DR: 本文提出了两种针对局部SGD的广义高斯近似结果，并探讨其应用。首先证明了最终迭代的Berry-Esseen定理，支持有效的乘数自举方法；其次，基于鲁棒性考虑，提出了两种时间均匀的高斯近似，支持检测对抗攻击的自举测试。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，局部SGD的统计保证研究有限，尤其是在收敛性之外的领域。本文旨在填补这一空白。

Method: 通过理论分析，提出了两种高斯近似方法：Berry-Esseen定理和时间均匀近似，并通过仿真验证。

Result: 证明了局部SGD的统计性质，支持自举方法和对抗攻击检测。

Conclusion: 本文为局部SGD提供了新的统计工具，扩展了其在联邦学习中的应用潜力。

Abstract: Federated Learning has gained traction in privacy-sensitive collaborative
environments, with local SGD emerging as a key optimization method in
decentralized settings. While its convergence properties are well-studied,
asymptotic statistical guarantees beyond convergence remain limited. In this
paper, we present two generalized Gaussian approximation results for local SGD
and explore their implications. First, we prove a Berry-Esseen theorem for the
final local SGD iterates, enabling valid multiplier bootstrap procedures.
Second, motivated by robustness considerations, we introduce two distinct
time-uniform Gaussian approximations for the entire trajectory of local SGD.
The time-uniform approximations support Gaussian bootstrap-based tests for
detecting adversarial attacks. Extensive simulations are provided to support
our theoretical results.

</details>


### [326] [SIM-Shapley: A Stable and Computationally Efficient Approach to Shapley Value Approximation](https://arxiv.org/abs/2505.08198)
*Wangxuan Fan,Siqi Li,Doudou Zhou,Yohei Okada,Chuan Hong,Molei Liu,Nan Liu*

Main category: stat.ML

TL;DR: SIM-Shapley是一种高效且稳定的Shapley值近似方法，通过随机优化显著降低了计算成本，同时保持特征归因质量。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如医疗和金融）中，可解释人工智能（XAI）对机器学习（ML）的可信性至关重要。Shapley值方法虽然提供了特征归因的理论框架，但计算成本高，限制了其在高维场景中的可扩展性。

Method: 提出SIM-Shapley方法，受随机优化启发，通过理论分析方差并证明线性$Q$-收敛，实现了稳定且高效的Shapley值近似。

Result: 实验表明，SIM-Shapley将计算时间减少了85%，同时保持了与现有方法相当的特征归因质量。

Conclusion: SIM-Shapley不仅改进了特征归因的计算效率，还扩展了样本平均近似问题的解决框架，为计算效率提升提供了新途径。

Abstract: Explainable artificial intelligence (XAI) is essential for trustworthy
machine learning (ML), particularly in high-stakes domains such as healthcare
and finance. Shapley value (SV) methods provide a principled framework for
feature attribution in complex models but incur high computational costs,
limiting their scalability in high-dimensional settings. We propose Stochastic
Iterative Momentum for Shapley Value Approximation (SIM-Shapley), a stable and
efficient SV approximation method inspired by stochastic optimization. We
analyze variance theoretically, prove linear $Q$-convergence, and demonstrate
improved empirical stability and low bias in practice on real-world datasets.
In our numerical experiments, SIM-Shapley reduces computation time by up to 85%
relative to state-of-the-art baselines while maintaining comparable feature
attribution quality. Beyond feature attribution, our stochastic mini-batch
iterative framework extends naturally to a broader class of sample average
approximation problems, offering a new avenue for improving computational
efficiency with stability guarantees. Code is publicly available at
https://github.com/nliulab/SIM-Shapley.

</details>


### [327] [Lie Group Symmetry Discovery and Enforcement Using Vector Fields](https://arxiv.org/abs/2505.08219)
*Ben Shaw,Sasidhar Kunapuli,Abram Magner,Kevin R. Moon*

Main category: stat.ML

TL;DR: 论文探讨了对称性在机器学习中的重要性，扩展了非仿射对称性发现到神经网络，并引入向量场强制对称性，同时研究了对称性搜索空间的限制。


<details>
  <summary>Details</summary>
Motivation: 对称性在机器学习中具有优势，但现有方法未充分探索非仿射对称性和对称性强制。

Method: 扩展非仿射对称性发现到神经网络，引入向量场强制对称性，并研究对称性搜索空间的限制。

Result: 提供了理论和实验支持，展示了对称性发现和强制的有效性。

Conclusion: 对称性发现和强制对提升机器学习模型性能具有潜力。

Abstract: Symmetry-informed machine learning can exhibit advantages over machine
learning which fails to account for symmetry. Additionally, recent attention
has been given to continuous symmetry discovery using vector fields which serve
as infinitesimal generators for Lie group symmetries. In this paper, we extend
the notion of non-affine symmetry discovery to functions defined by neural
networks. We further extend work in this area by introducing symmetry
enforcement of smooth models using vector fields. Finally, we extend work on
symmetry discovery using vector fields by providing both theoretical and
experimental material on the restriction of the symmetry search space to
infinitesimal isometries.

</details>


### [328] [Iteratively reweighted kernel machines efficiently learn sparse functions](https://arxiv.org/abs/2505.08277)
*Libin Zhu,Damek Davis,Dmitriy Drusvyatskiy,Maryam Fazel*

Main category: stat.ML

TL;DR: 论文指出神经网络的低维表示和分层结构能力并非独有，经典核方法也能实现类似效果。通过核预测器的导数检测关键坐标，并通过迭代重加权数据训练核机器，高效学习分层多项式。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络的低维表示和分层结构能力是否仅为神经网络特有，证明经典核方法也能实现类似功能。

Method: 利用核预测器的导数检测关键坐标，通过迭代重加权数据和训练核机器学习分层多项式。

Result: 理论表明核方法能以低样本复杂度检测关键坐标，并通过迭代学习高效构建分层多项式。数值实验验证了理论。

Conclusion: 经典核方法能够实现与神经网络相似的低维表示和分层结构学习能力，为核方法的应用提供了新视角。

Abstract: The impressive practical performance of neural networks is often attributed
to their ability to learn low-dimensional data representations and hierarchical
structure directly from data. In this work, we argue that these two phenomena
are not unique to neural networks, and can be elicited from classical kernel
methods. Namely, we show that the derivative of the kernel predictor can detect
the influential coordinates with low sample complexity. Moreover, by
iteratively using the derivatives to reweight the data and retrain kernel
machines, one is able to efficiently learn hierarchical polynomials with finite
leap complexity. Numerical experiments illustrate the developed theory.

</details>


### [329] [Learning Treatment Allocations with Risk Control Under Partial Identifiability](https://arxiv.org/abs/2505.08378)
*Sofia Ek,Dave Zachariah*

Main category: stat.ML

TL;DR: 提出了一种在部分识别设置下控制治疗风险的认证学习方法。


<details>
  <summary>Details</summary>
Motivation: 精准医疗中，许多治疗伴随副作用，可能对未受益患者造成不必要伤害，需控制治疗风险。

Method: 提出一种认证学习方法，在有限样本下控制治疗风险。

Result: 方法在模拟和真实数据中验证有效。

Conclusion: 该方法为精准医疗中的治疗分配提供了风险可控的解决方案。

Abstract: Learning beneficial treatment allocations for a patient population is an
important problem in precision medicine. Many treatments come with adverse side
effects that are not commensurable with their potential benefits. Patients who
do not receive benefits after such treatments are thereby subjected to
unnecessary harm. This is a `treatment risk' that we aim to control when
learning beneficial allocations. The constrained learning problem is challenged
by the fact that the treatment risk is not in general identifiable using either
randomized trial or observational data. We propose a certifiable learning
method that controls the treatment risk with finite samples in the partially
identified setting. The method is illustrated using both simulated and real
data.

</details>


### [330] [neuralGAM: An R Package for Fitting Generalized Additive Neural Networks](https://arxiv.org/abs/2505.08610)
*Ines Ortega-Fernandez,Marta Sestelo*

Main category: stat.ML

TL;DR: 介绍了一个名为neuralGAM的R包，用于解决神经网络的黑箱问题，通过基于广义可加模型的神经网络拓扑结构，提供高精度且可解释的深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 神经网络在多个任务中表现优异，但其黑箱特性导致决策过程难以理解。

Method: neuralGAM包采用广义可加模型（GAM）的神经网络拓扑结构，为每个特征独立训练神经网络以估计其对输出的贡献。

Result: neuralGAM提供了一个灵活且不受限制的框架，适用于合成和真实数据。

Conclusion: neuralGAM成功结合了神经网络的强大性能和可解释性，解决了黑箱问题。

Abstract: Nowadays, Neural Networks are considered one of the most effective methods
for various tasks such as anomaly detection, computer-aided disease detection,
or natural language processing. However, these networks suffer from the
``black-box'' problem which makes it difficult to understand how they make
decisions. In order to solve this issue, an R package called neuralGAM is
introduced. This package implements a Neural Network topology based on
Generalized Additive Models, allowing to fit an independent Neural Network to
estimate the contribution of each feature to the output variable, yielding a
highly accurate and interpretable Deep Learning model. The neuralGAM package
provides a flexible framework for training Generalized Additive Neural
Networks, which does not impose any restrictions on the Neural Network
architecture. We illustrate the use of the neuralGAM package in both synthetic
and real data examples.

</details>


### [331] [Uncertainty-Aware Surrogate-based Amortized Bayesian Inference for Computationally Expensive Models](https://arxiv.org/abs/2505.08683)
*Stefania Scheurer,Philipp Reiser,Tim Brünnette,Wolfgang Nowak,Anneli Guthke,Paul-Christian Bürkner*

Main category: stat.ML

TL;DR: 提出了一种结合替代模型和ABI的框架UA-SABI，用于在计算昂贵模型中实现快速、可靠的贝叶斯推断。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯推断方法（如MCMC和ABI）在计算昂贵模型中效率低下，且替代模型可能引入误差。

Method: 结合替代建模和ABI，显式量化并传播替代模型的不确定性。

Result: 实验表明，UA-SABI能在时间限制下实现可靠、快速的贝叶斯推断。

Conclusion: UA-SABI为计算昂贵模型提供了一种高效且可靠的推断解决方案。

Abstract: Bayesian inference typically relies on a large number of model evaluations to
estimate posterior distributions. Established methods like Markov Chain Monte
Carlo (MCMC) and Amortized Bayesian Inference (ABI) can become computationally
challenging. While ABI enables fast inference after training, generating
sufficient training data still requires thousands of model simulations, which
is infeasible for expensive models. Surrogate models offer a solution by
providing approximate simulations at a lower computational cost, allowing the
generation of large data sets for training. However, the introduced
approximation errors and uncertainties can lead to overconfident posterior
estimates. To address this, we propose Uncertainty-Aware Surrogate-based
Amortized Bayesian Inference (UA-SABI) - a framework that combines surrogate
modeling and ABI while explicitly quantifying and propagating surrogate
uncertainties through the inference pipeline. Our experiments show that this
approach enables reliable, fast, and repeated Bayesian inference for
computationally expensive models, even under tight time constraints.

</details>


### [332] [Continuous Temporal Learning of Probability Distributions via Neural ODEs with Applications in Continuous Glucose Monitoring Data](https://arxiv.org/abs/2505.08698)
*Antonio Álvarez-López,Marcos Matabuena*

Main category: stat.ML

TL;DR: 提出了一种基于高斯混合和神经ODE的概率模型，用于捕捉时间依赖数据分布的动态变化，并通过MMD非参数估计时间依赖函数。


<details>
  <summary>Details</summary>
Motivation: 分析生物标志物（如血糖）的时间分布变化，以反映慢性疾病（如糖尿病）的进展。

Method: 使用高斯混合模型和神经ODE参数化时间依赖函数，通过MMD进行非参数估计。

Result: 模型在估计准确性上与现有方法竞争，同时具有高解释性和计算效率。

Conclusion: 方法在临床试验数据中有效，能够比较干预对血糖分布的影响，提供新的数学和临床视角。

Abstract: Modeling the continuous--time dynamics of probability distributions from
time--dependent data samples is a fundamental problem in many fields, including
digital health. The aim is to analyze how the distribution of a biomarker, such
as glucose, evolves over time and how these changes may reflect the progression
of chronic diseases such as diabetes. In this paper, we propose a novel
probabilistic model based on a mixture of Gaussian distributions to capture how
samples from a continuous-time stochastic process evolve over the time. To
model potential distribution shifts over time, we introduce a time-dependent
function parameterized by a Neural Ordinary Differential Equation (Neural ODE)
and estimate it non--parametrically using the Maximum Mean Discrepancy (MMD).
The proposed model is highly interpretable, detects subtle temporal shifts, and
remains computationally efficient. Through simulation studies, we show that it
performs competitively in terms of estimation accuracy against
state-of-the-art, less interpretable methods such as normalized gradient--flows
and non--parameteric kernel density estimators. Finally, we demonstrate the
utility of our method on digital clinical--trial data, showing how the
interventions alters the time-dependent distribution of glucose levels and
enabling a rigorous comparison of control and treatment groups from novel
mathematical and clinical perspectives.

</details>


### [333] [PCS-UQ: Uncertainty Quantification via the Predictability-Computability-Stability Framework](https://arxiv.org/abs/2505.08784)
*Abhineet Agarwal,Michael Xiao,Rebecca Barter,Omer Ronen,Boyu Fan,Bin Yu*

Main category: stat.ML

TL;DR: 提出了一种基于PCS框架的不确定性量化方法（PCS-UQ），解决了传统方法对模型误设的脆弱性和共形推断的区间过大问题，实验显示其覆盖率和区间宽度优于共形方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在高风险领域部署时，可靠的不确定性量化至关重要。传统方法依赖真实生成模型且对误设不鲁棒，共形推断虽灵活但未考虑模型选择导致区间过大。

Method: 基于PCS框架，通过预测检查筛选模型，多轮bootstrap评估样本间变异性和算法不稳定性，提出新的校准方案以提高局部适应性。

Result: 在17个回归和6个分类数据集上，PCS-UQ实现目标覆盖率，区间宽度比共形方法减少约20%。局部分析显示其子组覆盖率更优。

Conclusion: PCS-UQ在覆盖率和区间宽度上优于共形方法，适用于深度学习模型，理论证明其变体为分裂共形推断的一种形式。

Abstract: As machine learning (ML) models are increasingly deployed in high-stakes
domains, trustworthy uncertainty quantification (UQ) is critical for ensuring
the safety and reliability of these models. Traditional UQ methods rely on
specifying a true generative model and are not robust to misspecification. On
the other hand, conformal inference allows for arbitrary ML models but does not
consider model selection, which leads to large interval sizes. We tackle these
drawbacks by proposing a UQ method based on the predictability, computability,
and stability (PCS) framework for veridical data science proposed by Yu and
Kumbier. Specifically, PCS-UQ addresses model selection by using a prediction
check to screen out unsuitable models. PCS-UQ then fits these screened
algorithms across multiple bootstraps to assess inter-sample variability and
algorithmic instability, enabling more reliable uncertainty estimates. Further,
we propose a novel calibration scheme that improves local adaptivity of our
prediction sets. Experiments across $17$ regression and $6$ classification
datasets show that PCS-UQ achieves the desired coverage and reduces width over
conformal approaches by $\approx 20\%$. Further, our local analysis shows
PCS-UQ often achieves target coverage across subgroups while conformal methods
fail to do so. For large deep-learning models, we propose computationally
efficient approximation schemes that avoid the expensive multiple bootstrap
trainings of PCS-UQ. Across three computer vision benchmarks, PCS-UQ reduces
prediction set size over conformal methods by $20\%$. Theoretically, we show a
modified PCS-UQ algorithm is a form of split conformal inference and achieves
the desired coverage with exchangeable data.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [334] [Justified Evidence Collection for Argument-based AI Fairness Assurance](https://arxiv.org/abs/2505.08064)
*Alpay Sabuncuoglu,Christopher Burr,Carsten Maple*

Main category: cs.HC

TL;DR: 论文提出了一种基于系统工程的框架，通过动态论证保证AI系统的公平性，分为需求规划和持续监控两个阶段。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统公平性这一复杂的社会技术挑战，需要全生命周期的持续监督和论证。

Method: 采用动态论证保证方法，结合多学科团队定义目标和证据，并通过软件工具持续监控。

Result: 在金融领域的案例研究中验证了框架的有效性，特别是支持公平性论证。

Conclusion: 该框架为AI系统的公平性提供了可操作的方法，适用于全生命周期管理。

Abstract: It is well recognised that ensuring fair AI systems is a complex
sociotechnical challenge, which requires careful deliberation and continuous
oversight across all stages of a system's lifecycle, from defining requirements
to model deployment and deprovisioning. Dynamic argument-based assurance cases,
which present structured arguments supported by evidence, have emerged as a
systematic approach to evaluating and mitigating safety risks and hazards in
AI-enabled system development and have also been extended to deal with broader
normative goals such as fairness and explainability. This paper introduces a
systems-engineering-driven framework, supported by software tooling, to
operationalise a dynamic approach to argument-based assurance in two stages. In
the first stage, during the requirements planning phase, a multi-disciplinary
and multi-stakeholder team define goals and claims to be established (and
evidenced) by conducting a comprehensive fairness governance process. In the
second stage, a continuous monitoring interface gathers evidence from existing
artefacts (e.g. metrics from automated tests), such as model, data, and use
case documentation, to support these arguments dynamically. The framework's
effectiveness is demonstrated through an illustrative case study in finance,
with a focus on supporting fairness-related arguments.

</details>


### [335] [Communication Styles and Reader Preferences of LLM and Human Experts in Explaining Health Information](https://arxiv.org/abs/2505.08143)
*Jiawei Zhou,Kritika Venkatachalam,Minje Choi,Koustuv Saha,Munmun De Choudhury*

Main category: cs.HC

TL;DR: 研究评估了大型语言模型（LLM）与人类在健康信息事实核查中的沟通风格差异，发现LLM在说服策略、确定性表达和社会价值一致性上得分较低，但人类评价更偏好LLM内容的清晰性和完整性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在信息辅助中的广泛应用，需考察其与人类沟通风格和价值观的匹配度，尤其是在健康信息事实核查这一关键领域。

Method: 通过收集权威事实核查组织的健康错误信息解释数据集，生成LLM响应，并从信息语言特征、发送者说服策略和接收者价值一致性三个维度评估沟通风格，同时进行99名参与者的盲评。

Result: LLM生成的内容在说服策略、确定性表达和社会价值一致性上得分较低，但人类评价显示60%以上参与者更偏好LLM内容的清晰性、完整性和说服力。

Conclusion: 尽管LLM在传统质量指标上得分较低，但其结构化信息呈现方式可能更有效吸引读者。

Abstract: With the wide adoption of large language models (LLMs) in information
assistance, it is essential to examine their alignment with human communication
styles and values. We situate this study within the context of fact-checking
health information, given the critical challenge of rectifying conceptions and
building trust. Recent studies have explored the potential of LLM for health
communication, but style differences between LLMs and human experts and
associated reader perceptions remain under-explored. In this light, our study
evaluates the communication styles of LLMs, focusing on how their explanations
differ from those of humans in three core components of health communication:
information, sender, and receiver. We compiled a dataset of 1498 health
misinformation explanations from authoritative fact-checking organizations and
generated LLM responses to inaccurate health information. Drawing from health
communication theory, we evaluate communication styles across three key
dimensions of information linguistic features, sender persuasive strategies,
and receiver value alignments. We further assessed human perceptions through a
blinded evaluation with 99 participants. Our findings reveal that LLM-generated
articles showed significantly lower scores in persuasive strategies, certainty
expressions, and alignment with social values and moral foundations. However,
human evaluation demonstrated a strong preference for LLM content, with over
60% responses favoring LLM articles for clarity, completeness, and
persuasiveness. Our results suggest that LLMs' structured approach to
presenting information may be more effective at engaging readers despite
scoring lower on traditional measures of quality in fact-checking and health
communication.

</details>


### [336] [VizCV: AI-assisted visualization of researchers' publications tracks](https://arxiv.org/abs/2505.08691)
*Vladimír Lazárik,Marco Agus,Barbora Kozlíková,Pere-Pau Vázquez*

Main category: cs.HC

TL;DR: VizCV是一个基于网络的端到端可视化分析框架，用于交互式探索研究人员的科学生涯轨迹，结合AI辅助分析和自动化报告功能。


<details>
  <summary>Details</summary>
Motivation: 评估科学家和研究团队的出版记录演变对学术环境管理和职业规划至关重要。

Method: VizCV通过三个关键维度建模职业发展：研究主题演变、出版记录及影响、合作动态。结合AI技术提供自动化解释和比较分析功能。

Result: 系统支持多视角交互分析，包括主题轨迹、影响力增长和合作网络变化，并提供AI驱动的自动化报告。

Conclusion: VizCV通过AI/ML技术实现了对科学生涯轨迹的全面分析和可视化，为学术职业发展提供了有力工具。

Abstract: Analyzing how the publication records of scientists and research groups have
evolved over the years is crucial for assessing their expertise since it can
support the management of academic environments by assisting with career
planning and evaluation. We introduce VizCV, a novel web-based end-to-end
visual analytics framework that enables the interactive exploration of
researchers' scientific trajectories. It incorporates AI-assisted analysis and
supports automated reporting of career evolution. Our system aims to model
career progression through three key dimensions: a) research topic evolution to
detect and visualize shifts in scholarly focus over time, b) publication record
and the corresponding impact, c) collaboration dynamics depicting the growth
and transformation of a researcher's co-authorship network. AI-driven insights
provide automated explanations of career transitions, detecting significant
shifts in research direction, impact surges, or collaboration expansions. The
system also supports comparative analysis between researchers, allowing users
to compare topic trajectories and impact growth. Our interactive, multi-tab and
multiview system allows for the exploratory analysis of career milestones under
different perspectives, such as the most impactful articles, emerging research
themes, or obtaining a detailed analysis of the contribution of the researcher
in a subfield. The key contributions include AI/ML techniques for: a) topic
analysis, b) dimensionality reduction for visualizing patterns and trends, c)
the interactive creation of textual descriptions of facets of data through
configurable prompt generation and large language models, that include key
indicators, to help understanding the career development of individuals or
groups.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [337] [Distributed Quantum Neural Networks on Distributed Photonic Quantum Computing](https://arxiv.org/abs/2505.08474)
*Kuan-Cheng Chen,Chen-Yu Liu,Yu Shang,Felix Burt,Kin K. Leung*

Main category: quant-ph

TL;DR: 提出了一种分布式量子-经典框架，结合光子量子神经网络（QNNs）和矩阵乘积态（MPS）映射，实现参数高效的经典神经网络训练。


<details>
  <summary>Details</summary>
Motivation: 通过结合量子计算的高维表达能力和经典神经网络的可部署性，探索一种实用的分布式量子机器学习路径。

Method: 利用光子QNNs生成高维概率分布，通过MPS模型映射到经典网络权重，实现参数高效训练。

Result: 在MNIST分类任务中，光子QT达到95.50%准确率，参数更少且压缩比高，优于经典压缩技术。

Conclusion: 该框架展示了光子量子计算在分布式机器学习中的潜力，结合了量子表达性和经典部署性。

Abstract: We introduce a distributed quantum-classical framework that synergizes
photonic quantum neural networks (QNNs) with matrix-product-state (MPS) mapping
to achieve parameter-efficient training of classical neural networks. By
leveraging universal linear-optical decompositions of $M$-mode interferometers
and photon-counting measurement statistics, our architecture generates neural
parameters through a hybrid quantum-classical workflow: photonic QNNs with
$M(M+1)/2$ trainable parameters produce high-dimensional probability
distributions that are mapped to classical network weights via an MPS model
with bond dimension $\chi$. Empirical validation on MNIST classification
demonstrates that photonic QT achieves an accuracy of $95.50\% \pm 0.84\%$
using 3,292 parameters ($\chi = 10$), compared to $96.89\% \pm 0.31\%$ for
classical baselines with 6,690 parameters. Moreover, a ten-fold compression
ratio is achieved at $\chi = 4$, with a relative accuracy loss of less than
$3\%$. The framework outperforms classical compression techniques (weight
sharing/pruning) by 6--12\% absolute accuracy while eliminating quantum
hardware requirements during inference through classical deployment of
compressed parameters. Simulations incorporating realistic photonic noise
demonstrate the framework's robustness to near-term hardware imperfections.
Ablation studies confirm quantum necessity: replacing photonic QNNs with random
inputs collapses accuracy to chance level ($10.0\% \pm 0.5\%$). Photonic
quantum computing's room-temperature operation, inherent scalability through
spatial-mode multiplexing, and HPC-integrated architecture establish a
practical pathway for distributed quantum machine learning, combining the
expressivity of photonic Hilbert spaces with the deployability of classical
neural networks.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [338] [Non-contact Vital Signs Detection in Dynamic Environments](https://arxiv.org/abs/2505.08366)
*Shuai Sun,Chong-Xi Liang,Chengwei Ye,Huanzhen Zhang,Kangsheng Wang*

Main category: eess.SP

TL;DR: 提出了一种新的DC偏移校准方法和HADCM解调算法，用于复杂环境下的毫米波雷达生命体征检测，显著提升了解调性能和抗噪能力。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，时变DC偏移和相位不平衡会严重影响毫米波雷达生命体征检测的解调性能。

Method: 通过估计信号峰谷的时变DC偏移，并结合I/Q通道信号的差分形式和希尔伯特变换提取生命体征信息。

Result: 仿真和实验结果表明，该方法在低信噪比下仍保持稳健性能，信号恢复更准确且能有效抑制噪声干扰。

Conclusion: 该方法在复杂环境中优于现有解调技术，适用于高要求的生命体征检测场景。

Abstract: Accurate phase demodulation is critical for vital sign detection using
millimeter-wave radar. However, in complex environments, time-varying DC
offsets and phase imbalances can severely degrade demodulation performance. To
address this, we propose a novel DC offset calibration method alongside a
Hilbert and Differential Cross-Multiply (HADCM) demodulation algorithm. The
approach estimates time-varying DC offsets from neighboring signal peaks and
valleys, then employs both differential forms and Hilbert transforms of the I/Q
channel signals to extract vital sign information. Simulation and experimental
results demonstrate that the proposed method maintains robust performance under
low signal-to-noise ratios. Compared to existing demodulation techniques, it
offers more accurate signal recovery in challenging scenarios and effectively
suppresses noise interference.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [339] [CellVerse: Do Large Language Models Really Understand Cell Biology?](https://arxiv.org/abs/2505.07865)
*Fan Zhang,Tianyu Liu,Zhihong Zhu,Hao Wu,Haixin Wang,Donghao Zhou,Yefeng Zheng,Kun Wang,Xian Wu,Pheng-Ann Heng*

Main category: q-bio.QM

TL;DR: CellVerse是一个基于语言的单细胞分析基准测试，评估了多种LLMs在细胞生物学任务中的表现，发现现有模型仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在语言驱动的单细胞分析任务中的表现，填补现有研究的空白。

Method: 引入CellVerse基准测试，整合多组学数据，评估14种LLMs在三个层次任务中的表现。

Result: 现有专家模型表现不佳，通用模型初步具备理解能力，但整体性能仍有提升空间。

Conclusion: CellVerse揭示了LLMs在细胞生物学应用中的挑战，为未来研究奠定了基础。

Abstract: Recent studies have demonstrated the feasibility of modeling single-cell data
as natural languages and the potential of leveraging powerful large language
models (LLMs) for understanding cell biology. However, a comprehensive
evaluation of LLMs' performance on language-driven single-cell analysis tasks
still remains unexplored. Motivated by this challenge, we introduce CellVerse,
a unified language-centric question-answering benchmark that integrates four
types of single-cell multi-omics data and encompasses three hierarchical levels
of single-cell analysis tasks: cell type annotation (cell-level), drug response
prediction (drug-level), and perturbation analysis (gene-level). Going beyond
this, we systematically evaluate the performance across 14 open-source and
closed-source LLMs ranging from 160M to 671B on CellVerse. Remarkably, the
experimental results reveal: (1) Existing specialist models (C2S-Pythia) fail
to make reasonable decisions across all sub-tasks within CellVerse, while
generalist models such as Qwen, Llama, GPT, and DeepSeek family models exhibit
preliminary understanding capabilities within the realm of cell biology. (2)
The performance of current LLMs falls short of expectations and has substantial
room for improvement. Notably, in the widely studied drug response prediction
task, none of the evaluated LLMs demonstrate significant performance
improvement over random guessing. CellVerse offers the first large-scale
empirical demonstration that significant challenges still remain in applying
LLMs to cell biology. By introducing CellVerse, we lay the foundation for
advancing cell biology through natural languages and hope this paradigm could
facilitate next-generation single-cell analysis.

</details>


### [340] [Automated Model-Free Sorting of Single-Molecule Fluorescence Events Using a Deep Learning Based Hidden-State Model](https://arxiv.org/abs/2505.08608)
*Wenqi Zeng,Shuqi Zhou,Yuan Yao,Chunlai Chen*

Main category: q-bio.QM

TL;DR: DASH是一种全自动的单分子荧光数据分析架构，无需用户输入，适用于平衡和非平衡系统。


<details>
  <summary>Details</summary>
Motivation: 传统单分子荧光数据分析依赖人工经验，难以扩展和复现，需要自动化解决方案。

Method: 提出DASH架构，实现轨迹分类、状态分配和自动排序，无需人工干预。

Result: DASH在Cas12a介导的DNA切割等实验中表现稳健。

Conclusion: DASH为单分子水平生物动力学研究提供了高效工具。

Abstract: Single-molecule fluorescence assays enable high-resolution analysis of
biomolecular dynamics, but traditional analysis pipelines are labor-intensive
and rely on users' experience, limiting scalability and reproducibility. Recent
deep learning models have automated aspects of data processing, yet many still
require manual thresholds, complex architectures, or extensive labeled data.
Therefore, we present DASH, a fully streamlined architecture for trace
classification, state assignment, and automatic sorting that requires no user
input. DASH demonstrates robust performance across users and experimental
conditions both in equilibrium and non-equilibrium systems such as
Cas12a-mediated DNA cleavage. This paper proposes a novel strategy for the
automatic and detailed sorting of single-molecule fluorescence events. The
dynamic cleavage process of Cas12a is used as an example to provide a
comprehensive analysis. This approach is crucial for studying biokinetic
structural changes at the single-molecule level.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [341] [Getting Ready for the EU AI Act in Healthcare. A call for Sustainable AI Development and Deployment](https://arxiv.org/abs/2505.07875)
*John Brandt Brodersen,Ilaria Amelia Caggiano,Pedro Kringen,Vince Istvan Madai,Walter Osika,Giovanni Sartor,Ellen Svensson,Magnus Westerlund,Roberto V. Zicari*

Main category: cs.CY

TL;DR: 论文探讨了AI在医疗领域中的可信度评估，强调开发者需主动遵守AI法案的伦理原则以确保合规。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域如医疗中，结合技术、证据和伦理实践以符合法律要求是紧迫任务。

Method: 提出开发者应主动采取措施，确保现有和未来的AI系统符合AI法案的要求。

Result: 通过遵守伦理原则，可以提高AI系统的有效性和可持续性。

Conclusion: 合规不仅是形式化任务，还需基于伦理原则的主动承诺。

Abstract: Assessments of trustworthiness have become a cornerstone of responsible AI
development. Especially in high-stakes fields like healthcare, aligning
technical, evidence-based, and ethical practices with forthcoming legal
requirements is increasingly urgent. We argue that developers and deployers of
AI systems for the medical domain should be proactive and take steps to
progressively ensure that such systems, both those currently in use and those
being developed or planned, respect the requirements of the AI Act, which has
come into force in August 2024. This is necessary if full and effective
compliance is to be ensured when the most relevant provisions of the Act become
effective (August 2026). The engagement with the AI Act cannot be viewed as a
formalistic exercise. Compliance with the AI Act needs to be carried out
through the proactive commitment to the ethical principles of trustworthy AI.
These principles provide the background for the Act, which mentions them
several times and connects them to the protection of public interest. They can
be used to interpret and apply the Act's provisions and to identify good
practices, increasing the validity and sustainability of AI systems over time.

</details>


### [342] [Multimodal Assessment of Classroom Discourse Quality: A Text-Centered Attention-Based Multi-Task Learning Approach](https://arxiv.org/abs/2505.07902)
*Ruikun Hou,Babette Bühler,Tim Fütterer,Efe Bozkir,Peter Gerjets,Ulrich Trautwein,Enkelejda Kasneci*

Main category: cs.CY

TL;DR: 该研究提出了一种多模态融合架构，用于评估课堂话语质量，结合文本、音频和视频数据，并通过多任务学习和有序分类提升评分准确性。


<details>
  <summary>Details</summary>
Motivation: 传统课堂话语评估依赖人工编码，耗时且成本高。现有AI研究多限于单句分析，缺乏对整个课程段话语质量的评估。

Method: 采用注意力机制捕捉多模态交互，多任务学习联合预测三个话语组件的质量分数，并将任务建模为有序分类问题。

Result: 在GTI德国数据集上，模型表现与人类评分者一致，整体Quadratic Weighted Kappa得分为0.384，接近人类评分者间可靠性（0.326）。

Conclusion: 该研究为自动化话语质量评估奠定了基础，支持通过多维反馈促进教师专业发展。

Abstract: Classroom discourse is an essential vehicle through which teaching and
learning take place. Assessing different characteristics of discursive
practices and linking them to student learning achievement enhances the
understanding of teaching quality. Traditional assessments rely on manual
coding of classroom observation protocols, which is time-consuming and costly.
Despite many studies utilizing AI techniques to analyze classroom discourse at
the utterance level, investigations into the evaluation of discursive practices
throughout an entire lesson segment remain limited. To address this gap, our
study proposes a novel text-centered multimodal fusion architecture to assess
the quality of three discourse components grounded in the Global Teaching
InSights (GTI) observation protocol: Nature of Discourse, Questioning, and
Explanations. First, we employ attention mechanisms to capture inter- and
intra-modal interactions from transcript, audio, and video streams. Second, a
multi-task learning approach is adopted to jointly predict the quality scores
of the three components. Third, we formulate the task as an ordinal
classification problem to account for rating level order. The effectiveness of
these designed elements is demonstrated through an ablation study on the GTI
Germany dataset containing 92 videotaped math lessons. Our results highlight
the dominant role of text modality in approaching this task. Integrating
acoustic features enhances the model's consistency with human ratings,
achieving an overall Quadratic Weighted Kappa score of 0.384, comparable to
human inter-rater reliability (0.326). Our study lays the groundwork for the
future development of automated discourse quality assessment to support teacher
professional development through timely feedback on multidimensional discourse
practices.

</details>


### [343] [LECTOR: Summarizing E-book Reading Content for Personalized Student Support](https://arxiv.org/abs/2505.07898)
*Erwin Daniel López Zapata,Cheng Tang,Valdemar Švábenský,Fumiya Okubo,Atsushi Shimada*

Main category: cs.CY

TL;DR: LECTOR模型通过整合阅读内容和活动数据，提升了教育电子书平台的分析能力，尤其在预测低绩效学生方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注阅读活动数据，而忽视了阅读内容数据的价值，LECTOR旨在填补这一空白。

Method: 提出LECTOR模型，结合NLP技术从讲座幻灯片中提取关键信息，并与阅读活动数据整合。

Result: 实验显示LECTOR在信息提取和预测低绩效学生方面优于现有模型，F1分数提升显著。

Conclusion: LECTOR为个性化教育干预提供了新思路，展示了阅读内容数据的潜在应用价值。

Abstract: Educational e-book platforms provide valuable information to teachers and
researchers through two main sources: reading activity data and reading content
data. While reading activity data is commonly used to analyze learning
strategies and predict low-performing students, reading content data is often
overlooked in these analyses. To address this gap, this study proposes LECTOR
(Lecture slides and Topic Relationships), a model that summarizes information
from reading content in a format that can be easily integrated with reading
activity data. Our first experiment compared LECTOR to representative Natural
Language Processing (NLP) models in extracting key information from 2,255
lecture slides, showing an average improvement of 5% in F1-score. These results
were further validated through a human evaluation involving 28 students, which
showed an average improvement of 21% in F1-score over a model predominantly
used in current educational tools. Our second experiment compared reading
preferences extracted by LECTOR with traditional reading activity data in
predicting low-performing students using 600,712 logs from 218 students. The
results showed a tendency to improve the predictive performance by integrating
LECTOR. Finally, we proposed examples showing the potential application of the
reading preferences extracted by LECTOR in designing personalized interventions
for students.

</details>


### [344] [One Bad NOFO? AI Governance in Federal Grantmaking](https://arxiv.org/abs/2505.08133)
*Dan Bateyko,Karen Levy*

Main category: cs.CY

TL;DR: 论文探讨了美国联邦机构如何通过拨款政策间接治理人工智能（AI），发现其监管力度不足，尤其在透明度和问责制方面。


<details>
  <summary>Details</summary>
Motivation: 研究联邦机构在拨款政策中对AI的治理作用，填补现有研究的空白。

Method: 分析2009至2024年间超4万份联邦拨款通知，筛选提及AI的记录并审查其目标与要求。

Result: 发现机构虽在拨款通知中推广AI，但极少设定AI专用评判标准或限制，监管力度不足。

Conclusion: 拨款政策是AI治理的新领域，但需与其他监管措施协调，并加强透明度和隐私保护。

Abstract: Much scholarship considers how U.S. federal agencies govern artificial
intelligence (AI) through rulemaking and their own internal use policies. But
agencies have an overlooked AI governance role: setting discretionary grant
policy when directing billions of dollars in federal financial assistance.
These dollars enable state and local entities to study, create, and use AI.
This funding not only goes to dedicated AI programs, but also to grantees using
AI in the course of meeting their routine grant objectives. As discretionary
grantmakers, agencies guide and restrict what grant winners do -- a hidden
lever for AI governance. Agencies pull this lever by setting program
objectives, judging criteria, and restrictions for AI use. Using a novel
dataset of over 40,000 non-defense federal grant notices of funding opportunity
(NOFOs) posted to Grants.gov between 2009 and 2024, we analyze how agencies
regulate the use of AI by grantees. We select records mentioning AI and review
their stated goals and requirements. We find agencies promoting AI in notice
narratives, shaping adoption in ways other records of grant policy might fail
to capture. Of the grant opportunities that mention AI, we find only a handful
of AI-specific judging criteria or restrictions. This silence holds even when
agencies fund AI uses in contexts affecting people's rights and which, under an
analogous federal procurement regime, would result in extra oversight. These
findings recast grant notices as a site of AI policymaking -- albeit one that
is developing out of step with other regulatory efforts and incomplete in its
consideration of transparency, accountability, and privacy protections. The
paper concludes by drawing lessons from AI procurement scholarship, while
identifying distinct challenges in grantmaking that invite further study.

</details>


### [345] [Reciprocity as the Foundational Substrate of Society: How Reciprocal Dynamics Scale into Social Systems](https://arxiv.org/abs/2505.08319)
*Egil Diau*

Main category: cs.CY

TL;DR: 论文提出了一种三阶段自下而上的框架，用于模拟社会结构的涌现过程，填补了多智能体AI中缺乏可模拟模型的空白。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体AI中缺乏基于现实行为约束的社会结构涌现模型的问题，同时挑战经济学和社会学中对“制度”和“规范”等概念的静态描述。

Method: 提出三阶段框架：互惠动力学（个体层面的互惠交换）、规范稳定化（共享期望的巩固）和制度构建（稳定模式的外部化）。

Result: 框架能够从认知最小化的互动中系统地探索道德、文化和制度结构的涌现。

Conclusion: 通过基于智能体互惠的社会涌现模型，为理解社会结构的起源和操作定义提供了新视角。

Abstract: A major bottleneck in multi-agent AI is the lack of simulateable models for
the bottom-up emergence of social structure under realistic behavioral
constraints. Similarly, many foundational theories in economics and sociology
including the concepts of "institutions" and "norms" tend to describe social
structures post hoc, often relying on implicit assumptions of shared culture,
morality, or symbolic agreement. These concepts are often treated as primitives
rather than reconstructed from agent-level behavior, leaving both their origins
and operational definitions under-specified. To address this, we propose a
three-stage bottom-up framework: Reciprocal Dynamics, capturing
individual-level reciprocal exchanges; Norm Stabilization, the consolidation of
shared expectations; and Institutional Construction, the externalization of
stable patterns into scalable structures. By grounding social emergence in
agent-level reciprocity, our framework enables the systematic exploration of
how moral, cultural, and institutional structures emerge from cognitively
minimal interactions.

</details>
