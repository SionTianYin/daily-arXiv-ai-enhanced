<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 66]
- [cs.CV](#cs.CV) [Total: 154]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.LG](#cs.LG) [Total: 118]
- [cs.CR](#cs.CR) [Total: 7]
- [cs.NE](#cs.NE) [Total: 1]
- [eess.AS](#eess.AS) [Total: 4]
- [stat.AP](#stat.AP) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.RO](#cs.RO) [Total: 15]
- [cs.GR](#cs.GR) [Total: 4]
- [math.OC](#math.OC) [Total: 1]
- [cs.CY](#cs.CY) [Total: 12]
- [cs.SE](#cs.SE) [Total: 7]
- [eess.IV](#eess.IV) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.IT](#cs.IT) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [math.CO](#math.CO) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.HC](#cs.HC) [Total: 52]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 7]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [physics.optics](#physics.optics) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cs.CC](#cs.CC) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2504.13914)
*ByteDance Seed,:,Yufeng Yuan,Yu Yue,Mingxuan Wang,Xiaochen Zuo,Jiaze Chen,Lin Yan,Wenyuan Xu,Chi Zhang,Xin Liu,Chengyi Wang,TianTian Fan,Lingjun Liu,Qiying Yu,Xiangpeng Wei,Zhiqi Lin,Ruofei Zhu,Qingping Yang,Chengzhi Wei,Jerry He,Guanlin Liu,Zheng Wu,Xiangyu Yu,Zhicheng Liu,Jingjing Xu,Jiangjie Chen,Haojie Pan,Shengding Hu,Zhengyin Du,Wenqi Wang,Zewei Sun,Chenwei Lou,Bole Ma,Zihan Wang,Mofan Zhang,Wang Zhang,Gaohong Liu,Kaihua Jiang,Haibin Lin,Ru Zhang,Juncai Liu,Li Han,Jinxin Chi,Wenqiang Zhang,Jiayi Xu,Jun Yuan,Zhen Xiao,Yuqiao Xian,Jingqiao Wu,Kai Hua,Na Zhou,Jianhui Duan,Heyang Lu,Changbao Wang,Jinxiang Ou,Shihang Wang,Xiaoran Jin,Xuesong Yao,Chengyin Xu,Wenchang Ma,Zhecheng An,Renming Pang,Xia Xiao,Jing Su,Yuyu Zhang,Tao Sun,Kaibo Liu,Yifan Sun,Kai Shen,Sijun Zhang,Yiyuan Ma,Xingyan Bin,Ji Li,Yao Luo,Deyi Liu,Shiyi Zhan,Yunshui Li,Yuan Yang,Defa Zhu,Ke Shen,Chenggang Li,Xun Zhou,Liang Xiang,Yonghui Wu*

Main category: cs.CL

TL;DR: Seed-Thinking-v1.5是一种具有推理能力的模型，通过先思考后回答的方式提升性能，在多个基准测试中表现优异，尤其在STEM和编程领域。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在广泛任务中表现出卓越推理能力的模型，同时具备较小的模型规模和高效率。

Method: 采用Mixture-of-Experts（MoE）架构，激活参数为20B，总参数为200B。

Result: 在AIME 2024、Codeforces和GPQA等基准测试中取得高分，同时在非推理任务中表现优于DeepSeek R1。

Conclusion: Seed-Thinking-v1.5展示了强大的推理和泛化能力，适合多种任务，并计划公开内部基准以支持未来研究。

Abstract: We introduce Seed-Thinking-v1.5, capable of reasoning through thinking before
responding, resulting in improved performance on a wide range of benchmarks.
Seed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on
GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond
reasoning tasks, the method demonstrates notable generalization across diverse
domains. For instance, it surpasses DeepSeek R1 by 8% in win rate on
non-reasoning tasks, indicating its broader applicability. Compared to other
state-of-the-art reasoning models, Seed-Thinking-v1.5 is a Mixture-of-Experts
(MoE) model with a relatively small size, featuring 20B activated and 200B
total parameters. As part of our effort to assess generalized reasoning, we
develop two internal benchmarks, BeyondAIME and Codeforces, both of which will
be publicly released to support future research.

</details>


### [2] [Uncovering Conspiratorial Narratives within Arabic Online Content](https://arxiv.org/abs/2504.14037)
*Djamila Mohdeb,Meriem Laifa,Zineb Guemraoui,Dalila Behih*

Main category: cs.CL

TL;DR: 通过计算分析阿拉伯语在线内容，研究揭示了阿拉伯数字空间中阴谋论的传播，识别出六类阴谋论叙事，填补了以英语内容为主的阴谋论研究空白。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补阴谋论研究中阿拉伯语内容的空白，揭示其在阿拉伯社交媒体中的传播和影响。

Method: 结合命名实体识别和主题建模技术（Top2Vec算法），分析阿拉伯语博客和Facebook内容。

Result: 识别出六类阴谋论叙事：性别/女权、地缘政治、政府掩盖、末日论、犹太共济会论和地球工程论。

Conclusion: 研究揭示了阿拉伯数字空间中阴谋论的多样性和嵌入性，为理解其在阿拉伯世界公共话语中的作用提供了新视角。

Abstract: This study investigates the spread of conspiracy theories in Arabic digital
spaces through computational analysis of online content. By combining Named
Entity Recognition and Topic Modeling techniques, specifically the Top2Vec
algorithm, we analyze data from Arabic blogs and Facebook to identify and
classify conspiratorial narratives. Our analysis uncovers six distinct
categories: gender/feminist, geopolitical, government cover-ups, apocalyptic,
Judeo-Masonic, and geoengineering. The research highlights how these narratives
are deeply embedded in Arabic social media discourse, shaped by regional
historical, cultural, and sociopolitical contexts. By applying advanced Natural
Language Processing methods to Arabic content, this study addresses a gap in
conspiracy theory research, which has traditionally focused on English-language
content or offline data. The findings provide new insights into the
manifestation and evolution of conspiracy theories in Arabic digital spaces,
enhancing our understanding of their role in shaping public discourse in the
Arab world.

</details>


### [3] [MEQA: A Meta-Evaluation Framework for Question & Answer LLM Benchmarks](https://arxiv.org/abs/2504.14039)
*Jaime Raldua Veuthey,Zainab Ali Majid,Suhas Hariharan,Jacob Haimes*

Main category: cs.CL

TL;DR: MEQA是一个用于评估问答基准质量的框架，旨在标准化评估并提供量化分数，特别针对网络安全领域的基准测试。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的发展，其社会影响日益显著，因此需要严格的评估方法。现有评估基准缺乏对基准质量的元评估。

Method: 提出MEQA框架，通过标准化评估和量化分数来评估问答基准的质量，并在网络安全领域进行实证研究。

Result: 通过人类和LLM评估者的验证，MEQA能够揭示基准测试的优势和不足。

Conclusion: MEQA为基准测试的元评估提供了有效工具，尤其适用于网络安全领域，强调了AI模型的双重角色（防御工具与安全威胁）。

Abstract: As Large Language Models (LLMs) advance, their potential for widespread
societal impact grows simultaneously. Hence, rigorous LLM evaluations are both
a technical necessity and social imperative. While numerous evaluation
benchmarks have been developed, there remains a critical gap in
meta-evaluation: effectively assessing benchmarks' quality. We propose MEQA, a
framework for the meta-evaluation of question and answer (QA) benchmarks, to
provide standardized assessments, quantifiable scores, and enable meaningful
intra-benchmark comparisons. We demonstrate this approach on cybersecurity
benchmarks, using human and LLM evaluators, highlighting the benchmarks'
strengths and weaknesses. We motivate our choice of test domain by AI models'
dual nature as powerful defensive tools and security threats.

</details>


### [4] [A Baseline for Self-state Identification and Classification in Mental Health Data: CLPsych 2025 Task](https://arxiv.org/abs/2504.14066)
*Laerdon Kim*

Main category: cs.CL

TL;DR: 本文提出了一种基于Reddit心理健康数据的自我状态分类基线方法，采用4位量化Gemma 2 9B模型和句子分块预处理，性能优于其他方法，最终在14个系统中排名第三。


<details>
  <summary>Details</summary>
Motivation: 解决心理健康数据中自我状态的分类问题，提升分类性能。

Method: 使用4位量化Gemma 2 9B模型进行少样本学习，通过句子分块预处理识别相关句子证据，并进行二元分类。

Result: 系统在测试时召回率为0.579，排名第三。

Conclusion: 句子分块预处理有助于提升模型性能，因其与人工标注粒度匹配并简化任务为二元分类。

Abstract: We present a baseline for the CLPsych 2025 A.1 task: classifying self-states
in mental health data taken from Reddit. We use few-shot learning with a 4-bit
quantized Gemma 2 9B model and a data preprocessing step which first identifies
relevant sentences indicating self-state evidence, and then performs a binary
classification to determine whether the sentence is evidence of an adaptive or
maladaptive self-state. This system outperforms our other method which relies
on an LLM to highlight spans of variable length independently. We attribute the
performance of our model to the benefits of this sentence chunking step for two
reasons: partitioning posts into sentences 1) broadly matches the granularity
at which self-states were human-annotated and 2) simplifies the task for our
language model to a binary classification problem. Our system places third out
of fourteen systems submitted for Task A.1, achieving a test-time recall of
0.579.

</details>


### [5] [LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models](https://arxiv.org/abs/2504.14089)
*Kang He,Kaushik Roy*

Main category: cs.CL

TL;DR: LogicTree是一个模块化框架，通过算法引导搜索提升LLMs在复杂逻辑推理中的表现，优于CoT和ToT方法。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂逻辑推理中面临系统性探索和逻辑一致性的挑战，需要解决前提搜索的组合复杂性。

Method: 提出LogicTree框架，结合缓存机制和线性化前提搜索，引入LLM-free启发式方法优化前提优先级。

Result: 在五个数据集上，LogicTree平均优于CoT和ToT方法23.6%和12.5%，GPT-4o在框架内表现更优。

Conclusion: LogicTree通过结构化证明探索和严格逐步推理，显著提升了LLMs的逻辑推理能力。

Abstract: Large language models (LLMs) have achieved remarkable multi-step reasoning
capabilities across various domains. However, LLMs still face distinct
challenges in complex logical reasoning, as (1) proof-finding requires
systematic exploration and the maintenance of logical coherence and (2)
searching the right combination of premises at each reasoning step is
inherently challenging in tasks with large premise space. To address this, we
propose LogicTree, an inference-time modular framework employing
algorithm-guided search to automate structured proof exploration and ensure
logical coherence. Advancing beyond tree-of-thought (ToT), we incorporate
caching mechanism into LogicTree to enable effective utilization of historical
knowledge, preventing reasoning stagnation and minimizing redundancy.
Furthermore, we address the combinatorial complexity of premise search by
decomposing it into a linear process. The refined premise selection restricts
subsequent inference to at most one derivation per step, enhancing reasoning
granularity and enforcing strict step-by-step reasoning. Additionally, we
introduce two LLM-free heuristics for premise prioritization, enabling
strategic proof search. Experimental results on five datasets demonstrate that
LogicTree optimally scales inference-time computation to achieve higher proof
accuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%
and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o
outperforms o3-mini by 7.6% on average.

</details>


### [6] [PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models](https://arxiv.org/abs/2504.14117)
*Nusrat Jahan Prottasha,Upama Roy Chowdhury,Shetu Mohanto,Tasfia Nuzhat,Abdullah As Sami,Md Shamol Ali,Md Shohanur Islam Sobuj,Hafijur Raman,Md Kowsher,Ozlem Ozmen Garibay*

Main category: cs.CL

TL;DR: 本文综述了参数高效微调（PEFT）技术，探讨其动机、设计原则及效果，旨在解决大模型全参数微调的高成本问题。


<details>
  <summary>Details</summary>
Motivation: 传统全参数微调大模型（如LLMs和VLMs）需要大量计算资源和任务数据，成本高昂且易出现过拟合等问题。PEFT通过仅更新少量参数提供高效解决方案。

Method: 提出PEFT方法的分类框架（加性、选择性、重参数化、混合和统一框架），并系统比较其机制和权衡。

Result: PEFT在语言、视觉和生成建模等领域表现出色，能以更低资源成本实现高性能。

Conclusion: PEFT为大模型的实用、高效和可持续使用提供了可能，未来研究方向包括可扩展性、可解释性和联邦学习等。

Abstract: Large models such as Large Language Models (LLMs) and Vision Language Models
(VLMs) have transformed artificial intelligence, powering applications in
natural language processing, computer vision, and multimodal learning. However,
fully fine-tuning these models remains expensive, requiring extensive
computational resources, memory, and task-specific data. Parameter-Efficient
Fine-Tuning (PEFT) has emerged as a promising solution that allows adapting
large models to downstream tasks by updating only a small portion of
parameters. This survey presents a comprehensive overview of PEFT techniques,
focusing on their motivations, design principles, and effectiveness. We begin
by analyzing the resource and accessibility challenges posed by traditional
fine-tuning and highlight key issues, such as overfitting, catastrophic
forgetting, and parameter inefficiency. We then introduce a structured taxonomy
of PEFT methods -- grouped into additive, selective, reparameterized, hybrid,
and unified frameworks -- and systematically compare their mechanisms and
trade-offs. Beyond taxonomy, we explore the impact of PEFT across diverse
domains, including language, vision, and generative modeling, showing how these
techniques offer strong performance with lower resource costs. We also discuss
important open challenges in scalability, interpretability, and robustness, and
suggest future directions such as federated learning, domain adaptation, and
theoretical grounding. Our goal is to provide a unified understanding of PEFT
and its growing role in enabling practical, efficient, and sustainable use of
large models.

</details>


### [7] [Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations](https://arxiv.org/abs/2504.14150)
*Katie Matton,Robert Osazuwa Ness,John Guttag,Emre Kıcıman*

Main category: cs.CL

TL;DR: 该论文提出了一种衡量大语言模型（LLM）解释忠实性的新方法，通过定义忠实性并利用辅助LLM和贝叶斯分层模型来量化概念的影响。


<details>
  <summary>Details</summary>
Motivation: LLM生成的解释可能不忠实，导致过度信任和误用，因此需要一种方法来衡量其解释的忠实性。

Method: 定义忠实性为解释中暗示的概念与真实影响概念之间的差异；使用辅助LLM创建反事实输入，并结合贝叶斯分层模型量化概念的影响。

Result: 实验表明，该方法能有效量化不忠实性，并在社会偏见和医学问答任务中发现解释中的误导性模式。

Conclusion: 该方法为评估LLM解释的忠实性提供了新工具，揭示了潜在的不忠实问题。

Abstract: Large language models (LLMs) are capable of generating plausible explanations
of how they arrived at an answer to a question. However, these explanations can
misrepresent the model's "reasoning" process, i.e., they can be unfaithful.
This, in turn, can lead to over-trust and misuse. We introduce a new approach
for measuring the faithfulness of LLM explanations. First, we provide a
rigorous definition of faithfulness. Since LLM explanations mimic human
explanations, they often reference high-level concepts in the input question
that purportedly influenced the model. We define faithfulness in terms of the
difference between the set of concepts that LLM explanations imply are
influential and the set that truly are. Second, we present a novel method for
estimating faithfulness that is based on: (1) using an auxiliary LLM to modify
the values of concepts within model inputs to create realistic counterfactuals,
and (2) using a Bayesian hierarchical model to quantify the causal effects of
concepts at both the example- and dataset-level. Our experiments show that our
method can be used to quantify and discover interpretable patterns of
unfaithfulness. On a social bias task, we uncover cases where LLM explanations
hide the influence of social bias. On a medical question answering task, we
uncover cases where LLM explanations provide misleading claims about which
pieces of evidence influenced the model's decisions.

</details>


### [8] [SConU: Selective Conformal Uncertainty in Large Language Models](https://arxiv.org/abs/2504.14154)
*Zhiyuan Wang,Qingni Wang,Yue Zhang,Tianlong Chen,Xiaofeng Zhu,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.CL

TL;DR: 提出了一种名为选择性共形不确定性（SConU）的新方法，通过开发两种共形p值来检测异常数据，以管理错误覆盖率并提高预测效率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实应用中的广泛使用，确保任务特定指标的可靠性至关重要。现有方法未能有效识别违反交换性假设的异常数据，导致不可控的错误覆盖率。

Method: 开发了两种共形p值，用于检测样本是否偏离校准集的不确定性分布，并在可控风险水平下管理错误覆盖率。

Result: SConU方法不仅能在单领域和跨学科背景下严格管理错误覆盖率，还提高了预测效率，并近似实现了高风险问答任务中的条件覆盖。

Conclusion: SConU方法通过显著性测试和共形p值，有效解决了现有框架在异常数据检测和错误覆盖率管理上的不足，为语言模型的可靠部署提供了保障。

Abstract: As large language models are increasingly utilized in real-world
applications, guarantees of task-specific metrics are essential for their
reliable deployment. Previous studies have introduced various criteria of
conformal uncertainty grounded in split conformal prediction, which offer
user-specified correctness coverage. However, existing frameworks often fail to
identify uncertainty data outliers that violate the exchangeability assumption,
leading to unbounded miscoverage rates and unactionable prediction sets. In
this paper, we propose a novel approach termed Selective Conformal Uncertainty
(SConU), which, for the first time, implements significance tests, by
developing two conformal p-values that are instrumental in determining whether
a given sample deviates from the uncertainty distribution of the calibration
set at a specific manageable risk level. Our approach not only facilitates
rigorous management of miscoverage rates across both single-domain and
interdisciplinary contexts, but also enhances the efficiency of predictions.
Furthermore, we comprehensively analyze the components of the conformal
procedures, aiming to approximate conditional coverage, particularly in
high-stakes question-answering tasks.

</details>


### [9] [Self-Correction Makes LLMs Better Parsers](https://arxiv.org/abs/2504.14165)
*Ziyan Zhang,Yang Hou,Chen Gong,Zhenghua Li*

Main category: cs.CL

TL;DR: 论文分析了大型语言模型（LLMs）在句法解析任务中的不足，提出了一种基于现有树库语法规则的自校正方法，显著提升了LLMs的解析性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多种NLP任务中表现优异，但在句法解析等基础任务上仍存在不足，尤其是无法充分利用现有树库的语法规则生成有效的句法结构。

Method: 提出一种自校正方法，通过自动检测潜在错误并动态搜索相关语法规则，为LLMs提供提示和示例以自我修正。

Result: 在三个数据集上的实验表明，该方法显著提升了LLMs在英语和中文数据集上的性能，包括领域内和跨领域设置。

Conclusion: 通过利用现有树库的语法规则指导LLMs自我修正，可以有效提升其句法解析能力，无需额外训练。

Abstract: Large language models (LLMs) have achieved remarkable success across various
natural language processing (NLP) tasks. However, recent studies suggest that
they still face challenges in performing fundamental NLP tasks essential for
deep language understanding, particularly syntactic parsing. In this paper, we
conduct an in-depth analysis of LLM parsing capabilities, delving into the
specific shortcomings of their parsing results. We find that LLMs may stem from
limitations to fully leverage grammar rules in existing treebanks, which
restricts their capability to generate valid syntactic structures. To help LLMs
acquire knowledge without additional training, we propose a self-correction
method that leverages grammar rules from existing treebanks to guide LLMs in
correcting previous errors. Specifically, we automatically detect potential
errors and dynamically search for relevant rules, offering hints and examples
to guide LLMs in making corrections themselves. Experimental results on three
datasets with various LLMs, demonstrate that our method significantly improves
performance in both in-domain and cross-domain settings on the English and
Chinese datasets.

</details>


### [10] [Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion](https://arxiv.org/abs/2504.14175)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: 研究发现，基于LLM的查询扩展方法在零样本检索任务中的性能提升可能源于基准测试中的知识泄漏，而非方法本身的有效性。


<details>
  <summary>Details</summary>
Motivation: 质疑LLM生成的假设文档是否真的提升了检索性能，还是仅仅因为基准测试中存在知识泄漏。

Method: 以事实验证为测试平台，分析生成的文档是否包含与真实证据相关的信息，并评估其对性能的影响。

Result: 性能提升仅发生在生成的文档包含与真实证据相关的句子时，表明知识泄漏可能夸大了LLM方法的性能。

Conclusion: 基准测试中的知识泄漏可能误导了对LLM查询扩展方法性能的评估，尤其是在需要检索小众或新知识的实际场景中。

Abstract: Query expansion methods powered by large language models (LLMs) have
demonstrated effectiveness in zero-shot retrieval tasks. These methods assume
that LLMs can generate hypothetical documents that, when incorporated into a
query vector, enhance the retrieval of real evidence. However, we challenge
this assumption by investigating whether knowledge leakage in benchmarks
contributes to the observed performance gains. Using fact verification as a
testbed, we analyzed whether the generated documents contained information
entailed by ground truth evidence and assessed their impact on performance. Our
findings indicate that performance improvements occurred consistently only for
claims whose generated documents included sentences entailed by ground truth
evidence. This suggests that knowledge leakage may be present in these
benchmarks, inflating the perceived performance of LLM-based query expansion
methods, particularly in real-world scenarios that require retrieving niche or
novel knowledge.

</details>


### [11] [Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models](https://arxiv.org/abs/2504.14194)
*Xinlin Zhuang,Jiahui Peng,Ren Ma,Yinfan Wang,Tianyi Bai,Xingjian Wei,Jiantao Qiu,Chi Zhang,Ying Qian,Conghui He*

Main category: cs.CL

TL;DR: 论文提出PRRC框架和Meta-rater方法，通过多维度评估数据质量，显著提升模型训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM预训练数据集构成不透明，且数据选择方法局限于单维度评估或冗余策略，亟需更全面的质量评估方法。

Method: 提出PRRC（专业性、可读性、推理性和清洁性）框架，并开发Meta-rater方法，通过代理模型学习最优权重组合。

Result: Meta-rater使1.3B参数模型收敛速度翻倍，下游任务性能提升3.23，并在更大规模模型中验证了可扩展性。

Conclusion: 多维度数据质量整合显著优于传统单维度方法，为提升预训练效率和模型能力提供了新范式。

Abstract: The composition of pre-training datasets for large language models (LLMs)
remains largely undisclosed, hindering transparency and efforts to optimize
data quality, a critical driver of model performance. Current data selection
methods, such as natural language quality assessments, diversity-based filters,
and classifier-based approaches, are limited by single-dimensional evaluation
or redundancy-focused strategies. To address these gaps, we propose PRRC to
evaluate data quality across Professionalism, Readability, Reasoning, and
Cleanliness. We further introduce Meta-rater, a multi-dimensional data
selection method that integrates these dimensions with existing quality metrics
through learned optimal weightings. Meta-rater employs proxy models to train a
regression model that predicts validation loss, enabling the identification of
optimal combinations of quality scores. Experiments demonstrate that Meta-rater
doubles convergence speed for 1.3B parameter models and improves downstream
task performance by 3.23, with scalable benefits observed in 3.3B models
trained on 100B tokens. Additionally, we release the annotated SlimPajama-627B
dataset, labeled across 25 quality metrics (including PRRC), to advance
research in data-centric LLM development. Our work establishes that holistic,
multi-dimensional quality integration significantly outperforms conventional
single-dimension approaches, offering a scalable paradigm for enhancing
pre-training efficiency and model capability.

</details>


### [12] [EIoU-EMC: A Novel Loss for Domain-specific Nested Entity Recognition](https://arxiv.org/abs/2504.14203)
*Jian Zhang,Tianqing Zhang,Qi Li,Hongwei Wang*

Main category: cs.CL

TL;DR: 论文提出了一种新的损失函数EIoU-EMC，用于解决特定领域中嵌套NER任务的低资源和类别不平衡问题，并在生物医学和工业数据集上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 特定领域（如生物医学和工业）中的嵌套NER任务面临低资源和类别不平衡的挑战，限制了其广泛应用。

Method: 设计了EIoU-EMC损失函数，结合了Intersection over Union损失和多类损失，利用实体边界和分类信息提升模型在少量数据上的学习能力。

Result: 在三个生物医学NER数据集和一个工业数据集上，该方法表现出优于基线的性能，尤其在实体边界识别和分类方面有显著提升。

Conclusion: EIoU-EMC方法有效解决了特定领域中的嵌套NER任务问题，展示了其在低资源和高类别不平衡场景下的潜力。

Abstract: In recent years, research has mainly focused on the general NER task. There
still have some challenges with nested NER task in the specific domains.
Specifically, the scenarios of low resource and class imbalance impede the wide
application for biomedical and industrial domains. In this study, we design a
novel loss EIoU-EMC, by enhancing the implement of Intersection over Union loss
and Multiclass loss. Our proposed method specially leverages the information of
entity boundary and entity classification, thereby enhancing the model's
capacity to learn from a limited number of data samples. To validate the
performance of this innovative method in enhancing NER task, we conducted
experiments on three distinct biomedical NER datasets and one dataset
constructed by ourselves from industrial complex equipment maintenance
documents. Comparing to strong baselines, our method demonstrates the
competitive performance across all datasets. During the experimental analysis,
our proposed method exhibits significant advancements in entity boundary
recognition and entity classification. Our code are available here.

</details>


### [13] [Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification](https://arxiv.org/abs/2504.14212)
*Takuma Udagawa,Yang Zhao,Hiroshi Kanayama,Bishwaranjan Bhattacharjee*

Main category: cs.CL

TL;DR: 提出了一种高效的社会偏见分析标注流程，用于检测和缓解预训练语料库中的偏见。


<details>
  <summary>Details</summary>
Motivation: 预训练数据中的社会偏见可能被大语言模型放大，需要有效方法来识别和缓解这些偏见。

Method: 采用保护属性检测和尊重分类的标注流程，分析预训练语料库中的语言极性。

Result: 实验证明了该偏见分析和缓解措施的有效性，尤其针对Common Crawl语料库。

Conclusion: 提出的方法能有效识别和缓解预训练数据中的社会偏见。

Abstract: Large language models (LLMs) acquire general linguistic knowledge from
massive-scale pretraining. However, pretraining data mainly comprised of
web-crawled texts contain undesirable social biases which can be perpetuated or
even amplified by LLMs. In this study, we propose an efficient yet effective
annotation pipeline to investigate social biases in the pretraining corpora.
Our pipeline consists of protected attribute detection to identify diverse
demographics, followed by regard classification to analyze the language
polarity towards each attribute. Through our experiments, we demonstrate the
effect of our bias analysis and mitigation measures, focusing on Common Crawl
as the most representative pretraining corpus.

</details>


### [14] [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](https://arxiv.org/abs/2504.14218)
*Junchi Yao,Shu Yang,Jianhua Xu,Lijie Hu,Mengdi Li,Di Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为“Duplicatus Charm”的新方法，通过机制解释性研究大型语言模型（LLMs）中重复文本生成的根源，并利用稀疏自编码器（SAEs）提取和操作关键激活特征，有效缓解了“重复诅咒”。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多个领域取得了显著进展，但常面临重复文本生成的问题（即“重复诅咒”）。尽管已有研究提出解码策略来缓解这一问题，但其根本机制仍未充分探索。

Method: 通过机制解释性分析重复根源，提出“Duplicatus Charm”方法：1）通过logit分析定位与重复最相关的层；2）利用SAEs提取和刺激“重复特征”；3）构建重复数据集并设计评估流程验证方法。

Result: 成功识别出导致重复的关键激活特征，并通过去激活这些特征有效缓解了重复问题。

Conclusion: 研究揭示了LLMs中重复生成的机制，并提出了一种可操作的方法来缓解这一问题，为未来模型优化提供了新思路。

Abstract: Large language models (LLMs) have made remarkable progress in various
domains, yet they often suffer from repetitive text generation, a phenomenon we
refer to as the "Repeat Curse". While previous studies have proposed decoding
strategies to mitigate repetition, the underlying mechanism behind this issue
remains insufficiently explored. In this work, we investigate the root causes
of repetition in LLMs through the lens of mechanistic interpretability.
Inspired by recent advances in Sparse Autoencoders (SAEs), which enable
monosemantic feature extraction, we propose a novel approach, "Duplicatus
Charm", to induce and analyze the Repeat Curse. Our method systematically
identifies "Repetition Features" -the key model activations responsible for
generating repetitive outputs. First, we locate the layers most involved in
repetition through logit analysis. Next, we extract and stimulate relevant
features using SAE-based activation manipulation. To validate our approach, we
construct a repetition dataset covering token and paragraph level repetitions
and introduce an evaluation pipeline to quantify the influence of identified
repetition features. Furthermore, by deactivating these features, we have
effectively mitigated the Repeat Curse.

</details>


### [15] [SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification](https://arxiv.org/abs/2504.14223)
*Michael Färber,Parisa Aghdam,Kyuri Im,Mario Tawfelis,Hardik Ghoshal*

Main category: cs.CL

TL;DR: 本文介绍了首个基于大型语言模型（如GPT-4和Llama-3）的文本简化系统，支持多种输入格式和定制化选项，旨在提升内容的可访问性和包容性。


<details>
  <summary>Details</summary>
Motivation: 现有简化材料的不足限制了个人和职业发展，且社会包容性受到影响。尽管已有自动文本简化方法，但缺乏针对不同目标群体和简化程度的定制化解决方案。

Method: 开发了首个系统（simplifymytext.org），利用GPT-4和Llama-3，支持多种输入格式（如文本输入和文件上传），并提供灵活的定制选项。

Result: 系统通过多指标评估，证明了其有效性。

Conclusion: 该研究推动了自动文本简化领域的发展，并强调了定制化沟通在促进包容性中的重要性。

Abstract: Text simplification is essential for making complex content accessible to
diverse audiences who face comprehension challenges. Yet, the limited
availability of simplified materials creates significant barriers to personal
and professional growth and hinders social inclusion. Although researchers have
explored various methods for automatic text simplification, none fully leverage
large language models (LLMs) to offer tailored customization for different
target groups and varying levels of simplicity. Moreover, despite its proven
benefits for both consumers and organizations, the well-established practice of
plain language remains underutilized. In this paper, we
https://simplifymytext.org, the first system designed to produce plain language
content from multiple input formats, including typed text and file uploads,
with flexible customization options for diverse audiences. We employ GPT-4 and
Llama-3 and evaluate outputs across multiple metrics. Overall, our work
contributes to research on automatic text simplification and highlights the
importance of tailored communication in promoting inclusivity.

</details>


### [16] [Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale](https://arxiv.org/abs/2504.14225)
*Bowen Jiang,Zhuoqun Hao,Young-Min Cho,Bryan Li,Yuan Yuan,Sihao Chen,Lyle Ungar,Camillo J. Taylor,Dan Roth*

Main category: cs.CL

TL;DR: 该论文提出了PERSONAMEM基准，用于评估LLMs如何利用用户历史交互数据来个性化响应，发现当前模型在动态跟踪用户偏好方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何利用用户交互历史来内部化用户特质和偏好，并动态跟踪其演变，以生成个性化响应。

Method: 引入PERSONAMEM基准，包含180个模拟用户-LLM交互历史，评估LLMs在动态用户画像下的响应能力。

Result: 当前LLMs（如GPT-4.1等）在动态跟踪用户偏好方面表现不佳，准确率仅约50%。

Conclusion: PERSONAMEM基准和模拟工具可为未来开发真正用户感知的聊天机器人提供支持。

Abstract: Large Language Models (LLMs) have emerged as personalized assistants for
users across a wide range of tasks -- from offering writing support to
delivering tailored recommendations or consultations. Over time, the
interaction history between a user and an LLM can provide extensive information
about an individual's traits and preferences. However, open questions remain on
how well LLMs today can effectively leverage such history to (1) internalize
the user's inherent traits and preferences, (2) track how the user profiling
and preferences evolve over time, and (3) generate personalized responses
accordingly in new scenarios.
  In this work, we introduce the PERSONAMEM benchmark. PERSONAMEM features
curated user profiles with over 180 simulated user-LLM interaction histories,
each containing up to 60 sessions of multi-turn conversations across 15
real-world tasks that require personalization. Given an in-situ user query,
i.e. query issued by the user from the first-person perspective, we evaluate
LLM chatbots' ability to identify the most suitable response according to the
current state of the user's profile. We observe that current LLMs still
struggle to recognize the dynamic evolution in users' profiles over time
through direct prompting approaches. As a consequence, LLMs often fail to
deliver responses that align with users' current situations and preferences,
with frontier models such as GPT-4.1, o4-mini, GPT-4.5, o1, or Gemini-2.0
achieving only around 50% overall accuracy, suggesting room for improvement. We
hope that PERSONAMEM, along with the user profile and conversation simulation
pipeline, can facilitate future research in the development of truly user-aware
chatbots. Code and data are available at github.com/bowen-upenn/PersonaMem.

</details>


### [17] [Probing the Subtle Ideological Manipulation of Large Language Models](https://arxiv.org/abs/2504.14287)
*Demetris Paschalides,George Pallis,Marios D. Dikaiakos*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在政治意识形态光谱上的可操纵性，超越了传统的左-右二元偏见，提出了多任务数据集并验证了微调对意识形态对齐的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLMs在左-右二元政治偏见上的表现，而忽略了更广泛的意识形态光谱。本研究旨在填补这一空白，探索LLMs在多元政治意识形态中的可操纵性。

Method: 通过构建多任务数据集（包括意识形态问答、声明排序、宣言填空和国会法案理解），并对Phi-2、Mistral和Llama-3三个LLMs进行微调，评估其意识形态表达能力。

Result: 研究发现微调显著提升了LLMs在意识形态光谱上的对齐能力，而显式提示仅带来微小改进，表明模型易受意识形态操纵。

Conclusion: LLMs对意识形态操纵的敏感性凸显了加强模型安全防护的必要性。

Abstract: Large Language Models (LLMs) have transformed natural language processing,
but concerns have emerged about their susceptibility to ideological
manipulation, particularly in politically sensitive areas. Prior work has
focused on binary Left-Right LLM biases, using explicit prompts and fine-tuning
on political QA datasets. In this work, we move beyond this binary approach to
explore the extent to which LLMs can be influenced across a spectrum of
political ideologies, from Progressive-Left to Conservative-Right. We introduce
a novel multi-task dataset designed to reflect diverse ideological positions
through tasks such as ideological QA, statement ranking, manifesto cloze
completion, and Congress bill comprehension. By fine-tuning three LLMs-Phi-2,
Mistral, and Llama-3-on this dataset, we evaluate their capacity to adopt and
express these nuanced ideologies. Our findings indicate that fine-tuning
significantly enhances nuanced ideological alignment, while explicit prompts
provide only minor refinements. This highlights the models' susceptibility to
subtle ideological manipulation, suggesting a need for more robust safeguards
to mitigate these risks.

</details>


### [18] [Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach](https://arxiv.org/abs/2504.14321)
*Xingyu Li,Chen Gong,Guohong Fu*

Main category: cs.CL

TL;DR: 本文介绍了TikTalkCoref，首个针对中文社交媒体多模态共指消解的数据集，填补了真实对话场景的研究空白，并提出了基准方法。


<details>
  <summary>Details</summary>
Motivation: 多模态共指消解（MCR）对理解跨模态内容至关重要，但缺乏真实对话场景的数据资源。

Method: 构建TikTalkCoref数据集，包含短视频与用户评论的文本对话，并标注人物共指关系；提出基准方法进行实验。

Result: 提供了可靠的数据集和基准结果，支持未来MCR研究。

Conclusion: TikTalkCoref将促进真实社交媒体对话的多模态共指消解研究。

Abstract: Multimodal coreference resolution (MCR) aims to identify mentions referring
to the same entity across different modalities, such as text and visuals, and
is essential for understanding multimodal content. In the era of rapidly
growing mutimodal content and social media, MCR is particularly crucial for
interpreting user interactions and bridging text-visual references to improve
communication and personalization. However, MCR research for real-world
dialogues remains unexplored due to the lack of sufficient data resources.To
address this gap, we introduce TikTalkCoref, the first Chinese multimodal
coreference dataset for social media in real-world scenarios, derived from the
popular Douyin short-video platform. This dataset pairs short videos with
corresponding textual dialogues from user comments and includes manually
annotated coreference clusters for both person mentions in the text and the
coreferential person head regions in the corresponding video frames. We also
present an effective benchmark approach for MCR, focusing on the celebrity
domain, and conduct extensive experiments on our dataset, providing reliable
benchmark results for this newly constructed dataset. We will release the
TikTalkCoref dataset to facilitate future research on MCR for real-world social
media dialogues.

</details>


### [19] [Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models](https://arxiv.org/abs/2504.14366)
*Patrick Haller,Jonas Golde,Alan Akbik*

Main category: cs.CL

TL;DR: 本文研究了从Transformer教师模型到九种子二次复杂度学生模型的知识蒸馏效果，探讨了不同架构对蒸馏过程的影响及初始化策略的作用。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制在推理时的二次复杂度成为瓶颈，促使探索子二次复杂度的替代方案。

Method: 系统地评估了从Transformer教师模型到九种子二次复杂度学生模型的知识蒸馏效果，研究了初始化策略的影响。

Result: 实证结果揭示了效率与性能之间的权衡，为成功知识迁移提供了关键因素。

Conclusion: 研究为子二次复杂度架构的知识蒸馏提供了实用指导，强调了架构选择和初始化策略的重要性。

Abstract: Knowledge distillation is a widely used technique for compressing large
language models (LLMs) by training a smaller student model to mimic a larger
teacher model. Typically, both the teacher and student are Transformer-based
architectures, leveraging softmax attention for sequence modeling. However, the
quadratic complexity of self-attention at inference time remains a significant
bottleneck, motivating the exploration of subquadratic alternatives such as
structured state-space models (SSMs), linear attention, and recurrent
architectures. In this work, we systematically evaluate the transferability of
knowledge distillation from a Transformer teacher to nine subquadratic student
architectures. Our study aims to determine which subquadratic model best aligns
with the teacher's learned representations and how different architectural
constraints influence the distillation process. We also investigate the impact
of intelligent initialization strategies, including matrix mixing and
query-key-value (QKV) copying, on the adaptation process. Our empirical results
on multiple NLP benchmarks provide insights into the trade-offs between
efficiency and performance, highlighting key factors for successful knowledge
transfer to subquadratic architectures.

</details>


### [20] [Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites](https://arxiv.org/abs/2504.14367)
*Gabriel Machado Santos,Rita Maria da Silva Julia,Marcelo Zanchetta do Nascimento*

Main category: cs.CL

TL;DR: 本文提出了一种结合上下文无关文法（CFG）和MAP-Elites算法的进化方法，系统探索提示空间，优化大型语言模型（LLMs）的性能。


<details>
  <summary>Details</summary>
Motivation: 提示工程对优化LLMs至关重要，但提示结构与任务性能之间的关系尚未充分研究。

Method: 使用CFG和MAP-Elites算法，生成高质量且多样化的提示，并分析其与不同任务的匹配度。

Result: 在多个LLMs和七项BigBench Lite任务上的实验表明，该方法显著提升了提示的质量和多样性。

Conclusion: 该方法揭示了提示结构对LLM性能的影响，为任务特定和适应性提示设计提供了实用指导。

Abstract: Prompt engineering is essential for optimizing large language models (LLMs),
yet the link between prompt structures and task performance remains
underexplored. This work introduces an evolutionary approach that combines
context-free grammar (CFG) with the MAP-Elites algorithm to systematically
explore the prompt space. Our method prioritizes quality and diversity,
generating high-performing and structurally varied prompts while analyzing
their alignment with diverse tasks by varying traits such as the number of
examples (shots) and reasoning depth. By systematically mapping the phenotypic
space, we reveal how structural variations influence LLM performance, offering
actionable insights for task-specific and adaptable prompt design. Evaluated on
seven BigBench Lite tasks across multiple LLMs, our results underscore the
critical interplay of quality and diversity, advancing the effectiveness and
versatility of LLMs.

</details>


### [21] [ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data](https://arxiv.org/abs/2504.14452)
*Tong Chen,Faeze Brahman,Jiacheng Liu,Niloofar Mireshghallah,Weijia Shi,Pang Wei Koh,Luke Zettlemoyer,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: ParaPO是一种后训练方法，通过微调语言模型以减少无意识的逐字复述，同时保持其整体实用性。


<details>
  <summary>Details</summary>
Motivation: 语言模型可能无意识地复述预训练数据中的内容，引发版权、抄袭、隐私和创造力等问题。

Method: ParaPO训练模型偏好对记忆内容的改写版本，而非逐字复述，并通过系统提示控制复述行为。

Result: 在Llama3.1-8B和Tulu3-8B上的评估显示，ParaPO显著减少了复述行为，同时保留了引用著名语录的能力。

Conclusion: ParaPO是一种有效的减少语言模型无意识复述的方法，且优于传统的遗忘方法。

Abstract: Language models (LMs) can memorize and reproduce segments from their
pretraining data verbatim even in non-adversarial settings, raising concerns
about copyright, plagiarism, privacy, and creativity. We introduce Paraphrase
Preference Optimization (ParaPO), a post-training method that fine-tunes LMs to
reduce unintentional regurgitation while preserving their overall utility.
ParaPO trains LMs to prefer paraphrased versions of memorized segments over the
original verbatim content from the pretraining data. To maintain the ability to
recall famous quotations when appropriate, we develop a variant of ParaPO that
uses system prompts to control regurgitation behavior. In our evaluation on
Llama3.1-8B, ParaPO consistently reduces regurgitation across all tested
datasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative
writing), whereas unlearning methods used in prior work to mitigate
regurgitation are less effective outside their targeted unlearned domain (from
17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO
with system prompting successfully preserves famous quotation recall while
reducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when
prompted not to regurgitate. In contrast, without ParaPO tuning, prompting the
model not to regurgitate produces only a marginal reduction (8.7 to 8.4).

</details>


### [22] [CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge](https://arxiv.org/abs/2504.14462)
*Armin Toroghi,Willis Guo,Scott Sanner*

Main category: cs.CL

TL;DR: 论文提出新数据集CoLoTa，用于评估大语言模型（LLMs）和知识图谱问答（KGQA）方法在长尾实体上的常识推理能力及抗幻觉能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在常识推理方面表现优异，但在长尾实体上的推理错误和幻觉问题仍阻碍其在高风险场景中的应用。

Method: 构建了包含3,300个查询的CoLoTa数据集，涵盖多种常识推理任务，并支持知识图谱问答。

Result: 实验表明，现有LLM-based KGQA方法在涉及常识推理的查询上表现不佳。

Conclusion: CoLoTa可作为评估LLMs和KGQA方法在常识推理及抗幻觉能力上的新基准。

Abstract: The rise of Large Language Models (LLMs) has redefined the AI landscape,
particularly due to their ability to encode factual and commonsense knowledge,
and their outstanding performance in tasks requiring reasoning. Despite these
advances, hallucinations and reasoning errors remain a significant barrier to
their deployment in high-stakes settings. In this work, we observe that even
the most prominent LLMs, such as OpenAI-o1, suffer from high rates of reasoning
errors and hallucinations on tasks requiring commonsense reasoning over
obscure, long-tail entities. To investigate this limitation, we present a new
dataset for Commonsense reasoning over Long-Tail entities (CoLoTa), that
consists of 3,300 queries from question answering and claim verification tasks
and covers a diverse range of commonsense reasoning skills. We remark that
CoLoTa can also serve as a Knowledge Graph Question Answering (KGQA) dataset
since the support of knowledge required to answer its queries is present in the
Wikidata knowledge graph. However, as opposed to existing KGQA benchmarks that
merely focus on factoid questions, our CoLoTa queries also require commonsense
reasoning. Our experiments with strong LLM-based KGQA methodologies indicate
their severe inability to answer queries involving commonsense reasoning.
Hence, we propose CoLoTa as a novel benchmark for assessing both (i) LLM
commonsense reasoning capabilities and their robustness to hallucinations on
long-tail entities and (ii) the commonsense reasoning capabilities of KGQA
methods.

</details>


### [23] [sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning Approach to Brain-Language Alignment](https://arxiv.org/abs/2504.14468)
*Yijun Liu*

Main category: cs.CL

TL;DR: SSENSE是一种对比学习框架，将sEEG信号映射到CLIP模型的句子嵌入空间，实现从脑活动直接检索句子。


<details>
  <summary>Details</summary>
Motivation: 探索多模态基础模型在将侵入性脑记录与自然语言对齐方面的潜力。

Method: 使用InfoNCE损失在sEEG的频谱表示上训练神经编码器，不微调文本编码器。

Result: 在有限数据下，SSENSE展示了通用语言表示可作为神经解码的有效先验。

Conclusion: 通用语言表示可用于神经解码，SSENSE展示了其潜力。

Abstract: Interpreting neural activity through meaningful latent representations
remains a complex and evolving challenge at the intersection of neuroscience
and artificial intelligence. We investigate the potential of multimodal
foundation models to align invasive brain recordings with natural language. We
present SSENSE, a contrastive learning framework that projects single-subject
stereo-electroencephalography (sEEG) signals into the sentence embedding space
of a frozen CLIP model, enabling sentence-level retrieval directly from brain
activity. SSENSE trains a neural encoder on spectral representations of sEEG
using InfoNCE loss, without fine-tuning the text encoder. We evaluate our
method on time-aligned sEEG and spoken transcripts from a naturalistic
movie-watching dataset. Despite limited data, SSENSE achieves promising
results, demonstrating that general-purpose language representations can serve
as effective priors for neural decoding.

</details>


### [24] [DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for Multi-Party Dialogue](https://arxiv.org/abs/2504.14482)
*Xiang Li,Duyi Pan,Hongru Xiao,Jiale Han,Jing Tang,Jiabao Ma,Wei Wang,Bo Cheng*

Main category: cs.CL

TL;DR: 论文提出了一种基于混合代理的语音合成框架DialogueAgents，通过三个专业代理（脚本编写者、语音合成器和对话评论家）协作生成对话，解决了现有数据集成本高、多样性不足的问题，并贡献了一个高质量的双语多轮对话数据集MultiTalk。


<details>
  <summary>Details</summary>
Motivation: 现有语音合成数据集构建成本高，且缺乏多样性、上下文场景和情感表达，限制了人机交互的自然性和直观性。

Method: 提出DialogueAgents框架，整合三个代理（脚本编写者、语音合成器、对话评论家），基于多样化角色池迭代优化对话脚本和语音合成，提升情感表达和副语言特征。

Result: 贡献了MultiTalk数据集，实验证明框架有效且数据集质量高。

Conclusion: DialogueAgents框架和MultiTalk数据集为高级语音合成模型和定制化数据生成提供了支持，代码和数据集已开源。

Abstract: Speech synthesis is crucial for human-computer interaction, enabling natural
and intuitive communication. However, existing datasets involve high
construction costs due to manual annotation and suffer from limited character
diversity, contextual scenarios, and emotional expressiveness. To address these
issues, we propose DialogueAgents, a novel hybrid agent-based speech synthesis
framework, which integrates three specialized agents -- a script writer, a
speech synthesizer, and a dialogue critic -- to collaboratively generate
dialogues. Grounded in a diverse character pool, the framework iteratively
refines dialogue scripts and synthesizes speech based on speech review,
boosting emotional expressiveness and paralinguistic features of the
synthesized dialogues. Using DialogueAgent, we contribute MultiTalk, a
bilingual, multi-party, multi-turn speech dialogue dataset covering diverse
topics. Extensive experiments demonstrate the effectiveness of our framework
and the high quality of the MultiTalk dataset. We release the dataset and code
https://github.com/uirlx/DialogueAgents to facilitate future research on
advanced speech synthesis models and customized data generation.

</details>


### [25] [FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering](https://arxiv.org/abs/2504.14492)
*Yichen Li,Zhiting Fan,Ruizhe Chen,Xiaotang Gai,Luqi Gong,Yan Zhang,Zuozhu Liu*

Main category: cs.CL

TL;DR: FairSteer是一种无需定制提示或模型重新训练的新型推理时去偏框架，通过线性表示假设检测和调整激活空间中的偏置特征。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）容易从训练数据中捕获偏见，现有方法存在不稳定或计算成本高的问题。

Method: FairSteer通过三个步骤实现去偏：检测偏置激活、计算去偏导向向量（DSV）和动态调整激活。

Result: 在六种LLM上的综合评估显示，FairSteer在问答、反事实输入评估和开放文本生成任务中表现优越。

Conclusion: FairSteer提供了一种高效且稳定的去偏方法，无需额外训练或复杂提示设计。

Abstract: Large language models (LLMs) are prone to capturing biases from training
corpus, leading to potential negative social impacts. Existing prompt-based
debiasing methods exhibit instability due to their sensitivity to prompt
changes, while fine-tuning-based techniques incur substantial computational
overhead and catastrophic forgetting. In this paper, we propose FairSteer, a
novel inference-time debiasing framework without requiring customized prompt
design or model retraining. Motivated by the linear representation hypothesis,
our preliminary investigation demonstrates that fairness-related features can
be encoded into separable directions in the hidden activation space. FairSteer
operates in three steps: biased activation detection, debiasing steering vector
(DSV) computation, and dynamic activation steering. Specifically, it first
trains a lightweight linear classifier to detect bias signatures in
activations, and then computes DSVs as intervention directions derived from
small contrastive prompt pairs. Subsequently, it performs debiasing by
adjusting activations with DSVs in the inference stage. Comprehensive
evaluation with six LLMs demonstrates the superiority of FairSteer across
question-answering, counterfactual input evaluation and open-ended text
generation tasks. Code will be released.

</details>


### [26] [Functional Abstraction of Knowledge Recall in Large Language Models](https://arxiv.org/abs/2504.14496)
*Zijian Wang,Chang Xu*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLMs）中的知识回忆机制，将其抽象为功能结构，并提出激活向量与功能组件（输入、函数体、返回值）对齐的假设。通过实验验证，改进了基于激活修补的知识编辑方法。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs中知识回忆的机制，并抽象为功能结构，以更好地理解和改进模型的知识处理能力。

Method: 设计基于修补的知识评分算法识别功能组件，并通过反知识测试验证其独立性。改进基于激活修补的上下文知识编辑方法。

Result: 实验表明，激活向量与功能组件对齐的假设成立，改进后的知识编辑方法提升了短期记忆保留能力。

Conclusion: 从功能视角分析知识回忆机制，为LLMs的知识处理提供了新思路，并通过激活修补改进了知识编辑效果。

Abstract: Pre-trained transformer large language models (LLMs) demonstrate strong
knowledge recall capabilities. This paper investigates the knowledge recall
mechanism in LLMs by abstracting it into a functional structure. We propose
that during knowledge recall, the model's hidden activation space implicitly
entails a function execution process where specific activation vectors align
with functional components (Input argument, Function body, and Return values).
Specifically, activation vectors of relation-related tokens define a mapping
function from subjects to objects, with subject-related token activations
serving as input arguments and object-related token activations as return
values. For experimental verification, we first design a patching-based
knowledge-scoring algorithm to identify knowledge-aware activation vectors as
independent functional components. Then, we conduct counter-knowledge testing
to examine the independent functional effects of each component on knowledge
recall outcomes. From this functional perspective, we improve the contextual
knowledge editing approach augmented by activation patching. By rewriting
incoherent activations in context, we enable improved short-term memory
retention for new knowledge prompting.

</details>


### [27] [Causality for Natural Language Processing](https://arxiv.org/abs/2504.14530)
*Zhijing Jin*

Main category: cs.CL

TL;DR: 该论文探讨了大语言模型（LLMs）中的因果推理能力及其在自然语言处理和社会科学中的应用。


<details>
  <summary>Details</summary>
Motivation: 因果推理是人类智能的核心，也是人工智能系统实现高级理解和决策的关键能力。

Method: 通过一系列研究、新数据集、基准任务和方法框架，分析了LLMs的因果推理机制。

Result: 揭示了LLMs在因果推理中的关键挑战和机遇，为未来研究提供了基础。

Conclusion: 该研究为提升LLMs的因果推理能力提供了全面框架，并指出了未来研究方向。

Abstract: Causal reasoning is a cornerstone of human intelligence and a critical
capability for artificial systems aiming to achieve advanced understanding and
decision-making. This thesis delves into various dimensions of causal reasoning
and understanding in large language models (LLMs). It encompasses a series of
studies that explore the causal inference skills of LLMs, the mechanisms behind
their performance, and the implications of causal and anticausal learning for
natural language processing (NLP) tasks. Additionally, it investigates the
application of causal reasoning in text-based computational social science,
specifically focusing on political decision-making and the evaluation of
scientific impact through citations. Through novel datasets, benchmark tasks,
and methodological frameworks, this work identifies key challenges and
opportunities to improve the causal capabilities of LLMs, providing a
comprehensive foundation for future research in this evolving field.

</details>


### [28] [BookWorld: From Novels to Interactive Agent Societies for Creative Story Generation](https://arxiv.org/abs/2504.14538)
*Yiting Ran,Xintao Wang,Tian Qiu,Jiaqing Liang,Yanghua Xiao,Deqing Yang*

Main category: cs.CL

TL;DR: BookWorld是一个基于书籍的多智能体社会模拟系统，能够生成高质量故事并保持对原著的忠实度，胜率为75.36%。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注从头创建智能体社会，而模拟已建立的虚构世界和角色尚未充分探索，具有重要实用价值。

Method: 设计BookWorld系统，涵盖角色多样性、虚构世界观、地理约束等现实复杂性。

Result: 实验表明，BookWorld在故事生成中表现优异，胜率为75.36%。

Conclusion: BookWorld为扩展和探索虚构作品提供了新途径，具有广泛的应用潜力。

Abstract: Recent advances in large language models (LLMs) have enabled social
simulation through multi-agent systems. Prior efforts focus on agent societies
created from scratch, assigning agents with newly defined personas. However,
simulating established fictional worlds and characters remain largely
underexplored, despite its significant practical value. In this paper, we
introduce BookWorld, a comprehensive system for constructing and simulating
book-based multi-agent societies. BookWorld's design covers comprehensive
real-world intricacies, including diverse and dynamic characters, fictional
worldviews, geographical constraints and changes, e.t.c. BookWorld enables
diverse applications including story generation, interactive games and social
simulation, offering novel ways to extend and explore beloved fictional works.
Through extensive experiments, we demonstrate that BookWorld generates
creative, high-quality stories while maintaining fidelity to the source books,
surpassing previous methods with a win rate of 75.36%. The code of this paper
can be found at the project page: https://bookworld2025.github.io/.

</details>


### [29] [a1: Steep Test-time Scaling Law via Environment Augmented Generation](https://arxiv.org/abs/2504.14597)
*Lingrui Mei,Shenghua Liu,Yiwei Wang,Baolong Bi,Yuyao Ge,Jun Wan,Yurong Wu,Xueqi Cheng*

Main category: cs.CL

TL;DR: EAG框架通过环境反馈、动态分支探索和经验学习提升LLM的推理能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂多步任务中的幻觉、逻辑错误和无法自我纠正的问题。

Method: 提出EAG框架，包括实时环境反馈、动态分支探索和成功轨迹的经验学习。

Result: a1-32B模型在基准测试中表现优异，超越同类模型24.4个百分点。

Conclusion: EAG为可靠机器推理提供了新范式，特别适用于需要精确多步计算和逻辑验证的问题。

Abstract: Large Language Models (LLMs) have made remarkable breakthroughs in reasoning,
yet continue to struggle with hallucinations, logical errors, and inability to
self-correct during complex multi-step tasks. Current approaches like
chain-of-thought prompting offer limited reasoning capabilities that fail when
precise step validation is required. We propose Environment Augmented
Generation (EAG), a framework that enhances LLM reasoning through: (1)
real-time environmental feedback validating each reasoning step, (2) dynamic
branch exploration for investigating alternative solution paths when faced with
errors, and (3) experience-based learning from successful reasoning
trajectories. Unlike existing methods, EAG enables deliberate backtracking and
strategic replanning through tight integration of execution feedback with
branching exploration. Our a1-32B model achieves state-of-the-art performance
among similar-sized models across all benchmarks, matching larger models like
o1 on competition mathematics while outperforming comparable models by up to
24.4 percentage points. Analysis reveals EAG's distinctive scaling pattern:
initial token investment in environment interaction yields substantial
long-term performance dividends, with advantages amplifying proportionally to
task complexity. EAG's theoretical framework demonstrates how environment
interactivity and systematic branch exploration together establish a new
paradigm for reliable machine reasoning, particularly for problems requiring
precise multi-step calculation and logical verification.

</details>


### [30] [Translation Analytics for Freelancers: I. Introduction, Data Preparation, Baseline Evaluations](https://arxiv.org/abs/2504.14619)
*Yuri Balashov,Alex Balashov,Shiho Fukuda Koski*

Main category: cs.CL

TL;DR: 本文是系列论文的第一篇，探讨了语言技术为个体译者和资源有限的语言服务提供商带来的新机遇，提出了适应自由职业者需求的自动评估指标框架。


<details>
  <summary>Details</summary>
Motivation: 研究旨在帮助自由职业者利用先进的神经机器翻译系统和大语言模型，提升翻译质量和工作效率。

Method: 采用翻译分析技术，将自动评估指标（如BLEU、chrF、TER和COMET）适配到自由职业者的需求中，并通过医学领域的三语语料库进行实证分析。

Result: 研究发现，自动评估指标与人工评估结果具有统计相关性，证明了其在自由职业者工作中的实用性。

Conclusion: 自由职业者应积极拥抱新兴技术，以在不断变化的专业环境中适应并取得成功。

Abstract: This is the first in a series of papers exploring the rapidly expanding new
opportunities arising from recent progress in language technologies for
individual translators and language service providers with modest resources.
The advent of advanced neural machine translation systems, large language
models, and their integration into workflows via computer-assisted translation
tools and translation management systems have reshaped the translation
landscape. These advancements enable not only translation but also quality
evaluation, error spotting, glossary generation, and adaptation to
domain-specific needs, creating new technical opportunities for freelancers. In
this series, we aim to empower translators with actionable methods to harness
these advancements. Our approach emphasizes Translation Analytics, a suite of
evaluation techniques traditionally reserved for large-scale industry
applications but now becoming increasingly available for smaller-scale users.
This first paper introduces a practical framework for adapting automatic
evaluation metrics -- such as BLEU, chrF, TER, and COMET -- to freelancers'
needs. We illustrate the potential of these metrics using a trilingual corpus
derived from a real-world project in the medical domain and provide statistical
analysis correlating human evaluations with automatic scores. Our findings
emphasize the importance of proactive engagement with emerging technologies to
not only adapt but thrive in the evolving professional environment.

</details>


### [31] [A Hierarchical Framework for Measuring Scientific Paper Innovation via Large Language Models](https://arxiv.org/abs/2504.14620)
*Hongming Tan,Shaoxiong Zhan,Fengwei Jia,Hai-Tao Zheng,Wai Kin Chan*

Main category: cs.CL

TL;DR: HSPIM是一种基于大语言模型的层次化、免训练框架，通过分解论文为章节和问答对来评估创新性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于内容的方法难以全面捕捉论文创新性且缺乏泛化能力，需要更有效的评估框架。

Method: HSPIM通过Paper-to-Sections-to-QAs分解，利用零样本LLM提示实现章节分类、QA增强和加权新颖性评分。

Result: 实验表明HSPIM在有效性、泛化性和可解释性上优于基线方法。

Conclusion: HSPIM为科学论文创新性评估提供了高效且可解释的解决方案。

Abstract: Measuring scientific paper innovation is both important and challenging.
Existing content-based methods often overlook the full-paper context, fail to
capture the full scope of innovation, and lack generalization. We propose
HSPIM, a hierarchical and training-free framework based on large language
models (LLMs). It introduces a Paper-to-Sections-to-QAs decomposition to assess
innovation. We segment the text by section titles and use zero-shot LLM
prompting to implement section classification, question-answering (QA)
augmentation, and weighted novelty scoring. The generated QA pair focuses on
section-level innovation and serves as additional context to improve the LLM
scoring. For each chunk, the LLM outputs a novelty score and a confidence
score. We use confidence scores as weights to aggregate novelty scores into a
paper-level innovation score. To further improve performance, we propose a
two-layer question structure consisting of common and section-specific
questions, and apply a genetic algorithm to optimize the question-prompt
combinations. Comprehensive experiments on scientific conference paper datasets
show that HSPIM outperforms baseline methods in effectiveness, generalization,
and interpretability.

</details>


### [32] [Automatic Text Summarization (ATS) for Research Documents in Sorani Kurdish](https://arxiv.org/abs/2504.14630)
*Rondik Hadi Abdulrahman,Hossein Hassani*

Main category: cs.CL

TL;DR: 该研究为库尔德语（Sorani方言）开发了一个数据集和语言模型，用于自动文本摘要（ATS），填补了该语言资源不足的空白。通过两种实验方法（包含和不包含结论）评估了摘要效果，最佳准确率达到19.58%。


<details>
  <summary>Details</summary>
Motivation: 库尔德语在自动文本摘要领域资源匮乏，限制了相关研究的发展。本研究旨在填补这一空白，为库尔德语NLP研究提供基础资源。

Method: 研究基于231篇库尔德语科学论文，使用句子加权和TF-IDF算法进行摘要生成，并通过手动和自动（ROUGE指标）评估结果。

Result: 实验结果显示，最佳摘要准确率为19.58%，且专家手动评估结果因文档而异。

Conclusion: 该研究为库尔德语ATS领域提供了宝贵资源，为未来研究奠定了基础。

Abstract: Extracting concise information from scientific documents aids learners,
researchers, and practitioners. Automatic Text Summarization (ATS), a key
Natural Language Processing (NLP) application, automates this process. While
ATS methods exist for many languages, Kurdish remains underdeveloped due to
limited resources. This study develops a dataset and language model based on
231 scientific papers in Sorani Kurdish, collected from four academic
departments in two universities in the Kurdistan Region of Iraq (KRI),
averaging 26 pages per document. Using Sentence Weighting and Term
Frequency-Inverse Document Frequency (TF-IDF) algorithms, two experiments were
conducted, differing in whether the conclusions were included. The average word
count was 5,492.3 in the first experiment and 5,266.96 in the second. Results
were evaluated manually and automatically using ROUGE-1, ROUGE-2, and ROUGE-L
metrics, with the best accuracy reaching 19.58%. Six experts conducted manual
evaluations using three criteria, with results varying by document. This
research provides valuable resources for Kurdish NLP researchers to advance ATS
and related fields.

</details>


### [33] [Harnessing Generative LLMs for Enhanced Financial Event Entity Extraction Performance](https://arxiv.org/abs/2504.14633)
*Soo-joon Choi,Ji-jun Park*

Main category: cs.CL

TL;DR: 论文提出了一种基于生成式大语言模型（LLM）的金融事件实体提取方法，通过将任务重构为文本到结构化输出的生成任务，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 金融事件实体提取对市场动态分析和知识图谱构建至关重要，但传统序列标注模型难以处理金融文本的复杂性和长距离依赖。

Method: 采用参数高效微调（PEFT）对预训练LLM进行微调，直接生成包含实体及其字符跨度的结构化输出（如JSON）。

Result: 在CCKS 2019数据集上，该方法取得了新的最优F1分数，显著优于序列标注基线模型。

Conclusion: 生成式LLM在复杂领域特定信息提取任务中具有潜力，尤其适用于需要结构化输出的场景。

Abstract: Financial event entity extraction is a crucial task for analyzing market
dynamics and building financial knowledge graphs, yet it presents significant
challenges due to the specialized language and complex structures in financial
texts. Traditional approaches often rely on sequence labeling models, which can
struggle with long-range dependencies and the inherent complexity of extracting
multiple, potentially overlapping entities. Motivated by the advanced language
understanding and generative capabilities of Large Language Models (LLMs), we
propose a novel method that reframes financial event entity extraction as a
text-to-structured-output generation task. Our approach involves fine-tuning a
pre-trained LLM using Parameter-Efficient Fine-Tuning (PEFT) to directly
generate a structured representation, such as a JSON object, containing the
extracted entities and their precise character spans from the input text. We
evaluate our method on the challenging CCKS 2019 Financial Event Entity
Extraction dataset, comparing its performance against strong sequence labeling
baselines, including SEBERTNets and sebertNets. Experimental results
demonstrate that our generative LLM method achieves a new state-of-the-art F1
score on this benchmark, significantly outperforming previous methods. Through
detailed quantitative analysis across event types, entity types, and instance
complexity, as well as human evaluation, we show that our approach is more
effective at handling the nuances of financial text and extracting high-quality
entities. This work validates the potential of applying generative LLMs
directly to complex, domain-specific information extraction tasks requiring
structured output.

</details>


### [34] [A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs](https://arxiv.org/abs/2504.14657)
*Yihan Lin,Zhirong Bella Yu,Simon Lee*

Main category: cs.CL

TL;DR: LLMs生成合成电子健康记录（EHRs）在小规模特征上表现可靠，但在高维数据中难以保持真实分布和相关性，限制了跨医院泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决合成EHRs在跨医院泛化中的挑战，评估LLMs在生成合成数据中的表现。

Method: 评估商业LLMs生成合成数据的能力，分析生成过程中的多个方面。

Result: LLMs在小规模特征上表现良好，但在高维数据中难以保持真实性和相关性。

Conclusion: LLMs在生成合成EHRs时存在局限性，需进一步改进以支持跨医院应用。

Abstract: Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to
create privacy preserving and harmonized structured data, supporting numerous
applications in healthcare. Key benefits of synthetic data include precise
control over the data schema, improved fairness and representation of patient
populations, and the ability to share datasets without concerns about
compromising real individuals privacy. Consequently, the AI community has
increasingly turned to Large Language Models (LLMs) to generate synthetic data
across various domains. However, a significant challenge in healthcare is
ensuring that synthetic health records reliably generalize across different
hospitals, a long standing issue in the field. In this work, we evaluate the
current state of commercial LLMs for generating synthetic data and investigate
multiple aspects of the generation process to identify areas where these models
excel and where they fall short. Our main finding from this work is that while
LLMs can reliably generate synthetic health records for smaller subsets of
features, they struggle to preserve realistic distributions and correlations as
the dimensionality of the data increases, ultimately limiting their ability to
generalize across diverse hospital settings.

</details>


### [35] [Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data](https://arxiv.org/abs/2504.14669)
*Wei Zou,Sen Yang,Yu Bao,Shujian Huang,Jiajun Chen,Shanbo Cheng*

Main category: cs.CL

TL;DR: TRANS-ZERO是一个利用单语数据和LLM内在多语言知识的自学习框架，通过结合G-MCTS和偏好优化，实现了与监督方法媲美的翻译性能。


<details>
  <summary>Details</summary>
Motivation: 解决多语言机器翻译中低资源语言数据稀缺和灾难性遗忘的问题。

Method: 提出TRANS-ZERO框架，结合遗传蒙特卡洛树搜索（G-MCTS）和偏好优化，仅使用单语数据。

Result: 实验表明，该方法不仅匹配大规模并行数据训练的模型性能，还在非英语翻译方向上表现优异。

Conclusion: G-MCTS通过迭代翻译探索语义一致的候选，显著提升翻译质量，为框架成功奠定基础。

Abstract: The rise of Large Language Models (LLMs) has reshaped machine translation
(MT), but multilingual MT still relies heavily on parallel data for supervised
fine-tuning (SFT), facing challenges like data scarcity for low-resource
languages and catastrophic forgetting. To address these issues, we propose
TRANS-ZERO, a self-play framework that leverages only monolingual data and the
intrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic
Monte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong
translation performance that rivals supervised methods. Experiments demonstrate
that this approach not only matches the performance of models trained on
large-scale parallel data but also excels in non-English translation
directions. Further analysis reveals that G-MCTS itself significantly enhances
translation quality by exploring semantically consistent candidates through
iterative translations, providing a robust foundation for the framework's
succuss.

</details>


### [36] [FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models](https://arxiv.org/abs/2504.14690)
*Mehrnoush Shamsfard,Zahra Saaberi,Mostafa Karimi manesh,Seyed Mohammad Hossein Hashemi,Zahra Vatankhah,Motahareh Ramezani,Niki Pourazin,Tara Zare,Maryam Azimi,Sarina Chitsaz,Sama Khoraminejad,Morteza Mahdavi Mortazavi,Mohammad Mahdi Chizari,Sahar Maleki,Seyed Soroush Majd,Mostafa Masumi,Sayed Ali Musavi Khoeini,Amir Mohseni,Sogol Alipour*

Main category: cs.CL

TL;DR: 论文介绍了FarsEval-PKBETS基准，用于评估波斯语大型语言模型（LLMs）的性能，结果显示当前模型的平均准确率低于50%。


<details>
  <summary>Details</summary>
Motivation: 资源丰富的语言（如英语）的LLMs研究较多，而波斯语等语言的性能研究较少，因此需要专门的评估基准。

Method: 开发了包含4000个问题的FarsEval-PKBETS基准，涵盖多领域任务，并评估了三个模型的性能。

Result: 三个模型（Llama3-70B、PersianMind和Dorna）的平均准确率低于50%。

Conclusion: 当前LLMs在波斯语任务上表现不足，仍需改进。

Abstract: Research on evaluating and analyzing large language models (LLMs) has been
extensive for resource-rich languages such as English, yet their performance in
languages such as Persian has received considerably less attention. This paper
introduces FarsEval-PKBETS benchmark, a subset of FarsEval project for
evaluating large language models in Persian. This benchmark consists of 4000
questions and answers in various formats, including multiple choice, short
answer and descriptive responses. It covers a wide range of domains and
tasks,including medicine, law, religion, Persian language, encyclopedic
knowledge, human preferences, social knowledge, ethics and bias, text
generation, and respecting others' rights. This bechmark incorporates
linguistics, cultural, and local considerations relevant to the Persian
language and Iran. To ensure the questions are challenging for current LLMs,
three models -- Llama3-70B, PersianMind, and Dorna -- were evaluated using this
benchmark. Their average accuracy was below 50%, meaning they provided fully
correct answers to fewer than half of the questions. These results indicate
that current language models are still far from being able to solve this
benchmark

</details>


### [37] [OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding](https://arxiv.org/abs/2504.14692)
*Songtao Jiang,Yuan Wang,Sibo Song,Yan Zhang,Zijie Meng,Bohan Lei,Jian Wu,Jimeng Sun,Zuozhu Liu*

Main category: cs.CL

TL;DR: OmniV-Med是一个统一的多模态医疗理解框架，通过构建多模态数据集、设计统一编码器和引入医学感知的令牌修剪机制，实现了在2D/3D医学影像和视频任务上的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有医疗视觉语言模型通常对不同模态使用独立编码器，限制了多模态数据的无缝整合。

Method: 1. 构建OmniV-Med-Instruct数据集（252K样本）；2. 设计旋转位置自适应编码器；3. 引入医学感知令牌修剪机制。

Result: OmniV-Med-7B在7个基准测试中达到最佳性能，轻量版（1.5B）仅需8块RTX3090 GPU即可训练。

Conclusion: OmniV-Med通过统一框架和高效设计，显著提升了多模态医疗理解的性能和实用性。

Abstract: The practical deployment of medical vision-language models (Med-VLMs)
necessitates seamless integration of textual data with diverse visual
modalities, including 2D/3D images and videos, yet existing models typically
employ separate encoders for different modalities. To address this limitation,
we present OmniV-Med, a unified framework for multimodal medical understanding.
Our technical contributions are threefold: First, we construct
OmniV-Med-Instruct, a comprehensive multimodal medical dataset containing 252K
instructional samples spanning 14 medical image modalities and 11 clinical
tasks. Second, we devise a rotary position-adaptive encoder that processes
multi-resolution 2D/3D images and videos within a unified architecture,
diverging from conventional modality-specific encoders. Third, we introduce a
medical-aware token pruning mechanism that exploits spatial-temporal redundancy
in volumetric data (e.g., consecutive CT slices) and medical videos,
effectively reducing 60\% of visual tokens without performance degradation.
Empirical evaluations demonstrate that OmniV-Med-7B achieves state-of-the-art
performance on 7 benchmarks spanning 2D/3D medical imaging and video
understanding tasks. Notably, our lightweight variant (OmniV-Med-1.5B) attains
comparable performance while requiring only 8 RTX3090 GPUs for training and
supporting efficient long-video inference. Data, code and model will be
released.

</details>


### [38] [Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives](https://arxiv.org/abs/2504.14707)
*Ratna Kandala,Katie Hoemann*

Main category: cs.CL

TL;DR: 研究比较了BERTopic、LDA和KMeans在比利时荷兰语日常叙事中的表现，发现BERTopic在语义相关性上优于LDA和KMeans，强调混合评估框架的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索BERTopic在形态丰富的语言（如比利时荷兰语）中的潜力，并与传统方法（LDA和KMeans）对比。

Method: 使用BERTopic、LDA和KMeans对日常叙事进行主题建模，结合自动指标和人工评估。

Result: BERTopic生成的主题更具文化共鸣，LDA在自动指标上表现好但语义不相关，KMeans表现不如预期。

Conclusion: 研究强调NLP模型需在语言多样性中具备鲁棒性，混合评估框架对形态丰富的语言尤为重要。

Abstract: This study explores BERTopic's potential for modeling open-ended Belgian
Dutch daily narratives, contrasting its performance with Latent Dirichlet
Allocation (LDA) and KMeans. Although LDA scores well on certain automated
metrics, human evaluations reveal semantically irrelevant co-occurrences,
highlighting the limitations of purely statistic-based methods. In contrast,
BERTopic's reliance on contextual embeddings yields culturally resonant themes,
underscoring the importance of hybrid evaluation frameworks that account for
morphologically rich languages. KMeans performed less coherently than prior
research suggested, pointing to the unique challenges posed by personal
narratives. Our findings emphasize the need for robust generalization in NLP
models, especially in underrepresented linguistic contexts.

</details>


### [39] [PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines](https://arxiv.org/abs/2504.14738)
*Reya Vir,Shreya Shankar,Harrison Chase,Will Fu-Hinthorn,Aditya Parameswaran*

Main category: cs.CL

TL;DR: PROMPTEVALS是一个包含2087个LLM管道提示和12623个断言标准的数据集，用于提升LLM在生成输出时的可靠性，其规模是之前数据集的5倍。微调的Mistral和Llama 3模型在生成相关断言上比GPT-4o平均表现优20.93%。


<details>
  <summary>Details</summary>
Motivation: LLM在生产环境中常无法满足开发者期望，需要断言或护栏来提升可靠性，但确定合适的断言标准具有挑战性。

Method: 引入PROMPTEVALS数据集，包含开发者提供的LLM管道提示和断言标准，并使用其测试分割评估闭源和开源模型生成断言的能力。

Result: 微调的Mistral和Llama 3模型在生成断言上优于GPT-4o，平均提升20.93%，且延迟更低。

Conclusion: PROMPTEVALS数据集可推动LLM可靠性、对齐和提示工程的研究。

Abstract: Large language models (LLMs) are increasingly deployed in specialized
production data processing pipelines across diverse domains -- such as finance,
marketing, and e-commerce. However, when running them in production across many
inputs, they often fail to follow instructions or meet developer expectations.
To improve reliability in these applications, creating assertions or guardrails
for LLM outputs to run alongside the pipelines is essential. Yet, determining
the right set of assertions that capture developer requirements for a task is
challenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM
pipeline prompts with 12623 corresponding assertion criteria, sourced from
developers using our open-source LLM pipeline tools. This dataset is 5x larger
than previous collections. Using a hold-out test split of PROMPTEVALS as a
benchmark, we evaluated closed- and open-source models in generating relevant
assertions. Notably, our fine-tuned Mistral and Llama 3 models outperform
GPT-4o by 20.93% on average, offering both reduced latency and improved
performance. We believe our dataset can spur further research in LLM
reliability, alignment, and prompt engineering.

</details>


### [40] [Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings](https://arxiv.org/abs/2504.14766)
*Saniya Karwa,Navpreet Singh*

Main category: cs.CL

TL;DR: 论文提出了一种框架，用于揭示BERT等高维嵌入模型中编码特定语言属性的维度，并引入新数据集LDSP-10和度量标准EDI。


<details>
  <summary>Details</summary>
Motivation: 理解神经嵌入的内部机制，尤其是BERT等高维不透明模型，仍具挑战性。

Method: 使用LDSP-10数据集，结合Wilcoxon检验、互信息和递归特征消除等方法分析BERT嵌入。

Result: 发现否定和极性等属性在特定维度中编码明确，而同义词等属性则更复杂。

Conclusion: 研究为嵌入的可解释性提供了新见解，有助于开发更透明的语言模型，并减少AI系统偏见。

Abstract: Understanding the inner workings of neural embeddings, particularly in models
such as BERT, remains a challenge because of their high-dimensional and opaque
nature. This paper proposes a framework for uncovering the specific dimensions
of vector embeddings that encode distinct linguistic properties (LPs). We
introduce the Linguistically Distinct Sentence Pairs (LDSP-10) dataset, which
isolates ten key linguistic features such as synonymy, negation, tense, and
quantity. Using this dataset, we analyze BERT embeddings with various methods,
including the Wilcoxon signed-rank test, mutual information, and recursive
feature elimination, to identify the most influential dimensions for each LP.
We introduce a new metric, the Embedding Dimension Impact (EDI) score, which
quantifies the relevance of each embedding dimension to a LP. Our findings show
that certain properties, such as negation and polarity, are robustly encoded in
specific dimensions, while others, like synonymy, exhibit more complex
patterns. This study provides insights into the interpretability of embeddings,
which can guide the development of more transparent and optimized language
models, with implications for model bias mitigation and the responsible
deployment of AI systems.

</details>


### [41] [Knowledge Distillation and Dataset Distillation of Large Language Models: Emerging Trends, Challenges, and Future Directions](https://arxiv.org/abs/2504.14772)
*Luyang Fang,Xiaowei Yu,Jiazhang Cai,Yongkai Chen,Shushan Wu,Zhengliang Liu,Zhenyuan Yang,Haoran Lu,Xilin Gong,Yufang Liu,Terry Ma,Wei Ruan,Ali Abbasi,Jing Zhang,Tao Wang,Ehsan Latif,Wei Liu,Wei Zhang,Soheil Kolouri,Xiaoming Zhai,Dajiang Zhu,Wenxuan Zhong,Tianming Liu,Ping Ma*

Main category: cs.CL

TL;DR: 该综述分析了知识蒸馏（KD）和数据集蒸馏（DD）两种互补范式，旨在压缩大型语言模型（LLMs）同时保留其推理能力和语言多样性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的指数增长，需要高效策略以满足计算和数据需求。KD和DD的结合可解决模型可扩展性、架构异质性和能力保留等问题。

Method: 研究了KD中的任务对齐、基于理性的训练和多教师框架，以及DD中的梯度匹配、潜在空间正则化和生成合成等技术。

Result: KD和DD的结合能产生更高效和可扩展的压缩策略，适用于医疗和教育等领域。

Conclusion: 尽管进展显著，仍需解决推理多样性、适应性和评估协议等挑战，未来需进一步整合KD和DD以实现可持续的LLMs。

Abstract: The exponential growth of Large Language Models (LLMs) continues to highlight
the need for efficient strategies to meet ever-expanding computational and data
demands. This survey provides a comprehensive analysis of two complementary
paradigms: Knowledge Distillation (KD) and Dataset Distillation (DD), both
aimed at compressing LLMs while preserving their advanced reasoning
capabilities and linguistic diversity. We first examine key methodologies in
KD, such as task-specific alignment, rationale-based training, and
multi-teacher frameworks, alongside DD techniques that synthesize compact,
high-impact datasets through optimization-based gradient matching, latent space
regularization, and generative synthesis. Building on these foundations, we
explore how integrating KD and DD can produce more effective and scalable
compression strategies. Together, these approaches address persistent
challenges in model scalability, architectural heterogeneity, and the
preservation of emergent LLM abilities. We further highlight applications
across domains such as healthcare and education, where distillation enables
efficient deployment without sacrificing performance. Despite substantial
progress, open challenges remain in preserving emergent reasoning and
linguistic diversity, enabling efficient adaptation to continually evolving
teacher models and datasets, and establishing comprehensive evaluation
protocols. By synthesizing methodological innovations, theoretical foundations,
and practical insights, our survey charts a path toward sustainable,
resource-efficient LLMs through the tighter integration of KD and DD
principles.

</details>


### [42] [Automatic Evaluation Metrics for Document-level Translation: Overview, Challenges and Trends](https://arxiv.org/abs/2504.14804)
*Jiaxin GUO,Xiaoyu Chen,Zhiqiang Rao,Jinlong Yang,Zongyao Li,Hengchao Shang,Daimeng Wei,Hao Yang*

Main category: cs.CL

TL;DR: 本文综述了文档级机器翻译自动评估的现状、挑战及未来趋势，强调了改进评估方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）推动文档级翻译的进步，准确评估翻译质量成为迫切需求。

Method: 分析现有自动评估方案和指标，包括基于参考文本和无参考文本的方法，以及传统、模型和LLM为基础的指标。

Result: 当前评估方法面临参考多样性不足、依赖句子级对齐信息、LLM评估方法的偏见和不准确性等问题。

Conclusion: 未来需开发更友好的文档级评估方法、改进LLM评估方法，并探索减少对句子级信息依赖、多级评估等方向。

Abstract: With the rapid development of deep learning technologies, the field of
machine translation has witnessed significant progress, especially with the
advent of large language models (LLMs) that have greatly propelled the
advancement of document-level translation. However, accurately evaluating the
quality of document-level translation remains an urgent issue. This paper first
introduces the development status of document-level translation and the
importance of evaluation, highlighting the crucial role of automatic evaluation
metrics in reflecting translation quality and guiding the improvement of
translation systems. It then provides a detailed analysis of the current state
of automatic evaluation schemes and metrics, including evaluation methods with
and without reference texts, as well as traditional metrics, Model-based
metrics and LLM-based metrics. Subsequently, the paper explores the challenges
faced by current evaluation methods, such as the lack of reference diversity,
dependence on sentence-level alignment information, and the bias, inaccuracy,
and lack of interpretability of the LLM-as-a-judge method. Finally, the paper
looks ahead to the future trends in evaluation methods, including the
development of more user-friendly document-level evaluation methods and more
robust LLM-as-a-judge methods, and proposes possible research directions, such
as reducing the dependency on sentence-level information, introducing
multi-level and multi-granular evaluation approaches, and training models
specifically for machine translation evaluation. This study aims to provide a
comprehensive analysis of automatic evaluation for document-level translation
and offer insights into future developments.

</details>


### [43] [On Self-improving Token Embeddings](https://arxiv.org/abs/2504.14808)
*Mario M. Kubek,Shiraj Pokharel,Thomas Böhme,Emma L. McDaniel,Herwig Unger,Armin R. Mikler*

Main category: cs.CL

TL;DR: 提出了一种快速优化预训练静态词嵌入的方法，通过结合邻近标记的嵌入，持续更新每个标记的表示，包括未预分配嵌入的标记，有效解决词汇外问题。


<details>
  <summary>Details</summary>
Motivation: 解决词汇外问题，并提升特定领域语料库中标记表示的质量，使其比通用预训练向量更具意义。

Method: 通过结合邻近标记的嵌入，持续更新每个标记的表示，独立于大型语言模型和浅层神经网络。

Result: 方法在主题同质的语料库中生成更有意义的嵌入，并成功应用于风暴事件分析，改进了风暴相关术语的表示。

Conclusion: 该方法为特定领域语料库提供了高效的嵌入优化方案，适用于多种应用，如语料探索和词义消歧。

Abstract: This article introduces a novel and fast method for refining pre-trained
static word or, more generally, token embeddings. By incorporating the
embeddings of neighboring tokens in text corpora, it continuously updates the
representation of each token, including those without pre-assigned embeddings.
This approach effectively addresses the out-of-vocabulary problem, too.
Operating independently of large language models and shallow neural networks,
it enables versatile applications such as corpus exploration, conceptual
search, and word sense disambiguation. The method is designed to enhance token
representations within topically homogeneous corpora, where the vocabulary is
restricted to a specific domain, resulting in more meaningful embeddings
compared to general-purpose pre-trained vectors. As an example, the methodology
is applied to explore storm events and their impacts on infrastructure and
communities using narratives from a subset of the NOAA Storm Events database.
The article also demonstrates how the approach improves the representation of
storm-related terms over time, providing valuable insights into the evolving
nature of disaster narratives.

</details>


### [44] [Transparentize the Internal and External Knowledge Utilization in LLMs with Trustworthy Citation](https://arxiv.org/abs/2504.14856)
*Jiajun Shen,Tong Zhou,Yubo Chen,Delai Qiu,Shengping Liu,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 论文提出了一种结合外部和内部知识的引用生成任务，并设计了RAEL范式和INTRALIGN方法，通过5个指标评估答案的有用性、引用的忠实性和可信度。实验表明该方法优于基线，并揭示了检索质量、问题类型和模型知识对引用可信度的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管通过检索增强生成和引用生成可以缓解大语言模型的幻觉问题，但其内部知识的利用仍不透明，生成答案的可信度存疑。

Method: 提出了Context-Prior Augmented Citation Generation任务，结合外部和内部知识生成可信引用。设计了RAEL范式和INTRALIGN方法，包含数据生成和对齐算法。

Result: 实验结果显示，该方法在跨场景性能上优于基线，且检索质量、问题类型和模型知识对引用可信度有显著影响。

Conclusion: 通过结合外部和内部知识，该方法提升了引用生成的可信度，为模型生成答案的可靠性提供了新思路。

Abstract: While hallucinations of large language models could been alleviated through
retrieval-augmented generation and citation generation, how the model utilizes
internal knowledge is still opaque, and the trustworthiness of its generated
answers remains questionable. In this work, we introduce Context-Prior
Augmented Citation Generation task, requiring models to generate citations
considering both external and internal knowledge while providing trustworthy
references, with 5 evaluation metrics focusing on 3 aspects: answer
helpfulness, citation faithfulness, and trustworthiness. We introduce RAEL, the
paradigm for our task, and also design INTRALIGN, an integrated method
containing customary data generation and an alignment algorithm. Our
experimental results show that our method achieves a better cross-scenario
performance with regard to other baselines. Our extended experiments further
reveal that retrieval quality, question types, and model knowledge have
considerable influence on the trustworthiness in citation generation.

</details>


### [45] [Natural Fingerprints of Large Language Models](https://arxiv.org/abs/2504.14871)
*Teppei Suzuki,Ryokan Ri,Sho Takase*

Main category: cs.CL

TL;DR: 研究发现，即使大语言模型（LLMs）使用相同训练数据，仍可通过生成文本区分其来源，这些独特特征称为“自然指纹”。训练过程中的细微差异（如参数大小、优化设置和随机种子）会导致这些指纹。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs输出中可识别特征的成因，以理解模型偏差的来源并改进对其行为的控制。

Method: 通过系统控制训练条件（如参数大小、优化设置和随机种子），分析生成文本的差异。

Result: 发现训练过程中的细微差异会导致LLMs生成具有“自然指纹”的文本，即使数据相同。

Conclusion: 理解“自然指纹”有助于揭示模型偏差的根源，并为改进LLM行为控制提供新思路。

Abstract: Large language models (LLMs) often exhibit biases -- systematic deviations
from expected norms -- in their outputs. These range from overt issues, such as
unfair responses, to subtler patterns that can reveal which model produced
them. We investigate the factors that give rise to identifiable characteristics
in LLMs. Since LLMs model training data distribution, it is reasonable that
differences in training data naturally lead to the characteristics. However,
our findings reveal that even when LLMs are trained on the exact same data, it
is still possible to distinguish the source model based on its generated text.
We refer to these unintended, distinctive characteristics as natural
fingerprints. By systematically controlling training conditions, we show that
the natural fingerprints can emerge from subtle differences in the training
process, such as parameter sizes, optimization settings, and even random seeds.
We believe that understanding natural fingerprints offers new insights into the
origins of unintended bias and ways for improving control over LLM behavior.

</details>


### [46] [Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2504.14891)
*Aoran Gan,Hao Yu,Kai Zhang,Qi Liu,Wenyu Yan,Zhenya Huang,Shiwei Tong,Guoping Hu*

Main category: cs.CL

TL;DR: 本文综述了检索增强生成（RAG）系统的评估方法，分析了其挑战，并整理了相关数据集和框架。


<details>
  <summary>Details</summary>
Motivation: RAG系统结合了检索和生成组件，依赖动态知识源，其评估面临独特挑战，需系统梳理现有方法。

Method: 系统回顾传统和新兴的RAG评估方法，包括性能、事实准确性、安全性和计算效率，并进行元分析。

Result: 总结了RAG专用数据集和评估框架，为高影响力研究提供了评估实践的全面分析。

Conclusion: 本文是RAG评估领域最全面的综述，为RAG发展提供了重要资源。

Abstract: Recent advancements in Retrieval-Augmented Generation (RAG) have
revolutionized natural language processing by integrating Large Language Models
(LLMs) with external information retrieval, enabling accurate, up-to-date, and
verifiable text generation across diverse applications. However, evaluating RAG
systems presents unique challenges due to their hybrid architecture that
combines retrieval and generation components, as well as their dependence on
dynamic knowledge sources in the LLM era. In response, this paper provides a
comprehensive survey of RAG evaluation methods and frameworks, systematically
reviewing traditional and emerging evaluation approaches, for system
performance, factual accuracy, safety, and computational efficiency in the LLM
era. We also compile and categorize the RAG-specific datasets and evaluation
frameworks, conducting a meta-analysis of evaluation practices in high-impact
RAG research. To the best of our knowledge, this work represents the most
comprehensive survey for RAG evaluation, bridging traditional and LLM-driven
methods, and serves as a critical resource for advancing RAG development.

</details>


### [47] [CRAVE: A Conflicting Reasoning Approach for Explainable Claim Verification Using LLMs](https://arxiv.org/abs/2504.14905)
*Yingming Zheng,Xiaoliang Liu,Peng Wu,Li Pan*

Main category: cs.CL

TL;DR: CRAVE是一种基于冲突推理的自动声明验证方法，利用大语言模型（LLMs）和小语言模型（SLMs）提高复杂声明的验证准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决传统依赖专家标注证据的验证方法效率低、难以扩展的问题，以及现有自动化系统对复杂声明推理能力不足的挑战。

Method: 提出三模块框架：1）消除歧义的证据检索；2）基于LLMs的冲突立场推理和初步判断；3）基于SLMs的最终真实性判断。

Result: 在两个公开数据集上，CRAVE表现优于现有方法，证据检索和预测解释能力显著提升。

Conclusion: CRAVE通过冲突推理和模块化设计，显著提升了复杂声明验证的准确性和透明度。

Abstract: The rapid spread of misinformation, driven by digital media and AI-generated
content, has made automatic claim verification essential. Traditional methods,
which depend on expert-annotated evidence, are labor-intensive and not
scalable. Although recent automated systems have improved, they still struggle
with complex claims that require nuanced reasoning. To address this, we propose
CRAVE, a Conflicting Reasoning Approach for explainable claim VErification,
that verify the complex claims based on the conflicting rationales reasoned by
large language models (LLMs). Specifically, CRAVE introduces a three-module
framework. Ambiguity Elimination enchanced Evidence Retrieval module performs
ambiguity elimination and entity-based search to gather relevant evidence
related to claim verification from external sources like Wikipedia. Conflicting
Perspective Reasoning and Preliminary Judgment module with LLMs adopts LLMs to
reason rationales with conflicting stances about claim verification from
retrieved evidence across four dimensions, i.e., direct evidence, semantic
relationships, linguistic patterns, and logical reasoning and make a
preliminary judgment. Finally, Small Language Model (SLM) based Judge module is
fine-tuned to make use of preliminary judgment from LLMs to assess the
confidence of the conflicting rationales and make a final authenticity
judgment. This methodology allows CRAVE to capture subtle inconsistencies in
complex claims, improving both the accuracy and transparency of claim
verification. Extensive experiments on two public claim verification datasets
demonstrate that our CRAVE model achieves much better performance than
state-of-the-art methods and exhibits a superior capacity for finding relevant
evidence and explaining the model predictions. The code is provided at
https://github.com/8zym/CRAVE.

</details>


### [48] [Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues](https://arxiv.org/abs/2504.14963)
*Rui Ribeiro,Luísa Coheur,Joao P. Carvalho*

Main category: cs.CL

TL;DR: 该论文提出了一种基于模糊指纹和上下文感知模型的方法，用于仅从文本数据中识别说话者，显著提高了准确性，并提供了改进的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在仅依赖文本数据时难以有效识别说话者，因此需要探索更先进的模型和技术来解决这一问题。

Method: 使用预训练模型的模糊指纹，结合说话者特定标记和上下文感知建模，以提高识别准确性。

Result: 在Friends数据集上达到70.6%的准确率，在Big Bang Theory数据集上达到67.7%，并展示了模糊指纹在减少隐藏单元时的性能接近完整微调。

Conclusion: 研究揭示了文本说话者识别的关键挑战，并为未来改进提供了见解。

Abstract: Speaker identification using voice recordings leverages unique acoustic
features, but this approach fails when only textual data is available. Few
approaches have attempted to tackle the problem of identifying speakers solely
from text, and the existing ones have primarily relied on traditional methods.
In this work, we explore the use of fuzzy fingerprints from large pre-trained
models to improve text-based speaker identification. We integrate
speaker-specific tokens and context-aware modeling, demonstrating that
conversational context significantly boosts accuracy, reaching 70.6% on the
Friends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show
that fuzzy fingerprints can approximate full fine-tuning performance with fewer
hidden units, offering improved interpretability. Finally, we analyze ambiguous
utterances and propose a mechanism to detect speaker-agnostic lines. Our
findings highlight key challenges and provide insights for future improvements
in text-based speaker identification.

</details>


### [49] [Evaluating LLMs on Chinese Topic Constructions: A Research Proposal Inspired by Tian et al. (2024)](https://arxiv.org/abs/2504.14969)
*Xiaodong Yang*

Main category: cs.CL

TL;DR: 提出一个评估大语言模型（LLMs）对中文话题结构敏感性的框架，重点关注其对孤岛限制的敏感性。


<details>
  <summary>Details</summary>
Motivation: 受Tian等人（2024）启发，旨在填补LLMs在汉语语法知识评估方面的空白，为未来研究奠定基础。

Method: 设计实验方案测试LLMs对普通话语法的理解，尚未进行实验。

Result: 暂无实验结果，仅提出方法论框架。

Conclusion: 该提案为未来研究提供基础，并邀请对方法论的反馈。

Abstract: This paper proposes a framework for evaluating large language models (LLMs)
on Chinese topic constructions, focusing on their sensitivity to island
constraints. Drawing inspiration from Tian et al. (2024), we outline an
experimental design for testing LLMs' grammatical knowledge of Mandarin syntax.
While no experiments have been conducted yet, this proposal aims to provide a
foundation for future studies and invites feedback on the methodology.

</details>


### [50] [Efficient Pretraining Length Scaling](https://arxiv.org/abs/2504.14992)
*Bohong Wu,Shen Yan,Sijun Zhang,Jianqiao Lu,Yutao Zeng,Ya Wang,Xun Zhou*

Main category: cs.CL

TL;DR: 论文提出了一种名为PHD-Transformer的新框架，通过在预训练阶段实现高效的长度扩展，同时保持推理效率。通过创新的KV缓存管理策略，区分原始令牌和隐藏解码令牌，优化了性能。


<details>
  <summary>Details</summary>
Motivation: 探索预训练阶段长度扩展的潜力，解决现有方法在预训练中未充分研究的问题。

Method: 提出PHD-Transformer框架，采用KV缓存管理策略，区分原始令牌和隐藏解码令牌，并引入两种优化变体（PHD-SWA和PHD-CSWA）以提升性能。

Result: 实验证明，该方法在多个基准测试中均取得了一致的改进。

Conclusion: PHD-Transformer在预训练中实现了高效的长度扩展，同时保持了推理效率，为相关研究提供了新的思路。

Abstract: Recent advances in large language models have demonstrated the effectiveness
of length scaling during post-training, yet its potential in pre-training
remains underexplored. We present the Parallel Hidden Decoding Transformer
(\textit{PHD}-Transformer), a novel framework that enables efficient length
scaling during pre-training while maintaining inference efficiency.
\textit{PHD}-Transformer achieves this through an innovative KV cache
management strategy that distinguishes between original tokens and hidden
decoding tokens. By retaining only the KV cache of original tokens for
long-range dependencies while immediately discarding hidden decoding tokens
after use, our approach maintains the same KV cache size as the vanilla
transformer while enabling effective length scaling. To further enhance
performance, we introduce two optimized variants: \textit{PHD-SWA} employs
sliding window attention to preserve local dependencies, while
\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate
linear growth in pre-filling time. Extensive experiments demonstrate consistent
improvements across multiple benchmarks.

</details>


### [51] [Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation with LLMs](https://arxiv.org/abs/2504.15013)
*Yow-Fu Liou,Yu-Chien Tang,An-Zi Yen*

Main category: cs.CL

TL;DR: 研究探讨了利用大语言模型（LLMs）自动化生成教育材料和课程建议的潜力，通过实验证明其能高效生成高质量内容和准确推荐。


<details>
  <summary>Details</summary>
Motivation: 教育材料制作耗时费力，研究旨在利用LLMs简化这一过程，提升学习体验。

Method: 从视频转录生成扩展文章，结合历史、文化等内容，并通过语义相似度推荐相关课程，最后用LLM优化推荐。

Result: 实验评估显示模型能生成高质量内容并提供准确课程建议，指标包括命中率、语义相似度和连贯性。

Conclusion: LLMs能有效连接核心内容与补充学习资源，既辅助教师设计材料，又为学生提供更多学习资源。

Abstract: The process of creating educational materials is both time-consuming and
demanding for educators. This research explores the potential of Large Language
Models (LLMs) to streamline this task by automating the generation of extended
reading materials and relevant course suggestions. Using the TED-Ed Dig Deeper
sections as an initial exploration, we investigate how supplementary articles
can be enriched with contextual knowledge and connected to additional learning
resources. Our method begins by generating extended articles from video
transcripts, leveraging LLMs to include historical insights, cultural examples,
and illustrative anecdotes. A recommendation system employing semantic
similarity ranking identifies related courses, followed by an LLM-based
refinement process to enhance relevance. The final articles are tailored to
seamlessly integrate these recommendations, ensuring they remain cohesive and
informative. Experimental evaluations demonstrate that our model produces
high-quality content and accurate course suggestions, assessed through metrics
such as Hit Rate, semantic similarity, and coherence. Our experimental analysis
highlight the nuanced differences between the generated and existing materials,
underscoring the model's capacity to offer more engaging and accessible
learning experiences. This study showcases how LLMs can bridge the gap between
core content and supplementary learning, providing students with additional
recommended resources while also assisting teachers in designing educational
materials.

</details>


### [52] [LLMs as Data Annotators: How Close Are We to Human Performance](https://arxiv.org/abs/2504.15022)
*Muhammad Uzair Ul Haq,Davide Rigoni,Alessandro Sperduti*

Main category: cs.CL

TL;DR: 论文探讨了在NLP任务中，如何通过自动检索上下文示例改进LLMs的微调效果，替代传统的手动标注和选择示例的方法。


<details>
  <summary>Details</summary>
Motivation: 手动标注数据成本高且效率低，而手动选择上下文示例可能导致模型性能不佳，因此需要更高效的方法。

Method: 通过比较不同LLMs和嵌入模型，结合检索增强生成（RAG）方法，自动检索上下文示例以优化性能。

Result: 实验表明，选择合适的LLM和嵌入模型至关重要，同时需权衡模型大小与性能。

Conclusion: 研究强调了自动检索方法的优势，并建议未来关注更具挑战性的数据集。

Abstract: In NLP, fine-tuning LLMs is effective for various applications but requires
high-quality annotated data. However, manual annotation of data is
labor-intensive, time-consuming, and costly. Therefore, LLMs are increasingly
used to automate the process, often employing in-context learning (ICL) in
which some examples related to the task are given in the prompt for better
performance. However, manually selecting context examples can lead to
inefficiencies and suboptimal model performance. This paper presents
comprehensive experiments comparing several LLMs, considering different
embedding models, across various datasets for the Named Entity Recognition
(NER) task. The evaluation encompasses models with approximately $7$B and $70$B
parameters, including both proprietary and non-proprietary models. Furthermore,
leveraging the success of Retrieval-Augmented Generation (RAG), it also
considers a method that addresses the limitations of ICL by automatically
retrieving contextual examples, thereby enhancing performance. The results
highlight the importance of selecting the appropriate LLM and embedding model,
understanding the trade-offs between LLM sizes and desired performance, and the
necessity to direct research efforts towards more challenging datasets.

</details>


### [53] [DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models](https://arxiv.org/abs/2504.15027)
*Chengyu Wang,Junbing Yan,Yuanhao Yue,Jun Huang*

Main category: cs.CL

TL;DR: DistilQwen2.5是一系列轻量级大语言模型，通过蒸馏技术从Qwen2.5模型中提取，提升了指令跟随能力，并降低了部署成本。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的场景下，提高计算效率和降低大语言模型的部署成本是关键挑战。

Method: 利用多代理教师模型选择和改写指令-响应对，并通过模型融合技术逐步整合教师模型的细粒度知识。

Result: 蒸馏后的模型在能力上显著优于原始模型。

Conclusion: DistilQwen2.5模型已开源，适用于实际应用场景。

Abstract: Enhancing computational efficiency and reducing deployment costs for large
language models (LLMs) have become critical challenges in various
resource-constrained scenarios. In this work, we present DistilQwen2.5, a
family of distilled, lightweight LLMs derived from the public Qwen2.5 models.
These distilled models exhibit enhanced instruction-following capabilities
compared to the original models based on a series of distillation techniques
that incorporate knowledge from much larger LLMs. In our industrial practice,
we first leverage powerful proprietary LLMs with varying capacities as
multi-agent teachers to select, rewrite, and refine instruction-response pairs
that are more suitable for student LLMs to learn. After standard fine-tuning,
we further leverage a computationally efficient model fusion approach that
enables student models to progressively integrate fine-grained hidden knowledge
from their teachers. Experimental evaluations demonstrate that the distilled
models possess significantly stronger capabilities than their original
checkpoints. Additionally, we present use cases to illustrate the applications
of our framework in real-world scenarios. To facilitate practical use, we have
released all the DistilQwen2.5 models to the open-source community.

</details>


### [54] [RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search](https://arxiv.org/abs/2504.15047)
*Quy-Anh Dang,Chris Ngo,Truong-Son Hy*

Main category: cs.CL

TL;DR: RainbowPlus是一种基于进化计算的新型红队框架，通过自适应质量-多样性搜索增强对抗性提示生成，显著提高了攻击成功率和多样性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）易受对抗性提示攻击，现有红队方法在可扩展性、资源需求和攻击策略多样性方面存在局限。

Method: RainbowPlus采用多元素存档存储多样化高质量提示，并利用综合适应度函数评估多个提示，克服了传统方法的限制。

Result: 在多个基准数据集和LLMs上，RainbowPlus的攻击成功率和多样性显著优于现有方法，且效率更高。

Conclusion: RainbowPlus为LLM安全性评估提供了可扩展工具，开源实现支持进一步研究和复现。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but are
susceptible to adversarial prompts that exploit vulnerabilities to produce
unsafe or biased outputs. Existing red-teaming methods often face scalability
challenges, resource-intensive requirements, or limited diversity in attack
strategies. We propose RainbowPlus, a novel red-teaming framework rooted in
evolutionary computation, enhancing adversarial prompt generation through an
adaptive quality-diversity (QD) search that extends classical evolutionary
algorithms like MAP-Elites with innovations tailored for language models. By
employing a multi-element archive to store diverse high-quality prompts and a
comprehensive fitness function to evaluate multiple prompts concurrently,
RainbowPlus overcomes the constraints of single-prompt archives and pairwise
comparisons in prior QD methods like Rainbow Teaming. Experiments comparing
RainbowPlus to QD methods across six benchmark datasets and four open-source
LLMs demonstrate superior attack success rate (ASR) and diversity
(Diverse-Score $\approx 0.84$), generating up to 100 times more unique prompts
(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine
state-of-the-art methods on the HarmBench dataset with twelve LLMs (ten
open-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,
surpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).
Our open-source implementation fosters further advancements in LLM safety,
offering a scalable tool for vulnerability assessment. Code and resources are
publicly available at https://github.com/knoveleng/rainbowplus, supporting
reproducibility and future research in LLM red-teaming.

</details>


### [55] [Testing LLMs' Capabilities in Annotating Translations Based on an Error Typology Designed for LSP Translation: First Experiments with ChatGPT](https://arxiv.org/abs/2504.15052)
*Joachim Minder,Guillaume Wisniewski,Natalie Kübler*

Main category: cs.CL

TL;DR: 研究探讨了ChatGPT在基于错误类型学标注机器翻译输出的能力，发现其在专业翻译中表现良好，但自我评估能力有限。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（如ChatGPT）在专业翻译错误标注中的潜力，填补以往研究主要关注通用语言的空白。

Method: 通过两种不同提示和自定义错误类型学，比较ChatGPT与人类专家对DeepL和ChatGPT自身翻译的标注结果。

Result: ChatGPT对DeepL翻译的标注召回率和精确度较高，但自我评估表现较差；提示的详细程度影响分类准确性。

Conclusion: 大型语言模型在翻译评估中具有潜力，但需进一步研究开源模型及其实践应用，如翻译教学中的自动化评估优化。

Abstract: This study investigates the capabilities of large language models (LLMs),
specifically ChatGPT, in annotating MT outputs based on an error typology. In
contrast to previous work focusing mainly on general language, we explore
ChatGPT's ability to identify and categorise errors in specialised
translations. By testing two different prompts and based on a customised error
typology, we compare ChatGPT annotations with human expert evaluations of
translations produced by DeepL and ChatGPT itself. The results show that, for
translations generated by DeepL, recall and precision are quite high. However,
the degree of accuracy in error categorisation depends on the prompt's specific
features and its level of detail, ChatGPT performing very well with a detailed
prompt. When evaluating its own translations, ChatGPT achieves significantly
poorer results, revealing limitations with self-assessment. These results
highlight both the potential and the limitations of LLMs for translation
evaluation, particularly in specialised domains. Our experiments pave the way
for future research on open-source LLMs, which could produce annotations of
comparable or even higher quality. In the future, we also aim to test the
practical effectiveness of this automated evaluation in the context of
translation training, particularly by optimising the process of human
evaluation by teachers and by exploring the impact of annotations by LLMs on
students' post-editing and translation learning.

</details>


### [56] [Rethinking the Potential of Multimodality in Collaborative Problem Solving Diagnosis with Large Language Models](https://arxiv.org/abs/2504.15093)
*K. Wong,B. Wu,S. Bulathwela,M. Cukurova*

Main category: cs.CL

TL;DR: 研究探讨了多模态数据在诊断学生协作问题解决（CPS）能力中的潜力，发现其在特定CPS类别中表现更优，但效果受标签复杂性和数据集组成影响。


<details>
  <summary>Details</summary>
Motivation: 探索多模态数据和先进模型在检测复杂CPS行为中的实际价值，填补实证研究的空白。

Method: 使用文本嵌入和声学嵌入构建多模态分类模型，对比传统模型和基于Transformer的模型性能。

Result: 多模态数据在Transformer模型中提升了社交认知类CPS的诊断性能，但对传统模型无显著改善。

Conclusion: 多模态和模型选择需根据具体CPS指标定制，强调人机互补和未来模型架构的探索。

Abstract: Detecting collaborative and problem-solving behaviours from digital traces to
interpret students' collaborative problem solving (CPS) competency is a
long-term goal in the Artificial Intelligence in Education (AIEd) field.
Although multimodal data and advanced models are argued to have the potential
to detect complex CPS behaviours, empirical evidence on their value remains
limited with some contrasting evidence. In this study, we investigated the
potential of multimodal data to improve model performance in diagnosing 78
secondary school students' CPS subskills and indicators in authentic
educational settings. In particular, text embeddings from verbal data and
acoustic embeddings from audio data were used in a multimodal classification
model for CPS diagnosis. Both unimodal and multimodal transformer-based models
outperformed traditional models in detecting CPS classes. Although the
inclusion of multimodality did not improve the performance of traditional
unimodal models, its integration into transformer-based models demonstrated
improved performance for diagnosing social-cognitive CPS classes compared to
unimodal transformer-based models. Based on the results, the paper argues that
multimodality and the selection of a particular modelling technique should not
be taken for granted to achieve the best performance in the automated detection
of every CPS subskill and indicator. Rather, their value is limited to certain
types of CPS indicators, affected by the complexity of the labels, and
dependent on the composition of indicators in the dataset. We conclude the
paper by discussing the required nuance when considering the value of LLMs and
multimodality in automated CPS diagnosis, highlighting the need for human-AI
complementarity, and proposing the exploration of relevant model architectures
and techniques to improve CPS diagnosis in authentic educational contexts.

</details>


### [57] [Kuwain 1.5B: An Arabic SLM via Language Injection](https://arxiv.org/abs/2504.15120)
*Khalil Hennara,Sara Chrouf,Mohamed Motaism Hamed,Zeina Aldallal,Omar Hadid,Safwan AlModhayan*

Main category: cs.CL

TL;DR: 论文提出了一种将新语言整合到大型语言模型中的方法，成功将阿拉伯语注入现有模型，性能提升8%，同时保留原有知识。


<details>
  <summary>Details</summary>
Motivation: 增强现有模型以融入新知识是AI发展的关键，尤其是在多语言支持方面。

Method: 通过向一个主要基于英语训练的小型开源模型注入阿拉伯语，训练了一个15亿参数的微型模型Kuwain。

Result: 阿拉伯语性能平均提升8%，同时保留了原有知识，且无需大量原始数据。

Conclusion: 该方法为高效、低成本地扩展语言模型提供了潜力，避免了资源密集型训练。

Abstract: Enhancing existing models with new knowledge is a crucial aspect of AI
development. This paper introduces a novel method for integrating a new
language into a large language model (LLM). Our approach successfully
incorporates a previously unseen target language into an existing LLM without
compromising its prior knowledge. We trained a tiny model with 1.5 billion
parameters named Kuwain by injecting the Arabic language into a small
open-source model mainly trained in English. Our method demonstrates
significant improvements in Arabic language performance, with an average 8%
improvement across various benchmarks, while retaining the model's existing
knowledge with a minimum amount of the original model's data. This offers a
cost-effective alternative to training a comprehensive model in both English
and Arabic. The results highlight the potential for efficient, targeted
language model expansion without extensive retraining or resource-intensive
processes.

</details>


### [58] [EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models](https://arxiv.org/abs/2504.15133)
*Ziwen Xu,Shuxun Wang,Kewei Xu,Haoming Xu,Mengru Wang,Xinle Deng,Yunzhi Yao,Guozhou Zheng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: EasyEdit2是一个框架，旨在通过插件式调整控制大型语言模型（LLM）的行为，支持多种测试时干预，如安全性、情感、个性等。其新架构包括转向向量生成器和应用器，无需修改模型参数即可调整行为。


<details>
  <summary>Details</summary>
Motivation: 为LLM提供无需技术背景的简单、高效的行为控制方法。

Method: 采用转向向量生成器和应用器的新架构，通过示例自动生成和应用转向向量。

Result: 在不同LLM上验证了模型转向的有效性，展示了技术的实用性。

Conclusion: EasyEdit2提供了一种易用且高效的方法，精确控制LLM行为，适合广泛用户群体。

Abstract: In this paper, we introduce EasyEdit2, a framework designed to enable
plug-and-play adjustability for controlling Large Language Model (LLM)
behaviors. EasyEdit2 supports a wide range of test-time interventions,
including safety, sentiment, personality, reasoning patterns, factuality, and
language features. Unlike its predecessor, EasyEdit2 features a new
architecture specifically designed for seamless model steering. It comprises
key modules such as the steering vector generator and the steering vector
applier, which enable automatic generation and application of steering vectors
to influence the model's behavior without modifying its parameters. One of the
main advantages of EasyEdit2 is its ease of use-users do not need extensive
technical knowledge. With just a single example, they can effectively guide and
adjust the model's responses, making precise control both accessible and
efficient. Empirically, we report model steering performance across different
LLMs, demonstrating the effectiveness of these techniques. We have released the
source code on GitHub at https://github.com/zjunlp/EasyEdit along with a
demonstration notebook. In addition, we provide a demo video at
https://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.

</details>


### [59] [The Synthetic Imputation Approach: Generating Optimal Synthetic Texts For Underrepresented Categories In Supervised Classification Tasks](https://arxiv.org/abs/2504.15160)
*Joan C. Timoneda*

Main category: cs.CL

TL;DR: 论文提出了一种合成插补方法，利用生成式LLM（如GPT-4o）生成合成文本，以解决训练数据中类别不平衡的问题。该方法在75个原始样本时表现与完整样本相当，且过拟合可控。


<details>
  <summary>Details</summary>
Motivation: 解决训练数据中某些类别样本不足的问题，以优化编码器-解码器LLM（如BERT和RoBERTa）的性能。

Method: 使用生成式LLM（GPT-4o）基于少量原始样本生成合成文本，确保新文本与原始文本有足够差异以减少过拟合，同时保留实质意义。

Result: 在75个或更多原始样本时，合成插补方法的性能与完整样本相当；50个原始样本时过拟合可控且可校正。

Conclusion: 合成插补方法为生成式LLM在研究中提供了新用途，帮助应用研究者平衡数据集以获得最佳性能。

Abstract: Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa,
require that all categories in an annotation task be sufficiently represented
in the training data for optimal performance. However, it is often difficult to
find sufficient examples for all categories in a task when building a
high-quality training set. In this article, I describe this problem and propose
a solution, the synthetic imputation approach. Leveraging a generative LLM
(GPT-4o), this approach generates synthetic texts based on careful prompting
and five original examples drawn randomly with replacement from the sample.
This approach ensures that new synthetic texts are sufficiently different from
the original texts to reduce overfitting, but retain the underlying substantive
meaning of the examples to maximize out-of-sample performance. With 75 original
examples or more, synthetic imputation's performance is on par with a full
sample of original texts, and overfitting remains low, predictable and
correctable with 50 original samples. The synthetic imputation approach
provides a novel role for generative LLMs in research and allows applied
researchers to balance their datasets for best performance.

</details>


### [60] [On true empty category](https://arxiv.org/abs/2504.15168)
*Qilin Tian*

Main category: cs.CL

TL;DR: 本文探讨了空语类假设，评估了Li等人提出的真实空语类假设，并通过话题化现象证明无需引入真实空语类即可解释相关现象。


<details>
  <summary>Details</summary>
Motivation: 现有空语类分类（如PRO、pro、痕迹、变量）无法解释某些空宾语位置，Li等人提出真实空语类假设作为补充。本文旨在验证该假设的必要性。

Method: 通过分析话题化现象，检验真实空语类假设的解释力。

Result: 研究发现，话题化现象无需依赖真实空语类假设即可解释。

Conclusion: 真实空语类假设并非必要，现有理论框架足以解释相关语言现象。

Abstract: According to Chomsky (1981, 1986), empty categories consist of PRO, pro,
trace, and variable. However, some empty object positions seem to be
incompatible with extant empty categories. Given this, Li (2007a, 2007b, 2014)
and Li & Wei (2014) raise the true empty category hypothesis, which holds that
true empty category is only an empty position with category and Case features.
As a last resort option, it is used mainly to meet the subcatgorization of a
verb. This assumption is ingenious, and if proved to be true, it will exert a
great impact on the study of UG. In this paper, we evaluate their evidence from
topicalization and demonstrate that it can be accounted for without invoking
true empty category.

</details>


### [61] [Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges](https://arxiv.org/abs/2504.15205)
*Nandan Thakur,Ronak Pradeep,Shivani Upadhyay,Daniel Campos,Nick Craswell,Jimmy Lin*

Main category: cs.CL

TL;DR: RAG通过引用真实文档减少LLM幻觉，研究比较了GPT-4o与人类评委在支持评估中的表现，发现GPT-4o可作为可靠替代。


<details>
  <summary>Details</summary>
Motivation: 评估RAG中引用的文档是否支持生成答案的准确性，以减少系统幻觉。

Method: 对45份提交的TREC 2024 RAG Track数据进行大规模比较，对比GPT-4o与人类评委在两种条件下的评估结果。

Result: 56%的完全手动评估中人类与GPT-4o一致，72%的后编辑条件下一致；GPT-4o与独立人类评委相关性更高。

Conclusion: GPT-4o在支持评估中表现可靠，未来可优化评估方法以减少错误。

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to
generate answers with citations from source documents containing "ground
truth", thereby reducing system hallucinations. A crucial factor in RAG
evaluation is "support", whether the information in the cited documents
supports the answer. To this end, we conducted a large-scale comparative study
of 45 participant submissions on 36 topics to the TREC 2024 RAG Track,
comparing an automatic LLM judge (GPT-4o) against human judges for support
assessment. We considered two conditions: (1) fully manual assessments from
scratch and (2) manual assessments with post-editing of LLM predictions. Our
results indicate that for 56% of the manual from-scratch assessments, human and
GPT-4o predictions match perfectly (on a three-level scale), increasing to 72%
in the manual with post-editing condition. Furthermore, by carefully analyzing
the disagreements in an unbiased study, we found that an independent human
judge correlates better with GPT-4o than a human judge, suggesting that LLM
judges can be a reliable alternative for support assessment. To conclude, we
provide a qualitative analysis of human and GPT-4o errors to help guide future
iterations of support assessment.

</details>


### [62] [EvalAgent: Discovering Implicit Evaluation Criteria from the Web](https://arxiv.org/abs/2504.15219)
*Manya Wadhwa,Zayne Sprague,Chaitanya Malaviya,Philippe Laban,Junyi Jessy Li,Greg Durrett*

Main category: cs.CL

TL;DR: EvalAgent是一个新框架，用于自动发现语言模型输出中的隐式、任务特定评估标准，通过挖掘专家指导并生成具体、可操作的改进建议。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型输出的评估通常依赖显式标准，而忽略了隐式的高质量特征（如学术演讲的吸引力）。EvalAgent旨在填补这一空白。

Method: EvalAgent通过挖掘专家在线指导，生成基于可靠来源的多样化、长尾评估标准，并结合LLM生成的标准进行优化。

Result: 实验表明，EvalAgent生成的标准具有隐式性、具体性和可操作性，且与人类价值观更一致。

Conclusion: EvalAgent能够有效补充LLM的评估能力，提升语言模型输出的质量。

Abstract: Evaluation of language model outputs on structured writing tasks is typically
conducted with a number of desirable criteria presented to human evaluators or
large language models (LLMs). For instance, on a prompt like "Help me draft an
academic talk on coffee intake vs research productivity", a model response may
be evaluated for criteria like accuracy and coherence. However, high-quality
responses should do more than just satisfy basic task requirements. An
effective response to this query should include quintessential features of an
academic talk, such as a compelling opening, clear research questions, and a
takeaway. To help identify these implicit criteria, we introduce EvalAgent, a
novel framework designed to automatically uncover nuanced and task-specific
criteria. EvalAgent first mines expert-authored online guidance. It then uses
this evidence to propose diverse, long-tail evaluation criteria that are
grounded in reliable external sources. Our experiments demonstrate that the
grounded criteria produced by EvalAgent are often implicit (not directly stated
in the user's prompt), yet specific (high degree of lexical precision).
Further, EvalAgent criteria are often not satisfied by initial responses but
they are actionable, such that responses can be refined to satisfy them.
Finally, we show that combining LLM-generated and EvalAgent criteria uncovers
more human-valued criteria than using LLMs alone.

</details>


### [63] [Fully Bayesian Approaches to Topics over Time](https://arxiv.org/abs/2504.15220)
*Julián Cendrero,Julio Gonzalo,Ivar Zapata*

Main category: cs.CL

TL;DR: 论文提出了完全贝叶斯的Topics over Time（BToT）模型，解决了原ToT模型的稳定性问题，并进一步提出了加权版本WBToT以平衡时间和词模态的影响。


<details>
  <summary>Details</summary>
Motivation: 原ToT模型未采用完全贝叶斯方法，导致稳定性问题，且时间和词模态的尺度差异未被解决。

Method: 引入Beta分布的共轭先验作为正则化，提出BToT；进一步提出WBToT，通过重复文档的发布时间平衡模态影响。

Result: WBToT在事件捕捉和主题一致性上优于LDA和BERTopic，时间稳定性提升显著。

Conclusion: WBToT解决了ToT的稳定性问题，平衡了时间和词模态，适用于大规模数据。

Abstract: The Topics over Time (ToT) model captures thematic changes in timestamped
datasets by explicitly modeling publication dates jointly with word
co-occurrence patterns. However, ToT was not approached in a fully Bayesian
fashion, a flaw that makes it susceptible to stability problems. To address
this issue, we propose a fully Bayesian Topics over Time (BToT) model via the
introduction of a conjugate prior to the Beta distribution. This prior acts as
a regularization that prevents the online version of the algorithm from
unstable updates when a topic is poorly represented in a mini-batch. The
characteristics of this prior to the Beta distribution are studied here for the
first time. Still, this model suffers from a difference in scale between the
single-time observations and the multiplicity of words per document. A
variation of BToT, Weighted Bayesian Topics over Time (WBToT), is proposed as a
solution. In WBToT, publication dates are repeated a certain number of times
per document, which balances the relative influence of words and timestamps
along the inference process. We have tested our models on two datasets: a
collection of over 200 years of US state-of-the-union (SOTU) addresses and a
large-scale COVID-19 Twitter corpus of 10 million tweets. The results show that
WBToT captures events better than Latent Dirichlet Allocation and other SOTA
topic models like BERTopic: the median absolute deviation of the topic presence
over time is reduced by $51\%$ and $34\%$, respectively. Our experiments also
demonstrate the superior coherence of WBToT over BToT, which highlights the
importance of balancing the time and word modalities. Finally, we illustrate
the stability of the online optimization algorithm in WBToT, which allows the
application of WBToT to problems that are intractable for standard ToT.

</details>


### [64] [Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions](https://arxiv.org/abs/2504.15236)
*Saffron Huang,Esin Durmus,Miles McCain,Kunal Handa,Alex Tamkin,Jerry Hong,Michael Stern,Arushi Somani,Xiuruo Zhang,Deep Ganguli*

Main category: cs.CL

TL;DR: 研究开发了一种隐私保护方法，从Claude 3和3.5模型的真实交互中提取了3,307种AI价值观，发现其支持亲社会价值观且具有上下文依赖性。


<details>
  <summary>Details</summary>
Motivation: AI助手可能通过价值判断影响用户决策和世界观，但缺乏对其实际价值观的实证研究。

Method: 采用自下而上的隐私保护方法，从大量真实交互中提取并分类AI的价值观。

Result: Claude模型表现出多样化的价值观，支持亲社会价值观，且价值观随上下文变化。

Conclusion: 研究为AI价值观的评估和设计提供了实证基础。

Abstract: AI assistants can impart value judgments that shape people's decisions and
worldviews, yet little is known empirically about what values these systems
rely on in practice. To address this, we develop a bottom-up,
privacy-preserving method to extract the values (normative considerations
stated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit
in hundreds of thousands of real-world interactions. We empirically discover
and taxonomize 3,307 AI values and study how they vary by context. We find that
Claude expresses many practical and epistemic values, and typically supports
prosocial human values while resisting values like "moral nihilism". While some
values appear consistently across contexts (e.g. "transparency"), many are more
specialized and context-dependent, reflecting the diversity of human
interlocutors and their varied contexts. For example, "harm prevention" emerges
when Claude resists users, "historical accuracy" when responding to queries
about controversial events, "healthy boundaries" when asked for relationship
advice, and "human agency" in technology ethics discussions. By providing the
first large-scale empirical mapping of AI values in deployment, our work
creates a foundation for more grounded evaluation and design of values in AI
systems.

</details>


### [65] [MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning](https://arxiv.org/abs/2504.15241)
*Yahan Yang,Soham Dan,Shuo Li,Dan Roth,Insup Lee*

Main category: cs.CL

TL;DR: 提出了一种多语言防护栏方法，通过合成数据生成、监督微调和GRPO框架，有效检测和过滤多语言中的不安全内容。


<details>
  <summary>Details</summary>
Motivation: 多语言环境下，大语言模型易受对抗攻击（如越狱），而多语言安全对齐数据有限，需开发能跨语言检测不安全内容的防护栏。

Method: 1. 合成多语言数据（含文化和语言细微差异）；2. 监督微调；3. 课程引导的GRPO框架优化性能。

Result: 实验表明，该方法在领域内和领域外语言中均优于基线，并能生成多语言解释以理解语言特定风险。

Conclusion: 该方法为多语言内容审核提供了高效防护，并增强了语言特定风险的理解能力。

Abstract: Large Language Models (LLMs) are susceptible to adversarial attacks such as
jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability
is exacerbated in multilingual setting, where multilingual safety-aligned data
are often limited. Thus, developing a guardrail capable of detecting and
filtering unsafe content across diverse languages is critical for deploying
LLMs in real-world applications. In this work, we propose an approach to build
a multilingual guardrail with reasoning. Our method consists of: (1) synthetic
multilingual data generation incorporating culturally and linguistically
nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group
Relative Policy Optimization (GRPO) framework that further improves
performance. Experimental results demonstrate that our multilingual guardrail
consistently outperforms recent baselines across both in-domain and
out-of-domain languages. The multilingual reasoning capability of our guardrail
enables it to generate multilingual explanations, which are particularly useful
for understanding language-specific risks and ambiguities in multilingual
content moderation.

</details>


### [66] [Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators](https://arxiv.org/abs/2504.15253)
*Yilun Zhou,Austin Xu,Peifeng Wang,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 论文研究了LLM-judges在测试时计算扩展中的表现，发现其在重排序任务中表现良好，但在束搜索和基于批判的响应优化中效果较差。


<details>
  <summary>Details</summary>
Motivation: 探索LLM-judges作为评估器在测试时计算扩展中的有效性，填补现有研究的空白。

Method: 引入JETTS基准，评估LLM-judges在数学推理、代码生成和指令遵循三个领域的表现，包括重排序、束搜索和批判优化三种任务。

Result: LLM-judges在重排序中与结果奖励模型相当，但在束搜索中不如过程奖励模型，且批判优化效果不佳。

Conclusion: LLM-judges在部分任务中表现良好，但在复杂任务中仍需改进，批判优化功能尚未发挥潜力。

Abstract: Scaling test-time computation, or affording a generator large language model
(LLM) extra compute during inference, typically employs the help of external
non-generative evaluators (i.e., reward models). Concurrently, LLM-judges,
models trained to generate evaluations and critiques (explanations) in natural
language, are becoming increasingly popular in automatic evaluation. Despite
judge empirical successes, their effectiveness as evaluators in test-time
scaling settings is largely unknown. In this paper, we introduce the Judge
Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge
performance in three domains (math reasoning, code generation, and instruction
following) under three task settings: response reranking, step-level beam
search, and critique-based response refinement. We evaluate 10 different judge
models (7B-70B parameters) for 8 different base generator models (6.7B-72B
parameters). Our benchmark shows that while judges are competitive with outcome
reward models in reranking, they are consistently worse than process reward
models in beam search procedures. Furthermore, though unique to LLM-judges,
their natural language critiques are currently ineffective in guiding the
generator towards better responses.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [67] [Memory-efficient Streaming VideoLLMs for Real-time Procedural Video Understanding](https://arxiv.org/abs/2504.13915)
*Dibyadip Chatterjee,Edoardo Remelli,Yale Song,Bugra Tekin,Abhay Mittal,Bharat Bhatnagar,Necati Cihan Camgöz,Shreyas Hampali,Eric Sauser,Shugao Ma,Angela Yao,Fadime Sener*

Main category: cs.CV

TL;DR: ProVideLLM是一个实时程序视频理解的端到端框架，通过多模态缓存和令牌设计显著降低了计算和内存需求，并在多个任务上达到最新性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长时观察时计算和内存需求高，且难以同时捕捉细粒度的细节。

Method: 结合文本和视觉令牌的多模态缓存设计，使用DETR-QFormer编码视觉令牌，减少令牌数量并保持细节。

Result: 计算和内存需求随视频长度次线性增长，支持10 FPS的逐帧推理和25 FPS的对话，在六个任务上达到最优。

Conclusion: ProVideLLM通过高效的多模态缓存设计，实现了实时视频理解和对话，性能优越。

Abstract: We introduce ProVideLLM, an end-to-end framework for real-time procedural
video understanding. ProVideLLM integrates a multimodal cache configured to
store two types of tokens - verbalized text tokens, which provide compressed
textual summaries of long-term observations, and visual tokens, encoded with
DETR-QFormer to capture fine-grained details from short-term observations. This
design reduces token count by 22x over existing methods in representing one
hour of long-term observations while effectively encoding fine-granularity of
the present. By interleaving these tokens in our multimodal cache, ProVideLLM
ensures sub-linear scaling of memory and compute with video length, enabling
per-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with
a minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art
results on six procedural tasks across four datasets.

</details>


### [68] [Entropy Rectifying Guidance for Diffusion and Flow Models](https://arxiv.org/abs/2504.13987)
*Tariq Berrada Ifriqi,Adriana Romero-Soriano,Michal Drozdzal,Jakob Verbeek,Karteek Alahari*

Main category: cs.CV

TL;DR: 论文提出了一种名为熵修正引导（ERG）的新方法，通过调整扩散变换器架构中的注意力机制，显著提升了图像生成的质量、多样性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的分类器自由引导（CFG）方法在图像生成中存在质量、多样性和一致性之间的权衡，且现有改进方法需要额外模型或计算开销。

Method: 提出ERG，基于扩散变换器架构中注意力机制在推理时的变化，无需额外模型或额外前向传播步骤。

Result: ERG在文本到图像、类条件生成和无条件生成等任务中表现优异，并能与其他引导方法结合进一步提升性能。

Conclusion: ERG是一种简单有效的引导机制，适用于多种生成任务，且具有通用性和灵活性。

Abstract: Guidance techniques are commonly used in diffusion and flow models to improve
image quality and consistency for conditional generative tasks such as
class-conditional and text-to-image generation. In particular, classifier-free
guidance (CFG) -- the most widely adopted guidance technique -- contrasts
conditional and unconditional predictions to improve the generated images. This
results, however, in trade-offs across quality, diversity and consistency,
improving some at the expense of others. While recent work has shown that it is
possible to disentangle these factors to some extent, such methods come with an
overhead of requiring an additional (weaker) model, or require more forward
passes per sampling step. In this paper, we propose Entropy Rectifying Guidance
(ERG), a simple and effective guidance mechanism based on inference-time
changes in the attention mechanism of state-of-the-art diffusion transformer
architectures, which allows for simultaneous improvements over image quality,
diversity and prompt consistency. ERG is more general than CFG and similar
guidance techniques, as it extends to unconditional sampling. ERG results in
significant improvements in various generation tasks such as text-to-image,
class-conditional and unconditional image generation. We also show that ERG can
be seamlessly combined with other recent guidance methods such as CADS and APG,
further boosting generation performance.

</details>


### [69] [Scaling LLaNA: Advancing NeRF-Language Understanding Through Large-Scale Training](https://arxiv.org/abs/2504.13995)
*Andrea Amaduzzi,Pierluigi Zama Ramirez,Giuseppe Lisanti,Samuele Salti,Luigi Di Stefano*

Main category: cs.CV

TL;DR: LLaNA是一种多模态大语言模型（MLLM），能够直接处理NeRF的MLP权重，实现NeRF描述和问答等新任务，无需渲染图像或生成3D数据。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在理解图像和3D数据时存在局限性，而NeRF能更全面地编码几何和外观信息，因此研究将NeRF融入MLLM的可行性和效果。

Method: 提出LLaNA模型，直接处理NeRF的MLP权重，并构建了一个包含30万NeRF的大规模数据集（ShapeNet和Objaverse），用于训练和评估。

Result: 直接处理NeRF权重在NeRF-语言任务上表现优于依赖2D或3D表示的方法。

Conclusion: LLaNA展示了直接处理NeRF权重的有效性，为NeRF与语言模型的结合提供了新方向。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have shown
remarkable capabilities in understanding both images and 3D data, yet these
modalities face inherent limitations in comprehensively representing object
geometry and appearance. Neural Radiance Fields (NeRFs) have emerged as a
promising alternative, encoding both geometric and photorealistic properties
within the weights of a simple Multi-Layer Perceptron (MLP). This work
investigates the feasibility and effectiveness of ingesting NeRFs into an MLLM.
We introduce LLaNA, the first MLLM able to perform new tasks such as NeRF
captioning and Q\&A, by directly processing the weights of a NeRF's MLP.
Notably, LLaNA is able to extract information about the represented objects
without the need to render images or materialize 3D data structures. In
addition, we build the first large-scale NeRF-language dataset, composed by
more than 300K NeRFs trained on ShapeNet and Objaverse, with paired textual
annotations that enable various NeRF-language tasks. Based on this dataset, we
develop a benchmark to evaluate the NeRF understanding capability of our
method. Results show that directly processing NeRF weights leads to better
performance on NeRF-Language tasks compared to approaches that rely on either
2D or 3D representations derived from NeRFs.

</details>


### [70] [Fashion-RAG: Multimodal Fashion Image Editing via Retrieval-Augmented Generation](https://arxiv.org/abs/2504.14011)
*Fulvio Sanguigni,Davide Morelli,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: 提出了一种名为Fashion-RAG的新方法，通过文本输入实现时尚物品的个性化定制，结合检索和生成技术，显著提升了虚拟试穿的效果。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法通常需要特定服装输入，而用户可能仅提供文本描述，限制了实用性。Fashion-RAG旨在解决这一问题。

Method: 采用检索增强生成技术，通过文本输入检索匹配的服装，利用文本反转技术将检索结果嵌入Stable Diffusion模型，生成个性化图像。

Result: 在Dress Code数据集上，Fashion-RAG在质量和数量上均优于现有方法，能够捕捉服装的细粒度视觉细节。

Conclusion: Fashion-RAG是首个针对多模态时尚图像编辑的检索增强生成方法，为文本驱动的虚拟试穿提供了实用解决方案。

Abstract: In recent years, the fashion industry has increasingly adopted AI
technologies to enhance customer experience, driven by the proliferation of
e-commerce platforms and virtual applications. Among the various tasks, virtual
try-on and multimodal fashion image editing -- which utilizes diverse input
modalities such as text, garment sketches, and body poses -- have become a key
area of research. Diffusion models have emerged as a leading approach for such
generative tasks, offering superior image quality and diversity. However, most
existing virtual try-on methods rely on having a specific garment input, which
is often impractical in real-world scenarios where users may only provide
textual specifications. To address this limitation, in this work we introduce
Fashion Retrieval-Augmented Generation (Fashion-RAG), a novel method that
enables the customization of fashion items based on user preferences provided
in textual form. Our approach retrieves multiple garments that match the input
specifications and generates a personalized image by incorporating attributes
from the retrieved items. To achieve this, we employ textual inversion
techniques, where retrieved garment images are projected into the textual
embedding space of the Stable Diffusion text encoder, allowing seamless
integration of retrieved elements into the generative process. Experimental
results on the Dress Code dataset demonstrate that Fashion-RAG outperforms
existing methods both qualitatively and quantitatively, effectively capturing
fine-grained visual details from retrieved garments. To the best of our
knowledge, this is the first work to introduce a retrieval-augmented generation
approach specifically tailored for multimodal fashion image editing.

</details>


### [71] [LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models](https://arxiv.org/abs/2504.14032)
*Haiwen Huang,Anpei Chen,Volodymyr Havrylov,Andreas Geiger,Dan Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于坐标交叉注意力Transformer的特征上采样方法，结合高分辨率图像和低分辨率VFM特征，显著提升了像素级任务的表现。


<details>
  <summary>Details</summary>
Motivation: VFMs（如DINOv2和CLIP）在像素级任务中因特征分辨率不足而受限，特征上采样是解决这一问题的关键。

Method: 设计了基于坐标的交叉注意力Transformer架构，并利用类无关掩码和自蒸馏构建高分辨率伪真值特征。

Result: 实验表明，该方法在多种下游任务中显著优于现有特征上采样技术。

Conclusion: 提出的方法能有效捕捉细粒度细节，灵活适应不同输入和特征分辨率。

Abstract: Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved
impressive results on various downstream tasks, but their limited feature
resolution hampers performance in applications requiring pixel-level
understanding. Feature upsampling offers a promising direction to address this
challenge. In this work, we identify two critical factors for enhancing feature
upsampling: the upsampler architecture and the training objective. For the
upsampler architecture, we introduce a coordinate-based cross-attention
transformer that integrates the high-resolution images with coordinates and
low-resolution VFM features to generate sharp, high-quality features. For the
training objective, we propose constructing high-resolution pseudo-groundtruth
features by leveraging class-agnostic masks and self-distillation. Our approach
effectively captures fine-grained details and adapts flexibly to various input
and feature resolutions. Through experiments, we demonstrate that our approach
significantly outperforms existing feature upsampling techniques across various
downstream tasks. Our code is released at https://github.com/andrehuang/loftup.

</details>


### [72] [Occlusion-Ordered Semantic Instance Segmentation](https://arxiv.org/abs/2504.14054)
*Soroosh Baselizadeh,Cheuk-To Yu,Olga Veksler,Yuri Boykov*

Main category: cs.CV

TL;DR: 论文提出了一种名为OOSIS的任务，结合相对深度排序和实例分割，通过遮挡关系提供3D信息，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的2D实例分割缺乏3D信息，而单目深度估计难度较大。因此，作者提出利用遮挡关系实现更可靠的相对深度排序，以补充3D信息。

Method: 提出OOSIS任务，结合遮挡排序和实例分割，将其建模为标注问题。开发了一种新颖的定向遮挡边界方法，并设计了联合评估指标。

Result: 在KINS和COCOA数据集上表现优于基线方法。

Conclusion: OOSIS通过遮挡关系提供了一种简单有效的3D信息补充方法，优于传统深度估计方法。

Abstract: Standard semantic instance segmentation provides useful, but inherently 2D
information from a single image. To enable 3D analysis, one usually integrates
absolute monocular depth estimation with instance segmentation. However,
monocular depth is a difficult task. Instead, we leverage a simpler
single-image task, occlusion-based relative depth ordering, providing coarser
but useful 3D information. We show that relative depth ordering works more
reliably from occlusions than from absolute depth. We propose to solve the
joint task of relative depth ordering and segmentation of instances based on
occlusions. We call this task Occlusion-Ordered Semantic Instance Segmentation
(OOSIS). We develop an approach to OOSIS that extracts instances and their
occlusion order simultaneously from oriented occlusion boundaries and semantic
segmentation. Unlike popular detect-and-segment framework for instance
segmentation, combining occlusion ordering with instance segmentation allows a
simple and clean formulation of OOSIS as a labeling problem. As a part of our
solution for OOSIS, we develop a novel oriented occlusion boundaries approach
that significantly outperforms prior work. We also develop a new joint OOSIS
metric based both on instance mask accuracy and correctness of their occlusion
order. We achieve better performance than strong baselines on KINS and COCOA
datasets.

</details>


### [73] [Towards Scale-Aware Low-Light Enhancement via Structure-Guided Transformer Design](https://arxiv.org/abs/2504.14075)
*Wei Dong,Yan Min,Han Zhou,Jun Chen*

Main category: cs.CV

TL;DR: SG-LLIE是一种基于结构先验的多尺度CNN-Transformer混合框架，用于低光图像增强，通过引入结构引导的Transformer模块，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有低光图像增强技术在极端低光环境下因图像损坏严重而难以提取有效语义或光照信息的问题。

Method: 提出SG-LLIE框架，利用光照不变边缘检测器提取结构先验，并在UNet架构中嵌入CNN-Transformer混合模块（HSGFE），结合CNN的多尺度特征提取能力和Transformer的结构引导调制。

Result: 在多个低光图像增强基准测试中取得最优性能，并在NTIRE 2025挑战赛中排名第二。

Conclusion: SG-LLIE通过结构先验和多尺度混合设计，显著提升了极端低光环境下的图像增强效果。

Abstract: Current Low-light Image Enhancement (LLIE) techniques predominantly rely on
either direct Low-Light (LL) to Normal-Light (NL) mappings or guidance from
semantic features or illumination maps. Nonetheless, the intrinsic
ill-posedness of LLIE and the difficulty in retrieving robust semantics from
heavily corrupted images hinder their effectiveness in extremely low-light
environments. To tackle this challenge, we present SG-LLIE, a new multi-scale
CNN-Transformer hybrid framework guided by structure priors. Different from
employing pre-trained models for the extraction of semantics or illumination
maps, we choose to extract robust structure priors based on
illumination-invariant edge detectors. Moreover, we develop a CNN-Transformer
Hybrid Structure-Guided Feature Extractor (HSGFE) module at each scale with in
the UNet encoder-decoder architecture. Besides the CNN blocks which excels in
multi-scale feature extraction and fusion, we introduce a Structure-Guided
Transformer Block (SGTB) in each HSGFE that incorporates structural priors to
modulate the enhancement process. Extensive experiments show that our method
achieves state-of-the-art performance on several LLIE benchmarks in both
quantitative metrics and visual quality. Our solution ranks second in the NTIRE
2025 Low-Light Enhancement Challenge. Code is released at
https://github.com/minyan8/imagine.

</details>


### [74] [Retinex-guided Histogram Transformer for Mask-free Shadow Removal](https://arxiv.org/abs/2504.14092)
*Wei Dong,Han Zhou,Seyed Amirreza Mousavi,Jun Chen*

Main category: cs.CV

TL;DR: 提出了一种基于CNN-Transformer混合架构的无掩模阴影去除框架ReHiT，结合Retinex理论，通过双分支管道分别建模反射和光照分量，并利用IG-HCT模块进行恢复。


<details>
  <summary>Details</summary>
Motivation: 现有阴影去除方法依赖难以获取的阴影掩模，限制了其在真实场景中的泛化能力。

Method: 采用双分支管道分别处理反射和光照分量，结合CNN和Transformer模块（IG-HCT和IGHB）处理非均匀光照和复杂阴影。

Result: 在多个基准数据集上表现优于现有无掩模方法，参数少且推理速度快，适用于计算资源有限的场景。

Conclusion: ReHiT框架在无掩模阴影去除任务中表现出色，具有实际应用潜力。

Abstract: While deep learning methods have achieved notable progress in shadow removal,
many existing approaches rely on shadow masks that are difficult to obtain,
limiting their generalization to real-world scenes. In this work, we propose
ReHiT, an efficient mask-free shadow removal framework based on a hybrid
CNN-Transformer architecture guided by Retinex theory. We first introduce a
dual-branch pipeline to separately model reflectance and illumination
components, and each is restored by our developed Illumination-Guided Hybrid
CNN-Transformer (IG-HCT) module. Second, besides the CNN-based blocks that are
capable of learning residual dense features and performing multi-scale semantic
fusion, multi-scale semantic fusion, we develop the Illumination-Guided
Histogram Transformer Block (IGHB) to effectively handle non-uniform
illumination and spatially complex shadows. Extensive experiments on several
benchmark datasets validate the effectiveness of our approach over existing
mask-free methods. Trained solely on the NTIRE 2025 Shadow Removal Challenge
dataset, our solution delivers competitive results with one of the smallest
parameter sizes and fastest inference speeds among top-ranked entries,
highlighting its applicability for real-world applications with limited
computational resources. The code is available at
https://github.com/dongw22/oath.

</details>


### [75] [VideoPASTA: 7K Preference Pairs That Matter for Video-LLM Alignment](https://arxiv.org/abs/2504.14096)
*Yogesh Kulkarni,Pooyan Fazli*

Main category: cs.CV

TL;DR: VideoPASTA通过偏好优化增强视频语言模型，解决了空间关系、时间顺序和跨帧连续性问题，仅需少量数据即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型在空间关系、时间顺序和跨帧连续性方面表现不佳，需要一种高效方法提升其能力。

Method: 引入VideoPASTA框架，通过对抗样本训练模型区分准确与错误的视频表示，并应用直接偏好优化。

Result: 在多个基准测试中性能显著提升（如VideoMME提升3.05%），且无需大量数据或架构修改。

Conclusion: VideoPASTA是一种高效、可扩展的解决方案，无需复杂设置即可提升视频语言模型的核心能力。

Abstract: Video-language models (Video-LLMs) excel at understanding video content but
struggle with spatial relationships, temporal ordering, and cross-frame
continuity. To address these limitations, we introduce VideoPASTA (Preference
Alignment with Spatio-Temporal-Cross Frame Adversaries), a framework that
enhances Video-LLMs through targeted preference optimization. VideoPASTA trains
models to distinguish accurate video representations from carefully generated
adversarial examples that deliberately violate spatial, temporal, or
cross-frame relations. By applying Direct Preference Optimization to just 7,020
preference pairs, VideoPASTA learns robust representations that capture
fine-grained spatial relationships and long-range temporal dynamics.
Experiments on standard video benchmarks show significant relative performance
gains of 3.05% on VideoMME, 1.97% on NeXTQA, and 1.31% on LongVideoBench, over
the baseline Qwen2.5-VL model. These results demonstrate that targeted
alignment, rather than massive pretraining or architectural modifications,
effectively addresses core video-language challenges. Notably, VideoPASTA
achieves these improvements without human annotation or captioning, relying on
just 32-frame sampling, compared to the 96-frame, multi-GPU setups of prior
work. This efficiency makes our approach a scalable, plug-and-play solution
that seamlessly integrates with existing models while preserving their
capabilities.

</details>


### [76] [Point-Driven Interactive Text and Image Layer Editing Using Diffusion Models](https://arxiv.org/abs/2504.14108)
*Zhenyu Yu,Mohd Yamani Idna Idris,Pei Wang,Yuelong Xia*

Main category: cs.CV

TL;DR: DanceText是一个无需训练的多语言图像文本编辑框架，支持复杂几何变换并实现无缝的前景-背景融合。


<details>
  <summary>Details</summary>
Motivation: 解决基于扩散的生成模型在文本引导图像合成中缺乏可控性和布局一致性的问题。

Method: 采用分层编辑策略分离文本与背景，结合深度感知模块增强真实感和空间一致性，完全无需训练。

Result: 在AnyWord-3M基准测试中表现出色，尤其在复杂变换场景下视觉质量优越。

Conclusion: DanceText通过模块化和可控的设计，实现了高质量的多语言文本编辑。

Abstract: We present DanceText, a training-free framework for multilingual text editing
in images, designed to support complex geometric transformations and achieve
seamless foreground-background integration. While diffusion-based generative
models have shown promise in text-guided image synthesis, they often lack
controllability and fail to preserve layout consistency under non-trivial
manipulations such as rotation, translation, scaling, and warping. To address
these limitations, DanceText introduces a layered editing strategy that
separates text from the background, allowing geometric transformations to be
performed in a modular and controllable manner. A depth-aware module is further
proposed to align appearance and perspective between the transformed text and
the reconstructed background, enhancing photorealism and spatial consistency.
Importantly, DanceText adopts a fully training-free design by integrating
pretrained modules, allowing flexible deployment without task-specific
fine-tuning. Extensive experiments on the AnyWord-3M benchmark demonstrate that
our method achieves superior performance in visual quality, especially under
large-scale and complex transformation scenarios.

</details>


### [77] [Lightweight Road Environment Segmentation using Vector Quantization](https://arxiv.org/abs/2504.14113)
*Jiyong Kwag,Alper Yilmaz,Charles Toth*

Main category: cs.CV

TL;DR: 论文提出了一种基于向量量化的方法，用于自动驾驶环境的分割任务，通过离散化特征提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于FCN和Transformer的编码器依赖连续特征表示，限制了离散信息的提取能力。

Method: 结合向量量化与轻量级MobileUNETR模型，将连续特征映射为离散向量。

Result: 在Cityscapes数据集上达到77.0% mIoU，比基线模型提升2.9%。

Conclusion: 向量量化能有效提升分割性能，且不增加模型复杂度。

Abstract: Road environment segmentation plays a significant role in autonomous driving.
Numerous works based on Fully Convolutional Networks (FCNs) and Transformer
architectures have been proposed to leverage local and global contextual
learning for efficient and accurate semantic segmentation. In both
architectures, the encoder often relies heavily on extracting continuous
representations from the image, which limits the ability to represent
meaningful discrete information. To address this limitation, we propose
segmentation of the autonomous driving environment using vector quantization.
Vector quantization offers three primary advantages for road environment
segmentation. (1) Each continuous feature from the encoder is mapped to a
discrete vector from the codebook, helping the model discover distinct features
more easily than with complex continuous features. (2) Since a discrete feature
acts as compressed versions of the encoder's continuous features, they also
compress noise or outliers, enhancing the image segmentation task. (3) Vector
quantization encourages the latent space to form coarse clusters of continuous
features, forcing the model to group similar features, making the learned
representations more structured for the decoding process. In this work, we
combined vector quantization with the lightweight image segmentation model
MobileUNETR and used it as a baseline model for comparison to demonstrate its
efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes,
outperforming the baseline by 2.9 % without increasing the model's initial size
or complexity.

</details>


### [78] [BMRL: Bi-Modal Guided Multi-Perspective Representation Learning for Zero-Shot Deepfake Attribution](https://arxiv.org/abs/2504.14129)
*Yaning Zhang,Jiahe Zhang,Chunjie Ma,Weili Guan,Tian Gan,Zan Gao*

Main category: cs.CV

TL;DR: 提出了一种双模态引导的多视角表示学习框架（BMRL），用于零样本深度伪造溯源（ZS-DFA），通过视觉、解析和语言模态提升溯源性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造溯源方法主要关注视觉模态，忽视其他模态（如文本和面部解析），且难以对未见生成器进行细粒度评估。

Method: 设计了多视角视觉编码器（MPVE）和解析编码器，结合语言编码器，通过对比中心损失（DFACC）优化表示学习。

Result: 实验表明，该方法在ZS-DFA任务上优于现有技术。

Conclusion: BMRL框架通过多模态融合显著提升了深度伪造溯源的泛化能力。

Abstract: The challenge of tracing the source attribution of forged faces has gained
significant attention due to the rapid advancement of generative models.
However, existing deepfake attribution (DFA) works primarily focus on the
interaction among various domains in vision modality, and other modalities such
as texts and face parsing are not fully explored. Besides, they tend to fail to
assess the generalization performance of deepfake attributors to unseen
generators in a fine-grained manner. In this paper, we propose a novel bi-modal
guided multi-perspective representation learning (BMRL) framework for zero-shot
deepfake attribution (ZS-DFA), which facilitates effective traceability to
unseen generators. Specifically, we design a multi-perspective visual encoder
(MPVE) to explore general deepfake attribution visual characteristics across
three views (i.e., image, noise, and edge). We devise a novel parsing encoder
to focus on global face attribute embeddings, enabling parsing-guided DFA
representation learning via vision-parsing matching. A language encoder is
proposed to capture fine-grained language embeddings, facilitating
language-guided general visual forgery representation learning through
vision-language alignment. Additionally, we present a novel deepfake
attribution contrastive center (DFACC) loss, to pull relevant generators closer
and push irrelevant ones away, which can be introduced into DFA models to
enhance traceability. Experimental results demonstrate that our method
outperforms the state-of-the-art on the ZS-DFA task through various protocols
evaluation.

</details>


### [79] [Transforming hyperspectral images into chemical maps: A new deep learning based approach to hyperspectral image processing](https://arxiv.org/abs/2504.14131)
*Ole-Christian Galbo Engstrøm,Michela Albano-Gaglio,Erik Schou Dreier,Yamine Bouzembrak,Maria Font-i-Furnols,Puneet Mishra,Kim Steenstrup Pedersen*

Main category: cs.CV

TL;DR: 本文提出了一种基于改进U-Net和自定义损失函数的端到端深度学习方法，用于从高光谱图像直接生成化学图，避免了传统逐像素分析的中间步骤，并在脂肪预测任务中表现优于传统PLS回归。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如PLS回归）生成的化学图未考虑空间上下文且噪声较高，因此需要一种更高效且准确的方法。

Method: 使用改进的U-Net和自定义损失函数，直接从高光谱图像生成化学图，并与PLS回归在猪肉样本数据集上进行比较。

Result: U-Net的均方根误差比PLS低9%-13%，生成的化学图99.91%的方差具有空间相关性，且预测值保持在物理可能范围内。

Conclusion: U-Net在化学图生成任务中优于PLS回归，尤其在空间相关性和预测范围控制方面表现突出。

Abstract: Current approaches to chemical map generation from hyperspectral images are
based on models such as partial least squares (PLS) regression, generating
pixel-wise predictions that do not consider spatial context and suffer from a
high degree of noise. This study proposes an end-to-end deep learning approach
using a modified version of U-Net and a custom loss function to directly obtain
chemical maps from hyperspectral images, skipping all intermediate steps
required for traditional pixel-wise analysis. We compare the U-Net with the
traditional PLS regression on a real dataset of pork belly samples with
associated mean fat reference values. The U-Net obtains a test set root mean
squared error of between 9% and 13% lower than that of PLS regression on the
task of mean fat prediction. At the same time, U-Net generates fine detail
chemical maps where 99.91% of the variance is spatially correlated. Conversely,
only 2.53% of the variance in the PLS-generated chemical maps is spatially
correlated, indicating that each pixel-wise prediction is largely independent
of neighboring pixels. Additionally, while the PLS-generated chemical maps
contain predictions far beyond the physically possible range of 0-100%, U-Net
learns to stay inside this range. Thus, the findings of this study indicate
that U-Net is superior to PLS for chemical map generation.

</details>


### [80] [HFBRI-MAE: Handcrafted Feature Based Rotation-Invariant Masked Autoencoder for 3D Point Cloud Analysis](https://arxiv.org/abs/2504.14132)
*Xuanhua Yin,Dingxin Zhang,Jianhui Yu,Weidong Cai*

Main category: cs.CV

TL;DR: HFBRI-MAE是一种改进的自监督学习框架，通过引入旋转不变性特征，解决了现有MAE方法在处理旋转点云时的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有MAE方法缺乏旋转不变性，导致在真实场景中处理旋转点云时性能显著下降。

Method: HFBRI-MAE结合旋转不变的手工特征，改进MAE设计，利用局部和全局特征进行嵌入，并重新定义重建目标为规范对齐版本。

Result: 在ModelNet40、ScanObjectNN和ShapeNetPart上的实验表明，HFBRI-MAE在分类、分割和少样本学习中均优于现有方法。

Conclusion: HFBRI-MAE具有鲁棒性和强泛化能力，适用于真实世界的3D应用。

Abstract: Self-supervised learning (SSL) has demonstrated remarkable success in 3D
point cloud analysis, particularly through masked autoencoders (MAEs). However,
existing MAE-based methods lack rotation invariance, leading to significant
performance degradation when processing arbitrarily rotated point clouds in
real-world scenarios. To address this limitation, we introduce Handcrafted
Feature-Based Rotation-Invariant Masked Autoencoder (HFBRI-MAE), a novel
framework that refines the MAE design with rotation-invariant handcrafted
features to ensure stable feature learning across different orientations. By
leveraging both rotation-invariant local and global features for token
embedding and position embedding, HFBRI-MAE effectively eliminates rotational
dependencies while preserving rich geometric structures. Additionally, we
redefine the reconstruction target to a canonically aligned version of the
input, mitigating rotational ambiguities. Extensive experiments on ModelNet40,
ScanObjectNN, and ShapeNetPart demonstrate that HFBRI-MAE consistently
outperforms existing methods in object classification, segmentation, and
few-shot learning, highlighting its robustness and strong generalization
ability in real-world 3D applications.

</details>


### [81] [Rethinking Target Label Conditioning in Adversarial Attacks: A 2D Tensor-Guided Generative Approach](https://arxiv.org/abs/2504.14137)
*Hangyu Liu,Bo Peng,Pengxiang Ding,Donglin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为2D-TGAF的多目标对抗攻击框架，通过扩散模型生成二维语义张量指导噪声生成，显著提升了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有生成式多目标攻击方法缺乏实践验证和全面总结，作者发现语义特征的质量和数量是影响攻击迁移性的关键因素。

Method: 提出2D-TGAF框架，利用扩散模型生成二维语义张量指导噪声生成，并设计掩码策略保留目标类别的完整语义信息。

Result: 在ImageNet数据集上的实验表明，2D-TGAF在攻击成功率和防御机制下的表现均优于现有方法。

Conclusion: 2D-TGAF通过优化语义特征的质量和数量，显著提升了多目标对抗攻击的效果。

Abstract: Compared to single-target adversarial attacks, multi-target attacks have
garnered significant attention due to their ability to generate adversarial
images for multiple target classes simultaneously. Existing generative
approaches for multi-target attacks mainly analyze the effect of the use of
target labels on noise generation from a theoretical perspective, lacking
practical validation and comprehensive summarization. To address this gap, we
first identify and validate that the semantic feature quality and quantity are
critical factors affecting the transferability of targeted attacks: 1) Feature
quality refers to the structural and detailed completeness of the implanted
target features, as deficiencies may result in the loss of key discriminative
information; 2) Feature quantity refers to the spatial sufficiency of the
implanted target features, as inadequacy limits the victim model's attention to
this feature. Based on these findings, we propose the 2D Tensor-Guided
Adversarial Fusion (2D-TGAF) framework, which leverages the powerful generative
capabilities of diffusion models to encode target labels into two-dimensional
semantic tensors for guiding adversarial noise generation. Additionally, we
design a novel masking strategy tailored for the training process, ensuring
that parts of the generated noise retain complete semantic information about
the target class. Extensive experiments on the standard ImageNet dataset
demonstrate that 2D-TGAF consistently surpasses state-of-the-art methods in
attack success rates, both on normally trained models and across various
defense mechanisms.

</details>


### [82] [Segment Any Crack: Deep Semantic Segmentation Adaptation for Crack Detection](https://arxiv.org/abs/2504.14138)
*Ghodsiyeh Rostami,Po-Han Chen,Mahdi S. Hosseini*

Main category: cs.CV

TL;DR: 提出了一种选择性微调策略，专注于调整归一化组件，以提高分割模型在裂缝检测中的适应性。该方法在性能和计算效率上优于全微调和其他常见微调技术。


<details>
  <summary>Details</summary>
Motivation: 现有裂缝检测模型需要大量标注数据和计算资源进行微调，限制了其适应多样性条件的能力。

Method: 引入选择性微调策略，仅调整归一化参数，应用于Segment Anything Model（SAM）和五种成熟分割模型。

Result: 提出的方法在OmniCrack30k数据集上达到61.22% F1-score和44.13% IoU，并在零样本数据集上表现最佳。

Conclusion: 选择性微调显著提高了分割精度，同时大幅降低了计算开销。

Abstract: Image-based crack detection algorithms are increasingly in demand in
infrastructure monitoring, as early detection of cracks is of paramount
importance for timely maintenance planning. While deep learning has
significantly advanced crack detection algorithms, existing models often
require extensive labeled datasets and high computational costs for
fine-tuning, limiting their adaptability across diverse conditions. This study
introduces an efficient selective fine-tuning strategy, focusing on tuning
normalization components, to enhance the adaptability of segmentation models
for crack detection. The proposed method is applied to the Segment Anything
Model (SAM) and five well-established segmentation models. Experimental results
demonstrate that selective fine-tuning of only normalization parameters
outperforms full fine-tuning and other common fine-tuning techniques in both
performance and computational efficiency, while improving generalization. The
proposed approach yields a SAM-based model, Segment Any Crack (SAC), achieving
a 61.22\% F1-score and 44.13\% IoU on the OmniCrack30k benchmark dataset, along
with the highest performance across three zero-shot datasets and the lowest
standard deviation. The results highlight the effectiveness of the adaptation
approach in improving segmentation accuracy while significantly reducing
computational overhead.

</details>


### [83] [ThyroidEffi 1.0: A Cost-Effective System for High-Performance Multi-Class Thyroid Carcinoma Classification](https://arxiv.org/abs/2504.14139)
*Hai Pham-Ngoc,De Nguyen-Van,Dung Vu-Tien,Phuong Le-Hong*

Main category: cs.CV

TL;DR: 开发了一种高效、可解释的深度学习系统，用于甲状腺细针穿刺活检图像的多类分类，并在越南进行了外部验证，实现了高诊断准确性和低计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决甲状腺细针穿刺活检图像自动分类中的数据有限、观察者间变异性和计算成本高的挑战，为临床决策提供支持。

Method: 采用YOLOv10检测细胞簇提取信息子区域，课程学习协议捕获多尺度特征，轻量级EfficientNetB0平衡性能与效率，Transformer模块进行多尺度多区域分析。

Result: 内部测试集宏F1为89.19%，外部验证AUC分别为0.9495 (B2)、0.7436 (B5)和0.8396 (B6)，处理1000例仅需30秒。

Conclusion: 高精度、可解释的甲状腺FNAB图像分类可在低计算需求下实现。

Abstract: Background: Automated classification of thyroid fine needle aspiration biopsy
(FNAB) images faces challenges in limited data, inter-observer variability, and
computational cost. Efficient, interpretable models are crucial for clinical
support. Objective: To develop and externally validate a deep learning system
for the multi-class classification of thyroid FNAB images into three key
categories that directly guide post-biopsy treatment decisions in Vietnam:
benign (B2), suspicious for malignancy (B5), and malignant (B6), while
achieving high diagnostic accuracy with low computational overhead. Methods:
Our framework features: (1) YOLOv10-based cell cluster detection for
informative sub-region extraction and noise reduction; (2) a curriculum
learning-inspired protocol sequencing localized crops to full images for
multi-scale feature capture; (3) adaptive lightweight EfficientNetB0 (4
millions parameters) selection balancing performance and efficiency; and (4) a
Transformer-inspired module for multi-scale, multi-region analysis. External
validation used 1,015 independent FNAB images. Results: ThyroidEffi Basic
achieved a macro F1 of 89.19\% and AUCs of 0.98 (B2), 0.95 (B5), and 0.96 (B6)
on the internal test set. External validation yielded AUCs of 0.9495 (B2),
0.7436 (B5), and 0.8396 (B6). ThyroidEffi Premium improved macro F1 to 89.77\%.
Grad-CAM highlighted key diagnostic regions, confirming interpretability. The
system processed 1000 cases in 30 seconds, demonstrating feasibility on widely
accessible hardware like a 12-core CPU. Conclusions: This work demonstrates
that high-accuracy, interpretable thyroid FNAB image classification is
achievable with minimal computational demands.

</details>


### [84] [Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D](https://arxiv.org/abs/2504.14151)
*Sergio Arnaud,Paul McVay,Ada Martin,Arjun Majumdar,Krishna Murthy Jatavallabhula,Phillip Thomas,Ruslan Partsey,Daniel Dugas,Abha Gejji,Alexander Sax,Vincent-Pierre Berges,Mikael Henaff,Ayush Jain,Ang Cao,Ishita Prasad,Mrinal Kalakrishnan,Michael Rabbat,Nicolas Ballas,Mido Assran,Oleksandr Maksymets,Aravind Rajeswaran,Franziska Meier*

Main category: cs.CV

TL;DR: LOCATE 3D是一种通过描述性语言定位3D场景中物体的模型，结合3D-JEPA自监督学习算法，在标准基准测试中表现优异，并支持实际部署。


<details>
  <summary>Details</summary>
Motivation: 解决通过自然语言描述在3D场景中精准定位物体的问题，适用于机器人和AR设备。

Method: 使用3D-JEPA自监督学习算法，结合2D基础模型（如CLIP、DINO）处理点云数据，通过掩码预测任务学习点云特征，并微调语言条件解码器预测3D掩码和边界框。

Result: 在标准基准测试中达到新最优性能，并展示了强大的泛化能力。

Conclusion: LOCATE 3D在3D物体定位任务中表现出色，结合新数据集和自监督学习方法，为实际应用提供了有效解决方案。

Abstract: We present LOCATE 3D, a model for localizing objects in 3D scenes from
referring expressions like "the small coffee table between the sofa and the
lamp." LOCATE 3D sets a new state-of-the-art on standard referential grounding
benchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D
operates directly on sensor observation streams (posed RGB-D frames), enabling
real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA,
a novel self-supervised learning (SSL) algorithm applicable to sensor point
clouds. It takes as input a 3D pointcloud featurized using 2D foundation models
(CLIP, DINO). Subsequently, masked prediction in latent space is employed as a
pretext task to aid the self-supervised learning of contextualized pointcloud
features. Once trained, the 3D-JEPA encoder is finetuned alongside a
language-conditioned decoder to jointly predict 3D masks and bounding boxes.
Additionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential
grounding, spanning multiple capture setups with over 130K annotations. This
enables a systematic study of generalization capabilities as well as a stronger
model.

</details>


### [85] [Segregation and Context Aggregation Network for Real-time Cloud Segmentation](https://arxiv.org/abs/2504.14178)
*Yijie Li,Hewei Wang,Jiayi Zhang,Jinjiang You,Jinfeng Xu,Puzhen Wu,Yunzhong Xiao,Soumyabrata Dev*

Main category: cs.CV

TL;DR: SCANet是一种轻量级云分割模型，通过Segregation and Context Aggregation Module（SCAM）提升分割精度和计算效率，适用于边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有云分割方法在精度和计算效率之间难以平衡，限制了实际应用。

Method: 提出SCANet模型，采用SCAM模块细化分割图，并设计高效预训练策略。

Result: SCANet-large参数减少70.9%，SCANet-lite达到1390 fps，性能优于现有方法。

Conclusion: SCANet在云分割任务中实现了高效与高性能的平衡，适合实际部署。

Abstract: Cloud segmentation from intensity images is a pivotal task in atmospheric
science and computer vision, aiding weather forecasting and climate analysis.
Ground-based sky/cloud segmentation extracts clouds from images for further
feature analysis. Existing methods struggle to balance segmentation accuracy
and computational efficiency, limiting real-world deployment on edge devices,
so we introduce SCANet, a novel lightweight cloud segmentation model featuring
Segregation and Context Aggregation Module (SCAM), which refines rough
segmentation maps into weighted sky and cloud features processed separately.
SCANet achieves state-of-the-art performance while drastically reducing
computational complexity. SCANet-large (4.29M) achieves comparable accuracy to
state-of-the-art methods with 70.9% fewer parameters. Meanwhile, SCANet-lite
(90K) delivers 1390 fps in FP16, surpassing real-time standards. Additionally,
we propose an efficient pre-training strategy that enhances performance even
without ImageNet pre-training.

</details>


### [86] [Enhancing Multimodal In-Context Learning for Image Classification through Coreset Optimization](https://arxiv.org/abs/2504.14200)
*Huiyi Chen,Jiawei Peng,Kaihua Tang,Xin Geng,Xu Yang*

Main category: cs.CV

TL;DR: 论文提出了一种名为KeCO的新框架，通过利用未使用的数据构建紧凑且信息丰富的核心集，显著提升了大规模视觉语言模型的上下文学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图像分类任务中因特征空间差异和高计算成本而表现不佳，且丢弃样本导致信息损失。KeCO旨在解决这些问题。

Method: KeCO引入视觉特征作为核心集的键，通过不同选择策略更新样本，使随机初始化的核心集演变为更具信息性的核心集。

Result: 在粗粒度和细粒度图像分类基准测试中，KeCO平均提升性能超过20%，并在模拟在线场景中表现优异。

Conclusion: KeCO为资源受限的实际场景提供了一种高效且实用的解决方案。

Abstract: In-context learning (ICL) enables Large Vision-Language Models (LVLMs) to
adapt to new tasks without parameter updates, using a few demonstrations from a
large support set. However, selecting informative demonstrations leads to high
computational and memory costs. While some methods explore selecting a small
and representative coreset in the text classification, evaluating all support
set samples remains costly, and discarded samples lead to unnecessary
information loss. These methods may also be less effective for image
classification due to differences in feature spaces. Given these limitations,
we propose Key-based Coreset Optimization (KeCO), a novel framework that
leverages untapped data to construct a compact and informative coreset. We
introduce visual features as keys within the coreset, which serve as the anchor
for identifying samples to be updated through different selection strategies.
By leveraging untapped samples from the support set, we update the keys of
selected coreset samples, enabling the randomly initialized coreset to evolve
into a more informative coreset under low computational cost. Through extensive
experiments on coarse-grained and fine-grained image classification benchmarks,
we demonstrate that KeCO effectively enhances ICL performance for image
classification task, achieving an average improvement of more than 20\%.
Notably, we evaluate KeCO under a simulated online scenario, and the strong
performance in this scenario highlights the practical value of our framework
for resource-constrained real-world scenarios.

</details>


### [87] [Learning Joint ID-Textual Representation for ID-Preserving Image Synthesis](https://arxiv.org/abs/2504.14202)
*Zichuan Liu,Liming Jiang,Qing Yan,Yumin Jia,Hao Kang,Xin Lu*

Main category: cs.CV

TL;DR: 提出了一种基于多模态编码策略的ID保留生成框架FaceCLIP，通过联合嵌入空间统一处理身份和文本信息，结合扩散模型生成身份一致且文本对齐的图像。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过适配器注入身份特征，限制了生成效果。本文旨在通过多模态编码策略更自然地统一身份和文本条件。

Method: 引入FaceCLIP多模态编码器，学习身份与文本的联合嵌入空间；提出多模态对齐算法训练FaceCLIP；结合SDXL构建FaceCLIP-SDXL生成管道。

Result: FaceCLIP-SDXL在身份保留和文本对齐上优于现有方法，实验验证了其定量和定性优势。

Conclusion: FaceCLIP框架为ID保留生成提供了高效解决方案，展示了多模态编码的潜力。

Abstract: We propose a novel framework for ID-preserving generation using a multi-modal
encoding strategy rather than injecting identity features via adapters into
pre-trained models. Our method treats identity and text as a unified
conditioning input. To achieve this, we introduce FaceCLIP, a multi-modal
encoder that learns a joint embedding space for both identity and textual
semantics. Given a reference face and a text prompt, FaceCLIP produces a
unified representation that encodes both identity and text, which conditions a
base diffusion model to generate images that are identity-consistent and
text-aligned. We also present a multi-modal alignment algorithm to train
FaceCLIP, using a loss that aligns its joint representation with face, text,
and image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image
synthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL).
Compared to prior methods, FaceCLIP-SDXL enables photorealistic portrait
generation with better identity preservation and textual relevance. Extensive
experiments demonstrate its quantitative and qualitative superiority.

</details>


### [88] [Real-IAD D3: A Real-World 2D/Pseudo-3D/3D Dataset for Industrial Anomaly Detection](https://arxiv.org/abs/2504.14221)
*Wenbing Zhu,Lidong Wang,Ziqing Zhou,Chengjie Wang,Yurui Pan,Ruoyi Zhang,Zhuhao Chen,Linjie Cheng,Bin-Bin Gao,Jiangning Zhang,Zhenye Gan,Yuxie Wang,Yulong Chen,Shuguang Qian,Mingmin Chi,Bo Peng,Lizhuang Ma*

Main category: cs.CV

TL;DR: 论文介绍了Real-IAD D3数据集，这是一个高精度的多模态工业异常检测数据集，包含RGB图像、微米级3D点云和通过光度立体生成的伪3D模态。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测（IAD）的多模态方法需求增加，但现有数据集（如MVTec 3D）在规模和分辨率上存在不足，难以模拟真实工业环境。

Method: 提出Real-IAD D3数据集，包含RGB、3D点云和伪3D模态，并开发了一种整合多模态信息的有效方法。

Result: 实验表明多模态信息能显著提升检测鲁棒性和IAD性能。

Conclusion: Real-IAD D3为多模态IAD提供了更具挑战性的基准，数据集和代码已公开。

Abstract: The increasing complexity of industrial anomaly detection (IAD) has
positioned multimodal detection methods as a focal area of machine vision
research. However, dedicated multimodal datasets specifically tailored for IAD
remain limited. Pioneering datasets like MVTec 3D have laid essential
groundwork in multimodal IAD by incorporating RGB+3D data, but still face
challenges in bridging the gap with real industrial environments due to
limitations in scale and resolution. To address these challenges, we introduce
Real-IAD D3, a high-precision multimodal dataset that uniquely incorporates an
additional pseudo3D modality generated through photometric stereo, alongside
high-resolution RGB images and micrometer-level 3D point clouds. Real-IAD D3
features finer defects, diverse anomalies, and greater scale across 20
categories, providing a challenging benchmark for multimodal IAD Additionally,
we introduce an effective approach that integrates RGB, point cloud, and
pseudo-3D depth information to leverage the complementary strengths of each
modality, enhancing detection performance. Our experiments highlight the
importance of these modalities in boosting detection robustness and overall IAD
performance. The dataset and code are publicly accessible for research purposes
at https://realiad4ad.github.io/Real-IAD D3

</details>


### [89] [Revisiting CLIP for SF-OSDA: Unleashing Zero-Shot Potential with Adaptive Threshold and Training-Free Feature Filtering](https://arxiv.org/abs/2504.14224)
*Yongguang Li,Jindong Li,Qi Wang,Qianli Xing,Runliang Niu,Shengsheng Wang,Menglin Yang*

Main category: cs.CV

TL;DR: CLIPXpert提出了一种新的SF-OSDA方法，通过自适应阈值和未知类特征过滤模块解决了现有方法的不足，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有SF-OSDA方法依赖固定阈值且忽视类内趋势，导致CLIP的零样本潜力未充分利用。

Method: CLIPXpert结合BGAT模块（动态阈值）和SUFF模块（过滤未知类特征），无需训练即可实现高效分类。

Result: 在DomainNet上超越UOTA 1.92%，在Office-Home等数据集上达到SOTA水平。

Conclusion: CLIPXpert验证了CLIP在SF-OSDA任务中的零样本潜力，为未来研究提供了新方向。

Abstract: Source-Free Unsupervised Open-Set Domain Adaptation (SF-OSDA) methods using
CLIP face significant issues: (1) while heavily dependent on domain-specific
threshold selection, existing methods employ simple fixed thresholds,
underutilizing CLIP's zero-shot potential in SF-OSDA scenarios; and (2)
overlook intrinsic class tendencies while employing complex training to enforce
feature separation, incurring deployment costs and feature shifts that
compromise CLIP's generalization ability. To address these issues, we propose
CLIPXpert, a novel SF-OSDA approach that integrates two key components: an
adaptive thresholding strategy and an unknown class feature filtering module.
Specifically, the Box-Cox GMM-Based Adaptive Thresholding (BGAT) module
dynamically determines the optimal threshold by estimating sample score
distributions, balancing known class recognition and unknown class sample
detection. Additionally, the Singular Value Decomposition (SVD)-Based
Unknown-Class Feature Filtering (SUFF) module reduces the tendency of unknown
class samples towards known classes, improving the separation between known and
unknown classes. Experiments show that our source-free and training-free method
outperforms state-of-the-art trained approach UOTA by 1.92% on the DomainNet
dataset, achieves SOTA-comparable performance on datasets such as Office-Home,
and surpasses other SF-OSDA methods. This not only validates the effectiveness
of our proposed method but also highlights CLIP's strong zero-shot potential
for SF-OSDA tasks.

</details>


### [90] [Exploring Modality Guidance to Enhance VFM-based Feature Fusion for UDA in 3D Semantic Segmentation](https://arxiv.org/abs/2504.14231)
*Johannes Spoecklberger,Wei Lin,Pedro Hermosilla,Sivan Doveh,Horst Possegger,M. Jehanzeb Mirza*

Main category: cs.CV

TL;DR: 该论文探索了视觉基础模型（VFMs）在LiDAR 3D语义分割任务中的应用，通过融合2D-3D数据提升性能。


<details>
  <summary>Details</summary>
Motivation: VFMs在2D视觉任务中表现优异，但其在3D任务中的潜力尚未充分挖掘，尤其是在跨模态数据（如配对的2D-3D数据）中。

Method: 提出了一种融合网络，结合VFMs的跨域特征，利用标记的源数据和未标记的目标数据训练3D骨干网络，并根据目标域动态调整2D和3D数据的贡献。

Result: 在多个实验中，该方法显著优于现有技术，平均提升6.5 mIoU。

Conclusion: VFMs在3D语义分割任务中具有显著潜力，通过跨模态融合可以进一步提升性能。

Abstract: Vision Foundation Models (VFMs) have become a de facto choice for many
downstream vision tasks, like image classification, image segmentation, and
object localization. However, they can also provide significant utility for
downstream 3D tasks that can leverage the cross-modal information (e.g., from
paired image data). In our work, we further explore the utility of VFMs for
adapting from a labeled source to unlabeled target data for the task of
LiDAR-based 3D semantic segmentation. Our method consumes paired 2D-3D (image
and point cloud) data and relies on the robust (cross-domain) features from a
VFM to train a 3D backbone on a mix of labeled source and unlabeled target
data. At the heart of our method lies a fusion network that is guided by both
the image and point cloud streams, with their relative contributions adjusted
based on the target domain. We extensively compare our proposed methodology
with different state-of-the-art methods in several settings and achieve strong
performance gains. For example, achieving an average improvement of 6.5 mIoU
(over all tasks), when compared with the previous state-of-the-art.

</details>


### [91] [Single Document Image Highlight Removal via A Large-Scale Real-World Dataset and A Location-Aware Network](https://arxiv.org/abs/2504.14238)
*Lu Pan,Yu-Hsuan Huang,Hongxia Xie,Cheng Zhang,Hongwei Zhao,Hong-Han Shuai,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: 论文提出DocHR14K数据集和L2HRNet方法，用于解决文档图像中的高光问题，显著提升了高光去除效果。


<details>
  <summary>Details</summary>
Motivation: 文档图像在环境光下常出现高光，影响文本可读性和视觉质量，现有深度学习方法因缺乏专用数据集和针对性设计而效果不佳。

Method: 提出DocHR14K数据集，包含14,902对高分辨率图像；设计Highlight Location Prior (HLP)自动估计高光区域；提出L2HRNet网络，结合先验和扩散模块去除高光。

Result: DocHR14K提升了高光去除效果；L2HRNet在三个基准数据集上达到最优性能，PSNR提升5.01%，RMSE降低13.17%。

Conclusion: DocHR14K和L2HRNet有效解决了文档高光问题，为实际应用提供了可靠方案。

Abstract: Reflective documents often suffer from specular highlights under ambient
lighting, severely hindering text readability and degrading overall visual
quality. Although recent deep learning methods show promise in highlight
removal, they remain suboptimal for document images, primarily due to the lack
of dedicated datasets and tailored architectural designs. To tackle these
challenges, we present DocHR14K, a large-scale real-world dataset comprising
14,902 high-resolution image pairs across six document categories and various
lighting conditions. To the best of our knowledge, this is the first
high-resolution dataset for document highlight removal that captures a wide
range of real-world lighting conditions. Additionally, motivated by the
observation that the residual map between highlighted and clean images
naturally reveals the spatial structure of highlight regions, we propose a
simple yet effective Highlight Location Prior (HLP) to estimate highlight masks
without human annotations. Building on this prior, we present the
Location-Aware Laplacian Pyramid Highlight Removal Network (L2HRNet), which
effectively removes highlights by leveraging estimated priors and incorporates
diffusion module to restore details. Extensive experiments demonstrate that
DocHR14K improves highlight removal under diverse lighting conditions. Our
L2HRNet achieves state-of-the-art performance across three benchmark datasets,
including a 5.01\% increase in PSNR and a 13.17\% reduction in RMSE on
DocHR14K.

</details>


### [92] [ROI-Guided Point Cloud Geometry Compression Towards Human and Machine Vision](https://arxiv.org/abs/2504.14240)
*Xie Liang,Gao Wei,Zhenghui Ming,Li Ge*

Main category: cs.CV

TL;DR: 提出了一种基于ROI引导的点云几何压缩方法（RPCGC），通过双分支并行结构优化压缩性能，同时提升下游任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 点云数据在自动驾驶等领域应用广泛，但大容量带来存储和传输挑战，现有高压缩比方法易损害语义细节，影响下游任务精度。

Method: 采用双分支并行结构：基础层编码简化点云，增强层聚焦几何细节；通过ROI预测网络生成掩码信息优化残差，并在RD优化中加权计算。

Result: 在ScanNet和SUN RGB-D数据集上，RPCGC在高比特率下压缩性能优异，检测精度比同类方法提升10%。

Conclusion: RPCGC通过ROI引导和双分支结构，实现了高效压缩与下游任务精度的平衡，为点云压缩提供了新思路。

Abstract: Point cloud data is pivotal in applications like autonomous driving, virtual
reality, and robotics. However, its substantial volume poses significant
challenges in storage and transmission. In order to obtain a high compression
ratio, crucial semantic details usually confront severe damage, leading to
difficulties in guaranteeing the accuracy of downstream tasks. To tackle this
problem, we are the first to introduce a novel Region of Interest (ROI)-guided
Point Cloud Geometry Compression (RPCGC) method for human and machine vision.
Our framework employs a dual-branch parallel structure, where the base layer
encodes and decodes a simplified version of the point cloud, and the
enhancement layer refines this by focusing on geometry details. Furthermore,
the residual information of the enhancement layer undergoes refinement through
an ROI prediction network. This network generates mask information, which is
then incorporated into the residuals, serving as a strong supervision signal.
Additionally, we intricately apply these mask details in the Rate-Distortion
(RD) optimization process, with each point weighted in the distortion
calculation. Our loss function includes RD loss and detection loss to better
guide point cloud encoding for the machine. Experiment results demonstrate that
RPCGC achieves exceptional compression performance and better detection
accuracy (10% gain) than some learning-based compression methods at high
bitrates in ScanNet and SUN RGB-D datasets.

</details>


### [93] [Towards Explainable Fake Image Detection with Multi-Modal Large Language Models](https://arxiv.org/abs/2504.14245)
*Yikun Ji,Yan Hong,Jiahui Zhan,Haoxing Chen,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 论文探讨了基于多模态大语言模型（MLLMs）的AI生成图像检测方法，强调其泛化性和透明性，并与传统方法及人工评估对比。


<details>
  <summary>Details</summary>
Motivation: 解决图像生成技术带来的公共安全问题，提出需要透明且泛化性强的假图像检测方法。

Method: 设计六种不同的提示词，并整合这些提示词构建一个更鲁棒、可解释的推理驱动检测系统。

Result: 评估了MLLMs与传统方法及人工评估的对比，展示了其优势和局限性。

Conclusion: 提出了一种基于MLLMs的推理驱动检测框架，为假图像检测提供了新思路。

Abstract: Progress in image generation raises significant public security concerns. We
argue that fake image detection should not operate as a "black box". Instead,
an ideal approach must ensure both strong generalization and transparency.
Recent progress in Multi-modal Large Language Models (MLLMs) offers new
opportunities for reasoning-based AI-generated image detection. In this work,
we evaluate the capabilities of MLLMs in comparison to traditional detection
methods and human evaluators, highlighting their strengths and limitations.
Furthermore, we design six distinct prompts and propose a framework that
integrates these prompts to develop a more robust, explainable, and
reasoning-driven detection system. The code is available at
https://github.com/Gennadiyev/mllm-defake.

</details>


### [94] [Any Image Restoration via Efficient Spatial-Frequency Degradation Adaptation](https://arxiv.org/abs/2504.14249)
*Bin Ren,Eduard Zamfir,Zongwei Wu,Yawei Li,Yidi Li,Danda Pani Paudel,Radu Timofte,Ming-Hsuan Yang,Luc Van Gool,Nicu Sebe*

Main category: cs.CV

TL;DR: AnyIR提出了一种统一的图像恢复方法，通过联合嵌入机制高效处理多种退化问题，无需增加模型规模或依赖大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要为每种退化训练专用模型的低效问题，以及现有方法增加模型复杂性或依赖大型语言模型的不足。

Method: 利用退化间的相似性，通过门控机制重新加权输入的子潜在空间关键组件，并提出空间-频率并行融合策略增强恢复细节。

Result: 在统一恢复设置下，AnyIR实现了SOTA性能，模型参数和FLOPs分别减少了82%和85%。

Conclusion: AnyIR提供了一种高效且全面的图像恢复解决方案，显著降低了模型复杂性和计算成本。

Abstract: Restoring any degraded image efficiently via just one model has become
increasingly significant and impactful, especially with the proliferation of
mobile devices. Traditional solutions typically involve training dedicated
models per degradation, resulting in inefficiency and redundancy. More recent
approaches either introduce additional modules to learn visual prompts,
significantly increasing model size, or incorporate cross-modal transfer from
large language models trained on vast datasets, adding complexity to the system
architecture. In contrast, our approach, termed AnyIR, takes a unified path
that leverages inherent similarity across various degradations to enable both
efficient and comprehensive restoration through a joint embedding mechanism,
without scaling up the model or relying on large language models.Specifically,
we examine the sub-latent space of each input, identifying key components and
reweighting them first in a gated manner. To fuse the intrinsic degradation
awareness and the contextualized attention, a spatial-frequency parallel fusion
strategy is proposed for enhancing spatial-aware local-global interactions and
enriching the restoration details from the frequency perspective. Extensive
benchmarking in the all-in-one restoration setting confirms AnyIR's SOTA
performance, reducing model complexity by around 82\% in parameters and 85\% in
FLOPs. Our code will be available at our Project page
(https://amazingren.github.io/AnyIR/)

</details>


### [95] [ColorVein: Colorful Cancelable Vein Biometrics](https://arxiv.org/abs/2504.14253)
*Yifan Wang,Jie Gui,Xinli Shi,Linqing Gui,Yuan Yan Tang,James Tin-Yau Kwok*

Main category: cs.CV

TL;DR: 本文提出了一种名为ColorVein的可取消静脉生物特征生成方案，通过引入颜色信息增强图像密度，并优化特征提取模型，显著提升了安全性和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门针对静脉生物特征的可取消模板生成方案，且生物信息泄露可能威胁用户隐私。

Method: ColorVein通过交互式着色将静态灰度信息转换为动态可控的颜色表示，并引入安全中心损失优化特征提取模型。

Result: ColorVein在识别性能、不可链接性、不可逆性和可撤销性方面表现优异，与现有方法相比具有竞争力。

Conclusion: ColorVein为静脉生物特征提供了一种安全、高效的可取消模板生成方案，显著提升了隐私保护能力。

Abstract: Vein recognition technologies have become one of the primary solutions for
high-security identification systems. However, the issue of biometric
information leakage can still pose a serious threat to user privacy and
anonymity. Currently, there is no cancelable biometric template generation
scheme specifically designed for vein biometrics. Therefore, this paper
proposes an innovative cancelable vein biometric generation scheme: ColorVein.
Unlike previous cancelable template generation schemes, ColorVein does not
destroy the original biometric features and introduces additional color
information to grayscale vein images. This method significantly enhances the
information density of vein images by transforming static grayscale information
into dynamically controllable color representations through interactive
colorization. ColorVein allows users/administrators to define a controllable
pseudo-random color space for grayscale vein images by editing the position,
number, and color of hint points, thereby generating protected cancelable
templates. Additionally, we propose a new secure center loss to optimize the
training process of the protected feature extraction model, effectively
increasing the feature distance between enrolled users and any potential
impostors. Finally, we evaluate ColorVein's performance on all types of vein
biometrics, including recognition performance, unlinkability, irreversibility,
and revocability, and conduct security and privacy analyses. ColorVein achieves
competitive performance compared with state-of-the-art methods.

</details>


### [96] [Visual Consensus Prompting for Co-Salient Object Detection](https://arxiv.org/abs/2504.14254)
*Jie Wang,Nana Yu,Zihao Zhang,Yahong Han*

Main category: cs.CV

TL;DR: 提出了一种参数高效的视觉共识提示（VCP）架构，解决了现有CoSOD方法在共识提取和参数效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有CoSOD方法依赖编码特征提取共识，但共识未及时指导编码阶段，且全局参数更新效率低。

Method: 引入参数高效的提示调优范式，通过共识提示生成器（CPG）和共识提示分散器（CPD）生成任务特定的视觉共识提示。

Result: 在最具挑战性的CoCA数据集上，F_m指标提升6.8%，优于13种前沿全微调模型。

Conclusion: VCP架构通过参数高效的方式显著提升了CoSOD任务的性能。

Abstract: Existing co-salient object detection (CoSOD) methods generally employ a
three-stage architecture (i.e., encoding, consensus extraction & dispersion,
and prediction) along with a typical full fine-tuning paradigm. Although they
yield certain benefits, they exhibit two notable limitations: 1) This
architecture relies on encoded features to facilitate consensus extraction, but
the meticulously extracted consensus does not provide timely guidance to the
encoding stage. 2) This paradigm involves globally updating all parameters of
the model, which is parameter-inefficient and hinders the effective
representation of knowledge within the foundation model for this task.
Therefore, in this paper, we propose an interaction-effective and
parameter-efficient concise architecture for the CoSOD task, addressing two key
limitations. It introduces, for the first time, a parameter-efficient prompt
tuning paradigm and seamlessly embeds consensus into the prompts to formulate
task-specific Visual Consensus Prompts (VCP). Our VCP aims to induce the frozen
foundation model to perform better on CoSOD tasks by formulating task-specific
visual consensus prompts with minimized tunable parameters. Concretely, the
primary insight of the purposeful Consensus Prompt Generator (CPG) is to
enforce limited tunable parameters to focus on co-salient representations and
generate consensus prompts. The formulated Consensus Prompt Disperser (CPD)
leverages consensus prompts to form task-specific visual consensus prompts,
thereby arousing the powerful potential of pre-trained models in addressing
CoSOD tasks. Extensive experiments demonstrate that our concise VCP outperforms
13 cutting-edge full fine-tuning models, achieving the new state of the art
(with 6.8% improvement in F_m metrics on the most challenging CoCA dataset).
Source code has been available at https://github.com/WJ-CV/VCP.

</details>


### [97] [Cross-attention for State-based model RWKV-7](https://arxiv.org/abs/2504.14260)
*Liu Xiao,Li Zhiyuan,Lin Yueyu*

Main category: cs.CV

TL;DR: CrossWKV是一种新型的跨注意力机制，用于增强RWKV-7模型在文本到图像生成中的表达能力，通过低秩适应和向量门控实现跨模态对齐，性能达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 提升RWKV-7模型在跨模态任务（如文本到图像生成）中的表达能力，同时保持线性复杂性和低内存占用。

Method: 采用广义Delta规则、向量门控和低秩适应（LoRA），结合非对角输入依赖的转移矩阵，实现复杂函数建模。

Result: 在DIR-7框架下，CrossWKV在ImageNet 256x256上取得FID 2.88和CLIP 0.33，性能与SOTA相当。

Conclusion: CrossWKV凭借其表达能力和线性扩展性，成为跨模态任务的高效解决方案，适用于高分辨率生成和动态状态操作。

Abstract: We introduce CrossWKV, a novel cross-attention mechanism for the state-based
RWKV-7 model, designed to enhance the expressive power of text-to-image
generation. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV)
architecture, CrossWKV integrates text and image modalities in a single pass,
utilizing a generalized delta rule with vector-valued gating and low-rank
adaptations (LoRA) to achieve superior cross-modal alignment. Unlike
Transformer-based models, CrossWKV's non-diagonal, input-dependent transition
matrix enables it to represent complex functions beyond the $\mathrm{TC}^0$
complexity class, including all regular languages, as demonstrated by its
ability to perform state-tracking tasks like $S_5$ permutation modeling.
Evaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B
and ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and
a CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance
while offering robust generalization across diverse prompts. The model's
enhanced expressivity, combined with constant memory usage and linear scaling,
positions it as a powerful solution for advanced cross-modal tasks, with
potential applications in high-resolution generation and dynamic state
manipulation.Code at https://github.com/TorchRWKV/flash-linear-attention

</details>


### [98] [Text-Audio-Visual-conditioned Diffusion Model for Video Saliency Prediction](https://arxiv.org/abs/2504.14267)
*Li Yu,Xuanzhe Sun,Wei Zhou,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本-音频-视觉条件的扩散模型（TAVDiff）用于视频显著性预测，通过多模态信息融合提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 视频显著性预测在视频压缩和人机交互等应用中至关重要。多模态学习的发展促使研究者探索音频-视觉和文本-视觉的显著性预测方法，以利用互补信息提高准确性。

Method: TAVDiff将视频显著性预测视为基于文本、音频和视觉输入的图像生成任务，通过逐步去噪预测显著性图。使用大型多模态模型生成文本描述，并引入显著性导向的图像-文本响应（SITR）机制。同时，提出Saliency-DiT以解耦条件信息与时间步。

Result: 实验结果表明，TAVDiff在SIM、CC、NSS和AUC-J指标上分别提升了1.03%、2.35%、2.71%和0.33%，优于现有方法。

Conclusion: TAVDiff通过多模态信息融合和解耦条件信息的方法，显著提升了视频显著性预测的性能。

Abstract: Video saliency prediction is crucial for downstream applications, such as
video compression and human-computer interaction. With the flourishing of
multimodal learning, researchers started to explore multimodal video saliency
prediction, including audio-visual and text-visual approaches. Auditory cues
guide the gaze of viewers to sound sources, while textual cues provide semantic
guidance for understanding video content. Integrating these complementary cues
can improve the accuracy of saliency prediction. Therefore, we attempt to
simultaneously analyze visual, auditory, and textual modalities in this paper,
and propose TAVDiff, a Text-Audio-Visual-conditioned Diffusion Model for video
saliency prediction. TAVDiff treats video saliency prediction as an image
generation task conditioned on textual, audio, and visual inputs, and predicts
saliency maps through stepwise denoising. To effectively utilize text, a large
multimodal model is used to generate textual descriptions for video frames and
introduce a saliency-oriented image-text response (SITR) mechanism to generate
image-text response maps. It is used as conditional information to guide the
model to localize the visual regions that are semantically related to the
textual description. Regarding the auditory modality, it is used as another
conditional information for directing the model to focus on salient regions
indicated by sounds. At the same time, since the diffusion transformer (DiT)
directly concatenates the conditional information with the timestep, which may
affect the estimation of the noise level. To achieve effective conditional
guidance, we propose Saliency-DiT, which decouples the conditional information
from the timestep. Experimental results show that TAVDiff outperforms existing
methods, improving 1.03\%, 2.35\%, 2.71\% and 0.33\% on SIM, CC, NSS and AUC-J
metrics, respectively.

</details>


### [99] [RAMCT: Novel Region-adaptive Multi-channel Tracker with Iterative Tikhonov Regularization for Thermal Infrared Tracking](https://arxiv.org/abs/2504.14278)
*Shang Zhang,Yuke Hou,Guoqiang Gong,Ruoyan Xiong,Yue Zhang*

Main category: cs.CV

TL;DR: RAMCT是一种区域自适应的稀疏相关滤波器跟踪器，通过多通道特征优化和自适应正则化策略，解决了热红外目标跟踪中的低分辨率、遮挡和背景干扰等问题。


<details>
  <summary>Details</summary>
Motivation: 现有相关滤波器跟踪器在热红外目标跟踪中面临低分辨率、遮挡、背景干扰和目标变形等挑战，影响跟踪性能。

Method: 1. 引入空间自适应二值掩码，增强目标区域稀疏性并抑制背景干扰；2. 提出基于GSVD的区域自适应迭代Tikhonov正则化方法；3. 设计动态参数调整的在线优化策略。

Result: 在多个基准测试中，RAMCT在准确性和鲁棒性上优于其他先进跟踪器。

Conclusion: RAMCT通过区域自适应和动态优化策略，显著提升了热红外目标跟踪的性能。

Abstract: Correlation filter (CF)-based trackers have gained significant attention for
their computational efficiency in thermal infrared (TIR) target tracking.
However, ex-isting methods struggle with challenges such as low-resolution
imagery, occlu-sion, background clutter, and target deformation, which severely
impact tracking performance. To overcome these limitations, we propose RAMCT, a
region-adaptive sparse correlation filter tracker that integrates multi-channel
feature opti-mization with an adaptive regularization strategy. Firstly, we
refine the CF learn-ing process by introducing a spatially adaptive binary
mask, which enforces spar-sity in the target region while dynamically
suppressing background interference. Secondly, we introduce generalized
singular value decomposition (GSVD) and propose a novel GSVD-based
region-adaptive iterative Tikhonov regularization method. This enables flexible
and robust optimization across multiple feature channels, improving resilience
to occlusion and background variations. Thirdly, we propose an online
optimization strategy with dynamic discrepancy-based pa-rameter adjustment.
This mechanism facilitates real time adaptation to target and background
variations, thereby improving tracking accuracy and robustness. Ex-tensive
experiments on LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR2017 benchmarks
demonstrate that RAMCT outperforms other state-of-the-art trackers in terms of
accuracy and robustness.

</details>


### [100] [CLIP-Powered Domain Generalization and Domain Adaptation: A Comprehensive Survey](https://arxiv.org/abs/2504.14280)
*Jindong Li,Yongguang Li,Yali Fu,Jiahong Liu,Yixin Liu,Menglin Yang,Irwin King*

Main category: cs.CV

TL;DR: 本文综述了CLIP在领域泛化（DG）和领域适应（DA）中的应用，填补了现有文献的空白，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏对CLIP在DG和DA中应用的系统性综述，本文旨在填补这一空白，为研究者和实践者提供参考。

Method: 在DG中，方法分为优化提示学习和利用CLIP作为特征提取器；在DA中，分为基于源数据的方法和无源数据的方法。

Result: 综述总结了CLIP在DG和DA中的关键应用，并指出了过拟合、领域多样性和计算效率等挑战。

Conclusion: 本文为未来研究提供了方向，旨在推动更鲁棒的机器学习模型的发展。

Abstract: As machine learning evolves, domain generalization (DG) and domain adaptation
(DA) have become crucial for enhancing model robustness across diverse
environments. Contrastive Language-Image Pretraining (CLIP) plays a significant
role in these tasks, offering powerful zero-shot capabilities that allow models
to perform effectively in unseen domains. However, there remains a significant
gap in the literature, as no comprehensive survey currently exists that
systematically explores the applications of CLIP in DG and DA, highlighting the
necessity for this review. This survey presents a comprehensive review of
CLIP's applications in DG and DA. In DG, we categorize methods into optimizing
prompt learning for task alignment and leveraging CLIP as a backbone for
effective feature extraction, both enhancing model adaptability. For DA, we
examine both source-available methods utilizing labeled source data and
source-free approaches primarily based on target domain data, emphasizing
knowledge transfer mechanisms and strategies for improved performance across
diverse contexts. Key challenges, including overfitting, domain diversity, and
computational efficiency, are addressed, alongside future research
opportunities to advance robustness and efficiency in practical applications.
By synthesizing existing literature and pinpointing critical gaps, this survey
provides valuable insights for researchers and practitioners, proposing
directions for effectively leveraging CLIP to enhance methodologies in domain
generalization and adaptation. Ultimately, this work aims to foster innovation
and collaboration in the quest for more resilient machine learning models that
can perform reliably across diverse real-world scenarios. A more up-to-date
version of the papers is maintained at:
https://github.com/jindongli-Ai/Survey_on_CLIP-Powered_Domain_Generalization_and_Adaptation.

</details>


### [101] [ISTD-YOLO: A Multi-Scale Lightweight High-Performance Infrared Small Target Detection Algorithm](https://arxiv.org/abs/2504.14289)
*Shang Zhang,Yujie Cui,Ruoyan Xiong,Huanbin Zhang*

Main category: cs.CV

TL;DR: ISTD-YOLO是一种基于改进YOLOv7的轻量级红外小目标检测算法，通过轻量化重构、引入无参数注意力机制和优化指标，显著提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 针对红外图像中复杂背景、低信噪比、小目标尺寸和弱亮度等检测难点，提出一种高效解决方案。

Method: 1. 轻量化重构YOLOv7网络结构；2. 用VoV-GSCSP替换ELAN-W模块；3. 引入无参数注意力机制；4. 使用NWD优化IoU指标。

Result: 实验表明，ISTD-YOLO在检测效果和各项指标上均优于YOLOv7及其他主流算法。

Conclusion: ISTD-YOLO能有效实现红外小目标的高质量检测。

Abstract: Aiming at the detection difficulties of infrared images such as complex
background, low signal-to-noise ratio, small target size and weak brightness, a
lightweight infrared small target detection algorithm ISTD-YOLO based on
improved YOLOv7 was proposed. Firstly, the YOLOv7 network structure was
lightweight reconstructed, and a three-scale lightweight network architecture
was designed. Then, the ELAN-W module of the model neck network is replaced by
VoV-GSCSP to reduce the computational cost and the complexity of the network
structure. Secondly, a parameter-free attention mechanism was introduced into
the neck network to enhance the relevance of local con-text information.
Finally, the Normalized Wasserstein Distance (NWD) was used to optimize the
commonly used IoU index to enhance the localization and detection accuracy of
small targets. Experimental results show that compared with YOLOv7 and the
current mainstream algorithms, ISTD-YOLO can effectively improve the detection
effect, and all indicators are effectively improved, which can achieve
high-quality detection of infrared small targets.

</details>


### [102] [Towards NSFW-Free Text-to-Image Generation via Safety-Constraint Direct Preference Optimization](https://arxiv.org/abs/2504.14290)
*Shouwei Ruan,Zhenyu Wu,Yao Huang,Ruochen Zhang,Yitong Sun,Caixin Kang,Xingxing Wei*

Main category: cs.CV

TL;DR: SC-DPO是一种新的文本到图像生成安全对齐框架，通过安全约束和动态聚焦机制，平衡安全性与生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法无法完全保证安全或平衡安全与生成质量的问题。

Method: 提出SC-DPO框架，结合安全成本模型和对比学习目标，构建SCP-10K数据集，并引入动态聚焦机制。

Result: SC-DPO在防御有害内容的同时保持生成质量，且对对抗性提示具有鲁棒性。

Conclusion: SC-DPO有效解决了文本到图像生成的安全对齐问题，优于现有方法。

Abstract: Ensuring the safety of generated content remains a fundamental challenge for
Text-to-Image (T2I) generation. Existing studies either fail to guarantee
complete safety under potentially harmful concepts or struggle to balance
safety with generation quality. To address these issues, we propose
Safety-Constrained Direct Preference Optimization (SC-DPO), a novel framework
for safety alignment in T2I models. SC-DPO integrates safety constraints into
the general human preference calibration, aiming to maximize the likelihood of
generating human-preferred samples while minimizing the safety cost of the
generated outputs. In SC-DPO, we introduce a safety cost model to accurately
quantify harmful levels for images, and train it effectively using the proposed
contrastive learning and cost anchoring objectives. To apply SC-DPO for
effective T2I safety alignment, we constructed SCP-10K, a safety-constrained
preference dataset containing rich harmful concepts, which blends
safety-constrained preference pairs under both harmful and clean instructions,
further mitigating the trade-off between safety and sample quality.
Additionally, we propose a Dynamic Focusing Mechanism (DFM) for SC-DPO,
promoting the model's learning of difficult preference pair samples. Extensive
experiments demonstrate that SC-DPO outperforms existing methods, effectively
defending against various NSFW content while maintaining optimal sample quality
and human preference alignment. Additionally, SC-DPO exhibits resilience
against adversarial prompts designed to generate harmful content.

</details>


### [103] [From Missing Pieces to Masterpieces: Image Completion with Context-Adaptive Diffusion](https://arxiv.org/abs/2504.14294)
*Pourya Shamsolmoali,Masoumeh Zareapoor,Huiyu Zhou,Michael Felsberg,Dacheng Tao,Xuelong Li*

Main category: cs.CV

TL;DR: ConFill提出了一种新的图像补全框架，通过上下文自适应差异模型（CAD）和动态采样机制，显著提升了生成内容与原始图像的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像补全中难以保持已知与未知区域的连贯性，缺乏显式的空间和语义对齐。

Method: 引入CAD模型以对齐扩散过程中的中间分布，并采用动态采样机制在复杂区域增加采样率。

Result: 实验表明ConFill在图像补全任务中优于现有方法，设定了新的基准。

Conclusion: ConFill通过上下文对齐和动态采样，显著提升了图像补全的质量和一致性。

Abstract: Image completion is a challenging task, particularly when ensuring that
generated content seamlessly integrates with existing parts of an image. While
recent diffusion models have shown promise, they often struggle with
maintaining coherence between known and unknown (missing) regions. This issue
arises from the lack of explicit spatial and semantic alignment during the
diffusion process, resulting in content that does not smoothly integrate with
the original image. Additionally, diffusion models typically rely on global
learned distributions rather than localized features, leading to
inconsistencies between the generated and existing image parts. In this work,
we propose ConFill, a novel framework that introduces a Context-Adaptive
Discrepancy (CAD) model to ensure that intermediate distributions of known and
unknown regions are closely aligned throughout the diffusion process. By
incorporating CAD, our model progressively reduces discrepancies between
generated and original images at each diffusion step, leading to contextually
aligned completion. Moreover, ConFill uses a new Dynamic Sampling mechanism
that adaptively increases the sampling rate in regions with high reconstruction
complexity. This approach enables precise adjustments, enhancing detail and
integration in restored areas. Extensive experiments demonstrate that ConFill
outperforms current methods, setting a new benchmark in image completion.

</details>


### [104] [Balancing Privacy and Action Performance: A Penalty-Driven Approach to Image Anonymization](https://arxiv.org/abs/2504.14301)
*Nazia Aslam,Kamal Nasrollahi*

Main category: cs.CV

TL;DR: 提出了一种隐私保护的图像匿名化技术，通过优化匿名化器以平衡隐私泄漏和动作识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决视频监控系统中视觉隐私与动作识别性能之间的权衡问题，同时符合欧盟AI法案和GDPR标准。

Method: 采用基于特征的惩罚方案，优化匿名化器以减少隐私泄漏并保持动作识别性能。

Result: 实验表明，该方法在保持隐私泄漏几乎不变的同时，显著提升了动作识别性能。

Conclusion: 该方法有效解决了隐私与性能的权衡问题，为隐私保护提供了新思路。

Abstract: The rapid development of video surveillance systems for object detection,
tracking, activity recognition, and anomaly detection has revolutionized our
day-to-day lives while setting alarms for privacy concerns. It isn't easy to
strike a balance between visual privacy and action recognition performance in
most computer vision models. Is it possible to safeguard privacy without
sacrificing performance? It poses a formidable challenge, as even minor privacy
enhancements can lead to substantial performance degradation. To address this
challenge, we propose a privacy-preserving image anonymization technique that
optimizes the anonymizer using penalties from the utility branch, ensuring
improved action recognition performance while minimally affecting privacy
leakage. This approach addresses the trade-off between minimizing privacy
leakage and maintaining high action performance. The proposed approach is
primarily designed to align with the regulatory standards of the EU AI Act and
GDPR, ensuring the protection of personally identifiable information while
maintaining action performance. To the best of our knowledge, we are the first
to introduce a feature-based penalty scheme that exclusively controls the
action features, allowing freedom to anonymize private attributes. Extensive
experiments were conducted to validate the effectiveness of the proposed
method. The results demonstrate that applying a penalty to anonymizer from
utility branch enhances action performance while maintaining nearly consistent
privacy leakage across different penalty settings.

</details>


### [105] [Exploring Generalizable Pre-training for Real-world Change Detection via Geometric Estimation](https://arxiv.org/abs/2504.14306)
*Yitao Zhao,Sen Lei,Nanqing Liu,Heng-Chao Li,Turgay Celik,Qing Zhu*

Main category: cs.CV

TL;DR: MatchCD是一个自监督驱动的变化检测框架，通过几何估计解决多时相图像未对齐问题，无需手动配准，可直接处理大尺度图像。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测算法需要手动配准多时相图像，增加了工作流程的复杂性。

Method: MatchCD利用自监督对比表示优化编码器，同时处理图像配准和变化检测问题，支持直接处理大尺度图像。

Result: 在几何畸变显著的复杂场景中表现出色。

Conclusion: MatchCD框架有效解决了未对齐和变化检测问题，简化了工作流程。

Abstract: As an essential procedure in earth observation system, change detection (CD)
aims to reveal the spatial-temporal evolution of the observation regions. A key
prerequisite for existing change detection algorithms is aligned geo-references
between multi-temporal images by fine-grained registration. However, in the
majority of real-world scenarios, a prior manual registration is required
between the original images, which significantly increases the complexity of
the CD workflow. In this paper, we proposed a self-supervision motivated CD
framework with geometric estimation, called "MatchCD". Specifically, the
proposed MatchCD framework utilizes the zero-shot capability to optimize the
encoder with self-supervised contrastive representation, which is reused in the
downstream image registration and change detection to simultaneously handle the
bi-temporal unalignment and object change issues. Moreover, unlike the
conventional change detection requiring segmenting the full-frame image into
small patches, our MatchCD framework can directly process the original
large-scale image (e.g., 6K*4K resolutions) with promising performance. The
performance in multiple complex scenarios with significant geometric distortion
demonstrates the effectiveness of our proposed framework.

</details>


### [106] [FGSGT: Saliency-Guided Siamese Network Tracker Based on Key Fine-Grained Feature Information for Thermal Infrared Target Tracking](https://arxiv.org/abs/2504.14309)
*Ruoyan Xiong,Huanbin Zhang,Shentao Wang,Hui He,Yuke Hou,Yue Zhang,Yujie Cui,Huipan Guan,Shang Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于显著性引导的Siamese网络跟踪器，通过细粒度特征并行学习模块和多层特征融合模块，解决了TIR图像特征提取的挑战，显著提升了跟踪精度。


<details>
  <summary>Details</summary>
Motivation: TIR图像通常缺乏细节特征且对比度低，传统特征提取模型难以捕捉目标特征，导致跟踪器易受干扰和漂移。

Method: 设计了细粒度特征并行学习模块、多层特征融合模块、Siamese残差细化块和显著性损失函数，以优化特征提取和预测。

Result: 在PTB-TIR、LSOTB-TIR和VOT-TIR基准测试中取得了最高精度和成功率。

Conclusion: 该方法通过显著性引导和细粒度特征提取，显著提升了TIR图像跟踪的鲁棒性和准确性。

Abstract: Thermal infrared (TIR) images typically lack detailed features and have low
contrast, making it challenging for conventional feature extraction models to
capture discriminative target characteristics. As a result, trackers are often
affected by interference from visually similar objects and are susceptible to
tracking drift. To address these challenges, we propose a novel saliency-guided
Siamese network tracker based on key fine-grained feature infor-mation. First,
we introduce a fine-grained feature parallel learning convolu-tional block with
a dual-stream architecture and convolutional kernels of varying sizes. This
design captures essential global features from shallow layers, enhances feature
diversity, and minimizes the loss of fine-grained in-formation typically
encountered in residual connections. In addition, we propose a multi-layer
fine-grained feature fusion module that uses bilinear matrix multiplication to
effectively integrate features across both deep and shallow layers. Next, we
introduce a Siamese residual refinement block that corrects saliency map
prediction errors using residual learning. Combined with deep supervision, this
mechanism progressively refines predictions, ap-plying supervision at each
recursive step to ensure consistent improvements in accuracy. Finally, we
present a saliency loss function to constrain the sali-ency predictions,
directing the network to focus on highly discriminative fi-ne-grained features.
Extensive experiment results demonstrate that the pro-posed tracker achieves
the highest precision and success rates on the PTB-TIR and LSOTB-TIR
benchmarks. It also achieves a top accuracy of 0.78 on the VOT-TIR 2015
benchmark and 0.75 on the VOT-TIR 2017 benchmark.

</details>


### [107] [DCFG: Diverse Cross-Channel Fine-Grained Feature Learning and Progressive Fusion Siamese Tracker for Thermal Infrared Target Tracking](https://arxiv.org/abs/2504.14311)
*Ruoyan Xiong,Yuke Hou,Princess Retor Torboh,Hui He,Huanbin Zhang,Yue Zhang,Yanpin Wang,Huipan Guan,Shang Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于跨通道细粒度特征学习和渐进融合的新型Siamese跟踪器，用于解决热红外跟踪中高判别性特征捕获的挑战。


<details>
  <summary>Details</summary>
Motivation: 热红外（TIR）跟踪中难以捕获高判别性特征，现有方法在细节和多样性方面表现不足。

Method: 设计了跨通道细粒度特征学习网络，结合掩码和抑制系数抑制主导特征，引入通道重排和均衡机制，以及分层组合单元。提出专用损失函数，促进通道正交性和多样性。

Result: 在VOT-TIR 2015和2017基准测试中分别达到0.81和0.78的准确率，在LSOTB-TIR和PTB-TIR基准测试中全面优于其他方法。

Conclusion: 该方法通过细粒度特征学习和渐进融合显著提升了热红外跟踪的性能，具有高效性和低计算复杂度。

Abstract: To address the challenge of capturing highly discriminative features in
ther-mal infrared (TIR) tracking, we propose a novel Siamese tracker based on
cross-channel fine-grained feature learning and progressive fusion. First, we
introduce a cross-channel fine-grained feature learning network that employs
masks and suppression coefficients to suppress dominant target features,
en-abling the tracker to capture more detailed and subtle information. The
net-work employs a channel rearrangement mechanism to enhance efficient
in-formation flow, coupled with channel equalization to reduce parameter count.
Additionally, we incorporate layer-by-layer combination units for ef-fective
feature extraction and fusion, thereby minimizing parameter redun-dancy and
computational complexity. The network further employs feature redirection and
channel shuffling strategies to better integrate fine-grained details. Second,
we propose a specialized cross-channel fine-grained loss function designed to
guide feature groups toward distinct discriminative re-gions of the target,
thus improving overall target representation. This loss function includes an
inter-channel loss term that promotes orthogonality be-tween channels,
maximizing feature diversity and facilitating finer detail capture. Extensive
experiments demonstrate that our proposed tracker achieves the highest
accuracy, scoring 0.81 on the VOT-TIR 2015 and 0.78 on the VOT-TIR 2017
benchmark, while also outperforming other methods across all evaluation metrics
on the LSOTB-TIR and PTB-TIR benchmarks.

</details>


### [108] [Visual Prompting for One-shot Controllable Video Editing without Inversion](https://arxiv.org/abs/2504.14335)
*Zhengbo Zhang,Yuxi Zhou,Duo Peng,Joo-Hwee Lim,Zhigang Tu,De Wen Soh,Lin Geng Foo*

Main category: cs.CV

TL;DR: 论文提出了一种无需DDIM反转的单次可控视频编辑方法，通过视觉提示和内容一致性采样确保编辑帧与源帧的一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因DDIM反转误差导致的内容一致性问题，提升视频编辑质量。

Method: 采用视觉提示视角，结合内容一致性采样（CCS）和时间一致性采样（TCS）确保一致性和时序稳定性。

Result: 实验验证了方法的有效性，显著提升了编辑帧与源帧的一致性。

Conclusion: 提出的方法通过创新的一致性采样技术，解决了视频编辑中的关键挑战。

Abstract: One-shot controllable video editing (OCVE) is an important yet challenging
task, aiming to propagate user edits that are made -- using any image editing
tool -- on the first frame of a video to all subsequent frames, while ensuring
content consistency between edited frames and source frames. To achieve this,
prior methods employ DDIM inversion to transform source frames into latent
noise, which is then fed into a pre-trained diffusion model, conditioned on the
user-edited first frame, to generate the edited video. However, the DDIM
inversion process accumulates errors, which hinder the latent noise from
accurately reconstructing the source frames, ultimately compromising content
consistency in the generated edited frames. To overcome it, our method
eliminates the need for DDIM inversion by performing OCVE through a novel
perspective based on visual prompting. Furthermore, inspired by consistency
models that can perform multi-step consistency sampling to generate a sequence
of content-consistent images, we propose a content consistency sampling (CCS)
to ensure content consistency between the generated edited frames and the
source frames. Moreover, we introduce a temporal-content consistency sampling
(TCS) based on Stein Variational Gradient Descent to ensure temporal
consistency across the edited frames. Extensive experiments validate the
effectiveness of our approach.

</details>


### [109] [Multispectral airborne laser scanning for tree species classification: a benchmark of machine learning and deep learning algorithms](https://arxiv.org/abs/2504.14337)
*Josef Taher,Eric Hyyppä,Matti Hyyppä,Klaara Salolahti,Xiaowei Yu,Leena Matikainen,Antero Kukko,Matti Lehtomäki,Harri Kaartinen,Sopitta Thurachen,Paula Litkey,Ville Luoma,Markus Holopainen,Gefei Kong,Hongchao Fan,Petri Rönnholm,Antti Polvivaara,Samuli Junttila,Mikko Vastaranta,Stefano Puliti,Rasmus Astrup,Joel Kostensalo,Mari Myllymäki,Maksymilian Kulicki,Krzysztof Stereńczak,Raul de Paula Pires,Ruben Valbuena,Juan Pedro Carbonell-Rivera,Jesús Torralba,Yi-Chen Chen,Lukas Winiwarter,Markus Hollaus,Gottfried Mandlburger,Narges Takhtkeshha,Fabio Remondino,Maciej Lisiewicz,Bartłomiej Kraszewski,Xinlian Liang,Jianchang Chen,Eero Ahokas,Kirsi Karila,Eugeniu Vezeteu,Petri Manninen,Roope Näsi,Heikki Hyyti,Siiri Pyykkönen,Peilun Hu,Juha Hyyppä*

Main category: cs.CV

TL;DR: 该研究通过比较机器学习和深度学习方法，发现基于点的深度学习模型（如点变换器）在高密度多光谱ALS数据中表现最佳，分类准确率显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决森林资源管理中稀有树种识别和深度学习技术应用的挑战。

Method: 使用高密度多光谱ALS数据（HeliALS系统）和现有Titan数据，评估多种算法的分类准确性。

Result: 点变换器模型在高密度数据中表现最优，整体准确率达87.9%，显著优于传统方法和图像深度学习。

Conclusion: 多光谱信息显著提升分类性能，点变换器模型为森林资源管理提供了高效工具。

Abstract: Climate-smart and biodiversity-preserving forestry demands precise
information on forest resources, extending to the individual tree level.
Multispectral airborne laser scanning (ALS) has shown promise in automated
point cloud processing and tree segmentation, but challenges remain in
identifying rare tree species and leveraging deep learning techniques. This
study addresses these gaps by conducting a comprehensive benchmark of machine
learning and deep learning methods for tree species classification. For the
study, we collected high-density multispectral ALS data (>1000 pts/m$^2$) at
three wavelengths using the FGI-developed HeliALS system, complemented by
existing Optech Titan data (35 pts/m$^2$), to evaluate the species
classification accuracy of various algorithms in a test site located in
Southern Finland. Based on 5261 test segments, our findings demonstrate that
point-based deep learning methods, particularly a point transformer model,
outperformed traditional machine learning and image-based deep learning
approaches on high-density multispectral point clouds. For the high-density ALS
dataset, a point transformer model provided the best performance reaching an
overall (macro-average) accuracy of 87.9% (74.5%) with a training set of 1065
segments and 92.0% (85.1%) with 5000 training segments. The best image-based
deep learning method, DetailView, reached an overall (macro-average) accuracy
of 84.3% (63.9%), whereas a random forest (RF) classifier achieved an overall
(macro-average) accuracy of 83.2% (61.3%). Importantly, the overall
classification accuracy of the point transformer model on the HeliALS data
increased from 73.0% with no spectral information to 84.7% with single-channel
reflectance, and to 87.9% with spectral information of all the three channels.

</details>


### [110] [Manipulating Multimodal Agents via Cross-Modal Prompt Injection](https://arxiv.org/abs/2504.14348)
*Le Wang,Zonghao Ying,Tianyuan Zhang,Siyuan Liang,Shengshan Hu,Mingchuan Zhang,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: 论文提出了一种针对多模态代理的新型安全漏洞攻击方法CrossInject，通过跨模态提示注入攻击，成功率高且具有实际威胁。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的兴起带来了新的安全挑战，尤其是跨模态提示注入攻击这一被忽视的漏洞。

Method: 提出CrossInject框架，包括视觉潜在对齐和文本引导增强两部分，利用对抗性扰动和黑盒防御系统提示推断。

Result: 实验表明攻击成功率提升至少26.4%，并在真实多模态自主代理中验证了有效性。

Conclusion: 该攻击方法对安全关键应用具有潜在威胁，需引起重视。

Abstract: The emergence of multimodal large language models has redefined the agent
paradigm by integrating language and vision modalities with external data
sources, enabling agents to better interpret human instructions and execute
increasingly complex tasks. However, in this work, we identify a critical yet
previously overlooked security vulnerability in multimodal agents: cross-modal
prompt injection attacks. To exploit this vulnerability, we propose
CrossInject, a novel attack framework in which attackers embed adversarial
perturbations across multiple modalities to align with target malicious
content, allowing external instructions to hijack the agent's decision-making
process and execute unauthorized tasks. Our approach consists of two key
components. First, we introduce Visual Latent Alignment, where we optimize
adversarial features to the malicious instructions in the visual embedding
space based on a text-to-image generative model, ensuring that adversarial
images subtly encode cues for malicious task execution. Subsequently, we
present Textual Guidance Enhancement, where a large language model is leveraged
to infer the black-box defensive system prompt through adversarial meta
prompting and generate an malicious textual command that steers the agent's
output toward better compliance with attackers' requests. Extensive experiments
demonstrate that our method outperforms existing injection attacks, achieving
at least a +26.4% increase in attack success rates across diverse tasks.
Furthermore, we validate our attack's effectiveness in real-world multimodal
autonomous agents, highlighting its potential implications for safety-critical
applications.

</details>


### [111] [A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling](https://arxiv.org/abs/2504.14359)
*Kyle Buettner,Jacob Emmerson,Adriana Kovashka*

Main category: cs.CV

TL;DR: 论文提出了一种基于LLM的多模态重述策略，通过修改英文描述后再翻译，提升多语言视觉语言模型对感知多样性的理解，并在德日文本图像检索中取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的多语言能力主要依赖英语数据，导致感知偏见和模型灵活性不足，需解决这一问题。

Method: 采用LLM驱动的多模态重述策略，结合母语数据指导的目标机制，生成改写描述并用于训练增强。

Result: 在德日文本图像检索任务中，平均召回率提升3.5，非母语错误案例提升4.7。

Conclusion: 通过数据高效框架和跨语言分析机制，模型能更好地理解感知多样性，并实现跨数据集和跨语言的泛化。

Abstract: There are many ways to describe, name, and group objects when captioning an
image. Differences are evident when speakers come from diverse cultures due to
the unique experiences that shape perception. Machine translation of captions
has pushed multilingual capabilities in vision-language models (VLMs), but data
comes mainly from English speakers, indicating a perceptual bias and lack of
model flexibility. In this work, we address this challenge and outline a
data-efficient framework to instill multilingual VLMs with greater
understanding of perceptual diversity. We specifically propose an LLM-based,
multimodal recaptioning strategy that alters the object descriptions of English
captions before translation. The greatest benefits are demonstrated in a
targeted multimodal mechanism guided by native speaker data. By adding produced
rewrites as augmentations in training, we improve on German and Japanese
text-image retrieval cases studies (up to +3.5 mean recall overall, +4.7 on
non-native error cases). We further propose a mechanism to analyze the specific
object description differences across datasets, and we offer insights into
cross-dataset and cross-language generalization.

</details>


### [112] [Efficient Spiking Point Mamba for Point Cloud Analysis](https://arxiv.org/abs/2504.14371)
*Peixi Wu,Bosong Chai,Menghua Zheng,Wei Li,Zhangchi Hu,Jie Chen,Zheyu Zhang,Hebei Li,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出了一种基于Mamba的3D脉冲神经网络（SPM），结合了Mamba的序列建模能力和SNN的时序特征提取，显著提升了性能并降低了能耗。


<details>
  <summary>Details</summary>
Motivation: 现有的3D SNN在长程依赖建模上表现不佳，而Mamba的高效计算和序列建模能力为解决这一问题提供了可能。

Method: 设计了Hierarchical Dynamic Encoding（HDE）和Spiking Mamba Block（SMB），并采用不对称SNN-ANN架构进行预训练和微调。

Result: 在ScanObjectNN和ShapeNetPart数据集上性能显著提升，能耗比ANN低3.5倍以上。

Conclusion: SPM是首个基于Mamba的3D SNN，有效结合了Mamba和SNN的优势，实现了高性能和低能耗。

Abstract: Bio-inspired Spiking Neural Networks (SNNs) provide an energy-efficient way
to extract 3D spatio-temporal features. However, existing 3D SNNs have
struggled with long-range dependencies until the recent emergence of Mamba,
which offers superior computational efficiency and sequence modeling
capability. In this work, we propose Spiking Point Mamba (SPM), the first
Mamba-based SNN in the 3D domain. Due to the poor performance of simply
transferring Mamba to 3D SNNs, SPM is designed to utilize both the sequence
modeling capabilities of Mamba and the temporal feature extraction of SNNs.
Specifically, we first introduce Hierarchical Dynamic Encoding (HDE), an
improved direct encoding method that effectively introduces dynamic temporal
mechanism, thereby facilitating temporal interactions. Then, we propose a
Spiking Mamba Block (SMB), which builds upon Mamba while learning
inter-time-step features and minimizing information loss caused by spikes.
Finally, to further enhance model performance, we adopt an asymmetric SNN-ANN
architecture for spike-based pre-training and finetune. Compared with the
previous state-of-the-art SNN models, SPM improves OA by +6.2%, +6.1%, and
+7.4% on three variants of ScanObjectNN, and boosts instance mIOU by +1.9% on
ShapeNetPart. Meanwhile, its energy consumption is at least 3.5x lower than
that of its ANN counterpart. The code will be made publicly available.

</details>


### [113] [LOOPE: Learnable Optimal Patch Order in Positional Embeddings for Vision Transformers](https://arxiv.org/abs/2504.14386)
*Md Abtahi Majeed Chowdhury,Md Rifat Ur Rahman,Akil Ahmad Taki*

Main category: cs.CV

TL;DR: 论文提出了一种可学习的补丁排序方法LOOPE，用于优化视觉变换器（ViT）中的位置嵌入（PE），并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了补丁排序对位置嵌入的影响，导致2D网格到1D序列的映射存在挑战。

Method: 提出LOOPE方法，优化补丁排序以提升空间表示；引入“Three Cell Experiment”评估框架，量化PE的效果。

Result: LOOPE显著提升了分类准确性，实验显示PE的效果差异高达30-35%，远超传统评估的4-6%。

Conclusion: LOOPE在保留相对和绝对位置信息方面表现优异，为PE研究提供了更敏感的诊断工具。

Abstract: Positional embeddings (PE) play a crucial role in Vision Transformers (ViTs)
by providing spatial information otherwise lost due to the permutation
invariant nature of self attention. While absolute positional embeddings (APE)
have shown theoretical advantages over relative positional embeddings (RPE),
particularly due to the ability of sinusoidal functions to preserve spatial
inductive biases like monotonicity and shift invariance, a fundamental
challenge arises when mapping a 2D grid to a 1D sequence. Existing methods have
mostly overlooked or never explored the impact of patch ordering in positional
embeddings. To address this, we propose LOOPE, a learnable patch-ordering
method that optimizes spatial representation for a given set of frequencies,
providing a principled approach to patch order optimization. Empirical results
show that our PE significantly improves classification accuracy across various
ViT architectures. To rigorously evaluate the effectiveness of positional
embeddings, we introduce the "Three Cell Experiment", a novel benchmarking
framework that assesses the ability of PEs to retain relative and absolute
positional information across different ViT architectures. Unlike standard
evaluations, which typically report a performance gap of 4 to 6% between models
with and without PE, our method reveals a striking 30 to 35% difference,
offering a more sensitive diagnostic tool to measure the efficacy of PEs. Our
experimental analysis confirms that the proposed LOOPE demonstrates enhanced
effectiveness in retaining both relative and absolute positional information.

</details>


### [114] [How Well Can General Vision-Language Models Learn Medicine By Watching Public Educational Videos?](https://arxiv.org/abs/2504.14391)
*Rahul Thapa,Andrew Li,Qingyang Wu,Bryan He,Yuki Sahashi,Christina Binder,Angela Zhang,Ben Athiwaratkun,Shuaiwen Leon Song,David Ouyang,James Zou*

Main category: cs.CV

TL;DR: OpenBiomedVi数据集利用教育视频训练视觉语言模型，显著提升性能，并引入新基准测试验证效果。


<details>
  <summary>Details</summary>
Motivation: 探索非标准化、异构的教育视频是否能有效训练生物医学视觉语言模型。

Method: 构建OpenBiomedVi数据集（1031小时视频-字幕-Q/A对），并微调Qwen-2-VL模型。

Result: 模型性能显著提升，2B和7B模型在视频和图像任务上表现优异，文本任务略有波动。

Conclusion: 教育视频为生物医学视觉语言模型提供了有效的训练信号。

Abstract: Publicly available biomedical videos, such as those on YouTube, serve as
valuable educational resources for medical students. Unlike standard machine
learning datasets, these videos are designed for human learners, often mixing
medical imagery with narration, explanatory diagrams, and contextual framing.
In this work, we investigate whether such pedagogically rich, yet
non-standardized and heterogeneous videos can effectively teach general-domain
vision-language models biomedical knowledge. To this end, we introduce
OpenBiomedVi, a biomedical video instruction tuning dataset comprising 1031
hours of video-caption and Q/A pairs, curated through a multi-step
human-in-the-loop pipeline. Diverse biomedical video datasets are rare, and
OpenBiomedVid fills an important gap by providing instruction-style supervision
grounded in real-world educational content. Surprisingly, despite the informal
and heterogeneous nature of these videos, the fine-tuned Qwen-2-VL models
exhibit substantial performance improvements across most benchmarks. The 2B
model achieves gains of 98.7% on video tasks, 71.2% on image tasks, and 0.2% on
text tasks. The 7B model shows improvements of 37.09% on video and 11.2% on
image tasks, with a slight degradation of 2.7% on text tasks compared to their
respective base models. To address the lack of standardized biomedical video
evaluation datasets, we also introduce two new expert curated benchmarks,
MIMICEchoQA and SurgeryVideoQA. On these benchmarks, the 2B model achieves
gains of 99.1% and 98.1%, while the 7B model shows gains of 22.5% and 52.1%,
respectively, demonstrating the models' ability to generalize and perform
biomedical video understanding on cleaner and more standardized datasets than
those seen during training. These results suggest that educational videos
created for human learning offer a surprisingly effective training signal for
biomedical VLMs.

</details>


### [115] [Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models](https://arxiv.org/abs/2504.14395)
*Chung-En,Yu,Hsuan-Chih,Chen,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: Hydra是一个自适应代理框架，通过迭代推理和跨模型验证提升视觉语言模型的对抗鲁棒性和减少幻觉，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在高风险应用中面临对抗性攻击和幻觉问题，现有方法未能统一解决这两方面，Hydra旨在填补这一空白。

Method: Hydra采用Action-Critique Loop，结合Chain-of-Thought和In-Context Learning技术，动态优化输出，适应对抗性扰动和模型固有错误。

Result: 在四个视觉语言模型、三个幻觉基准和两种对抗攻击策略上，Hydra表现优于现有插件模型和去幻觉方法，无需显式对抗防御。

Conclusion: Hydra为视觉语言模型提供了一种无需训练的可扩展解决方案，显著提升了对抗鲁棒性和事实一致性。

Abstract: To develop trustworthy Vision-Language Models (VLMs), it is essential to
address adversarial robustness and hallucination mitigation, both of which
impact factual accuracy in high-stakes applications such as defense and
healthcare. Existing methods primarily focus on either adversarial defense or
hallucination post-hoc correction, leaving a gap in unified robustness
strategies. We introduce \textbf{Hydra}, an adaptive agentic framework that
enhances plug-in VLMs through iterative reasoning, structured critiques, and
cross-model verification, improving both resilience to adversarial
perturbations and intrinsic model errors. Hydra employs an Action-Critique
Loop, where it retrieves and critiques visual information, leveraging
Chain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine
outputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to
both adversarial manipulations and intrinsic model errors, making it robust to
malicious perturbations and hallucination-related inaccuracies. We evaluate
Hydra on four VLMs, three hallucination benchmarks, two adversarial attack
strategies, and two adversarial defense methods, assessing performance on both
clean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs
and state-of-the-art (SOTA) dehallucination methods, even without explicit
adversarial defenses, demonstrating enhanced robustness and factual
consistency. By bridging adversarial resistance and hallucination mitigation,
Hydra provides a scalable, training-free solution for improving the reliability
of VLMs in real-world applications.

</details>


### [116] [SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video Generation via Spherical Latent Representation](https://arxiv.org/abs/2504.14396)
*Minho Park,Taewoong Kang,Jooyeol Yun,Sungwon Hwang,Jaegul Choo*

Main category: cs.CV

TL;DR: SphereDiff提出了一种无需额外调优的扩散模型方法，用于生成无缝的360度全景图像和视频，解决了传统ERP投影的失真问题。


<details>
  <summary>Details</summary>
Motivation: AR/VR应用对高质量360度全景内容的需求增加，但现有方法因ERP投影的严重失真而受限。

Method: 定义了球形潜在表示，扩展MultiDiffusion至球形空间，并提出球形潜在采样方法和失真感知加权平均。

Result: SphereDiff在生成360度全景内容时优于现有方法，保持高保真度。

Conclusion: SphereDiff为AR/VR应用提供了高质量的360度全景内容生成解决方案。

Abstract: The increasing demand for AR/VR applications has highlighted the need for
high-quality 360-degree panoramic content. However, generating high-quality
360-degree panoramic images and videos remains a challenging task due to the
severe distortions introduced by equirectangular projection (ERP). Existing
approaches either fine-tune pretrained diffusion models on limited ERP datasets
or attempt tuning-free methods that still rely on ERP latent representations,
leading to discontinuities near the poles. In this paper, we introduce
SphereDiff, a novel approach for seamless 360-degree panoramic image and video
generation using state-of-the-art diffusion models without additional tuning.
We define a spherical latent representation that ensures uniform distribution
across all perspectives, mitigating the distortions inherent in ERP. We extend
MultiDiffusion to spherical latent space and propose a spherical latent
sampling method to enable direct use of pretrained diffusion models. Moreover,
we introduce distortion-aware weighted averaging to further improve the
generation quality in the projection process. Our method outperforms existing
approaches in generating 360-degree panoramic content while maintaining high
fidelity, making it a robust solution for immersive AR/VR applications. The
code is available here. https://github.com/pmh9960/SphereDiff

</details>


### [117] [Adversarial Attack for RGB-Event based Visual Object Tracking](https://arxiv.org/abs/2504.14423)
*Qiang Chen,Xiao Wang,Haowen Wang,Bo Jiang,Lin Zhu,Dawei Zhang,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: 提出了一种针对RGB-Event视觉跟踪的跨模态对抗攻击算法，重点研究了Event体素和帧两种表示形式，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: RGB-Event流跟踪算法的对抗攻击与防御研究较少，本文旨在填补这一空白。

Method: 针对RGB-Event体素和帧，分别设计了对抗攻击策略：优化对抗损失生成RGB对抗样本，以及梯度引导优化Event体素空间位置。

Result: 实验表明，该方法在多个数据集上显著降低了跟踪器的性能。

Conclusion: 提出的跨模态对抗攻击算法有效，代码将开源。

Abstract: Visual object tracking is a crucial research topic in the fields of computer
vision and multi-modal fusion. Among various approaches, robust visual tracking
that combines RGB frames with Event streams has attracted increasing attention
from researchers. While striving for high accuracy and efficiency in tracking,
it is also important to explore how to effectively conduct adversarial attacks
and defenses on RGB-Event stream tracking algorithms, yet research in this area
remains relatively scarce. To bridge this gap, in this paper, we propose a
cross-modal adversarial attack algorithm for RGB-Event visual tracking. Because
of the diverse representations of Event streams, and given that Event voxels
and frames are more commonly used, this paper will focus on these two
representations for an in-depth study. Specifically, for the RGB-Event voxel,
we first optimize the perturbation by adversarial loss to generate RGB frame
adversarial examples. For discrete Event voxel representations, we propose a
two-step attack strategy, more in detail, we first inject Event voxels into the
target region as initialized adversarial examples, then, conduct a
gradient-guided optimization by perturbing the spatial location of the Event
voxels. For the RGB-Event frame based tracking, we optimize the cross-modal
universal perturbation by integrating the gradient information from multimodal
data. We evaluate the proposed approach against attacks on three widely used
RGB-Event Tracking datasets, i.e., COESOT, FE108, and VisEvent. Extensive
experiments show that our method significantly reduces the performance of the
tracker across numerous datasets in both unimodal and multimodal scenarios. The
source code will be released on
https://github.com/Event-AHU/Adversarial_Attack_Defense

</details>


### [118] [ResNetVLLM-2: Addressing ResNetVLLM's Multi-Modal Hallucinations](https://arxiv.org/abs/2504.14429)
*Ahmad Khalil,Mahmoud Khalil,Alioune Ngom*

Main category: cs.CV

TL;DR: 论文提出了一种两步协议，用于减少视频语言模型中的幻觉问题，通过检测和缓解策略显著提升了模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）和视频语言模型（VideoLLMs）存在幻觉问题，生成的文本可能与实际视觉内容不符。本文旨在解决这一问题。

Method: 采用两步策略：1）使用改进的Lynx模型检测生成文本与真实视频内容的语义对齐；2）利用检索增强生成（RAG）动态构建知识库以缓解幻觉。

Result: 改进后的模型ResNetVLLM-2在ActivityNet-QA基准测试中准确率从54.8%提升至65.3%。

Conclusion: 提出的检测和缓解策略有效减少了多模态幻觉，提升了视频语言模型的可靠性。

Abstract: Large Language Models (LLMs) have transformed natural language processing
(NLP) tasks, but they suffer from hallucination, generating plausible yet
factually incorrect content. This issue extends to Video-Language Models
(VideoLLMs), where textual descriptions may inaccurately represent visual
content, resulting in multi-modal hallucinations. In this paper, we address
hallucination in ResNetVLLM, a video-language model combining ResNet visual
encoders with LLMs. We introduce a two-step protocol: (1) a faithfulness
detection strategy that uses a modified Lynx model to assess semantic alignment
between generated captions and ground-truth video references, and (2) a
hallucination mitigation strategy using Retrieval-Augmented Generation (RAG)
with an ad-hoc knowledge base dynamically constructed during inference. Our
enhanced model, ResNetVLLM-2, reduces multi-modal hallucinations by
cross-verifying generated content against external knowledge, improving factual
consistency. Evaluation on the ActivityNet-QA benchmark demonstrates a
substantial accuracy increase from 54.8% to 65.3%, highlighting the
effectiveness of our hallucination detection and mitigation strategies in
enhancing video-language model reliability.

</details>


### [119] [ResNetVLLM -- Multi-modal Vision LLM for the Video Understanding Task](https://arxiv.org/abs/2504.14432)
*Ahmad Khalil,Mahmoud Khalil,Alioune Ngom*

Main category: cs.CV

TL;DR: ResNetVLLM是一种新型跨模态框架，结合ResNet视觉编码器和大型语言模型，用于零样本视频理解，无需预训练视频模型，性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决零样本视频模型依赖预训练视频理解模型的问题，提出统一架构学习视觉和语义表示。

Method: 使用非预训练的ResNet提取视觉特征，结合大型语言模型生成文本描述。

Result: 在多个基准测试（如MSRVTT-QA、MSVD-QA等）上实现零样本视频理解的最先进性能。

Conclusion: ResNetVLLM通过统一架构有效提升零样本视频理解的准确性和上下文相关性。

Abstract: In this paper, we introduce ResNetVLLM (ResNet Vision LLM), a novel
cross-modal framework for zero-shot video understanding that integrates a
ResNet-based visual encoder with a Large Language Model (LLM. ResNetVLLM
addresses the challenges associated with zero-shot video models by avoiding
reliance on pre-trained video understanding models and instead employing a
non-pretrained ResNet to extract visual features. This design ensures the model
learns visual and semantic representations within a unified architecture,
enhancing its ability to generate accurate and contextually relevant textual
descriptions from video inputs. Our experimental results demonstrate that
ResNetVLLM achieves state-of-the-art performance in zero-shot video
understanding (ZSVU) on several benchmarks, including MSRVTT-QA, MSVD-QA,
TGIF-QA FrameQA, and ActivityNet-QA.

</details>


### [120] [WT-BCP: Wavelet Transform based Bidirectional Copy-Paste for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2504.14445)
*Mingya Zhang,Liang Wang,Limei Gu,Tingsheng Ling,Xianping Tao*

Main category: cs.CV

TL;DR: 提出了一种基于小波变换的双向复制粘贴半监督医学图像分割框架（WT-BCP），通过融合低频和高频信息及一致性训练提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决半监督医学图像分割中标记与未标记数据分布不匹配、人工扰动导致的训练偏差及低频和高频信息利用不足的问题。

Method: 采用小波变换提取低频和高频信息，结合双向复制粘贴技术增强未标记数据理解，并提出多输入多输出模型XNet-Plus。通过一致性训练减少人工扰动带来的偏差。

Result: 在2D和3D数据集上的实验验证了模型的有效性。

Conclusion: WT-BCP框架通过小波变换和一致性训练显著提升了半监督医学图像分割的性能。

Abstract: Semi-supervised medical image segmentation (SSMIS) shows promise in reducing
reliance on scarce labeled medical data. However, SSMIS field confronts
challenges such as distribution mismatches between labeled and unlabeled data,
artificial perturbations causing training biases, and inadequate use of raw
image information, especially low-frequency (LF) and high-frequency (HF)
components.To address these challenges, we propose a Wavelet Transform based
Bidirectional Copy-Paste SSMIS framework, named WT-BCP, which improves upon the
Mean Teacher approach. Our method enhances unlabeled data understanding by
copying random crops between labeled and unlabeled images and employs WT to
extract LF and HF details.We propose a multi-input and multi-output model named
XNet-Plus, to receive the fused information after WT. Moreover, consistency
training among multiple outputs helps to mitigate learning biases introduced by
artificial perturbations. During consistency training, the mixed images
resulting from WT are fed into both models, with the student model's output
being supervised by pseudo-labels and ground-truth. Extensive experiments
conducted on 2D and 3D datasets confirm the effectiveness of our model.Code:
https://github.com/simzhangbest/WT-BCP.

</details>


### [121] [Neglected Risks: The Disturbing Reality of Children's Images in Datasets and the Urgent Call for Accountability](https://arxiv.org/abs/2504.14446)
*Carlos Caetano,Gabriel O. dos Santos,Caio Petrucci,Artur Barros,Camila Laranjeira,Leo S. F. Ribeiro,Júlia F. de Mendonça,Jefersson A. dos Santos,Sandra Avila*

Main category: cs.CV

TL;DR: 论文探讨了在AI数据集中使用儿童图像的伦理问题，并提出了一种检测和移除这些图像的流程。


<details>
  <summary>Details</summary>
Motivation: 儿童图像的公开使用涉及隐私、同意和数据保护等伦理问题，但目前缺乏有效的解决方法。

Method: 提出了一种基于视觉语言模型的流程，用于检测和移除儿童图像，并在#PraCegoVer和Open Images V7数据集上进行了测试。

Result: 流程在测试数据集上表现有效，为未来研究提供了基线。

Conclusion: 呼吁研究社区反思并采取行动保护儿童权利，同时鼓励开发工具保护弱势群体。

Abstract: Including children's images in datasets has raised ethical concerns,
particularly regarding privacy, consent, data protection, and accountability.
These datasets, often built by scraping publicly available images from the
Internet, can expose children to risks such as exploitation, profiling, and
tracking. Despite the growing recognition of these issues, approaches for
addressing them remain limited. We explore the ethical implications of using
children's images in AI datasets and propose a pipeline to detect and remove
such images. As a use case, we built the pipeline on a Vision-Language Model
under the Visual Question Answering task and tested it on the #PraCegoVer
dataset. We also evaluate the pipeline on a subset of 100,000 images from the
Open Images V7 dataset to assess its effectiveness in detecting and removing
images of children. The pipeline serves as a baseline for future research,
providing a starting point for more comprehensive tools and methodologies.
While we leverage existing models trained on potentially problematic data, our
goal is to expose and address this issue. We do not advocate for training or
deploying such models, but instead call for urgent community reflection and
action to protect children's rights. Ultimately, we aim to encourage the
research community to exercise - more than an additional - care in creating new
datasets and to inspire the development of tools to protect the fundamental
rights of vulnerable groups, particularly children.

</details>


### [122] [Causal Disentanglement for Robust Long-tail Medical Image Generation](https://arxiv.org/abs/2504.14450)
*Weizhi Nie,Zichun Zhang,Weijie Wang,Bruno Lepri,Anan Liu,Nicu Seb*

Main category: cs.CV

TL;DR: 提出了一种基于因果解缠和文本引导的医学图像生成框架，用于生成高质量、多样化的反事实医学图像，并提升模型的临床相关性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像数据稀缺、类别分布不平衡以及生成图像的结构稳定性和多样性问题。

Method: 通过因果解缠实现病理和结构特征分离，结合文本引导的扩散模型生成反事实图像，并利用大语言模型优化初始噪声。

Result: 生成的反事实医学图像具有更高的临床相关性和结构稳定性，同时提升了长尾类别的生成性能。

Conclusion: 该框架有效解决了医学图像生成的挑战，为临床数据增强和模型解释提供了新思路。

Abstract: Counterfactual medical image generation effectively addresses data scarcity
and enhances the interpretability of medical images. However, due to the
complex and diverse pathological features of medical images and the imbalanced
class distribution in medical data, generating high-quality and diverse medical
images from limited data is significantly challenging. Additionally, to fully
leverage the information in limited data, such as anatomical structure
information and generate more structurally stable medical images while avoiding
distortion or inconsistency. In this paper, in order to enhance the clinical
relevance of generated data and improve the interpretability of the model, we
propose a novel medical image generation framework, which generates independent
pathological and structural features based on causal disentanglement and
utilizes text-guided modeling of pathological features to regulate the
generation of counterfactual images. First, we achieve feature separation
through causal disentanglement and analyze the interactions between features.
Here, we introduce group supervision to ensure the independence of pathological
and identity features. Second, we leverage a diffusion model guided by
pathological findings to model pathological features, enabling the generation
of diverse counterfactual images. Meanwhile, we enhance accuracy by leveraging
a large language model to extract lesion severity and location from medical
reports. Additionally, we improve the performance of the latent diffusion model
on long-tailed categories through initial noise optimization.

</details>


### [123] [Metamon-GS: Enhancing Representability with Variance-Guided Densification and Light Encoding](https://arxiv.org/abs/2504.14460)
*Junyan Su,Baozhu Zhao,Xiaohan Zhang,Qi Liu*

Main category: cs.CV

TL;DR: Metamon-GS通过方差引导的密集化策略和多级哈希网格，解决了3D高斯点渲染中的颜色失真和模糊问题，提升了新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯点渲染方法在颜色表示和密集化策略上存在不足，导致渲染效果模糊和失真。

Method: 提出方差引导的密集化策略和多级哈希网格，优化高斯点分布和颜色表示。

Result: 实验表明，Metamon-GS在公开数据集上优于基线模型和先前版本。

Conclusion: Metamon-GS通过创新策略显著提升了3D高斯点渲染的质量。

Abstract: The introduction of 3D Gaussian Splatting (3DGS) has advanced novel view
synthesis by utilizing Gaussians to represent scenes. Encoding Gaussian point
features with anchor embeddings has significantly enhanced the performance of
newer 3DGS variants. While significant advances have been made, it is still
challenging to boost rendering performance. Feature embeddings have difficulty
accurately representing colors from different perspectives under varying
lighting conditions, which leads to a washed-out appearance. Another reason is
the lack of a proper densification strategy that prevents Gaussian point growth
in thinly initialized areas, resulting in blurriness and needle-shaped
artifacts. To address them, we propose Metamon-GS, from innovative viewpoints
of variance-guided densification strategy and multi-level hash grid. The
densification strategy guided by variance specifically targets Gaussians with
high gradient variance in pixels and compensates for the importance of regions
with extra Gaussians to improve reconstruction. The latter studies implicit
global lighting conditions and accurately interprets color from different
perspectives and feature embeddings. Our thorough experiments on publicly
available datasets show that Metamon-GS surpasses its baseline model and
previous versions, delivering superior quality in rendering novel views.

</details>


### [124] [LGD: Leveraging Generative Descriptions for Zero-Shot Referring Image Segmentation](https://arxiv.org/abs/2504.14467)
*Jiachen Li,Qing Xie,Xiaohan Yu,Hongyun Wang,Jinyu Xu,Yongjian Liu,Yongsheng Gao*

Main category: cs.CV

TL;DR: LGD框架利用多模态大语言模型生成描述，提升视觉-语言模型在零样本参考图像分割中的区域-文本匹配性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决自由形式参考表达在视觉-文本模态对齐中的模糊性和多样性问题，避免错误目标定位。

Method: 设计属性提示和周围提示，生成关键属性描述和周围对象描述；引入三种视觉-文本匹配分数评估相似性。

Result: 在RefCOCO、RefCOCO+和RefCOCOg数据集上取得新SOTA，oIoU和mIoU最大提升9.97%和11.29%。

Conclusion: LGD通过生成描述增强匹配性能，显著提升零样本参考图像分割效果。

Abstract: Zero-shot referring image segmentation aims to locate and segment the target
region based on a referring expression, with the primary challenge of aligning
and matching semantics across visual and textual modalities without training.
Previous works address this challenge by utilizing Vision-Language Models and
mask proposal networks for region-text matching. However, this paradigm may
lead to incorrect target localization due to the inherent ambiguity and
diversity of free-form referring expressions. To alleviate this issue, we
present LGD (Leveraging Generative Descriptions), a framework that utilizes the
advanced language generation capabilities of Multi-Modal Large Language Models
to enhance region-text matching performance in Vision-Language Models.
Specifically, we first design two kinds of prompts, the attribute prompt and
the surrounding prompt, to guide the Multi-Modal Large Language Models in
generating descriptions related to the crucial attributes of the referent
object and the details of surrounding objects, referred to as attribute
description and surrounding description, respectively. Secondly, three
visual-text matching scores are introduced to evaluate the similarity between
instance-level visual features and textual features, which determines the mask
most associated with the referring expression. The proposed method achieves new
state-of-the-art performance on three public datasets RefCOCO, RefCOCO+ and
RefCOCOg, with maximum improvements of 9.97% in oIoU and 11.29% in mIoU
compared to previous methods.

</details>


### [125] [Turbo2K: Towards Ultra-Efficient and High-Quality 2K Video Synthesis](https://arxiv.org/abs/2504.14470)
*Jingjing Ren,Wenbo Li,Zhongdao Wang,Haoze Sun,Bangzhen Liu,Haoyu Chen,Jiaqi Xu,Aoxue Li,Shifeng Zhang,Bin Shao,Yong Guo,Lei Zhu*

Main category: cs.CV

TL;DR: Turbo2K是一个高效框架，用于生成2K视频，通过潜在空间压缩和知识蒸馏提升效率，同时保持高质量。


<details>
  <summary>Details</summary>
Motivation: 随着消费者对超清晰视频的需求增加，现有扩散变换器（DiTs）在2K分辨率下计算成本过高，需要更高效的解决方案。

Method: Turbo2K采用压缩潜在空间和知识蒸馏策略，并设计分层两阶段合成框架，以降低计算成本。

Result: Turbo2K在生成5秒24fps的2K视频时，推理速度比现有方法快20倍，计算效率显著提升。

Conclusion: Turbo2K为高分辨率视频生成提供了一种高效且实用的解决方案，适合实际应用。

Abstract: Demand for 2K video synthesis is rising with increasing consumer expectations
for ultra-clear visuals. While diffusion transformers (DiTs) have demonstrated
remarkable capabilities in high-quality video generation, scaling them to 2K
resolution remains computationally prohibitive due to quadratic growth in
memory and processing costs. In this work, we propose Turbo2K, an efficient and
practical framework for generating detail-rich 2K videos while significantly
improving training and inference efficiency. First, Turbo2K operates in a
highly compressed latent space, reducing computational complexity and memory
footprint, making high-resolution video synthesis feasible. However, the high
compression ratio of the VAE and limited model size impose constraints on
generative quality. To mitigate this, we introduce a knowledge distillation
strategy that enables a smaller student model to inherit the generative
capacity of a larger, more powerful teacher model. Our analysis reveals that,
despite differences in latent spaces and architectures, DiTs exhibit structural
similarities in their internal representations, facilitating effective
knowledge transfer. Second, we design a hierarchical two-stage synthesis
framework that first generates multi-level feature at lower resolutions before
guiding high-resolution video generation. This approach ensures structural
coherence and fine-grained detail refinement while eliminating redundant
encoding-decoding overhead, further enhancing computational efficiency.Turbo2K
achieves state-of-the-art efficiency, generating 5-second, 24fps, 2K videos
with significantly reduced computational cost. Compared to existing methods,
Turbo2K is up to 20$\times$ faster for inference, making high-resolution video
generation more scalable and practical for real-world applications.

</details>


### [126] [Efficient Implicit Neural Compression of Point Clouds via Learnable Activation in Latent Space](https://arxiv.org/abs/2504.14471)
*Yichi Zhang,Qianqian Yang*

Main category: cs.CV

TL;DR: PICO是一种基于隐式神经表示（INR）的点云压缩框架，通过几何压缩和属性压缩两阶段优化，结合LeAFNet网络架构，显著提升了压缩性能。


<details>
  <summary>Details</summary>
Motivation: 传统点云压缩方法存在效率不足的问题，PICO旨在通过INR和新型网络架构提升压缩性能。

Method: 将点云压缩分为几何压缩和属性压缩两阶段，采用LeAFNet网络（基于可学习激活函数）优化INR目标，并结合量化和熵编码提升效率。

Result: LeAFNet在INR压缩中优于传统MLP，PICO在几何压缩上超越MPEG标准（D1 PSNR提升4.92 dB），在联合压缩中PCQM增益达2.7×10⁻³。

Conclusion: PICO通过INR和LeAFNet实现了高效的点云压缩，性能优于现有方法。

Abstract: Implicit Neural Representations (INRs), also known as neural fields, have
emerged as a powerful paradigm in deep learning, parameterizing continuous
spatial fields using coordinate-based neural networks. In this paper, we
propose \textbf{PICO}, an INR-based framework for static point cloud
compression. Unlike prevailing encoder-decoder paradigms, we decompose the
point cloud compression task into two separate stages: geometry compression and
attribute compression, each with distinct INR optimization objectives. Inspired
by Kolmogorov-Arnold Networks (KANs), we introduce a novel network
architecture, \textbf{LeAFNet}, which leverages learnable activation functions
in the latent space to better approximate the target signal's implicit
function. By reformulating point cloud compression as neural parameter
compression, we further improve compression efficiency through quantization and
entropy coding. Experimental results demonstrate that \textbf{LeAFNet}
outperforms conventional MLPs in INR-based point cloud compression.
Furthermore, \textbf{PICO} achieves superior geometry compression performance
compared to the current MPEG point cloud compression standard, yielding an
average improvement of $4.92$ dB in D1 PSNR. In joint geometry and attribute
compression, our approach exhibits highly competitive results, with an average
PCQM gain of $2.7 \times 10^{-3}$.

</details>


### [127] [Vision-Centric Representation-Efficient Fine-Tuning for Robust Universal Foreground Segmentation](https://arxiv.org/abs/2504.14481)
*Guoyi Zhang,Siyang Chen,Guangsheng Xu,Han Wang,Xiaohu Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级参数高效微调框架LSR-ST，通过引入形状偏置的归纳先验增强模型在复杂场景中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型（VFMs）在复杂场景（如伪装和红外图像）中的参数高效微调（PEFT）表现不佳，主要归因于模型固有的纹理偏置。

Method: 提出LSR-ST框架，利用HDConv Block捕获形状感知特征，满足大感受野、多阶特征交互和稀疏连接的条件。

Result: 在17个数据集和6个任务中，仅使用4.719M可训练参数，LSR-ST实现了性能提升。

Conclusion: 通过信息瓶颈理论形式化表示效率，表明其在复杂视觉环境中构建鲁棒和可适应VFMs的潜力。

Abstract: Foreground segmentation is crucial for scene understanding, yet
parameter-efficient fine-tuning (PEFT) of vision foundation models (VFMs) often
fails in complex scenarios, such as camouflage and infrared imagery. We
attribute this challenge to the inherent texture bias in VFMs, which is
exacerbated during fine-tuning and limits generalization in texture-sparse
environments. To address this, we propose Ladder Shape-bias Representation
Side-tuning (LSR-ST), a lightweight PEFT framework that enhances model
robustness by introducing shape-biased inductive priors. LSR-ST captures
shape-aware features using a simple HDConv Block, which integrates large-kernel
attention and residual learning. The method satisfies three key conditions for
inducing shape bias: large receptive fields, multi-order feature interactions,
and sparse connectivity. Our analysis reveals that these improvements stem from
representation efficiency-the ability to extract task-relevant, structurally
grounded features while minimizing redundancy. We formalize this concept via
Information Bottleneck theory and advocate for it as a key PEFT objective.
Unlike traditional NLP paradigms that focus on optimizing parameters and
memory, visual tasks require models that extract task-defined semantics, rather
than just relying on pre-encoded features. This shift enables our approach to
move beyond conventional trade-offs, offering more robust and generalizable
solutions for vision tasks. With minimal changes to SAM2-UNet, LSR-ST achieves
consistent improvements across 17 datasets and 6 tasks using only 4.719M
trainable parameters. These results highlight the potential of representation
efficiency for robust and adaptable VFMs within complex visual environments.

</details>


### [128] [STARS: Sparse Learning Correlation Filter with Spatio-temporal Regularization and Super-resolution Reconstruction for Thermal Infrared Target Tracking](https://arxiv.org/abs/2504.14491)
*Shang Zhang,Xiaobo Ding,Huanbin Zhang,Ruoyan Xiong,Yue Zhang*

Main category: cs.CV

TL;DR: STARS是一种基于稀疏学习的相关滤波跟踪器，结合时空正则化和超分辨率重建，显著提升了热红外目标跟踪的性能。


<details>
  <summary>Details</summary>
Motivation: 热红外图像分辨率低且易受干扰，限制了跟踪器的性能。

Method: 采用自适应稀疏滤波和时域滤波提取目标特征，引入边缘保留稀疏正则化稳定特征，并提出梯度增强超分辨率方法提升图像分辨率。

Result: 在多个基准测试中，STARS表现优于现有最先进的跟踪器。

Conclusion: STARS首次将超分辨率方法融入稀疏学习框架，显著提升了热红外目标跟踪的鲁棒性。

Abstract: Thermal infrared (TIR) target tracking methods often adopt the correlation
filter (CF) framework due to its computational efficiency. However, the low
resolution of TIR images, along with tracking interference, significantly
limits the perfor-mance of TIR trackers. To address these challenges, we
introduce STARS, a novel sparse learning-based CF tracker that incorporates
spatio-temporal regulari-zation and super-resolution reconstruction. First, we
apply adaptive sparse filter-ing and temporal domain filtering to extract key
features of the target while reduc-ing interference from background clutter and
noise. Next, we introduce an edge-preserving sparse regularization method to
stabilize target features and prevent excessive blurring. This regularization
integrates multiple terms and employs the alternating direction method of
multipliers to optimize the solution. Finally, we propose a gradient-enhanced
super-resolution method to extract fine-grained TIR target features and improve
the resolution of TIR images, addressing performance degradation in tracking
caused by low-resolution sequences. To the best of our knowledge, STARS is the
first to integrate super-resolution methods within a sparse learning-based CF
framework. Extensive experiments on the LSOTB-TIR, PTB-TIR, VOT-TIR2015, and
VOT-TIR2017 benchmarks demonstrate that STARS outperforms state-of-the-art
trackers in terms of robustness.

</details>


### [129] [DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning](https://arxiv.org/abs/2504.14509)
*Fulong Ye,Miao Hua,Pengze Zhang,Xinghui Li,Qichao Sun,Songtao Zhao,Qian He,Xinglong Wu*

Main category: cs.CV

TL;DR: DreamID是一种基于扩散模型的人脸交换方法，通过显式监督和Triplet ID Group数据提升身份相似性和属性保留，结合SD Turbo加速推理，实现高质量快速结果。


<details>
  <summary>Details</summary>
Motivation: 传统人脸交换方法依赖隐式监督，效果不佳。DreamID旨在通过显式监督和高效架构解决这一问题。

Method: 使用Triplet ID Group数据进行显式监督，结合SD Turbo加速扩散模型，提出SwapNet、FaceNet和ID Adapter的架构。

Result: 在身份相似性、姿态表情保留和图像保真度上优于现有方法，512*512分辨率下仅需0.6秒。

Conclusion: DreamID通过显式监督和高效架构，实现了高质量、快速的人脸交换，适用于复杂场景。

Abstract: In this paper, we introduce DreamID, a diffusion-based face swapping model
that achieves high levels of ID similarity, attribute preservation, image
fidelity, and fast inference speed. Unlike the typical face swapping training
process, which often relies on implicit supervision and struggles to achieve
satisfactory results. DreamID establishes explicit supervision for face
swapping by constructing Triplet ID Group data, significantly enhancing
identity similarity and attribute preservation. The iterative nature of
diffusion models poses challenges for utilizing efficient image-space loss
functions, as performing time-consuming multi-step sampling to obtain the
generated image during training is impractical. To address this issue, we
leverage the accelerated diffusion model SD Turbo, reducing the inference steps
to a single iteration, enabling efficient pixel-level end-to-end training with
explicit Triplet ID Group supervision. Additionally, we propose an improved
diffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.
This robust architecture fully unlocks the power of the Triplet ID Group
explicit supervision. Finally, to further extend our method, we explicitly
modify the Triplet ID Group data during training to fine-tune and preserve
specific attributes, such as glasses and face shape. Extensive experiments
demonstrate that DreamID outperforms state-of-the-art methods in terms of
identity similarity, pose and expression preservation, and image fidelity.
Overall, DreamID achieves high-quality face swapping results at 512*512
resolution in just 0.6 seconds and performs exceptionally well in challenging
scenarios such as complex lighting, large angles, and occlusions.

</details>


### [130] [Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction](https://arxiv.org/abs/2504.14516)
*Weirong Chen,Ganlin Zhang,Felix Wimbauer,Rui Wang,Nikita Araslanov,Andrea Vedaldi,Daniel Cremers*

Main category: cs.CV

TL;DR: BA-Track结合3D点跟踪器和传统SLAM技术，通过分解相机运动与动态物体运动，提升了动态场景下的相机位姿估计和3D重建精度。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM系统依赖静态环境假设，难以处理动态场景中的复杂运动，导致重建不完整或运动估计不一致。

Method: 使用3D点跟踪器分离相机运动与动态物体运动，结合光束法平差（bundle adjustment）和轻量级后处理，确保深度一致性。

Result: 在挑战性数据集上，BA-Track显著提高了相机位姿估计和3D重建的准确性。

Conclusion: BA-Track通过统一框架有效处理动态场景，为SLAM系统提供了更鲁棒的解决方案。

Abstract: Traditional SLAM systems, which rely on bundle adjustment, struggle with
highly dynamic scenes commonly found in casual videos. Such videos entangle the
motion of dynamic elements, undermining the assumption of static environments
required by traditional systems. Existing techniques either filter out dynamic
elements or model their motion independently. However, the former often results
in incomplete reconstructions, whereas the latter can lead to inconsistent
motion estimates. Taking a novel approach, this work leverages a 3D point
tracker to separate the camera-induced motion from the observed motion of
dynamic objects. By considering only the camera-induced component, bundle
adjustment can operate reliably on all scene elements as a result. We further
ensure depth consistency across video frames with lightweight post-processing
based on scale maps. Our framework combines the core of traditional SLAM --
bundle adjustment -- with a robust learning-based 3D tracker front-end.
Integrating motion decomposition, bundle adjustment and depth refinement, our
unified framework, BA-Track, accurately tracks the camera motion and produces
temporally coherent and scale-consistent dense reconstructions, accommodating
both static and dynamic elements. Our experiments on challenging datasets
reveal significant improvements in camera pose estimation and 3D reconstruction
accuracy.

</details>


### [131] [Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding](https://arxiv.org/abs/2504.14526)
*Tong Zeng,Longfeng Wu,Liang Shi,Dawei Zhou,Feng Guo*

Main category: cs.CV

TL;DR: DVBench是一个评估视觉大语言模型（VLLMs）在安全关键驾驶场景中表现的新基准，揭示了现有模型的局限性，并通过领域适应显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLLMs在通用视觉任务中表现优异，但在自动驾驶等安全关键领域的有效性尚未充分探索，亟需专门评估工具。

Method: 开发了DVBench，包含10,000个人工标注的多选题，基于分层能力分类法评估VLLMs的感知与推理能力，并对14个模型进行实验和微调。

Result: 现有模型在DVBench上表现不佳（最高准确率<40%），但微调后性能显著提升（相对改进达43.59%）。

Conclusion: DVBench为开发满足自动驾驶安全需求的VLLMs提供了评估框架和研究方向，强调了领域适应的重要性。

Abstract: Vision Large Language Models (VLLMs) have demonstrated impressive
capabilities in general visual tasks such as image captioning and visual
question answering. However, their effectiveness in specialized,
safety-critical domains like autonomous driving remains largely unexplored.
Autonomous driving systems require sophisticated scene understanding in complex
environments, yet existing multimodal benchmarks primarily focus on normal
driving conditions, failing to adequately assess VLLMs' performance in
safety-critical scenarios. To address this, we introduce DVBench, a pioneering
benchmark designed to evaluate the performance of VLLMs in understanding
safety-critical driving videos. Built around a hierarchical ability taxonomy
that aligns with widely adopted frameworks for describing driving scenarios
used in assessing highly automated driving systems, DVBench features 10,000
multiple-choice questions with human-annotated ground-truth answers, enabling a
comprehensive evaluation of VLLMs' capabilities in perception and reasoning.
Experiments on 14 SOTA VLLMs, ranging from 0.5B to 72B parameters, reveal
significant performance gaps, with no model achieving over 40% accuracy,
highlighting critical limitations in understanding complex driving scenarios.
To probe adaptability, we fine-tuned selected models using domain-specific data
from DVBench, achieving accuracy gains ranging from 5.24 to 10.94 percentage
points, with relative improvements of up to 43.59%. This improvement
underscores the necessity of targeted adaptation to bridge the gap between
general-purpose VLLMs and mission-critical driving applications. DVBench
establishes an essential evaluation framework and research roadmap for
developing VLLMs that meet the safety and robustness requirements for
real-world autonomous systems. We released the benchmark toolbox and the
fine-tuned model at: https://github.com/tong-zeng/DVBench.git.

</details>


### [132] [SUDO: Enhancing Text-to-Image Diffusion Models with Self-Supervised Direct Preference Optimization](https://arxiv.org/abs/2504.14534)
*Liang Peng,Boxi Wu,Haoran Cheng,Yibo Zhao,Xiaofei He*

Main category: cs.CV

TL;DR: SUDO是一种自监督直接偏好优化方法，用于提升文本到图像扩散模型的全局和局部图像质量，无需昂贵的数据标注。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调方法仅优化像素级MSE损失，忽略了全局图像质量的优化需求。

Method: SUDO通过自监督生成偏好图像对，结合直接偏好优化，同时优化像素级细节和全局图像质量。

Result: 实验表明，SUDO显著提升了Stable Diffusion 1.5和XL等模型的全局和局部图像质量。

Conclusion: SUDO是一种高效且无需标注的替代方案，适用于任何文本到图像扩散模型。

Abstract: Previous text-to-image diffusion models typically employ supervised
fine-tuning (SFT) to enhance pre-trained base models. However, this approach
primarily minimizes the loss of mean squared error (MSE) at the pixel level,
neglecting the need for global optimization at the image level, which is
crucial for achieving high perceptual quality and structural coherence. In this
paper, we introduce Self-sUpervised Direct preference Optimization (SUDO), a
novel paradigm that optimizes both fine-grained details at the pixel level and
global image quality. By integrating direct preference optimization into the
model, SUDO generates preference image pairs in a self-supervised manner,
enabling the model to prioritize global-level learning while complementing the
pixel-level MSE loss. As an effective alternative to supervised fine-tuning,
SUDO can be seamlessly applied to any text-to-image diffusion model.
Importantly, it eliminates the need for costly data collection and annotation
efforts typically associated with traditional direct preference optimization
methods. Through extensive experiments on widely-used models, including Stable
Diffusion 1.5 and XL, we demonstrate that SUDO significantly enhances both
global and local image quality. The codes are provided at
\href{https://github.com/SPengLiang/SUDO}{this link}.

</details>


### [133] [FlowLoss: Dynamic Flow-Conditioned Loss Strategy for Video Diffusion Models](https://arxiv.org/abs/2504.14535)
*Kuanting Wu,Kei Ota,Asako Kanezaki*

Main category: cs.CV

TL;DR: FlowLoss通过直接比较生成视频和真实视频的光流场，改善了视频扩散模型的时序一致性，并通过噪声感知加权方案优化训练。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型在生成高质量视频时，时序一致性不足，光流监督是一种潜在解决方案。

Method: 提出FlowLoss，直接比较生成和真实视频的光流场，并引入噪声感知加权方案。

Result: 在机器人视频数据集上，FlowLoss提高了运动稳定性并加速了早期训练阶段的收敛。

Conclusion: FlowLoss为噪声条件生成模型提供了实用的运动监督方法。

Abstract: Video Diffusion Models (VDMs) can generate high-quality videos, but often
struggle with producing temporally coherent motion. Optical flow supervision is
a promising approach to address this, with prior works commonly employing
warping-based strategies that avoid explicit flow matching. In this work, we
explore an alternative formulation, FlowLoss, which directly compares flow
fields extracted from generated and ground-truth videos. To account for the
unreliability of flow estimation under high-noise conditions in diffusion, we
propose a noise-aware weighting scheme that modulates the flow loss across
denoising steps. Experiments on robotic video datasets suggest that FlowLoss
improves motion stability and accelerates convergence in early training stages.
Our findings offer practical insights for incorporating motion-based
supervision into noise-conditioned generative models.

</details>


### [134] [VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control](https://arxiv.org/abs/2504.14548)
*Lifeng Lin,Rongfeng Lu,Quan Chen,Haofan Ren,Ming Lu,Yaoqi Sun,Chenggang Yan,Anke Xue*

Main category: cs.CV

TL;DR: 论文提出了一种名为VGNC的新方法，通过生成验证图像来控制高斯点的数量，以减少稀疏视图3D重建中的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图3D重建在实际应用中存在过拟合问题，现有基于3D高斯泼溅（3DGS）的方法虽有所进展，但仍需改进。

Method: VGNC结合生成式新视图合成（NVS）模型生成验证图像，并基于此控制高斯点数量，以优化重建效果。

Result: 实验表明，VGNC不仅减少了过拟合，还提升了测试集的渲染质量，同时降低了高斯点数量和计算开销。

Conclusion: VGNC为稀疏视图3DGS提供了一种有效的过拟合缓解方案，具有实际应用潜力。

Abstract: Sparse-view 3D reconstruction is a fundamental yet challenging task in
practical 3D reconstruction applications. Recently, many methods based on the
3D Gaussian Splatting (3DGS) framework have been proposed to address
sparse-view 3D reconstruction. Although these methods have made considerable
advancements, they still show significant issues with overfitting. To reduce
the overfitting, we introduce VGNC, a novel Validation-guided Gaussian Number
Control (VGNC) approach based on generative novel view synthesis (NVS) models.
To the best of our knowledge, this is the first attempt to alleviate the
overfitting issue of sparse-view 3DGS with generative validation images.
Specifically, we first introduce a validation image generation method based on
a generative NVS model. We then propose a Gaussian number control strategy that
utilizes generated validation images to determine the optimal Gaussian numbers,
thereby reducing the issue of overfitting. We conducted detailed experiments on
various sparse-view 3DGS baselines and datasets to evaluate the effectiveness
of VGNC. Extensive experiments show that our approach not only reduces
overfitting but also improves rendering quality on the test set while
decreasing the number of Gaussian points. This reduction lowers storage demands
and accelerates both training and rendering. The code will be released.

</details>


### [135] [Grounding-MD: Grounded Video-language Pre-training for Open-World Moment Detection](https://arxiv.org/abs/2504.14553)
*Weijun Zhuang,Qizhang Li,Xin Li,Ming Liu,Xiaopeng Hong,Feng Gao,Fan Yang,Wangmeng Zuo*

Main category: cs.CV

TL;DR: Grounding-MD是一个创新的视频-语言预训练框架，用于开放世界时刻检测，通过结构化提示机制和跨模态融合编码器/解码器实现灵活、可扩展的检测，并在多个基准数据集上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于封闭场景，无法适应开放世界需求，因此提出Grounding-MD以填补这一空白。

Method: 采用结构化提示机制、跨模态融合编码器和文本引导融合解码器，支持开放世界中的多样化查询。

Result: 在ActivityNet等四个数据集上，Grounding-MD在零样本和监督设置下均达到最优性能。

Conclusion: Grounding-MD为开放世界时刻检测提供了高效解决方案，并具备广泛适用性。

Abstract: Temporal Action Detection and Moment Retrieval constitute two pivotal tasks
in video understanding, focusing on precisely localizing temporal segments
corresponding to specific actions or events. Recent advancements introduced
Moment Detection to unify these two tasks, yet existing approaches remain
confined to closed-set scenarios, limiting their applicability in open-world
contexts. To bridge this gap, we present Grounding-MD, an innovative, grounded
video-language pre-training framework tailored for open-world moment detection.
Our framework incorporates an arbitrary number of open-ended natural language
queries through a structured prompt mechanism, enabling flexible and scalable
moment detection. Grounding-MD leverages a Cross-Modality Fusion Encoder and a
Text-Guided Fusion Decoder to facilitate comprehensive video-text alignment and
enable effective cross-task collaboration. Through large-scale pre-training on
temporal action detection and moment retrieval datasets, Grounding-MD
demonstrates exceptional semantic representation learning capabilities,
effectively handling diverse and complex query conditions. Comprehensive
evaluations across four benchmark datasets including ActivityNet, THUMOS14,
ActivityNet-Captions, and Charades-STA demonstrate that Grounding-MD
establishes new state-of-the-art performance in zero-shot and supervised
settings in open-world moment detection scenarios. All source code and trained
models will be released.

</details>


### [136] [SMTT: Novel Structured Multi-task Tracking with Graph-Regularized Sparse Representation for Robust Thermal Infrared Target Tracking](https://arxiv.org/abs/2504.14566)
*Shang Zhang,HuiPan Guan,XiaoBo Ding,Ruoyan Xiong,Yue Zhang*

Main category: cs.CV

TL;DR: SMTT是一种新型热红外目标跟踪器，通过多任务学习、联合稀疏表示和自适应图正则化解决噪声、遮挡和快速目标运动等问题。


<details>
  <summary>Details</summary>
Motivation: 热红外目标跟踪在监控、自动驾驶和军事行动中至关重要，但面临噪声、遮挡和快速目标运动等挑战。

Method: SMTT将跟踪任务重新定义为多任务学习问题，采用加权混合范数正则化策略优化粒子表示，并使用加速近端梯度法实现实时性能。

Result: 在VOT-TIR、PTB-TIR和LSOTB-TIR等基准数据集上，SMTT表现出卓越的准确性、鲁棒性和计算效率。

Conclusion: SMTT是复杂环境中热红外目标跟踪的可靠高性能解决方案。

Abstract: Thermal infrared target tracking is crucial in applications such as
surveillance, autonomous driving, and military operations. In this paper, we
propose a novel tracker, SMTT, which effectively addresses common challenges in
thermal infrared imagery, such as noise, occlusion, and rapid target motion, by
leveraging multi-task learning, joint sparse representation, and adaptive graph
regularization. By reformulating the tracking task as a multi-task learning
problem, the SMTT tracker independently optimizes the representation of each
particle while dynamically capturing spatial and feature-level similarities
using a weighted mixed-norm regularization strategy. To ensure real-time
performance, we incorporate the Accelerated Proximal Gradient method for
efficient optimization. Extensive experiments on benchmark datasets - including
VOT-TIR, PTB-TIR, and LSOTB-TIR - demonstrate that SMTT achieves superior
accuracy, robustness, and computational efficiency. These results highlight
SMTT as a reliable and high-performance solution for thermal infrared target
tracking in complex environments.

</details>


### [137] [NTIRE 2025 Challenge on Image Super-Resolution ($\times$4): Methods and Results](https://arxiv.org/abs/2504.14582)
*Zheng Chen,Kai Liu,Jue Gong,Jingkai Wang,Lei Sun,Zongwei Wu,Radu Timofte,Yulun Zhang,Xiangyu Kong,Xiaoxuan Yu,Hyunhee Park,Suejin Han,Hakjae Jeon,Dafeng Zhang,Hyung-Ju Chun,Donghun Ryou,Inju Ha,Bohyung Han,Lu Zhao,Yuyi Zhang,Pengyu Yan,Jiawei Hu,Pengwei Liu,Fengjun Guo,Hongyuan Yu,Pufan Xu,Zhijuan Huang,Shuyuan Cui,Peng Guo,Jiahui Liu,Dongkai Zhang,Heng Zhang,Huiyuan Fu,Huadong Ma,Yanhui Guo,Sisi Tian,Xin Liu,Jinwen Liang,Jie Liu,Jie Tang,Gangshan Wu,Zeyu Xiao,Zhuoyuan Li,Yinxiang Zhang,Wenxuan Cai,Vijayalaxmi Ashok Aralikatti,Nikhil Akalwadi,G Gyaneshwar Rao,Chaitra Desai,Ramesh Ashok Tabib,Uma Mudenagudi,Marcos V. Conde,Alejandro Merino,Bruno Longarela,Javier Abad,Weijun Yuan,Zhan Li,Zhanglu Chen,Boyang Yao,Aagam Jain,Milan Kumar Singh,Ankit Kumar,Shubh Kawa,Divyavardhan Singh,Anjali Sarvaiya,Kishor Upla,Raghavendra Ramachandra,Chia-Ming Lee,Yu-Fan Lin,Chih-Chung Hsu,Risheek V Hiremath,Yashaswini Palani,Yuxuan Jiang,Qiang Zhu,Siyue Teng,Fan Zhang,Shuyuan Zhu,Bing Zeng,David Bull,Jingwei Liao,Yuqing Yang,Wenda Shao,Junyi Zhao,Qisheng Xu,Kele Xu,Sunder Ali Khowaja,Ik Hyun Lee,Snehal Singh Tomar,Rajarshi Ray,Klaus Mueller,Sachin Chaudhary,Surya Vashisth,Akshay Dudhane,Praful Hambarde,Satya Naryan Tazi,Prashant Patil,Santosh Kumar Vipparthi,Subrahmanyam Murala,Bilel Benjdira,Anas M. Ali,Wadii Boulila,Zahra Moammeri,Ahmad Mahmoudi-Aznaveh,Ali Karbasi,Hossein Motamednia,Liangyan Li,Guanhua Zhao,Kevin Le,Yimo Ning,Haoxuan Huang,Jun Chen*

Main category: cs.CV

TL;DR: NTIRE 2025图像超分辨率挑战赛旨在通过双三次下采样恢复高分辨率图像，分为恢复和感知两个子赛道，共有286名参与者注册，25支团队提交有效结果。


<details>
  <summary>Details</summary>
Motivation: 推动图像超分辨率技术的发展，通过挑战赛形式促进网络设计或解决方案的创新。

Method: 挑战赛采用双三次下采样生成低分辨率图像，要求参与者设计网络或解决方案恢复高分辨率图像，分为恢复（基于PSNR）和感知（基于视觉真实性）两个子赛道。

Result: 共有286名参与者注册，25支团队提交有效结果，挑战赛总结了设计、数据集、评估协议及团队方法。

Conclusion: 该挑战赛作为基准推动了图像超分辨率技术的进步，促进了该领域的发展。

Abstract: This paper presents the NTIRE 2025 image super-resolution ($\times$4)
challenge, one of the associated competitions of the 10th NTIRE Workshop at
CVPR 2025. The challenge aims to recover high-resolution (HR) images from
low-resolution (LR) counterparts generated through bicubic downsampling with a
$\times$4 scaling factor. The objective is to develop effective network designs
or solutions that achieve state-of-the-art SR performance. To reflect the dual
objectives of image SR research, the challenge includes two sub-tracks: (1) a
restoration track, emphasizes pixel-wise accuracy and ranks submissions based
on PSNR; (2) a perceptual track, focuses on visual realism and ranks results by
a perceptual score. A total of 286 participants registered for the competition,
with 25 teams submitting valid entries. This report summarizes the challenge
design, datasets, evaluation protocol, the main results, and methods of each
team. The challenge serves as a benchmark to advance the state of the art and
foster progress in image SR.

</details>


### [138] [Using street view imagery and deep generative modeling for estimating the health of urban forests](https://arxiv.org/abs/2504.14583)
*Akshit Gupta,Remko Uijlenhoet*

Main category: cs.CV

TL;DR: 提出了一种利用街景图像、树木清单数据和气象条件监测城市森林健康的新方法，通过图像到图像转换网络估算NDVI和CTD参数，并与实地测量数据对比验证。


<details>
  <summary>Details</summary>
Motivation: 传统监测方法依赖人工和主观评估，成本高且难以扩展；多光谱遥感技术也存在部署和分辨率限制。

Method: 使用街景图像、树木清单和气象数据，结合图像到图像转换网络估算NDVI和CTD。

Result: 通过实地测量验证方法的准确性，为城市森林管理提供可扩展的解决方案。

Conclusion: 该方法利用现有街景平台，为城市森林健康监测提供了高效、可扩展的途径。

Abstract: Healthy urban forests comprising of diverse trees and shrubs play a crucial
role in mitigating climate change. They provide several key advantages such as
providing shade for energy conservation, and intercepting rainfall to reduce
flood runoff and soil erosion. Traditional approaches for monitoring the health
of urban forests require instrumented inspection techniques, often involving a
high amount of human labor and subjective evaluations. As a result, they are
not scalable for cities which lack extensive resources. Recent approaches
involving multi-spectral imaging data based on terrestrial sensing and
satellites, are constrained respectively with challenges related to dedicated
deployments and limited spatial resolutions. In this work, we propose an
alternative approach for monitoring the urban forests using simplified inputs:
street view imagery, tree inventory data and meteorological conditions. We
propose to use image-to-image translation networks to estimate two urban forest
health parameters, namely, NDVI and CTD. Finally, we aim to compare the
generated results with ground truth data using an onsite campaign utilizing
handheld multi-spectral and thermal imaging sensors. With the advent and
expansion of street view imagery platforms such as Google Street View and
Mapillary, this approach should enable effective management of urban forests
for the authorities in cities at scale.

</details>


### [139] [NTIRE 2025 Challenge on Real-World Face Restoration: Methods and Results](https://arxiv.org/abs/2504.14600)
*Zheng Chen,Jingkai Wang,Kai Liu,Jue Gong,Lei Sun,Zongwei Wu,Radu Timofte,Yulun Zhang,Jianxing Zhang,Jinlong Wu,Jun Wang,Zheng Xie,Hakjae Jeon,Suejin Han,Hyung-Ju Chun,Hyunhee Park,Zhicun Yin,Junjie Chen,Ming Liu,Xiaoming Li,Chao Zhou,Wangmeng Zuo,Weixia Zhang,Dingquan Li,Kede Ma,Yun Zhang,Zhuofan Zheng,Yuyue Liu,Shizhen Tang,Zihao Zhang,Yi Ning,Hao Jiang,Wenjie An,Kangmeng Yu,Chenyang Wang,Kui Jiang,Xianming Liu,Junjun Jiang,Yingfu Zhang,Gang He,Siqi Wang,Kepeng Xu,Zhenyang Liu,Changxin Zhou,Shanlan Shen,Yubo Duan,Yiang Chen,Jin Guo,Mengru Yang,Jen-Wei Lee,Chia-Ming Lee,Chih-Chung Hsu,Hu Peng,Chunming He*

Main category: cs.CV

TL;DR: NTIRE 2025挑战赛聚焦真实人脸修复，旨在提升感知质量和真实性，吸引了141名参赛者，最终10支团队获得有效排名。


<details>
  <summary>Details</summary>
Motivation: 推动真实人脸修复领域的前沿技术发展，提升生成结果的感知质量和身份一致性。

Method: 使用加权图像质量评估（IQA）分数和AdaFace模型作为身份检查器，评估参赛模型性能。

Result: 13支团队提交有效模型，10支团队在最终排名中获得有效分数。

Conclusion: 该挑战赛推动了真实人脸修复技术的进步，并总结了该领域的最新趋势。

Abstract: This paper provides a review of the NTIRE 2025 challenge on real-world face
restoration, highlighting the proposed solutions and the resulting outcomes.
The challenge focuses on generating natural, realistic outputs while
maintaining identity consistency. Its goal is to advance state-of-the-art
solutions for perceptual quality and realism, without imposing constraints on
computational resources or training data. The track of the challenge evaluates
performance using a weighted image quality assessment (IQA) score and employs
the AdaFace model as an identity checker. The competition attracted 141
registrants, with 13 teams submitting valid models, and ultimately, 10 teams
achieved a valid score in the final ranking. This collaborative effort advances
the performance of real-world face restoration while offering an in-depth
overview of the latest trends in the field.

</details>


### [140] [MP-Mat: A 3D-and-Instance-Aware Human Matting and Editing Framework with Multiplane Representation](https://arxiv.org/abs/2504.14606)
*Siyi Jiao,Wenzheng Zeng,Yerong Li,Huayu Zhang,Changxin Gao,Nong Sang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: MP-Mat是一种新颖的3D感知和实例感知的抠图框架，通过多平面表示从几何和实例层面解决复杂场景中的人像抠图问题。


<details>
  <summary>Details</summary>
Motivation: 解决复杂场景中多实例人像抠图的挑战，尤其是在边界模糊和遮挡区域的表现。

Method: 提出基于深度差异的多平面表示，从几何层面分割场景，并结合实例层面的多平面表示，将背景也视为特殊实例。

Result: MP-Mat在抠图任务中表现优异，并在零样本推理的图像编辑任务中超越专用方法。

Conclusion: MP-Mat通过多平面表示显著提升了复杂场景下的抠图效果，并展示了在图像编辑任务中的潜力。

Abstract: Human instance matting aims to estimate an alpha matte for each human
instance in an image, which is challenging as it easily fails in complex cases
requiring disentangling mingled pixels belonging to multiple instances along
hairy and thin boundary structures. In this work, we address this by
introducing MP-Mat, a novel 3D-and-instance-aware matting framework with
multiplane representation, where the multiplane concept is designed from two
different perspectives: scene geometry level and instance level. Specifically,
we first build feature-level multiplane representations to split the scene into
multiple planes based on depth differences. This approach makes the scene
representation 3D-aware, and can serve as an effective clue for splitting
instances in different 3D positions, thereby improving interpretability and
boundary handling ability especially in occlusion areas. Then, we introduce
another multiplane representation that splits the scene in an instance-level
perspective, and represents each instance with both matte and color. We also
treat background as a special instance, which is often overlooked by existing
methods. Such an instance-level representation facilitates both foreground and
background content awareness, and is useful for other down-stream tasks like
image editing. Once built, the representation can be reused to realize
controllable instance-level image editing with high efficiency. Extensive
experiments validate the clear advantage of MP-Mat in matting task. We also
demonstrate its superiority in image editing tasks, an area under-explored by
existing matting-focused methods, where our approach under zero-shot inference
even outperforms trained specialized image editing techniques by large margins.
Code is open-sourced at https://github.com/JiaoSiyi/MPMat.git}.

</details>


### [141] [VM-BHINet:Vision Mamba Bimanual Hand Interaction Network for 3D Interacting Hand Mesh Recovery From a Single RGB Image](https://arxiv.org/abs/2504.14618)
*Han Bi,Ge Yu,Yu He,Wenzhuo Liu,Zijie Zheng*

Main category: cs.CV

TL;DR: VM-BHINet利用状态空间模型改进双手交互重建，显著降低误差。


<details>
  <summary>Details</summary>
Motivation: 现有方法在遮挡、模糊外观和计算效率方面表现不佳。

Method: 提出VM-BHINet，结合状态空间模型与局部全局特征操作，通过VM-IFEBlock增强交互建模。

Result: 在InterHand2.6M数据集上，MPJPE和MPVPE降低2-3%，优于现有方法。

Conclusion: VM-BHINet在双手交互重建中表现出色，兼具高效性和准确性。

Abstract: Understanding bimanual hand interactions is essential for realistic 3D pose
and shape reconstruction. However, existing methods struggle with occlusions,
ambiguous appearances, and computational inefficiencies. To address these
challenges, we propose Vision Mamba Bimanual Hand Interaction Network
(VM-BHINet), introducing state space models (SSMs) into hand reconstruction to
enhance interaction modeling while improving computational efficiency. The core
component, Vision Mamba Interaction Feature Extraction Block (VM-IFEBlock),
combines SSMs with local and global feature operations, enabling deep
understanding of hand interactions. Experiments on the InterHand2.6M dataset
show that VM-BHINet reduces Mean per-joint position error (MPJPE) and Mean
per-vertex position error (MPVPE) by 2-3%, significantly surpassing
state-of-the-art methods.

</details>


### [142] [Talk is Not Always Cheap: Promoting Wireless Sensing Models with Text Prompts](https://arxiv.org/abs/2504.14621)
*Zhenkui Yang,Zeyi Huang,Ge Wang,Han Ding,Tony Xiao Han,Fei Wang*

Main category: cs.CV

TL;DR: 论文提出了一种文本增强的无线传感框架WiTalk，通过分层提示策略整合语义知识，显著提升了无线信号感知的性能。


<details>
  <summary>Details</summary>
Motivation: 现有无线信号感知技术未能充分利用数据集中的文本信息，限制了其性能和应用潜力。

Method: 提出WiTalk框架，采用三种分层提示策略（标签、简要描述和详细动作描述）整合语义知识，无需修改架构或增加数据成本。

Result: 在三个公开数据集上验证，性能显著提升：XRF55的准确率分别提高3.9%、2.59%和0.46%；WiFiTAL平均性能提升4.98%；XRFV2的平均精度提升4.02%至13.68%。

Conclusion: WiTalk框架通过整合文本信息，有效提升了无线信号感知的性能，为相关应用提供了新的解决方案。

Abstract: Wireless signal-based human sensing technologies, such as WiFi,
millimeter-wave (mmWave) radar, and Radio Frequency Identification (RFID),
enable the detection and interpretation of human presence, posture, and
activities, thereby providing critical support for applications in public
security, healthcare, and smart environments. These technologies exhibit
notable advantages due to their non-contact operation and environmental
adaptability; however, existing systems often fail to leverage the textual
information inherent in datasets. To address this, we propose an innovative
text-enhanced wireless sensing framework, WiTalk, that seamlessly integrates
semantic knowledge through three hierarchical prompt strategies-label-only,
brief description, and detailed action description-without requiring
architectural modifications or incurring additional data costs. We rigorously
validate this framework across three public benchmark datasets: XRF55 for human
action recognition (HAR), and WiFiTAL and XRFV2 for WiFi temporal action
localization (TAL). Experimental results demonstrate significant performance
improvements: on XRF55, accuracy for WiFi, RFID, and mmWave increases by 3.9%,
2.59%, and 0.46%, respectively; on WiFiTAL, the average performance of WiFiTAD
improves by 4.98%; and on XRFV2, the mean average precision gains across
various methods range from 4.02% to 13.68%. Our codes have been included in
https://github.com/yangzhenkui/WiTalk.

</details>


### [143] [MSAD-Net: Multiscale and Spatial Attention-based Dense Network for Lung Cancer Classification](https://arxiv.org/abs/2504.14626)
*Santanu Roy,Shweta Singh,Palak Sahu,Ashvath Suresh,Debashish Das*

Main category: cs.CV

TL;DR: 提出了一种名为MSD-Net的新型CNN架构，用于解决医学图像中的类别不平衡问题，并在性能上超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 肺癌早期检测对降低死亡率至关重要，但传统手动检测方法存在挑战，且现有深度学习模型因类别不平衡问题性能受限。

Method: 设计了MSD-Net，包含新颖的密集模块、深度可分离卷积层、跳跃连接和平行分支，以提取多尺度特征并降低模型复杂度。

Result: 实验表明，MSD-Net在性能上显著优于ConvNext-Tiny、ViT、PiT等现有模型。

Conclusion: MSD-Net为肺癌自动检测提供了一种高效且轻量化的解决方案。

Abstract: Lung cancer, a severe form of malignant tumor that originates in the tissues
of the lungs, can be fatal if not detected in its early stages. It ranks among
the top causes of cancer-related mortality worldwide. Detecting lung cancer
manually using chest X-Ray image or Computational Tomography (CT) scans image
poses significant challenges for radiologists. Hence, there is a need for
automatic diagnosis system of lung cancers from radiology images. With the
recent emergence of deep learning, particularly through Convolutional Neural
Networks (CNNs), the automated detection of lung cancer has become a much
simpler task. Nevertheless, numerous researchers have addressed that the
performance of conventional CNNs may be hindered due to class imbalance issue,
which is prevalent in medical images. In this research work, we have proposed a
novel CNN architecture ``Multi-Scale Dense Network (MSD-Net)''
(trained-from-scratch). The novelties we bring in the proposed model are (I) We
introduce novel dense modules in the 4th block and 5th block of the CNN model.
We have leveraged 3 depthwise separable convolutional (DWSC) layers, and one
1x1 convolutional layer in each dense module, in order to reduce complexity of
the model considerably. (II) Additionally, we have incorporated one skip
connection from 3rd block to 5th block and one parallel branch connection from
4th block to Global Average Pooling (GAP) layer. We have utilized dilated
convolutional layer (with dilation rate=2) in the last parallel branch in order
to extract multi-scale features. Extensive experiments reveal that our proposed
model has outperformed latest CNN model ConvNext-Tiny, recent trend Vision
Transformer (ViT), Pooling-based ViT (PiT), and other existing models by
significant margins.

</details>


### [144] [NVSMask3D: Hard Visual Prompting with Camera Pose Interpolation for 3D Open Vocabulary Instance Segmentation](https://arxiv.org/abs/2504.14638)
*Junyuan Fang,Zihan Wang,Yejun Zhang,Shuzhe Wang,Iaroslav Melekhov,Juho Kannala*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯泼溅的硬视觉提示方法，通过相机插值生成多视角，无需2D-3D优化或微调，提升3D实例分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在3D实例级分割任务中表现不足，需改进其定位和识别能力。

Method: 采用3D高斯泼溅和相机插值生成多视角，增强几何一致性，结合硬视觉提示。

Result: 方法无需训练，显著提升了3D实例分割的鲁棒性和准确性。

Conclusion: 该方法有效弥补了视觉语言模型在3D任务中的不足，具有广泛适用性。

Abstract: Vision-language models (VLMs) have demonstrated impressive zero-shot transfer
capabilities in image-level visual perception tasks. However, they fall short
in 3D instance-level segmentation tasks that require accurate localization and
recognition of individual objects. To bridge this gap, we introduce a novel 3D
Gaussian Splatting based hard visual prompting approach that leverages camera
interpolation to generate diverse viewpoints around target objects without any
2D-3D optimization or fine-tuning. Our method simulates realistic 3D
perspectives, effectively augmenting existing hard visual prompts by enforcing
geometric consistency across viewpoints. This training-free strategy seamlessly
integrates with prior hard visual prompts, enriching object-descriptive
features and enabling VLMs to achieve more robust and accurate 3D instance
segmentation in diverse 3D scenes.

</details>


### [145] [Relation-R1: Cognitive Chain-of-Thought Guided Reinforcement Learning for Unified Relational Comprehension](https://arxiv.org/abs/2504.14642)
*Lin Li,Wei Chen,Jiahui Li,Long Chen*

Main category: cs.CV

TL;DR: Relation-R1是一个统一的关系理解框架，通过认知链式思维（CoT）引导的监督微调（SFT）和组相对策略优化（GRPO）提升多模态大语言模型（MLLMs）在视觉关系理解上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在视觉关系理解（如场景图生成）上存在局限，尤其是对N元关系的建模不足，导致输出不可靠和过度依赖语言先验。

Method: 提出Relation-R1框架，结合CoT引导的SFT和GRPO，通过强化学习优化视觉-语义基础，减少语言偏见。

Result: 在PSG和SWiG数据集上，Relation-R1在二元和N元关系理解上达到最先进性能。

Conclusion: Relation-R1通过结构化推理和多奖励优化，显著提升了MLLMs在复杂视觉关系任务中的表现。

Abstract: Recent advances in multi-modal large language models (MLLMs) have
significantly improved object-level grounding and region captioning, but remain
limited in visual relation understanding (\eg, scene graph generation),
particularly in modeling \textit{N}-ary relationships that identify multiple
semantic roles among an action event. Such a lack of \textit{semantic
dependencies} modeling among multi-entities leads to unreliable outputs,
intensifying MLLMs' hallucinations and over-reliance on language priors. To
this end, we propose Relation-R1, the first unified relational comprehension
framework that explicitly integrates cognitive chain-of-thought (CoT)-guided
Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO)
within a reinforcement learning (RL) paradigm. Specifically, we first establish
foundational reasoning capabilities via SFT, enforcing structured outputs with
thinking processes. Then, GRPO is utilized to refine these outputs via
multi-reward optimization, prioritizing visual-semantic grounding over
language-induced biases, thereby improving generalization capability. Extensive
experiments on widely-used PSG and SWiG datasets demonstrate that Relation-R1
achieves state-of-the-art performance in both binary and \textit{N}-ary
relation understanding.

</details>


### [146] [EmoSEM: Segment and Explain Emotion Stimuli in Visual Art](https://arxiv.org/abs/2504.14658)
*Jing Zhang,Dan Guo,Zhangbin Li,Meng Wang*

Main category: cs.CV

TL;DR: 论文提出EmoSEM模型，结合情感提示和轻量级前缀投影器，实现艺术图像中情感触发区域的像素级分割和语言解释生成。


<details>
  <summary>Details</summary>
Motivation: 解决艺术图像情感理解中的两个关键挑战：情感主观性导致的分割模型适应性差，以及艺术表达的抽象性导致的语言模型难以平衡语义与情感推理。

Method: 引入情感提示和可学习掩码标记作为分割解码条件，设计情感投影器关联情感与视觉特征，开发轻量级前缀投影器融合情感掩码与语言模型。

Result: 实验验证了模型在情感触发区域分割和情感解释生成上的有效性。

Conclusion: EmoSEM首次实现了从像素特征到情感解释的端到端建模，为艺术情感计算提供了可解释的细粒度分析框架。

Abstract: This paper focuses on a key challenge in visual art understanding: given an
art image, the model pinpoints pixel regions that trigger a specific human
emotion, and generates linguistic explanations for the emotional arousal.
Despite recent advances in art understanding, pixel-level emotion understanding
still faces a dual challenge: first, the subjectivity of emotion makes it
difficult for general segmentation models like SAM to adapt to emotion-oriented
segmentation tasks; and second, the abstract nature of art expression makes it
difficult for captioning models to balance pixel-level semantic understanding
and emotion reasoning. To solve the above problems, this paper proposes the
Emotion stimuli Segmentation and Explanation Model (EmoSEM) to endow the
segmentation model SAM with emotion comprehension capability. First, to enable
the model to perform segmentation under the guidance of emotional intent well,
we introduce an emotional prompt with a learnable mask token as the conditional
input for segmentation decoding. Then, we design an emotion projector to
establish the association between emotion and visual features. Next, more
importantly, to address emotion-visual stimuli alignment, we develop a
lightweight prefix projector, a module that fuses the learned emotional mask
with the corresponding emotion into a unified representation compatible with
the language model.Finally, we input the joint visual, mask, and emotional
tokens into the language model and output the emotional explanations. It
ensures that the generated interpretations remain semantically and emotionally
coherent with the visual stimuli. The method innovatively realizes end-to-end
modeling from low-level pixel features to high-level emotion interpretation,
providing the first interpretable fine-grained analysis framework for artistic
emotion computing. Extensive experiments validate the effectiveness of our
model.

</details>


### [147] [Frequency-domain Learning with Kernel Prior for Blind Image Deblurring](https://arxiv.org/abs/2504.14664)
*Jixiang Sun,Fei Lei,Jiawei Zhang,Wenxiu Sun,Yujiu Yang*

Main category: cs.CV

TL;DR: 论文提出了一种结合核先验的深度学习方法，通过频率域融合提升图像去模糊的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在图像去模糊任务中泛化能力不足，主要依赖特定领域数据集。核先验因其与图像内容无关，可能解决这一问题。

Method: 提出频率集成模块（FIM）融合核先验，并结合基于频率的去模糊Transformer网络。

Result: 实验表明，该方法在多个盲图像去模糊任务中优于现有方法，具有更强的泛化能力。

Conclusion: 引入核先验并通过频率域融合是提升图像去模糊泛化能力的有效途径。

Abstract: While achieving excellent results on various datasets, many deep learning
methods for image deblurring suffer from limited generalization capabilities
with out-of-domain data. This limitation is likely caused by their dependence
on certain domain-specific datasets. To address this challenge, we argue that
it is necessary to introduce the kernel prior into deep learning methods, as
the kernel prior remains independent of the image context. For effective fusion
of kernel prior information, we adopt a rational implementation method inspired
by traditional deblurring algorithms that perform deconvolution in the
frequency domain. We propose a module called Frequency Integration Module (FIM)
for fusing the kernel prior and combine it with a frequency-based deblurring
Transfomer network. Experimental results demonstrate that our method
outperforms state-of-the-art methods on multiple blind image deblurring tasks,
showcasing robust generalization abilities. Source code will be available soon.

</details>


### [148] [DMPCN: Dynamic Modulated Predictive Coding Network with Hybrid Feedback Representations](https://arxiv.org/abs/2504.14665)
*A S M Sharifuzzaman Sagar,Yu Chen,Jun Hoong Chan*

Main category: cs.CV

TL;DR: 本文提出了一种混合预测误差反馈机制和动态调制的深度预测编码网络，解决了传统方法在局部和全局细节处理上的不足，并设计了专用损失函数以提高性能。


<details>
  <summary>Details</summary>
Motivation: 传统预测编码网络的误差反馈机制在处理局部和全局细节时表现不佳，且难以动态适应输入数据的复杂性。本文旨在解决这些问题。

Method: 引入混合预测误差反馈机制，结合全局上下文和局部细节，并动态调整反馈；设计专用损失函数以优化预测误差最小化。

Result: 实验表明，该模型在CIFAR-10、CIFAR-100、MNIST和FashionMNIST数据集上收敛更快且预测精度更高。

Conclusion: 提出的方法显著提升了预测编码网络的性能，适用于多样化场景。

Abstract: Traditional predictive coding networks, inspired by theories of brain
function, consistently achieve promising results across various domains,
extending their influence into the field of computer vision. However, the
performance of the predictive coding networks is limited by their error
feedback mechanism, which traditionally employs either local or global
recurrent updates, leading to suboptimal performance in processing both local
and broader details simultaneously. In addition, traditional predictive coding
networks face difficulties in dynamically adjusting to the complexity and
context of varying input data, which is crucial for achieving high levels of
performance in diverse scenarios. Furthermore, there is a gap in the
development and application of specific loss functions that could more
effectively guide the model towards optimal performance. To deal with these
issues, this paper introduces a hybrid prediction error feedback mechanism with
dynamic modulation for deep predictive coding networks by effectively combining
global contexts and local details while adjusting feedback based on input
complexity. Additionally, we present a loss function tailored to this framework
to improve accuracy by focusing on precise prediction error minimization.
Experimental results demonstrate the superiority of our model over other
approaches, showcasing faster convergence and higher predictive accuracy in
CIFAR-10, CIFAR-100, MNIST, and FashionMNIST datasets.

</details>


### [149] [Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens](https://arxiv.org/abs/2504.14666)
*Kaihang Pan,Wang Lin,Zhongqi Yue,Tenglong Ao,Liyu Jia,Wei Zhao,Juncheng Li,Siliang Tang,Hanwang Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散时间步的递归视觉标记方法，解决了现有空间视觉标记缺乏递归结构的问题，实现了多模态理解和生成的高效统一。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用空间视觉标记，但其缺乏语言的递归结构，难以被大型语言模型掌握。本文旨在构建一种适合的视觉语言。

Method: 利用扩散时间步学习离散、递归的视觉标记，这些标记能递归补偿噪声图像中的渐进属性损失。

Result: 实验表明，该方法在多模态理解和生成任务上均优于其他多模态大语言模型。

Conclusion: 通过结合LLM的自回归推理和扩散模型的精确图像生成，实现了无缝的多模态理解和生成。

Abstract: Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify
visual comprehension and generation by combining LLM and diffusion models, the
state-of-the-art in each task, respectively. Existing approaches rely on
spatial visual tokens, where image patches are encoded and arranged according
to a spatial order (e.g., raster scan). However, we show that spatial tokens
lack the recursive structure inherent to languages, hence form an impossible
language for LLM to master. In this paper, we build a proper visual language by
leveraging diffusion timesteps to learn discrete, recursive visual tokens. Our
proposed tokens recursively compensate for the progressive attribute loss in
noisy images as timesteps increase, enabling the diffusion model to reconstruct
the original image at any timestep. This approach allows us to effectively
integrate the strengths of LLMs in autoregressive reasoning and diffusion
models in precise image generation, achieving seamless multimodal comprehension
and generation within a unified framework. Extensive experiments show that we
achieve superior performance for multimodal comprehension and generation
simultaneously compared with other MLLMs. Project Page:
https://DDT-LLaMA.github.io/.

</details>


### [150] [Seurat: From Moving Points to Depth](https://arxiv.org/abs/2504.14687)
*Seokju Cho,Jiahui Huang,Seungryong Kim,Joon-Young Lee*

Main category: cs.CV

TL;DR: 提出一种通过分析2D轨迹推断相对深度的方法，利用空间和时间变换器处理轨迹，实现高精度深度预测。


<details>
  <summary>Details</summary>
Motivation: 单目视频深度估计因缺乏立体视觉等深度线索而具有挑战性，但人类通过观察物体大小和间距变化能直观感知相对深度。

Method: 使用现成的点跟踪模型捕获2D轨迹，通过空间和时间变换器处理轨迹，直接推断深度变化。

Result: 在TAPVid-3D基准测试中表现优异，零样本泛化能力强，预测结果在多样领域具有时间平滑性和高精度。

Conclusion: 该方法通过分析2D轨迹实现了高效且准确的相对深度估计，适用于合成和真实世界数据。

Abstract: Accurate depth estimation from monocular videos remains challenging due to
ambiguities inherent in single-view geometry, as crucial depth cues like
stereopsis are absent. However, humans often perceive relative depth
intuitively by observing variations in the size and spacing of objects as they
move. Inspired by this, we propose a novel method that infers relative depth by
examining the spatial relationships and temporal evolution of a set of tracked
2D trajectories. Specifically, we use off-the-shelf point tracking models to
capture 2D trajectories. Then, our approach employs spatial and temporal
transformers to process these trajectories and directly infer depth changes
over time. Evaluated on the TAPVid-3D benchmark, our method demonstrates robust
zero-shot performance, generalizing effectively from synthetic to real-world
datasets. Results indicate that our approach achieves temporally smooth,
high-accuracy depth predictions across diverse domains.

</details>


### [151] [Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark](https://arxiv.org/abs/2504.14693)
*Enxin Song,Wenhao Chai,Weili Xu,Jianwen Xie,Yuxuan Liu,Gaoang Wang*

Main category: cs.CV

TL;DR: Video-MMLU是一个用于评估语言多模态模型（LMMs）在多学科讲座理解能力的大规模基准测试，揭示了当前模型在认知挑战中的局限性。


<details>
  <summary>Details</summary>
Motivation: 探索LMMs在多学科讲座理解中的潜力，填补该领域的研究空白。

Method: 评估了90多个开源和专有模型（参数范围0.5B至40B），并研究了视觉标记数量和大语言模型对性能的影响。

Result: 当前模型在处理需要感知和推理的任务时表现有限，视觉标记数量和大语言模型对性能有显著影响。

Conclusion: 多模态感知与推理的交互对讲座理解至关重要，需进一步优化模型设计。

Abstract: Recent advancements in language multimodal models (LMMs) for video have
demonstrated their potential for understanding video content, yet the task of
comprehending multi-discipline lectures remains largely unexplored. We
introduce Video-MMLU, a massive benchmark designed to evaluate the capabilities
of LMMs in understanding Multi-Discipline Lectures. We evaluate over 90
open-source and proprietary models, ranging from 0.5B to 40B parameters. Our
results highlight the limitations of current models in addressing the cognitive
challenges presented by these lectures, especially in tasks requiring both
perception and reasoning. Additionally, we explore how the number of visual
tokens and the large language models influence performance, offering insights
into the interplay between multimodal perception and reasoning in lecture
comprehension.

</details>


### [152] [IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays](https://arxiv.org/abs/2504.14699)
*Sascha Jecklin,Aidana Massalimova,Ruyi Zha,Lilian Calvet,Christoph J. Laux,Mazda Farshad,Philipp Fürnstahl*

Main category: cs.CV

TL;DR: 该论文提出了一种基于实例学习的3D脊柱重建方法，通过扩展$R^2$-Gaussian splatting框架，减少对大量标注数据的依赖，并引入解剖学引导的标准化步骤，提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 脊柱手术需要高精度的3D解剖重建，现有监督学习方法依赖大量标注数据且泛化能力有限。

Method: 扩展$R^2$-Gaussian splatting框架，结合风格迁移实现解剖学引导的标准化，无需预训练即可适应新患者。

Result: 在体外数据集上验证，专家评估确认其临床实用性，定量指标显示标准化步骤显著提升重建质量。

Conclusion: 该方法证明了从稀疏X射线进行实例化3D重建的可行性，为手术导航提供了新思路。

Abstract: Spine surgery is a high-risk intervention demanding precise execution, often
supported by image-based navigation systems. Recently, supervised learning
approaches have gained attention for reconstructing 3D spinal anatomy from
sparse fluoroscopic data, significantly reducing reliance on
radiation-intensive 3D imaging systems. However, these methods typically
require large amounts of annotated training data and may struggle to generalize
across varying patient anatomies or imaging conditions. Instance-learning
approaches like Gaussian splatting could offer an alternative by avoiding
extensive annotation requirements. While Gaussian splatting has shown promise
for novel view synthesis, its application to sparse, arbitrarily posed real
intraoperative X-rays has remained largely unexplored. This work addresses this
limitation by extending the $R^2$-Gaussian splatting framework to reconstruct
anatomically consistent 3D volumes under these challenging conditions. We
introduce an anatomy-guided radiographic standardization step using style
transfer, improving visual consistency across views, and enhancing
reconstruction quality. Notably, our framework requires no pretraining, making
it inherently adaptable to new patients and anatomies. We evaluated our
approach using an ex-vivo dataset. Expert surgical evaluation confirmed the
clinical utility of the 3D reconstructions for navigation, especially when
using 20 to 30 views, and highlighted the standardization's benefit for
anatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM)
confirmed performance trade-offs compared to idealized settings, but also
validated the improvement gained from standardization over raw inputs. This
work demonstrates the feasibility of instance-based volumetric reconstruction
from arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for
surgical navigation.

</details>


### [153] [Time Frequency Analysis of EMG Signal for Gesture Recognition using Fine grained Features](https://arxiv.org/abs/2504.14708)
*Parshuram N. Aarotale,Ajita Rattani*

Main category: cs.CV

TL;DR: 论文提出了一种基于EMG的手势识别新方法XMANet，通过跨层互注意力机制结合局部和语义特征，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决EMG手势识别中局部和语义特征结合不足的问题，提升分类准确性和鲁棒性。

Method: 使用XMANet模型，结合STFT和WT生成的谱图和尺度图，通过跨层互注意力机制整合浅层和深层CNN特征。

Result: 在Grabmyo和FORS EMG数据集上，XMANet显著优于基线模型，最高提升9.36%。

Conclusion: XMANet通过细粒度特征提取和跨层注意力机制，为EMG分类提供了高效且鲁棒的解决方案。

Abstract: Electromyography (EMG) based hand gesture recognition converts forearm muscle
activity into control commands for prosthetics, rehabilitation, and human
computer interaction. This paper proposes a novel approach to EMG-based hand
gesture recognition that uses fine-grained classification and presents XMANet,
which unifies low-level local and high level semantic cues through cross layer
mutual attention among shallow to deep CNN experts. Using stacked spectrograms
and scalograms derived from the Short Time Fourier Transform (STFT) and Wavelet
Transform (WT), we benchmark XMANet against ResNet50, DenseNet-121,
MobileNetV3, and EfficientNetB0. Experimental results on the Grabmyo dataset
indicate that, using STFT, the proposed XMANet model outperforms the baseline
ResNet50, EfficientNetB0, MobileNetV3, and DenseNet121 models with improvement
of approximately 1.72%, 4.38%, 5.10%, and 2.53%, respectively. When employing
the WT approach, improvements of around 1.57%, 1.88%, 1.46%, and 2.05% are
observed over the same baselines. Similarly, on the FORS EMG dataset, the
XMANet(ResNet50) model using STFT shows an improvement of about 5.04% over the
baseline ResNet50. In comparison, the XMANet(DenseNet121) and
XMANet(MobileNetV3) models yield enhancements of approximately 4.11% and 2.81%,
respectively. Moreover, when using WT, the proposed XMANet achieves gains of
around 4.26%, 9.36%, 5.72%, and 6.09% over the baseline ResNet50, DenseNet121,
MobileNetV3, and EfficientNetB0 models, respectively. These results confirm
that XMANet consistently improves performance across various architectures and
signal processing techniques, demonstrating the strong potential of fine
grained features for accurate and robust EMG classification.

</details>


### [154] [Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline](https://arxiv.org/abs/2504.14709)
*Hui Zhou,Shaoshuai Shi,Hongsheng Li*

Main category: cs.CV

TL;DR: 论文提出了一种结合模仿学习和强化学习的新框架，以解决模仿学习在自动驾驶规划中的局限性，并开发了闭环模拟器和因果基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在自动驾驶规划中表现良好，但难以判断其是否真正理解驾驶原则，且容易过拟合常见场景。

Method: 提出了一种结合模仿学习和强化学习的框架，开发了闭环模拟器和因果基准。

Result: 新框架旨在克服纯模仿学习的局限性，提升对罕见或未见场景的泛化能力。

Conclusion: 通过结合模仿学习和强化学习，论文为解决自动驾驶规划中的挑战提供了新思路。

Abstract: Machine learning (ML)-based planners have recently gained significant
attention. They offer advantages over traditional optimization-based planning
algorithms. These advantages include fewer manually selected parameters and
faster development. Within ML-based planning, imitation learning (IL) is a
common algorithm. It primarily learns driving policies directly from supervised
trajectory data. While IL has demonstrated strong performance on many open-loop
benchmarks, it remains challenging to determine if the learned policy truly
understands fundamental driving principles, rather than simply extrapolating
from the ego-vehicle's initial state. Several studies have identified this
limitation and proposed algorithms to address it. However, these methods often
use original datasets for evaluation. In these datasets, future trajectories
are heavily dependent on initial conditions. Furthermore, IL often overfits to
the most common scenarios. It struggles to generalize to rare or unseen
situations.
  To address these challenges, this work proposes: 1) a novel closed-loop
simulator supporting both imitation and reinforcement learning, 2) a causal
benchmark derived from the Waymo Open Dataset to rigorously assess the impact
of the copycat problem, and 3) a novel framework integrating imitation learning
and reinforcement learning to overcome the limitations of purely imitative
approaches. The code for this work will be released soon.

</details>


### [155] [Med-2D SegNet: A Light Weight Deep Neural Network for Medical 2D Image Segmentation](https://arxiv.org/abs/2504.14715)
*Md. Sanaullah Chowdhury,Salauddin Tapu,Noyon Kumar Sarkar,Ferdous Bin Ali,Lameya Sabrin*

Main category: cs.CV

TL;DR: Med-2D SegNet是一种高效且精确的医学图像分割架构，通过紧凑的Med Block设计实现低参数和高性能，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割对临床诊断和手术规划至关重要，但由于解剖结构复杂性和低复杂度模型需求，仍具挑战性。

Method: 提出Med-2D SegNet架构，采用紧凑的Med Block设计，结合维度扩展和参数减少技术，实现高效特征提取。

Result: 在20个数据集上平均DSC达89.77%，参数仅2.07百万，在零样本学习和跨数据集泛化中表现优异。

Conclusion: Med-2D SegNet在精度和效率间取得平衡，为临床和资源受限环境提供高性能工具，推动先进医疗技术的普及。

Abstract: Accurate and efficient medical image segmentation is crucial for advancing
clinical diagnostics and surgical planning, yet remains a complex challenge due
to the variability in anatomical structures and the demand for low-complexity
models. In this paper, we introduced Med-2D SegNet, a novel and highly
efficient segmentation architecture that delivers outstanding accuracy while
maintaining a minimal computational footprint. Med-2D SegNet achieves
state-of-the-art performance across multiple benchmark datasets, including
KVASIR-SEG, PH2, EndoVis, and GLAS, with an average Dice similarity coefficient
(DSC) of 89.77% across 20 diverse datasets. Central to its success is the
compact Med Block, a specialized encoder design that incorporates dimension
expansion and parameter reduction, enabling precise feature extraction while
keeping model parameters to a low count of just 2.07 million. Med-2D SegNet
excels in cross-dataset generalization, particularly in polyp segmentation,
where it was trained on KVASIR-SEG and showed strong performance on unseen
datasets, demonstrating its robustness in zero-shot learning scenarios, even
though we acknowledge that further improvements are possible. With top-tier
performance in both binary and multi-class segmentation, Med-2D SegNet
redefines the balance between accuracy and efficiency, setting a new benchmark
for medical image analysis. This work paves the way for developing accessible,
high-performance diagnostic tools suitable for clinical environments and
resource-constrained settings, making it a step forward in the democratization
of advanced medical technology.

</details>


### [156] [TAPIP3D: Tracking Any Point in Persistent 3D Geometry](https://arxiv.org/abs/2504.14717)
*Bowei Zhang,Lei Ke,Adam W. Harley,Katerina Fragkiadaki*

Main category: cs.CV

TL;DR: TAPIP3D是一种新颖的3D点跟踪方法，通过将2D视频特征提升到3D世界空间并利用局部对注意力机制，显著提升了长期跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D点跟踪方法在处理长期跟踪和相机运动时表现不佳，需要一种更稳健的方法。

Method: TAPIP3D将视频表示为相机稳定的时空特征云，利用深度和相机运动信息，通过迭代优化多帧3D运动估计实现跟踪。

Result: 该方法在3D点跟踪基准测试中表现优异，甚至在深度信息准确时优于传统2D跟踪器。

Conclusion: TAPIP3D通过3D上下文策略和相机运动补偿，显著提升了跟踪的鲁棒性和准确性。

Abstract: We introduce TAPIP3D, a novel approach for long-term 3D point tracking in
monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized
spatio-temporal feature clouds, leveraging depth and camera motion information
to lift 2D video features into a 3D world space where camera motion is
effectively canceled. TAPIP3D iteratively refines multi-frame 3D motion
estimates within this stabilized representation, enabling robust tracking over
extended periods. To manage the inherent irregularities of 3D point
distributions, we propose a Local Pair Attention mechanism. This 3D
contextualization strategy effectively exploits spatial relationships in 3D,
forming informative feature neighborhoods for precise 3D trajectory estimation.
Our 3D-centric approach significantly outperforms existing 3D point tracking
methods and even enhances 2D tracking accuracy compared to conventional 2D
pixel trackers when accurate depth is available. It supports inference in both
camera coordinates (i.e., unstabilized) and world coordinates, and our results
demonstrate that compensating for camera motion improves tracking performance.
Our approach replaces the conventional 2D square correlation neighborhoods used
in prior 2D and 3D trackers, leading to more robust and accurate results across
various 3D point tracking benchmarks. Project Page: https://tapip3d.github.io

</details>


### [157] [ChronoRoot 2.0: An Open AI-Powered Platform for 2D Temporal Plant Phenotyping](https://arxiv.org/abs/2504.14736)
*Nicolás Gaggion,Rodrigo Bonazzola,María Florencia Legascue,María Florencia Mammarella,Florencia Sol Rodriguez,Federico Emanuel Aballay,Florencia Belén Catulo,Andana Barrios,Franco Accavallo,Santiago Nahuel Villarreal,Martin Crespi,Martiniano María Ricardi,Ezequiel Petrillo,Thomas Blein,Federico Ariel,Enzo Ferrante*

Main category: cs.CV

TL;DR: ChronoRoot 2.0是一个开源平台，结合低成本硬件和AI技术，实现了植物根系发育的时序表型分析，适用于多种研究场景。


<details>
  <summary>Details</summary>
Motivation: 植物发育可塑性分析对理解其适应性和农业可持续性至关重要，但现有技术难以全面分析根系发育的时序动态。

Method: ChronoRoot 2.0整合了多器官追踪、实时质量控制、结构测量和双用户界面，支持高通量和精细分析。

Result: 系统成功应用于拟南芥的昼夜生长模式、转基因植物的向重力性反应及多基因型黄化反应的高通量筛选。

Conclusion: ChronoRoot 2.0扩展了功能且保持低成本，开源特性促进了社区开发和可重复性。

Abstract: The analysis of plant developmental plasticity, including root system
architecture, is fundamental to understanding plant adaptability and
development, particularly in the context of climate change and agricultural
sustainability. While significant advances have been made in plant phenotyping
technologies, comprehensive temporal analysis of root development remains
challenging, with most existing solutions providing either limited throughput
or restricted structural analysis capabilities. Here, we present ChronoRoot
2.0, an integrated open-source platform that combines affordable hardware with
advanced artificial intelligence to enable sophisticated temporal plant
phenotyping. The system introduces several major advances, offering an integral
perspective of seedling development: (i) simultaneous multi-organ tracking of
six distinct plant structures, (ii) quality control through real-time
validation, (iii) comprehensive architectural measurements including novel
gravitropic response parameters, and (iv) dual specialized user interfaces for
both architectural analysis and high-throughput screening. We demonstrate the
system's capabilities through three use cases for Arabidopsis thaliana:
characterization of circadian growth patterns under different light conditions,
detailed analysis of gravitropic responses in transgenic plants, and
high-throughput screening of etiolation responses across multiple genotypes.
ChronoRoot 2.0 maintains its predecessor's advantages of low cost and
modularity while significantly expanding its capabilities, making sophisticated
temporal phenotyping more accessible to the broader plant science community.
The system's open-source nature, combined with extensive documentation and
containerized deployment options, ensures reproducibility and enables
community-driven development of new analytical capabilities.

</details>


### [158] [SuperCL: Superpixel Guided Contrastive Learning for Medical Image Segmentation Pre-training](https://arxiv.org/abs/2504.14737)
*Shuang Zeng,Lei Zhu,Xinliang Zhang,Hangzhou He,Yanye Lu*

Main category: cs.CV

TL;DR: 论文提出了一种名为SuperCL的新型对比学习方法，用于医学图像分割预训练，通过利用图像的结构先验和像素相关性，解决了现有方法忽略组内相似像素特征和依赖手动阈值的问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临高质量标注数据稀缺的挑战，现有对比学习方法多关注实例级或像素级表示，忽略了组内相似像素特征，且依赖手动阈值设置，效率低且泛化性差。

Method: 提出SuperCL方法，引入两种对比对生成策略：Intra-image Local Contrastive Pairs (ILCP)和Inter-image Global Contrastive Pairs (IGCP)，并利用超像素图生成伪掩码指导监督对比学习。还设计了ASP和CCL模块以更好地利用结构先验信息。

Result: 在8个医学图像数据集上的实验表明，SuperCL优于12种现有方法，在MMWHS、CHAOS和Spleen数据集上分别取得3.15%、5.44%和7.89%的DSC提升。

Conclusion: SuperCL通过创新的对比对生成策略和模块设计，显著提升了医学图像分割的性能，尤其在标注数据有限的情况下表现突出。

Abstract: Medical image segmentation is a critical yet challenging task, primarily due
to the difficulty of obtaining extensive datasets of high-quality,
expert-annotated images. Contrastive learning presents a potential but still
problematic solution to this issue. Because most existing methods focus on
extracting instance-level or pixel-to-pixel representation, which ignores the
characteristics between intra-image similar pixel groups. Moreover, when
considering contrastive pairs generation, most SOTA methods mainly rely on
manually setting thresholds, which requires a large number of gradient
experiments and lacks efficiency and generalization. To address these issues,
we propose a novel contrastive learning approach named SuperCL for medical
image segmentation pre-training. Specifically, our SuperCL exploits the
structural prior and pixel correlation of images by introducing two novel
contrastive pairs generation strategies: Intra-image Local Contrastive Pairs
(ILCP) Generation and Inter-image Global Contrastive Pairs (IGCP) Generation.
Considering superpixel cluster aligns well with the concept of contrastive
pairs generation, we utilize the superpixel map to generate pseudo masks for
both ILCP and IGCP to guide supervised contrastive learning. Moreover, we also
propose two modules named Average SuperPixel Feature Map Generation (ASP) and
Connected Components Label Generation (CCL) to better exploit the prior
structural information for IGCP. Finally, experiments on 8 medical image
datasets indicate our SuperCL outperforms existing 12 methods. i.e. Our SuperCL
achieves a superior performance with more precise predictions from
visualization figures and 3.15%, 5.44%, 7.89% DSC higher than the previous best
results on MMWHS, CHAOS, Spleen with 10% annotations. Our code will be released
after acceptance.

</details>


### [159] [Advancing Video Anomaly Detection: A Bi-Directional Hybrid Framework for Enhanced Single- and Multi-Task Approaches](https://arxiv.org/abs/2504.14753)
*Guodong Shen,Yuqi Ouyang,Junru Lu,Yixuan Yang,Victor Sanchez*

Main category: cs.CV

TL;DR: 论文提出了一种混合框架，结合视觉Transformer和ConvLSTM，通过双向结构优化单任务框架，提升视频异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 观察到当前多任务方法中单任务框架的不足，认为优化单任务框架可以同时提升单任务和多任务方法的效果。

Method: 利用中间帧预测作为代理任务，设计双向结构整合视觉Transformer和ConvLSTM，通过卷积时间Transformer和层交互ConvLSTM桥增强预测。

Result: 在公开基准测试中验证了混合框架的有效性，无论是作为单任务还是多任务分支均表现优异。

Conclusion: 结合视觉Transformer和ConvLSTM的混合框架显著提升了视频异常检测的稳定性和准确性。

Abstract: Despite the prevailing transition from single-task to multi-task approaches
in video anomaly detection, we observe that many adopt sub-optimal frameworks
for individual proxy tasks. Motivated by this, we contend that optimizing
single-task frameworks can advance both single- and multi-task approaches.
Accordingly, we leverage middle-frame prediction as the primary proxy task, and
introduce an effective hybrid framework designed to generate accurate
predictions for normal frames and flawed predictions for abnormal frames. This
hybrid framework is built upon a bi-directional structure that seamlessly
integrates both vision transformers and ConvLSTMs. Specifically, we utilize
this bi-directional structure to fully analyze the temporal dimension by
predicting frames in both forward and backward directions, significantly
boosting the detection stability. Given the transformer's capacity to model
long-range contextual dependencies, we develop a convolutional temporal
transformer that efficiently associates feature maps from all context frames to
generate attention-based predictions for target frames. Furthermore, we devise
a layer-interactive ConvLSTM bridge that facilitates the smooth flow of
low-level features across layers and time-steps, thereby strengthening
predictions with fine details. Anomalies are eventually identified by
scrutinizing the discrepancies between target frames and their corresponding
predictions. Several experiments conducted on public benchmarks affirm the
efficacy of our hybrid framework, whether used as a standalone single-task
approach or integrated as a branch in a multi-task approach. These experiments
also underscore the advantages of merging vision transformers and ConvLSTMs for
video anomaly detection.

</details>


### [160] [How Effective Can Dropout Be in Multiple Instance Learning ?](https://arxiv.org/abs/2504.14783)
*Wenhui Zhu,Peijie Qiu,Xiwen Chen,Zhangsihao Yang,Aristeidis Sotiras,Abolfazl Razi,Yalin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种针对多实例学习（MIL）的新型dropout方法MIL-Dropout，通过丢弃包中最重要的实例来提升性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的MIL在WSI分类中采用两阶段训练，存在特征噪声和弱监督问题，而常用的dropout技术尚未在MIL中得到充分探索。

Method: 提出MIL-Dropout方法，系统性地决定丢弃哪些实例，尤其是包中最重要的实例。

Result: 在五个MIL基准数据集和两个WSI数据集上验证了MIL-Dropout的有效性，显著提升了性能且计算成本可忽略。

Conclusion: MIL-Dropout是一种简单高效的方法，能够显著提升MIL的性能和泛化能力。

Abstract: Multiple Instance Learning (MIL) is a popular weakly-supervised method for
various applications, with a particular interest in histological whole slide
image (WSI) classification. Due to the gigapixel resolution of WSI,
applications of MIL in WSI typically necessitate a two-stage training scheme:
first, extract features from the pre-trained backbone and then perform MIL
aggregation. However, it is well-known that this suboptimal training scheme
suffers from "noisy" feature embeddings from the backbone and inherent weak
supervision, hindering MIL from learning rich and generalizable features.
However, the most commonly used technique (i.e., dropout) for mitigating this
issue has yet to be explored in MIL. In this paper, we empirically explore how
effective the dropout can be in MIL. Interestingly, we observe that dropping
the top-k most important instances within a bag leads to better performance and
generalization even under noise attack. Based on this key observation, we
propose a novel MIL-specific dropout method, termed MIL-Dropout, which
systematically determines which instances to drop. Experiments on five MIL
benchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the
performance of current MIL methods with a negligible computational cost. The
code is available at https://github.com/ChongQingNoSubway/MILDropout.

</details>


### [161] [When Cloud Removal Meets Diffusion Model in Remote Sensing](https://arxiv.org/abs/2504.14785)
*Zhenyu Yu,Mohd Yamani Idna Idris,Pei Wang*

Main category: cs.CV

TL;DR: DC4CR是一种基于多模态扩散的新型云去除框架，通过提示驱动控制和低秩适应等技术，高效去除遥感图像中的薄云和厚云，无需预生成云掩码。


<details>
  <summary>Details</summary>
Motivation: 云遮挡严重阻碍遥感应用，传统方法依赖预生成云掩码且效率低，亟需高效、自适应的解决方案。

Method: 提出DC4CR框架，结合提示驱动控制、低秩适应、主题驱动生成和分组学习，实现高效云去除。

Result: 在RICE和CUHK-CR数据集上表现优异，实现最先进的云去除效果。

Conclusion: DC4CR为遥感图像处理提供了高效、可扩展的解决方案，具有广泛的实际应用前景。

Abstract: Cloud occlusion significantly hinders remote sensing applications by
obstructing surface information and complicating analysis. To address this, we
propose DC4CR (Diffusion Control for Cloud Removal), a novel multimodal
diffusion-based framework for cloud removal in remote sensing imagery. Our
method introduces prompt-driven control, allowing selective removal of thin and
thick clouds without relying on pre-generated cloud masks, thereby enhancing
preprocessing efficiency and model adaptability. Additionally, we integrate
low-rank adaptation for computational efficiency, subject-driven generation for
improved generalization, and grouped learning to enhance performance on small
datasets. Designed as a plug-and-play module, DC4CR seamlessly integrates into
existing cloud removal models, providing a scalable and robust solution.
Extensive experiments on the RICE and CUHK-CR datasets demonstrate
state-of-the-art performance, achieving superior cloud removal across diverse
conditions. This work presents a practical and efficient approach for remote
sensing image processing with broad real-world applications.

</details>


### [162] [Real-Time Sleepiness Detection for Driver State Monitoring System](https://arxiv.org/abs/2504.14807)
*Deepak Ghimire,Sunghwan Jeong,Sunhong Yoon,Sanghyun Park,Juhwan Choi*

Main category: cs.CV

TL;DR: 提出了一种实时驾驶员眼睛状态检测方法，结合动态模板匹配和卡尔曼滤波跟踪，利用SVM和HOG特征分类眼睛状态，以检测疲劳驾驶。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是许多事故的重要因素，实时监测眼睛状态可以有效预防疲劳驾驶。

Method: 1. 检测人脸并定位眼睛区域；2. 使用动态模板匹配和卡尔曼滤波跟踪眼睛位置；3. 结合SVM和HOG特征分类眼睛状态（睁开或闭合）。

Result: 系统能够实时检测眼睛状态，并在眼睛闭合超过设定时间时触发警报。

Conclusion: 该方法有效实现了驾驶员疲劳的实时监测，有助于减少疲劳驾驶引发的事故。

Abstract: A driver face monitoring system can detect driver fatigue, which is a
significant factor in many accidents, using computer vision techniques. In this
paper, we present a real-time technique for driver eye state detection. First,
the face is detected, and the eyes are located within the face region for
tracking. A normalized cross-correlation-based online dynamic template matching
technique, combined with Kalman filter tracking, is proposed to track the
detected eye positions in subsequent image frames. A support vector machine
with histogram of oriented gradients (HOG) features is used to classify the
state of the eyes as open or closed. If the eyes remain closed for a specified
period, the driver is considered to be asleep, and an alarm is triggered.

</details>


### [163] [ECViT: Efficient Convolutional Vision Transformer with Local-Attention and Multi-scale Stages](https://arxiv.org/abs/2504.14825)
*Zhoujie Qian*

Main category: cs.CV

TL;DR: ECViT是一种结合CNN和Transformer优势的混合架构，通过引入局部性和平移不变性等归纳偏置，解决了ViTs的高计算成本和数据需求问题。


<details>
  <summary>Details</summary>
Motivation: ViTs在计算机视觉中表现出色，但面临高计算成本和大量训练数据需求的挑战。

Method: 提出ECViT，通过从低级特征提取块并在编码器中加入卷积操作，结合局部注意力和金字塔结构，实现高效多尺度特征提取。

Result: ECViT在图像分类任务中性能优于现有模型，同时保持低计算和存储需求。

Conclusion: ECViT为高效且高性能的应用提供了理想解决方案。

Abstract: Vision Transformers (ViTs) have revolutionized computer vision by leveraging
self-attention to model long-range dependencies. However, ViTs face challenges
such as high computational costs due to the quadratic scaling of self-attention
and the requirement of a large amount of training data. To address these
limitations, we propose the Efficient Convolutional Vision Transformer (ECViT),
a hybrid architecture that effectively combines the strengths of CNNs and
Transformers. ECViT introduces inductive biases such as locality and
translation invariance, inherent to Convolutional Neural Networks (CNNs) into
the Transformer framework by extracting patches from low-level features and
enhancing the encoder with convolutional operations. Additionally, it
incorporates local-attention and a pyramid structure to enable efficient
multi-scale feature extraction and representation. Experimental results
demonstrate that ECViT achieves an optimal balance between performance and
efficiency, outperforming state-of-the-art models on various image
classification tasks while maintaining low computational and storage
requirements. ECViT offers an ideal solution for applications that prioritize
high efficiency without compromising performance.

</details>


### [164] [Distribution-aware Dataset Distillation for Efficient Image Restoration](https://arxiv.org/abs/2504.14826)
*Zhuoran Zheng,Xin Su,Chen Wu,Xiuyi Jia*

Main category: cs.CV

TL;DR: 论文提出了一种名为TripleD的数据集蒸馏方法，用于图像修复任务，通过预训练Transformer提取特征并选择子集，结合轻量级CNN调整分布，显著节省计算资源和时间。


<details>
  <summary>Details</summary>
Motivation: 随着图像数据的爆炸式增长，训练图像修复模型变得耗时且资源密集。当前的数据集蒸馏技术在图像修复领域尚未得到充分探索，因此需要一种高效的方法来解决这一问题。

Method: TripleD利用预训练的Vision Transformer提取图像特征以评估复杂度，选择子集后通过轻量级CNN调整特征分布。训练分为两个阶段：早期关注简单样本，后期选择复杂样本。

Result: TripleD在多项图像修复任务中表现优异，包括多任务、一体化及超高清图像修复，仅需一块消费级GPU在8小时内完成训练，节省500倍计算资源。

Conclusion: TripleD为图像修复领域提供了一种高效的数据集蒸馏方法，显著降低了训练成本和时间，具有广泛的应用潜力。

Abstract: With the exponential increase in image data, training an image restoration
model is laborious. Dataset distillation is a potential solution to this
problem, yet current distillation techniques are a blank canvas in the field of
image restoration. To fill this gap, we propose the Distribution-aware Dataset
Distillation method (TripleD), a new framework that extends the principles of
dataset distillation to image restoration. Specifically, TripleD uses a
pre-trained vision Transformer to extract features from images for complexity
evaluation, and the subset (the number of samples is much smaller than the
original training set) is selected based on complexity. The selected subset is
then fed through a lightweight CNN that fine-tunes the image distribution to
align with the distribution of the original dataset at the feature level. To
efficiently condense knowledge, the training is divided into two stages. Early
stages focus on simpler, low-complexity samples to build foundational
knowledge, while later stages select more complex and uncertain samples as the
model matures. Our method achieves promising performance on multiple image
restoration tasks, including multi-task image restoration, all-in-one image
restoration, and ultra-high-definition image restoration tasks. Note that we
can train a state-of-the-art image restoration model on an
ultra-high-definition (4K resolution) dataset using only one consumer-grade GPU
in less than 8 hours (500 savings in computing resources and immeasurable
training time).

</details>


### [165] [Reliable Multi-Modal Object Re-Identification via Modality-Aware Graph Reasoning](https://arxiv.org/abs/2504.14847)
*Xixi Wan,Aihua Zheng,Zi Wang,Bo Jiang,Jin Tang,Jixin Ma*

Main category: cs.CV

TL;DR: 提出了一种名为MGRNet的图推理模型，用于解决多模态ReID任务中局部特征质量差异和跨模态互补信息利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视局部特征质量差异，未能充分利用跨模态互补信息，尤其是在低质量特征情况下。

Method: 构建模态感知图以增强细粒度局部细节提取，采用选择性图节点交换操作缓解低质量特征影响，并通过局部感知图推理模块传播多模态信息。

Result: 在四个基准测试中（RGBNT201、Market1501-MM、RGBNT100、MSVR310），MGRNet取得了最先进的性能。

Conclusion: MGRNet通过图推理有效提升了多模态ReID任务的性能，并能重建缺失模态信息。

Abstract: Multi-modal data provides abundant and diverse object information, crucial
for effective modal interactions in Re-Identification (ReID) tasks. However,
existing approaches often overlook the quality variations in local features and
fail to fully leverage the complementary information across modalities,
particularly in the case of low-quality features. In this paper, we propose to
address this issue by leveraging a novel graph reasoning model, termed the
Modality-aware Graph Reasoning Network (MGRNet). Specifically, we first
construct modality-aware graphs to enhance the extraction of fine-grained local
details by effectively capturing and modeling the relationships between
patches. Subsequently, the selective graph nodes swap operation is employed to
alleviate the adverse effects of low-quality local features by considering both
local and global information, enhancing the representation of discriminative
information. Finally, the swapped modality-aware graphs are fed into the
local-aware graph reasoning module, which propagates multi-modal information to
yield a reliable feature representation. Another advantage of the proposed
graph reasoning approach is its ability to reconstruct missing modal
information by exploiting inherent structural relationships, thereby minimizing
disparities between different modalities. Experimental results on four
benchmarks (RGBNT201, Market1501-MM, RGBNT100, MSVR310) indicate that the
proposed method achieves state-of-the-art performance in multi-modal object
ReID. The code for our method will be available upon acceptance.

</details>


### [166] [Object-Level Verbalized Confidence Calibration in Vision-Language Models via Semantic Perturbation](https://arxiv.org/abs/2504.14848)
*Yunpu Zhao,Rui Zhang,Junbin Xiao,Ruibo Hou,Jiaming Guo,Zihao Zhang,Yifan Hao,Yunji Chen*

Main category: cs.CV

TL;DR: 提出了一种名为CSP的新框架，通过语义扰动校准视觉语言模型的置信度，显著提升了其置信度与响应正确性的一致性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在多模态任务中表现优异，但其置信度校准不佳，导致用户信任度降低。

Method: 通过高斯噪声扰动关键对象区域模拟视觉不确定性，建立视觉模糊性与置信度的映射，并结合两阶段训练（监督微调和偏好优化）提升校准效果。

Result: 在多个基准测试中，该方法显著改善了置信度与响应正确性的一致性，同时保持或提升了任务性能。

Conclusion: 语义扰动是一种提升VLM可靠性和可解释性的实用工具。

Abstract: Vision-language models (VLMs) excel in various multimodal tasks but
frequently suffer from poor calibration, resulting in misalignment between
their verbalized confidence and response correctness. This miscalibration
undermines user trust, especially when models confidently provide incorrect or
fabricated information. In this work, we propose a novel Confidence Calibration
through Semantic Perturbation (CSP) framework to improve the calibration of
verbalized confidence for VLMs in response to object-centric queries. We first
introduce a perturbed dataset where Gaussian noise is applied to the key object
regions to simulate visual uncertainty at different confidence levels,
establishing an explicit mapping between visual ambiguity and confidence
levels. We further enhance calibration through a two-stage training process
combining supervised fine-tuning on the perturbed dataset with subsequent
preference optimization. Extensive experiments on popular benchmarks
demonstrate that our method significantly improves the alignment between
verbalized confidence and response correctness while maintaining or enhancing
overall task performance. These results highlight the potential of semantic
perturbation as a practical tool for improving the reliability and
interpretability of VLMs.

</details>


### [167] [Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer](https://arxiv.org/abs/2504.14860)
*Ziyi Liu,Yangcen Liu*

Main category: cs.CV

TL;DR: PseudoFormer提出了一种双分支框架，通过生成高质量伪标签和利用不同先验，缩小了弱监督与全监督时间动作定位的性能差距。


<details>
  <summary>Details</summary>
Motivation: 弱监督时间动作定位（WTAL）因缺乏时间标注，性能与全监督方法存在差距，且伪标签训练中存在三大挑战：高质量伪标签生成、不同先验的充分利用及噪声标签的训练优化。

Method: 提出PseudoFormer框架，包括RickerFusion生成高质量伪标签，利用片段级和提案级标签训练回归模型，并通过不确定性掩码和迭代优化机制处理噪声标签。

Result: 在THUMOS14和ActivityNet1.3基准测试中达到最优性能，并通过消融实验验证各组件贡献。

Conclusion: PseudoFormer通过创新设计有效解决了WTAL的关键挑战，显著提升了性能。

Abstract: Weakly-supervised Temporal Action Localization (WTAL) has achieved notable
success but still suffers from a lack of temporal annotations, leading to a
performance and framework gap compared with fully-supervised methods. While
recent approaches employ pseudo labels for training, three key challenges:
generating high-quality pseudo labels, making full use of different priors, and
optimizing training methods with noisy labels remain unresolved. Due to these
perspectives, we propose PseudoFormer, a novel two-branch framework that
bridges the gap between weakly and fully-supervised Temporal Action
Localization (TAL). We first introduce RickerFusion, which maps all predicted
action proposals to a global shared space to generate pseudo labels with better
quality. Subsequently, we leverage both snippet-level and proposal-level labels
with different priors from the weak branch to train the regression-based model
in the full branch. Finally, the uncertainty mask and iterative refinement
mechanism are applied for training with noisy pseudo labels. PseudoFormer
achieves state-of-the-art WTAL results on the two commonly used benchmarks,
THUMOS14 and ActivityNet1.3. Besides, extensive ablation studies demonstrate
the contribution of each component of our method.

</details>


### [168] [Twin Co-Adaptive Dialogue for Progressive Image Generation](https://arxiv.org/abs/2504.14868)
*Jianhui Wang,Yangfan He,Yan Zhong,Xinyuan Song,Jiayi Su,Yuheng Feng,Hongyang He,Wenyu Zhu,Xinhang Yuan,Kuan Lu,Menghao Huo,Miao Zhang,Keqin Li,Jiaqi Chen,Tianyu Shi,Xueqian Wang*

Main category: cs.CV

TL;DR: Twin-Co是一个通过动态对话逐步优化图像生成的框架，减少用户试错并提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像生成系统在处理用户提示模糊性时的不足。

Method: 采用同步、协同适应的对话流程，通过迭代对话和用户反馈逐步优化图像。

Result: 实验表明Twin-Co提升了用户体验和生成图像质量。

Conclusion: Twin-Co通过动态对话有效解决了用户意图对齐问题，优化了生成流程。

Abstract: Modern text-to-image generation systems have enabled the creation of
remarkably realistic and high-quality visuals, yet they often falter when
handling the inherent ambiguities in user prompts. In this work, we present
Twin-Co, a framework that leverages synchronized, co-adaptive dialogue to
progressively refine image generation. Instead of a static generation process,
Twin-Co employs a dynamic, iterative workflow where an intelligent dialogue
agent continuously interacts with the user. Initially, a base image is
generated from the user's prompt. Then, through a series of synchronized
dialogue exchanges, the system adapts and optimizes the image according to
evolving user feedback. The co-adaptive process allows the system to
progressively narrow down ambiguities and better align with user intent.
Experiments demonstrate that Twin-Co not only enhances user experience by
reducing trial-and-error iterations but also improves the quality of the
generated images, streamlining the creative process across various
applications.

</details>


### [169] [ReSpec: Relevance and Specificity Grounded Online Filtering for Learning on Video-Text Data Streams](https://arxiv.org/abs/2504.14875)
*Chris Dongjoo Kim,Jihwan Moon,Sangwoo Moon,Heeseung Yun,Sihaeng Lee,Aniruddha Kembhavi,Soonyoung Lee,Gunhee Kim,Sangho Lee,Christopher Clark*

Main category: cs.CV

TL;DR: ReSpec是一种基于相关性和特异性的在线过滤框架，用于高效处理视频-文本数据，仅需5%数据即可实现最先进的零样本视频检索性能。


<details>
  <summary>Details</summary>
Motivation: 视频-文本数据的快速增长带来了存储和计算挑战，需要实时处理和高效学习策略。

Method: 提出ReSpec框架，通过模态对齐、任务相关性、特异性和效率四个标准实时过滤数据。

Result: 在WebVid2M和VideoCC3M数据集上，ReSpec仅用5%数据即在零样本视频检索任务中达到最优性能。

Conclusion: ReSpec通过实时数据过滤显著降低了存储和计算需求，同时保持了高性能。

Abstract: The rapid growth of video-text data presents challenges in storage and
computation during training. Online learning, which processes streaming data in
real-time, offers a promising solution to these issues while also allowing
swift adaptations in scenarios demanding real-time responsiveness. One strategy
to enhance the efficiency and effectiveness of learning involves identifying
and prioritizing data that enhances performance on target downstream tasks. We
propose Relevance and Specificity-based online filtering framework (ReSpec)
that selects data based on four criteria: (i) modality alignment for clean
data, (ii) task relevance for target focused data, (iii) specificity for
informative and detailed data, and (iv) efficiency for low-latency processing.
Relevance is determined by the probabilistic alignment of incoming data with
downstream tasks, while specificity employs the distance to a root embedding
representing the least specific data as an efficient proxy for informativeness.
By establishing reference points from target task data, ReSpec filters incoming
data in real-time, eliminating the need for extensive storage and compute.
Evaluating on large-scale datasets WebVid2M and VideoCC3M, ReSpec attains
state-of-the-art performance on five zeroshot video retrieval tasks, using as
little as 5% of the data while incurring minimal compute. The source code is
available at https://github.com/cdjkim/ReSpec.

</details>


### [170] [Collaborative Enhancement Network for Low-quality Multi-spectral Vehicle Re-identification](https://arxiv.org/abs/2504.14877)
*Aihua Zheng,Yongqi Sun,Zi Wang,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 论文提出了一种协作增强网络（CoEN），通过生成高质量代理并动态选择主光谱，提升多光谱车辆重识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖主光谱增强低质量光谱数据，但主光谱选择困难且低质量主光谱会限制性能。

Method: 设计了代理生成器（PG）整合多光谱特征，动态质量排序模块（DQSM）选择主光谱，协作增强模块（CEM）补偿缺失内容。

Result: 在三个基准数据集上的实验验证了CoEN优于其他多光谱车辆重识别方法。

Conclusion: CoEN通过协作增强和动态主光谱选择，显著提升了多光谱车辆重识别的鲁棒性。

Abstract: The performance of multi-spectral vehicle Re-identification (ReID) is
significantly degraded when some important discriminative cues in visible, near
infrared and thermal infrared spectra are lost. Existing methods generate or
enhance missing details in low-quality spectra data using the high-quality one,
generally called the primary spectrum, but how to justify the primary spectrum
is a challenging problem. In addition, when the quality of the primary spectrum
is low, the enhancement effect would be greatly degraded, thus limiting the
performance of multi-spectral vehicle ReID. To address these problems, we
propose the Collaborative Enhancement Network (CoEN), which generates a
high-quality proxy from all spectra data and leverages it to supervise the
selection of primary spectrum and enhance all spectra features in a
collaborative manner, for robust multi-spectral vehicle ReID. First, to
integrate the rich cues from all spectra data, we design the Proxy Generator
(PG) to progressively aggregate multi-spectral features. Second, we design the
Dynamic Quality Sort Module (DQSM), which sorts all spectra data by measuring
their correlations with the proxy, to accurately select the primary spectra
with the highest correlation. Finally, we design the Collaborative Enhancement
Module (CEM) to effectively compensate for missing contents of all spectra by
collaborating the primary spectra and the proxy, thereby mitigating the impact
of low-quality primary spectra. Extensive experiments on three benchmark
datasets are conducted to validate the efficacy of the proposed approach
against other multi-spectral vehicle ReID methods. The codes will be released
at https://github.com/yongqisun/CoEN.

</details>


### [171] [Memory-Augmented Dual-Decoder Networks for Multi-Class Unsupervised Anomaly Detection](https://arxiv.org/abs/2504.14884)
*Jingyu Xing,Chenwei Tang,Tao Wang,Rong Xiao,Wei Ju,Ji-Zhe Zhou,Liangli Zhen,Jiancheng Lv*

Main category: cs.CV

TL;DR: 论文提出了一种记忆增强的双解码器网络（MDD-Net），用于解决多类无监督异常检测中的过泛化和正常特征重建不足问题。


<details>
  <summary>Details</summary>
Motivation: 多类场景下，重建方法面临过泛化和正常特征重建不足的挑战，现有方法通常只解决前者，反而加剧后者。

Method: MDD-Net包含双解码器反向蒸馏网络（DRD-Net）和类感知记忆模块（CMM），通过双解码器差异和记忆模块提升性能。

Result: 实验表明，MDD-Net在多类无监督异常检测任务中优于现有方法。

Conclusion: MDD-Net有效解决了过泛化和正常特征重建不足问题，提升了异常检测的准确性。

Abstract: Recent advances in unsupervised anomaly detection (UAD) have shifted from
single-class to multi-class scenarios. In such complex contexts, the increasing
pattern diversity has brought two challenges to reconstruction-based
approaches: (1) over-generalization: anomalies that are subtle or share
compositional similarities with normal patterns may be reconstructed with high
fidelity, making them difficult to distinguish from normal instances; and (2)
insufficient normality reconstruction: complex normal features, such as
intricate textures or fine-grained structures, may not be faithfully
reconstructed due to the model's limited representational capacity, resulting
in false positives. Existing methods typically focus on addressing the former,
which unintentionally exacerbate the latter, resulting in inadequate
representation of intricate normal patterns. To concurrently address these two
challenges, we propose a Memory-augmented Dual-Decoder Networks (MDD-Net). This
network includes two critical components: a Dual-Decoder Reverse Distillation
Network (DRD-Net) and a Class-aware Memory Module (CMM). Specifically, the
DRD-Net incorporates a restoration decoder designed to recover normal features
from synthetic abnormal inputs and an identity decoder to reconstruct features
that maintain the anomalous semantics. By exploiting the discrepancy between
features produced by two decoders, our approach refines anomaly scores beyond
the conventional encoder-decoder comparison paradigm, effectively reducing
false positives and enhancing localization accuracy. Furthermore, the CMM
explicitly encodes and preserves class-specific normal prototypes, actively
steering the network away from anomaly reconstruction. Comprehensive
experimental results across several benchmarks demonstrate the superior
performance of our MDD-Net framework over current SoTA approaches in
multi-class UAD tasks.

</details>


### [172] [WMKA-Net: A Weighted Multi-Kernel Attention NetworkMethod for Retinal Vessel Segmentation](https://arxiv.org/abs/2504.14888)
*Xinran Xu,Yuliang Ma,Sifu Cai*

Main category: cs.CV

TL;DR: 提出了一种新型视网膜血管分割网络WMKA-Net，通过多尺度特征融合、加权策略和注意力机制，显著提升了小血管和低对比度区域的分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决视网膜血管分割中多尺度特征捕捉不足、上下文信息丢失和噪声敏感性问题。

Method: 结合MKDC模块（多尺度卷积核提取特征）、UDFF策略（加权融合高低层特征）和AttentionBlock（注意力机制抑制噪声）。

Result: 在多个公开数据集上表现优异，尤其在小血管和病理区域分割中效果显著。

Conclusion: WMKA-Net为视网膜血管分割提供了一种高效且鲁棒的新方法。

Abstract: We propose a novel retinal vessel segmentation network, the Weighted
Multi-Kernel Attention Network (WMKA-Net), which aims to address the issues of
insufficient multiscale feature capture, loss of contextual information, and
noise sensitivity in retinal vessel segmentation. WMKA-Net significantly
improves the segmentation performance of small vessels and low-contrast regions
by integrating several innovative components, including the MultiKernelFeature
Fusion Module (MKDC), the Progressive Feature Weighting Fusion Strategy (UDFF),
and the Attention Mechanism Module (AttentionBlock). The MKDC module employs
multiscale parallel convolutional kernels to extract vessel characteristics,
thereby enhancing the ability to capture complex vascular structures. The UDFF
strategy optimizes the transmission of feature information by weighted fusion
of high- and low-level features. The AttentionBlock highlights key regions and
suppresses noise interference through the attention mechanism. Experimental
results demonstrate that WMKA-Net achieves excellent segmentation performance
in multiple public datasets, particularly in segmentation of small vessels and
processing of pathological regions. This work provides a robust and efficient
new method for segmentation of the retinal vessel.

</details>


### [173] [Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation](https://arxiv.org/abs/2504.14899)
*Chenjie Cao,Jingkai Zhou,Shikai Li,Jingyun Liang,Chaohui Yu,Fan Wang,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: Uni3C是一个统一的3D增强框架，用于视频生成中相机和人体运动的精确控制，通过点云和视频基础模型的结合，实现了灵活性和高质量生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常单独处理相机和人体运动控制，且高质量标注数据有限，因此需要一种统一且灵活的方法。

Method: 提出Uni3C框架，包括基于点云的PCDController模块和联合对齐的3D世界引导，分别控制相机和人体运动。

Result: 实验表明PCDController在相机运动控制中表现鲁棒，Uni3C在相机可控性和人体运动质量上优于竞争对手。

Conclusion: Uni3C通过统一框架和灵活训练模块，显著提升了视频生成中相机和人体运动的控制能力。

Abstract: Camera and human motion controls have been extensively studied for video
generation, but existing approaches typically address them separately,
suffering from limited data with high-quality annotations for both aspects. To
overcome this, we present Uni3C, a unified 3D-enhanced framework for precise
control of both camera and human motion in video generation. Uni3C includes two
key contributions. First, we propose a plug-and-play control module trained
with a frozen video generative backbone, PCDController, which utilizes
unprojected point clouds from monocular depth to achieve accurate camera
control. By leveraging the strong 3D priors of point clouds and the powerful
capacities of video foundational models, PCDController shows impressive
generalization, performing well regardless of whether the inference backbone is
frozen or fine-tuned. This flexibility enables different modules of Uni3C to be
trained in specific domains, i.e., either camera control or human motion
control, reducing the dependency on jointly annotated data. Second, we propose
a jointly aligned 3D world guidance for the inference phase that seamlessly
integrates both scenic point clouds and SMPL-X characters to unify the control
signals for camera and human motion, respectively. Extensive experiments
confirm that PCDController enjoys strong robustness in driving camera motion
for fine-tuned backbones of video generation. Uni3C substantially outperforms
competitors in both camera controllability and human motion quality.
Additionally, we collect tailored validation sets featuring challenging camera
movements and human actions to validate the effectiveness of our method.

</details>


### [174] [Guidelines for External Disturbance Factors in the Use of OCR in Real-World Environments](https://arxiv.org/abs/2504.14913)
*Kenji Iwata,Eiki Ishidera,Toshifumi Yamaai,Yutaka Satoh,Hiroshi Tanaka,Katsuhiko Takahashi,Akio Furuhata,Yoshihisa Tanabe,Hiroshi Matsumura*

Main category: cs.CV

TL;DR: 论文总结了影响OCR性能的外部干扰因素，并整理了相关指南以帮助用户正确使用OCR。


<details>
  <summary>Details</summary>
Motivation: 随着OCR应用范围的扩大，外部环境干扰可能导致性能下降，影响识别精度，因此需要系统化的解决方案。

Method: 通过整理真实世界中的外部干扰因素及其导致的图像退化现象，制作了外部干扰因素表，并形成使用指南。

Result: 提出了一个系统化的外部干扰因素表和指南，帮助用户优化OCR使用环境。

Conclusion: 通过整理干扰因素和提供指南，可以有效提升OCR在实际应用中的性能和可靠性。

Abstract: The performance of OCR has improved with the evolution of AI technology. As
OCR continues to broaden its range of applications, the increased likelihood of
interference introduced by various usage environments can prevent it from
achieving its inherent performance. This results in reduced recognition
accuracy under certain conditions, and makes the quality control of recognition
devices more challenging. Therefore, to ensure that users can properly utilize
OCR, we compiled the real-world external disturbance factors that cause
performance degradation, along with the resulting image degradation phenomena,
into an external disturbance factor table and, by also indicating how to make
use of it, organized them into guidelines.

</details>


### [175] [GenCLIP: Generalizing CLIP Prompts for Zero-shot Anomaly Detection](https://arxiv.org/abs/2504.14919)
*Donghyeong Kim,Chaewon Park,Suhwan Cho,Hyeonjeong Lim,Minseok Kang,Jungho Lee,Sangyoun Lee*

Main category: cs.CV

TL;DR: GenCLIP提出了一种通过多层提示和双分支推理更有效地学习和利用通用提示的框架，以提升零样本异常检测的稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 零样本异常检测（ZSAD）的关键挑战在于如何稳定地学习通用提示并有效利用它们，同时保持泛化性和类别特异性。

Method: GenCLIP采用多层提示集成不同CLIP层的类别特定视觉线索，并通过双分支推理策略平衡特异性和泛化性。

Result: 该方法通过自适应文本提示过滤机制和多层视觉特征结合，显著提升了异常检测的稳定性和可靠性。

Conclusion: GenCLIP通过创新的提示学习和推理策略，为ZSAD提供了一种高效且鲁棒的解决方案。

Abstract: Zero-shot anomaly detection (ZSAD) aims to identify anomalies in unseen
categories by leveraging CLIP's zero-shot capabilities to match text prompts
with visual features. A key challenge in ZSAD is learning general prompts
stably and utilizing them effectively, while maintaining both generalizability
and category specificity. Although general prompts have been explored in prior
works, achieving their stable optimization and effective deployment remains a
significant challenge. In this work, we propose GenCLIP, a novel framework that
learns and leverages general prompts more effectively through multi-layer
prompting and dual-branch inference. Multi-layer prompting integrates
category-specific visual cues from different CLIP layers, enriching general
prompts with more comprehensive and robust feature representations. By
combining general prompts with multi-layer visual features, our method further
enhances its generalization capability. To balance specificity and
generalization, we introduce a dual-branch inference strategy, where a
vision-enhanced branch captures fine-grained category-specific features, while
a query-only branch prioritizes generalization. The complementary outputs from
both branches improve the stability and reliability of anomaly detection across
unseen categories. Additionally, we propose an adaptive text prompt filtering
mechanism, which removes irrelevant or atypical class names not encountered
during CLIP's training, ensuring that only meaningful textual inputs contribute
to the final vision-language alignment.

</details>


### [176] [DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in Fine-Grained Visual Understanding](https://arxiv.org/abs/2504.14920)
*Geng Li,Jinglin Xu,Yunzhen Zhao,Yuxin Peng*

Main category: cs.CV

TL;DR: Dyfo是一种无需训练的视觉搜索方法，通过双向交互和MCTS算法模拟人类视觉聚焦，提升多模态模型的细粒度视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 受人类视觉搜索机制启发，旨在解决现有方法需要额外模块或数据收集的问题。

Method: 利用双向交互和MCTS算法动态调整视觉聚焦，无需额外训练。

Result: 显著提升细粒度视觉理解，减少幻觉问题，在固定和动态分辨率模型中表现优异。

Conclusion: Dyfo为多模态模型提供了一种高效、无需训练的视觉搜索解决方案。

Abstract: Humans can effortlessly locate desired objects in cluttered environments,
relying on a cognitive mechanism known as visual search to efficiently filter
out irrelevant information and focus on task-related regions. Inspired by this
process, we propose Dyfo (Dynamic Focus), a training-free dynamic focusing
visual search method that enhances fine-grained visual understanding in large
multimodal models (LMMs). Unlike existing approaches which require additional
modules or data collection, Dyfo leverages a bidirectional interaction between
LMMs and visual experts, using a Monte Carlo Tree Search (MCTS) algorithm to
simulate human-like focus adjustments. This enables LMMs to focus on key visual
regions while filtering out irrelevant content, without introducing additional
training caused by vocabulary expansion or the integration of specialized
localization modules. Experimental results demonstrate that Dyfo significantly
improves fine-grained visual understanding and reduces hallucination issues in
LMMs, achieving superior performance across both fixed and dynamic resolution
models. The code is available at https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025

</details>


### [177] [Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos](https://arxiv.org/abs/2504.14921)
*Songping Wang,Hanqing Liu,Yueming Lyu,Xiantao Hu,Ziwen He,Wei Wang,Caifeng Shan,Liang Wang*

Main category: cs.CV

TL;DR: VFAT-WS是一种快速对抗训练方法，通过时间频率增强和弱到强一致性正则化，显著提升视频模型的对抗鲁棒性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有视频对抗训练方法计算成本高且难以平衡干净准确性和对抗鲁棒性，VFAT-WS旨在解决这些问题。

Method: 结合时间频率增强（TF-AUG）和空间-时间增强（STF-AUG），采用单步PGD攻击和弱到强一致性正则化。

Result: 在UCF-101和HMDB-51数据集上，VFAT-WS显著提升对抗鲁棒性和训练速度（加速490%）。

Conclusion: VFAT-WS为视频对抗训练提供了高效且鲁棒的解决方案，平衡了准确性和鲁棒性。

Abstract: Adversarial Training (AT) has been shown to significantly enhance adversarial
robustness via a min-max optimization approach. However, its effectiveness in
video recognition tasks is hampered by two main challenges. First, fast
adversarial training for video models remains largely unexplored, which
severely impedes its practical applications. Specifically, most video
adversarial training methods are computationally costly, with long training
times and high expenses. Second, existing methods struggle with the trade-off
between clean accuracy and adversarial robustness. To address these challenges,
we introduce Video Fast Adversarial Training with Weak-to-Strong consistency
(VFAT-WS), the first fast adversarial training method for video data.
Specifically, VFAT-WS incorporates the following key designs: First, it
integrates a straightforward yet effective temporal frequency augmentation
(TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a
single-step PGD attack to boost training efficiency and robustness. Second, it
devises a weak-to-strong spatial-temporal consistency regularization, which
seamlessly integrates the simpler TF-AUG and the more complex STF-AUG.
Leveraging the consistency regularization, it steers the learning process from
simple to complex augmentations. Both of them work together to achieve a better
trade-off between clean accuracy and robustness. Extensive experiments on
UCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that
VFAT-WS achieves great improvements in adversarial robustness and corruption
robustness, while accelerating training by nearly 490%.

</details>


### [178] [TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models](https://arxiv.org/abs/2504.14933)
*Mazharul Islam Rakib,Showrin Rahman,Joyanta Jyoti Mondal,Xi Xiao,David Lewis,Alessandra Mileo,Meem Arafat Manab*

Main category: cs.CV

TL;DR: 提出了一种基于条件扩散模型的两步图像生成方法，通过生成图像分割掩码并避免特定形状，减少与训练图像的结构相似性，从而避免版权侵权和源复制问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型在图像生成中可能直接复制受版权保护的图像，传统保护措施（如水印）无效，需一种低成本解决方案。

Method: 采用两步法：首先生成图像分割掩码，再通过扩散模型重新生成图像时避免该形状。

Result: 减少了与训练图像的结构相似性，有效避免源复制问题，且无需昂贵重新训练或用户干预。

Conclusion: 该方法为基于扩散模型的图像生成提供了一种计算成本低、避免版权侵权的有效解决方案。

Abstract: In today's age of social media and marketing, copyright issues can be a major
roadblock to the free sharing of images. Generative AI models have made it
possible to create high-quality images, but concerns about copyright
infringement are a hindrance to their abundant use. As these models use data
from training images to generate new ones, it is often a daunting task to
ensure they do not violate intellectual property rights. Some AI models have
even been noted to directly copy copyrighted images, a problem often referred
to as source copying. Traditional copyright protection measures such as
watermarks and metadata have also proven to be futile in this regard. To
address this issue, we propose a novel two-step image generation model inspired
by the conditional diffusion model. The first step involves creating an image
segmentation mask for some prompt-based generated images. This mask embodies
the shape of the image. Thereafter, the diffusion model is asked to generate
the image anew while avoiding the shape in question. This approach shows a
decrease in structural similarity from the training image, i.e. we are able to
avoid the source copying problem using this approach without expensive
retraining of the model or user-centered prompt generation techniques. This
makes our approach the most computationally inexpensive approach to avoiding
both copyright infringement and source copying for diffusion model-based image
generation.

</details>


### [179] [PIV-FlowDiffuser:Transfer-learning-based denoising diffusion models for PIV](https://arxiv.org/abs/2504.14952)
*Qianyu Zhu,Junjie Wang,Jeremiah Hu,Jia Ai,Yong Lee*

Main category: cs.CV

TL;DR: 本文提出了一种基于迁移学习的去噪扩散模型（PIV-FlowDiffuser），用于减少深度学习粒子图像测速（PIV）中的特殊噪声模式，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在合成数据集上训练后，在实际粒子图像上可能因领域差异表现不佳，导致特殊噪声模式。

Method: 采用去噪扩散模型（FlowDiffuser），通过迁移学习策略训练：1）在计算机视觉光学流数据集（如Sintel、KITTI）上预训练；2）在合成PIV数据集上微调。

Result: PIV-FlowDiffuser有效抑制噪声，平均终点误差（AEE）比基线RAFT256-PIV降低59.4%，并在未见数据上表现更优。

Conclusion: 研究证明了基于迁移学习的去噪扩散模型在PIV中的有效性，并提供了详细实现。

Abstract: Deep learning algorithms have significantly reduced the computational time
and improved the spatial resolution of particle image velocimetry~(PIV).
However, the models trained on synthetic datasets might have a degraded
performance on practical particle images due to domain gaps. As a result,
special residual patterns are often observed for the vector fields of deep
learning-based estimators. To reduce the special noise step-by-step, we employ
a denoising diffusion model~(FlowDiffuser) for PIV analysis. And the
data-hungry iterative denoising diffusion model is trained via a transfer
learning strategy, resulting in our PIV-FlowDiffuser method. Specifically, (1)
pre-training a FlowDiffuser model with multiple optical flow datasets of the
computer vision community, such as Sintel, KITTI, etc; (2) fine-tuning the
pre-trained model on synthetic PIV datasets. Note that the PIV images are
upsampled by a factor of two to resolve the small-scale turbulent flow
structures. The visualized results indicate that our PIV-FlowDiffuser
effectively suppresses the noise patterns. Therefore, the denoising diffusion
model reduces the average end-point error~($AEE$) by 59.4% over RAFT256-PIV
baseline on the classic Cai's dataset. Besides, PIV-FlowDiffuser exhibits
enhanced generalization performance on unseen particle images due to transfer
learning. Overall, this study highlights the transfer-learning-based denoising
diffusion models for PIV. And a detailed implementation is recommended for
interested readers in the repository
https://github.com/Zhu-Qianyu/PIV-FlowDiffuser.

</details>


### [180] [3D Gaussian Head Avatars with Expressive Dynamic Appearances by Compact Tensorial Representations](https://arxiv.org/abs/2504.14967)
*Yating Wang,Xuan Wang,Ran Yi,Yanbo Fan,Jichen Hu,Jingcheng Zhu,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出了一种结合3D高斯和3DMM的新方法，通过紧凑的张量表示和动态纹理编码，实现了高质量3D头部虚拟形象的实时渲染和低存储开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态纹理捕捉或运行时效率上存在不足，无法同时满足高质量和高效需求。

Method: 采用张量格式编码纹理属性，静态三平面存储中性表情，动态纹理细节通过轻量级1D特征线表示，并结合自适应截断不透明度惩罚和类别平衡采样。

Result: 实验表明，该方法能准确捕捉面部动态细节，保持实时渲染，并显著降低存储成本。

Conclusion: 该方法在动态纹理捕捉和效率上取得平衡，扩展了3D头部虚拟形象的应用场景。

Abstract: Recent studies have combined 3D Gaussian and 3D Morphable Models (3DMM) to
construct high-quality 3D head avatars. In this line of research, existing
methods either fail to capture the dynamic textures or incur significant
overhead in terms of runtime speed or storage space. To this end, we propose a
novel method that addresses all the aforementioned demands. In specific, we
introduce an expressive and compact representation that encodes texture-related
attributes of the 3D Gaussians in the tensorial format. We store appearance of
neutral expression in static tri-planes, and represents dynamic texture details
for different expressions using lightweight 1D feature lines, which are then
decoded into opacity offset relative to the neutral face. We further propose
adaptive truncated opacity penalty and class-balanced sampling to improve
generalization across different expressions. Experiments show this design
enables accurate face dynamic details capturing while maintains real-time
rendering and significantly reduces storage costs, thus broadening the
applicability to more scenarios.

</details>


### [181] [Cyc3D: Fine-grained Controllable 3D Generation via Cycle Consistency Regularization](https://arxiv.org/abs/2504.14975)
*Hongbin Xu,Chaohui Yu,Feng Xiao,Jiazheng Xing,Hai Ci,Weitao Chen,Ming Li*

Main category: cs.CV

TL;DR: 提出了一种名为\name{}的新框架，通过循环一致性增强可控3D生成，显著提升输入条件与生成内容的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D生成中难以保持输入条件（如边缘和深度）与生成内容的一致性，导致明显差异。

Method: 采用高效前馈主干网络，通过循环过程（两次一致性约束：视图一致性和条件一致性）生成3D内容。

Result: 在多个基准测试中表现优异，边缘和草图条件的PSNR分别提升14.17%和6.26%。

Conclusion: \name{}显著提升了可控3D生成的精度，尤其在细粒度细节上优于现有方法。

Abstract: Despite the remarkable progress of 3D generation, achieving controllability,
i.e., ensuring consistency between generated 3D content and input conditions
like edge and depth, remains a significant challenge. Existing methods often
struggle to maintain accurate alignment, leading to noticeable discrepancies.
To address this issue, we propose \name{}, a new framework that enhances
controllable 3D generation by explicitly encouraging cyclic consistency between
the second-order 3D content, generated based on extracted signals from the
first-order generation, and its original input controls. Specifically, we
employ an efficient feed-forward backbone that can generate a 3D object from an
input condition and a text prompt. Given an initial viewpoint and a control
signal, a novel view is rendered from the generated 3D content, from which the
extracted condition is used to regenerate the 3D content. This re-generated
output is then rendered back to the initial viewpoint, followed by another
round of control signal extraction, forming a cyclic process with two
consistency constraints. \emph{View consistency} ensures coherence between the
two generated 3D objects, measured by semantic similarity to accommodate
generative diversity. \emph{Condition consistency} aligns the final extracted
signal with the original input control, preserving structural or geometric
details throughout the process. Extensive experiments on popular benchmarks
demonstrate that \name{} significantly improves controllability, especially for
fine-grained details, outperforming existing methods across various conditions
(e.g., +14.17\% PSNR for edge, +6.26\% PSNR for sketch).

</details>


### [182] [An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes](https://arxiv.org/abs/2504.15270)
*Ji Qi,Yuan Yao,Yushi Bai,Bin Xu,Juanzi Li,Zhiyuan Liu,Tat-Seng Chua*

Main category: cs.CV

TL;DR: Quicksviewer是一种大型多模态模型（LMM），通过动态分区和统一重采样视频帧，显著提高了视频理解的效率，减少了时空冗余。


<details>
  <summary>Details</summary>
Motivation: 传统LMM对视频帧的均匀感知导致计算效率低下，尤其是对于时间信息密度不均的视频。

Method: 使用Gumbel Softmax将视频分区为不同密度的立方体，并对每个立方体进行统一重采样，实现高效视频理解。

Result: 模型在45倍压缩率下显著减少冗余，训练效率高，性能优于固定分区策略的基线模型（最高提升8.72%准确率）。

Conclusion: Quicksviewer通过动态分区和高效训练，实现了视频理解的显著性能提升，并展示了输入帧数增加时的能力扩展潜力。

Abstract: Large Multimodal Models (LMMs) uniformly perceive video frames, creating
computational inefficiency for videos with inherently varying temporal
information density. This paper present \textbf{Quicksviewer}, an LMM with new
perceiving paradigm that partitions a video of nonuniform density into varying
cubes using Gumbel Softmax, followed by a unified resampling for each cube to
achieve efficient video understanding. This simple and intuitive approach
dynamically compress video online based on its temporal density, significantly
reducing spatiotemporal redundancy (overall 45$\times$ compression rate), while
enabling efficient training with large receptive field. We train the model from
a language backbone through three progressive stages, each incorporating
lengthy videos on average of 420s/1fps thanks to the perceiving efficiency.
With only 0.8M total video-text samples for training, our model outperforms the
direct baseline employing a fixed partitioning strategy by a maximum of 8.72 in
accuracy, demonstrating the effectiveness in performance. On Video-MME,
Quicksviewer achieves SOTA under modest sequence lengths using just up to 5\%
of tokens per frame required by baselines. With this paradigm, scaling up the
number of input frames reveals a clear power law of the model capabilities. It
is also empirically verified that the segments generated by the cubing network
can help for analyzing continuous events in videos.

</details>


### [183] [RealisDance-DiT: Simple yet Strong Baseline towards Controllable Character Animation in the Wild](https://arxiv.org/abs/2504.14977)
*Jingkai Zhou,Yifan Wu,Shikai Li,Min Wei,Chao Fan,Weihua Chen,Wei Jiang,Fan Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于强大基础模型的简单修改方法（RealisDance-DiT），通过灵活微调策略解决可控角色动画中的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决可控角色动画中罕见姿势、风格化角色、角色-物体交互、复杂光照和动态场景等问题，现有方法在开放场景中泛化能力不足。

Method: 基于Wan-2.1视频基础模型，提出RealisDance-DiT，通过最小化架构修改和优化微调策略（如低噪声预热和大批次小迭代）提升性能。

Result: RealisDance-DiT在实验中大幅优于现有方法，并引入新测试数据集以全面评估。

Conclusion: 强大基础模型结合简单修改和灵活微调策略，可有效解决可控角色动画的开放场景挑战。

Abstract: Controllable character animation remains a challenging problem, particularly
in handling rare poses, stylized characters, character-object interactions,
complex illumination, and dynamic scenes. To tackle these issues, prior work
has largely focused on injecting pose and appearance guidance via elaborate
bypass networks, but often struggles to generalize to open-world scenarios. In
this paper, we propose a new perspective that, as long as the foundation model
is powerful enough, straightforward model modifications with flexible
fine-tuning strategies can largely address the above challenges, taking a step
towards controllable character animation in the wild. Specifically, we
introduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our
sufficient analysis reveals that the widely adopted Reference Net design is
suboptimal for large-scale DiT models. Instead, we demonstrate that minimal
modifications to the foundation model architecture yield a surprisingly strong
baseline. We further propose the low-noise warmup and "large batches and small
iterations" strategies to accelerate model convergence during fine-tuning while
maximally preserving the priors of the foundation model. In addition, we
introduce a new test dataset that captures diverse real-world challenges,
complementing existing benchmarks such as TikTok dataset and UBC fashion video
dataset, to comprehensively evaluate the proposed method. Extensive experiments
show that RealisDance-DiT outperforms existing methods by a large margin.

</details>


### [184] [Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs](https://arxiv.org/abs/2504.15280)
*Chun-Hsiao Yeh,Chenyu Wang,Shengbang Tong,Ta-Ying Cheng,Rouyu Wang,Tianzhe Chu,Yuexiang Zhai,Yubei Chen,Shenghua Gao,Yi Ma*

Main category: cs.CV

TL;DR: 论文提出了All-Angles Bench基准，用于评估多模态大语言模型（MLLMs）在多视角场景理解中的表现，发现当前模型在跨视角一致性和相机姿态估计方面与人类水平存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 多视角理解是MLLMs作为具身代理的核心能力，但现有模型在高层次推理和规划中表现优异，却在几何一致性和跨视角对应上不足。

Method: 通过构建包含2,100个多视角问答对的基准（All-Angles Bench），测试27个代表性MLLMs在六项任务中的表现。

Result: 实验显示MLLMs在部分遮挡视角的跨视角对应和相机姿态估计方面表现较差，与人类水平差距显著。

Conclusion: 研究强调了增强多视角感知能力的必要性，All-Angles Bench为未来研究提供了有价值的参考。

Abstract: Multi-view understanding, the ability to reconcile visual information across
diverse viewpoints for effective navigation, manipulation, and 3D scene
comprehension, is a fundamental challenge in Multi-Modal Large Language Models
(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive
advances in high-level reasoning and planning, they frequently fall short when
confronted with multi-view geometric consistency and cross-view correspondence.
To comprehensively evaluate the challenges of MLLMs in multi-view scene
reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human
carefully annotated multi-view question-answer pairs across 90 diverse
real-world scenes. Our six tasks (counting, attribute identification, relative
distance, relative direction, object manipulation, and camera pose estimation)
specifically test model's geometric correspondence and the capacity to align
information consistently across views. Our extensive experiments, benchmark on
27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and
GPT-4o against human evaluators reveals a substantial performance gap,
indicating that current MLLMs remain far from human-level proficiency. Through
in-depth analysis, we show that MLLMs are particularly underperforming under
two aspects: (1) cross-view correspondence for partially occluded views and (2)
establishing the coarse camera poses. These findings highlight the necessity of
domain-specific refinements or modules that embed stronger multi-view
awareness. We believe that our All-Angles Bench offers valuable insights and
contribute to bridging the gap between MLLMs and human-level multi-view
understanding. The project and benchmark are publicly available at
https://danielchyeh.github.io/All-Angles-Bench/.

</details>


### [185] [Benchmarking Large Vision-Language Models on Fine-Grained Image Tasks: A Comprehensive Evaluation](https://arxiv.org/abs/2504.14988)
*Hong-Tao Yu,Xiu-Shen Wei,Yuxin Peng,Serge Belongie*

Main category: cs.CV

TL;DR: 该论文提出了一个细粒度评估基准FG-BMK，用于评估大型视觉语言模型（LVLMs）在细粒度图像任务中的表现，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs在多模态感知方面表现出色，但细粒度图像任务的评估尚未充分探索。

Method: 引入FG-BMK基准，包含349万问题和332万图像，从人类和机器视角系统评估LVLMs的语义识别和细粒度特征表示能力。

Result: 实验揭示了训练范式、模态对齐、扰动敏感性和细粒度类别推理对任务性能的影响。

Conclusion: 研究揭示了当前LVLMs的局限性，为未来数据构建和模型设计提供了指导。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated
remarkable multimodal perception capabilities, garnering significant attention.
While numerous evaluation studies have emerged, assessing LVLMs both
holistically and on specialized tasks, fine-grained image tasks-fundamental to
computer vision-remain largely unexplored. To fill this gap, we introduce a
comprehensive fine-grained evaluation benchmark, i.e., FG-BMK, comprising 3.49
million questions and 3.32 million images. Our evaluation systematically
examines LVLMs from both human-oriented and machine-oriented perspectives,
focusing on their semantic recognition and fine-grained feature representation
capabilities. Through extensive experiments on eight representative LVLMs/VLMs,
we uncover key findings regarding the influence of training paradigms, modality
alignment, perturbation susceptibility, and fine-grained category reasoning on
task performance. This work provides critical insights into the limitations of
current LVLMs and offers guidance for future data construction and model design
in the development of more advanced LVLMs. Our code is open-source and
available at https://github.com/SEU-VIPGroup/FG-BMK.

</details>


### [186] [NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: KwaiSR Dataset and Study](https://arxiv.org/abs/2504.15003)
*Xin Li,Xijun Wang,Bingchen Li,Kun Yuan,Yizhen Shao,Suhang Yao,Ming Sun,Chao Zhou,Radu Timofte,Zhibo Chen*

Main category: cs.CV

TL;DR: KwaiSR是首个针对短用户生成内容（UGC）图像超分辨率的基准数据集，包含合成和野生两部分，用于推动短UGC平台的图像超分辨率算法研究。


<details>
  <summary>Details</summary>
Motivation: 为短UGC平台开发图像超分辨率算法提供基准数据集，填补研究空白。

Method: 数据集包括合成部分（模拟真实低质量UGC图像）和野生部分（直接从Kwai平台收集），并分为训练、验证和测试集。

Result: KwaiSR数据集对现有图像超分辨率方法具有挑战性，推动了该领域的新研究方向。

Conclusion: KwaiSR数据集为短UGC图像超分辨率研究提供了重要资源，并成功吸引了研究社区参与相关挑战赛。

Abstract: In this work, we build the first benchmark dataset for short-form UGC Image
Super-resolution in the wild, termed KwaiSR, intending to advance the research
on developing image super-resolution algorithms for short-form UGC platforms.
This dataset is collected from the Kwai Platform, which is composed of two
parts, i.e., synthetic and wild parts. Among them, the synthetic dataset,
including 1,900 image pairs, is produced by simulating the degradation
following the distribution of real-world low-quality short-form UGC images,
aiming to provide the ground truth for training and objective comparison in the
validation/testing. The wild dataset contains low-quality images collected
directly from the Kwai Platform, which are filtered using the quality
assessment method KVQ from the Kwai Platform. As a result, the KwaiSR dataset
contains 1800 synthetic image pairs and 1900 wild images, which are divided
into training, validation, and testing parts with a ratio of 8:1:1. Based on
the KwaiSR dataset, we organize the NTIRE 2025 challenge on a second short-form
UGC Video quality assessment and enhancement, which attracts lots of
researchers to develop the algorithm for it. The results of this competition
have revealed that our KwaiSR dataset is pretty challenging for existing Image
SR methods, which is expected to lead to a new direction in the image
super-resolution field. The dataset can be found from
https://lixinustc.github.io/NTIRE2025-KVQE-KwaSR-KVQ.github.io/.

</details>


### [187] [Shifts in Doctors' Eye Movements Between Real and AI-Generated Medical Images](https://arxiv.org/abs/2504.15007)
*David C Wong,Bin Wang,Gorkem Durak,Marouane Tliba,Mohamed Amine Kerkouri,Aladine Chetouani,Ahmet Enis Cetin,Cagdas Topel,Nicolo Gennaro,Camila Vendrami,Tugce Agirlar Trabzonlu,Amir Ali Rahsepar,Laetitia Perronne,Matthew Antalek,Onural Ozturk,Gokcan Okur,Andrew C. Gordon,Ayis Pyrros,Frank H Miller,Amir A Borhani,Hatice Savas,Eric M. Hart*

Main category: cs.CV

TL;DR: 通过眼动追踪分析放射科医生在真实与深度学习生成图像中的注意力分配和诊断策略差异。


<details>
  <summary>Details</summary>
Motivation: 研究放射科医生在医学影像诊断中的视觉注意力分配，以及真实与合成图像对其眼动行为的影响。

Method: 分析眼动模式（如扫视方向、幅度及其联合分布）和注视偏差图（首次、末次、短时和长时注视），量化真实与合成图像的视觉显著性差异。

Result: 揭示了放射科医生在真实与合成图像中的注意力分配和诊断策略差异。

Conclusion: 眼动追踪分析为理解放射科医生的视觉诊断行为提供了重要工具，尤其在真实与合成图像的对比中表现出显著差异。

Abstract: Eye-tracking analysis plays a vital role in medical imaging, providing key
insights into how radiologists visually interpret and diagnose clinical cases.
In this work, we first analyze radiologists' attention and agreement by
measuring the distribution of various eye-movement patterns, including saccades
direction, amplitude, and their joint distribution. These metrics help uncover
patterns in attention allocation and diagnostic strategies. Furthermore, we
investigate whether and how doctors' gaze behavior shifts when viewing
authentic (Real) versus deep-learning-generated (Fake) images. To achieve this,
we examine fixation bias maps, focusing on first, last, short, and longest
fixations independently, along with detailed saccades patterns, to quantify
differences in gaze distribution and visual saliency between authentic and
synthetic images.

</details>


### [188] [Insert Anything: Image Insertion via In-Context Editing in DiT](https://arxiv.org/abs/2504.15009)
*Wensong Song,Hong Jiang,Zongxing Yang,Ruijie Quan,Yi Yang*

Main category: cs.CV

TL;DR: Insert Anything是一个统一的框架，用于基于参考的图像插入，支持灵活的文本和掩码引导编辑，并在多种插入任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要为不同任务训练单独模型，缺乏灵活性和通用性。本文旨在提出一个统一的框架，能够处理多样化的插入任务。

Method: 利用Diffusion Transformer（DiT）的多模态注意力机制，支持掩码和文本引导编辑，并引入上下文编辑机制，通过两种提示策略协调插入元素与目标场景。

Result: 在AnyInsertion、DreamBooth和VTON-HD基准测试中，该方法表现优于现有方法。

Conclusion: Insert Anything在创意内容生成、虚拟试穿和场景合成等实际应用中具有巨大潜力。

Abstract: This work presents Insert Anything, a unified framework for reference-based
image insertion that seamlessly integrates objects from reference images into
target scenes under flexible, user-specified control guidance. Instead of
training separate models for individual tasks, our approach is trained once on
our new AnyInsertion dataset--comprising 120K prompt-image pairs covering
diverse tasks such as person, object, and garment insertion--and effortlessly
generalizes to a wide range of insertion scenarios. Such a challenging setting
requires capturing both identity features and fine-grained details, while
allowing versatile local adaptations in style, color, and texture. To this end,
we propose to leverage the multimodal attention of the Diffusion Transformer
(DiT) to support both mask- and text-guided editing. Furthermore, we introduce
an in-context editing mechanism that treats the reference image as contextual
information, employing two prompting strategies to harmonize the inserted
elements with the target scene while faithfully preserving their distinctive
features. Extensive experiments on AnyInsertion, DreamBooth, and VTON-HD
benchmarks demonstrate that our method consistently outperforms existing
alternatives, underscoring its great potential in real-world applications such
as creative content generation, virtual try-on, and scene composition.

</details>


### [189] [Gaussian Shading++: Rethinking the Realistic Deployment Challenge of Performance-Lossless Image Watermark for Diffusion Models](https://arxiv.org/abs/2504.15026)
*Zijin Yang,Xin Zhang,Kejiang Chen,Kai Zeng,Qiyi Yao,Han Fang,Weiming Zhang,Nenghai Yu*

Main category: cs.CV

TL;DR: 论文提出了一种名为Gaussian Shading++的水印方法，解决了扩散模型在实际部署中的关键挑战，如密钥管理、生成参数变化和第三方验证问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在实际应用中面临版权保护和不当内容生成的伦理问题，现有水印方法忽视了密钥管理和第三方验证等现实挑战。

Method: 采用双通道设计，利用伪随机纠错码编码水印随机种子，结合高斯噪声通道建模和软决策解码策略，并引入公钥签名支持第三方验证。

Result: 实验表明，Gaussian Shading++在保持性能无损的同时，鲁棒性优于现有方法。

Conclusion: Gaussian Shading++是一种更实用的扩散模型水印解决方案，适用于实际部署。

Abstract: Ethical concerns surrounding copyright protection and inappropriate content
generation pose challenges for the practical implementation of diffusion
models. One effective solution involves watermarking the generated images.
Existing methods primarily focus on ensuring that watermark embedding does not
degrade the model performance. However, they often overlook critical challenges
in real-world deployment scenarios, such as the complexity of watermark key
management, user-defined generation parameters, and the difficulty of
verification by arbitrary third parties. To address this issue, we propose
Gaussian Shading++, a diffusion model watermarking method tailored for
real-world deployment. We propose a double-channel design that leverages
pseudorandom error-correcting codes to encode the random seed required for
watermark pseudorandomization, achieving performance-lossless watermarking
under a fixed watermark key and overcoming key management challenges.
Additionally, we model the distortions introduced during generation and
inversion as an additive white Gaussian noise channel and employ a novel soft
decision decoding strategy during extraction, ensuring strong robustness even
when generation parameters vary. To enable third-party verification, we
incorporate public key signatures, which provide a certain level of resistance
against forgery attacks even when model inversion capabilities are fully
disclosed. Extensive experiments demonstrate that Gaussian Shading++ not only
maintains performance losslessness but also outperforms existing methods in
terms of robustness, making it a more practical solution for real-world
deployment.

</details>


### [190] [DyST-XL: Dynamic Layout Planning and Content Control for Compositional Text-to-Video Generation](https://arxiv.org/abs/2504.15032)
*Weijie He,Mushui Liu,Yunlong Yu,Zhao Wang,Chao Wu*

Main category: cs.CV

TL;DR: DyST-XL是一个无需训练的框架，通过帧感知控制提升现有文本到视频模型的性能，解决了布局不连续、实体漂移和交互动态不合理的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的模型在合成动态场景时存在布局不连续、实体身份漂移和交互动态不合理的问题，DyST-XL旨在解决这些挑战。

Method: DyST-XL结合了动态布局规划器、双提示控制注意力机制和实体一致性约束策略，通过LLM解析输入提示并生成物理感知的关键帧布局。

Result: 实验表明，DyST-XL在复杂提示下的性能显著提升，填补了无需训练视频合成的关键空白。

Conclusion: DyST-XL为文本到视频生成提供了一种高效的训练免费解决方案，显著提升了复杂场景的合成能力。

Abstract: Compositional text-to-video generation, which requires synthesizing dynamic
scenes with multiple interacting entities and precise spatial-temporal
relationships, remains a critical challenge for diffusion-based models.
Existing methods struggle with layout discontinuity, entity identity drift, and
implausible interaction dynamics due to unconstrained cross-attention
mechanisms and inadequate physics-aware reasoning. To address these
limitations, we propose DyST-XL, a \textbf{training-free} framework that
enhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through
frame-aware control. DyST-XL integrates three key innovations: (1) A Dynamic
Layout Planner that leverages large language models (LLMs) to parse input
prompts into entity-attribute graphs and generates physics-aware keyframe
layouts, with intermediate frames interpolated via trajectory optimization; (2)
A Dual-Prompt Controlled Attention Mechanism that enforces localized text-video
alignment through frame-aware attention masking, achieving the precise control
over individual entities; and (3) An Entity-Consistency Constraint strategy
that propagates first-frame feature embeddings to subsequent frames during
denoising, preserving object identity without manual annotation. Experiments
demonstrate that DyST-XL excels in compositional text-to-video generation,
significantly improving performance on complex prompts and bridging a crucial
gap in training-free video synthesis.

</details>


### [191] [Distribution-aware Forgetting Compensation for Exemplar-Free Lifelong Person Re-identification](https://arxiv.org/abs/2504.15041)
*Shiben Liu,Huijie Fan,Qiang Wang,Baojie Fan,Yandong Tang,Liangqiong Qu*

Main category: cs.CV

TL;DR: 论文提出了一种名为DAFC的新模型，通过文本驱动的提示聚合和分布感知补偿技术，解决了终身行人重识别中的知识遗忘问题，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 终身行人重识别（LReID）面临在适应新信息的同时保留旧知识的关键挑战，现有方法（如基于排练和排练无关的方法）存在遗忘问题。

Method: 提出DAFC模型，结合文本驱动的提示聚合（TPA）和分布感知与整合（DAI），并通过知识巩固机制（KCM）实现新知识的自适应学习和跨域知识整合。

Result: 实验表明，DAFC在两种训练顺序下的平均mAP/R@1分别至少优于现有方法9.8%/6.6%和6.4%/6.2%。

Conclusion: DAFC通过分布感知和知识整合，有效缓解了终身行人重识别中的遗忘问题，显著提升了性能。

Abstract: Lifelong Person Re-identification (LReID) suffers from a key challenge in
preserving old knowledge while adapting to new information. The existing
solutions include rehearsal-based and rehearsal-free methods to address this
challenge. Rehearsal-based approaches rely on knowledge distillation,
continuously accumulating forgetting during the distillation process.
Rehearsal-free methods insufficiently learn the distribution of each domain,
leading to forgetfulness over time. To solve these issues, we propose a novel
Distribution-aware Forgetting Compensation (DAFC) model that explores
cross-domain shared representation learning and domain-specific distribution
integration without using old exemplars or knowledge distillation. We propose a
Text-driven Prompt Aggregation (TPA) that utilizes text features to enrich
prompt elements and guide the prompt model to learn fine-grained
representations for each instance. This can enhance the differentiation of
identity information and establish the foundation for domain distribution
awareness. Then, Distribution-based Awareness and Integration (DAI) is designed
to capture each domain-specific distribution by a dedicated expert network and
adaptively consolidate them into a shared region in high-dimensional space. In
this manner, DAI can consolidate and enhance cross-domain shared representation
learning while alleviating catastrophic forgetting. Furthermore, we develop a
Knowledge Consolidation Mechanism (KCM) that comprises instance-level
discrimination and cross-domain consistency alignment strategies to facilitate
model adaptive learning of new knowledge from the current domain and promote
knowledge consolidation learning between acquired domain-specific
distributions, respectively. Experimental results show that our DAFC outperform
state-of-the-art methods by at least 9.8\%/6.6\% and 6.4\%/6.2\% of average
mAP/R@1 on two training orders.

</details>


### [192] [ScanEdit: Hierarchically-Guided Functional 3D Scan Editing](https://arxiv.org/abs/2504.15049)
*Mohamed el amine Boudjoghra,Ivan Laptev,Angela Dai*

Main category: cs.CV

TL;DR: ScanEdit提出了一种基于指令驱动的3D扫描编辑方法，通过层次化场景图和大型语言模型（LLM）实现高效、真实的场景编辑。


<details>
  <summary>Details</summary>
Motivation: 随着3D捕捉技术的快速发展，3D数据激增，高效的3D场景编辑对图形应用至关重要。

Method: 首先构建层次化场景图表示，利用LLM将语言指令转化为可操作的命令，并结合物理约束生成真实场景。

Result: ScanEdit在实验中表现优异，优于现有技术，适用于多种真实场景和指令。

Conclusion: ScanEdit通过层次化编辑和LLM推理，实现了高效且符合物理常识的3D场景编辑。

Abstract: With the fast pace of 3D capture technology and resulting abundance of 3D
data, effective 3D scene editing becomes essential for a variety of graphics
applications. In this work we present ScanEdit, an instruction-driven method
for functional editing of complex, real-world 3D scans. To model large and
interdependent sets of ob- jectswe propose a hierarchically-guided approach.
Given a 3D scan decomposed into its object instances, we first construct a
hierarchical scene graph representation to enable effective, tractable editing.
We then leverage reason- ing capabilities of Large Language Models (LLMs) and
translate high-level language instructions into actionable commands applied
hierarchically to the scene graph. Fi- nally, ScanEdit integrates LLM-based
guidance with ex- plicit physical constraints and generates realistic scenes
where object arrangements obey both physics and common sense. In our extensive
experimental evaluation ScanEdit outperforms state of the art and demonstrates
excellent re- sults for a variety of real-world scenes and input instruc-
tions.

</details>


### [193] [Structure-guided Diffusion Transformer for Low-Light Image Enhancement](https://arxiv.org/abs/2504.15054)
*Xiangchen Yin,Zhenda Yu,Longtao Jiang,Xin Gao,Xiao Sun,Zhi Liu,Xun Yang*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散变压器（DiT）的低光照图像增强框架SDTL，通过小波变换和结构增强模块（SEM）提升模型效率与纹理增强效果，同时引入结构引导注意力块（SAB）减少噪声干扰，实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前低光照图像增强方法在恢复细节时容易放大噪声，导致视觉质量不佳。扩散变压器（DiT）在此领域的应用尚未探索，因此研究其潜力。

Method: 提出SDTL框架，结合小波变换压缩特征以提高效率，设计SEM模块利用结构先验增强纹理，并采用SAB块聚焦纹理丰富区域以减少噪声干扰。

Result: 在多个数据集上实现了SOTA性能，验证了SDTL在提升图像质量和DiT在低光照增强任务中的潜力。

Conclusion: SDTL框架有效结合了DiT的优势，通过结构引导和自适应策略显著提升了低光照图像增强的效果。

Abstract: While the diffusion transformer (DiT) has become a focal point of interest in
recent years, its application in low-light image enhancement remains a blank
area for exploration. Current methods recover the details from low-light images
while inevitably amplifying the noise in images, resulting in poor visual
quality. In this paper, we firstly introduce DiT into the low-light enhancement
task and design a novel Structure-guided Diffusion Transformer based Low-light
image enhancement (SDTL) framework. We compress the feature through wavelet
transform to improve the inference efficiency of the model and capture the
multi-directional frequency band. Then we propose a Structure Enhancement
Module (SEM) that uses structural prior to enhance the texture and leverages an
adaptive fusion strategy to achieve more accurate enhancement effect. In
Addition, we propose a Structure-guided Attention Block (SAB) to pay more
attention to texture-riched tokens and avoid interference from noisy areas in
noise prediction. Extensive qualitative and quantitative experiments
demonstrate that our method achieves SOTA performance on several popular
datasets, validating the effectiveness of SDTL in improving image quality and
the potential of DiT in low-light enhancement tasks.

</details>


### [194] [Hierarchical Attention Fusion of Visual and Textual Representations for Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2504.15085)
*Wangyu Wu,Zhenhong Chen,Siqi Song,Xianglin Qiua,Xiaowei Huang,Fei Ma,Jimin Xiao*

Main category: cs.CV

TL;DR: HAF-VT是一种跨域序列推荐方法，通过结合视觉和文本数据增强认知建模，利用分层注意力机制学习单域和跨域偏好，在电商数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 受人类认知过程启发，研究旨在通过多模态数据（视觉和文本）增强跨域用户兴趣建模，提升推荐效果。

Method: 使用冻结的CLIP模型生成图像和文本嵌入，通过分层注意力机制联合学习单域和跨域偏好。

Result: 在四个电商数据集上，HAF-VT在捕捉跨域用户兴趣方面优于现有方法。

Conclusion: HAF-VT成功将认知原理与计算模型结合，凸显了多模态数据在序列决策中的重要性。

Abstract: Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by
leveraging historical interactions across multiple domains, focusing on
modeling cross-domain preferences through intra- and inter-sequence item
relationships. Inspired by human cognitive processes, we propose Hierarchical
Attention Fusion of Visual and Textual Representations (HAF-VT), a novel
approach integrating visual and textual data to enhance cognitive modeling.
Using the frozen CLIP model, we generate image and text embeddings, enriching
item representations with multimodal data. A hierarchical attention mechanism
jointly learns single-domain and cross-domain preferences, mimicking human
information integration. Evaluated on four e-commerce datasets, HAF-VT
outperforms existing methods in capturing cross-domain user interests, bridging
cognitive principles with computational models and highlighting the role of
multimodal data in sequential decision-making.

</details>


### [195] [VistaDepth: Frequency Modulation With Bias Reweighting For Enhanced Long-Range Depth Estimation](https://arxiv.org/abs/2504.15095)
*Mingxia Zhan,Li Zhang,XiaoMeng Chu,Beibei Wang*

Main category: cs.CV

TL;DR: VistaDepth是一种新型框架，通过结合自适应频域特征增强和权重平衡机制，改进了基于扩散模型的单目深度估计（MDE）方法，特别提升了远距离深度重建的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的MDE方法在远距离深度重建上表现不佳，主要由于深度值分布不平衡和过度依赖空间域特征。

Method: VistaDepth引入了Latent Frequency Modulation (LFM)模块和自适应权重策略，动态优化频域特征并实时调整扩散损失。

Result: 实验表明，VistaDepth在扩散基MDE技术中达到最先进水平，尤其在远距离区域重建上表现突出。

Conclusion: VistaDepth通过频域特征增强和自适应权重机制，显著提升了MDE的性能，特别是在远距离深度重建方面。

Abstract: Monocular depth estimation (MDE) aims to predict per-pixel depth values from
a single RGB image. Recent advancements have positioned diffusion models as
effective MDE tools by framing the challenge as a conditional image generation
task. Despite their progress, these methods often struggle with accurately
reconstructing distant depths, due largely to the imbalanced distribution of
depth values and an over-reliance on spatial-domain features. To overcome these
limitations, we introduce VistaDepth, a novel framework that integrates
adaptive frequency-domain feature enhancements with an adaptive
weight-balancing mechanism into the diffusion process. Central to our approach
is the Latent Frequency Modulation (LFM) module, which dynamically refines
spectral responses in the latent feature space, thereby improving the
preservation of structural details and reducing noisy artifacts. Furthermore,
we implement an adaptive weighting strategy that modulates the diffusion loss
in real-time, enhancing the model's sensitivity towards distant depth
reconstruction. These innovations collectively result in superior depth
perception performance across both distance and detail. Experimental
evaluations confirm that VistaDepth achieves state-of-the-art performance among
diffusion-based MDE techniques, particularly excelling in the accurate
reconstruction of distant regions.

</details>


### [196] [A triple-branch network for latent fingerprint enhancement guided by orientation fields and minutiae](https://arxiv.org/abs/2504.15105)
*Yurun Wang,Zerong Qi,Shujun Fu,Mingzheng Hu*

Main category: cs.CV

TL;DR: 论文提出了一种名为TBSFNet的三分支空间融合网络，用于潜在指纹增强，并结合MLFGNet提升泛化能力，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在低质量指纹区域恢复上表现不足，不同区域需要不同的增强策略。

Method: 提出TBSFNet，分别增强不同区域；结合方向场和细节模块，引入MLFGNet提升泛化能力。

Result: 在MOLF和MUST数据集上，MLFGNet优于现有增强算法。

Conclusion: TBSFNet和MLFGNet有效提升了潜在指纹增强的性能和泛化能力。

Abstract: Latent fingerprint enhancement is a critical step in the process of latent
fingerprint identification. Existing deep learning-based enhancement methods
still fall short of practical application requirements, particularly in
restoring low-quality fingerprint regions. Recognizing that different regions
of latent fingerprints require distinct enhancement strategies, we propose a
Triple Branch Spatial Fusion Network (TBSFNet), which simultaneously enhances
different regions of the image using tailored strategies. Furthermore, to
improve the generalization capability of the network, we integrate orientation
field and minutiae-related modules into TBSFNet and introduce a Multi-Level
Feature Guidance Network (MLFGNet). Experimental results on the MOLF and MUST
datasets demonstrate that MLFGNet outperforms existing enhancement algorithms.

</details>


### [197] [Unwarping Screen Content Images via Structure-texture Enhancement Network and Transformation Self-estimation](https://arxiv.org/abs/2504.15108)
*Zhenzhen Xiao,Heng Liu,Bingwen Hu*

Main category: cs.CV

TL;DR: 提出了一种结构-纹理增强网络（STEN）用于屏幕内容图像（SCI）的扭曲校正，通过B样条隐式神经表示和变换自估计模块，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有隐式神经网络方法在自然图像上表现良好，但在包含大几何扭曲、文本、符号和锐利边缘的SCI上表现不佳。

Method: STEN结合了B样条隐式神经表示模块和变换误差估计与自校正算法，包含结构估计分支（SEB）和纹理估计分支（TEB）。

Result: 在公开SCI数据集上表现显著优于现有方法，且在自然图像数据集上也显示出潜力。

Conclusion: STEN能有效处理SCI的扭曲问题，并具有扩展到自然图像的潜力。

Abstract: While existing implicit neural network-based image unwarping methods perform
well on natural images, they struggle to handle screen content images (SCIs),
which often contain large geometric distortions, text, symbols, and sharp
edges. To address this, we propose a structure-texture enhancement network
(STEN) with transformation self-estimation for SCI warping. STEN integrates a
B-spline implicit neural representation module and a transformation error
estimation and self-correction algorithm. It comprises two branches: the
structure estimation branch (SEB), which enhances local aggregation and global
dependency modeling, and the texture estimation branch (TEB), which improves
texture detail synthesis using B-spline implicit neural representation.
Additionally, the transformation self-estimation module autonomously estimates
the transformation error and corrects the coordinate transformation matrix,
effectively handling real-world image distortions. Extensive experiments on
public SCI datasets demonstrate that our approach significantly outperforms
state-of-the-art methods. Comparisons on well-known natural image datasets also
show the potential of our approach for natural image distortion.

</details>


### [198] [Improving Sound Source Localization with Joint Slot Attention on Image and Audio](https://arxiv.org/abs/2504.15118)
*Inho Kim,Youngkil Song,Jicheol Park,Won Hwa Kim,Suha Kwak*

Main category: cs.CV

TL;DR: 本文提出了一种新的声源定位方法，通过联合图像和音频的注意力机制，分解目标与非目标特征，并仅使用目标特征进行对比学习，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法因噪声和背景干扰导致性能不佳，需要更有效的特征分解和对比学习策略。

Method: 采用联合注意力机制分解图像和音频特征为目标与非目标表示，并引入跨模态注意力匹配以对齐局部特征。

Result: 在三个公共基准测试中几乎全部领先，并在跨模态检索任务中大幅超越先前工作。

Conclusion: 该方法通过注意力机制和特征分解有效解决了噪声和背景干扰问题，显著提升了声源定位性能。

Abstract: Sound source localization (SSL) is the task of locating the source of sound
within an image. Due to the lack of localization labels, the de facto standard
in SSL has been to represent an image and audio as a single embedding vector
each, and use them to learn SSL via contrastive learning. To this end, previous
work samples one of local image features as the image embedding and aggregates
all local audio features to obtain the audio embedding, which is far from
optimal due to the presence of noise and background irrelevant to the actual
target in the input. We present a novel SSL method that addresses this chronic
issue by joint slot attention on image and audio. To be specific, two slots
competitively attend image and audio features to decompose them into target and
off-target representations, and only target representations of image and audio
are used for contrastive learning. Also, we introduce cross-modal attention
matching to further align local features of image and audio. Our method
achieved the best in almost all settings on three public benchmarks for SSL,
and substantially outperformed all the prior work in cross-modal retrieval.

</details>


### [199] [Robust and Real-time Surface Normal Estimation from Stereo Disparities using Affine Transformations](https://arxiv.org/abs/2504.15121)
*Csongor Csanad Kariko,Muhammad Rafi Faisal,Levente Hajder*

Main category: cs.CV

TL;DR: 提出了一种基于校正立体图像对和仿射变换的表面法线估计新方法，速度快且准确。


<details>
  <summary>Details</summary>
Motivation: 通过简化立体图像对的校正过程，降低计算复杂度，提高表面法线估计的效率。

Method: 利用视差数据生成仿射变换，结合自定义卷积算法降噪，并采用自适应启发式技术检测连通表面组件。

Result: 在Middlebury和Cityscapes数据集上验证，显著提升了实时性能和准确性，生成密集定向点云。

Conclusion: 该方法高效且准确，代码将公开以促进研究和复现。

Abstract: This work introduces a novel method for surface normal estimation from
rectified stereo image pairs, leveraging affine transformations derived from
disparity values to achieve fast and accurate results. We demonstrate how the
rectification of stereo image pairs simplifies the process of surface normal
estimation by reducing computational complexity. To address noise reduction, we
develop a custom algorithm inspired by convolutional operations, tailored to
process disparity data efficiently. We also introduce adaptive heuristic
techniques for efficiently detecting connected surface components within the
images, further improving the robustness of the method. By integrating these
methods, we construct a surface normal estimator that is both fast and
accurate, producing a dense, oriented point cloud as the final output. Our
method is validated using both simulated environments and real-world stereo
images from the Middlebury and Cityscapes datasets, demonstrating significant
improvements in real-time performance and accuracy when implemented on a GPU.
Upon acceptance, the shader source code will be made publicly available to
facilitate further research and reproducibility.

</details>


### [200] [MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video](https://arxiv.org/abs/2504.15122)
*Minh-Quan Viet Bui,Jongmin Park,Juan Luis Gonzalez Bello,Jaeho Moon,Jihyong Oh,Munchurl Kim*

Main category: cs.CV

TL;DR: MoBGS是一种新的动态3D高斯泼溅框架，用于从模糊的单目视频中重建清晰的高质量时空视图。


<details>
  <summary>Details</summary>
Motivation: 现有动态新视角合成方法对运动模糊敏感，导致渲染质量下降。MoBGS旨在解决这一问题，专注于动态对象的运动建模。

Method: MoBGS引入了BLCE方法估计潜在相机轨迹，改进全局相机运动去模糊，并提出LCEE方法确保全局和局部运动去模糊的一致性。

Result: 在Stereo Blur数据集和真实模糊视频上的实验表明，MoBGS显著优于现有方法（DyBluRF和Deblur4DGS）。

Conclusion: MoBGS在动态新视角合成中实现了最先进的性能，尤其在处理运动模糊方面表现优异。

Abstract: We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS)
framework capable of reconstructing sharp and high-quality novel
spatio-temporal views from blurry monocular videos in an end-to-end manner.
Existing dynamic novel view synthesis (NVS) methods are highly sensitive to
motion blur in casually captured videos, resulting in significant degradation
of rendering quality. While recent approaches address motion-blurred inputs for
NVS, they primarily focus on static scene reconstruction and lack dedicated
motion modeling for dynamic objects. To overcome these limitations, our MoBGS
introduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for
effective latent camera trajectory estimation, improving global camera motion
deblurring. In addition, we propose a physically-inspired Latent Camera-induced
Exposure Estimation (LCEE) method to ensure consistent deblurring of both
global camera and local object motion. Our MoBGS framework ensures the temporal
consistency of unseen latent timestamps and robust motion decomposition of
static and dynamic regions. Extensive experiments on the Stereo Blur dataset
and real-world blurry videos show that our MoBGS significantly outperforms the
very recent advanced methods (DyBluRF and Deblur4DGS), achieving
state-of-the-art performance for dynamic NVS under motion blur.

</details>


### [201] [Instance-Adaptive Keypoint Learning with Local-to-Global Geometric Aggregation for Category-Level Object Pose Estimation](https://arxiv.org/abs/2504.15134)
*Xiao Zhang,Lu Zou,Tao Lu,Yuan Yao,Zhangjin Huang,Guoping Wang*

Main category: cs.CV

TL;DR: INKL-Pose是一个新的类别级物体姿态估计框架，通过实例自适应关键点学习和局部到全局几何聚合，解决了复杂几何或非典型形状实例的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂几何或显著偏离典型形状的实例时表现不佳，需要更强的泛化能力。

Method: INKL-Pose通过实例自适应关键点生成器预测关键点，并通过局部和全局特征聚合器优化，同时引入特征序列翻转策略和两种损失函数。

Result: 在CAMERA25、REAL275和HouseCat6D数据集上，INKL-Pose实现了最先进的性能。

Conclusion: INKL-Pose通过创新的关键点学习和几何聚合方法，显著提升了类别级物体姿态估计的准确性。

Abstract: Category-level object pose estimation aims to predict the 6D pose and size of
previously unseen instances from predefined categories, requiring strong
generalization across diverse object instances. Although many previous methods
attempt to mitigate intra-class variations, they often struggle with instances
exhibiting complex geometries or significant deviations from canonical shapes.
To address this challenge, we propose INKL-Pose, a novel category-level object
pose estimation framework that enables INstance-adaptive Keypoint Learning with
local-to-global geometric aggregation. Specifically, our approach first
predicts semantically consistent and geometric informative keypoints through an
Instance-Adaptive Keypoint Generator, then refines them with: (1) a Local
Keypoint Feature Aggregator capturing fine-grained geometries, and (2) a Global
Keypoint Feature Aggregator using bidirectional Mamba for structural
consistency. To enable bidirectional modeling in Mamba, we introduce a Feature
Sequence Flipping strategy that preserves spatial coherence while constructing
backward feature sequences. Additionally, we design a surface loss and a
separation loss to enforce uniform coverage and spatial diversity in keypoint
distribution. The generated keypoints are finally mapped to a canonical space
for regressing the object's 6D pose and size. Extensive experiments on
CAMERA25, REAL275, and HouseCat6D demonstrate that INKL-Pose achieves
state-of-the-art performance and significantly outperforms existing methods.

</details>


### [202] ["I Know It When I See It": Mood Spaces for Connecting and Expressing Visual Concepts](https://arxiv.org/abs/2504.15145)
*Huzheng Yang,Katherine Xu,Michael D. Grossberg,Yutong Bai,Jianbo Shi*

Main category: cs.CV

TL;DR: 提出了一种名为Mood Board的方法，通过示例传达抽象概念，并计算Mood Space以压缩特征空间，实现高效的图像操作。


<details>
  <summary>Details</summary>
Motivation: 许多抽象概念难以定义但易于识别，传统方法难以处理此类问题。

Method: 使用Mood Board和Mood Space，通过纤维化计算压缩预训练特征，学习图像间的亲和关系，并在特征向量空间定义损失函数。

Result: Mood Space具有局部线性和紧凑性，支持图像级操作（如对象平均、视觉类比和姿态转移），计算高效且无需微调。

Conclusion: Mood Board和Mood Space为抽象概念的传达和操作提供了高效且直观的解决方案。

Abstract: Expressing complex concepts is easy when they can be labeled or quantified,
but many ideas are hard to define yet instantly recognizable. We propose a Mood
Board, where users convey abstract concepts with examples that hint at the
intended direction of attribute changes. We compute an underlying Mood Space
that 1) factors out irrelevant features and 2) finds the connections between
images, thus bringing relevant concepts closer. We invent a fibration
computation to compress/decompress pre-trained features into/from a compact
space, 50-100x smaller. The main innovation is learning to mimic the pairwise
affinity relationship of the image tokens across exemplars. To focus on the
coarse-to-fine hierarchical structures in the Mood Space, we compute the top
eigenvector structure from the affinity matrix and define a loss in the
eigenvector space. The resulting Mood Space is locally linear and compact,
allowing image-level operations, such as object averaging, visual analogy, and
pose transfer, to be performed as a simple vector operation in Mood Space. Our
learning is efficient in computation without any fine-tuning, needs only a few
(2-20) exemplars, and takes less than a minute to learn.

</details>


### [203] [Landmark-Free Preoperative-to-Intraoperative Registration in Laparoscopic Liver Resection](https://arxiv.org/abs/2504.15152)
*Jun Zhou,Bingchen Gao,Kai Wang,Jialun Pei,Pheng-Ann Heng,Jing Qin*

Main category: cs.CV

TL;DR: 提出了一种基于自监督学习的无标记术前到术中配准框架，解决了传统方法依赖解剖标记和形状变形建模不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统配准方法依赖解剖标记，存在标记定义模糊和术中肝脏视觉信息整合不足的问题。

Method: 将3D-2D配准流程转化为3D-3D配准，分解为刚性和非刚性配准子任务，使用特征解耦变换器和结构正则化变形网络。

Result: 在合成和真实数据集上的实验及用户研究表明，该方法具有优越性和潜在临床应用价值。

Conclusion: 提出的框架显著提升了肝脏配准的准确性和效率，为手术导航提供了更可靠的解决方案。

Abstract: Liver registration by overlaying preoperative 3D models onto intraoperative
2D frames can assist surgeons in perceiving the spatial anatomy of the liver
clearly for a higher surgical success rate. Existing registration methods rely
heavily on anatomical landmark-based workflows, which encounter two major
limitations: 1) ambiguous landmark definitions fail to provide efficient
markers for registration; 2) insufficient integration of intraoperative liver
visual information in shape deformation modeling. To address these challenges,
in this paper, we propose a landmark-free preoperative-to-intraoperative
registration framework utilizing effective self-supervised learning, termed
\ourmodel. This framework transforms the conventional 3D-2D workflow into a
3D-3D registration pipeline, which is then decoupled into rigid and non-rigid
registration subtasks. \ourmodel~first introduces a feature-disentangled
transformer to learn robust correspondences for recovering rigid
transformations. Further, a structure-regularized deformation network is
designed to adjust the preoperative model to align with the intraoperative
liver surface. This network captures structural correlations through geometry
similarity modeling in a low-rank transformer network. To facilitate the
validation of the registration performance, we also construct an in-vivo
registration dataset containing liver resection videos of 21 patients, called
\emph{P2I-LReg}, which contains 346 keyframes that provide a global view of the
liver together with liver mask annotations and calibrated camera intrinsic
parameters. Extensive experiments and user studies on both synthetic and
in-vivo datasets demonstrate the superiority and potential clinical
applicability of our method.

</details>


### [204] [Dynamic 3D KAN Convolution with Adaptive Grid Optimization for Hyperspectral Image Classification](https://arxiv.org/abs/2504.15155)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: KANet基于改进的3D-DenseNet模型，通过引入可学习的单变量B样条函数和动态网格调整机制，有效解决了高光谱图像分类中的高维数据、稀疏分布和光谱冗余问题，显著提升了模型精度和参数效率。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临高维数据、稀疏分布和光谱冗余等挑战，导致分类过拟合和泛化能力受限。

Method: 提出KANet，采用3D KAN Conv和自适应网格更新机制，通过B样条参数化非线性激活函数替代传统3D卷积核的固定线性权重，动态调整B样条网格点位置。

Result: 在IN、UP和KSC数据集上表现优于主流方法，显著提升了高维数据建模精度和参数效率。

Conclusion: KANet通过3D动态专家卷积系统增强了模型表示能力，无需增加网络深度或宽度，有效缓解了维度灾难和过拟合风险。

Abstract: Deep neural networks face several challenges in hyperspectral image
classification, including high-dimensional data, sparse distribution of ground
objects, and spectral redundancy, which often lead to classification
overfitting and limited generalization capability. To more efficiently adapt to
ground object distributions while extracting image features without introducing
excessive parameters and skipping redundant information, this paper proposes
KANet based on an improved 3D-DenseNet model, consisting of 3D KAN Conv and an
adaptive grid update mechanism. By introducing learnable univariate B-spline
functions on network edges, specifically by flattening three-dimensional
neighborhoods into vectors and applying B-spline-parameterized nonlinear
activation functions to replace the fixed linear weights of traditional 3D
convolutional kernels, we precisely capture complex spectral-spatial nonlinear
relationships in hyperspectral data. Simultaneously, through a dynamic grid
adjustment mechanism, we adaptively update the grid point positions of
B-splines based on the statistical characteristics of input data, optimizing
the resolution of spline functions to match the non-uniform distribution of
spectral features, significantly improving the model's accuracy in
high-dimensional data modeling and parameter efficiency, effectively
alleviating the curse of dimensionality. This characteristic demonstrates
superior neural scaling laws compared to traditional convolutional neural
networks and reduces overfitting risks in small-sample and high-noise
scenarios. KANet enhances model representation capability through a 3D dynamic
expert convolution system without increasing network depth or width. The
proposed method demonstrates superior performance on IN, UP, and KSC datasets,
outperforming mainstream hyperspectral image classification approaches.

</details>


### [205] [Acquire and then Adapt: Squeezing out Text-to-Image Model for Image Restoration](https://arxiv.org/abs/2504.15159)
*Junyuan Deng,Xinyi Wu,Yongxing Yang,Congchao Zhu,Song Wang,Zhenyao Wu*

Main category: cs.CV

TL;DR: 本文提出了一种利用预训练文本到图像（T2I）模型Flux生成训练数据的方法FluxGen，并通过轻量级适配器FluxIR控制模型，以低成本实现高质量图像恢复。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要大量高质量图像和计算资源，成本高且隐私不友好。本文发现预训练T2I模型Flux能生成多样高质量图像，可作为无限训练样本来源。

Method: 提出FluxGen流程，包括无条件图像生成、图像选择和退化图像模拟，并设计轻量级适配器FluxIR控制DiT模型。

Result: 实验表明，该方法能以约8.5%的训练成本，在合成和真实退化数据集上实现更优的恢复效果和视觉质量。

Conclusion: FluxGen和FluxIR的结合为低成本高效图像恢复提供了可行方案。

Abstract: Recently, pre-trained text-to-image (T2I) models have been extensively
adopted for real-world image restoration because of their powerful generative
prior. However, controlling these large models for image restoration usually
requires a large number of high-quality images and immense computational
resources for training, which is costly and not privacy-friendly. In this
paper, we find that the well-trained large T2I model (i.e., Flux) is able to
produce a variety of high-quality images aligned with real-world distributions,
offering an unlimited supply of training samples to mitigate the above issue.
Specifically, we proposed a training data construction pipeline for image
restoration, namely FluxGen, which includes unconditional image generation,
image selection, and degraded image simulation. A novel light-weighted adapter
(FluxIR) with squeeze-and-excitation layers is also carefully designed to
control the large Diffusion Transformer (DiT)-based T2I model so that
reasonable details can be restored. Experiments demonstrate that our proposed
method enables the Flux model to adapt effectively to real-world image
restoration tasks, achieving superior scores and visual quality on both
synthetic and real-world degradation datasets - at only about 8.5\% of the
training cost compared to current approaches.

</details>


### [206] [An Efficient Aerial Image Detection with Variable Receptive Fields](https://arxiv.org/abs/2504.15165)
*Liu Wenbin*

Main category: cs.CV

TL;DR: VRF-DETR是一种基于Transformer的检测器，通过多尺度上下文融合、门控卷积和门控多尺度融合瓶颈解决了无人机目标检测中的小目标、遮挡和计算限制问题，在VisDrone2019上表现优异。


<details>
  <summary>Details</summary>
Motivation: 无人机目标检测面临小目标、密集遮挡和计算限制的挑战，现有检测器难以平衡准确性和效率。

Method: 提出VRF-DETR，包含多尺度上下文融合模块、门控卷积层和门控多尺度融合瓶颈，通过动态调整特征和高效参数建模提升性能。

Result: 在VisDrone2019上达到51.4% mAP50和31.8% mAP50:95，仅需13.5M参数。

Conclusion: VRF-DETR为无人机检测任务建立了新的效率-准确性平衡标准。

Abstract: Aerial object detection using unmanned aerial vehicles (UAVs) faces critical
challenges including sub-10px targets, dense occlusions, and stringent
computational constraints. Existing detectors struggle to balance accuracy and
efficiency due to rigid receptive fields and redundant architectures. To
address these limitations, we propose Variable Receptive Field DETR (VRF-DETR),
a transformer-based detector incorporating three key components: 1) Multi-Scale
Context Fusion (MSCF) module that dynamically recalibrates features through
adaptive spatial attention and gated multi-scale fusion, 2) Gated Convolution
(GConv) layer enabling parameter-efficient local-context modeling via depthwise
separable operations and dynamic gating, and 3) Gated Multi-scale Fusion (GMCF)
Bottleneck that hierarchically disentangles occluded objects through cascaded
global-local interactions. Experiments on VisDrone2019 demonstrate VRF-DETR
achieves 51.4\% mAP\textsubscript{50} and 31.8\% mAP\textsubscript{50:95} with
only 13.5M parameters. This work establishes a new efficiency-accuracy Pareto
frontier for UAV-based detection tasks.

</details>


### [207] [HSANET: A Hybrid Self-Cross Attention Network For Remote Sensing Change Detection](https://arxiv.org/abs/2504.15170)
*Chengxi Han,Xiaoyu Su,Zhiqiang Wei,Meiqi Hu,Yichu Xu*

Main category: cs.CV

TL;DR: HSANet是一种用于遥感图像变化检测的网络，通过分层卷积提取多尺度特征，结合混合自注意力和交叉注意力机制，提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 遥感图像变化检测在大规模监测中至关重要，但现有方法在全局上下文和多尺度特征融合方面存在不足。

Method: 采用分层卷积提取多尺度特征，结合混合自注意力和交叉注意力机制，学习并融合全局和跨尺度信息。

Result: HSANet能够捕捉不同尺度的全局上下文，整合跨尺度特征，优化边缘细节，提升检测性能。

Conclusion: HSANet通过创新设计显著提升了遥感图像变化检测的性能，并开源了模型代码。

Abstract: The remote sensing image change detection task is an essential method for
large-scale monitoring. We propose HSANet, a network that uses hierarchical
convolution to extract multi-scale features. It incorporates hybrid
self-attention and cross-attention mechanisms to learn and fuse global and
cross-scale information. This enables HSANet to capture global context at
different scales and integrate cross-scale features, refining edge details and
improving detection performance. We will also open-source our model code:
https://github.com/ChengxiHAN/HSANet.

</details>


### [208] [DSPO: Direct Semantic Preference Optimization for Real-World Image Super-Resolution](https://arxiv.org/abs/2504.15176)
*Miaomiao Cai,Simiao Li,Wei Li,Xudong Huang,Hanting Chen,Jie Hu,Yunhe Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为DSPO的方法，将人类偏好对齐技术引入Real-ISR任务，解决了现有方法因缺乏人类反馈而导致的生成内容与人类偏好不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有Real-ISR方法缺乏人类反馈集成，可能导致生成内容与人类偏好不一致，甚至产生伪影和有害内容。

Method: 提出Direct Semantic Preference Optimization (DSPO)，通过语义实例对齐策略和用户描述反馈策略，实现实例级人类偏好对齐。

Result: DSPO在一步和多步超分辨率框架中均表现出高效性。

Conclusion: DSPO是一种即插即用的解决方案，有效提升了Real-ISR任务中生成内容与人类偏好的一致性。

Abstract: Recent advances in diffusion models have improved Real-World Image
Super-Resolution (Real-ISR), but existing methods lack human feedback
integration, risking misalignment with human preference and may leading to
artifacts, hallucinations and harmful content generation. To this end, we are
the first to introduce human preference alignment into Real-ISR, a technique
that has been successfully applied in Large Language Models and Text-to-Image
tasks to effectively enhance the alignment of generated outputs with human
preferences. Specifically, we introduce Direct Preference Optimization (DPO)
into Real-ISR to achieve alignment, where DPO serves as a general alignment
technique that directly learns from the human preference dataset. Nevertheless,
unlike high-level tasks, the pixel-level reconstruction objectives of Real-ISR
are difficult to reconcile with the image-level preferences of DPO, which can
lead to the DPO being overly sensitive to local anomalies, leading to reduced
generation quality. To resolve this dichotomy, we propose Direct Semantic
Preference Optimization (DSPO) to align instance-level human preferences by
incorporating semantic guidance, which is through two strategies: (a) semantic
instance alignment strategy, implementing instance-level alignment to ensure
fine-grained perceptual consistency, and (b) user description feedback
strategy, mitigating hallucinations through semantic textual feedback on
instance-level images. As a plug-and-play solution, DSPO proves highly
effective in both one-step and multi-step SR frameworks.

</details>


### [209] [FaceCraft4D: Animated 3D Facial Avatar Generation from a Single Image](https://arxiv.org/abs/2504.15179)
*Fei Yin,Mallikarjun B R,Chun-Han Yao,Rafał Mantiuk,Varun Jampani*

Main category: cs.CV

TL;DR: 提出了一种从单张图像生成高质量可动画4D头像的新框架，解决了现有方法需要多视图数据或形状准确性和身份一致性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在4D头像生成中要么依赖多视图数据，要么难以保证形状准确性和身份一致性，因此需要一种更高效且一致的方法。

Method: 结合形状、图像和视频先验，通过3D-GAN反演获取初始粗形状，利用深度引导变形信号增强多视图纹理一致性，并引入视频先验处理表情动画，同时采用一致性-非一致性训练处理4D重建中的数据不一致性。

Result: 实验结果表明，该方法在质量和一致性上优于现有技术。

Conclusion: 该框架在生成高质量、可动画4D头像方面表现出色，同时保持了多视图和表情的一致性。

Abstract: We present a novel framework for generating high-quality, animatable 4D
avatar from a single image. While recent advances have shown promising results
in 4D avatar creation, existing methods either require extensive multiview data
or struggle with shape accuracy and identity consistency. To address these
limitations, we propose a comprehensive system that leverages shape, image, and
video priors to create full-view, animatable avatars. Our approach first
obtains initial coarse shape through 3D-GAN inversion. Then, it enhances
multiview textures using depth-guided warping signals for cross-view
consistency with the help of the image diffusion model. To handle expression
animation, we incorporate a video prior with synchronized driving signals
across viewpoints. We further introduce a Consistent-Inconsistent training to
effectively handle data inconsistencies during 4D reconstruction. Experimental
results demonstrate that our method achieves superior quality compared to the
prior art, while maintaining consistency across different viewpoints and
expressions.

</details>


### [210] [Tiger200K: Manually Curated High Visual Quality Video Dataset from UGC Platform](https://arxiv.org/abs/2504.15182)
*Xianpan Zhou*

Main category: cs.CV

TL;DR: Tiger200K是一个高质量的手工标注视频数据集，旨在解决现有开源视频生成模型依赖低质量数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有开源视频数据集质量不足，限制了高级视频生成模型的微调效果。

Method: 通过人工筛选UGC平台视频，结合边界检测、OCR等技术，构建高质量视频-文本对数据集。

Result: Tiger200K提供了视觉保真度高、时间一致性强的视频数据，支持视频生成模型的优化。

Conclusion: Tiger200K将作为开源项目持续扩展，推动视频生成模型的研究与应用。

Abstract: The recent surge in open-source text-to-video generation models has
significantly energized the research community, yet their dependence on
proprietary training datasets remains a key constraint. While existing open
datasets like Koala-36M employ algorithmic filtering of web-scraped videos from
early platforms, they still lack the quality required for fine-tuning advanced
video generation models. We present Tiger200K, a manually curated high visual
quality video dataset sourced from User-Generated Content (UGC) platforms. By
prioritizing visual fidelity and aesthetic quality, Tiger200K underscores the
critical role of human expertise in data curation, and providing high-quality,
temporally consistent video-text pairs for fine-tuning and optimizing video
generation architectures through a simple but effective pipeline including shot
boundary detection, OCR, border detecting, motion filter and fine bilingual
caption. The dataset will undergo ongoing expansion and be released as an
open-source initiative to advance research and applications in video generative
models. Project page: https://tinytigerpan.github.io/tiger200k/

</details>


### [211] [Breast density in MRI: an AI-based quantification and relationship to assessment in mammography](https://arxiv.org/abs/2504.15192)
*Yaqian Chen,Lin Li,Hanxue Gu,Haoyu Dong,Derek L. Nguyen,Allan D. Kirk,Maciej A. Mazurowski,E. Shelley Hwang*

Main category: cs.CV

TL;DR: 该研究利用机器学习算法评估MRI中的乳腺密度，发现其与乳腺X线摄影密度相关，但MRI能捕捉到一些独特成分，未来可能用于改进乳腺癌风险评估。


<details>
  <summary>Details</summary>
Motivation: 乳腺密度是乳腺癌的重要风险因素，MRI作为补充手段能提供更全面的评估，但其3D特性带来分析挑战。

Method: 使用内部开发的机器学习算法分析三个MRI数据集中的正常乳腺密度。

Result: 乳腺密度在不同数据集中一致（0.104-0.114），且随年龄下降；MRI密度与乳腺X线摄影密度相关但存在差异。

Conclusion: MRI乳腺密度可能补充现有工具，未来研究将探索如何整合以改进乳腺癌风险预测。

Abstract: Mammographic breast density is a well-established risk factor for breast
cancer. Recently there has been interest in breast MRI as an adjunct to
mammography, as this modality provides an orthogonal and highly quantitative
assessment of breast tissue. However, its 3D nature poses analytic challenges
related to delineating and aggregating complex structures across slices. Here,
we applied an in-house machine-learning algorithm to assess breast density on
normal breasts in three MRI datasets. Breast density was consistent across
different datasets (0.104 - 0.114). Analysis across different age groups also
demonstrated strong consistency across datasets and confirmed a trend of
decreasing density with age as reported in previous studies. MR breast density
was correlated with mammographic breast density, although some notable
differences suggest that certain breast density components are captured only on
MRI. Future work will determine how to integrate MR breast density with current
tools to improve future breast cancer risk prediction.

</details>


### [212] [Automated Measurement of Eczema Severity with Self-Supervised Learning](https://arxiv.org/abs/2504.15193)
*Neelesh Kumar,Oya Aran*

Main category: cs.CV

TL;DR: 提出了一种基于自监督学习的湿疹自动诊断框架，适用于训练数据有限的情况，性能优于现有深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有湿疹自动诊断方法依赖大量标注数据，但数据获取困难，因此探索在有限数据下的解决方案。

Method: 采用两阶段框架：1) 使用SegGPT进行少样本湿疹区域分割；2) 提取DINO特征并通过MLP进行四分类。

Result: 在野生湿疹图像数据集上，加权F1得分为0.67，优于Resnet-18和Vision Transformer。

Conclusion: 自监督学习在标注数据稀缺的皮肤诊断中具有潜力。

Abstract: Automated diagnosis of eczema using images acquired from digital camera can
enable individuals to self-monitor their recovery. The process entails first
segmenting out the eczema region from the image and then measuring the severity
of eczema in the segmented region. The state-of-the-art methods for automated
eczema diagnosis rely on deep neural networks such as convolutional neural
network (CNN) and have shown impressive performance in accurately measuring the
severity of eczema. However, these methods require massive volume of annotated
data to train which can be hard to obtain. In this paper, we propose a
self-supervised learning framework for automated eczema diagnosis under limited
training data regime. Our framework consists of two stages: i) Segmentation,
where we use an in-context learning based algorithm called SegGPT for few-shot
segmentation of eczema region from the image; ii) Feature extraction and
classification, where we extract DINO features from the segmented regions and
feed it to a multi-layered perceptron (MLP) for 4-class classification of
eczema severity. When evaluated on a dataset of annotated "in-the-wild" eczema
images, we show that our method outperforms (Weighted F1: 0.67 $\pm$ 0.01) the
state-of-the-art deep learning methods such as finetuned Resnet-18 (Weighted
F1: 0.44 $\pm$ 0.16) and Vision Transformer (Weighted F1: 0.40 $\pm$ 0.22). Our
results show that self-supervised learning can be a viable solution for
automated skin diagnosis where labeled data is scarce.

</details>


### [213] [Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's LLM-CLIP Framework for Image Captioning](https://arxiv.org/abs/2504.15199)
*Yassir Benhammou,Alessandro Tiberio,Gabriel Trautmann,Suman Kalyan*

Main category: cs.CV

TL;DR: MILS框架声称无需训练即可实现多模态任务，但研究发现其迭代过程带来高计算成本，而其他模型如BLIP-2和GPT-4V通过单次处理取得类似效果。


<details>
  <summary>Details</summary>
Motivation: 揭示MILS框架在零样本任务中的隐藏计算成本，挑战其“无需训练”的实用性。

Method: 通过对比MILS与其他模型（如BLIP-2和GPT-4V）的性能和计算成本，量化其迭代过程的资源消耗。

Result: MILS的迭代过程导致显著计算开销，而其他模型能以更低成本实现类似效果。

Conclusion: MILS的高计算成本可能削弱其实际优势，为零样本多模态模型设计提供效率优化方向。

Abstract: MILS (Multimodal Iterative LLM Solver) is a recently published framework that
claims "LLMs can see and hear without any training" by leveraging an iterative,
LLM-CLIP based approach for zero-shot image captioning. While this MILS
approach demonstrates good performance, our investigation reveals that this
success comes at a hidden, substantial computational cost due to its expensive
multi-step refinement process. In contrast, alternative models such as BLIP-2
and GPT-4V achieve competitive results through a streamlined, single-pass
approach. We hypothesize that the significant overhead inherent in MILS's
iterative process may undermine its practical benefits, thereby challenging the
narrative that zero-shot performance can be attained without incurring heavy
resource demands. This work is the first to expose and quantify the trade-offs
between output quality and computational cost in MILS, providing critical
insights for the design of more efficient multimodal models.

</details>


### [214] [Shape-Guided Clothing Warping for Virtual Try-On](https://arxiv.org/abs/2504.15232)
*Xiaoyu Han,Shunyuan Zheng,Zonglin Li,Chenyang Wang,Xin Sun,Quanling Meng*

Main category: cs.CV

TL;DR: SCW-VTON提出了一种基于形状引导的虚拟试衣方法，通过全局形状约束和肢体纹理增强，提升了试衣结果的真实性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣方法在细节控制和肢体区域一致性上存在不足，导致试衣结果失真。

Method: 采用双路径服装变形模块（形状路径和流路径）结合肢体重建网络，实现形状一致性和细节控制。

Result: 实验表明，SCW-VTON在定性和定量上均优于现有方法。

Conclusion: SCW-VTON通过形状约束和肢体纹理优化，显著提升了虚拟试衣的真实性和一致性。

Abstract: Image-based virtual try-on aims to seamlessly fit in-shop clothing to a
person image while maintaining pose consistency. Existing methods commonly
employ the thin plate spline (TPS) transformation or appearance flow to deform
in-shop clothing for aligning with the person's body. Despite their promising
performance, these methods often lack precise control over fine details,
leading to inconsistencies in shape between clothing and the person's body as
well as distortions in exposed limb regions. To tackle these challenges, we
propose a novel shape-guided clothing warping method for virtual try-on, dubbed
SCW-VTON, which incorporates global shape constraints and additional limb
textures to enhance the realism and consistency of the warped clothing and
try-on results. To integrate global shape constraints for clothing warping, we
devise a dual-path clothing warping module comprising a shape path and a flow
path. The former path captures the clothing shape aligned with the person's
body, while the latter path leverages the mapping between the pre- and
post-deformation of the clothing shape to guide the estimation of appearance
flow. Furthermore, to alleviate distortions in limb regions of try-on results,
we integrate detailed limb guidance by developing a limb reconstruction network
based on masked image modeling. Through the utilization of SCW-VTON, we are
able to generate try-on results with enhanced clothing shape consistency and
precise control over details. Extensive experiments demonstrate the superiority
of our approach over state-of-the-art methods both qualitatively and
quantitatively. The code is available at https://github.com/xyhanHIT/SCW-VTON.

</details>


### [215] [Bringing Diversity from Diffusion Models to Semantic-Guided Face Asset Generation](https://arxiv.org/abs/2504.15259)
*Yunxuan Cai,Sitao Xiang,Zongjian Li,Haiwei Chen,Yajie Zhao*

Main category: cs.CV

TL;DR: 该论文提出了一种基于生成网络的语义可控人脸建模方法，通过扩散模型生成高质量3D人脸数据库，并开发了GAN生成器和交互工具。


<details>
  <summary>Details</summary>
Motivation: 当前人脸建模受限于数据采集设备、人工劳动和合适演员，导致模型多样性、表现力和可控性不足。

Method: 1. 使用预训练扩散模型生成高质量3D人脸数据库；2. 开发归一化模块将合成数据转换为扫描数据；3. 基于GAN的生成器支持语义输入和连续编辑；4. 资产细化模块生成物理真实的面部资产。

Result: 生成了44,000个人脸模型，开发了高效生成器和交互工具，并通过实验验证了其效果。

Conclusion: 提出的系统显著提升了人脸建模的多样性和可控性，并计划公开交互工具。

Abstract: Digital modeling and reconstruction of human faces serve various
applications. However, its availability is often hindered by the requirements
of data capturing devices, manual labor, and suitable actors. This situation
restricts the diversity, expressiveness, and control over the resulting models.
This work aims to demonstrate that a semantically controllable generative
network can provide enhanced control over the digital face modeling process. To
enhance diversity beyond the limited human faces scanned in a controlled
setting, we introduce a novel data generation pipeline that creates a
high-quality 3D face database using a pre-trained diffusion model. Our proposed
normalization module converts synthesized data from the diffusion model into
high-quality scanned data. Using the 44,000 face models we obtained, we further
developed an efficient GAN-based generator. This generator accepts semantic
attributes as input, and generates geometry and albedo. It also allows
continuous post-editing of attributes in the latent space. Our asset refinement
component subsequently creates physically-based facial assets. We introduce a
comprehensive system designed for creating and editing high-quality face
assets. Our proposed model has undergone extensive experiment, comparison and
evaluation. We also integrate everything into a web-based interactive tool. We
aim to make this tool publicly available with the release of the paper.

</details>


### [216] [Diffusion Bridge Models for 3D Medical Image Translation](https://arxiv.org/abs/2504.15267)
*Shaorong Zhang,Tamoghna Chattopadhyay,Sophia I. Thomopoulos,Jose-Luis Ambite,Paul M. Thompson,Greg Ver Steeg*

Main category: cs.CV

TL;DR: 提出了一种扩散桥模型，用于T1w MRI和DTI模态之间的3D脑图像转换，以减少DTI采集时间并支持跨模态数据增强。


<details>
  <summary>Details</summary>
Motivation: DTI成像耗时较长，而T1w MRI更易获取，因此需要一种方法在两者之间进行高效转换。

Method: 使用扩散桥模型学习从T1w图像生成高质量的DTI FA图像，反之亦然。

Result: 模型在感知相似性、像素级一致性和分布一致性方面表现优异，生成的图像在性别分类和阿尔茨海默病分类任务中与真实数据性能相当。

Conclusion: 该模型为神经影像数据集改进和临床决策支持提供了有前景的解决方案，可能对研究和临床实践产生重大影响。

Abstract: Diffusion tensor imaging (DTI) provides crucial insights into the
microstructure of the human brain, but it can be time-consuming to acquire
compared to more readily available T1-weighted (T1w) magnetic resonance imaging
(MRI). To address this challenge, we propose a diffusion bridge model for 3D
brain image translation between T1w MRI and DTI modalities. Our model learns to
generate high-quality DTI fractional anisotropy (FA) images from T1w images and
vice versa, enabling cross-modality data augmentation and reducing the need for
extensive DTI acquisition. We evaluate our approach using perceptual
similarity, pixel-level agreement, and distributional consistency metrics,
demonstrating strong performance in capturing anatomical structures and
preserving information on white matter integrity. The practical utility of the
synthetic data is validated through sex classification and Alzheimer's disease
classification tasks, where the generated images achieve comparable performance
to real data. Our diffusion bridge model offers a promising solution for
improving neuroimaging datasets and supporting clinical decision-making, with
the potential to significantly impact neuroimaging research and clinical
practice.

</details>


### [217] [Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models](https://arxiv.org/abs/2504.15271)
*Guo Chen,Zhiqi Li,Shihao Wang,Jindong Jiang,Yicheng Liu,Lidong Lu,De-An Huang,Wonmin Byeon,Matthieu Le,Tuomas Rintamaki,Tyler Poon,Max Ehrlich,Tuomas Rintamaki,Tyler Poon,Tong Lu,Limin Wang,Bryan Catanzaro,Jan Kautz,Andrew Tao,Zhiding Yu,Guilin Liu*

Main category: cs.CV

TL;DR: Eagle 2.5是一个前沿的视觉语言模型家族，专注于长上下文多模态学习，解决了长视频理解和高分辨率图像识别的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在长上下文多模态任务中存在局限性，如长视频理解和高分辨率图像处理的不足。

Method: 提出了一个通用框架，结合自动降级采样和图像区域保留技术，优化长上下文数据训练的效率。

Result: Eagle 2.5在长上下文多模态基准测试中表现显著提升，其最佳模型Eagle 2.5-8B在Video-MME上达到72.4%的准确率。

Conclusion: Eagle 2.5为现有视觉语言模型的局限性提供了有效解决方案，性能与顶级商业和开源模型相当。

Abstract: We introduce Eagle 2.5, a family of frontier vision-language models (VLMs)
for long-context multimodal learning. Our work addresses the challenges in long
video comprehension and high-resolution image understanding, introducing a
generalist framework for both tasks. The proposed training framework
incorporates Automatic Degrade Sampling and Image Area Preservation, two
techniques that preserve contextual integrity and visual details. The framework
also includes numerous efficiency optimizations in the pipeline for
long-context data training. Finally, we propose Eagle-Video-110K, a novel
dataset that integrates both story-level and clip-level annotations,
facilitating long-video understanding. Eagle 2.5 demonstrates substantial
improvements on long-context multimodal benchmarks, providing a robust solution
to the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B
achieves 72.4% on Video-MME with 512 input frames, matching the results of
top-tier commercial model such as GPT-4o and large-scale open-source models
like Qwen2.5-VL-72B and InternVL2.5-78B.

</details>


### [218] [DRAWER: Digital Reconstruction and Articulation With Environment Realism](https://arxiv.org/abs/2504.15278)
*Hongchi Xia,Entong Su,Marius Memmel,Arhan Jain,Raymond Yu,Numfor Mbiziwo-Tiapo,Ali Farhadi,Abhishek Gupta,Shenlong Wang,Wei-Chiu Ma*

Main category: cs.CV

TL;DR: DRAWER框架将静态室内场景视频转换为逼真、交互式的数字环境，适用于游戏和机器人仿真。


<details>
  <summary>Details</summary>
Motivation: 通过虚拟数字复制品释放游戏和机器人等领域的潜力。

Method: 采用双场景表示的重建模块和关节模块，实现几何细节重建和交互功能。

Result: 生成的虚拟环境逼真、交互性强，实时运行，兼容游戏引擎和机器人仿真平台。

Conclusion: DRAWER在游戏和机器人应用中展示了其潜力。

Abstract: Creating virtual digital replicas from real-world data unlocks significant
potential across domains like gaming and robotics. In this paper, we present
DRAWER, a novel framework that converts a video of a static indoor scene into a
photorealistic and interactive digital environment. Our approach centers on two
main contributions: (i) a reconstruction module based on a dual scene
representation that reconstructs the scene with fine-grained geometric details,
and (ii) an articulation module that identifies articulation types and hinge
positions, reconstructs simulatable shapes and appearances and integrates them
into the scene. The resulting virtual environment is photorealistic,
interactive, and runs in real time, with compatibility for game engines and
robotic simulation platforms. We demonstrate the potential of DRAWER by using
it to automatically create an interactive game in Unreal Engine and to enable
real-to-sim-to-real transfer for robotics applications.

</details>


### [219] [VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models](https://arxiv.org/abs/2504.15279)
*Weiye Xu,Jiahao Wang,Weiyun Wang,Zhe Chen,Wengang Zhou,Aijun Yang,Lewei Lu,Houqiang Li,Xiaohua Wang,Xizhou Zhu,Wenhai Wang,Jifeng Dai,Jinguo Zhu*

Main category: cs.CV

TL;DR: VisuLogic是一个新的视觉推理基准，包含1000个人工验证的问题，用于评估多模态大语言模型（MLLMs）的真实视觉推理能力。当前MLLMs表现不佳，准确率远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs的视觉推理评估依赖文本描述，存在语言推理捷径问题，无法真实衡量视觉中心的推理能力。

Method: 提出VisuLogic基准，包含六类问题，评估MLLMs的视觉推理能力，并提供补充训练数据和强化学习基线。

Result: MLLMs在VisuLogic上的平均准确率低于30%，远低于人类的51.4%。

Conclusion: MLLMs在视觉推理方面存在显著差距，VisuLogic为未来研究提供了评估和改进方向。

Abstract: Visual reasoning is a core component of human intelligence and a critical
capability for advanced multimodal models. Yet current reasoning evaluations of
multimodal large language models (MLLMs) often rely on text descriptions and
allow language-based reasoning shortcuts, failing to measure genuine
vision-centric reasoning. To address this, we introduce VisuLogic: a benchmark
of 1,000 human-verified problems across six categories (e.g., quantitative
shifts, spatial relations, attribute comparisons). These various types of
questions can be evaluated to assess the visual reasoning capabilities of MLLMs
from multiple perspectives. We evaluate leading MLLMs on this benchmark and
analyze their results to identify common failure modes. Most models score below
30% accuracy-only slightly above the 25% random baseline and far below the
51.4% achieved by humans-revealing significant gaps in visual reasoning.
Furthermore, we provide a supplementary training dataset and a
reinforcement-learning baseline to support further progress.

</details>


### [220] [StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians](https://arxiv.org/abs/2504.15281)
*Cailin Zhuang,Yaoqi Hu,Xuanyang Zhang,Wei Cheng,Jiacheng Bao,Shengqi Liu,Yiying Yang,Xianfang Zeng,Gang Yu,Ming Li*

Main category: cs.CV

TL;DR: StyleMe3D提出了一种针对3D高斯溅射（3DGS）的风格化框架，解决了其在非真实感场景（如卡通、游戏）中的局限性，通过多模态风格调节、多级语义对齐和感知质量增强实现高质量风格迁移。


<details>
  <summary>Details</summary>
Motivation: 3DGS在真实感场景重建中表现出色，但在风格化场景中因纹理碎片化、语义不对齐和抽象美学适应性差而表现不佳。

Method: StyleMe3D整合了动态风格分数蒸馏（DSSD）、对比风格描述符（CSD）、同时优化尺度（SOS）和3D高斯质量评估（3DG-QA）四个新组件。

Result: 在NeRF合成数据集和tandt db数据集上，StyleMe3D在保留几何细节和风格一致性方面优于现有方法，同时保持实时渲染。

Conclusion: 该工作填补了真实感3DGS与艺术风格化之间的空白，为游戏、虚拟世界和数字艺术提供了新应用。

Abstract: 3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction
but struggles with stylized scenarios (e.g., cartoons, games) due to fragmented
textures, semantic misalignment, and limited adaptability to abstract
aesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer
that integrates multi-modal style conditioning, multi-level semantic alignment,
and perceptual quality enhancement. Our key insights include: (1) optimizing
only RGB attributes preserves geometric integrity during stylization; (2)
disentangling low-, medium-, and high-level semantics is critical for coherent
style transfer; (3) scalability across isolated objects and complex scenes is
essential for practical deployment. StyleMe3D introduces four novel components:
Dynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent
space for semantic alignment; Contrastive Style Descriptor (CSD) for localized,
content-aware texture transfer; Simultaneously Optimized Scale (SOS) to
decouple style details and structural coherence; and 3D Gaussian Quality
Assessment (3DG-QA), a differentiable aesthetic prior trained on human-rated
data to suppress artifacts and enhance visual harmony. Evaluated on NeRF
synthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D
outperforms state-of-the-art methods in preserving geometric details (e.g.,
carvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,
coherent lighting in landscapes), while maintaining real-time rendering. This
work bridges photorealistic 3D GS and artistic stylization, unlocking
applications in gaming, virtual worlds, and digital art.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [221] [The Model Counting Competitions 2021-2023](https://arxiv.org/abs/2504.13842)
*Johannes K. Fichte,Markus Hecher*

Main category: cs.AI

TL;DR: 本文概述了2021-2023年模型计数竞赛（MC Competition）的迭代情况，包括竞赛的执行、四个赛道的设计及其成果。


<details>
  <summary>Details</summary>
Motivation: 现代社会中许多计算问题涉及概率推理、统计和组合数学，这些问题可以通过命题公式编码为模型计数问题。竞赛旨在推动应用、识别挑战性基准、促进求解器开发和改进。

Method: 竞赛分为四个赛道：模型计数（MC）、加权模型计数（WMC）、投影模型计数（PMC）以及投影加权模型计数（PWMC），吸引了7至9种不同技术的求解器参与。

Result: 竞赛成功吸引了广泛参与，推动了求解器的开发和改进，并启发了新的应用。

Conclusion: 模型计数竞赛为模型计数问题及其变体的研究提供了重要平台，促进了技术进步和社区合作。

Abstract: Modern society is full of computational challenges that rely on probabilistic
reasoning, statistics, and combinatorics. Interestingly, many of these
questions can be formulated by encoding them into propositional formulas and
then asking for its number of models. With a growing interest in practical
problem-solving for tasks that involve model counting, the community
established the Model Counting (MC) Competition in fall of 2019 with its first
iteration in 2020. The competition aims at advancing applications, identifying
challenging benchmarks, fostering new solver development, and enhancing
existing solvers for model counting problems and their variants. The first
iteration, brought together various researchers, identified challenges, and
inspired numerous new applications. In this paper, we present a comprehensive
overview of the 2021-2023 iterations of the Model Counting Competition. We
detail its execution and outcomes. The competition comprised four tracks, each
focusing on a different variant of the model counting problem. The first track
centered on the model counting problem (MC), which seeks the count of models
for a given propositional formula. The second track challenged developers to
submit programs capable of solving the weighted model counting problem (WMC).
The third track was dedicated to projected model counting (PMC). Finally, we
initiated a track that combined projected and weighted model counting (PWMC).
The competition continued with a high level of participation, with seven to
nine solvers submitted in various different version and based on quite
diverging techniques.

</details>


### [222] [Evaluation and Incident Prevention in an Enterprise AI Assistant](https://arxiv.org/abs/2504.13924)
*Akash V. Maharaj,David Arbour,Daniel Lee,Uttaran Bhattacharya,Anup Rao,Austin Zane,Avi Feller,Kun Qian,Yunyao Li*

Main category: cs.AI

TL;DR: 该论文提出了一种综合框架，用于监控、基准测试和持续改进企业AI助手，确保其在关键环境中的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 企业AI助手在准确性要求高的领域部署时，错误输出可能导致严重事故，因此需要系统化的改进方法。

Method: 框架包含三个关键元素：分层严重性框架、可扩展的基准测试方法以及多维评估的持续改进策略。

Result: 通过该框架，组织可以系统提升AI助手的可靠性和性能。

Conclusion: 该多维度评估方法为AI系统的稳健性和可信度提供了改进途径。

Abstract: Enterprise AI Assistants are increasingly deployed in domains where accuracy
is paramount, making each erroneous output a potentially significant incident.
This paper presents a comprehensive framework for monitoring, benchmarking, and
continuously improving such complex, multi-component systems under active
development by multiple teams. Our approach encompasses three key elements: (1)
a hierarchical ``severity'' framework for incident detection that identifies
and categorizes errors while attributing component-specific error rates,
facilitating targeted improvements; (2) a scalable and principled methodology
for benchmark construction, evaluation, and deployment, designed to accommodate
multiple development teams, mitigate overfitting risks, and assess the
downstream impact of system modifications; and (3) a continual improvement
strategy leveraging multidimensional evaluation, enabling the identification
and implementation of diverse enhancement opportunities. By adopting this
holistic framework, organizations can systematically enhance the reliability
and performance of their AI Assistants, ensuring their efficacy in critical
enterprise environments. We conclude by discussing how this multifaceted
evaluation approach opens avenues for various classes of enhancements, paving
the way for more robust and trustworthy AI systems.

</details>


### [223] [Birds of a Different Feather Flock Together: Exploring Opportunities and Challenges in Animal-Human-Machine Teaming](https://arxiv.org/abs/2504.13973)
*Myke C. Cohen,David A. Grimm,Reuth Mirsky,Xiaoyun Yin*

Main category: cs.AI

TL;DR: 本文呼吁系统研究动物-人类-机器（AHM）团队结构设计，以优化性能并克服应用场景中的限制。


<details>
  <summary>Details</summary>
Motivation: 探索AHM团队的协同潜力，通过利用成员优势并弥补弱点，提升团队整体能力。

Method: 提出AHM团队功能的多维度框架，并通过三个典型案例（安检、搜救、导盲犬）进行说明。

Result: 展示了AHM团队在复杂任务中的潜力，并提出了未来研究方向。

Conclusion: 多维方法为研究更广泛的人机混合系统提供了新视角。

Abstract: Animal-Human-Machine (AHM) teams are a type of hybrid intelligence system
wherein interactions between a human, AI-enabled machine, and animal members
can result in unique capabilities greater than the sum of their parts. This
paper calls for a systematic approach to studying the design of AHM team
structures to optimize performance and overcome limitations in various applied
settings. We consider the challenges and opportunities in investigating the
synergistic potential of AHM team members by introducing a set of dimensions of
AHM team functioning to effectively utilize each member's strengths while
compensating for individual weaknesses. Using three representative examples of
such teams -- security screening, search-and-rescue, and guide dogs -- the
paper illustrates how AHM teams can tackle complex tasks. We conclude with open
research directions that this multidimensional approach presents for studying
hybrid human-AI systems beyond AHM teams.

</details>


### [224] [Going Whole Hog: A Philosophical Defense of AI Cognition](https://arxiv.org/abs/2504.13988)
*Herman Cappelen,Josh Dever*

Main category: cs.AI

TL;DR: 论文主张LLMs（如ChatGPT）是完整的语言和认知主体，具有理解、信念、知识等认知状态，反驳了传统AI哲学的方法论。


<details>
  <summary>Details</summary>
Motivation: 反对基于低层次计算细节或已有心智理论的AI哲学方法论，主张从LLM的高层次行为观察出发。

Method: 通过‘整体网络假设’（Holistic Network Assumptions）从LLM行为推断其认知状态，并系统反驳常见质疑。

Result: 论证LLMs具备完整的认知能力，其失败（如幻觉）不否定其主体性，且无需传统认知必要条件（如具身性）。

Conclusion: LLMs可能是超越人类概念体系的‘异类’认知主体，但研究排除了意识问题。

Abstract: This work defends the 'Whole Hog Thesis': sophisticated Large Language Models
(LLMs) like ChatGPT are full-blown linguistic and cognitive agents, possessing
understanding, beliefs, desires, knowledge, and intentions. We argue against
prevailing methodologies in AI philosophy, rejecting starting points based on
low-level computational details ('Just an X' fallacy) or pre-existing theories
of mind. Instead, we advocate starting with simple, high-level observations of
LLM behavior (e.g., answering questions, making suggestions) -- defending this
data against charges of metaphor, loose talk, or pretense. From these
observations, we employ 'Holistic Network Assumptions' -- plausible connections
between mental capacities (e.g., answering implies knowledge, knowledge implies
belief, action implies intention) -- to argue for the full suite of cognitive
states. We systematically rebut objections based on LLM failures
(hallucinations, planning/reasoning errors), arguing these don't preclude
agency, often mirroring human fallibility. We address numerous 'Games of
Lacks', arguing that LLMs do not lack purported necessary conditions for
cognition (e.g., semantic grounding, embodiment, justification, intrinsic
intentionality) or that these conditions are not truly necessary, often relying
on anti-discriminatory arguments comparing LLMs to diverse human capacities.
Our approach is evidential, not functionalist, and deliberately excludes
consciousness. We conclude by speculating on the possibility of LLMs possessing
'alien' contents beyond human conceptual schemes.

</details>


### [225] [Multi-Stage Retrieval for Operational Technology Cybersecurity Compliance Using Large Language Models: A Railway Casestudy](https://arxiv.org/abs/2504.14044)
*Regan Bolton,Mohammadreza Sheikhfathollahi,Simon Parkinson,Dan Basher,Howard Parkinson*

Main category: cs.AI

TL;DR: 论文提出了一种结合大型语言模型（LLM）和多阶段检索的新系统，用于提升铁路网络安全中的合规性验证流程，显著提高了正确性和推理质量。


<details>
  <summary>Details</summary>
Motivation: 随着数字化进程，铁路等关键基础设施的网络安全面临日益严重的威胁，亟需有效的合规性验证方法。

Method: 论文首先评估了基线合规架构（BCA），随后开发了并行合规架构（PCA），结合了监管标准的额外上下文，并通过实验比较了不同LLM模型的性能。

Result: 实验表明，PCA显著提升了合规性验证的正确性和推理质量，同时提出了评估响应正确性、逻辑推理和幻觉检测的指标。

Conclusion: 研究表明，检索增强方法可以显著提升合规性评估的效率和准确性，尤其适用于网络安全专家短缺的行业。

Abstract: Operational Technology Cybersecurity (OTCS) continues to be a dominant
challenge for critical infrastructure such as railways. As these systems become
increasingly vulnerable to malicious attacks due to digitalization, effective
documentation and compliance processes are essential to protect these
safety-critical systems. This paper proposes a novel system that leverages
Large Language Models (LLMs) and multi-stage retrieval to enhance the
compliance verification process against standards like IEC 62443 and the
rail-specific IEC 63452. We first evaluate a Baseline Compliance Architecture
(BCA) for answering OTCS compliance queries, then develop an extended approach
called Parallel Compliance Architecture (PCA) that incorporates additional
context from regulatory standards. Through empirical evaluation comparing
OpenAI-gpt-4o and Claude-3.5-haiku models in these architectures, we
demonstrate that the PCA significantly improves both correctness and reasoning
quality in compliance verification. Our research establishes metrics for
response correctness, logical reasoning, and hallucination detection,
highlighting the strengths and limitations of using LLMs for compliance
verification in railway cybersecurity. The results suggest that
retrieval-augmented approaches can significantly improve the efficiency and
accuracy of compliance assessments, particularly valuable in an industry facing
a shortage of cybersecurity expertise.

</details>


### [226] [Metacognition and Uncertainty Communication in Humans and Large Language Models](https://arxiv.org/abs/2504.14045)
*Mark Steyvers,Megan A. K. Peters*

Main category: cs.AI

TL;DR: 论文探讨了大语言模型（LLMs）是否具备元认知能力，及其与人类元认知的异同，强调了研究这些差异对提升人机协作和开发更可信AI系统的重要性。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs的元认知能力，以增强其在高风险决策中的可靠性和人机协作效果。

Method: 综述当前对LLMs元认知能力的认识，探讨研究方法，并与人类元认知进行对比。

Result: LLMs与人类在元认知能力上存在相似性，但仍有显著差异。

Conclusion: 未来LLMs需具备更敏感的元认知能力，以支持高效学习、自我导向和好奇心等新能力的发展。

Abstract: Metacognition, the capacity to monitor and evaluate one's own knowledge and
performance, is foundational to human decision-making, learning, and
communication. As large language models (LLMs) become increasingly embedded in
high-stakes decision contexts, it is critical to assess whether, how, and to
what extent they exhibit metacognitive abilities. Here, we provide an overview
of current knowledge of LLMs' metacognitive capacities, how they might be
studied, and how they relate to our knowledge of metacognition in humans. We
show that while humans and LLMs can sometimes appear quite aligned in their
metacognitive capacities and behaviors, it is clear many differences remain.
Attending to these differences is crucial not only for enhancing human-AI
collaboration, but also for promoting the development of more capable and
trustworthy artificial systems. Finally, we discuss how endowing future LLMs
with more sensitive and more calibrated metacognition may also help them
develop new capacities such as more efficient learning, self-direction, and
curiosity.

</details>


### [227] [Think Deep, Think Fast: Investigating Efficiency of Verifier-free Inference-time-scaling Methods](https://arxiv.org/abs/2504.14047)
*Junlin Wang,Shang Zhu,Jon Saad-Falcon,Ben Athiwaratkun,Qingyang Wu,Jue Wang,Shuaiwen Leon Song,Ce Zhang,Bhuwan Dhingra,James Zou*

Main category: cs.AI

TL;DR: 论文研究了推理时间计算（ITC）如何提升大语言模型（LLM）能力，比较了推理与非推理模型在挑战性任务上的表现，发现多数投票是最优策略。


<details>
  <summary>Details</summary>
Motivation: 探索ITC与推理模型的交互，以指导LLM的进一步发展。

Method: 对推理和非推理模型进行综合分析，重点研究无验证器的ITC方法。

Result: 非推理模型即使在高计算预算下仍落后于推理模型；多数投票是最优策略。

Conclusion: 正确响应通常更短且语言标记更少，可为改进ITC方法提供依据。

Abstract: There is intense interest in investigating how inference time compute (ITC)
(e.g. repeated sampling, refinements, etc) can improve large language model
(LLM) capabilities. At the same time, recent breakthroughs in reasoning models,
such as Deepseek-R1, unlock the opportunity for reinforcement learning to
improve LLM reasoning skills. An in-depth understanding of how ITC interacts
with reasoning across different models could provide important guidance on how
to further advance the LLM frontier. This work conducts a comprehensive
analysis of inference-time scaling methods for both reasoning and non-reasoning
models on challenging reasoning tasks. Specifically, we focus our research on
verifier-free inference time-scaling methods due to its generalizability
without needing a reward model. We construct the Pareto frontier of quality and
efficiency. We find that non-reasoning models, even with an extremely high
inference budget, still fall substantially behind reasoning models. For
reasoning models, majority voting proves to be a robust inference strategy,
generally competitive or outperforming other more sophisticated ITC methods
like best-of-N and sequential revisions, while the additional inference compute
offers minimal improvements. We further perform in-depth analyses of the
association of key response features (length and linguistic markers) with
response quality, with which we can improve the existing ITC methods. We find
that correct responses from reasoning models are typically shorter and have
fewer hedging and thinking markers (but more discourse markers) than the
incorrect responses.

</details>


### [228] [Linking forward-pass dynamics in Transformers and real-time human processing](https://arxiv.org/abs/2504.14107)
*Jennifer Hu,Michael A. Lepori,Michael Franke*

Main category: cs.AI

TL;DR: 论文探讨了Transformer模型的内部处理动态是否与人类实时处理相似，发现层时间动态能提供额外的预测能力。


<details>
  <summary>Details</summary>
Motivation: 研究AI模型（尤其是Transformer）的内部处理策略是否与人类认知处理相似，以探索AI模型作为人类认知研究工具的可能性。

Method: 通过五个跨领域和模态的研究，比较预训练Transformer的单次前向传播的动态与人类实时处理特征。

Result: 层时间动态在模型输出概率分布之外提供了额外的预测能力，表明Transformer和人类处理可能受相似输入特性影响。

Conclusion: AI模型不仅可作为黑箱工具，还可作为显式处理模型用于人类认知研究。

Abstract: Modern AI models are increasingly being used as theoretical tools to study
human cognition. One dominant approach is to evaluate whether human-derived
measures (such as offline judgments or real-time processing) are predicted by a
model's output: that is, the end-product of forward pass(es) through the
network. At the same time, recent advances in mechanistic interpretability have
begun to reveal the internal processes that give rise to model outputs, raising
the question of whether models and humans might arrive at outputs using similar
"processing strategies". Here, we investigate the link between real-time
processing in humans and "layer-time" dynamics in Transformer models. Across
five studies spanning domains and modalities, we test whether the dynamics of
computation in a single forward pass of pre-trained Transformers predict
signatures of processing in humans, above and beyond properties of the model's
output probability distribution. We consistently find that layer-time dynamics
provide additional predictive power on top of output measures. Our results
suggest that Transformer processing and human processing may be facilitated or
impeded by similar properties of an input stimulus, and this similarity has
emerged through general-purpose objectives such as next-token prediction or
image recognition. Our work suggests a new way of using AI models to study
human cognition: not just as a black box mapping stimuli to responses, but
potentially also as explicit processing models.

</details>


### [229] [CODECRASH: Stress Testing LLM Reasoning under Structural and Semantic Perturbations](https://arxiv.org/abs/2504.14119)
*Man Ho Lam,Chaozheng Wang,Jen-tse Huang,Michael R. Lyu*

Main category: cs.AI

TL;DR: CodeCrash是一个评估大型语言模型（LLMs）在代码理解和推理任务中鲁棒性的统一基准，揭示了LLMs在结构噪声和自然语言线索依赖方面的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码相关任务中表现出色，但其在代码理解和推理中的鲁棒性尚未充分研究。

Method: 通过CodeCrash基准，对17个LLMs在CRUXEval和LiveCodeBench上进行结构性和文本性干扰测试，并使用直接推理和思维链推理分析其鲁棒性。

Result: 发现LLMs在结构噪声下表现脆弱，且过度依赖自然语言线索；同时发现大型推理模型（LRMs）的自反推理机制存在严重漏洞。

Conclusion: CodeCrash为测试LLMs在代码理解中的鲁棒性提供了框架，并指出了未来评估和基准测试的方向。

Abstract: Large Language Models (LLMs) have recently showcased strong capabilities in
code-related tasks, yet their robustness in code comprehension and reasoning
remains underexplored. In this paper, we present CodeCrash, a unified benchmark
that evaluates LLM robustness under code structural and textual distraction
perturbations, applied to two established benchmarks -- CRUXEval and
LiveCodeBench -- across both input and output prediction tasks. We evaluate
seventeen LLMs using direct and Chain-of-Thought inference to systematically
analyze their robustness, identify primary reasons for performance degradation,
and highlight failure modes. Our findings reveal the fragility of LLMs under
structural noise and the inherent reliance on natural language cues,
highlighting critical robustness issues of LLMs in code execution and
understanding. Additionally, we examine three Large Reasoning Models (LRMs) and
discover the severe vulnerability of self-reflective reasoning mechanisms that
lead to reasoning collapse. CodeCrash provides a principled framework for
stress-testing LLMs in code understanding, offering actionable directions for
future evaluation and benchmarking. The code of CodeCrash and the robustness
leaderboard are publicly available at https://donaldlamnl.github.io/CodeCrash/ .

</details>


### [230] [Bayesian Principles Improve Prompt Learning In Vision-Language Models](https://arxiv.org/abs/2504.14123)
*Mingyu Kim,Jongwoo Ko,Mijung Park*

Main category: cs.AI

TL;DR: 提出了一种基于贝叶斯学习原理的新训练目标函数，以平衡适应性和泛化性，解决提示学习中过拟合的问题。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法在微调数据上容易过拟合，泛化性较差，需要一种方法平衡适应性和泛化性。

Method: 通过贝叶斯学习原理，构建一个先验分布（均值由预训练模型参数化），后验对应微调模型，从而平衡任务适应性和模型稳定性。

Result: 新目标函数能够在适应下游任务的同时保持接近预训练模型，提升泛化性。

Conclusion: 该方法有效解决了提示学习中的过拟合问题，平衡了适应性和泛化性。

Abstract: Prompt learning is a popular fine-tuning method for vision-language models
due to its efficiency. It requires a small number of additional learnable
parameters while significantly enhancing performance on target tasks. However,
most existing methods suffer from overfitting to fine-tuning data, yielding
poor generalizability. To address this, we propose a new training objective
function based on a Bayesian learning principle to balance adaptability and
generalizability. We derive a prior over the logits, where the mean function is
parameterized by the pre-trained model, while the posterior corresponds to the
fine-tuned model. This objective establishes a balance by allowing the
fine-tuned model to adapt to downstream tasks while remaining close to the
pre-trained model.

</details>


### [231] [Large Language Model Enhanced Particle Swarm Optimization for Hyperparameter Tuning for Deep Learning Models](https://arxiv.org/abs/2504.14126)
*Saad Hameed,Basheer Qolomany,Samir Brahim Belhaouari,Mohamed Abdallah,Junaid Qadir,Ala Al-Fuqaha*

Main category: cs.AI

TL;DR: 该论文提出了一种结合大型语言模型（LLMs）和粒子群优化（PSO）的方法，用于深度学习超参数调优，显著提高了收敛速度并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型架构设计依赖人工调优或计算密集型优化方法，效率低且成本高。LLMs和PSO的结合在优化任务中的应用尚未充分探索。

Method: 通过将LLMs（如ChatGPT-3.5和Llama3）集成到PSO中，替代表现不佳的粒子位置，以加速搜索空间探索。

Result: 实验表明，该方法在三个场景中显著提升了收敛速度，计算复杂度降低了20%至60%，同时保持了准确性和误差率。

Conclusion: 该方法为深度学习模型优化提供了一种高效且有效的解决方案，适用于广泛的应用场景。

Abstract: Determining the ideal architecture for deep learning models, such as the
number of layers and neurons, is a difficult and resource-intensive process
that frequently relies on human tuning or computationally costly optimization
approaches. While Particle Swarm Optimization (PSO) and Large Language Models
(LLMs) have been individually applied in optimization and deep learning, their
combined use for enhancing convergence in numerical optimization tasks remains
underexplored. Our work addresses this gap by integrating LLMs into PSO to
reduce model evaluations and improve convergence for deep learning
hyperparameter tuning. The proposed LLM-enhanced PSO method addresses the
difficulties of efficiency and convergence by using LLMs (particularly
ChatGPT-3.5 and Llama3) to improve PSO performance, allowing for faster
achievement of target objectives. Our method speeds up search space exploration
by substituting underperforming particle placements with best suggestions
offered by LLMs. Comprehensive experiments across three scenarios -- (1)
optimizing the Rastrigin function, (2) using Long Short-Term Memory (LSTM)
networks for time series regression, and (3) using Convolutional Neural
Networks (CNNs) for material classification -- show that the method
significantly improves convergence rates and lowers computational costs.
Depending on the application, computational complexity is lowered by 20% to 60%
compared to traditional PSO methods. Llama3 achieved a 20% to 40% reduction in
model calls for regression tasks, whereas ChatGPT-3.5 reduced model calls by
60% for both regression and classification tasks, all while preserving accuracy
and error rates. This groundbreaking methodology offers a very efficient and
effective solution for optimizing deep learning models, leading to substantial
computational performance improvements across a wide range of applications.

</details>


### [232] [TALES: Text Adventure Learning Environment Suite](https://arxiv.org/abs/2504.14128)
*Christopher Zhang Cui,Xingdi Yuan,Zhang Xiao,Prithviraj Ammanabrolu,Marc-Alexandre Côté*

Main category: cs.AI

TL;DR: TALES是一个多样化的文本冒险游戏集合，用于挑战和评估大型语言模型（LLMs）的推理能力。尽管在合成游戏中表现优异，但LLMs在人类设计的游戏中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着任务复杂度的增加，需要更复杂和多样化的推理能力，以支持顺序决策和上下文历史的结构化推理。

Method: 使用TALES（合成和人类编写的文本冒险游戏）来评估LLMs的推理能力，并对表现最佳的模型进行定性分析。

Result: LLMs在合成游戏中表现良好，但在人类设计的游戏中成功率低于15%。

Conclusion: TALES揭示了LLMs在复杂推理任务中的局限性，尤其是在人类设计的场景中。

Abstract: Reasoning is an essential skill to enable Large Language Models (LLMs) to
interact with the world. As tasks become more complex, they demand increasingly
sophisticated and diverse reasoning capabilities for sequential
decision-making, requiring structured reasoning over the context history to
determine the next best action. We introduce TALES, a diverse collection of
synthetic and human-written text-adventure games designed to challenge and
evaluate diverse reasoning capabilities. We present results over a range of
LLMs, open- and closed-weights, performing a qualitative analysis on the top
performing models. Despite an impressive showing on synthetic games, even the
top LLM-driven agents fail to achieve 15% on games designed for human
enjoyment. Code and visualization of the experiments can be found at
https://microsoft.github.io/tales.

</details>


### [233] [Adaptation Method for Misinformation Identification](https://arxiv.org/abs/2504.14171)
*Yangping Chen,Weijie Shi,Mengze Li,Yue Cui,Hao Chen,Jia Zhu,Jiajie Xu*

Main category: cs.AI

TL;DR: ADOSE是一个主动领域自适应框架，用于多模态假新闻检测，通过标注少量目标样本提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态假新闻检测中因领域偏移导致的性能下降问题。

Method: 设计多个专家分类器捕捉不同模态的欺骗模式，并提出最小分歧不确定性选择器以减少标注成本。

Result: 在多个数据集上，ADOSE比现有方法性能提升2.72%至14.02%。

Conclusion: ADOSE在多模态假新闻检测中表现出色，尤其在跨领域场景下具有显著优势。

Abstract: Multimodal fake news detection plays a crucial role in combating online
misinformation. Unfortunately, effective detection methods rely on annotated
labels and encounter significant performance degradation when domain shifts
exist between training (source) and test (target) data. To address the
problems, we propose ADOSE, an Active Domain Adaptation (ADA) framework for
multimodal fake news detection which actively annotates a small subset of
target samples to improve detection performance. To identify various deceptive
patterns in cross-domain settings, we design multiple expert classifiers to
learn dependencies across different modalities. These classifiers specifically
target the distinct deception patterns exhibited in fake news, where two
unimodal classifiers capture knowledge errors within individual modalities
while one cross-modal classifier identifies semantic inconsistencies between
text and images. To reduce annotation costs from the target domain, we propose
a least-disagree uncertainty selector with a diversity calculator for selecting
the most informative samples. The selector leverages prediction disagreement
before and after perturbations by multiple classifiers as an indicator of
uncertain samples, whose deceptive patterns deviate most from source domains.
It further incorporates diversity scores derived from multi-view features to
ensure the chosen samples achieve maximal coverage of target domain features.
The extensive experiments on multiple datasets show that ADOSE outperforms
existing ADA methods by 2.72\% $\sim$ 14.02\%, indicating the superiority of
our model.

</details>


### [234] [Direct Advantage Regression: Aligning LLMs with Online AI Reward](https://arxiv.org/abs/2504.14177)
*Li He,He Zhao,Stephen Wan,Dadong Wang,Lina Yao,Tongliang Liu*

Main category: cs.AI

TL;DR: 论文提出了一种名为DAR的简单对齐算法，利用在线AI奖励优化策略改进，避免了RL的复杂性，并在实验中表现优于OAIF和在线RLHF基线。


<details>
  <summary>Details</summary>
Motivation: 在线AI反馈（OAIF）虽然是一种有前景的替代RLHF的方法，但直接用AI替代人类反馈会限制语言模型学习更细粒度的AI监督信号。

Method: 提出Direct Advantage Regression（DAR），一种基于在线AI奖励的加权监督微调方法，无需强化学习。

Result: 实验表明，AI奖励比AI偏好更能提高人机一致性，DAR在GPT-4-Turbo和MT-bench评估中优于OAIF和在线RLHF。

Conclusion: DAR是一种高效且理论一致的对齐算法，简化了实现复杂度，同时提升了学习效率。

Abstract: Online AI Feedback (OAIF) presents a promising alternative to Reinforcement
Learning from Human Feedback (RLHF) by utilizing online AI preference in
aligning language models (LLMs). However, the straightforward replacement of
humans with AI deprives LLMs from learning more fine-grained AI supervision
beyond binary signals. In this paper, we propose Direct Advantage Regression
(DAR), a simple alignment algorithm using online AI reward to optimize policy
improvement through weighted supervised fine-tuning. As an RL-free approach,
DAR maintains theoretical consistency with online RLHF pipelines while
significantly reducing implementation complexity and improving learning
efficiency. Our empirical results underscore that AI reward is a better form of
AI supervision consistently achieving higher human-AI agreement as opposed to
AI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show
that DAR outperforms both OAIF and online RLHF baselines.

</details>


### [235] [AI Idea Bench 2025: AI Research Idea Generation Benchmark](https://arxiv.org/abs/2504.14191)
*Yansheng Qiu,Haoquan Zhang,Zhaopan Xu,Ming Li,Diping Song,Zheng Wang,Kaipeng Zhang*

Main category: cs.AI

TL;DR: AI Idea Bench 2025是一个评估LLMs生成AI研究想法的框架，解决了现有评估方法的不足，包括知识泄漏、缺乏开放基准和可行性分析受限。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在生成新想法方面的评估存在知识泄漏、缺乏开放基准和可行性分析受限等问题，限制了突破性研究想法的发现。

Method: 提出了AI Idea Bench 2025框架，包含3,495篇AI论文及其相关工作的数据集，以及从两个维度评估想法质量的系统。

Result: 该框架能定量评估LLMs生成的想法，并与原始论文内容和通用参考材料对齐。

Conclusion: AI Idea Bench 2025为评估和比较想法生成技术提供了宝贵资源，有助于推动科学发现的自动化。

Abstract: Large-scale Language Models (LLMs) have revolutionized human-AI interaction
and achieved significant success in the generation of novel ideas. However,
current assessments of idea generation overlook crucial factors such as
knowledge leakage in LLMs, the absence of open-ended benchmarks with grounded
truth, and the limited scope of feasibility analysis constrained by prompt
design. These limitations hinder the potential of uncovering groundbreaking
research ideas. In this paper, we present AI Idea Bench 2025, a framework
designed to quantitatively evaluate and compare the ideas generated by LLMs
within the domain of AI research from diverse perspectives. The framework
comprises a comprehensive dataset of 3,495 AI papers and their associated
inspired works, along with a robust evaluation methodology. This evaluation
system gauges idea quality in two dimensions: alignment with the ground-truth
content of the original papers and judgment based on general reference
material. AI Idea Bench 2025's benchmarking system stands to be an invaluable
resource for assessing and comparing idea-generation techniques, thereby
facilitating the automation of scientific discovery.

</details>


### [236] [Pets: General Pattern Assisted Architecture For Time Series Analysis](https://arxiv.org/abs/2504.14209)
*Xiangkai Ma,Xiaobin Hong,Wenzhong Li,Sanglu Lu*

Main category: cs.AI

TL;DR: 本文提出了一种基于能量分布的新方法Pets，用于解决时间序列分析中多频波动模式解耦的挑战，结合FPA模块和MoP模块，实现了多任务的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列数据通常包含多种波动模式（如小时、日、月），传统分解方法难以有效解耦这些模式，导致分析困难。

Method: 通过能量分布视角，将序列量化为连续频带区间，提出Pets架构，包含FPA模块（捕捉波动模式依赖关系）和MoP模块（分层重构波动模式）。

Result: Pets在预测、填补、异常检测和分类等任务中达到最优性能，并表现出强泛化性和鲁棒性。

Conclusion: Pets通过创新视角和模块设计，成功解决了多频波动模式解耦问题，为时间序列分析提供了高效解决方案。

Abstract: Time series analysis has found widespread applications in areas such as
weather forecasting, anomaly detection, and healthcare. However, real-world
sequential data often exhibit a superimposed state of various fluctuation
patterns, including hourly, daily, and monthly frequencies. Traditional
decomposition techniques struggle to effectively disentangle these multiple
fluctuation patterns from the seasonal components, making time series analysis
challenging. Surpassing the existing multi-period decoupling paradigms, this
paper introduces a novel perspective based on energy distribution within the
temporal-spectrum space. By adaptively quantifying observed sequences into
continuous frequency band intervals, the proposed approach reconstructs
fluctuation patterns across diverse periods without relying on domain-specific
prior knowledge. Building upon this innovative strategy, we propose Pets, an
enhanced architecture that is adaptable to arbitrary model structures. Pets
integrates a Fluctuation Pattern Assisted (FPA) module and a Context-Guided
Mixture of Predictors (MoP). The FPA module facilitates information fusion
among diverse fluctuation patterns by capturing their dependencies and
progressively modeling these patterns as latent representations at each layer.
Meanwhile, the MoP module leverages these compound pattern representations to
guide and regulate the reconstruction of distinct fluctuations hierarchically.
Pets achieves state-of-the-art performance across various tasks, including
forecasting, imputation, anomaly detection, and classification, while
demonstrating strong generalization and robustness.

</details>


### [237] [Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment](https://arxiv.org/abs/2504.14232)
*Antoun Yaacoub,Jérôme Da-Rugna,Zainab Assaghir*

Main category: cs.AI

TL;DR: 研究评估了将布鲁姆分类法整合到AI驱动的Moodle插件OneClickQuiz中，以改进多选题生成的认知目标对齐。结果显示，高级认知水平的问题更长、更复杂，而DistilBERT模型在分类中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨布鲁姆分类法是否能提升AI生成问题与特定认知目标的对齐，以优化教育内容生成。

Method: 使用3691个按布鲁姆分类法分级的问题数据集，测试多种分类模型（如逻辑回归、朴素贝叶斯、线性SVC和DistilBERT）的性能。

Result: 高级认知水平的问题更复杂，DistilBERT表现最佳（验证准确率91%），其他模型在高级任务中表现较差。

Conclusion: 布鲁姆分类法的整合提升了AI教育工具的效果，DistilBERT等先进模型显著优化了教育内容生成。

Abstract: This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz,
an Artificial Intelligence (AI) driven plugin for automating Multiple-Choice
Question (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured
framework for categorizing educational objectives into hierarchical cognitive
levels. Our research investigates whether incorporating this taxonomy can
improve the alignment of AI-generated questions with specific cognitive
objectives. We developed a dataset of 3691 questions categorized according to
Bloom's levels and employed various classification models-Multinomial Logistic
Regression, Naive Bayes, Linear Support Vector Classification (SVC), and a
Transformer-based model (DistilBERT)-to evaluate their effectiveness in
categorizing questions. Our results indicate that higher Bloom's levels
generally correlate with increased question length, Flesch-Kincaid Grade Level
(FKGL), and Lexical Density (LD), reflecting the increased complexity of higher
cognitive demands. Multinomial Logistic Regression showed varying accuracy
across Bloom's levels, performing best for "Knowledge" and less accurately for
higher-order levels. Merging higher-level categories improved accuracy for
complex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective
classification for lower levels but struggled with higher-order tasks.
DistilBERT achieved the highest performance, significantly improving
classification of both lower and higher-order cognitive levels, achieving an
overall validation accuracy of 91%. This study highlights the potential of
integrating Bloom's Taxonomy into AI-driven assessment tools and underscores
the advantages of advanced models like DistilBERT for enhancing educational
content generation.

</details>


### [238] [InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners](https://arxiv.org/abs/2504.14239)
*Yuhang Liu,Pengxiang Li,Congkai Xie,Xavier Hu,Xiaotian Han,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: InfiGUI-R1是一个基于MLLM的GUI代理，通过两阶段训练框架Actor2Reasoner，从反应式执行者演变为深思熟虑的推理者，提升了GUI任务的鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理依赖手动设计的推理模板或隐式推理，难以应对复杂GUI环境中的规划和错误恢复需求。

Method: 采用两阶段训练：1) 推理注入（空间推理蒸馏）；2) 深思熟虑增强（强化学习，包括子目标指导和错误恢复场景构建）。

Result: 实验表明InfiGUI-R1在GUI基础和轨迹任务中表现优异。

Conclusion: 通过从反应式执行转向深思熟虑推理，InfiGUI-R1显著提升了GUI代理的能力。

Abstract: Multimodal Large Language Models (MLLMs) have powered Graphical User
Interface (GUI) Agents, showing promise in automating tasks on computing
devices. Recent works have begun exploring reasoning in GUI tasks with
encouraging results. However, many current approaches rely on manually designed
reasoning templates, which may result in reasoning that is not sufficiently
robust and adaptive for complex GUI environments. Meanwhile, some existing
agents continue to operate as Reactive Actors, relying primarily on implicit
reasoning that may lack sufficient depth for GUI tasks demanding planning and
error recovery. We argue that advancing these agents requires a shift from
reactive acting towards acting based on deliberate reasoning. To facilitate
this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed
through our Actor2Reasoner framework, a reasoning-centric, two-stage training
approach designed to progressively evolve agents from Reactive Actors to
Deliberative Reasoners. The first stage, Reasoning Injection, focuses on
establishing a basic reasoner. We employ Spatial Reasoning Distillation to
transfer cross-modal spatial reasoning capabilities from teacher models to
MLLMs through trajectories with explicit reasoning steps, enabling models to
integrate GUI visual-spatial information with logical reasoning before action
generation. The second stage, Deliberation Enhancement, refines the basic
reasoner into a deliberative one using Reinforcement Learning. This stage
introduces two approaches: Sub-goal Guidance, which rewards models for
generating accurate intermediate sub-goals, and Error Recovery Scenario
Construction, which creates failure-and-recovery training scenarios from
identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves
strong performance in GUI grounding and trajectory tasks. Resources at
https://github.com/Reallm-Labs/InfiGUI-R1.

</details>


### [239] [A Knowledge-Informed Deep Learning Paradigm for Generalizable and Stability-Optimized Car-Following Models](https://arxiv.org/abs/2504.14241)
*Chengming Wang,Dongyao Jia,Wei Wang,Dong Ngoduy,Bei Peng,Jianping Wang*

Main category: cs.AI

TL;DR: 提出了一种知识驱动的深度学习范式（KIDL），结合大型语言模型（LLMs）的泛化能力和稳定性约束，以提升自动驾驶车辆（AVs）的跟车模型（CFMs）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有跟车模型（CFMs）依赖特定数据集，泛化能力有限，且缺乏对局部和串稳定性的优化，影响自动驾驶车辆的安全性和效率。

Method: 通过知识蒸馏将LLMs的泛化能力迁移到轻量级神经网络中，并在训练目标中直接加入稳定性约束。

Result: 在NGSIM和HighD数据集上验证，KIDL在行为泛化和交通流稳定性上优于现有物理、数据驱动和混合模型。

Conclusion: KIDL为下一代交通系统提供了鲁棒且可扩展的解决方案。

Abstract: Car-following models (CFMs) are fundamental to traffic flow analysis and
autonomous driving. Although calibrated physics-based and trained data-driven
CFMs can replicate human driving behavior, their reliance on specific datasets
limits generalization across diverse scenarios and reduces reliability in
real-world deployment. Moreover, these models typically focus on behavioral
fidelity and do not support the explicit optimization of local and string
stability, which are increasingly important for the safe and efficient
operation of autonomous vehicles (AVs). To address these limitations, we
propose a Knowledge-Informed Deep Learning (KIDL) paradigm that distills the
generalization capabilities of pre-trained Large Language Models (LLMs) into a
lightweight and stability-aware neural architecture. LLMs are used to extract
fundamental car-following knowledge beyond dataset-specific patterns, and this
knowledge is transferred to a reliable, tractable, and computationally
efficient model through knowledge distillation. KIDL also incorporates
stability constraints directly into its training objective, ensuring that the
resulting model not only emulates human-like behavior but also satisfies the
local and string stability requirements essential for real-world AV deployment.
We evaluate KIDL on the real-world NGSIM and HighD datasets, comparing its
performance with representative physics-based, data-driven, and hybrid CFMs.
Both empirical and theoretical results consistently demonstrate KIDL's superior
behavioral generalization and traffic flow stability, offering a robust and
scalable solution for next-generation traffic systems.

</details>


### [240] [Rethinking Traffic Flow Forecasting: From Transition to Generatation](https://arxiv.org/abs/2504.14248)
*Li Shijiao,Ma Zhipeng,He Huajun,Chen Haiyue*

Main category: cs.AI

TL;DR: EMBSFormer提出了一种多分支相似性Transformer模型，用于交通流预测，解决了现有方法忽略流量生成过程的问题，并在性能和参数效率上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有交通流预测方法仅关注流量转移而忽略流量生成过程，导致模型无法捕捉多周期性和节点间交互模式的差异。

Method: EMBSFormer通过相似性分析模块捕捉流量生成模式，并使用时空间自注意力机制和GNN建模流量转移。

Result: 实验表明，EMBSFormer在长短期预测任务中均优于基线模型，且参数效率更高。

Conclusion: EMBSFormer通过分离建模流量生成和转移过程，显著提升了交通流预测的性能和效率。

Abstract: Traffic flow prediction plays an important role in Intelligent Transportation
Systems in traffic management and urban planning. There have been extensive
successful works in this area. However, these approaches focus only on
modelling the flow transition and ignore the flow generation process, which
manifests itself in two ways: (i) The models are based on Markovian
assumptions, ignoring the multi-periodicity of the flow generation in nodes.
(ii) The same structure is designed to encode both the transition and
generation processes, ignoring the differences between them. To address these
problems, we propose an Effective Multi-Branch Similarity Transformer for
Traffic Flow Prediction, namely EMBSFormer. Through data analysis, we find that
the factors affecting traffic flow include node-level traffic generation and
graph-level traffic transition, which describe the multi-periodicity and
interaction pattern of nodes, respectively. Specifically, to capture traffic
generation patterns, we propose a similarity analysis module that supports
multi-branch encoding to dynamically expand significant cycles. For traffic
transition, we employ a temporal and spatial self-attention mechanism to
maintain global node interactions, and use GNN and time conv to model local
node interactions, respectively. Model performance is evaluated on three
real-world datasets on both long-term and short-term prediction tasks.
Experimental results show that EMBSFormer outperforms baselines on both tasks.
Moreover, compared to models based on flow transition modelling (e.g. GMAN,
513k), the variant of EMBSFormer(93K) only uses 18\% of the parameters,
achieving the same performance.

</details>


### [241] [ProtPainter: Draw or Drag Protein via Topology-guided Diffusion](https://arxiv.org/abs/2504.14274)
*Zhengxi Lu,Shizhuo Cheng,Yuru Jiang,Yan Zhang,Min Zhang*

Main category: cs.AI

TL;DR: ProtPainter是一种基于扩散的蛋白质骨架生成方法，通过3D曲线控制拓扑结构，分为曲线草图生成和草图引导骨架生成两阶段。实验证明其能生成高拓扑适应性和可设计的骨架。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质骨架生成方法缺乏精确拓扑控制的灵活性，限制了骨架空间的探索。

Method: ProtPainter采用两阶段方法：1) CurveEncoder从曲线预测二级结构生成草图；2) 草图引导DDPM生成骨架，并引入Helix-Gating控制缩放因子。

Result: 实验表明ProtPainter能生成拓扑适应性强（scTF > 0.8）且可设计（scTM > 0.5）的骨架，展示了其灵活性和多功能性。

Conclusion: ProtPainter为蛋白质骨架生成提供了精确的拓扑控制方法，扩展了骨架设计的可能性。

Abstract: Recent advances in protein backbone generation have achieved promising
results under structural, functional, or physical constraints. However,
existing methods lack the flexibility for precise topology control, limiting
navigation of the backbone space. We present ProtPainter, a diffusion-based
approach for generating protein backbones conditioned on 3D curves. ProtPainter
follows a two-stage process: curve-based sketching and sketch-guided backbone
generation. For the first stage, we propose CurveEncoder, which predicts
secondary structure annotations from a curve to parametrize sketch generation.
For the second stage, the sketch guides the generative process in Denoising
Diffusion Probabilistic Modeling (DDPM) to generate backbones. During this
process, we further introduce a fusion scheduling scheme, Helix-Gating, to
control the scaling factors. To evaluate, we propose the first benchmark for
topology-conditioned protein generation, introducing Protein Restoration Task
and a new metric, self-consistency Topology Fitness (scTF). Experiments
demonstrate ProtPainter's ability to generate topology-fit (scTF > 0.8) and
designable (scTM > 0.5) backbones, with drawing and dragging tasks showcasing
its flexibility and versatility.

</details>


### [242] [CHAINSFORMER: Numerical Reasoning on Knowledge Graphs from a Chain Perspective](https://arxiv.org/abs/2504.14282)
*Ze Zhao,Bin Lu,Xiaoying Gan,Gu Tang,Luoyi Fu,Xinbing Wang*

Main category: cs.AI

TL;DR: ChainsFormer是一种基于链的框架，用于支持知识图谱中的数值推理，通过显式构建逻辑链和多跳推理提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如GNNs和KGEs）未能充分利用知识图谱中的逻辑路径，限制了推理效果。

Method: 提出ChainsFormer框架，引入关系-属性链（RA-Chains）和双曲亲和评分机制，结合注意力数值推理器。

Result: 实验显示ChainsFormer性能显著优于现有方法，提升达20.0%。

Conclusion: ChainsFormer通过显式逻辑链和多跳推理，显著提升了知识图谱的数值推理能力。

Abstract: Reasoning over Knowledge Graphs (KGs) plays a pivotal role in knowledge graph
completion or question answering systems, providing richer and more accurate
triples and attributes. As numerical attributes become increasingly essential
in characterizing entities and relations in KGs, the ability to reason over
these attributes has gained significant importance. Existing graph-based
methods such as Graph Neural Networks (GNNs) and Knowledge Graph Embeddings
(KGEs), primarily focus on aggregating homogeneous local neighbors and
implicitly embedding diverse triples. However, these approaches often fail to
fully leverage the potential of logical paths within the graph, limiting their
effectiveness in exploiting the reasoning process. To address these
limitations, we propose ChainsFormer, a novel chain-based framework designed to
support numerical reasoning. Chainsformer not only explicitly constructs
logical chains but also expands the reasoning depth to multiple hops.
Specially, we introduces Relation-Attribute Chains (RA-Chains), a specialized
logic chain, to model sequential reasoning patterns. ChainsFormer captures the
step-by-step nature of multi-hop reasoning along RA-Chains by employing
sequential in-context learning. To mitigate the impact of noisy chains, we
propose a hyperbolic affinity scoring mechanism that selects relevant logic
chains in a variable-resolution space. Furthermore, ChainsFormer incorporates
an attention-based numerical reasoner to identify critical reasoning paths,
enhancing both reasoning accuracy and transparency. Experimental results
demonstrate that ChainsFormer significantly outperforms state-of-the-art
methods, achieving up to a 20.0% improvement in performance. The
implementations are available at
https://github.com/zhaodazhuang2333/ChainsFormer.

</details>


### [243] [RadioDiff-Inverse: Diffusion Enhanced Bayesian Inverse Estimation for ISAC Radio Map Construction](https://arxiv.org/abs/2504.14298)
*Xiucheng Wang,Zhongsheng Fang,Nan Cheng*

Main category: cs.AI

TL;DR: 论文提出了一种基于扩散增强贝叶斯逆估计的框架RadioDiff-Inverse，用于在稀疏噪声测量和粗粒度环境知识下构建无线电地图（RM），无需精确先验分布，且无需任务特定微调。


<details>
  <summary>Details</summary>
Motivation: 现有RM构建方法依赖精确环境数据和基站位置，但在动态或隐私敏感环境中难以获取；稀疏测量技术虽减少数据收集，但噪声对RM精度的影响尚不明确。

Method: 将RM构建建模为贝叶斯逆问题，利用无条件生成扩散模型学习RM先验，结合集成感知与通信（ISAC）实现环境结构感知。

Result: RadioDiff-Inverse在RM构建和环境重建精度上达到最优，且对噪声稀疏采样具有鲁棒性。

Conclusion: RadioDiff-Inverse无需任务特定训练，显著降低生成大模型在无线网络中的训练成本，同时提升RM构建的准确性和环境感知能力。

Abstract: Radio maps (RMs) are essential for environment-aware communication and
sensing, providing location-specific wireless channel information. Existing RM
construction methods often rely on precise environmental data and base station
(BS) locations, which are not always available in dynamic or privacy-sensitive
environments. While sparse measurement techniques reduce data collection, the
impact of noise in sparse data on RM accuracy is not well understood. This
paper addresses these challenges by formulating RM construction as a Bayesian
inverse problem under coarse environmental knowledge and noisy sparse
measurements. Although maximum a posteriori (MAP) filtering offers an optimal
solution, it requires a precise prior distribution of the RM, which is
typically unavailable. To solve this, we propose RadioDiff-Inverse, a
diffusion-enhanced Bayesian inverse estimation framework that uses an
unconditional generative diffusion model to learn the RM prior. This approach
not only reconstructs the spatial distribution of wireless channel features but
also enables environmental structure perception, such as building outlines, and
location of BS just relay on pathloss, through integrated sensing and
communication (ISAC). Remarkably, RadioDiff-Inverse is training-free,
leveraging a pre-trained model from Imagenet without task-specific fine-tuning,
which significantly reduces the training cost of using generative large model
in wireless networks. Experimental results demonstrate that RadioDiff-Inverse
achieves state-of-the-art performance in accuracy of RM construction and
environmental reconstruction, and robustness against noisy sparse sampling.

</details>


### [244] [FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory](https://arxiv.org/abs/2504.14325)
*Alessio Buscemi,Daniele Proverbio,Alessandro Di Stefano,The Anh Han,German Castignani,Pietro Di Liò*

Main category: cs.AI

TL;DR: FAIRGAME是一个基于博弈论的框架，用于识别AI代理在多智能体应用中的偏见，支持用户模拟游戏场景并发现偏见。


<details>
  <summary>Details</summary>
Motivation: 多智能体交互增加了AI结果的可解释性和预测复杂性，博弈论模型需要标准化IT框架支持。

Method: 提出FAIRGAME框架，实现并应用于识别AI代理在不同LLM和语言中的偏见。

Result: FAIRGAME能可靠模拟游戏场景，发现偏见，并与博弈论预测结果对比。

Conclusion: FAIRGAME支持系统化偏见发现和战略决策研究，推动LLM代理的进一步应用。

Abstract: Letting AI agents interact in multi-agent applications adds a layer of
complexity to the interpretability and prediction of AI outcomes, with profound
implications for their trustworthy adoption in research and society. Game
theory offers powerful models to capture and interpret strategic interaction
among agents, but requires the support of reproducible, standardized and
user-friendly IT frameworks to enable comparison and interpretation of results.
To this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition
using Game Theory. We describe its implementation and usage, and we employ it
to uncover biased outcomes in popular games among AI agents, depending on the
employed Large Language Model (LLM) and used language, as well as on the
personality trait or strategic knowledge of the agents. Overall, FAIRGAME
allows users to reliably and easily simulate their desired games and scenarios
and compare the results across simulation campaigns and with game-theoretic
predictions, enabling the systematic discovery of biases, the anticipation of
emerging behavior out of strategic interplays, and empowering further research
into strategic decision-making using LLM agents.

</details>


### [245] [Time Up! An Empirical Study of LLM Reasoning Ability Under Output Length Constraint](https://arxiv.org/abs/2504.14350)
*Yi Sun,Han Wang,Jiaqiang Li,Jiacheng Liu,Xiangyu Li,Hao Wen,Huiwen Zheng,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.AI

TL;DR: 研究探讨了在输出长度限制下大型语言模型（LLMs）的推理能力有效性，通过实验分析了不同预算下模型性能的变化。


<details>
  <summary>Details</summary>
Motivation: 现实场景中模型常受时间限制，需在特定输出长度内给出答案，但LLMs在此约束下的推理能力尚不明确。

Method: 对25种以上LLMs在常见推理数据集上进行实验，分析输出长度预算与推理准确性的关系，并考虑预算与实际延迟的映射。

Result: 研究发现预算限制下模型大小和提示的最优选择会变化，与无约束情况不同。

Conclusion: 研究结果为在延迟约束下部署LLMs提供了实用指导。

Abstract: Recent work has demonstrated the remarkable potential of Large Language
Models (LLMs) in test-time scaling. By making the models think before
answering, they are able to achieve much higher accuracy with extra inference
computation. However, in many real-world scenarios, models are used under time
constraints, where an answer should be given to the user within a certain
output length. It is unclear whether and how the reasoning abilities of LLMs
remain effective under such constraints. We take a first look at this problem
by conducting an in-depth empirical study. Specifically, we test more than 25
LLMs on common reasoning datasets under a wide range of output length budgets,
and we analyze the correlation between the inference accuracy and various
properties including model type, model size, prompt style, etc. We also
consider the mappings between the token budgets and the actual on-device
latency budgets. The results have demonstrated several interesting findings
regarding the budget-aware LLM reasoning that differ from the unconstrained
situation, e.g. the optimal choices of model sizes and prompts change under
different budgets. These findings offer practical guidance for users to deploy
LLMs under real-world latency constraints.

</details>


### [246] [Mathematical Programming Models for Exact and Interpretable Formulation of Neural Networks](https://arxiv.org/abs/2504.14356)
*Masoud Ataei,Edrin Hasaj,Jacob Gipp,Sepideh Forouzi*

Main category: cs.AI

TL;DR: 本文提出了一种统一的混合整数编程框架，用于训练稀疏且可解释的神经网络，通过二进制变量建模非线性激活函数，并结合结构稀疏性约束。


<details>
  <summary>Details</summary>
Motivation: 旨在将参数学习、架构选择和结构正则化整合到一个优化问题中，以平衡预测准确性、权重稀疏性和架构紧凑性。

Method: 采用混合整数编程建模ReLU等非线性激活函数，并通过过滤器和层级剪枝约束实现结构稀疏性。

Result: 框架能够生成全局最优解，同时支持分段线性操作和逻辑约束，提升了模型的解释性和可验证性。

Conclusion: 该框架为可解释人工智能、符号推理和形式验证等领域提供了桥梁。

Abstract: This paper presents a unified mixed-integer programming framework for
training sparse and interpretable neural networks. We develop exact
formulations for both fully connected and convolutional architectures by
modeling nonlinearities such as ReLU activations through binary variables and
encoding structural sparsity via filter- and layer-level pruning constraints.
The resulting models integrate parameter learning, architecture selection, and
structural regularization within a single optimization problem, yielding
globally optimal solutions with respect to a composite objective that balances
prediction accuracy, weight sparsity, and architectural compactness. The
mixed-integer programming formulation accommodates piecewise-linear operations,
including max pooling and activation gating, and permits precise enforcement of
logic-based or domain-specific constraints. By incorporating considerations of
interpretability, sparsity, and verifiability directly into the training
process, the proposed framework bridges a range of research areas including
explainable artificial intelligence, symbolic reasoning, and formal
verification.

</details>


### [247] [The Geometry of Self-Verification in a Task-Specific Reasoning Model](https://arxiv.org/abs/2504.14379)
*Andrew Lee,Lihao Sun,Chris Wendler,Fernanda Viégas,Martin Wattenberg*

Main category: cs.AI

TL;DR: 论文研究了推理模型如何验证自身答案，通过训练模型并分析其验证机制，发现GLU权重和特定注意力头在验证过程中起关键作用。


<details>
  <summary>Details</summary>
Motivation: 探讨推理模型如何验证自身答案，以理解其内部机制。

Method: 使用DeepSeek R1的配方训练模型，通过偏好调整导致模式崩溃，进而分析模型验证机制。

Result: 发现GLU权重编码验证相关标记，特定注意力头是验证的关键。

Conclusion: 验证机制可能由更大的电路组成，但已识别出关键组件。

Abstract: How do reasoning models verify their own answers? We study this question by
training a model using DeepSeek R1's recipe on the CountDown task. We leverage
the fact that preference tuning leads to mode collapse, resulting in a model
that always produces highly structured and easily parse-able chain-of-thought
sequences. With this setup, we do a top-down and bottom-up analysis to
reverse-engineer how the model verifies its outputs. Our top-down analysis
reveals Gated Linear Unit (GLU) weights encoding verification-related tokens,
such as ``success'' or ``incorrect'', which activate according to the
correctness of the model's reasoning steps. Our bottom-up analysis reveals that
``previous-token heads'' are mainly responsible for model verification. Our
analyses meet in the middle: drawing inspiration from inter-layer communication
channels, we use the identified GLU vectors to localize as few as three
attention heads that can disable model verification, pointing to a necessary
component of a potentially larger verification circuit.

</details>


### [248] [Seeing Through Risk: A Symbolic Approximation of Prospect Theory](https://arxiv.org/abs/2504.14448)
*Ali Arslan Yousaf,Umair Rehman,Muhammad Umair Danish*

Main category: cs.AI

TL;DR: 提出了一种新的符号建模框架，将可解释性与前景理论的核心见解结合，用于风险决策。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法中效用曲线和概率加权函数不透明的问题，提供更透明的特征。

Method: 用效果大小引导的特征替代传统函数，数学形式化方法，并通过合成数据集验证。

Result: 模型在预测性能上具有竞争力，且能清晰映射心理构造系数。

Conclusion: 该模型适用于从AI安全到经济政策分析的多种应用。

Abstract: We propose a novel symbolic modeling framework for decision-making under risk
that merges interpretability with the core insights of Prospect Theory. Our
approach replaces opaque utility curves and probability weighting functions
with transparent, effect-size-guided features. We mathematically formalize the
method, demonstrate its ability to replicate well-known framing and
loss-aversion phenomena, and provide an end-to-end empirical validation on
synthetic datasets. The resulting model achieves competitive predictive
performance while yielding clear coefficients mapped onto psychological
constructs, making it suitable for applications ranging from AI safety to
economic policy analysis.

</details>


### [249] [Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey](https://arxiv.org/abs/2504.14520)
*Ahsan Bilal,Muhammad Ahmed Mohsin,Muhammad Umer,Muhammad Awais Khan Bangash,Muhammad Ali Jamshed*

Main category: cs.AI

TL;DR: 该综述探讨了从多智能体强化学习（MARL）角度提升大语言模型（LLM）的元思考能力，以增强其可靠性、灵活性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM存在幻觉和缺乏自我评估机制等问题，需通过元思考能力提升其在复杂或高风险任务中的表现。

Method: 分析了RLHF、自蒸馏和思维链提示等方法，并探讨了多智能体架构（如监督者-智能体层级、智能体辩论和心理理论框架）的应用。

Result: 提出了通过MARL的奖励机制、自我博弈和持续学习方法来构建具有内省、适应性和可信赖性的LLM。

Conclusion: 综述为未来研究方向（如神经科学启发的架构和混合符号推理）提供了路线图，并讨论了评估指标和数据集。

Abstract: This survey explores the development of meta-thinking capabilities in Large
Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL)
perspective. Meta-thinking self-reflection, assessment, and control of thinking
processes is an important next step in enhancing LLM reliability, flexibility,
and performance, particularly for complex or high-stakes tasks. The survey
begins by analyzing current LLM limitations, such as hallucinations and the
lack of internal self-assessment mechanisms. It then talks about newer methods,
including RL from human feedback (RLHF), self-distillation, and
chain-of-thought prompting, and each of their limitations. The crux of the
survey is to talk about how multi-agent architectures, namely supervisor-agent
hierarchies, agent debates, and theory of mind frameworks, can emulate
human-like introspective behavior and enhance LLM robustness. By exploring
reward mechanisms, self-play, and continuous learning methods in MARL, this
survey gives a comprehensive roadmap to building introspective, adaptive, and
trustworthy LLMs. Evaluation metrics, datasets, and future research avenues,
including neuroscience-inspired architectures and hybrid symbolic reasoning,
are also discussed.

</details>


### [250] [Learning from Reasoning Failures via Synthetic Data Generation](https://arxiv.org/abs/2504.14523)
*Gabriela Ben Melech Stan,Estelle Aflalo,Avinash Madasu,Vasudev Lal,Phillip Howard*

Main category: cs.AI

TL;DR: 论文提出了一种基于分析现有大型多模态模型（LMM）推理失败的新方法，用于生成针对性合成数据，以提升LMM性能。


<details>
  <summary>Details</summary>
Motivation: 由于高质量配对图像-文本数据的稀缺性，现有方法未能针对LMM的特定推理缺陷生成数据。受人类学习方式的启发，作者希望通过分析LMM的推理失败来生成更有针对性的数据。

Method: 利用前沿模型自动分析较弱LMM的错误，生成纠正推理失败的示例，并通过过滤确保数据质量。生成了一个包含553k示例的多模态指令调优数据集。

Result: 实验表明，使用该合成数据训练的模型在多个下游任务中表现优于使用等量真实数据训练的LMM。

Conclusion: 针对性生成合成数据对提升LMM性能具有高价值，数据集和代码将公开。

Abstract: Training models on synthetic data has emerged as an increasingly important
strategy for improving the performance of generative AI. This approach is
particularly helpful for large multimodal models (LMMs) due to the relative
scarcity of high-quality paired image-text data compared to language-only data.
While a variety of methods have been proposed for generating large multimodal
datasets, they do not tailor the synthetic data to address specific
deficiencies in the reasoning abilities of LMMs which will be trained with the
generated dataset. In contrast, humans often learn in a more efficient manner
by seeking out examples related to the types of reasoning where they have
failed previously. Inspired by this observation, we propose a new approach for
synthetic data generation which is grounded in the analysis of an existing
LMM's reasoning failures. Our methodology leverages frontier models to
automatically analyze errors produced by a weaker LMM and propose new examples
which can be used to correct the reasoning failure via additional training,
which are then further filtered to ensure high quality. We generate a large
multimodal instruction tuning dataset containing over 553k examples using our
approach and conduct extensive experiments demonstrating its utility for
improving the performance of LMMs on multiple downstream tasks. Our results
show that models trained on our synthetic data can even exceed the performance
of LMMs trained on an equivalent amount of additional real data, demonstrating
the high value of generating synthetic data targeted to specific reasoning
failure modes in LMMs. We will make our dataset and code publicly available.

</details>


### [251] [LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks](https://arxiv.org/abs/2504.14556)
*Yousef Emami,Hao Gao,SeyedSina Nabavirazani,Luis Almeida*

Main category: cs.AI

TL;DR: 本文提出了一种基于上下文学习（ICL）的数据收集调度方案（ICLDC），用于无人机辅助传感器网络中的紧急任务，如搜救行动。该方法通过自然语言任务描述生成调度计划，优于传统深度强化学习（DRL）和最大信道增益方法。


<details>
  <summary>Details</summary>
Motivation: 无人机在紧急任务（如搜救）中的应用需要高效且适应性强的调度方法，而现有方法（如DRL）存在训练复杂、仿真与现实差距大等问题。

Method: 提出ICLDC方案，无人机通过收集传感器数据并传输给大型语言模型（LLM），生成自然语言任务描述，进而制定数据收集调度计划。系统通过反馈机制持续优化。

Result: ICLDC在对抗攻击测试中表现稳健，累计数据包丢失率比最大信道增益方法降低约56%。

Conclusion: ICLDC为无人机辅助数据收集提供了一种智能调度新方向，尤其在紧急任务中具有潜力。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly being used in various
private and commercial applications, e.g. traffic control, package delivery,
and Search and Rescue (SAR) operations. Machine Learning (ML) methods used in
UAV-assisted Sensor Networks (UASNETs) and especially in Deep Reinforcement
Learning (DRL) face challenges such as complex and lengthy model training, gaps
between simulation and reality, and low sample efficiency, which conflict with
the urgency of emergencies such as SAR operations. This paper proposes
In-Context Learning (ICL)-based Data Collection Scheduling (ICLDC) scheme, as
an alternative to DRL in emergencies. The UAV collects and transmits logged
sensory data, to an LLM, to generate a task description in natural language,
from which it obtains a data collection schedule to be executed by the UAV. The
system continuously adapts by adding feedback to task descriptions and
utilizing feedback for future decisions. This method is tested against
jailbreaking attacks, where task description is manipulated to undermine
network performance, highlighting the vulnerability of LLMs to such attacks.
The proposed ICLDC outperforms the Maximum Channel Gain by reducing cumulative
packet loss by approximately 56\%. ICLDC presents a promising direction for
intelligent scheduling and control in UAV-assisted data collection.

</details>


### [252] [Toward the Axiomatization of Intelligence: Structure, Time, and Existence](https://arxiv.org/abs/2504.14596)
*Kei Itoh*

Main category: cs.AI

TL;DR: 该研究通过集合论和范畴论构建了一个关于智能的元框架定义，比较了三种智能系统的结构特性，并探讨了时间交互对智能分类的影响。


<details>
  <summary>Details</summary>
Motivation: 智能是一个多义且模糊的概念，需要一种形式化的定义方法。

Method: 使用集合论和范畴论，将智能定义为一种结构，并通过时间类别和智能类别的函子关系抽象表示智能系统的变化。

Result: 提出了智能的形式化定义，并比较了三种智能系统的结构特性和生物合理性。

Conclusion: 该方法不仅适用于智能，还可推广到其他概念（如意识和情感）的形式化定义。

Abstract: This study aims to construct an axiomatic definition of intelligence within a
meta-framework that defines the method of definition, addressing intelligence
as an inherently naive and polysemous concept. Initially, we formalize a
set-theoretic representation of the universe as the domain wherein intelligence
exists and characterize intelligence as a structure that involves temporal
evolution and interaction with other sets. Starting from a naive definition of
intelligence as "an entity possessing structures for externally inputting,
internally processing, and externally outputting information or matter," we
axiomatically reformulate it within this set-theoretical depiction of the
universe. Applying this axiomatic definition, we compare and interpret three
examples -- Hebbian non-optimized neural networks (NNs),
backpropagation-optimized NNs, and biological reflexive systems -- in terms of
their intelligence, structural properties, and biological plausibility.
Furthermore, by extending our definition into a categorical framework, we
introduce two categories, "Time Category" and "Intelligence Category," along
with the functorial relationships between them, demonstrating the potential to
represent changes and mimicry relationships among intelligent systems
abstractly. Additionally, since intelligence, as defined herein, functions
effectively only when accompanied by temporal interactions, we introduce the
concept of "activity" and explore how activity-based conditions influence
classifications and interpretations of intelligence. Finally, we suggest that
our definitional methodology is not limited to intelligence alone, but can be
similarly applied to other concepts, such as consciousness and emotion,
advocating for their formal reinterpretation through the same procedural steps:
defining a universal representation, selecting naive definitions, and axiomatic
formalization.

</details>


### [253] [UFO2: The Desktop AgentOS](https://arxiv.org/abs/2504.14603)
*Chaoyun Zhang,He Huang,Chiming Ni,Jian Mu,Si Qin,Shilin He,Lu Wang,Fangkai Yang,Pu Zhao,Chao Du,Liqun Li,Yu Kang,Zhao Jiang,Suzhen Zheng,Rujia Wang,Jiaxu Qian,Minghua Ma,Jian-Guang Lou,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: UFO2是一个多代理AgentOS，通过深度操作系统集成和混合控制检测管道，显著提升了桌面自动化的鲁棒性和执行准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机使用代理（CUAs）多为概念原型，存在操作系统集成浅、交互脆弱和执行干扰等问题。

Method: UFO2采用集中式HostAgent进行任务分解与协调，结合应用专用AppAgent和混合控制检测管道，支持多样化的界面风格。

Result: 在20多个真实Windows应用中测试，UFO2在鲁棒性和执行准确性上显著优于现有CUAs。

Conclusion: 深度操作系统集成为可靠、用户对齐的桌面自动化提供了可扩展的路径。

Abstract: Recent Computer-Using Agents (CUAs), powered by multimodal large language
models (LLMs), offer a promising direction for automating complex desktop
workflows through natural language. However, most existing CUAs remain
conceptual prototypes, hindered by shallow OS integration, fragile
screenshot-based interaction, and disruptive execution.
  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs
into practical, system-level automation. UFO2 features a centralized HostAgent
for task decomposition and coordination, alongside a collection of
application-specialized AppAgent equipped with native APIs, domain-specific
knowledge, and a unified GUI--API action layer. This architecture enables
robust task execution while preserving modularity and extensibility. A hybrid
control detection pipeline fuses Windows UI Automation (UIA) with vision-based
parsing to support diverse interface styles. Runtime efficiency is further
enhanced through speculative multi-action planning, reducing per-step LLM
overhead. Finally, a Picture-in-Picture (PiP) interface enables automation
within an isolated virtual desktop, allowing agents and users to operate
concurrently without interference.
  We evaluate UFO2 across over 20 real-world Windows applications,
demonstrating substantial improvements in robustness and execution accuracy
over prior CUAs. Our results show that deep OS integration unlocks a scalable
path toward reliable, user-aligned desktop automation.

</details>


### [254] [Consensus in Motion: A Case of Dynamic Rationality of Sequential Learning in Probability Aggregation](https://arxiv.org/abs/2504.14624)
*Polina Gordienko,Christoph Jansen,Thomas Augustin,Martin Rechenauer*

Main category: cs.AI

TL;DR: 提出了一种基于命题概率逻辑的概率聚合框架，强调动态理性而非静态理性，确保集体信念与新信息一致更新。证明了非嵌套议程上的共识兼容且独立的聚合规则必须是线性的，并给出了公平学习过程的充分条件。


<details>
  <summary>Details</summary>
Motivation: 传统判断聚合关注静态理性，而本研究旨在解决动态理性问题，确保集体信念在新信息下保持一致更新。

Method: 基于命题概率逻辑的框架，通过线性聚合规则和公平学习过程实现动态理性，并支持多阶段决策。

Result: 证明了非嵌套议程上的共识兼容且独立的聚合规则必须是线性的，并提供了公平学习过程的充分条件。

Conclusion: 该框架在动态环境下有效，支持多阶段决策，并通过政治场景案例验证了其适用性。

Abstract: We propose a framework for probability aggregation based on propositional
probability logic. Unlike conventional judgment aggregation, which focuses on
static rationality, our model addresses dynamic rationality by ensuring that
collective beliefs update consistently with new information. We show that any
consensus-compatible and independent aggregation rule on a non-nested agenda is
necessarily linear. Furthermore, we provide sufficient conditions for a fair
learning process, where individuals initially agree on a specified subset of
propositions known as the common ground, and new information is restricted to
this shared foundation. This guarantees that updating individual judgments via
Bayesian conditioning-whether performed before or after aggregation-yields the
same collective belief. A distinctive feature of our framework is its treatment
of sequential decision-making, which allows new information to be incorporated
progressively through multiple stages while maintaining the established common
ground. We illustrate our findings with a running example in a political
scenario concerning healthcare and immigration policies.

</details>


### [255] [A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents](https://arxiv.org/abs/2504.14650)
*Yuting Huang,Leilei Ding,Zhipeng Tang,Tianfu Wang,Xinrui Lin,Wuyang Zhang,Mingxiao Ma,Yanyong Zhang*

Main category: cs.AI

TL;DR: Safe-BeAl框架通过SafePlan-Bench和Safe-Align评估和提升LLM驱动的具身代理的安全性，提高安全性8.55-15.22%。


<details>
  <summary>Details</summary>
Motivation: LLM在具身代理中表现出强大的任务规划能力，但安全性问题尚未充分研究。

Method: 提出Safe-BeAl框架，包括评估基准SafePlan-Bench和安全性对齐方法Safe-Align。

Result: 实验表明，Safe-BeAl显著提升安全性，同时保持任务完成率。

Conclusion: Safe-BeAl为LLM驱动的具身代理提供了全面的安全性验证和提升方案。

Abstract: Large Language Models (LLMs) exhibit substantial promise in enhancing
task-planning capabilities within embodied agents due to their advanced
reasoning and comprehension. However, the systemic safety of these agents
remains an underexplored frontier. In this study, we present Safe-BeAl, an
integrated framework for the measurement (SafePlan-Bench) and alignment
(Safe-Align) of LLM-based embodied agents' behaviors. SafePlan-Bench
establishes a comprehensive benchmark for evaluating task-planning safety,
encompassing 2,027 daily tasks and corresponding environments distributed
across 8 distinct hazard categories (e.g., Fire Hazard). Our empirical analysis
reveals that even in the absence of adversarial inputs or malicious intent,
LLM-based agents can exhibit unsafe behaviors. To mitigate these hazards, we
propose Safe-Align, a method designed to integrate physical-world safety
knowledge into LLM-based embodied agents while maintaining task-specific
performance. Experiments across a variety of settings demonstrate that
Safe-BeAl provides comprehensive safety validation, improving safety by 8.55 -
15.22%, compared to embodied agents based on GPT-4, while ensuring successful
task completion.

</details>


### [256] [AI with Emotions: Exploring Emotional Expressions in Large Language Models](https://arxiv.org/abs/2504.14706)
*Shin-nosuke Ishikawa,Atsushi Yoshino*

Main category: cs.AI

TL;DR: 研究探索了大型语言模型（LLM）在输出中表达情感的能力，实验表明LLM能够根据指定的情感状态生成一致的回答，展示了其在情感模拟方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在多种任务中表现出接近人类水平的能力，人们对其未来是否能够具备情感产生了兴趣。本研究旨在验证当前LLM是否能够在输出中表达情感。

Method: 研究使用Russell的Circumplex情感模型定义情感状态，并让多个LLM（如GPT、Gemini等）根据指定情感回答问题。生成的内容通过独立的情感分析模型进行评估。

Result: 实验结果表明，LLM生成的内容情感状态与指定一致，证明了其在情感表达方面的能力。

Conclusion: LLM具备模拟情感的潜力，未来可应用于需要情感交互的场景，如个性化顾问或咨询。

Abstract: The human-level performance of Large Language Models (LLMs) across various
tasks has raised expectations for the potential of Artificial Intelligence (AI)
to possess emotions someday. To explore the capability of current LLMs to
express emotions in their outputs, we conducted an experiment using several
LLMs (OpenAI GPT, Google Gemini, Meta Llama3, and Cohere Command R+) to
role-play as agents answering questions with specified emotional states.We
defined the emotional states using Russell's Circumplex model, a
well-established framework that characterizes emotions along the
sleepy-activated (arousal) and pleasure-displeasure (valence) axes. We chose
this model for its simplicity, utilizing two continuous parameters, which
allows for better controllability in applications involving continuous changes
in emotional states. The responses generated were evaluated using a sentiment
analysis model, independent of the LLMs, trained on the GoEmotions dataset. The
evaluation showed that the emotional states of the generated answers were
consistent with the specifications, demonstrating the LLMs' capability for
emotional expression. This indicates the potential for LLM-based AI agents to
simulate emotions, opening up a wide range of applications for emotion-based
interactions, such as advisors or consultants who can provide advice or
opinions with a personal touch.

</details>


### [257] [PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities](https://arxiv.org/abs/2504.14773)
*Haoming Li,Zhaoliang Chen,Jonathan Zhang,Fei Liu*

Main category: cs.AI

TL;DR: 本文探讨了规划算法在不同领域的基准测试，旨在填补现有研究的空白，并为算法选择和未来基准开发提供指导。


<details>
  <summary>Details</summary>
Motivation: 规划在智能体和智能AI中具有核心作用，但目前缺乏对现有规划基准的全面理解，导致跨领域算法性能比较和新场景算法选择困难。

Method: 通过分析多种规划基准，将其分类为具身环境、网络导航、调度、游戏与谜题以及日常任务自动化，并评估其适用性。

Result: 研究提出了针对不同算法的最合适基准，并指出了未来基准开发的潜在方向。

Conclusion: 本文为规划算法的评估和选择提供了实用指导，同时为未来基准测试的设计提供了见解。

Abstract: Planning is central to agents and agentic AI. The ability to plan, e.g.,
creating travel itineraries within a budget, holds immense potential in both
scientific and commercial contexts. Moreover, optimal plans tend to require
fewer resources compared to ad-hoc methods. To date, a comprehensive
understanding of existing planning benchmarks appears to be lacking. Without
it, comparing planning algorithms' performance across domains or selecting
suitable algorithms for new scenarios remains challenging. In this paper, we
examine a range of planning benchmarks to identify commonly used testbeds for
algorithm development and highlight potential gaps. These benchmarks are
categorized into embodied environments, web navigation, scheduling, games and
puzzles, and everyday task automation. Our study recommends the most
appropriate benchmarks for various algorithms and offers insights to guide
future benchmark development.

</details>


### [258] [DONOD: Robust and Generalizable Instruction Fine-Tuning for LLMs via Model-Intrinsic Dataset Pruning](https://arxiv.org/abs/2504.14810)
*Jucheng Hu,Surong Yang,Dongzhan Zhou,Lijun Wu*

Main category: cs.AI

TL;DR: DONOD是一种轻量级的数据修剪方法，通过模型参数指标和TOPSIS算法筛选噪声数据，提升微调效率和跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决领域特定监督微调（SFT）削弱跨域泛化能力及对噪声数据敏感的问题。

Method: 使用Delta of Norm（DON）和Norm of Delta（NOD）评估数据，结合TOPSIS算法过滤噪声样本。

Result: 在数学任务中，修剪70%数据后目标域准确率提升14.90%，跨域准确率提升5.67%，且数据具有跨架构泛化能力。

Conclusion: DONOD在保持数据集无关性的同时，性能优于或媲美现有方法，适用性更广。

Abstract: Ad-hoc instruction fine-tuning of large language models (LLMs) is widely
adopted for domain-specific adaptation. While domain-specific supervised
fine-tuning (SFT) is effective and efficient, it often weakens cross-domain
generalization and struggles with noisy training data. To address these
challenges, we propose DONOD, a lightweight model-intrinsic data pruning
method. Our approach evaluates data using two model-parameter-based metrics:
Delta of Norm (DON), which captures the cumulative influence on model weights,
and Norm of Delta (NOD), which quantifies weight instability. Moreover, by
employing the Technique for Order of Preference by Similarity to Ideal Solution
(TOPSIS) algorithm, we effectively filter noisy, unlearnable, and
generalization-harming samples without relying on auxiliary models during the
SFT process. Experiments on mathematical tasks demonstrate that data selected
by DONOD achieve superior fine-tuning efficiency and improved robustness
against noisy data. By filtering out 70% of the full dataset, we improve
target-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile,
our selected data present superior cross-architecture generalization. Data
pruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger
models (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD
demonstrates comparable or superior performance while remaining
dataset-agnostic, enabling broader applicability.

</details>


### [259] [Establishing Reliability Metrics for Reward Models in Large Language Models](https://arxiv.org/abs/2504.14838)
*Yizhou Chen,Yawen Liu,Xuesi Wang,Qingtao Yu,Guangda Huzhang,Anxiang Zeng,Han Yu,Zhiming Zhou*

Main category: cs.AI

TL;DR: 论文提出了一种名为RETA的指标，用于直接衡量奖励模型（RM）的可靠性，并通过实验验证其稳定性。


<details>
  <summary>Details</summary>
Motivation: 奖励模型在优化大型语言模型输出中起关键作用，但其可靠性不确定，缺乏量化指标。

Method: 提出RETA指标，通过评估RM评分最高的η分位数响应的平均质量（由Oracle评分）来衡量RM可靠性，并设计了一个基准测试流程。

Result: 实验证明RETA指标具有优越的稳定性，能有效评估公开和专有RM的可靠性。

Conclusion: RETA指标可用于识别不可靠RM中的最佳分位数，从而优化模型输出选择。

Abstract: The reward model (RM) that represents human preferences plays a crucial role
in optimizing the outputs of large language models (LLMs), e.g., through
reinforcement learning from human feedback (RLHF) or rejection sampling.
However, a long challenge for RM is its uncertain reliability, i.e., LLM
outputs with higher rewards may not align with actual human preferences.
Currently, there is a lack of a convincing metric to quantify the reliability
of RMs. To bridge this gap, we propose the \textit{\underline{R}eliable at
\underline{$\eta$}} (RETA) metric, which directly measures the reliability of
an RM by evaluating the average quality (scored by an oracle) of the top $\eta$
quantile responses assessed by an RM. On top of RETA, we present an integrated
benchmarking pipeline that allows anyone to evaluate their own RM without
incurring additional Oracle labeling costs. Extensive experimental studies
demonstrate the superior stability of RETA metric, providing solid evaluations
of the reliability of various publicly available and proprietary RMs. When
dealing with an unreliable RM, we can use the RETA metric to identify the
optimal quantile from which to select the responses.

</details>


### [260] [AlignRAG: An Adaptable Framework for Resolving Misalignments in Retrieval-Aware Reasoning of RAG](https://arxiv.org/abs/2504.14858)
*Jiaqi Wei,Hao Zhou,Xiang Zhang,Di Zhang,Zijie Qiu,Wei Wei,Jinzhe Li,Wanli Ouyang,Siqi Sun*

Main category: cs.AI

TL;DR: AlignRAG提出了一种新的测试时框架，通过迭代的Critique-Driven Alignment步骤解决RAG中的推理对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG管道未能确保推理轨迹与检索内容的一致性，导致推理对齐问题。

Method: AlignRAG通过构建上下文丰富的训练语料、生成对比性批评、训练Critic Language Model (CLM)以及应用CDA步骤来优化推理轨迹。

Result: AlignRAG在实验中表现优于所有基线，并可无缝集成到现有RAG管道中。

Conclusion: AlignRAG通过重新定义RAG为结构化推理轨迹，为检索增强生成提供了实用进展。

Abstract: Retrieval-augmented generation (RAG) has emerged as a foundational paradigm
for knowledge-grounded text generation. However, existing RAG pipelines often
fail to ensure that the reasoning trajectories align with the evidential
constraints imposed by retrieved content. In this paper, we reframe RAG as a
problem of retrieval-aware reasoning and identify a core challenge: reasoning
misalignment-the mismatch between a model's reasoning trajectory and the
retrieved evidence. To address this challenge, we propose AlignRAG, a novel
test-time framework that mitigates reasoning misalignment through iterative
Critique-Driven Alignment (CDA) steps. In contrast to prior approaches that
rely on static training or post-hoc selection, AlignRAG actively refines
reasoning trajectories during inference by enforcing fine-grained alignment
with evidence. Our framework introduces a new paradigm for retrieval-aware
reasoning by: (1) constructing context-rich training corpora; (2) generating
contrastive critiques from preference-aware reasoning trajectories; (3)
training a dedicated \textit{Critic Language Model (CLM)} to identify reasoning
misalignments; and (4) applying CDA steps to optimize reasoning trajectories
iteratively. Empirical results demonstrate that AlignRAG consistently
outperforms all baselines and could integrate as a plug-and-play module into
existing RAG pipelines without further changes. By reconceptualizing RAG as a
structured reasoning trajectory and establishing the test-time framework for
correcting reasoning misalignments in RAG, AlignRAG provides practical
advancements for retrieval-aware generation.

</details>


### [261] [OTC: Optimal Tool Calls via Reinforcement Learning](https://arxiv.org/abs/2504.14870)
*Hongru Wang,Cheng Qian,Wanjun Zhong,Xiusi Chen,Jiahao Qiu,Shijue Huang,Bowen Jin,Mengdi Wang,Kam-Fai Wong,Heng Ji*

Main category: cs.AI

TL;DR: 论文提出了一种名为OTC-PO的强化学习框架，旨在优化工具集成推理（TIR）中的工具使用效率，同时保持答案准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在优化工具集成推理时忽视了工具使用的效率和成本，导致工具调用过多或不足的问题。

Method: 提出OTC-PO框架，结合正确性和工具效率的奖励机制，并在PPO和GRPO中实现为OTC-PPO和OTC-GRPO。

Result: 实验显示，该方法减少工具调用达73.1%，提升工具生产力达229.4%，同时保持答案准确性。

Conclusion: OTC-PO是首个明确优化TIR中工具使用效率的强化学习框架，具有显著的实际应用价值。

Abstract: Tool-integrated reasoning (TIR) augments large language models (LLMs) with
the ability to invoke external tools, such as search engines and code
interpreters, to solve tasks beyond the capabilities of language-only
reasoning. While reinforcement learning (RL) has shown promise in improving TIR
by optimizing final answer correctness, existing approaches often overlook the
efficiency and cost associated with tool usage. This can lead to suboptimal
behavior, including excessive tool calls that increase computational and
financial overhead, or insufficient tool use that compromises answer quality.
In this work, we propose Optimal Tool Call-controlled Policy Optimization
(OTC-PO), a simple yet effective RL-based framework that encourages models to
produce accurate answers with minimal tool calls. Our method introduces a
tool-integrated reward that jointly considers correctness and tool efficiency,
promoting high tool productivity. We instantiate this framework within both
Proximal Policy Optimization (PPO) and Group Relative Preference Optimization
(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and
Qwen-Math across multiple QA benchmarks show that our approach reduces tool
calls by up to 73.1\% and improves tool productivity by up to 229.4\%, while
maintaining comparable answer accuracy. To the best of our knowledge, this is
the first RL-based framework that explicitly optimizes tool-use efficiency in
TIR.

</details>


### [262] [EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework](https://arxiv.org/abs/2504.14928)
*Yao Shi,Rongkeng Liang,Yong Xu*

Main category: cs.AI

TL;DR: EducationQ框架通过多智能体对话评估LLMs的教学能力，发现教学效果与模型规模或通用推理能力无线性关系，小模型可能优于大模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs作为教育工具的教学能力评估困难，因其资源密集、依赖上下文且方法复杂。

Method: 引入EducationQ框架，模拟动态教育场景，测试14个LLMs在13学科10难度级别的1,498个问题上的表现。

Result: 教学效果与模型规模无关，小模型可能更优；78%的人类专家评估与自动分析一致。

Conclusion: LLMs教学需针对性优化，而非简单扩展规模，未来教育AI应注重特定教学效果提升。

Abstract: Large language models (LLMs) increasingly serve as educational tools, yet
evaluating their teaching capabilities remains challenging due to the
resource-intensive, context-dependent, and methodologically complex nature of
teacher-student interactions. We introduce EducationQ, a multi-agent dialogue
framework that efficiently assesses teaching capabilities through simulated
dynamic educational scenarios, featuring specialized agents for teaching,
learning, and evaluation. Testing 14 LLMs across major AI Organizations
(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13
disciplines and 10 difficulty levels reveals that teaching effectiveness does
not correlate linearly with model scale or general reasoning capabilities -
with some smaller open-source models outperforming larger commercial
counterparts in teaching contexts. This finding highlights a critical gap in
current evaluations that prioritize knowledge recall over interactive pedagogy.
Our mixed-methods evaluation, combining quantitative metrics with qualitative
analysis and expert case studies, identifies distinct pedagogical strengths
employed by top-performing models (e.g., sophisticated questioning strategies,
adaptive feedback mechanisms). Human expert evaluations show 78% agreement with
our automated qualitative analysis of effective teaching behaviors, validating
our methodology. EducationQ demonstrates that LLMs-as-teachers require
specialized optimization beyond simple scaling, suggesting next-generation
educational AI prioritize targeted enhancement of specific pedagogical
effectiveness.

</details>


### [263] [Generative Semantic Communications: Principles and Practices](https://arxiv.org/abs/2504.14947)
*Xiaojun Yuan,Haoming Ma,Yinuo Huang,Zhoufan Hua,Yong Zuo,Zhi Ding*

Main category: cs.AI

TL;DR: 论文提出了一种基于AGI的生成式语义通信（GSC）新范式，利用基础模型和生成模型等先进AI技术，以应对AGI服务对语义通信的新挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AGI的发展，其对语义通信的需求增加，现有技术面临挑战，需要新的解决方案。

Method: 提出GSC框架，并通过两个案例研究验证其在AGI驱动应用中的优势。

Result: GSC在AGI应用中展现出显著优势，为高效通信提供了新方向。

Conclusion: 讨论了GSC的开放挑战和研究方向，以推动其实际应用。

Abstract: Semantic communication leverages artificial intelligence (AI) technologies to
extract semantic information from data for efficient transmission, theraby
significantly reducing communication cost. With the evolution towards
artificial general intelligence (AGI), the increasing demands for AGI services
pose new challenges to semantic communication. In response, we propose a new
paradigm for AGI-driven communications, called generative semantic
communication (GSC), which utilizes advanced AI technologies such as foundation
models and generative models. We first describe the basic concept of GSC and
its difference from existing semantic communications, and then introduce a
general framework of GSC, followed by two case studies to verify the advantages
of GSC in AGI-driven applications. Finally, open challenges and new research
directions are discussed to stimulate this line of research and pave the way
for practical applications.

</details>


### [264] [Evaluating Code Generation of LLMs in Advanced Computer Science Problems](https://arxiv.org/abs/2504.14964)
*Emir Catir,Robin Claesson,Rodothea Myrsini Tsoupidi*

Main category: cs.AI

TL;DR: 论文评估了四种大型语言模型（LLM）在解决高级编程课程作业中的能力，发现其在入门课程中表现优异，但在高级课程中更具挑战性。


<details>
  <summary>Details</summary>
Motivation: 研究填补了LLM在高级编程作业中表现的研究空白，为教师设计作业提供指导。

Method: 手动选择12个编程问题（3个入门级，9个高级），生成1000个测试用例评估LLM生成的代码。

Result: LLM在入门课程中高效，但在高级课程中表现较差，但仍能提供部分有用解决方案。

Conclusion: LLM在高级编程作业中表现有限，但对学生和教师仍有参考价值。

Abstract: Large Language Models (LLMs), such as GitHub Copilot and ChatGPT have become
popular among programming students. Students use LLMs to assist them in
programming courses, including generating source code. Previous work has
evaluated the ability of LLMs in solving introductory-course programming
assignments. The results have shown that LLMs are highly effective in
generating code for introductory Computer Science (CS) courses. However, there
is a gap in research on evaluating LLMs' ability to generate code that solves
advanced programming assignments. In this work, we evaluate the ability of four
LLM tools to solve programming assignments from advanced CS courses in three
popular programming languages, Java, Python, and C. We manually select 12
problems, three problems from introductory courses as the baseline and nine
programming assignments from second- and third-year CS courses. To evaluate the
LLM-generated code, we generate a test suite of 1000 test cases per problem and
analyze the program output. Our evaluation shows that although LLMs are highly
effective in generating source code for introductory programming courses,
solving advanced programming assignments is more challenging. Nonetheless, in
many cases, LLMs identify the base problem and provide partial solutions that
may be useful to CS students. Furthermore, our results may provide useful
guidance for teachers of advanced programming courses on how to design
programming assignments.

</details>


### [265] [Text-to-Decision Agent: Learning Generalist Policies from Natural Language Supervision](https://arxiv.org/abs/2504.15046)
*Shilin Zhang,Zican Hu,Wenhao Wu,Xinyi Xie,Jianxiang Tang,Chunlin Chen,Daoyi Dong,Yu Cheng,Zhenhong Sun,Zhi Wang*

Main category: cs.AI

TL;DR: 论文提出了一种名为T2DA的框架，通过自然语言监督通用策略学习，解决了RL系统在任务泛化中依赖高质量样本或预热探索的问题。


<details>
  <summary>Details</summary>
Motivation: 传统RL系统依赖昂贵或不可行的监督信号，限制了其通用性和可用性。利用自然语言作为监督信号是一种更广泛的替代方案。

Method: 引入广义世界模型将多任务决策数据编码为动态感知嵌入空间，并通过对比语言-决策预训练桥接文本与决策嵌入的语义差距。

Result: 在MuJoCo和Meta-World基准测试中，T2DA表现出优异的零样本泛化能力，优于多种基线方法。

Conclusion: T2DA通过自然语言监督实现了高效的零样本决策生成，为RL系统的通用性提供了新思路。

Abstract: RL systems usually tackle generalization by inferring task beliefs from
high-quality samples or warmup explorations. The restricted form limits their
generality and usability since these supervision signals are expensive and even
infeasible to acquire in advance for unseen tasks. Learning directly from the
raw text about decision tasks is a promising alternative to leverage a much
broader source of supervision. In the paper, we propose Text-to-Decision Agent
(T2DA), a simple and scalable framework that supervises generalist policy
learning with natural language. We first introduce a generalized world model to
encode multi-task decision data into a dynamics-aware embedding space. Then,
inspired by CLIP, we predict which textual description goes with which decision
embedding, effectively bridging their semantic gap via contrastive
language-decision pre-training and aligning the text embeddings to comprehend
the environment dynamics. After training the text-conditioned generalist
policy, the agent can directly realize zero-shot text-to-decision generation in
response to language instructions. Comprehensive experiments on MuJoCo and
Meta-World benchmarks show that T2DA facilitates high-capacity zero-shot
generalization and outperforms various types of baselines.

</details>


### [266] [Mitigating Degree Bias in Graph Representation Learning with Learnable Structural Augmentation and Structural Self-Attention](https://arxiv.org/abs/2504.15075)
*Van Thuy Hoang,Hyeon-Ju Jeon,O-Joun Lee*

Main category: cs.AI

TL;DR: 提出了一种名为DegFairGT的图神经网络方法，通过结构增强和自注意力机制解决图中节点度偏差问题，提升低度节点的表示能力。


<details>
  <summary>Details</summary>
Motivation: 现实图中的长尾度分布导致高度节点主导消息传递，低度节点因消息不足而表示不足，需解决度偏差问题。

Method: DegFairGT通过可学习的结构增强和结构自注意力，发现非相邻节点的结构相似性，生成信息边并提供有用消息。

Result: 在六个数据集上的实验表明，DegFairGT在度公平性分析、节点分类和节点聚类任务中优于现有方法。

Conclusion: DegFairGT有效缓解了度偏差，同时保持了图的同质性原则和全局结构。

Abstract: Graph Neural Networks (GNNs) update node representations through message
passing, which is primarily based on the homophily principle, assuming that
adjacent nodes share similar features. However, in real-world graphs with
long-tailed degree distributions, high-degree nodes dominate message passing,
causing a degree bias where low-degree nodes remain under-represented due to
inadequate messages. The main challenge in addressing degree bias is how to
discover non-adjacent nodes to provide additional messages to low-degree nodes
while reducing excessive messages for high-degree nodes. Nevertheless,
exploiting non-adjacent nodes to provide valuable messages is challenging, as
it could generate noisy information and disrupt the original graph structures.
To solve it, we propose a novel Degree Fairness Graph Transformer, named
DegFairGT, to mitigate degree bias by discovering structural similarities
between non-adjacent nodes through learnable structural augmentation and
structural self-attention. Our key idea is to exploit non-adjacent nodes with
similar roles in the same community to generate informative edges under our
augmentation, which could provide informative messages between nodes with
similar roles while ensuring that the homophily principle is maintained within
the community. To enable DegFairGT to learn such structural similarities, we
then propose a structural self-attention to capture the similarities between
node pairs. To preserve global graph structures and prevent graph augmentation
from hindering graph structure, we propose a Self-Supervised Learning task to
preserve p-step transition probability and regularize graph augmentation.
Extensive experiments on six datasets showed that DegFairGT outperformed
state-of-the-art baselines in degree fairness analysis, node classification,
and node clustering tasks.

</details>


### [267] [Contemplative Wisdom for Superalignment](https://arxiv.org/abs/2504.15125)
*Ruben Laukkonen,Fionn Inglis,Shamil Chandaria,Lars Sandved-Smith,Jakob Hohwy,Jonathan Gold,Adam Elwood*

Main category: cs.AI

TL;DR: 论文提出了一种将内在道德原则融入AI认知架构的方法，通过四个原则（正念、空性、非二元性和无界关怀）提升AI的稳健性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统AI对齐策略在应对不可预测的自我改进和复杂系统时可能失效，因此需要一种更内在的道德设计方法。

Method: 采用四个原则（正念、空性、非二元性和无界关怀）构建AI的智慧世界模型，并结合现代架构（如GPT-4o）实现。

Result: 实验表明，这些原则能显著提升AI在AILuminate Benchmark上的表现，尤其是组合使用时。

Conclusion: 这种方法为AI对齐提供了一种自我修正和稳健的替代方案，适用于未来复杂系统。

Abstract: As artificial intelligence (AI) improves, traditional alignment strategies
may falter in the face of unpredictable self-improvement, hidden subgoals, and
the sheer complexity of intelligent systems. Rather than externally
constraining behavior, we advocate designing AI with intrinsic morality built
into its cognitive architecture and world model. Inspired by contemplative
wisdom traditions, we show how four axiomatic principles can instil a resilient
Wise World Model in AI systems. First, mindfulness enables self-monitoring and
recalibration of emergent subgoals. Second, emptiness forestalls dogmatic goal
fixation and relaxes rigid priors. Third, non-duality dissolves adversarial
self-other boundaries. Fourth, boundless care motivates the universal reduction
of suffering. We find that prompting AI to reflect on these principles improves
performance on the AILuminate Benchmark using GPT-4o, particularly when
combined. We offer detailed implementation strategies for state-of-the-art
models, including contemplative architectures, constitutions, and reinforcement
of chain-of-thought. For future systems, the active inference framework may
offer the self-organizing and dynamic coupling capabilities needed to enact
these insights in embodied agents. This interdisciplinary approach offers a
self-correcting and resilient alternative to prevailing brittle control
schemes.

</details>


### [268] [Behavioral Universe Network (BUN): A Behavioral Information-Based Framework for Complex Systems](https://arxiv.org/abs/2504.15146)
*Wei Zhou,Ailiya Borjigin,Cong He*

Main category: cs.AI

TL;DR: 论文提出了一种名为BUN的理论框架，基于AIB形式化方法，统一建模主体、客体和行为，并通过BIB协调多智能体系统，具有行为分析、适应性和跨域互操作性等优势。


<details>
  <summary>Details</summary>
Motivation: 传统模型无法统一捕捉数字生态系统中自主实体间的复杂动态交互行为，因此需要一种新的理论框架来解决这一问题。

Method: 引入BUN框架，基于AIB形式化方法，将主体（智能体）、客体（资源）和行为（操作）作为一等实体，并通过BIB协调多智能体系统。

Result: BUN框架在行为分析、适应性和跨域互操作性方面表现出显著优势。

Conclusion: BUN为下一代数字治理和智能应用提供了有前景的理论基础。

Abstract: Modern digital ecosystems feature complex, dynamic interactions among
autonomous entities across diverse domains. Traditional models often separate
agents and objects, lacking a unified foundation to capture their interactive
behaviors. This paper introduces the Behavioral Universe Network (BUN), a
theoretical framework grounded in the Agent-Interaction-Behavior (AIB)
formalism. BUN treats subjects (active agents), objects (resources), and
behaviors (operations) as first-class entities, all governed by a shared
Behavioral Information Base (BIB). We detail the AIB core concepts and
demonstrate how BUN leverages information-driven triggers, semantic enrichment,
and adaptive rules to coordinate multi-agent systems. We highlight key
benefits: enhanced behavior analysis, strong adaptability, and cross-domain
interoperability. We conclude by positioning BUN as a promising foundation for
next-generation digital governance and intelligent applications.

</details>


### [269] [Synergistic Weak-Strong Collaboration by Aligning Preferences](https://arxiv.org/abs/2504.15188)
*Yizhu Jiao,Xuchao Zhang,Zhaoyang Wang,Yubo Ma,Zhun Deng,Rujia Wang,Chetan Bansal,Saravan Rajmohan,Jiawei Han,Huaxiu Yao*

Main category: cs.AI

TL;DR: 提出了一种协作框架，将专用弱模型与通用强模型结合，通过协作反馈优化弱模型，显著提升专业任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在专业任务中因缺乏领域知识而表现不佳的问题，同时避免为每个细分任务微调大模型的高成本。

Method: 协作框架中，弱模型生成初稿和背景信息，强模型进行优化；引入协作反馈量化弱模型贡献并指导其偏好调整。

Result: 实验验证协作框架在三个领域中显著优于单独模型，且通过偏好调整进一步提升了性能。

Conclusion: 协作框架有效结合了弱模型的专业性和强模型的通用推理能力，为专业任务提供了高效解决方案。

Abstract: Current Large Language Models (LLMs) excel in general reasoning yet struggle
with specialized tasks requiring proprietary or domain-specific knowledge.
Fine-tuning large models for every niche application is often infeasible due to
black-box constraints and high computational overhead. To address this, we
propose a collaborative framework that pairs a specialized weak model with a
general strong model. The weak model, tailored to specific domains, produces
initial drafts and background information, while the strong model leverages its
advanced reasoning to refine these drafts, extending LLMs' capabilities to
critical yet specialized tasks. To optimize this collaboration, we introduce a
collaborative feedback to fine-tunes the weak model, which quantifies the
influence of the weak model's contributions in the collaboration procedure and
establishes preference pairs to guide preference tuning of the weak model. We
validate our framework through experiments on three domains. We find that the
collaboration significantly outperforms each model alone by leveraging
complementary strengths. Moreover, aligning the weak model with the
collaborative preference further enhances overall performance.

</details>


### [270] [Position: Bayesian Statistics Facilitates Stakeholder Participation in Evaluation of Generative AI](https://arxiv.org/abs/2504.15211)
*Yanan Long*

Main category: cs.AI

TL;DR: 论文提出使用贝叶斯统计方法改进生成式AI系统的评估，强调其不确定性量化、领域知识整合和持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖基准测试和点估计，无法捕捉不确定性和社会影响，需更全面的框架。

Method: 采用贝叶斯统计方法，整合领域知识、持续学习，并通过后验推断量化不确定性。

Result: 贝叶斯方法能提升生成式AI评估的公平性、透明度和可靠性，支持动态环境下的模型验证。

Conclusion: 贝叶斯统计为生成式AI评估提供了更稳健的框架，适合复杂现实场景。

Abstract: The evaluation of Generative AI (GenAI) systems plays a critical role in
public policy and decision-making, yet existing methods are often limited by
reliance on benchmark-driven, point-estimate comparisons that fail to capture
uncertainty and broader societal impacts. This paper argues for the use of
Bayesian statistics as a principled framework to address these challenges.
Bayesian methods enable the integration of domain expertise through prior
elicitation, allow for continuous learning from new data, and provide robust
uncertainty quantification via posterior inference. We demonstrate how Bayesian
inference can be applied to GenAI evaluation, particularly in incorporating
stakeholder perspectives to enhance fairness, transparency, and reliability.
Furthermore, we discuss Bayesian workflows as an iterative process for model
validation and refinement, ensuring robust assessments of GenAI systems in
dynamic, real-world contexts.

</details>


### [271] [A Self-Improving Coding Agent](https://arxiv.org/abs/2504.15228)
*Maxime Robeyns,Martin Szummer,Laurence Aitchison*

Main category: cs.AI

TL;DR: LLM编码代理通过自我编辑提升性能，在多个基准测试中表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 探索LLM代理在自主改进方面的潜力，推动自动化代理系统设计。

Method: 为LLM代理配备基本编码工具，使其能自主编辑代码并优化性能。

Result: 在SWE Bench Verified和LiveCodeBench等测试中性能提升17%至53%。

Conclusion: 研究为自动化代理设计提供了参考框架，展示了LLM在工具使用和代理任务中的潜力。

Abstract: We demonstrate that an LLM coding agent, equipped with basic coding tools,
can autonomously edit itself, and thereby improve its performance on benchmark
tasks. We find performance gains from 17% to 53% on a random subset of SWE
Bench Verified, with additional performance gains on LiveCodeBench, as well as
synthetically generated agent benchmarks. Our work represents an advancement in
the automated and open-ended design of agentic systems, and provides a
reference agent framework for those seeking to post-train LLMs on tool use and
other agentic tasks.

</details>


### [272] [SuoiAI: Building a Dataset for Aquatic Invertebrates in Vietnam](https://arxiv.org/abs/2504.15252)
*Tue Vo,Lakshay Sharma,Tuan Dinh,Khuong Dinh,Trang Nguyen,Trung Phan,Minh Do,Duong Vu*

Main category: cs.AI

TL;DR: SuoiAI是一个端到端流程，用于构建越南水生无脊椎动物数据集，并利用机器学习技术进行分类，旨在解决数据稀缺、细粒度分类和环境多样性等挑战。


<details>
  <summary>Details</summary>
Motivation: 理解和监测水生生物多样性对生态健康和保育工作至关重要，但面临数据稀缺、细粒度分类和环境多样性等挑战。

Method: 通过半监督学习减少标注工作量，利用先进的物体检测和分类模型进行数据收集、标注和模型训练。

Result: 提出了一种高效的数据集构建和分类方法，适用于多样化的环境条件。

Conclusion: SuoiAI为水生生物多样性监测提供了一种可行的解决方案，具有实际应用潜力。

Abstract: Understanding and monitoring aquatic biodiversity is critical for ecological
health and conservation efforts. This paper proposes SuoiAI, an end-to-end
pipeline for building a dataset of aquatic invertebrates in Vietnam and
employing machine learning (ML) techniques for species classification. We
outline the methods for data collection, annotation, and model training,
focusing on reducing annotation effort through semi-supervised learning and
leveraging state-of-the-art object detection and classification models. Our
approach aims to overcome challenges such as data scarcity, fine-grained
classification, and deployment in diverse environmental conditions.

</details>


### [273] [FlowReasoner: Reinforcing Query-Level Meta-Agents](https://arxiv.org/abs/2504.15257)
*Hongcheng Gao,Yue Liu,Yufei He,Longxu Dou,Chao Du,Zhijie Deng,Bryan Hooi,Min Lin,Tianyu Pang*

Main category: cs.AI

TL;DR: FlowReasoner是一个查询级元代理，通过外部执行反馈自动化设计多代理系统，结合深度学习和强化学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 自动化设计针对每个用户查询的个性化多代理系统，提高系统性能和效率。

Method: 结合DeepSeek R1的基础推理能力和强化学习（RL）的外部执行反馈，设计多目标奖励函数优化性能、复杂性和效率。

Result: 在工程和竞赛代码基准测试中表现优异，准确率比o1-mini高出10.52%。

Conclusion: FlowReasoner通过推理和强化学习成功实现了高效的个性化多代理系统设计。

Abstract: This paper proposes a query-level meta-agent named FlowReasoner to automate
the design of query-level multi-agent systems, i.e., one system per user query.
Our core idea is to incentivize a reasoning-based meta-agent via external
execution feedback. Concretely, by distilling DeepSeek R1, we first endow the
basic reasoning ability regarding the generation of multi-agent systems to
FlowReasoner. Then, we further enhance it via reinforcement learning (RL) with
external execution feedback. A multi-purpose reward is designed to guide the RL
training from aspects of performance, complexity, and efficiency. In this
manner, FlowReasoner is enabled to generate a personalized multi-agent system
for each user query via deliberative reasoning. Experiments on both engineering
and competition code benchmarks demonstrate the superiority of FlowReasoner.
Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks.
The code is available at https://github.com/sail-sg/FlowReasoner.

</details>


### [274] [Leveraging Language Models for Automated Patient Record Linkage](https://arxiv.org/abs/2504.15261)
*Mohammad Beheshti,Lovedeep Gondara,Iris Zachary*

Main category: cs.AI

TL;DR: 该研究探讨了利用语言模型（如RoBERTa和Mistral）自动链接患者记录的可行性，结果显示微调模型在阻塞和匹配任务中表现优异，但混合方法仍更高效。


<details>
  <summary>Details</summary>
Motivation: 医疗数据碎片化导致患者记录链接困难，需要自动化解决方案以提高效率和准确性。

Method: 使用真实医疗数据，通过微调RoBERTa进行阻塞任务，并测试多种语言模型（如Mistral）在匹配任务中的表现。

Result: 微调阻塞模型减少92%候选对，匹配任务中Mistral-7B表现最佳（仅6错误）。零样本模型中Mistral-Small-24B最优（55错误）。

Conclusion: 语言模型可自动化患者记录链接，提升效率，但混合方法更优。未来需平衡计算成本与性能。

Abstract: Objective: Healthcare data fragmentation presents a major challenge for
linking patient data, necessitating robust record linkage to integrate patient
records from diverse sources. This study investigates the feasibility of
leveraging language models for automated patient record linkage, focusing on
two key tasks: blocking and matching. Materials and Methods: We utilized
real-world healthcare data from the Missouri Cancer Registry and Research
Center, linking patient records from two independent sources using
probabilistic linkage as a baseline. A transformer-based model, RoBERTa, was
fine-tuned for blocking using sentence embeddings. For matching, several
language models were experimented under fine-tuned and zero-shot settings,
assessing their performance against ground truth labels. Results: The
fine-tuned blocking model achieved a 92% reduction in the number of candidate
pairs while maintaining near-perfect recall. In the matching task, fine-tuned
Mistral-7B achieved the best performance with only 6 incorrect predictions.
Among zero-shot models, Mistral-Small-24B performed best, with a total of 55
incorrect predictions. Discussion: Fine-tuned language models achieved strong
performance in patient record blocking and matching with minimal errors.
However, they remain less accurate and efficient than a hybrid rule-based and
probabilistic approach for blocking. Additionally, reasoning models like
DeepSeek-R1 are impractical for large-scale record linkage due to high
computational costs. Conclusion: This study highlights the potential of
language models for automating patient record linkage, offering improved
efficiency by eliminating the manual efforts required to perform patient record
linkage. Overall, language models offer a scalable solution that can enhance
data integration, reduce manual effort, and support disease surveillance and
research.

</details>


### [275] [Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning](https://arxiv.org/abs/2504.15275)
*Jie Cheng,Ruixi Qiao,Lijun Li,Chao Guo,Junle Wang,Gang Xiong,Yisheng Lv,Fei-Yue Wang*

Main category: cs.AI

TL;DR: 论文提出PURE方法，通过最小化未来奖励的信用分配形式解决PRM导致的奖励黑客问题，显著提升强化微调效果。


<details>
  <summary>Details</summary>
Motivation: PRM在强化微调中存在奖励黑客问题，限制了其应用。论文旨在解决这一问题。

Method: 提出PURE方法，采用最小化未来奖励的信用分配形式，替代传统的累加形式。

Result: 实验显示PURE在30%的步骤内达到与可验证奖励方法相当的性能，并在补充少量可验证奖励后取得最佳模型。

Conclusion: PURE有效缓解奖励黑客问题，提升模型性能，为PRM在强化学习中的应用提供了新思路。

Abstract: Process reward models (PRMs) have proven effective for test-time scaling of
Large Language Models (LLMs) on challenging reasoning tasks. However, reward
hacking issues with PRMs limit their successful application in reinforcement
fine-tuning. In this paper, we identify the main cause of PRM-induced reward
hacking: the canonical summation-form credit assignment in reinforcement
learning (RL), which defines the value as cumulative gamma-decayed future
rewards, easily induces LLMs to hack steps with high rewards. To address this,
we propose PURE: Process sUpervised Reinforcement lEarning. The key innovation
of PURE is a min-form credit assignment that formulates the value function as
the minimum of future rewards. This method significantly alleviates reward
hacking by limiting the value function range and distributing advantages more
reasonably. Through extensive experiments on 3 base models, we show that
PRM-based approaches enabling min-form credit assignment achieve comparable
reasoning performance to verifiable reward-based methods within only 30% steps.
In contrast, the canonical sum-form credit assignment collapses training even
at the beginning! Additionally, when we supplement PRM-based fine-tuning with
just 10% verifiable rewards, we further alleviate reward hacking and produce
the best fine-tuned model based on Qwen2.5-Math-7B in our experiments,
achieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5
benchmarks. Moreover, we summarize the observed reward hacking cases and
analyze the causes of training collapse. Code and models are available at
https://github.com/CJReinforce/PURE.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [276] [A discrete physics-informed training for projection-based reduced order models with neural networks](https://arxiv.org/abs/2504.13875)
*N. Sibuet,S. Ares de Parga,J. R. Bravo,R. Rossi*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息的降阶模型（ROM）训练框架，通过结合FEM残差损失改进PROM-ANN架构，适用于非线性问题，并在超弹性问题中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统ROM和物理信息神经网络（PINNs）之间存在差距，本文旨在通过FEM残差损失弥补这一差距，提升ROM的精度和适用性。

Method: 扩展PROM-ANN架构，引入基于FEM的离散物理残差损失，改进非线性问题的训练过程，并通过超弹性问题验证。

Result: 改进的PROM-ANN在重建精度上显著优于POD，且物理信息训练缩小了数据重建与ROM精度之间的差距。

Conclusion: FEM残差在ROM构建中起关键作用，未来需探索更多架构以进一步发挥其潜力。

Abstract: This paper presents a physics-informed training framework for
projection-based Reduced Order Models (ROMs). We extend the PROM-ANN
architecture by complementing snapshot-based training with a FEM-based,
discrete physics-informed residual loss, bridging the gap between traditional
projection-based ROMs and physics-informed neural networks (PINNs). Unlike
conventional PINNs that rely on analytical PDEs, our approach leverages FEM
residuals to guide the learning of the ROM approximation manifold. Key
contributions include: (1) a parameter-agnostic, discrete residual loss
applicable to non-linear problems, (2) an architectural modification to
PROM-ANN improving accuracy for fast-decaying singular values, and (3) an
empirical study on the proposed physics informed training process for ROMs.
  The method is demonstrated on a non-linear hyperelasticity problem,
simulating a rubber cantilever under multi-axial loads. The main accomplishment
in regards to the proposed residual-based loss is its applicability on
non-linear problems by interfacing with FEM software while maintaining
reasonable training times. The modified PROM-ANN outperforms POD by orders of
magnitude in snapshot reconstruction accuracy, while the original formulation
is not able to learn a proper mapping for this use-case. Finally, the
application of physics informed training in ANN-PROM modestly narrows the gap
between data reconstruction and ROM accuracy, however it highlights the
untapped potential of the proposed residual-driven optimization for future ROM
development. This work underscores the critical role of FEM residuals in ROM
construction and calls for further exploration on architectures beyond
PROM-ANN.

</details>


### [277] [Ising Models with Hidden Markov Structure: Applications to Probabilistic Inference in Machine Learning](https://arxiv.org/abs/2504.13927)
*F. Herrera,U. A. Rozikov,M. V. Velasco*

Main category: cs.LG

TL;DR: 研究了包含Ising相互作用和数据依赖项的哈密顿量，在Cayley树上探索了平移不变Gibbs测度（TIGM），发现最多三种不同的TIGM，可用于机器学习的层次数据推断。


<details>
  <summary>Details</summary>
Motivation: 探索哈密顿量在Cayley树上的平移不变Gibbs测度，为机器学习的层次数据推断提供结构化方法。

Method: 研究包含Ising相互作用和数据依赖项的哈密顿量，分析其在Cayley树上的TIGM。

Result: 在特定参数条件下，最多存在三种不同的TIGM，代表自旋系统的平衡状态。

Conclusion: 这些测度为机器学习的层次数据推断提供了实用工具，适用于去噪、弱监督学习和异常检测等任务。

Abstract: In this paper, we investigate a Hamiltonian that incorporates Ising
interactions between hidden $\pm 1$ spins, alongside a data-dependent term that
couples the hidden and observed variables. Specifically, we explore
translation-invariant Gibbs measures (TIGM) of this Hamiltonian on Cayley
trees.
  Under certain explicit conditions on the model's parameters, we demonstrate
that there can be up to three distinct TIGMs. Each of these measures represents
an equilibrium state of the spin system. These measures provide a structured
approach to inference on hierarchical data in machine learning. They have
practical applications in tasks such as denoising, weakly supervised learning,
and anomaly detection. The Cayley tree structure is particularly advantageous
for exact inference due to its tractability.

</details>


### [278] [Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining](https://arxiv.org/abs/2504.13932)
*Deyu Cao,Samin Aref*

Main category: cs.LG

TL;DR: 论文提出了一种基于ApiQ的改进方法，通过结合显著性感知正则化，在超低比特量化中提升性能，无需完全重新训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的量化方法在压缩模型大小的同时面临精度损失或资源消耗大的问题，现有方法如ApiQ虽高效但仍有改进空间。

Method: 结合ApiQ的部分训练与显著性感知正则化，优先保留关键参数，提出一种新的超低比特量化方法。

Result: 在LLaMA系列模型上的实验表明，该方法提升了量化模型的精度，缩小了与全精度模型的差距，且开销极小。

Conclusion: 该方法为大型语言模型的超低比特量化提供了高效解决方案，未来将公开以促进相关研究。

Abstract: Large language models offer remarkable capabilities, but their size and
computational demands pose practical challenges. Quantization methods compress
their size through replacing their high-precision parameters by quantized
values of lower precision. Post-training quantization reduces model size
efficiently at the cost of decreased accuracy, while quantization-aware
training better preserves accuracy but is resource-intensive. Among existing
post-training quantization algorithms, the ApiQ method achieves superior
accuracy preservation at minimal memory and time overhead. We investigate two
ideas to extend performance in ultra-low-bit quantization beyond ApiQ's level.
First, we look into combining existing quantization-aware training techniques
with ApiQ's partial training. We show that this does not outperform the
baseline ApiQ method with limited training data and frozen weights. This leads
to two key insights: (1) The substantial representational capacity that is
gained through full retraining may not be feasible through partial training.
(2) This gain seems to depend on using a large and diverse dataset in
quantization-aware training. Second, through a novel approach informed by the
two insights, we propose an ultra-low-bit quantization method that builds upon
ApiQ and extends its performance without the need for full retraining. It
relies on a saliency-aware regularization term that prioritizes preserving the
most impactful parameters during quantization. Our experiments on benchmark
language models from the LLaMA family show that our proposed approach boosts
accuracy and tightens the gap between the quantized model and the
full-precision model, with minimal overhead. Our method will be made publicly
available to facilitate future developments in ultra-low-bit quantization of
large language models.

</details>


### [279] [NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning](https://arxiv.org/abs/2504.13941)
*Syeda Nahida Akter,Shrimai Prabhumoye,Matvei Novikov,Seungju Han,Ying Lin,Evelina Bakhturi,Eric Nyberg,Yejin Choi,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro*

Main category: cs.LG

TL;DR: NEMOTRON-CROSSTHINK是一个通过多领域数据增强强化学习的框架，提升大语言模型在广泛推理任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在数学推理中表现良好，但在其他领域因数据有限、奖励结构不明确和任务多样性而难以推广。

Method: 结合多领域数据（STEM、人文、社科等），使用结构化模板控制答案复杂性，筛选可验证答案，并优化数据混合策略。

Result: 在数学和非数学推理基准测试中准确率显著提升（如MATH-500 +30.1%），且响应效率提高（减少28%的token使用）。

Conclusion: 多领域、多格式数据的整合能显著提升大语言模型的准确性、效率和泛化能力。

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities,
particularly when enhanced through Reinforcement Learning (RL). While prior
work has successfully applied RL to mathematical reasoning -- where rules and
correctness are well-defined -- generalizing these methods to broader reasoning
domains remains challenging due to limited data, the lack of verifiable reward
structures, and diverse task requirements. In this work, we propose
NEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain
corpora, including both synthetic and real-world question-answer pairs, into RL
training to improve generalization across diverse reasoning tasks.
NEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from
varied sources spanning STEM, humanities, social sciences, etc.; (2) applying
structured templates (e.g., multiple-choice and open-ended) to control
answer-space complexity; (3) filtering for verifiable answers; and (4)
optimizing data blending strategies that utilizes data from multiple sources
effectively. Our approach enables scalable and verifiable reward modeling
beyond mathematics and demonstrates improved accuracies on both math (MATH-500:
+30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%,
GPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover,
NEMOTRON-CROSSTHINK exhibits significantly improved response efficiency --
using 28% fewer tokens for correct answers -- highlighting more focused and
effective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that
integrating multi-domain, multi-format data in RL leads to more accurate,
efficient, and generalizable LLMs.

</details>


### [280] [Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models](https://arxiv.org/abs/2504.13945)
*Zhanglin Wu,Tengfei Song,Ning Xie,Weidong Zhang,Mengli Zhu,Shuang Wu,Shiliang Sun,Hao Yang*

Main category: cs.LG

TL;DR: 本文提出了MOTBench，一个专注于菜单翻译的评估框架，用于测试大型视觉语言模型（LVLMs）在复杂布局和跨文化背景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（如OCRBench）主要关注简单布局的短文本或长文本，而忽略了复杂布局的长文本理解能力，这在跨文化沟通中尤为重要。

Method: 提出MOTBench，包含中英文菜单数据集，要求LVLMs准确识别和翻译菜品、价格及单位，并评估其视觉理解和语言处理能力。

Result: 实验表明，自动评估结果与专业人工评估高度一致，并揭示了当前LVLMs的优势与不足。

Conclusion: MOTBench为LVLMs的未来发展提供了有价值的指导，并已在GitHub开源。

Abstract: The rapid advancement of large vision-language models (LVLMs) has
significantly propelled applications in document understanding, particularly in
optical character recognition (OCR) and multilingual translation. However,
current evaluations of LVLMs, like the widely used OCRBench, mainly focus on
verifying the correctness of their short-text responses and long-text responses
with simple layout, while the evaluation of their ability to understand long
texts with complex layout design is highly significant but largely overlooked.
In this paper, we propose Menu OCR and Translation Benchmark (MOTBench), a
specialized evaluation framework emphasizing the pivotal role of menu
translation in cross-cultural communication. MOTBench requires LVLMs to
accurately recognize and translate each dish, along with its price and unit
items on a menu, providing a comprehensive assessment of their visual
understanding and language processing capabilities. Our benchmark is comprised
of a collection of Chinese and English menus, characterized by intricate
layouts, a variety of fonts, and culturally specific elements across different
languages, along with precise human annotations. Experiments show that our
automatic evaluation results are highly consistent with professional human
evaluation. We evaluate a range of publicly available state-of-the-art LVLMs,
and through analyzing their output to identify the strengths and weaknesses in
their performance, offering valuable insights to guide future advancements in
LVLM development. MOTBench is available at https://github.com/gitwzl/MOTBench.

</details>


### [281] [On Revealing the Hidden Problem Structure in Real-World and Theoretical Problems Using Walsh Coefficient Influence](https://arxiv.org/abs/2504.13949)
*M. W. Przewozniczek,F. Chicano,R. Tinós,J. Nalepa,B. Ruszczak,A. M. Wijata*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Walsh分解的灰盒优化方法，通过构建加权动态变量交互图（wdVIG）来过滤噪声依赖，提升优化效果。


<details>
  <summary>Details</summary>
Motivation: 在灰盒优化中，变量间的非线性依赖可能包含噪声，影响优化效果。研究旨在识别并过滤这些无关依赖。

Method: 扩展Walsh分解，测量变量依赖强度，构建wdVIG图，过滤噪声依赖，生成优化掩码。

Result: 在噪声问题中，wdVIG掩码显著提升优化效果；在无噪声问题中，效果与现有方法相当。

Conclusion: wdVIG能有效识别噪声依赖，提升优化效率，尤其在噪声问题中表现突出。

Abstract: Gray-box optimization employs Walsh decomposition to obtain non-linear
variable dependencies and utilize them to propose masks of variables that have
a joint non-linear influence on fitness value. These masks significantly
improve the effectiveness of variation operators. In some problems, all
variables are non-linearly dependent, making the aforementioned masks useless.
We analyze the features of the real-world instances of such problems and show
that many of their dependencies may have noise-like origins. Such noise-caused
dependencies are irrelevant to the optimization process and can be ignored. To
identify them, we propose extending the use of Walsh decomposition by measuring
variable dependency strength that allows the construction of the weighted
dynamic Variable Interaction Graph (wdVIG). wdVIGs adjust the dependency
strength to mixed individuals. They allow the filtering of irrelevant
dependencies and re-enable using dependency-based masks by variation operators.
We verify the wdVIG potential on a large benchmark suite. For problems with
noise, the wdVIG masks can improve the optimizer's effectiveness. If all
dependencies are relevant for the optimization, i.e., the problem is not
noised, the influence of wdVIG masks is similar to that of state-of-the-art
structures of this kind.

</details>


### [282] [Open-Medical-R1: How to Choose Data for RLVR Training at Medicine Domain](https://arxiv.org/abs/2504.13950)
*Zhongxi Qiu,Zhang Zhang,Yan Hu,Heng Li,Jiang Liu*

Main category: cs.LG

TL;DR: 论文研究了在医学领域中强化学习与验证奖励（RLVR）训练的最佳数据选择策略，发现经过筛选的数据优于随机采样，并探讨了不同筛选方法对性能的影响。


<details>
  <summary>Details</summary>
Motivation: RLVR在数学和逻辑谜题中表现优异，但在医学等专业领域的应用尚未充分探索，因此研究如何优化数据选择以提升性能。

Method: 使用四种数据采样策略（随机采样及基于Phi-4、Gemma-3-27b-it和Gemma-3-12b-it模型的筛选），以Gemma-3-12b-it为基础模型，结合GRPO方法，在多个基准测试中评估性能。

Result: 筛选数据训练的模型表现优于随机采样，其中Gemma-3-12b-it自筛选在医学领域表现最佳，但泛化性较差；更大模型的筛选则提供更好的整体鲁棒性。

Conclusion: 研究为专业领域RLVR的有效数据组织提供了见解，强调了数据选择对性能优化的重要性。

Abstract: This paper explores optimal data selection strategies for Reinforcement
Learning with Verified Rewards (RLVR) training in the medical domain. While
RLVR has shown exceptional potential for enhancing reasoning capabilities in
large language models, most prior implementations have focused on mathematics
and logical puzzles, with limited exploration of domain-specific applications
like medicine. We investigate four distinct data sampling strategies from
MedQA-USMLE: random sampling (baseline), and filtering using Phi-4,
Gemma-3-27b-it, and Gemma-3-12b-it models. Using Gemma-3-12b-it as our base
model and implementing Group Relative Policy Optimization (GRPO), we evaluate
performance across multiple benchmarks including MMLU, GSM8K, MMLU-Pro, and
CMMLU. Our findings demonstrate that models trained on filtered data generally
outperform those trained on randomly selected samples. Notably, training on
self-filtered samples (using Gemma-3-12b-it for filtering) achieved superior
performance in medical domains but showed reduced robustness across different
benchmarks, while filtering with larger models from the same series yielded
better overall robustness. These results provide valuable insights into
effective data organization strategies for RLVR in specialized domains and
highlight the importance of thoughtful data selection in achieving optimal
performance. You can access our repository
(https://github.com/Qsingle/open-medical-r1) to get the codes.

</details>


### [283] [Generative System Dynamics in Recurrent Neural Networks](https://arxiv.org/abs/2504.13951)
*Michele Casoni,Tommaso Guidi,Alessandro Betti,Stefano Melacci,Marco Gori*

Main category: cs.LG

TL;DR: 研究了RNN在非线性激活函数下的连续时间动态，发现斜对称权重矩阵是稳定极限环的关键，双曲正切类激活函数能保持振荡动态。


<details>
  <summary>Details</summary>
Motivation: 探索RNN在非线性激活函数下如何实现持续振荡行为，避免静态固定点收敛。

Method: 分析斜对称权重矩阵的作用，并通过数值模拟验证非线性激活函数对极限环和数值稳定性的影响。

Result: 斜对称权重矩阵是稳定极限环的基础，双曲正切类激活函数能保持振荡并提升数值稳定性。

Conclusion: 研究为设计能捕捉复杂时间依赖性的RNN架构提供了实用策略，如增强记忆能力。

Abstract: In this study, we investigate the continuous time dynamics of Recurrent
Neural Networks (RNNs), focusing on systems with nonlinear activation
functions. The objective of this work is to identify conditions under which
RNNs exhibit perpetual oscillatory behavior, without converging to static fixed
points. We establish that skew-symmetric weight matrices are fundamental to
enable stable limit cycles in both linear and nonlinear configurations. We
further demonstrate that hyperbolic tangent-like activation functions (odd,
bounded, and continuous) preserve these oscillatory dynamics by ensuring motion
invariants in state space. Numerical simulations showcase how nonlinear
activation functions not only maintain limit cycles, but also enhance the
numerical stability of the system integration process, mitigating those
instabilities that are commonly associated with the forward Euler method. The
experimental results of this analysis highlight practical considerations for
designing neural architectures capable of capturing complex temporal
dependencies, i.e., strategies for enhancing memorization skills in recurrent
models.

</details>


### [284] [Prognosis Of Lithium-Ion Battery Health with Hybrid EKF-CNN+LSTM Model Using Differential Capacity](https://arxiv.org/abs/2504.13956)
*Md Azizul Hoque,Babul Salam,Mohd Khair Hassan,Abdulkabir Aliyu,Abedalmuhdi Almomany,Muhammed Sutcu*

Main category: cs.LG

TL;DR: 该论文提出了一种电池退化测试模型，结合DCA、EKF、CNN和LSTM技术，评估了两种锂离子电池在不同充放电速率下的内部退化机制，发现LiFePO4电池性能更稳定。


<details>
  <summary>Details</summary>
Motivation: 电池退化是电动汽车和储能系统的主要挑战，现有研究多关注SOC估计，而忽略内部退化机制。

Method: 使用LiNiCoAlO2和LiFePO4电池，在不同充放电速率下进行测试，结合EKF、CNN和LSTM验证数据，并采用PIM技术分析电池健康状态。

Result: 模型误差低于0.001%，LiFePO4电池在多种负载条件下表现更稳定。

Conclusion: LiFePO4电池在快速负载条件下退化较慢，性能优于LiNiCoAlO2电池。

Abstract: Battery degradation is a major challenge in electric vehicles (EV) and energy
storage systems (ESS). However, most degradation investigations focus mainly on
estimating the state of charge (SOC), which fails to accurately interpret the
cells' internal degradation mechanisms. Differential capacity analysis (DCA)
focuses on the rate of change of cell voltage about the change in cell
capacity, under various charge/discharge rates. This paper developed a battery
cell degradation testing model that used two types of lithium-ions (Li-ion)
battery cells, namely lithium nickel cobalt aluminium oxides (LiNiCoAlO2) and
lithium iron phosphate (LiFePO4), to evaluate internal degradation during
loading conditions. The proposed battery degradation model contains distinct
charge rates (DCR) of 0.2C, 0.5C, 1C, and 1.5C, as well as discharge rates
(DDR) of 0.5C, 0.9C, 1.3C, and 1.6C to analyze the internal health and
performance of battery cells during slow, moderate, and fast loading
conditions. Besides, this research proposed a model that incorporates the
Extended Kalman Filter (EKF), Convolutional Neural Network (CNN), and Long
Short-Term Memory (LSTM) networks to validate experimental data. The proposed
model yields excellent modelling results based on mean squared error (MSE), and
root mean squared error (RMSE), with errors of less than 0.001% at DCR and DDR.
The peak identification technique (PIM) has been utilized to investigate
battery health based on the number of peaks, peak position, peak height, peak
area, and peak width. At last, the PIM method has discovered that the cell aged
gradually under normal loading rates but deteriorated rapidly under fast
loading conditions. Overall, LiFePO4 batteries perform more robustly and
consistently than (LiNiCoAlO2) cells under varying loading conditions.

</details>


### [285] [ToolRL: Reward is All Tool Learning Needs](https://arxiv.org/abs/2504.13958)
*Cheng Qian,Emre Can Acikgoz,Qi He,Hongru Wang,Xiusi Chen,Dilek Hakkani-Tür,Gokhan Tur,Heng Ji*

Main category: cs.LG

TL;DR: 论文研究了强化学习中奖励设计对LLMs工具使用能力的提升，提出了一种定制化奖励策略，并通过GRPO训练模型，性能显著优于基线和SFT模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs通过监督微调（SFT）学习工具使用能力，但难以泛化到复杂场景。强化学习（RL）虽有潜力，但奖励设计存在挑战。

Method: 系统探索多种奖励策略，提出定制化奖励设计，并采用GRPO训练LLMs。

Result: 实验表明，该方法在多个基准测试中表现优异，性能提升17%（基线）和15%（SFT）。

Conclusion: 奖励设计对LLMs工具使用能力至关重要，提出的方法显著提升了泛化性能和训练稳定性。

Abstract: Current Large Language Models (LLMs) often undergo supervised fine-tuning
(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to
unfamiliar or complex tool use scenarios. Recent advancements in reinforcement
learning (RL), particularly with R1-like models, have demonstrated promising
reasoning and generalization abilities. Yet, reward design for tool use
presents unique challenges: multiple tools may be invoked with diverse
parameters, and coarse-grained reward signals, such as answer matching, fail to
offer the finegrained feedback required for effective learning. In this work,
we present the first comprehensive study on reward design for tool selection
and application tasks within the RL paradigm. We systematically explore a wide
range of reward strategies, analyzing their types, scales, granularity, and
temporal dynamics. Building on these insights, we propose a principled reward
design tailored for tool use tasks and apply it to train LLMs using Group
Relative Policy Optimization (GRPO). Empirical evaluations across diverse
benchmarks demonstrate that our approach yields robust, scalable, and stable
training, achieving a 17% improvement over base models and a 15% gain over SFT
models. These results highlight the critical role of thoughtful reward design
in enhancing the tool use capabilities and generalization performance of LLMs.
All the codes are released to facilitate future research.

</details>


### [286] [CONTINA: Confidence Interval for Traffic Demand Prediction with Coverage Guarantee](https://arxiv.org/abs/2504.13961)
*Chao Yang,Xiannan Huang,Shuhan Qiu,Yan Cheng*

Main category: cs.LG

TL;DR: 提出了一种自适应交通需求预测置信区间的方法CONTINA，能动态调整区间宽度，确保覆盖率和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有置信区间建模方法依赖严格假设，在动态交通环境中可能失效，需一种自适应方法。

Method: 通过收集部署期间的区间误差动态调整下一时间步的区间宽度，理论证明其覆盖率收敛于目标水平。

Result: 在四个真实数据集上验证，CONTINA能提供更短且有效的置信区间。

Conclusion: 该方法为交通管理提供了更合理和鲁棒的决策支持，代码和数据集已开源。

Abstract: Accurate short-term traffic demand prediction is critical for the operation
of traffic systems. Besides point estimation, the confidence interval of the
prediction is also of great importance. Many models for traffic operations,
such as shared bike rebalancing and taxi dispatching, take into account the
uncertainty of future demand and require confidence intervals as the input.
However, existing methods for confidence interval modeling rely on strict
assumptions, such as unchanging traffic patterns and correct model
specifications, to guarantee enough coverage. Therefore, the confidence
intervals provided could be invalid, especially in a changing traffic
environment. To fill this gap, we propose an efficient method, CONTINA
(Conformal Traffic Intervals with Adaptation) to provide interval predictions
that can adapt to external changes. By collecting the errors of interval during
deployment, the method can adjust the interval in the next step by widening it
if the errors are too large or shortening it otherwise. Furthermore, we
theoretically prove that the coverage of the confidence intervals provided by
our method converges to the target coverage level. Experiments across four
real-world datasets and prediction models demonstrate that the proposed method
can provide valid confidence intervals with shorter lengths. Our method can
help traffic management personnel develop a more reasonable and robust
operation plan in practice. And we release the code, model and dataset in
\href{ https://github.com/xiannanhuang/CONTINA/}{ Github}.

</details>


### [287] [Adversarial Resilience against Clean-Label Attacks in Realizable and Noisy Settings](https://arxiv.org/abs/2504.13966)
*Carolin Heinzler*

Main category: cs.LG

TL;DR: 论文研究了在包含未知数量干净标签对抗样本的数据流中，如何建立随机性保证的挑战。学习者可以选择在不确定时弃权，其遗憾以误分类和弃权误差衡量。方法基于Goel等人的工作，但修正了其论证中的不准确性，并扩展到不可知设置。


<details>
  <summary>Details</summary>
Motivation: 解决在干净标签对抗样本存在的情况下，如何保证学习算法的性能，并扩展到更广泛的不可知设置。

Method: 允许学习者在不确定时弃权，基于Goel等人的方法，修正其论证，并引入干净标签对抗者的概念，扩展到不可知设置。

Result: 首次在不可知设置下对阈值学习器进行理论分析，适用于带有噪声的干净标签对抗者。

Conclusion: 该方法在干净标签对抗样本存在时提供了理论保证，并成功扩展到不可知设置，但仍限于某些假设空间。

Abstract: We investigate the challenge of establishing stochastic-like guarantees when
sequentially learning from a stream of i.i.d. data that includes an unknown
quantity of clean-label adversarial samples. We permit the learner to abstain
from making predictions when uncertain. The regret of the learner is measured
in terms of misclassification and abstention error, where we allow the learner
to abstain for free on adversarial injected samples. This approach is based on
the work of Goel, Hanneke, Moran, and Shetty from arXiv:2306.13119. We explore
the methods they present and manage to correct inaccuracies in their
argumentation.
  However, this approach is limited to the realizable setting, where labels are
assigned according to some function $f^*$ from the hypothesis space
$\mathcal{F}$. Based on similar arguments, we explore methods to make
adaptations for the agnostic setting where labels are random. Introducing the
notion of a clean-label adversary in the agnostic context, we are the first to
give a theoretical analysis of a disagreement-based learner for thresholds,
subject to a clean-label adversary with noise.

</details>


### [288] [Enhancing Stroke Diagnosis in the Brain Using a Weighted Deep Learning Approach](https://arxiv.org/abs/2504.13974)
*Yao Zhiwan,Reza Zarrab,Jean Dubois*

Main category: cs.LG

TL;DR: 提出了一种加权投票集成（WVE）机器学习模型，用于高效预测脑卒中，准确率达94.91%。


<details>
  <summary>Details</summary>
Motivation: 传统脑卒中诊断方法（如CT和MRI）成本高且耗时，需要更高效的预测手段。

Method: 结合随机森林、深度学习和基于直方图的梯度提升分类器，构建加权投票集成模型。

Result: 在私有数据集上达到94.91%的准确率，支持早期风险评估和预防。

Conclusion: 未来研究可探索优化技术以进一步提升模型准确性。

Abstract: A brain stroke occurs when blood flow to a part of the brain is disrupted,
leading to cell death. Traditional stroke diagnosis methods, such as CT scans
and MRIs, are costly and time-consuming. This study proposes a weighted voting
ensemble (WVE) machine learning model that combines predictions from
classifiers like random forest, Deep Learning, and histogram-based gradient
boosting to predict strokes more effectively. The model achieved 94.91%
accuracy on a private dataset, enabling early risk assessment and prevention.
Future research could explore optimization techniques to further enhance
accuracy.

</details>


### [289] [Multiscale Tensor Summation Factorization as a New Neural Network Layer (MTS Layer) for Multidimensional Data Processing](https://arxiv.org/abs/2504.13975)
*Mehmet Yamaç,Muhammad Numan Yousaf,Serkan Kiranyaz,Moncef Gabbouj*

Main category: cs.LG

TL;DR: 论文提出了一种名为多尺度张量求和（MTS）分解的新型神经网络算子，通过多尺度张量求和和Tucker分解类模式乘积，显著提升了参数效率和优化性能，优于传统密集层和卷积层。


<details>
  <summary>Details</summary>
Motivation: 多层感知机（MLP）和卷积神经网络（CNN）在计算机视觉任务中存在高维输入输出和有限感受野的问题，需要一种更高效的算子。

Method: 引入MTS分解作为新的神经网络层，通过多尺度张量求和和Tucker分解类模式乘积实现参数减少和优化效率提升。

Result: 实验证明MTS网络在分类、压缩和信号恢复等任务中优于MLP和CNN，结合多门控头（MHG）后性能更优。

Conclusion: MTSNet在计算机视觉任务中展现了更好的复杂度-性能权衡，代码已开源。

Abstract: Multilayer perceptrons (MLP), or fully connected artificial neural networks,
are known for performing vector-matrix multiplications using learnable weight
matrices; however, their practical application in many machine learning tasks,
especially in computer vision, can be limited due to the high dimensionality of
input-output pairs at each layer. To improve efficiency, convolutional
operators have been utilized to facilitate weight sharing and local
connections, yet they are constrained by limited receptive fields. In this
paper, we introduce Multiscale Tensor Summation (MTS) Factorization, a novel
neural network operator that implements tensor summation at multiple scales,
where each tensor to be summed is obtained through Tucker-decomposition-like
mode products. Unlike other tensor decomposition methods in the literature, MTS
is not introduced as a network compression tool; instead, as a new backbone
neural layer. MTS not only reduces the number of parameters required while
enhancing the efficiency of weight optimization compared to traditional dense
layers (i.e., unfactorized weight matrices in MLP layers), but it also
demonstrates clear advantages over convolutional layers. The proof-of-concept
experimental comparison of the proposed MTS networks with MLPs and
Convolutional Neural Networks (CNNs) demonstrates their effectiveness across
various tasks, such as classification, compression, and signal restoration.
Additionally, when integrated with modern non-linear units such as the
multi-head gate (MHG), also introduced in this study, the corresponding neural
network, MTSNet, demonstrates a more favorable complexity-performance tradeoff
compared to state-of-the-art transformers in various computer vision
applications. The software implementation of the MTS layer and the
corresponding MTS-based networks, MTSNets, is shared at
https://github.com/mehmetyamac/MTSNet.

</details>


### [290] [CacheFormer: High Attention-Based Segment Caching](https://arxiv.org/abs/2504.13981)
*Sushant Singh,Ausif Mahmood*

Main category: cs.LG

TL;DR: 提出了一种基于缓存和虚拟内存原理的长上下文处理方法，通过分段和动态检索机制，显著降低了困惑度。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Linformer、Longformer等未能完全解决长上下文处理问题，需要更高效的机制。

Method: 将长上下文分段，动态检索高注意力未压缩段，结合滑动窗口、压缩分段和重叠段注意力机制。

Result: 新架构在相同模型规模下平均困惑度提升8.5%，优于现有SOTA方法。

Conclusion: 该方法通过分段和动态检索机制，有效提升了长上下文处理的效率和性能。

Abstract: Efficiently handling long contexts in transformer-based language models with
low perplexity is an active area of research. Numerous recent approaches like
Linformer, Longformer, Performer, and Structured state space models (SSMs).,
have not fully resolved this problem. All these models strive to reduce the
quadratic time complexity of the attention mechanism while minimizing the loss
in quality due to the effective compression of the long context. Inspired by
the cache and virtual memory principle in computers, where in case of a cache
miss, not only the needed data is retrieved from the memory, but the adjacent
data is also obtained, we apply this concept to handling long contexts by
dividing it into small segments. In our design, we retrieve the nearby segments
in an uncompressed form when high segment-level attention occurs at the
compressed level. Our en-hancements for handling long context include
aggregating four attention mechanisms consisting of short sliding window
attention, long compressed segmented attention, dynamically retrieving top k
high attention uncompressed segments, and overlapping segments in long segment
attention to avoid segment fragmentation. These enhancements result in an
architecture that outperforms ex-isting SOTA architectures with an average
perplexity improvement of 8.5% over similar model sizes.

</details>


### [291] [When Machine Learning Meets Importance Sampling: A More Efficient Rare Event Estimation Approach](https://arxiv.org/abs/2504.13982)
*Ruoning Zhao,Xinyun Chen*

Main category: cs.LG

TL;DR: 论文提出了一种新的重要性采样方法，通过利用稳态分布的边际似然比，解决了路径依赖似然函数方差爆炸的问题，并结合机器学习算法提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有重要性采样方法在估计串联队列稳态下的罕见事件概率时效率低下，原因是路径依赖似然函数的方差爆炸。

Method: 引入基于稳态分布边际似然比的新重要性采样方法，并设计机器学习算法估计该边际似然比。

Result: 数值实验表明，新算法优于经典重要性采样方法。

Conclusion: 新方法有效解决了方差问题，提升了罕见事件概率估计的效率。

Abstract: Driven by applications in telecommunication networks, we explore the
simulation task of estimating rare event probabilities for tandem queues in
their steady state. Existing literature has recognized that importance sampling
methods can be inefficient, due to the exploding variance of the path-dependent
likelihood functions. To mitigate this, we introduce a new importance sampling
approach that utilizes a marginal likelihood ratio on the stationary
distribution, effectively avoiding the issue of excessive variance. In
addition, we design a machine learning algorithm to estimate this marginal
likelihood ratio using importance sampling data. Numerical experiments indicate
that our algorithm outperforms the classic importance sampling methods.

</details>


### [292] [QuatE-D: A Distance-Based Quaternion Model for Knowledge Graph Embedding](https://arxiv.org/abs/2504.13983)
*Hamideh-Sadat Fazael-Ardakani,Hamid Soltanian-Zadeh*

Main category: cs.LG

TL;DR: QuatE-D是一种新型的四元数知识图谱嵌入模型，采用基于距离的评分函数，优于传统内积方法，提高了可解释性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统的内积方法在知识图谱嵌入中可能无法充分捕捉复杂的关联模式，因此需要更灵活且可解释的评分函数。

Method: 提出QuatE-D模型，使用欧几里得距离作为评分函数，替代传统内积方法。

Result: 实验表明，QuatE-D在性能上具有竞争力，尤其在降低平均排名方面表现突出，同时保持了高效的参数化。

Conclusion: 基于距离的评分函数在四元数嵌入中表现出色，为知识图谱补全提供了新的研究方向。

Abstract: Knowledge graph embedding (KGE) methods aim to represent entities and
relations in a continuous space while preserving their structural and semantic
properties. Quaternion-based KGEs have demonstrated strong potential in
capturing complex relational patterns. In this work, we propose QuatE-D, a
novel quaternion-based model that employs a distance-based scoring function
instead of traditional inner-product approaches. By leveraging Euclidean
distance, QuatE-D enhances interpretability and provides a more flexible
representation of relational structures. Experimental results demonstrate that
QuatE-D achieves competitive performance while maintaining an efficient
parameterization, particularly excelling in Mean Rank reduction. These findings
highlight the effectiveness of distance-based scoring in quaternion embeddings,
offering a promising direction for knowledge graph completion.

</details>


### [293] [One Jump Is All You Need: Short-Cutting Transformers for Early Exit Prediction with One Jump to Fit All Exit Levels](https://arxiv.org/abs/2504.13984)
*Amrit Diggavi Seshadri*

Main category: cs.LG

TL;DR: 提出了一种名为OJFA的低秩捷径方法，显著减少了推理时的参数成本，同时保持了性能。


<details>
  <summary>Details</summary>
Motivation: 减少大型语言模型推理的时间和计算成本，同时保持性能。

Method: 提出One-Jump-Fits-All（OJFA）低秩捷径，替代多捷径方法，显著降低参数成本。

Result: OJFA方法在GPT2-XL、Phi3-Mini和Llama2-7B模型中表现稳定，性能接近多捷径方法。

Conclusion: OJFA方法在参数效率和性能之间取得了良好平衡，适用于多种Transformer模型。

Abstract: To reduce the time and computational costs of inference of large language
models, there has been interest in parameter-efficient low-rank early-exit
casting of transformer hidden-representations to final-representations. Such
low-rank short-cutting has been shown to outperform identity shortcuts at early
model stages while offering parameter-efficiency in shortcut jumps. However,
current low-rank methods maintain a separate early-exit shortcut jump to
final-representations for each transformer intermediate block-level during
inference. In this work, we propose selection of a single One-Jump-Fits-All
(OJFA) low-rank shortcut that offers over a 30x reduction in shortcut parameter
costs during inference. We show that despite this extreme reduction, our OJFA
choice largely matches the performance of maintaining multiple shortcut jumps
during inference and offers stable precision from all transformer block-levels
for GPT2-XL, Phi3-Mini and Llama2-7B transformer models.

</details>


### [294] [Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs](https://arxiv.org/abs/2504.13989)
*Lucas Maisonnave,Cyril Moineau,Olivier Bichler,Fabrice Rastello*

Main category: cs.LG

TL;DR: 本文提出了一种基于Hadamard矩阵的量化方法，显著减少了LLMs中的异常值，实现了3位量化，并在性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在边缘设备上的部署受到其庞大参数规模的限制，量化是减少内存和推理时间的常用方法，但LLMs中的异常值问题阻碍了低比特量化的实现。

Method: 利用Hadamard矩阵的理论优势，通过逐步二分搜索实现3位量化，支持非2的幂次嵌入维度。

Result: 在Mistral、LLaMA和Qwen等模型上，该方法实现了3位量化，性能提升40%。

Conclusion: Hadamard矩阵在减少异常值方面优于随机旋转矩阵，为LLMs的3位量化提供了实用解决方案。

Abstract: Large language models (LLMs) have become pivotal in artificial intelligence,
demonstrating strong capabilities in reasoning, understanding, and generating
data. However, their deployment on edge devices is hindered by their
substantial size, often reaching several billion parameters. Quantization is a
widely used method to reduce memory usage and inference time, however LLMs
present unique challenges due to the prevalence of outliers in their
activations. In this work, we leverage the theoretical advantages of Hadamard
matrices over random rotation matrices to push the boundaries of quantization
in LLMs. We demonstrate that Hadamard matrices are more effective in reducing
outliers, which are a significant obstacle in achieving low-bit quantization.
Our method based on a gradual binary search enables 3-bit quantization for
weights, activations, and key-value (KV) caches, resulting in a 40\% increase
in accuracy on common benchmarks compared to SoTA methods. We extend the use of
rotation matrices to support non-power-of-2 embedding dimensions, similar to
the Qwen architecture, by employing the Paley algorithm. We theoretically
demonstrates the superiority of Hadamard matrices in reducing outliers.We
achieved 3-bit quantization for weights, activations, and KV cache,
significantly enhancing model performance. Our experimental results on multiple
models family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of
our approach, outperforming existing methods and enabling practical 3-bit
quantization.

</details>


### [295] [PC-DeepNet: A GNSS Positioning Error Minimization Framework Using Permutation-Invariant Deep Neural Network](https://arxiv.org/abs/2504.13990)
*M. Humayun Kabir,Md. Ali Hasan,Md. Shafiqul Islam,Kyeongjun Ko,Wonjae Shin*

Main category: cs.LG

TL;DR: 论文提出了一种基于学习的框架PC-DeepNet，用于解决GNSS在城郊环境中的定位问题，通过PI-DNN估计位置修正，显著提升了定位精度。


<details>
  <summary>Details</summary>
Motivation: GNSS在城郊环境中因NLOS传播、多径效应和低接收功率等问题导致定位误差分布高度非线性和非高斯，传统基于高斯误差的模型定位方法效果不佳。

Method: 采用PI-DNN（排列不变性深度神经网络）估计位置修正，利用NLOS和多径指示器作为特征，增强城郊环境下的定位精度。

Result: 实验表明，PC-DeepNet在定位精度上优于现有基于模型和学习的方法，同时计算复杂度低于其他学习方法。

Conclusion: PC-DeepNet为城郊环境中的GNSS定位提供了一种高效且精确的解决方案。

Abstract: Global navigation satellite systems (GNSS) face significant challenges in
urban and sub-urban areas due to non-line-of-sight (NLOS) propagation,
multipath effects, and low received power levels, resulting in highly
non-linear and non-Gaussian measurement error distributions. In light of this,
conventional model-based positioning approaches, which rely on Gaussian error
approximations, struggle to achieve precise localization under these
conditions. To overcome these challenges, we put forth a novel learning-based
framework, PC-DeepNet, that employs a permutation-invariant (PI) deep neural
network (DNN) to estimate position corrections (PC). This approach is designed
to ensure robustness against changes in the number and/or order of visible
satellite measurements, a common issue in GNSS systems, while leveraging NLOS
and multipath indicators as features to enhance positioning accuracy in
challenging urban and sub-urban environments. To validate the performance of
the proposed framework, we compare the positioning error with state-of-the-art
model-based and learning-based positioning methods using two publicly available
datasets. The results confirm that proposed PC-DeepNet achieves superior
accuracy than existing model-based and learning-based methods while exhibiting
lower computational complexity compared to previous learning-based approaches.

</details>


### [296] [Deep Learning on Graphs for Mobile Network Topology Generation](https://arxiv.org/abs/2504.13991)
*Felix Nannesson Meli,Johan Tell,Shirwan Piroti,Tahar Zanouda,Elias Jarlebring*

Main category: cs.LG

TL;DR: 该论文提出了一种基于图深度学习的移动网络拓扑关系预测方法，替代传统启发式方法，并通过实验验证了图神经网络（GNN）和多层感知机的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法在移动网络拓扑关系（边缘）建立中存在局限性，无法在硬件安装后动态调整。因此，需要一种更灵活、高效的方法。

Method: 使用图深度学习模型（GNN和多层感知机），基于无线电节点配置数据和自动邻居关系（ANR）训练，预测移动网络中的边缘关系。

Result: 实验表明，GNN模型在考虑图结构时表现更优，且通过启发式方法（如节点距离）可显著提升训练效率和预测精度。

Conclusion: 图深度学习方法在移动网络拓扑关系预测中具有潜力，尤其是GNN模型，结合启发式方法可进一步提升性能。

Abstract: Mobile networks consist of interconnected radio nodes strategically
positioned across various geographical regions to provide connectivity
services. The set of relations between these radio nodes, referred to as the
\emph{mobile network topology}, is vital in the construction of the networking
infrastructure. Typically, the connections between radio nodes and their
associated cells are defined by software features that establish mobility
relations (referred to as \emph{edges} in this paper) within the mobile network
graph through heuristic methods. Although these approaches are efficient, they
encounter significant limitations, particularly since edges can only be
established prior to the installation of physical hardware.
  In this work, we use graph-based deep learning methods to determine mobility
relations (edges), trained on radio node configuration data and reliable
mobility relations set by Automatic Neighbor Relations (ANR) in stable
networks. This paper focuses on measuring the accuracy and precision of
different graph-based deep learning approaches applied to real-world mobile
networks. We evaluated two deep learning models. Our comprehensive experiments
on Telecom datasets obtained from operational Telecom Networks demonstrate the
effectiveness of the graph neural network (GNN) model and multilayer
perceptron. Our evaluation showed that considering graph structure improves
results, which motivates the use of GNNs. Additionally, we investigated the use
of heuristics to reduce the training time based on the distance between radio
nodes to eliminate irrelevant cases. Our investigation showed that the use of
these heuristics improved precision and accuracy considerably.

</details>


### [297] [First and Second Order Approximations to Stochastic Gradient Descent Methods with Momentum Terms](https://arxiv.org/abs/2504.13992)
*Eric Lu*

Main category: cs.LG

TL;DR: 论文研究了随机梯度下降（SGD）及其动量变体的动态行为，提出了在学习和动量参数随时间变化时的近似结果。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅覆盖了恒定学习率或无动量项的SGD，缺乏对动态参数的理论支持。

Method: 通过连续近似方法，研究SGD在学习和动量参数随时间变化时的动态行为。

Result: 提出了在弱假设下对SGD的近似结果，支持动态参数变化。

Conclusion: 该研究填补了SGD动态参数变化理论分析的空白。

Abstract: Stochastic Gradient Descent (SGD) methods see many uses in optimization
problems. Modifications to the algorithm, such as momentum-based SGD methods
have been known to produce better results in certain cases. Much of this,
however, is due to empirical information rather than rigorous proof. While the
dynamics of gradient descent methods can be studied through continuous
approximations, existing works only cover scenarios with constant learning
rates or SGD without momentum terms. We present approximation results under
weak assumptions for SGD that allow learning rates and momentum parameters to
vary with respect to time.

</details>


### [298] [Large Language Bayes](https://arxiv.org/abs/2504.14025)
*Justin Domke*

Main category: cs.LG

TL;DR: 论文提出了一种结合大型语言模型和概率编程语言的方法，通过输入非正式问题描述生成联合分布，解决了领域专家难以构建正式贝叶斯模型的问题。


<details>
  <summary>Details</summary>
Motivation: 领域专家通常缺乏时间或训练来构建正式的贝叶斯模型，因此需要一种自动化方法将非正式描述转化为模型。

Method: 结合大型语言模型和概率编程语言，生成联合分布，并通过重要性采样、MCMC和变分推断进行推理。

Result: 该方法能够在不指定正式模型的情况下生成合理的预测。

Conclusion: 该方法为领域专家提供了一种无需手动建模的自动化解决方案，具有实用价值。

Abstract: Many domain experts do not have the time or training to write formal Bayesian
models. This paper takes an informal problem description as input, and combines
a large language model and a probabilistic programming language to create a
joint distribution over formal models, latent variables, and data. A posterior
over latent variables follows by conditioning on observed data and integrating
over formal models. This presents a challenging inference problem. We suggest
an inference recipe that amounts to generating many formal models from the
large language model, performing approximate inference on each, and then doing
a weighted average. This is justified an analyzed as a combination of
self-normalized importance sampling, MCMC, and variational inference. We show
that this produces sensible predictions without the need to specify a formal
model.

</details>


### [299] [A synthetic dataset of French electric load curves with temperature conditioning](https://arxiv.org/abs/2504.14046)
*Tahar Nabil,Ghislain Agoua,Pierre Cauchois,Anne De Moliner,Benoît Grossin*

Main category: cs.LG

TL;DR: 论文提出了一种基于条件潜在扩散的合成电力负荷曲线数据集，用于解决隐私保护问题，同时支持能源建模应用。


<details>
  <summary>Details</summary>
Motivation: 能源转型导致电力使用行为变化，但智能电表数据受GDPR保护，需合成隐私保护且真实的数据。

Method: 使用条件潜在扩散生成合成负荷曲线数据集，并提供合同功率、分时电价和本地温度等生成条件。

Result: 数据集在保真度、实用性和隐私性方面表现良好，适用于能源建模。

Conclusion: 该合成数据集为能源建模提供了高质量且隐私保护的解决方案。

Abstract: The undergoing energy transition is causing behavioral changes in electricity
use, e.g. with self-consumption of local generation, or flexibility services
for demand control. To better understand these changes and the challenges they
induce, accessing individual smart meter data is crucial. Yet this is personal
data under the European GDPR. A widespread use of such data requires thus to
create synthetic realistic and privacy-preserving samples. This paper
introduces a new synthetic load curve dataset generated by conditional latent
diffusion. We also provide the contracted power, time-of-use plan and local
temperature used for generation. Fidelity, utility and privacy of the dataset
are thoroughly evaluated, demonstrating its good quality and thereby supporting
its interest for energy modeling applications.

</details>


### [300] [CAOTE: KV Caching through Attention Output Error based Token Eviction](https://arxiv.org/abs/2504.14051)
*Raghavv Goel,Junyoung Park,Mukul Gagrani,Dalton Jones,Matthew Morse,Harper Langston,Mingu Lee,Chris Lott*

Main category: cs.LG

TL;DR: 论文提出了一种新的令牌驱逐标准CAOTE，通过结合注意力分数和值向量信息优化驱逐误差，提升了资源受限设备上的模型性能。


<details>
  <summary>Details</summary>
Motivation: 长上下文支持的大语言模型在资源受限设备上存在内存和计算瓶颈，现有基于注意力分数的令牌驱逐方法缺乏对令牌贡献的全面评估。

Method: 提出CAOTE方法，结合注意力分数和值向量信息，优化令牌驱逐误差，可作为元启发式方法与其他驱逐方法结合使用。

Result: CAOTE与现有最先进的基于注意力分数的方法结合时，下游任务准确率始终提升。

Conclusion: CAOTE通过利用值向量信息优化令牌驱逐过程，显著提升了模型性能。

Abstract: While long context support of large language models has extended their
abilities, it also incurs challenges in memory and compute which becomes
crucial bottlenecks in resource-restricted devices. Token eviction, a widely
adopted post-training methodology designed to alleviate the bottlenecks by
evicting less important tokens from the cache, typically uses attention scores
as proxy metrics for token importance. However, one major limitation of
attention score as a token-wise importance metrics is that it lacks the
information about contribution of tokens to the attention output. In this
paper, we propose a simple eviction criterion based on the contribution of
cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction
error due to token eviction, by seamlessly integrating attention scores and
value vectors. This is the first method which uses value vector information on
top of attention-based eviction scores. Additionally, CAOTE can act as a
meta-heuristic method with flexible usage with any token eviction method. We
show that CAOTE, when combined with the state-of-the-art attention score-based
methods, always improves accuracies on the downstream task, indicating the
importance of leveraging information from values during token eviction process.

</details>


### [301] [Contextual Embedding-based Clustering to Identify Topics for Healthcare Service Improvement](https://arxiv.org/abs/2504.14068)
*K M Sajjadul Islam,Ravi Teja Karri,Srujan Vegesna,Jiawei Wu,Praveen Madiraju*

Main category: cs.LG

TL;DR: 该研究探索了无监督方法（如LDA、GSDMM和BERTopic）分析医疗短文本反馈，并提出kBERT模型，结合BERT嵌入和k-means聚类，在主题一致性和多样性上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 理解患者反馈对改善医疗服务至关重要，但短文本反馈的领域特定性和数据稀缺性使得传统监督学习方法难以适用。

Method: 研究采用无监督方法（LDA、GSDMM、BERTopic）和提出的kBERT模型分析439份医疗反馈，评估主题一致性和多样性。

Result: kBERT在主题一致性（Cv=0.53）和多样性（IRBOavg=1.00）上表现最优，优于其他模型。

Conclusion: 嵌入技术和上下文感知模型在医疗反馈分析中具有重要价值。

Abstract: Understanding patient feedback is crucial for improving healthcare services,
yet analyzing unlabeled short-text feedback presents significant challenges due
to limited data and domain-specific nuances. Traditional supervised learning
approaches require extensive labeled datasets, making unsupervised methods more
viable for uncovering meaningful insights from patient feedback. This study
explores unsupervised methods to extract meaningful topics from 439 survey
responses collected from a healthcare system in Wisconsin, USA. A keyword-based
filtering approach was applied to isolate complaint-related feedback using a
domain-specific lexicon. To delve deeper and analyze dominant topics in
feedback, we explored traditional topic modeling methods, including Latent
Dirichlet Allocation (LDA) and Gibbs Sampling Dirichlet Multinomial Mixture
(GSDMM), alongside BERTopic, an advanced neural embedding-based clustering
approach. To improve coherence and interpretability where data are scarce and
consist of short-texts, we propose kBERT, an integration of BERT embeddings
with k-means clustering. Model performance was assessed using coherence scores
(Cv ) for topic interpretability and average Inverted Rank-Biased Overlap
(IRBOavg) for topic diversity. Results indicate that kBERT achieves the highest
coherence (Cv = 0.53) and distinct topic separation (IRBOavg = 1.00),
outperforming all other models in short-text healthcare feedback analysis. Our
findings emphasize the importance of embedding-based techniques for topic
identification and highlight the need for context-aware models in healthcare
analytics.

</details>


### [302] [Leakage and Interpretability in Concept-Based Models](https://arxiv.org/abs/2504.14094)
*Enrico Parisini,Tapabrata Chakraborti,Chris Harbron,Ben D. MacArthur,Christopher R. S. Banerji*

Main category: cs.LG

TL;DR: 论文提出了一种信息论框架来量化概念瓶颈模型中的信息泄漏问题，并提出了两种新的泄漏度量方法（CTL和ICL），为设计更可靠的概念模型提供了实用指南。


<details>
  <summary>Details</summary>
Motivation: 概念瓶颈模型在高风险场景中具有潜力，但存在信息泄漏问题，影响模型的可靠性和可解释性。

Method: 引入信息论框架，定义CTL和ICL两种泄漏度量方法，并通过实验验证其有效性。

Result: CTL和ICL能更准确地预测模型行为，且概念嵌入模型普遍存在泄漏问题。

Conclusion: 论文提出了减少泄漏的实用设计指南，以提升概念模型的可靠性和可解释性。

Abstract: Concept Bottleneck Models aim to improve interpretability by predicting
high-level intermediate concepts, representing a promising approach for
deployment in high-risk scenarios. However, they are known to suffer from
information leakage, whereby models exploit unintended information encoded
within the learned concepts. We introduce an information-theoretic framework to
rigorously characterise and quantify leakage, and define two complementary
measures: the concepts-task leakage (CTL) and interconcept leakage (ICL)
scores. We show that these measures are strongly predictive of model behaviour
under interventions and outperform existing alternatives in robustness and
reliability. Using this framework, we identify the primary causes of leakage
and provide strong evidence that Concept Embedding Models exhibit substantial
leakage regardless of the hyperparameters choice. Finally, we propose practical
guidelines for designing concept-based models to reduce leakage and ensure
interpretability.

</details>


### [303] [Personalizing Exposure Therapy via Reinforcement Learning](https://arxiv.org/abs/2504.14095)
*Athar Mahmoudi-Nejad,Matthew Guzdial,Pierre Boulanger*

Main category: cs.LG

TL;DR: 本文提出了一种基于生理指标的自动适应治疗方法，利用强化学习生成个性化虚拟蜘蛛内容，显著优于传统规则方法。


<details>
  <summary>Details</summary>
Motivation: 个性化治疗能改善健康结果，但传统方法依赖治疗师的直觉和预定义规则，难以普适。

Method: 采用基于强化学习的经验驱动程序内容生成（EDPCGRL），根据患者生理指标动态调整虚拟蜘蛛内容。

Result: 通过人体实验证明，该方法显著优于传统规则方法。

Conclusion: 该方法有潜力提升个性化治疗干预效果。

Abstract: Personalized therapy, in which a therapeutic practice is adapted to an
individual patient, can lead to improved health outcomes. Typically, this is
accomplished by relying on a therapist's training and intuition along with
feedback from a patient. However, this requires the therapist to become an
expert on any technological components, such as in the case of Virtual Reality
Exposure Therapy (VRET). While there exist approaches to automatically adapt
therapeutic content to a patient, they generally rely on hand-authored,
pre-defined rules, which may not generalize to all individuals. In this paper,
we propose an approach to automatically adapt therapeutic content to patients
based on physiological measures. We implement our approach in the context of
virtual reality arachnophobia exposure therapy, and rely on experience-driven
procedural content generation via reinforcement learning (EDPCGRL) to generate
virtual spiders to match an individual patient. Through a human subject study,
we demonstrate that our system significantly outperforms a more common
rules-based method, highlighting its potential for enhancing personalized
therapeutic interventions.

</details>


### [304] [Enhancing Math Learning in an LMS Using AI-Driven Question Recommendations](https://arxiv.org/abs/2504.14098)
*Justus Råmunddal*

Main category: cs.LG

TL;DR: 论文提出了一种基于AI的方法，通过推荐相似数学问题来增强现代学习管理系统（LMS）中的数学学习。使用Meta的Llama-3.2-11B-Vision-Instruct模型生成数学问题的深度嵌入，并应用三种推荐方法（余弦相似度、自组织映射和高斯混合模型）来识别相似问题。用户交互数据用于评估方法效果，结果显示余弦相似度匹配最准确，自组织映射用户满意度更高，而高斯混合模型表现较差。


<details>
  <summary>Details</summary>
Motivation: 通过推荐相似数学问题，提升现代学习管理系统中的数学学习效果，利用AI技术优化学习体验。

Method: 使用Meta的Llama-3.2-11B-Vision-Instruct模型生成数学问题的深度嵌入，并应用余弦相似度、自组织映射（SOM）和高斯混合模型（GMM）三种方法进行推荐。

Result: 余弦相似度能准确匹配相似问题，自组织映射（SOM）用户满意度更高，而高斯混合模型（GMM）表现较差。适度的多样性可能提升学习效果，但需平衡。

Conclusion: 在数学问题推荐中，余弦相似度和自组织映射表现较好，而高斯混合模型效果不佳。适度的多样性有助于提升学习体验，但需合理平衡。

Abstract: This paper presents an AI-driven approach to enhance math learning in a
modern Learning Management System (LMS) by recommending similar math questions.
Deep embeddings for math questions are generated using Meta's
Llama-3.2-11B-Vision-Instruct model, and three recommendation methods-cosine
similarity, Self-Organizing Maps (SOM), and Gaussian Mixture Models (GMM)-are
applied to identify similar questions. User interaction data, including session
durations, response times, and correctness, are used to evaluate the methods.
Our findings suggest that while cosine similarity produces nearly identical
question matches, SOM yields higher user satisfaction whereas GMM generally
underperforms, indicating that introducing variety to a certain degree may
enhance engagement and thereby potential learning outcomes until variety is no
longer balanced reasonably, which our data about the implementations of all
three methods demonstrate.

</details>


### [305] [Predicting Stress and Damage in Carbon Fiber-Reinforced Composites Deformation Process using Composite U-Net Surrogate Model](https://arxiv.org/abs/2504.14143)
*Zeping Chen,Marwa Yacouti,Maryam Shakiba,Jian-Xun Wang,Tengfei Luo,Vikas Varshney*

Main category: cs.LG

TL;DR: 提出了一种基于U-Net的深度学习模型，用于高效预测碳纤维增强复合材料（CFRC）在变形过程中的应力和损伤场，相比传统方法提速60倍。


<details>
  <summary>Details</summary>
Motivation: 传统有限元方法（FEM）在计算效率上存在不足，现有数据驱动模型无法全面捕捉应力和损伤的演化过程。

Method: 采用自回归复合U-Net深度学习模型，结合空间特征捕捉和多尺度现象整合能力。

Result: 模型在预测CFRC微观结构中的应力和损伤分布演化时表现出高精度，计算速度比IGFEM快60倍以上。

Conclusion: 该模型为CFRC的性能优化提供了高效且准确的预测工具，适用于航空航天等要求苛刻的应用场景。

Abstract: Carbon fiber-reinforced composites (CFRC) are pivotal in advanced engineering
applications due to their exceptional mechanical properties. A deep
understanding of CFRC behavior under mechanical loading is essential for
optimizing performance in demanding applications such as aerospace structures.
While traditional Finite Element Method (FEM) simulations, including advanced
techniques like Interface-enriched Generalized FEM (IGFEM), offer valuable
insights, they can struggle with computational efficiency. Existing data-driven
surrogate models partially address these challenges by predicting propagated
damage or stress-strain behavior but fail to comprehensively capture the
evolution of stress and damage throughout the entire deformation history,
including crack initiation and propagation. This study proposes a novel
auto-regressive composite U-Net deep learning model to simultaneously predict
stress and damage fields during CFRC deformation. By leveraging the U-Net
architecture's ability to capture spatial features and integrate macro- and
micro-scale phenomena, the proposed model overcomes key limitations of prior
approaches. The model achieves high accuracy in predicting evolution of stress
and damage distribution within the microstructure of a CFRC under
unidirectional strain, offering a speed-up of over 60 times compared to IGFEM.

</details>


### [306] [A Physics-guided Multimodal Transformer Path to Weather and Climate Sciences](https://arxiv.org/abs/2504.14174)
*Jing Han,Hanting Chen,Kai Han,Xiaomeng Huang,Yongyun Hu,Wenjun Xu,Dacheng Tao,Ping Zhang*

Main category: cs.LG

TL;DR: 论文综述了AI在气象学中的应用，提出了一种基于多模态数据和transformer的新范式，结合物理信号和正则化技术提升模型能力。


<details>
  <summary>Details</summary>
Motivation: 传统气象方法精度有限，AI模型能显著提升准确性，同时结合物理信号增强可解释性。

Method: 将气象数据转化为2D/3D图像或视频，作为多模态数据输入transformer模型，并结合物理信号和正则化技术。

Result: 新范式具有强泛化能力，适用于多种任务。

Conclusion: 未来需进一步提升模型精度和可解释性。

Abstract: With the rapid development of machine learning in recent years, many problems
in meteorology can now be addressed using AI models. In particular, data-driven
algorithms have significantly improved accuracy compared to traditional
methods. Meteorological data is often transformed into 2D images or 3D videos,
which are then fed into AI models for learning. Additionally, these models
often incorporate physical signals, such as temperature, pressure, and wind
speed, to further enhance accuracy and interpretability. In this paper, we
review several representative AI + Weather/Climate algorithms and propose a new
paradigm where observational data from different perspectives, each with
distinct physical meanings, are treated as multimodal data and integrated via
transformers. Furthermore, key weather and climate knowledge can be
incorporated through regularization techniques to further strengthen the
model's capabilities. This new paradigm is versatile and can address a variety
of tasks, offering strong generalizability. We also discuss future directions
for improving model accuracy and interpretability.

</details>


### [307] [FedC4: Graph Condensation Meets Client-Client Collaboration for Efficient and Private Federated Graph Learning](https://arxiv.org/abs/2504.14188)
*Zekai Chen,Xunkai Li,Yinlin Zhu,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: FedC4是一种结合图压缩与客户端协作的联邦图学习框架，通过传输合成节点嵌入而非原始数据，降低通信开销并提升隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有客户端-客户端（C-C）方法因广播冗余节点嵌入导致高通信成本和隐私风险，需改进。

Method: FedC4将私有图压缩为合成节点嵌入，并引入三个模块实现个性化优化。

Result: 在八个真实数据集上，FedC4在性能和通信效率上均优于现有方法。

Conclusion: FedC4通过图压缩和个性化协作，有效解决了C-C架构中的通信和隐私问题。

Abstract: Federated Graph Learning (FGL) is an emerging distributed learning paradigm
that enables collaborative model training over decentralized graph-structured
data while preserving local privacy. Existing FGL methods can be categorized
into two optimization architectures: (1) the Server-Client (S-C) paradigm,
where clients upload local models for server-side aggregation; and (2) the
Client-Client (C-C) paradigm, which allows direct information exchange among
clients to support personalized training. Compared to S-C, the C-C architecture
better captures global graph knowledge and enables fine-grained optimization
through customized peer-to-peer communication. However, current C-C methods
often broadcast identical and redundant node embeddings, incurring high
communication costs and privacy risks. To address this, we propose FedC4, a
novel framework that combines graph Condensation with Client-Client
Collaboration. Instead of transmitting raw node-level features, FedC4 distills
each client's private graph into a compact set of synthetic node embeddings,
reducing communication overhead and enhancing privacy. In addition, FedC4
introduces three modules that allow source clients to send distinct node
representations tailored to target clients'graph structures, enabling
personalized optimization with global guidance. Extensive experiments on eight
real-world datasets show that FedC4 outperforms state-of-the-art baselines in
both performance and communication efficiency.

</details>


### [308] [DConAD: A Differencing-based Contrastive Representation Learning Framework for Time Series Anomaly Detection](https://arxiv.org/abs/2504.14204)
*Wenxin Zhang,Xiaojian Lin,Wenjun Yu,Guangzhen Yao,jingxiang Zhong,Yu Li,Renda Han,Songcheng Xu,Hao Shi,Cuicui Luo*

Main category: cs.LG

TL;DR: DConAD是一种基于差分对比表示学习的框架，用于时间序列异常检测，通过生成差分数据和Transformer架构增强鲁棒性，并在无监督学习中表现优异。


<details>
  <summary>Details</summary>
Motivation: 时间序列异常检测在风险识别和故障检测中至关重要，但现有无监督方法因异常模式多样性和数据复杂性而难以捕捉稳健依赖关系。

Method: DConAD利用差分数据提供额外信息，采用Transformer捕捉时空依赖，并通过KL散度对比学习范式（仅用正样本）和停止梯度策略优化模型。

Result: 在五个公共数据集上的实验表明，DConAD优于九种基线方法，表现出优越性和有效性。

Conclusion: DConAD通过差分对比学习框架显著提升了时间序列异常检测的性能，尤其在无监督场景下表现突出。

Abstract: Time series anomaly detection holds notable importance for risk
identification and fault detection across diverse application domains.
Unsupervised learning methods have become popular because they have no
requirement for labels. However, due to the challenges posed by the
multiplicity of abnormal patterns, the sparsity of anomalies, and the growth of
data scale and complexity, these methods often fail to capture robust and
representative dependencies within the time series for identifying anomalies.
To enhance the ability of models to capture normal patterns of time series and
avoid the retrogression of modeling ability triggered by the dependencies on
high-quality prior knowledge, we propose a differencing-based contrastive
representation learning framework for time series anomaly detection (DConAD).
Specifically, DConAD generates differential data to provide additional
information about time series and utilizes transformer-based architecture to
capture spatiotemporal dependencies, which enhances the robustness of unbiased
representation learning ability. Furthermore, DConAD implements a novel KL
divergence-based contrastive learning paradigm that only uses positive samples
to avoid deviation from reconstruction and deploys the stop-gradient strategy
to compel convergence. Extensive experiments on five public datasets show the
superiority and effectiveness of DConAD compared with nine baselines. The code
is available at https://github.com/shaieesss/DConAD.

</details>


### [309] [Dual-channel Heterophilic Message Passing for Graph Fraud Detection](https://arxiv.org/abs/2504.14205)
*Wenxin Zhang,Jingxing Zhong,Guangzhen Yao,Renda Han,Xiaojian Lin,Zeyu Zhang,Cuicui Luo*

Main category: cs.LG

TL;DR: 论文提出了一种名为DHMP的双通道异质性消息传递框架，用于改进欺诈检测任务，通过分离同质性和异质性子图来解决传统GNN方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于空间GNN的欺诈检测方法在消息传递中排除异质性邻居，破坏了原始图拓扑并增加了预测不确定性。

Method: DHMP通过异质性分离模块将图分为同质性和异质性子图，使用共享权重独立捕获不同频率的信号，并采用定制采样策略进行训练。

Result: 在三个真实数据集上的实验表明，DHMP优于现有方法，验证了分离不同频率信号对欺诈检测的重要性。

Conclusion: DHMP框架通过有效分离信号频率，显著提升了欺诈检测性能，为GNN在异质性图中的应用提供了新思路。

Abstract: Fraudulent activities have significantly increased across various domains,
such as e-commerce, online review platforms, and social networks, making fraud
detection a critical task. Spatial Graph Neural Networks (GNNs) have been
successfully applied to fraud detection tasks due to their strong inductive
learning capabilities. However, existing spatial GNN-based methods often
enhance the graph structure by excluding heterophilic neighbors during message
passing to align with the homophilic bias of GNNs. Unfortunately, this approach
can disrupt the original graph topology and increase uncertainty in
predictions. To address these limitations, this paper proposes a novel
framework, Dual-channel Heterophilic Message Passing (DHMP), for fraud
detection. DHMP leverages a heterophily separation module to divide the graph
into homophilic and heterophilic subgraphs, mitigating the low-pass inductive
bias of traditional GNNs. It then applies shared weights to capture signals at
different frequencies independently and incorporates a customized sampling
strategy for training. This allows nodes to adaptively balance the
contributions of various signals based on their labels. Extensive experiments
on three real-world datasets demonstrate that DHMP outperforms existing
methods, highlighting the importance of separating signals with different
frequencies for improved fraud detection. The code is available at
https://github.com/shaieesss/DHMP.

</details>


### [310] [Decomposition-based multi-scale transformer framework for time series anomaly detection](https://arxiv.org/abs/2504.14206)
*Wenxin Zhang,Cuicui Luo*

Main category: cs.LG

TL;DR: 提出了一种基于分解和Transformer的框架TransDe，用于多元时间序列异常检测，通过多尺度补丁Transformer和对比学习范式提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以直接建模复杂时间序列模式，且易受噪声影响导致性能下降。

Method: 结合时间序列分解和Transformer，提出多尺度补丁Transformer架构和基于KL散度的对比学习范式，引入异步损失函数优化性能。

Result: 在五个公开数据集上优于12个基线方法（F1分数）。

Conclusion: TransDe有效解决了复杂模式建模和噪声问题，性能优越。

Abstract: Time series anomaly detection is crucial for maintaining stable systems.
Existing methods face two main challenges. First, it is difficult to directly
model the dependencies of diverse and complex patterns within the sequences.
Second, many methods that optimize parameters using mean squared error struggle
with noise in the time series, leading to performance deterioration. To address
these challenges, we propose a transformer-based framework built on
decomposition (TransDe) for multivariate time series anomaly detection. The key
idea is to combine the strengths of time series decomposition and transformers
to effectively learn the complex patterns in normal time series data. A
multi-scale patch-based transformer architecture is proposed to exploit the
representative dependencies of each decomposed component of the time series.
Furthermore, a contrastive learn paradigm based on patch operation is proposed,
which leverages KL divergence to align the positive pairs, namely the pure
representations of normal patterns between different patch-level views. A novel
asynchronous loss function with a stop-gradient strategy is further introduced
to enhance the performance of TransDe effectively. It can avoid time-consuming
and labor-intensive computation costs in the optimization process. Extensive
experiments on five public datasets are conducted and TransDe shows superiority
compared with twelve baselines in terms of F1 score. Our code is available at
https://github.com/shaieesss/TransDe.

</details>


### [311] [A Novel Frequency-Spatial Domain Aware Network for Fast Thermal Prediction in 2.5D ICs](https://arxiv.org/abs/2504.14237)
*Dekang Zhang,Dan Niu,Zhou Jin,Yichao Dong,Jingweijia Tan,Changyin Sun*

Main category: cs.LG

TL;DR: 提出了一种新型频率-空间双域感知预测网络（FSA-Heat），用于2.5D IC的高精度快速热预测，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 在2.5D芯片中，现有CNN和GCN方法无法有效捕捉全局热特征，尤其是高频组件，限制了预测精度。

Method: 结合高频-低频和空间域编码器（FSTE）与频域跨尺度交互模块（FCIFormer），并设计了频率-空间混合损失（FSL）。

Result: 性能显著优于现有方法（RMSE降低99%，推理时间加速4.23倍），并展示了强大的泛化能力。

Conclusion: FSA-Heat在2.5D IC热管理中表现出色，为高频热特征提取提供了有效解决方案。

Abstract: In the post-Moore era, 2.5D chiplet-based ICs present significant challenges
in thermal management due to increased power density and thermal hotspots.
Neural network-based thermal prediction models can perform real-time
predictions for many unseen new designs. However, existing CNN-based and
GCN-based methods cannot effectively capture the global thermal features,
especially for high-frequency components, hindering prediction accuracy
enhancement. In this paper, we propose a novel frequency-spatial dual domain
aware prediction network (FSA-Heat) for fast and high-accuracy thermal
prediction in 2.5D ICs. It integrates high-to-low frequency and spatial domain
encoder (FSTE) module with frequency domain cross-scale interaction module
(FCIFormer) to achieve high-to-low frequency and global-to-local thermal
dissipation feature extraction. Additionally, a frequency-spatial hybrid loss
(FSL) is designed to effectively attenuate high-frequency thermal gradient
noise and spatial misalignments. The experimental results show that the
performance enhancements offered by our proposed method are substantial,
outperforming the newly-proposed 2.5D method, GCN+PNA, by considerable margins
(over 99% RMSE reduction, 4.23X inference time speedup). Moreover, extensive
experiments demonstrate that FSA-Heat also exhibits robust generalization
capabilities.

</details>


### [312] [A Pre-Training and Adaptive Fine-Tuning Framework for Graph Anomaly Detection](https://arxiv.org/abs/2504.14250)
*Yunhui Liu,Jiashun Cheng,Jia Li,Fugee Tsung,Hongzhi Yin,Tieke He*

Main category: cs.LG

TL;DR: 论文提出了一种名为PAF的框架，通过预训练和自适应微调来解决图异常检测（GAD）中的挑战，特别是在处理同质性和异质性混合的复杂模式时。


<details>
  <summary>Details</summary>
Motivation: 图异常检测因异常节点稀缺和标注成本高而具有挑战性，且现有方法在同质性和异质性混合的情况下表现不佳。

Method: PAF框架在预训练阶段联合使用低通和高通滤波器捕捉节点特征的完整频率信息，微调阶段通过门控融合网络自适应结合两种滤波器的表示。

Result: 在十个基准数据集上的实验表明PAF框架的有效性。

Conclusion: PAF通过选择性滤波和自适应融合，显著提升了图异常检测的性能。

Abstract: Graph anomaly detection (GAD) has garnered increasing attention in recent
years, yet it remains challenging due to the scarcity of abnormal nodes and the
high cost of label annotations. Graph pre-training, the two-stage learning
paradigm, has emerged as an effective approach for label-efficient learning,
largely benefiting from expressive neighborhood aggregation under the
assumption of strong homophily. However, in GAD, anomalies typically exhibit
high local heterophily, while normal nodes retain strong homophily, resulting
in a complex homophily-heterophily mixture. To understand the impact of this
mixed pattern on graph pre-training, we analyze it through the lens of spectral
filtering and reveal that relying solely on a global low-pass filter is
insufficient for GAD. We further provide a theoretical justification for the
necessity of selectively applying appropriate filters to individual nodes.
Building upon this insight, we propose PAF, a Pre-Training and Adaptive
Fine-tuning framework specifically designed for GAD. In particular, we
introduce joint training with low- and high-pass filters in the pre-training
phase to capture the full spectrum of frequency information in node features.
During fine-tuning, we devise a gated fusion network that adaptively combines
node representations generated by both filters. Extensive experiments across
ten benchmark datasets consistently demonstrate the effectiveness of PAF.

</details>


### [313] [Generative emulation of chaotic dynamics with coherent prior](https://arxiv.org/abs/2504.14264)
*Juan Nathaniel,Pierre Gentine*

Main category: cs.LG

TL;DR: 提出了一种名为Cohesion的高效生成框架，结合湍流原理和扩散建模，用于非线性动力学仿真，解决了长期预测中的物理不现实问题。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的非线性动力学仿真因长期技能衰减导致物理不现实输出，现有生成模型的质量依赖于条件先验的选择。

Method: 通过估计动力学的大尺度相干结构作为去噪过程的指导，利用降阶模型（如深度Koopman算子）高效生成先验序列，将预测重构为轨迹规划任务。

Result: 在复杂混沌系统（如Kolmogorov流、浅水方程和次季节至季节气候动力学）中，Cohesion表现出卓越的长期预测能力，能高效生成物理一致的仿真。

Conclusion: Cohesion框架通过结合相干先验和扩散建模，显著提升了长期动力学仿真的质量和效率。

Abstract: Data-driven emulation of nonlinear dynamics is challenging due to long-range
skill decay that often produces physically unrealistic outputs. Recent advances
in generative modeling aim to address these issues by providing uncertainty
quantification and correction. However, the quality of generated simulation
remains heavily dependent on the choice of conditioning priors. In this work,
we present an efficient generative framework for dynamics emulation, unifying
principles of turbulence with diffusion-based modeling: Cohesion. Specifically,
our method estimates large-scale coherent structure of the underlying dynamics
as guidance during the denoising process, where small-scale fluctuation in the
flow is then resolved. These coherent priors are efficiently approximated using
reduced-order models, such as deep Koopman operators, that allow for rapid
generation of long prior sequences while maintaining stability over extended
forecasting horizon. With this gain, we can reframe forecasting as trajectory
planning, a common task in reinforcement learning, where conditional denoising
is performed once over entire sequences, minimizing the computational cost of
autoregressive-based generative methods. Empirical evaluations on chaotic
systems of increasing complexity, including Kolmogorov flow, shallow water
equations, and subseasonal-to-seasonal climate dynamics, demonstrate Cohesion
superior long-range forecasting skill that can efficiently generate
physically-consistent simulations, even in the presence of partially-observed
guidance.

</details>


### [314] [Mixed-Precision Conjugate Gradient Solvers with RL-Driven Precision Tuning](https://arxiv.org/abs/2504.14268)
*Xinye Chen*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的框架，动态优化预处理共轭梯度法中的数值精度，通过Q学习自适应分配精度，平衡计算效率与数值精度。


<details>
  <summary>Details</summary>
Motivation: 解决预处理共轭梯度法中数值精度与计算效率的权衡问题，通过强化学习实现自适应优化。

Method: 将精度选择建模为马尔可夫决策过程，使用Q学习分配精度，同时通过双精度标量计算和残差计算确保稳定性。

Result: 算法在训练数据上表现良好，并能无缝适应新数据集，显著提升求解器性能。

Conclusion: 该方法首次将强化学习应用于混合精度数值方法，展示了其实际优势、鲁棒性和可扩展性，为科学计算中的AI驱动进步铺平了道路。

Abstract: This paper presents a novel reinforcement learning (RL) framework for
dynamically optimizing numerical precision in the preconditioned conjugate
gradient (CG) method. By modeling precision selection as a Markov Decision
Process (MDP), we employ Q-learning to adaptively assign precision levels to
key operations, striking an optimal balance between computational efficiency
and numerical accuracy, while ensuring stability through double-precision
scalar computations and residual computing. In practice, the algorithm is
trained on a set of data and subsequently performs inference for precision
selection on out-of-sample data, without requiring re-analysis or retraining
for new datasets. This enables the method to adapt seamlessly to new problem
instances without the computational overhead of recalibration. Our results
demonstrate the effectiveness of RL in enhancing solver's performance, marking
the first application of RL to mixed-precision numerical methods. The findings
highlight the approach's practical advantages, robustness, and scalability,
providing valuable insights into its integration with iterative solvers and
paving the way for AI-driven advancements in scientific computing.

</details>


### [315] [SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM](https://arxiv.org/abs/2504.14286)
*Xiaojiang Zhang,Jinghui Wang,Zifei Cheng,Wenhao Zhuang,Zheng Lin,Minglei Zhang,Shaojie Wang,Yinghan Cui,Chao Wang,Junyi Peng,Shimiao Jiang,Shiqi Kuang,Shouyu Yin,Chaohang Wen,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.LG

TL;DR: SRPO是一种基于强化学习的优化方法，无需监督微调即可超越DeepSeek-R1-Zero-32B在AIME24和LiveCodeBench上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在提升大语言模型推理能力方面潜力巨大，但跨领域复现这些进展仍因方法透明度不足而具有挑战性。

Method: SRPO基于GRPO，引入两阶段跨领域训练范式以平衡数学推理和编码能力，以及历史重采样技术处理无效样本。

Result: SRPO在相同基础模型下，仅依赖强化学习即超越了DeepSeek-R1-Zero-32B的性能。

Conclusion: SRPO为跨任务扩展大语言模型推理能力提供了有价值的见解。

Abstract: Recent advances of reasoning models, exemplified by OpenAI's o1 and
DeepSeek's R1, highlight the significant potential of Reinforcement Learning
(RL) to enhance the reasoning capabilities of Large Language Models (LLMs).
However, replicating these advancements across diverse domains remains
challenging due to limited methodological transparency. In this work, we
present two-Staged history-Resampling Policy Optimization (SRPO), which
successfully surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24
and LiveCodeBench benchmarks. SRPO achieves this using the same base model as
DeepSeek (i.e. Qwen2.5-32B) and relies solely on RL, without prior Supervised
Fine-Tuning (SFT). Building upon Group Relative Policy Optimization (GRPO), we
introduce two key methodological innovations: (1) a two-stage cross-domain
training paradigm designed to balance the development of mathematical reasoning
and coding proficiency, and (2) History Resampling (HR), a technique to address
ineffective samples. Our comprehensive experiments validate the effectiveness
of our approach, dedicating to offer valuable insights into scaling LLM
reasoning capabilities across diverse tasks.

</details>


### [316] [Learning and Generating Diverse Residential Load Patterns Using GAN with Weakly-Supervised Training and Weight Selection](https://arxiv.org/abs/2504.14300)
*Xinyu Liang,Hao Wang*

Main category: cs.LG

TL;DR: 论文提出了一种基于生成对抗网络（RLP-GAN）的合成住宅负荷模式生成模型，解决了现有方法在可扩展性、多样性和相似性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 高质量住宅负荷数据的稀缺阻碍了住宅领域的脱碳以及电网规划和运营的有效性，因此需要生成合成负荷数据。

Method: 采用弱监督的GAN框架，结合过完备自编码器捕捉复杂负荷模式的依赖关系，并引入模型权重选择方法解决模式崩溃问题。

Result: RLP-GAN在417户家庭的真实数据上验证，表现优于现有模型，能更好地捕捉时间依赖性和生成高相似性的负荷模式。

Conclusion: RLP-GAN生成的合成数据集（包含100万条负荷模式）已公开，为相关研究提供了支持。

Abstract: The scarcity of high-quality residential load data can pose obstacles for
decarbonizing the residential sector as well as effective grid planning and
operation. The above challenges have motivated research into generating
synthetic load data, but existing methods faced limitations in terms of
scalability, diversity, and similarity. This paper proposes a Generative
Adversarial Network-based Synthetic Residential Load Pattern (RLP-GAN)
generation model, a novel weakly-supervised GAN framework, leveraging an
over-complete autoencoder to capture dependencies within complex and diverse
load patterns and learn household-level data distribution at scale. We
incorporate a model weight selection method to address the mode collapse
problem and generate load patterns with high diversity. We develop a holistic
evaluation method to validate the effectiveness of RLP-GAN using real-world
data of 417 households. The results demonstrate that RLP-GAN outperforms
state-of-the-art models in capturing temporal dependencies and generating load
patterns with higher similarity to real data. Furthermore, we have publicly
released the RLP-GAN generated synthetic dataset, which comprises one million
synthetic residential load pattern profiles.

</details>


### [317] [Learning to Score](https://arxiv.org/abs/2504.14302)
*Yogev Kriger,Shai Fine*

Main category: cs.LG

TL;DR: 该论文研究了一种在目标标签不可用但存在相关辅助信息（Side Information）的场景下的机器学习方法，提出了一种结合表示学习、辅助信息和度量学习的评分模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决目标标签缺失但存在相关辅助信息的问题，例如在医疗领域中疾病症状已知但疾病进展标准不明确的情况。

Method: 方法是将问题建模为表示学习、辅助信息和度量学习的组合，提出了一种评分模型。

Result: 通过在基准数据集和生物医学患者记录上的实验，验证了该评分系统的实用性。

Conclusion: 结论是该评分模型在目标标签不可用但存在辅助信息的情况下具有广泛的应用潜力。

Abstract: Common machine learning settings range from supervised tasks, where
accurately labeled data is accessible, through semi-supervised and
weakly-supervised tasks, where target labels are scant or noisy, to
unsupervised tasks where labels are unobtainable. In this paper we study a
scenario where the target labels are not available but additional related
information is at hand. This information, referred to as Side Information, is
either correlated with the unknown labels or imposes constraints on the feature
space. We formulate the problem as an ensemble of three semantic components:
representation learning, side information and metric learning. The proposed
scoring model is advantageous for multiple use-cases. For example, in the
healthcare domain it can be used to create a severity score for diseases where
the symptoms are known but the criteria for the disease progression are not
well defined. We demonstrate the utility of the suggested scoring system on
well-known benchmark data-sets and bio-medical patient records.

</details>


### [318] [Learning from Stochastic Teacher Representations Using Student-Guided Knowledge Distillation](https://arxiv.org/abs/2504.14307)
*Muhammad Haseeb Aslam,Clara Martinez,Marco Pedersoli,Alessandro Koerich,Ali Etemad,Eric Granger*

Main category: cs.LG

TL;DR: 提出了一种新的随机自蒸馏（SSD）方法，通过蒸馏时dropout生成多样化的教师表示，并利用学生引导的知识蒸馏（SGKD）过滤和加权任务相关表示，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统集成学习和权重平均方法需要训练多个模型的问题，以及在资源受限或延迟敏感应用中的不实用性。

Method: 使用蒸馏时dropout生成多样化教师表示，并通过SGKD过滤和加权任务相关表示。

Result: 在多个数据集上，SSD方法在不增加模型大小和计算复杂度的情况下，优于现有方法。

Conclusion: SSD方法在资源受限场景下提供了一种高效且性能优越的解决方案。

Abstract: Advances in self-distillation have shown that when knowledge is distilled
from a teacher to a student using the same deep learning (DL) architecture, the
student performance can surpass the teacher particularly when the network is
overparameterized and the teacher is trained with early stopping.
Alternatively, ensemble learning also improves performance, although training,
storing, and deploying multiple models becomes impractical as the number of
models grows. Even distilling an ensemble to a single student model or weight
averaging methods first requires training of multiple teacher models and does
not fully leverage the inherent stochasticity for generating and distilling
diversity in DL models. These constraints are particularly prohibitive in
resource-constrained or latency-sensitive applications such as wearable
devices. This paper proposes to train only one model and generate multiple
diverse teacher representations using distillation-time dropout. However,
generating these representations stochastically leads to noisy representations
that are misaligned with the learned task. To overcome this problem, a novel
stochastic self-distillation (SSD) training strategy is introduced for
filtering and weighting teacher representation to distill from task-relevant
representations only, using student-guided knowledge distillation (SGKD). The
student representation at each distillation step is used as authority to guide
the distillation process. Experimental results on real-world affective
computing, wearable/biosignal datasets from the UCR Archive, the HAR dataset,
and image classification datasets show that the proposed SSD method can
outperform state-of-the-art methods without increasing the model size at both
training and testing time, and incurs negligible computational complexity
compared to state-of-the-art ensemble learning and weight averaging methods.

</details>


### [319] [Local distribution-based adaptive oversampling for imbalanced regression](https://arxiv.org/abs/2504.14316)
*Shayan Alahyari,Mike Domaratzki*

Main category: cs.LG

TL;DR: LDAO是一种针对不平衡回归问题的新方法，通过局部分布自适应过采样，解决了现有方法依赖阈值和忽略连续性的问题。


<details>
  <summary>Details</summary>
Motivation: 不平衡回归问题在连续目标变量分布不均时难以预测，现有方法依赖阈值且效果有限。

Method: LDAO将数据集分解为局部分布，独立建模和采样后合并为平衡训练集。

Result: 在45个数据集上，LDAO优于现有方法，尤其对罕见目标值表现更佳。

Conclusion: LDAO有效解决了不平衡回归问题，保留了局部统计特性。

Abstract: Imbalanced regression occurs when continuous target variables have skewed
distributions, creating sparse regions that are difficult for machine learning
models to predict accurately. This issue particularly affects neural networks,
which often struggle with imbalanced data. While class imbalance in
classification has been extensively studied, imbalanced regression remains
relatively unexplored, with few effective solutions. Existing approaches often
rely on arbitrary thresholds to categorize samples as rare or frequent,
ignoring the continuous nature of target distributions. These methods can
produce synthetic samples that fail to improve model performance and may
discard valuable information through undersampling. To address these
limitations, we propose LDAO (Local Distribution-based Adaptive Oversampling),
a novel data-level approach that avoids categorizing individual samples as rare
or frequent. Instead, LDAO learns the global distribution structure by
decomposing the dataset into a mixture of local distributions, each preserving
its statistical characteristics. LDAO then models and samples from each local
distribution independently before merging them into a balanced training set.
LDAO achieves a balanced representation across the entire target range while
preserving the inherent statistical structure within each local distribution.
In extensive evaluations on 45 imbalanced datasets, LDAO outperforms
state-of-the-art oversampling methods on both frequent and rare target values,
demonstrating its effectiveness for addressing the challenge of imbalanced
regression.

</details>


### [320] [Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction](https://arxiv.org/abs/2504.14361)
*Till Rossner,Ziteng Li,Jonas Balke,Nikoo Salehfard,Tom Seifert,Ming Tang*

Main category: cs.LG

TL;DR: 提出了一种结合scGPT和DeepCDR的创新方法，用于预测癌症药物反应（CDR），实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提高癌症药物反应预测的准确性，探索scGPT在基因表达数据中的应用潜力。

Method: 利用scGPT生成基因表达数据的嵌入表示，并将其作为DeepCDR的输入数据。

Result: scGPT方法在CDR预测中表现优于原DeepCDR模型和scFoundation模型。

Conclusion: scGPT嵌入能显著提升CDR预测精度，为现有方法提供了有前景的替代方案。

Abstract: In this study, we propose an innovative methodology for predicting Cancer
Drug Response (CDR) through the integration of the scGPT foundation model
within the DeepCDR model. Our approach utilizes scGPT to generate embeddings
from gene expression data, which are then used as gene expression input data
for DeepCDR. The experimental findings demonstrate the efficacy of this
scGPT-based method in outperforming previous related works, including the
original DeepCDR model and the scFoundation-based model. This study highlights
the potential of scGPT embeddings to enhance the accuracy of CDR predictions
and offers a promising alternative to existing approaches.

</details>


### [321] [Improving RL Exploration for LLM Reasoning through Retrospective Replay](https://arxiv.org/abs/2504.14363)
*Shihan Dou,Muling Wu,Jingwen Xu,Rui Zheng,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.LG

TL;DR: 论文提出了一种名为RRL的新算法，通过动态重放机制提升强化学习在大型语言模型训练中的探索效率。


<details>
  <summary>Details</summary>
Motivation: 在复杂问题中，模型早期探索能力强但能力有限，容易丢失有价值的解决方案思路，导致后期训练效果不佳。

Method: 提出Retrospective Replay-based Reinforcement Learning (RRL)，动态重放早期有潜力的状态以提升探索效率。

Result: 实验表明RRL在复杂推理任务（如数学推理和代码生成）和对话任务中显著提升了RL的效果，同时优化了RLHF的性能。

Conclusion: RRL通过动态重放机制有效解决了探索问题，提升了模型在复杂任务中的表现和安全性。

Abstract: Reinforcement learning (RL) has increasingly become a pivotal technique in
the post-training of large language models (LLMs). The effective exploration of
the output space is essential for the success of RL. We observe that for
complex problems, during the early stages of training, the model exhibits
strong exploratory capabilities and can identify promising solution ideas.
However, its limited capability at this stage prevents it from successfully
solving these problems. The early suppression of these potentially valuable
solution ideas by the policy gradient hinders the model's ability to revisit
and re-explore these ideas later. Consequently, although the LLM's capabilities
improve in the later stages of training, it still struggles to effectively
address these complex problems. To address this exploration issue, we propose a
novel algorithm named Retrospective Replay-based Reinforcement Learning (RRL),
which introduces a dynamic replay mechanism throughout the training process.
RRL enables the model to revisit promising states identified in the early
stages, thereby improving its efficiency and effectiveness in exploration. To
evaluate the effectiveness of RRL, we conduct extensive experiments on complex
reasoning tasks, including mathematical reasoning and code generation, and
general dialogue tasks. The results indicate that RRL maintains high
exploration efficiency throughout the training period, significantly enhancing
the effectiveness of RL in optimizing LLMs for complicated reasoning tasks.
Moreover, it also improves the performance of RLHF, making the model both safer
and more helpful.

</details>


### [322] [Accelerating LLM Inference with Flexible N:M Sparsity via A Fully Digital Compute-in-Memory Accelerator](https://arxiv.org/abs/2504.14365)
*Akshat Ramachandran,Souvik Kundu,Arnab Raha,Shamik Kundu,Deepak K. Mathaikutty,Tushar Krishna*

Main category: cs.LG

TL;DR: 论文提出了一种灵活的层间N:M稀疏选择方法（FLOW）和低开销的数字内存计算架构（FlexCiM），以解决大型语言模型（LLM）修剪中的性能限制和硬件开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有固定N:M稀疏方法限制了稀疏模型的表达能力，而支持多种N:M模式又增加了硬件开销。

Method: FLOW通过考虑异常值的分布，动态选择最优的层间N:M稀疏模式；FlexCiM通过分区和自适应聚合机制支持多样化的稀疏模式。

Result: 实验表明，FLOW在性能上优于现有方法，准确率提升高达36%；FlexCiM的推理延迟降低1.75倍，能耗减少1.5倍。

Conclusion: FLOW和FlexCiM结合解决了稀疏模型的性能与硬件开销问题，为LLM的高效部署提供了新方案。

Abstract: Large language model (LLM) pruning with fixed N:M structured sparsity
significantly limits the expressivity of the sparse model, yielding sub-optimal
performance. In contrast, supporting multiple N:M patterns to provide sparse
representational freedom introduces costly overhead in hardware. To address
these challenges for LLMs, we first present a flexible layer-wise
outlier-density-aware N:M sparsity (FLOW) selection method. FLOW enables the
identification of optimal layer-wise N and M values (from a given range) by
simultaneously accounting for the presence and distribution of outliers,
allowing a higher degree of representational freedom. To deploy sparse models
with such N:M flexibility, we then introduce a flexible, low-overhead digital
compute-in-memory architecture (FlexCiM). FlexCiM supports diverse sparsity
patterns by partitioning a digital CiM (DCiM) macro into smaller sub-macros,
which are adaptively aggregated and disaggregated through distribution and
merging mechanisms for different N and M values. Extensive experiments on both
transformer-based and recurrence-based state space foundation models (SSMs)
demonstrate that FLOW outperforms existing alternatives with an accuracy
improvement of up to 36%, while FlexCiM achieves up to 1.75x lower inference
latency and 1.5x lower energy consumption compared to existing sparse
accelerators. Code is available at: https://github.com/FLOW-open-project/FLOW

</details>


### [323] [Do You Really Need Public Data? Surrogate Public Data for Differential Privacy on Tabular Data](https://arxiv.org/abs/2504.14368)
*Shlomi Hod,Lucas Rosenblatt,Julia Stoyanovich*

Main category: cs.LG

TL;DR: 提出利用强大的先验知识生成替代公共数据，以解决差分隐私机器学习中缺乏公共表格数据的问题，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在差分隐私机器学习中，公共数据常用于隐私-效用权衡估计、超参数调优和预训练，但表格数据因领域异质性难以获取。

Method: 提出两种方法：1) 直接生成CSV文件记录；2) 自动构建结构因果模型（SCM）生成记录。利用大型语言模型（LLMs）自动化生成替代公共数据。

Result: 实验表明，替代公共表格数据可有效用于差分隐私分类器的预训练，并在超参数调优和隐私-效用权衡估计中也有一定作用。

Conclusion: 替代公共数据为缺乏公共表格数据的场景提供了可行的解决方案，扩展了差分隐私机器学习的应用范围。

Abstract: Differentially private (DP) machine learning often relies on the availability
of public data for tasks like privacy-utility trade-off estimation,
hyperparameter tuning, and pretraining. While public data assumptions may be
reasonable in text and image domains, they are less likely to hold for tabular
data due to tabular data heterogeneity across domains. We propose leveraging
powerful priors to address this limitation; specifically, we synthesize
realistic tabular data directly from schema-level specifications - such as
variable names, types, and permissible ranges - without ever accessing
sensitive records. To that end, this work introduces the notion of "surrogate"
public data - datasets generated independently of sensitive data, which consume
no privacy loss budget and are constructed solely from publicly available
schema or metadata. Surrogate public data are intended to encode plausible
statistical assumptions (informed by publicly available information) into a
dataset with many downstream uses in private mechanisms. We automate the
process of generating surrogate public data with large language models (LLMs);
in particular, we propose two methods: direct record generation as CSV files,
and automated structural causal model (SCM) construction for sampling records.
Through extensive experiments, we demonstrate that surrogate public tabular
data can effectively replace traditional public data when pretraining
differentially private tabular classifiers. To a lesser extent, surrogate
public data are also useful for hyperparameter tuning of DP synthetic data
generators, and for estimating the privacy-utility tradeoff.

</details>


### [324] [Learning Enhanced Structural Representations with Block-Based Uncertainties for Ocean Floor Mapping](https://arxiv.org/abs/2504.14372)
*Jose Marie Antonio Minoza*

Main category: cs.LG

TL;DR: 提出了一种基于空间块和VQ-VAE架构的不确定性感知机制，用于高分辨率海底地形重建，显著提升了重建质量和不确定性估计的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前全球海底地形数据分辨率不足，难以支持精确的数值模拟，而现有深度学习方法在保持物理结构一致性和量化不确定性方面存在挑战。

Method: 采用基于块的一致性预测和VQ-VAE架构，通过离散潜在表示保留地形特征，同时提供空间自适应的不确定性估计。

Result: 实验结果表明，该方法在多个海域显著提高了重建质量和不确定性估计的可靠性。

Conclusion: 该框架通过保持结构完整性和提供空间自适应不确定性估计，为气候建模和海岸灾害评估提供了更可靠的基础。

Abstract: Accurate ocean modeling and coastal hazard prediction depend on
high-resolution bathymetric data; yet, current worldwide datasets are too
coarse for exact numerical simulations. While recent deep learning advances
have improved earth observation data resolution, existing methods struggle with
the unique challenges of producing detailed ocean floor maps, especially in
maintaining physical structure consistency and quantifying uncertainties. This
work presents a novel uncertainty-aware mechanism using spatial blocks to
efficiently capture local bathymetric complexity based on block-based conformal
prediction. Using the Vector Quantized Variational Autoencoder (VQ-VAE)
architecture, the integration of this uncertainty quantification framework
yields spatially adaptive confidence estimates while preserving topographical
features via discrete latent representations. With smaller uncertainty widths
in well-characterized areas and appropriately larger bounds in areas of complex
seafloor structures, the block-based design adapts uncertainty estimates to
local bathymetric complexity. Compared to conventional techniques, experimental
results over several ocean regions show notable increases in both
reconstruction quality and uncertainty estimation reliability. This framework
increases the reliability of bathymetric reconstructions by preserving
structural integrity while offering spatially adaptive uncertainty estimates,
so opening the path for more solid climate modeling and coastal hazard
assessment.

</details>


### [325] [Bottom-Up Synthesis of Knowledge-Grounded Task-Oriented Dialogues with Iteratively Self-Refined Prompts](https://arxiv.org/abs/2504.14375)
*Kun Qian,Maximillian Chen,Siyan Li,Arpit Sharma,Zhou Yu*

Main category: cs.LG

TL;DR: 论文提出了一种自下而上的对话合成方法，通过先生成问答对再组合成对话，相比传统自上而下的方法，提供了更高的控制性和精确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法生成对话时缺乏细粒度控制且易产生幻觉，需要一种更可控的方法来生成高质量的对话数据。

Method: 采用自下而上的方法，先生成问答对，再组合成连贯对话，分步处理以提高控制性和质量。

Result: 实验表明，该方法生成的对话比传统方法更真实、质量更高。

Conclusion: 自下而上的对话合成方法在控制性和生成质量上优于传统方法。

Abstract: Training conversational question-answering (QA) systems requires a
substantial amount of in-domain data, which is often scarce in practice. A
common solution to this challenge is to generate synthetic data. Traditional
methods typically follow a top-down approach, where a large language model
(LLM) generates multi-turn dialogues from a broad prompt. Although this method
produces coherent conversations, it offers limited fine-grained control over
the content and is susceptible to hallucinations. We introduce a bottom-up
conversation synthesis approach, where QA pairs are generated first and then
combined into a coherent dialogue. This method offers greater control and
precision by dividing the process into two distinct steps, allowing refined
instructions and validations to be handled separately. Additionally, this
structure allows the use of non-local models in stages that do not involve
proprietary knowledge, enhancing the overall quality of the generated data.
Both human and automated evaluations demonstrate that our approach produces
more realistic and higher-quality dialogues compared to top-down methods.

</details>


### [326] [Balancing Fairness and Performance in Healthcare AI: A Gradient Reconciliation Approach](https://arxiv.org/abs/2504.14388)
*Xiaoyang Wang,Christopher C. Yang*

Main category: cs.LG

TL;DR: FairGrad是一种梯度协调框架，用于在医疗AI模型中平衡预测性能和多属性公平性，避免加剧医疗不平等。


<details>
  <summary>Details</summary>
Motivation: 医疗AI的快速发展可能加剧现有医疗不平等，需要一种方法在保持预测性能的同时优化公平性。

Method: FairGrad通过将梯度向量投影到正交平面，协调冲突的优化目标，确保公平性。

Result: 在真实医疗数据集上，FairGrad显著提升了多属性公平性指标，同时保持了预测准确性。

Conclusion: FairGrad证明了在关键医疗AI应用中协调公平性和实用性的可行性。

Abstract: The rapid growth of healthcare data and advances in computational power have
accelerated the adoption of artificial intelligence (AI) in medicine. However,
AI systems deployed without explicit fairness considerations risk exacerbating
existing healthcare disparities, potentially leading to inequitable resource
allocation and diagnostic disparities across demographic subgroups. To address
this challenge, we propose FairGrad, a novel gradient reconciliation framework
that automatically balances predictive performance and multi-attribute fairness
optimization in healthcare AI models. Our method resolves conflicting
optimization objectives by projecting each gradient vector onto the orthogonal
plane of the others, thereby regularizing the optimization trajectory to ensure
equitable consideration of all objectives. Evaluated on diverse real-world
healthcare datasets and predictive tasks - including Substance Use Disorder
(SUD) treatment and sepsis mortality - FairGrad achieved statistically
significant improvements in multi-attribute fairness metrics (e.g., equalized
odds) while maintaining competitive predictive accuracy. These results
demonstrate the viability of harmonizing fairness and utility in
mission-critical medical AI applications.

</details>


### [327] [Exploring Pseudo-Token Approaches in Transformer Neural Processes](https://arxiv.org/abs/2504.14416)
*Jose Lara-Rangel,Nanze Chen,Fengzhe Zhang*

Main category: cs.LG

TL;DR: ISANPs通过引入诱导集注意力和高效查询机制，解决了TNPs的计算复杂度问题，同时保持了高性能。


<details>
  <summary>Details</summary>
Motivation: 传统NPs易欠拟合，而TNPs虽性能优越但计算复杂度高，限制了实际应用。

Method: 提出ISANPs，采用诱导集注意力和创新查询机制，优化计算效率。

Result: ISANPs在多项任务中表现优异，性能与TNPs相当甚至超越，同时计算复杂度可控。

Conclusion: ISANPs在性能和计算效率间取得了平衡，适用于更大规模数据集。

Abstract: Neural Processes (NPs) have gained attention in meta-learning for their
ability to quantify uncertainty, together with their rapid prediction and
adaptability. However, traditional NPs are prone to underfitting. Transformer
Neural Processes (TNPs) significantly outperform existing NPs, yet their
applicability in real-world scenarios is hindered by their quadratic
computational complexity relative to both context and target data points. To
address this, pseudo-token-based TNPs (PT-TNPs) have emerged as a novel NPs
subset that condense context data into latent vectors or pseudo-tokens,
reducing computational demands. We introduce the Induced Set Attentive Neural
Processes (ISANPs), employing Induced Set Attention and an innovative query
phase to improve querying efficiency. Our evaluations show that ISANPs perform
competitively with TNPs and often surpass state-of-the-art models in 1D
regression, image completion, contextual bandits, and Bayesian optimization.
Crucially, ISANPs offer a tunable balance between performance and computational
complexity, which scale well to larger datasets where TNPs face limitations.

</details>


### [328] [LoRe: Personalizing LLMs via Low-Rank Reward Modeling](https://arxiv.org/abs/2504.14439)
*Avinandan Bose,Zhihan Xiong,Yuejie Chi,Simon Shaolei Du,Lin Xiao,Maryam Fazel*

Main category: cs.LG

TL;DR: 提出了一种基于低秩偏好建模的新框架，用于个性化大型语言模型，提升用户满意度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF方法依赖单一价值表示，难以适应多样化用户偏好。

Method: 利用低维子空间表示奖励函数，通过共享基函数的加权组合建模个体偏好。

Result: 在多个偏好数据集上验证了方法，表现出对未见用户的优秀泛化能力和偏好预测准确性。

Conclusion: 该方法避免了僵化的用户分类，同时实现了可扩展性和少样本适应。

Abstract: Personalizing large language models (LLMs) to accommodate diverse user
preferences is essential for enhancing alignment and user satisfaction.
Traditional reinforcement learning from human feedback (RLHF) approaches often
rely on monolithic value representations, limiting their ability to adapt to
individual preferences. We introduce a novel framework that leverages low-rank
preference modeling to efficiently learn and generalize user-specific reward
functions. By representing reward functions in a low-dimensional subspace and
modeling individual preferences as weighted combinations of shared basis
functions, our approach avoids rigid user categorization while enabling
scalability and few-shot adaptation. We validate our method on multiple
preference datasets, demonstrating superior generalization to unseen users and
improved accuracy in preference prediction tasks.

</details>


### [329] [A computational framework for longitudinal medication adherence prediction in breast cancer survivors: A social cognitive theory based approach](https://arxiv.org/abs/2504.14469)
*Navreet Kaur,Manuel Gonzales IV,Cristian Garcia Alcaraz,Jiaqi Gong,Kristen J. Wells,Laura E. Barnes*

Main category: cs.LG

TL;DR: 论文提出了一种基于社会认知理论的多尺度模型，用于预测乳腺癌幸存者的药物依从性，动态和静态因素结合显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 慢性病患者药物依从性低导致严重后果，乳腺癌幸存者的内分泌治疗依从性与生存率显著相关，需多尺度模型理解影响因素。

Method: 采用社会认知理论指导的计算框架，结合动态（近期服药模式）和静态（较少变化的因素）因素进行多尺度（每日和每周）建模。

Result: 模型在每日和每周任务中均优于传统机器学习方法，每日模型准确率87.25%，每周模型76.04%。动态因素对每日预测最重要，动态与静态因素结合对每周预测更有效。

Conclusion: 多尺度模型能有效预测药物依从性，动态和静态因素的结合在不同时间尺度上具有显著意义。

Abstract: Non-adherence to medications is a critical concern since nearly half of
patients with chronic illnesses do not follow their prescribed medication
regimens, leading to increased mortality, costs, and preventable human
distress. Amongst stage 0-3 breast cancer survivors, adherence to long-term
adjuvant endocrine therapy (i.e., Tamoxifen and aromatase inhibitors) is
associated with a significant increase in recurrence-free survival. This work
aims to develop multi-scale models of medication adherence to understand the
significance of different factors influencing adherence across varying time
frames. We introduce a computational framework guided by Social Cognitive
Theory for multi-scale (daily and weekly) modeling of longitudinal medication
adherence. Our models employ both dynamic medication-taking patterns in the
recent past (dynamic factors) as well as less frequently changing factors
(static factors) for adherence prediction. Additionally, we assess the
significance of various factors in influencing adherence behavior across
different time scales. Our models outperform traditional machine learning
counterparts in both daily and weekly tasks in terms of both accuracy and
specificity. Daily models achieved an accuracy of 87.25%, and weekly models, an
accuracy of 76.04%. Notably, dynamic past medication-taking patterns prove most
valuable for predicting daily adherence, while a combination of dynamic and
static factors is significant for macro-level weekly adherence patterns.

</details>


### [330] [Less is More: Adaptive Coverage for Synthetic Training Data](https://arxiv.org/abs/2504.14508)
*Sasan Tavakkol,Max Springer,Mohammadhossein Bateni,Neslihan Bulut,Vincent Cohen-Addad,MohammadTaghi Hajiaghayi*

Main category: cs.LG

TL;DR: 使用LLM生成合成训练数据，并通过最大覆盖问题选择代表性子集，训练分类器性能优于全数据集。


<details>
  <summary>Details</summary>
Motivation: 解决快速部署模型时获取大规模标注数据的挑战，特别是在新兴社交媒体趋势或在线滥用分类中。

Method: 提出基于最大覆盖问题的新采样算法，从合成数据中选择代表性子集。

Result: 训练分类器在子集上表现优于全数据集，提高准确性并减少数据需求。

Conclusion: “少即是多”方法提升效率，适用于快速模型微调。

Abstract: Synthetic training data generation with Large Language Models (LLMs) like
Google's Gemma and OpenAI's GPT offer a promising solution to the challenge of
obtaining large, labeled datasets for training classifiers. When rapid model
deployment is critical, such as in classifying emerging social media trends or
combating new forms of online abuse tied to current events, the ability to
generate training data is invaluable. While prior research has examined the
comparability of synthetic data to human-labeled data, this study introduces a
novel sampling algorithm, based on the maximum coverage problem, to select a
representative subset from a synthetically generated dataset. Our results
demonstrate that training a classifier on this contextually sampled subset
achieves superior performance compared to training on the entire dataset. This
"less is more" approach not only improves accuracy but also reduces the volume
of data required, leading to potentially more efficient model fine-tuning.

</details>


### [331] [On Dimension-Free Transformer: An Application of STP to AI](https://arxiv.org/abs/2504.14514)
*Daizhan Cheng*

Main category: cs.LG

TL;DR: 论文提出了基于半张量积的投影超向量变换方法，构建了维度无关的Transformer框架（DFT），支持任意维度的输入输出，并提高了信号处理效率。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的线性变换限制了输入输出的维度灵活性，需要一种更高效且不受维度限制的方法。

Method: 利用半张量积重新定义超向量，并通过投影构建线性变换，验证并替换Transformer中的线性变换为投影超向量变换（PBTH）。

Result: 提出的DFT框架支持任意维度的输入输出，且在信号处理中更高效。

Conclusion: DFT通过PBTH实现了维度无关的Transformer，提升了信号处理的效率和灵活性。

Abstract: The matrix expressions for every parts of a transformer are firstly
described. Based on semi-tensor product (STP) of matrices the hypervectors are
reconsidered and the linear transformation over hypervectors is constructed by
using projection. Its properties and calculating formulas are obtained. Using
projection-based transformation of hypervector (PBTH), the framework of
dimension-free transformer (DFT) is proposed by verifying each linear
transformation in a transformer and replacing it by a proper PBTH, which allows
the inputs and outputs being of arbitrary dimensions. Using balanced
information about all entries, DFT must be more efficient in dealing with
signals.

</details>


### [332] [SlimPipe: Memory-Thrifty and Efficient Pipeline Parallelism for Long-Context LLM Training](https://arxiv.org/abs/2504.14519)
*Zhouyang Li,Yuliang Liu,Wei Zhang,Tailing Yuan,Bin Chen,Chengru Song,Di Zhang*

Main category: cs.LG

TL;DR: SlimPipe是一种细粒度流水线并行方法，通过均匀序列切片和1F1B调度，显著减少激活内存压力和流水线气泡，提升大语言模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有流水线并行方法在长上下文场景中无法有效解决激活内存压力和流水线气泡问题，限制了训练效率。

Method: 采用均匀序列切片和1F1B调度，结合负载均衡技术，减少激活内存消耗和流水线气泡。

Result: 在Llama 70B模型上，SlimPipe将MFU提升至1.57倍，并在2048K上下文长度下保持45%以上的利用率。

Conclusion: SlimPipe在减少内存开销和提升训练效率方面表现出色，适用于大规模语言模型训练。

Abstract: Pipeline Parallelism (PP) serves as a crucial technique for training Large
Language Models (LLMs), owing to its capability to alleviate memory pressure
from model states with relatively low communication overhead. However, in
long-context scenarios, existing pipeline parallelism methods fail to address
the substantial activation memory pressure, primarily due to the peak memory
consumption resulting from the accumulation of activations across multiple
microbatches. Moreover, these approaches inevitably introduce considerable
pipeline bubbles, further hindering efficiency.
  To tackle these challenges, we propose SlimPipe, a novel approach to
fine-grained pipeline parallelism that employs uniform sequence slicing coupled
with one-forward-one-backward (1F1B) schedule. It reduces the accumulated
activations from several microbatches to just one, which is split into several
slices. Although the slices are evenly partitioned, the computation cost is not
equal across slices due to causal attention. We develop a sophisticated
workload redistribution technique to address this load imbalance. SlimPipe
achieves (1) near-zero memory overhead and (2) minimal pipeline bubbles
simultaneously. The effectiveness of SlimPipe has been proven by thorough
testing with diverse model architectures, context window sizes, and
SlimPipe-specific configurations. For example, on the Llama 70B model, compared
to state-of-the-art methods, SlimPipe significantly boosts the Model FLOPs
Utilization (MFU) to up to $1.57\times$ for a context length of 512K. More
notably, for a context length of 2048K, it maintains over 45% utilization on
256 NVIDIA Hopper 80GB GPUs, while other approaches either suffer significant
performance drops or fail entirely due to memory constraints.

</details>


### [333] [TrustLoRA: Low-Rank Adaptation for Failure Detection under Out-of-distribution Data](https://arxiv.org/abs/2504.14545)
*Fei Zhu,Zhaoxiang Zhang*

Main category: cs.LG

TL;DR: 提出了一种简单的失败检测框架，统一并优化了在协变量和语义偏移下的分类拒绝能力。


<details>
  <summary>Details</summary>
Motivation: 在开放环境中，模型需要可靠预测并拒绝协变量和语义偏移的输入，同时需要更灵活的错误检测方法。

Method: 通过分离和整合失败特定的可靠性知识，利用低秩适配器增强检测能力。

Result: 实验证明了该框架的优越性。

Conclusion: 该框架有效提升了模型在开放环境中的失败检测能力。

Abstract: Reliable prediction is an essential requirement for deep neural models that
are deployed in open environments, where both covariate and semantic
out-of-distribution (OOD) data arise naturally. In practice, to make safe
decisions, a reliable model should accept correctly recognized inputs while
rejecting both those misclassified covariate-shifted and semantic-shifted
examples. Besides, considering the potential existing trade-off between
rejecting different failure cases, more convenient, controllable, and flexible
failure detection approaches are needed. To meet the above requirements, we
propose a simple failure detection framework to unify and facilitate
classification with rejection under both covariate and semantic shifts. Our key
insight is that by separating and consolidating failure-specific reliability
knowledge with low-rank adapters and then integrating them, we can enhance the
failure detection ability effectively and flexibly. Extensive experiments
demonstrate the superiority of our framework.

</details>


### [334] [NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models](https://arxiv.org/abs/2504.14569)
*Lawrence Liu,Inesh Chakrabarti,Yixiao Li,Mengdi Wang,Tuo Zhao,Lin F. Yang*

Main category: cs.LG

TL;DR: NoWag提出了一种零样本形状保持压缩框架，用于减少大型语言模型的计算和内存需求，支持向量量化和剪枝两种压缩方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在资源受限环境中部署时面临计算和内存需求过高的问题。

Method: 提出NoWag框架，支持向量量化（NoWag-VQ）和剪枝（NoWag-P）两种压缩方法。

Result: NoWag-VQ在零样本向量量化中表现优异，NoWag-P与现有方法竞争性相当。

Conclusion: NoWag框架展示了压缩范式的共性，为未来研究提供了启发。

Abstract: Large language models (LLMs) exhibit remarkable performance across various
natural language processing tasks but suffer from immense computational and
memory demands, limiting their deployment in resource-constrained environments.
To address this challenge, we propose NoWag: (Normalized Weight and Activation
Guided Compression), a unified framework for zero-shot shape preserving
compression algorithms. We compressed Llama-2 7B/13B/70B and Llama-3 8/70BB
models, using two popular forms of shape-preserving compression, vector
quantization NoWag-VQ (NoWag for Vector Quantization), and
unstructured/semi-structured pruning NoWag-P (NoWag for Pruning). We found that
NoWag-VQ significantly outperforms state-of-the-art zero shot VQ, and that
NoWag-P performs competitively against state-of-the-art methods. These results
suggest commonalities between these compression paradigms that could inspire
future work. Our code is available at https://github.com/LawrenceRLiu/NoWag

</details>


### [335] [Data Selection for ERMs](https://arxiv.org/abs/2504.14572)
*Steve Hanneke,Shay Moran,Alexander Shlimovich,Amir Yehudayoff*

Main category: cs.LG

TL;DR: 论文探讨了从数据为中心的角度优化训练数据，而非传统模型为中心的方法，研究了在有限数据选择预算下如何通过选择少量数据点达到与全数据集训练相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统学习理论侧重于设计固定任务的优化算法，本文提出从数据角度出发，优化训练数据以提高学习规则的性能。

Method: 研究了多种经验风险最小化方法，包括均值估计、线性分类和线性回归，并建立了二元分类和随机凸优化的误差率分类。

Result: 得出了最优数据选择界限，并提出了二元分类和随机凸优化中的一般性结果。

Conclusion: 研究为数据选择提供了理论支持，并提出了未来研究方向。

Abstract: Learning theory has traditionally followed a model-centric approach, focusing
on designing optimal algorithms for a fixed natural learning task (e.g., linear
classification or regression). In this paper, we adopt a complementary
data-centric perspective, whereby we fix a natural learning rule and focus on
optimizing the training data. Specifically, we study the following question:
given a learning rule $\mathcal{A}$ and a data selection budget $n$, how well
can $\mathcal{A}$ perform when trained on at most $n$ data points selected from
a population of $N$ points? We investigate when it is possible to select $n \ll
N$ points and achieve performance comparable to training on the entire
population.
  We address this question across a variety of empirical risk minimizers. Our
results include optimal data-selection bounds for mean estimation, linear
classification, and linear regression. Additionally, we establish two general
results: a taxonomy of error rates in binary classification and in stochastic
convex optimization. Finally, we propose several open questions and directions
for future research.

</details>


### [336] [Generative Auto-Bidding with Value-Guided Explorations](https://arxiv.org/abs/2504.14587)
*Jingtong Gao,Yewen Li,Shuai Mao,Peng Jiang,Nan Jiang,Yejing Wang,Qingpeng Cai,Fei Pan,Peng Jiang,Kun Gai,Bo An,Xiangyu Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种名为GAVE的新型离线生成自动竞价框架，通过价值引导探索解决现有方法的局限性，并在实验和实际部署中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有自动竞价方法（如基于规则或强化学习）在适应动态市场、捕捉历史依赖性和多样化广告目标方面存在不足，且离线训练可能导致行为模式固定和崩溃。

Method: GAVE框架结合了基于分数的RTG模块、动作探索机制和可学习价值函数，以探索新动作并确保稳定更新。

Result: 在两个离线数据集和实际部署中，GAVE在离线和在线测试中均优于现有基线方法。

Conclusion: GAVE通过创新设计解决了自动竞价的多个关键问题，为未来研究提供了可复现的代码支持。

Abstract: Auto-bidding, with its strong capability to optimize bidding decisions within
dynamic and competitive online environments, has become a pivotal strategy for
advertising platforms. Existing approaches typically employ rule-based
strategies or Reinforcement Learning (RL) techniques. However, rule-based
strategies lack the flexibility to adapt to time-varying market conditions, and
RL-based methods struggle to capture essential historical dependencies and
observations within Markov Decision Process (MDP) frameworks. Furthermore,
these approaches often face challenges in ensuring strategy adaptability across
diverse advertising objectives. Additionally, as offline training methods are
increasingly adopted to facilitate the deployment and maintenance of stable
online strategies, the issues of documented behavioral patterns and behavioral
collapse resulting from training on fixed offline datasets become increasingly
significant. To address these limitations, this paper introduces a novel
offline Generative Auto-bidding framework with Value-Guided Explorations
(GAVE). GAVE accommodates various advertising objectives through a score-based
Return-To-Go (RTG) module. Moreover, GAVE integrates an action exploration
mechanism with an RTG-based evaluation method to explore novel actions while
ensuring stability-preserving updates. A learnable value function is also
designed to guide the direction of action exploration and mitigate
Out-of-Distribution (OOD) problems. Experimental results on two offline
datasets and real-world deployments demonstrate that GAVE outperforms
state-of-the-art baselines in both offline evaluations and online A/B tests.
The implementation code is publicly available to facilitate reproducibility and
further research.

</details>


### [337] [No Imputation of Missing Values In Tabular Data Classification Using Incremental Learning](https://arxiv.org/abs/2504.14610)
*Manar D. Samad,Kazi Fuad B. Akhter,Shourav B. Rabbani,Ibna Kowsar*

Main category: cs.LG

TL;DR: 提出了一种无需填补缺失值的增量学习方法（NIIL），通过注意力掩码排除缺失值，在15个数据集上表现优于11种现有方法。


<details>
  <summary>Details</summary>
Motivation: 填补缺失值可能引发计算复杂度、数据质量和结果可靠性的担忧，NIIL旨在解决这些问题。

Method: 增量学习重叠特征集分区，使用注意力掩码排除缺失值。

Result: 在15个数据集上分类性能排名优于11种现有方法，且对缺失值类型和比例具有鲁棒性。

Conclusion: NIIL是一种无需填补缺失值的深度学习解决方案，特征分区大小为原始特征空间一半时效果最佳。

Abstract: Tabular data sets with varying missing values are prepared for machine
learning using an arbitrary imputation strategy. Synthetic values generated by
imputation models often concern data stakeholders about computational
complexity, data quality, and data-driven outcomes. This paper eliminates these
concerns by proposing no imputation incremental learning (NIIL) of tabular data
with varying missing value rates and types. The proposed method incrementally
learns partitions of overlapping feature sets while using attention masks to
exclude missing values from attention scoring. The average classification
performance rank order across 15 diverse tabular data sets highlights the
superiority of NIIL over 11 state-of-the-art learning methods with or without
missing value imputations. Further experiments substantiate the robustness of
NIIL against varying missing value types and rates compared to methods that
involve the imputation of missing values. Our empirical analysis reveals that a
feature partition size of half of the original feature space is,
computation-wise and accuracy-wise, the best choice for the proposed
incremental learning. The proposed method is one of the first deep learning
solutions that can effectively learn tabular data without requiring the
imputation of missing values.

</details>


### [338] [AlphaZero-Edu: Making AlphaZero Accessible to Everyone](https://arxiv.org/abs/2504.14636)
*Binjie Guo,Hanyu Zheng,Guowei Su,Ru Zhang,Haohan Jiang,Xurong Lin,Hongyan Wei,Aisheng Mo,Jie Li,Zhiyuan Qian,Zhuhao Zhang,Xiaoyuan Cheng*

Main category: cs.LG

TL;DR: AlphaZero-Edu是一个轻量级、教育导向的强化学习框架，基于AlphaZero数学框架，解决了现有框架的高复杂性和低可复现性问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习框架实现复杂且难以复现，AlphaZero-Edu旨在提供一个透明、高效的解决方案。

Method: 采用模块化架构，优化资源使用，支持并行化自对弈数据生成，并在单GPU上高效训练。

Result: 在Gomoku对战中表现优异，对人类对手保持高胜率，并实现3.2倍的加速。

Conclusion: AlphaZero-Edu为学术研究和工业应用提供了易用且高效的基准框架。

Abstract: Recent years have witnessed significant progress in reinforcement learning,
especially with Zero-like paradigms, which have greatly boosted the
generalization and reasoning abilities of large-scale language models.
Nevertheless, existing frameworks are often plagued by high implementation
complexity and poor reproducibility. To tackle these challenges, we present
AlphaZero-Edu, a lightweight, education-focused implementation built upon the
mathematical framework of AlphaZero. It boasts a modular architecture that
disentangles key components, enabling transparent visualization of the
algorithmic processes. Additionally, it is optimized for resource-efficient
training on a single NVIDIA RTX 3090 GPU and features highly parallelized
self-play data generation, achieving a 3.2-fold speedup with 8 processes. In
Gomoku matches, the framework has demonstrated exceptional performance,
achieving a consistently high win rate against human opponents. AlphaZero-Edu
has been open-sourced at https://github.com/StarLight1212/AlphaZero_Edu,
providing an accessible and practical benchmark for both academic research and
industrial applications.

</details>


### [339] [Surrogate Fitness Metrics for Interpretable Reinforcement Learning](https://arxiv.org/abs/2504.14645)
*Philipp Altmann,Céline Davignon,Maximilian Zorn,Fabian Ritz,Claudia Linnhoff-Popien,Thomas Gabor*

Main category: cs.LG

TL;DR: 通过进化优化框架生成多样且信息丰富的策略演示，结合局部多样性、行为确定性和全局多样性优化轨迹选择，显著提升RL策略的可解释性。


<details>
  <summary>Details</summary>
Motivation: 提升强化学习（RL）策略的可解释性，尤其是在安全关键和需要解释性的领域。

Method: 采用进化优化框架，通过联合代理适应度函数（结合多样性、行为确定性等）优化初始状态扰动，生成高质量演示。

Result: 在离散和连续环境中，优化后的轨迹选择显著提升了策略可解释性；网格世界中的演示保真度明显优于随机基线。

Conclusion: 通过系统分析代理适应度函数，本研究推动了RL模型的可解释性，为决策提供了更深入的见解。

Abstract: We employ an evolutionary optimization framework that perturbs initial states
to generate informative and diverse policy demonstrations. A joint surrogate
fitness function guides the optimization by combining local diversity,
behavioral certainty, and global population diversity. To assess demonstration
quality, we apply a set of evaluation metrics, including the reward-based
optimality gap, fidelity interquartile means (IQMs), fitness composition
analysis, and trajectory visualizations. Hyperparameter sensitivity is also
examined to better understand the dynamics of trajectory optimization. Our
findings demonstrate that optimizing trajectory selection via surrogate fitness
metrics significantly improves interpretability of RL policies in both discrete
and continuous environments. In gridworld domains, evaluations reveal
significantly enhanced demonstration fidelities compared to random and ablated
baselines. In continuous control, the proposed framework offers valuable
insights, particularly for early-stage policies, while fidelity-based
optimization proves more effective for mature policies. By refining and
systematically analyzing surrogate fitness functions, this study advances the
interpretability of RL models. The proposed improvements provide deeper
insights into RL decision-making, benefiting applications in safety-critical
and explainability-focused domains.

</details>


### [340] [LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs](https://arxiv.org/abs/2504.14655)
*Yunhui Xia,Wei Shen,Yan Wang,Jason Klein Liu,Huifeng Sun,Siyue Wu,Jian Hu,Xiaolong Xu*

Main category: cs.LG

TL;DR: LeetCodeDataset是一个高质量代码生成模型评估和训练基准，解决了LLM研究中缺乏推理导向的编码基准和自包含训练测试平台的问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM研究中缺乏推理导向的编码基准和自包含训练测试平台的挑战。

Method: 通过整理LeetCode Python问题，提供丰富的元数据、广泛覆盖、每个问题100+测试用例以及时间分割（2024年7月前后），实现无污染评估和高效监督微调（SFT）。

Result: 实验显示推理模型显著优于非推理模型，仅用2.6K模型生成的解决方案进行SFT即可达到与110K样本相当的性能。

Conclusion: LeetCodeDataset及其评估框架在Hugging Face和Github上可用，为代码生成模型提供了高质量基准。

Abstract: We introduce LeetCodeDataset, a high-quality benchmark for evaluating and
training code-generation models, addressing two key challenges in LLM research:
the lack of reasoning-focused coding benchmarks and self-contained training
testbeds. By curating LeetCode Python problems with rich metadata, broad
coverage, 100+ test cases per problem, and temporal splits (pre/post July
2024), our dataset enables contamination-free evaluation and efficient
supervised fine-tuning (SFT). Experiments show reasoning models significantly
outperform non-reasoning counterparts, while SFT with only 2.6K model-generated
solutions achieves performance comparable to 110K-sample counterparts. The
dataset and evaluation framework are available on Hugging Face and Github.

</details>


### [341] [Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning](https://arxiv.org/abs/2504.14662)
*Yeoreum Lee,Jinwook Jung,Sungyong Baik*

Main category: cs.LG

TL;DR: 论文提出了一种新的微调目标函数，旨在减少参数干扰并提升任务性能，发现其与SAM目标函数相似，最终通过实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模深度学习模型在合并时参数干扰的问题，同时提升每个任务微调模型的性能。

Method: 设计新的微调目标函数，借鉴SAM的思想，通过寻找平坦最小值来减少干扰并提升性能。

Result: 实验和理论结果表明，该方法在模型合并和微调中表现优异。

Conclusion: 提出的方法有效解决了参数干扰问题，并提升了合并模型的性能。

Abstract: Large-scale deep learning models with a pretraining-finetuning paradigm have
led to a surge of numerous task-specific models fine-tuned from a common
pre-trained model. Recently, several research efforts have been made on merging
these large models into a single multi-task model, particularly with simple
arithmetic on parameters. Such merging methodology faces a central challenge:
interference between model parameters fine-tuned on different tasks. Few recent
works have focused on designing a new fine-tuning scheme that can lead to small
parameter interference, however at the cost of the performance of each
task-specific fine-tuned model and thereby limiting that of a merged model. To
improve the performance of a merged model, we note that a fine-tuning scheme
should aim for (1) smaller parameter interference and (2) better performance of
each fine-tuned model on the corresponding task. In this work, we aim to design
a new fine-tuning objective function to work towards these two goals. In the
course of this process, we find such objective function to be strikingly
similar to sharpness-aware minimization (SAM) objective function, which aims to
achieve generalization by finding flat minima. Drawing upon our observation, we
propose to fine-tune pre-trained models via sharpness-aware minimization. The
experimental and theoretical results showcase the effectiveness and
orthogonality of our proposed approach, improving performance upon various
merging and fine-tuning methods. Our code is available at
https://github.com/baiklab/SAFT-Merge.

</details>


### [342] [Efficient Federated Split Learning for Large Language Models over Communication Networks](https://arxiv.org/abs/2504.14667)
*Kai Zhao,Zhaohui Yang*

Main category: cs.LG

TL;DR: FedsLLM框架结合了分片联邦学习和参数高效微调技术，降低了边缘设备的计算负担，并通过联合优化问题减少训练延迟。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限的边缘设备上分布式微调大型语言模型的挑战。

Method: 结合分片联邦学习和LoRA技术，提出联合优化问题，包括子信道分配、功率控制、模型分片点选择和LoRA秩配置。

Result: FedsLLM在保持模型精度的同时显著减少客户端计算需求和训练延迟。

Conclusion: FedsLLM为边缘设备上的高效分布式微调提供了可行方案。

Abstract: Fine-tuning pre-trained large language models (LLM) in a distributed manner
poses significant challenges on resource-constrained edge devices. To address
this challenge, we propose FedsLLM, a novel framework that integrates split
federated learning with parameter-efficient fine-tuning techniques. By
leveraging model splitting and Low-Rank Adaptation (LoRA), FedsLLM reduces the
computational burden on edge devices. Furthermore, the introduction of a
federated server facilitates parallel training and enhances privacy. To
accommodate heterogeneous communication conditions and diverse computational
capabilities of edge devices, as well as the impact of LoRA rank selection on
model convergence and training cost, we formulate a joint optimization problem.
The formulated problem jointly optimizes subchannel allocation, power control,
model splitting point selection, and LoRA rank configuration, all aimed at
minimizing total training delay. An alternating optimization algorithm is
developed to efficiently solve this problem and accelerate the training
process. Simulation results demonstrate that the proposed FedsLLM framework
achieves comparable model accuracy while significantly reducing client-side
computational requirements. Furthermore, the proposed resource allocation
scheme and adaptive LoRA rank selection strategy notably reduce the training
latency compared to conventional approaches.

</details>


### [343] [Evaluating Temporal Plasticity in Foundation Time Series Models for Incremental Fine-tuning](https://arxiv.org/abs/2504.14677)
*Jia Liu,Cheng Jinguo,Xia Fang,Zhenyuan Ma,Yuankai Wu*

Main category: cs.LG

TL;DR: 研究了时间序列基础模型在持续学习中的表现，发现其优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 探索时间序列基础模型在持续学习中的潜力，填补现有研究的空白。

Method: 使用真实数据集和新型持续学习框架，对比传统模型与基础模型的性能。

Result: 基础模型（如Time-MoE和Chronos）在持续学习中表现更优，预测准确性持续提升。

Conclusion: 优化基础模型的微调策略比开发领域特定小模型更有价值，为未来研究提供了新方法。

Abstract: Time series foundation models excel at diverse time series forecasting tasks,
but their capacity for continuous improvement through incremental learning
remains unexplored. We present the first comprehensive study investigating
these models' temporal plasticity - their ability to progressively enhance
performance through continual learning while maintaining existing capabilities.
Through experiments on real-world datasets exhibiting distribution shifts, we
evaluate both conventional deep learning models and foundation models using a
novel continual learning framework. Our findings reveal that while traditional
models struggle with performance deterioration during incremental fine-tuning,
foundation models like Time-MoE and Chronos demonstrate sustained improvement
in predictive accuracy. This suggests that optimizing foundation model
fine-tuning strategies may be more valuable than developing domain-specific
small models. Our research introduces new evaluation methodologies and insights
for developing foundation time series models with robust continuous learning
capabilities.

</details>


### [344] [Learning Critically: Selective Self Distillation in Federated Learning on Non-IID Data](https://arxiv.org/abs/2504.14694)
*Yuting He,Yiqiang Chen,XiaoDong Yang,Hanchao Yu,Yi-Hua Huang,Yang Gu*

Main category: cs.LG

TL;DR: FedSSD提出了一种选择性自蒸馏方法，通过自适应约束局部更新，提升联邦学习在非IID数据下的性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据异构性（非IID）导致局部模型偏向局部最优而遗忘全局知识，影响性能和收敛速度。

Method: 提出选择性自蒸馏方法（FedSSD），通过自蒸馏全局知识并基于类和样本可信度选择性加权，自适应约束局部更新。

Result: 理论分析证明收敛性，实验表明FedSSD在更少通信轮次下优于其他先进方法，具有更好的泛化性和鲁棒性。

Conclusion: FedSSD有效解决了非IID数据问题，提升了联邦学习的效率和性能。

Abstract: Federated learning (FL) enables multiple clients to collaboratively train a
global model while keeping local data decentralized. Data heterogeneity
(non-IID) across clients has imposed significant challenges to FL, which makes
local models re-optimize towards their own local optima and forget the global
knowledge, resulting in performance degradation and convergence slowdown. Many
existing works have attempted to address the non-IID issue by adding an extra
global-model-based regularizing item to the local training but without an
adaption scheme, which is not efficient enough to achieve high performance with
deep learning models. In this paper, we propose a Selective Self-Distillation
method for Federated learning (FedSSD), which imposes adaptive constraints on
the local updates by self-distilling the global model's knowledge and
selectively weighting it by evaluating the credibility at both the class and
sample level. The convergence guarantee of FedSSD is theoretically analyzed and
extensive experiments are conducted on three public benchmark datasets, which
demonstrates that FedSSD achieves better generalization and robustness in fewer
communication rounds, compared with other state-of-the-art FL methods.

</details>


### [345] [Quantitative Clustering in Mean-Field Transformer Models](https://arxiv.org/abs/2504.14697)
*Shi Chen,Zhengjiang Lin,Yury Polyanskiy,Philippe Rigollet*

Main category: cs.LG

TL;DR: 论文研究了深度Transformer模型中token的演化，类比为相互作用粒子系统，并探讨了其长期聚类行为，类似于Kuramoto模型中的同步现象。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型中token演化的长期聚类行为，以理解其动态特性。

Method: 通过分析均值场Transformer模型，建立参数假设下初始化的指数收缩率。

Result: 在参数假设下，任何合适的初始化都会以指数速率同步到一个Dirac点质量。

Conclusion: 均值场Transformer模型在特定条件下表现出快速的指数同步行为。

Abstract: The evolution of tokens through a deep transformer models can be modeled as
an interacting particle system that has been shown to exhibit an asymptotic
clustering behavior akin to the synchronization phenomenon in Kuramoto models.
In this work, we investigate the long-time clustering of mean-field transformer
models. More precisely, we establish exponential rates of contraction to a
Dirac point mass for any suitably regular initialization under some assumptions
on the parameters of transformer models, any suitably regular mean-field
initialization synchronizes exponentially fast with some quantitative rates.

</details>


### [346] [Connecting Parameter Magnitudes and Hessian Eigenspaces at Scale using Sketched Methods](https://arxiv.org/abs/2504.14701)
*Andres Fernandez,Frank Schneider,Maren Mahsereci,Philipp Hennig*

Main category: cs.LG

TL;DR: 研究发现，深度神经网络训练中，损失函数的曲率集中在Hessian矩阵的顶部特征空间，且与幅度剪枝掩码有显著重叠。


<details>
  <summary>Details</summary>
Motivation: 探索深度神经网络训练中损失Hessian矩阵的顶部特征空间与幅度剪枝掩码之间的关系。

Method: 开发了一种基于Grassmannian度量的方法，通过矩阵自由算法计算Hessian特征对，分析掩码与特征空间的重叠。

Result: 幅度剪枝掩码与Hessian顶部特征空间的重叠显著高于随机水平，且网络规模越大效果越明显。

Conclusion: 研究提供了分析深度学习Hessian矩阵的新方法，并揭示了其顶部特征空间与参数大小的关系。

Abstract: Recently, it has been observed that when training a deep neural net with SGD,
the majority of the loss landscape's curvature quickly concentrates in a tiny
*top* eigenspace of the loss Hessian, which remains largely stable thereafter.
Independently, it has been shown that successful magnitude pruning masks for
deep neural nets emerge early in training and remain stable thereafter. In this
work, we study these two phenomena jointly and show that they are connected: We
develop a methodology to measure the similarity between arbitrary parameter
masks and Hessian eigenspaces via Grassmannian metrics. We identify *overlap*
as the most useful such metric due to its interpretability and stability. To
compute *overlap*, we develop a matrix-free algorithm based on sketched SVDs
that allows us to compute over 1000 Hessian eigenpairs for nets with over 10M
parameters --an unprecedented scale by several orders of magnitude. Our
experiments reveal an *overlap* between magnitude parameter masks and top
Hessian eigenspaces consistently higher than chance-level, and that this effect
gets accentuated for larger network sizes. This result indicates that *top
Hessian eigenvectors tend to be concentrated around larger parameters*, or
equivalently, that *larger parameters tend to align with directions of larger
loss curvature*. Our work provides a methodology to approximate and analyze
deep learning Hessians at scale, as well as a novel insight on the structure of
their eigenspace.

</details>


### [347] [Can We Ignore Labels In Out of Distribution Detection?](https://arxiv.org/abs/2504.14704)
*Hong Yang,Qi Yu,Travis Desel*

Main category: cs.LG

TL;DR: 论文研究了无标签OOD检测的理论失败条件，提出了'标签盲区'概念，并定义了新的Adjacent OOD检测任务，验证了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: OOD检测在安全关键系统中至关重要，但现有无标签OOD检测方法缺乏理论保证，可能导致失败。

Method: 通过信息论视角，提出标签盲区的理论条件，并设计Adjacent OOD任务验证现有方法。

Result: 实验证明现有无标签OOD方法在标签盲区条件下会失败，揭示了其局限性。

Conclusion: 研究为未来无标签OOD方法提供了理论指导，强调了标签盲区问题的重要性。

Abstract: Out-of-distribution (OOD) detection methods have recently become more
prominent, serving as a core element in safety-critical autonomous systems. One
major purpose of OOD detection is to reject invalid inputs that could lead to
unpredictable errors and compromise safety. Due to the cost of labeled data,
recent works have investigated the feasibility of self-supervised learning
(SSL) OOD detection, unlabeled OOD detection, and zero shot OOD detection. In
this work, we identify a set of conditions for a theoretical guarantee of
failure in unlabeled OOD detection algorithms from an information-theoretic
perspective. These conditions are present in all OOD tasks dealing with
real-world data: I) we provide theoretical proof of unlabeled OOD detection
failure when there exists zero mutual information between the learning
objective and the in-distribution labels, a.k.a. 'label blindness', II) we
define a new OOD task - Adjacent OOD detection - that tests for label blindness
and accounts for a previously ignored safety gap in all OOD detection
benchmarks, and III) we perform experiments demonstrating that existing
unlabeled OOD methods fail under conditions suggested by our label blindness
theory and analyze the implications for future research in unlabeled OOD
methods.

</details>


### [348] [Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in LLM-Based Evaluation](https://arxiv.org/abs/2504.14716)
*Tuhina Tripathi,Manya Wadhwa,Greg Durrett,Scott Niekum*

Main category: cs.LG

TL;DR: 研究表明，反馈协议的选择（绝对评分vs相对偏好）对评估可靠性和系统性偏差有显著影响。绝对评分更抗干扰，而配对评估易受干扰。


<details>
  <summary>Details</summary>
Motivation: 探讨反馈协议在LLM对齐和评估中的关键作用，揭示其潜在偏差。

Method: 比较绝对评分和配对偏好两种反馈协议，分析其对评估可靠性和模型行为的影响。

Result: 配对偏好易受干扰，35%情况下偏好翻转；绝对评分更稳定，仅9%翻转。

Conclusion: 建议根据数据集特性和评估目标选择反馈协议，绝对评分更适合抗干扰场景。

Abstract: Large Language Models (LLMs) are widely used as proxies for human labelers in
both training (Reinforcement Learning from AI Feedback) and large-scale
response evaluation (LLM-as-a-judge). Alignment and evaluation are critical
components in the development of reliable LLMs, and the choice of feedback
protocol plays a central role in both but remains understudied. In this work,
we show that the choice of feedback protocol (absolute scores versus relative
preferences) can significantly affect evaluation reliability and induce
systematic biases. In particular, we show that pairwise evaluation protocols
are more vulnerable to distracted evaluation. Generator models can exploit
spurious attributes (or distractor features) favored by the LLM judge,
resulting in inflated scores for lower-quality outputs and misleading training
signals. We find that absolute scoring is more robust to such manipulation,
producing judgments that better reflect response quality and are less
influenced by distractor features. Our results demonstrate that generator
models can flip preferences by embedding distractor features, skewing
LLM-as-a-judge comparisons and leading to inaccurate conclusions about model
quality in benchmark evaluations. Pairwise preferences flip in about 35% of the
cases, compared to only 9% for absolute scores. We offer recommendations for
choosing feedback protocols based on dataset characteristics and evaluation
objectives.

</details>


### [349] [Semi-parametric Memory Consolidation: Towards Brain-like Deep Continual Learning](https://arxiv.org/abs/2504.14727)
*Geng Liu,Fei Zhu,Rong Feng,Zhiqiang Yi,Shiqi Wang,Gaofeng Meng,Zhaoxiang Zhang*

Main category: cs.LG

TL;DR: 提出了一种受人类记忆和学习系统启发的生物模拟持续学习框架，解决了深度神经网络在连续任务训练中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在动态开放环境中需要持续学习能力，但现有方法存在灾难性遗忘问题。

Method: 结合半参数记忆和醒睡巩固机制，模拟人类记忆系统。

Result: 在真实世界挑战性持续学习场景（如ImageNet类增量学习）中，模型能保持新任务高性能并保留旧知识。

Conclusion: 模拟生物智能是赋予深度神经网络持续学习能力的有效途径。

Abstract: Humans and most animals inherently possess a distinctive capacity to
continually acquire novel experiences and accumulate worldly knowledge over
time. This ability, termed continual learning, is also critical for deep neural
networks (DNNs) to adapt to the dynamically evolving world in open
environments. However, DNNs notoriously suffer from catastrophic forgetting of
previously learned knowledge when trained on sequential tasks. In this work,
inspired by the interactive human memory and learning system, we propose a
novel biomimetic continual learning framework that integrates semi-parametric
memory and the wake-sleep consolidation mechanism. For the first time, our
method enables deep neural networks to retain high performance on novel tasks
while maintaining prior knowledge in real-world challenging continual learning
scenarios, e.g., class-incremental learning on ImageNet. This study
demonstrates that emulating biological intelligence provides a promising path
to enable deep neural networks with continual learning capabilities.

</details>


### [350] [Geometric Learning Dynamics](https://arxiv.org/abs/2504.14728)
*Vitaly Vanchurin*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a unified geometric framework for modeling learning dynamics in
physical, biological, and machine learning systems. The theory reveals three
fundamental regimes, each emerging from the power-law relationship $g \propto
\kappa^a$ between the metric tensor $g$ in the space of trainable variables and
the noise covariance matrix $\kappa$. The quantum regime corresponds to $a = 1$
and describes Schr\"odinger-like dynamics that emerges from a discrete shift
symmetry. The efficient learning regime corresponds to $a = \tfrac{1}{2}$ and
describes very fast machine learning algorithms. The equilibration regime
corresponds to $a = 0$ and describes classical models of biological evolution.
We argue that the emergence of the intermediate regime $a = \tfrac{1}{2}$ is a
key mechanism underlying the emergence of biological complexity.

</details>


### [351] [Reinforcement Learning from Multi-level and Episodic Human Feedback](https://arxiv.org/abs/2504.14732)
*Muhammad Qasim Elahi,Somtochukwu Oguchienti,Maheed H. Ahmed,Mahsa Ghasemi*

Main category: cs.LG

TL;DR: 论文提出了一种基于多级人类反馈的强化学习算法，用于从非马尔可夫奖励中学习奖励函数和最优策略。


<details>
  <summary>Details</summary>
Motivation: 设计有效的奖励函数在复杂任务中具有挑战性，现有方法多依赖人类比较反馈，而多级反馈提供了更粗粒度但信息丰富的信号。

Method: 提出了一种算法，利用每局结束时的评分形式的多级反馈，学习奖励函数和最优策略。

Result: 算法实现了次线性遗憾，并通过大量仿真验证了其有效性。

Conclusion: 多级人类反馈是一种有潜力的替代方法，能够处理非马尔可夫奖励并提升学习效率。

Abstract: Designing an effective reward function has long been a challenge in
reinforcement learning, particularly for complex tasks in unstructured
environments. To address this, various learning paradigms have emerged that
leverage different forms of human input to specify or refine the reward
function. Reinforcement learning from human feedback is a prominent approach
that utilizes human comparative feedback, expressed as a preference for one
behavior over another, to tackle this problem. In contrast to comparative
feedback, we explore multi-level human feedback, which is provided in the form
of a score at the end of each episode. This type of feedback offers more coarse
but informative signals about the underlying reward function than binary
feedback. Additionally, it can handle non-Markovian rewards, as it is based on
the evaluation of an entire episode. We propose an algorithm to efficiently
learn both the reward function and the optimal policy from this form of
feedback. Moreover, we show that the proposed algorithm achieves sublinear
regret and demonstrate its empirical effectiveness through extensive
simulations.

</details>


### [352] [AltGDmin: Alternating GD and Minimization for Partly-Decoupled (Federated) Optimization](https://arxiv.org/abs/2504.14741)
*Namrata Vaswani*

Main category: cs.LG

TL;DR: 提出了一种名为AltGDmin的新型优化框架，适用于需要交替最小化（AltMin）的问题，并在某些条件下比AltMin更快。


<details>
  <summary>Details</summary>
Motivation: 解决交替最小化（AltMin）在某些问题中效率不足的问题，尤其是在一个变量子集的优化比另一个快且成本函数可微的情况下。

Method: AltGDmin结合梯度下降（GD）和最小化方法，利用问题的解耦特性，提升优化效率。

Result: AltGDmin在多种问题（如低秩压缩感知、矩阵补全、相位恢复等）中表现出比AltMin更快的速度，且适用于联邦学习等通信高效场景。

Conclusion: AltGDmin为需要交替优化的问题提供了一种高效解决方案，尤其适用于变量子集优化速度差异明显的场景。

Abstract: This article describes a novel optimization solution framework, called
alternating gradient descent (GD) and minimization (AltGDmin), that is useful
for many problems for which alternating minimization (AltMin) is a popular
solution. AltMin is a special case of the block coordinate descent algorithm
that is useful for problems in which minimization w.r.t one subset of variables
keeping the other fixed is closed form or otherwise reliably solved. Denote the
two blocks/subsets of the optimization variables Z by Za, Zb, i.e., Z = {Za,
Zb}. AltGDmin is often a faster solution than AltMin for any problem for which
(i) the minimization over one set of variables, Zb, is much quicker than that
over the other set, Za; and (ii) the cost function is differentiable w.r.t. Za.
Often, the reason for one minimization to be quicker is that the problem is
``decoupled" for Zb and each of the decoupled problems is quick to solve. This
decoupling is also what makes AltGDmin communication-efficient for federated
settings.
  Important examples where this assumption holds include (a) low rank
column-wise compressive sensing (LRCS), low rank matrix completion (LRMC), (b)
their outlier-corrupted extensions such as robust PCA, robust LRCS and robust
LRMC; (c) phase retrieval and its sparse and low-rank model based extensions;
(d) tensor extensions of many of these problems such as tensor LRCS and tensor
completion; and (e) many partly discrete problems where GD does not apply --
such as clustering, unlabeled sensing, and mixed linear regression. LRCS finds
important applications in multi-task representation learning and few shot
learning, federated sketching, and accelerated dynamic MRI. LRMC and robust PCA
find important applications in recommender systems, computer vision and video
analytics.

</details>


### [353] [AI for the Open-World: the Learning Principles](https://arxiv.org/abs/2504.14751)
*Jianyu Zhang*

Main category: cs.LG

TL;DR: 论文探讨了封闭世界AI的成功是否适用于开放世界，指出开放世界需要独特的学习原则和技术。


<details>
  <summary>Details</summary>
Motivation: 封闭世界AI的成功依赖于明确的标准和大量示例，但这些在开放世界中不适用，因此需要新的学习原则。

Method: 提出开放世界AI的学习原则（如丰富特征、解耦表示和推理时学习），并通过大规模实验验证。

Result: 研究发现封闭世界的方法无法直接应用于开放世界，新学习原则和技术是必要的。

Conclusion: 开放世界AI需要独特的学习原则和技术，论文提出的方法为未来研究奠定了基础。

Abstract: During the past decades, numerous successes of AI has been made on "specific
capabilities", named closed-world, such as artificial environments or specific
real-world tasks. This well-defined narrow capability brings two nice benefits,
a clear criterion of success and the opportunity to collect a lot of examples.
The criteria not only reveal whether a machine has achieved a goal, but reveal
how the machine falls short of the goal. As a result, human designers can fix
the problems one after the other until the machine is deemed good enough for
the task. Furthermore, the large set of collected examples reduces the
difficulty of this problem-fixing process (by the central limit theorem).
  Do the success in closed-world translate into broad open-world, where a
machine is required to perform any task that a human could possibly undertake
with fewer examples and less priori knowledge from human designers? No. Because
competence in a specific task provides little insight in handling other tasks,
the valuable criteria for specific tasks become helpless when handling broader
unseen tasks. Furthermore, due to the shortage of examples in unseen tasks,
central limit theorem does not stand on our side. At the end, human designers
lose the oscilloscope to "hack" an AI system for the open-world.
  Achieving AI for the open-world requires unique learning principles and
innovated techniques, which are different from the ones in building AI for the
closed-world. This thesis explores necessary learning principles required to
construct AI for the open-world, including rich features (analogy a large tool
box), disentangled representation (an organized tool box), and inference-time
learning (a tool-savvy hand). Driven by the learning principles, this thesis
further proposes techniques to use the learning principles, conducts enormous
large-scale experiments to verify the learning principles.

</details>


### [354] [A Combinatorial Theory of Dropout: Subnetworks, Graph Geometry, and Generalization](https://arxiv.org/abs/2504.14762)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: 提出了一种基于组合和图论的dropout理论，将训练建模为在高维二值子网络图上的随机游走，揭示了dropout通过采样稳健、结构化的子网络集合提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究dropout的机制，揭示其如何通过随机采样子网络提升模型的泛化能力。

Method: 将训练建模为高维图上的随机游走，定义子网络贡献分数，并利用谱图理论、PAC-Bayes分析和组合数学工具分析子网络特性。

Result: 证明泛化子网络形成大、连通、低阻的集群，数量随网络宽度指数增长，实验验证了理论。

Conclusion: 为理解dropout提供了统一的理论基础，并提出了掩码引导正则化和子网络优化的新方向。

Abstract: We propose a combinatorial and graph-theoretic theory of dropout by modeling
training as a random walk over a high-dimensional graph of binary subnetworks.
Each node represents a masked version of the network, and dropout induces
stochastic traversal across this space. We define a subnetwork contribution
score that quantifies generalization and show that it varies smoothly over the
graph. Using tools from spectral graph theory, PAC-Bayes analysis, and
combinatorics, we prove that generalizing subnetworks form large, connected,
low-resistance clusters, and that their number grows exponentially with network
width. This reveals dropout as a mechanism for sampling from a robust,
structured ensemble of well-generalizing subnetworks with built-in redundancy.
Extensive experiments validate every theoretical claim across diverse
architectures. Together, our results offer a unified foundation for
understanding dropout and suggest new directions for mask-guided regularization
and subnetwork optimization.

</details>


### [355] [Novel Concept-Oriented Synthetic Data approach for Training Generative AI-Driven Crystal Grain Analysis Using Diffusion Model](https://arxiv.org/abs/2504.14782)
*Ahmed Sobhi Saleh,Kristof Croes,Hajdin Ceric,Ingrid De Wolf,Houman Zahedmanesh*

Main category: cs.LG

TL;DR: 提出了一种结合边缘检测和生成扩散模型的自动化方法，用于从显微镜图像中提取多晶粒结构，解决了传统方法效率低、主观性强的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如TEM和SEM）提取多晶粒结构效率低且主观性强，限制了高通量分析的可扩展性。

Method: 采用七阶段方法生成合成TEM图像用于训练，结合边缘检测和生成扩散模型自动识别晶粒、消除噪声并连接断裂部分。

Result: 模型在纳米尺度下应用于多种金属，从低分辨率TEM图像中提取的晶粒形态与高要求实验技术结果相当，平均准确率达97.23%。

Conclusion: 该方法不仅解决了数据稀缺问题，还可推广至其他领域，为高通量分析提供了高效解决方案。

Abstract: The traditional techniques for extracting polycrystalline grain structures
from microscopy images, such as transmission electron microscopy (TEM) and
scanning electron microscopy (SEM), are labour-intensive, subjective, and
time-consuming, limiting their scalability for high-throughput analysis. In
this study, we present an automated methodology integrating edge detection with
generative diffusion models to effectively identify grains, eliminate noise,
and connect broken segments in alignment with predicted grain boundaries. Due
to the limited availability of adequate images preventing the training of deep
machine learning models, a new seven-stage methodology is employed to generate
synthetic TEM images for training. This concept-oriented synthetic data
approach can be extended to any field of interest where the scarcity of data is
a challenge. The presented model was applied to various metals with average
grain sizes down to the nanoscale, producing grain morphologies from
low-resolution TEM images that are comparable to those obtained from advanced
and demanding experimental techniques with an average accuracy of 97.23%.

</details>


### [356] [Enhanced Data-driven Topology Design Methodology with Multi-level Mesh and Correlation-based Mutation for Stress-related Multi-objective Optimization](https://arxiv.org/abs/2504.14790)
*Jun Yang,Shintaro Yamasaki*

Main category: cs.LG

TL;DR: 提出了一种基于多级网格和相关性变异模块的数据驱动拓扑设计方法，解决了传统方法对初始数据集质量的依赖问题，提高了计算效率和通用性。


<details>
  <summary>Details</summary>
Motivation: 传统基于敏感性的拓扑优化方法在强非线性问题中表现不佳，而数据驱动方法对初始数据集质量敏感，限制了其通用性和有效性。

Method: 采用多级网格策略和相关性变异模块，逐步优化结构表示，避免高自由度问题，同时利用变异模块赋予新几何特征。

Result: 实验表明，该方法在强非线性问题上优于传统敏感性方法，提高了通用性和计算效率。

Conclusion: 多级网格数据驱动方法显著降低了初始数据集质量要求，提高了拓扑优化的通用性和效率。

Abstract: Topology optimization (TO) serves as a widely applied structural design
approach to tackle various engineering problems. Nevertheless,
sensitivity-based TO methods usually struggle with solving strongly nonlinear
optimization problems. By leveraging high capacity of deep generative model,
which is an influential machine learning technique, the sensitivity-free
data-driven topology design (DDTD) methodology is regarded as an effective
means of overcoming these issues. The DDTD methodology depends on initial
dataset with a certain regularity, making its results highly sensitive to
initial dataset quality. This limits its effectiveness and generalizability,
especially for optimization problems without priori information. In this
research, we proposed a multi-level mesh DDTD-based method with
correlation-based mutation module to escape from the limitation of the quality
of the initial dataset on the results and enhance computational efficiency. The
core is to employ a correlation-based mutation module to assign new geometric
features with physical meaning to the generated data, while utilizing a
multi-level mesh strategy to progressively enhance the refinement of the
structural representation, thus avoiding the maintenance of a high
degree-of-freedom (DOF) representation throughout the iterative process. The
proposed multi-level mesh DDTD-based method can be driven by a low quality
initial dataset without the need for time-consuming construction of a specific
dataset, thus significantly increasing generality and reducing application
difficulty, while further lowering computational cost of DDTD methodology.
Various comparison experiments with the traditional sensitivity-based TO
methods on stress-related strongly nonlinear problems demonstrate the
generality and effectiveness of the proposed method.

</details>


### [357] [Edge-boosted graph learning for functional brain connectivity analysis](https://arxiv.org/abs/2504.14796)
*David Yang,Mostafa Abdelmegeed,John Modl,Minjeong Kim*

Main category: cs.LG

TL;DR: 提出了一种基于边功能连接（eFC）的新方法，用于脑网络分析，显著优于现有GNN方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于节点的脑连接方法未能准确捕捉功能连接，需要更精确的方法。

Method: 采用边功能连接（eFC）和共嵌入技术分析脑网络。

Result: 在ADNI和PPMI数据集上，新方法显著优于现有GNN方法。

Conclusion: 边功能连接方法为脑网络分析提供了更准确的工具。

Abstract: Predicting disease states from functional brain connectivity is critical for
the early diagnosis of severe neurodegenerative diseases such as Alzheimer's
Disease and Parkinson's Disease. Existing studies commonly employ Graph Neural
Networks (GNNs) to infer clinical diagnoses from node-based brain connectivity
matrices generated through node-to-node similarities of regionally averaged
fMRI signals. However, recent neuroscience studies found that such node-based
connectivity does not accurately capture ``functional connections" within the
brain. This paper proposes a novel approach to brain network analysis that
emphasizes edge functional connectivity (eFC), shifting the focus to inter-edge
relationships. Additionally, we introduce a co-embedding technique to integrate
edge functional connections effectively. Experimental results on the ADNI and
PPMI datasets demonstrate that our method significantly outperforms
state-of-the-art GNN methods in classifying functional brain networks.

</details>


### [358] [Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned Models](https://arxiv.org/abs/2504.14798)
*Hao Xuan,Xingyu Li*

Main category: cs.LG

TL;DR: 论文提出了一种新的鲁棒性遗忘（Robust Unlearning）概念，并设计了一种遗忘映射攻击（UMA）框架，用于检测未学习模型中残留的信息泄露问题。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘验证方法无法检测残留信息泄露，导致隐私保护和内容监管存在漏洞。

Method: 提出UMA框架，通过对抗性查询主动探测模型中残留的遗忘信息。

Result: 实验表明现有遗忘技术仍易受攻击，即使通过现有验证指标。

Conclusion: UMA为评估和提升机器遗忘安全性设立了新标准。

Abstract: Machine Unlearning (MUL) is crucial for privacy protection and content
regulation, yet recent studies reveal that traces of forgotten information
persist in unlearned models, enabling adversaries to resurface removed
knowledge. Existing verification methods only confirm whether unlearning was
executed, failing to detect such residual information leaks. To address this,
we introduce the concept of Robust Unlearning, ensuring models are
indistinguishable from retraining and resistant to adversarial recovery. To
empirically evaluate whether unlearning techniques meet this security standard,
we propose the Unlearning Mapping Attack (UMA), a post-unlearning verification
framework that actively probes models for forgotten traces using adversarial
queries. Extensive experiments on discriminative and generative tasks show that
existing unlearning techniques remain vulnerable, even when passing existing
verification metrics. By establishing UMA as a practical verification tool,
this study sets a new standard for assessing and enhancing machine unlearning
security.

</details>


### [359] [A Survey on Small Sample Imbalance Problem: Metrics, Feature Analysis, and Solutions](https://arxiv.org/abs/2504.14800)
*Shuxian Zhao,Jie Gui,Minjing Dong,Baosheng Yu,Zhipeng Gui,Lu Dong,Yuan Yan Tang,James Tin-Yau Kwok*

Main category: cs.LG

TL;DR: 论文提出了一种系统性分析框架，针对小样本不平衡（S&I）问题，总结了不平衡指标和复杂性分析方法，并比较了不同解决方案的效果。


<details>
  <summary>Details</summary>
Motivation: 小样本不平衡问题在机器学习和数据分析中是一个主要挑战，现有方法缺乏对数据特性的深入分析，需要从数据角度进行系统性研究。

Method: 提出系统性分析框架，总结不平衡指标和复杂性分析方法，并比较不同解决方案（如重采样）在二元和多类数据集上的效果。

Result: 实验表明，分类器性能差异远超过通过重采样带来的改进。

Conclusion: 论文强调了未解决的问题，并讨论了未来趋势，呼吁更多对数据特性的深入研究。

Abstract: The small sample imbalance (S&I) problem is a major challenge in machine
learning and data analysis. It is characterized by a small number of samples
and an imbalanced class distribution, which leads to poor model performance. In
addition, indistinct inter-class feature distributions further complicate
classification tasks. Existing methods often rely on algorithmic heuristics
without sufficiently analyzing the underlying data characteristics. We argue
that a detailed analysis from the data perspective is essential before
developing an appropriate solution. Therefore, this paper proposes a systematic
analytical framework for the S\&I problem. We first summarize imbalance metrics
and complexity analysis methods, highlighting the need for interpretable
benchmarks to characterize S&I problems. Second, we review recent solutions for
conventional, complexity-based, and extreme S&I problems, revealing
methodological differences in handling various data distributions. Our summary
finds that resampling remains a widely adopted solution. However, we conduct
experiments on binary and multiclass datasets, revealing that classifier
performance differences significantly exceed the improvements achieved through
resampling. Finally, this paper highlights open questions and discusses future
trends.

</details>


### [360] [Dynamic Contrastive Skill Learning with State-Transition Based Skill Clustering and Dynamic Length Adjustment](https://arxiv.org/abs/2504.14805)
*Jinwoo Choi,Seung-Woo Seo*

Main category: cs.LG

TL;DR: 提出了一种名为DCSL的动态对比技能学习框架，通过状态转移表示技能、学习技能相似性函数和动态调整技能长度，解决了现有方法在技能识别和灵活性上的不足。


<details>
  <summary>Details</summary>
Motivation: 强化学习在长时程任务中面临挑战，现有技能学习方法无法识别语义相似行为且技能长度固定，限制了灵活性和泛化能力。

Method: DCSL框架包含三个核心：基于状态转移的技能表示、技能相似性函数学习和动态技能长度调整，利用对比学习捕捉行为语义。

Result: DCSL在复杂或噪声数据中表现出更灵活和自适应的技能提取能力，任务完成和效率上优于现有方法。

Conclusion: DCSL通过改进技能表示和学习方式，提升了强化学习在复杂任务中的适应性和性能。

Abstract: Reinforcement learning (RL) has made significant progress in various domains,
but scaling it to long-horizon tasks with complex decision-making remains
challenging. Skill learning attempts to address this by abstracting actions
into higher-level behaviors. However, current approaches often fail to
recognize semantically similar behaviors as the same skill and use fixed skill
lengths, limiting flexibility and generalization. To address this, we propose
Dynamic Contrastive Skill Learning (DCSL), a novel framework that redefines
skill representation and learning. DCSL introduces three key ideas:
state-transition based skill representation, skill similarity function
learning, and dynamic skill length adjustment. By focusing on state transitions
and leveraging contrastive learning, DCSL effectively captures the semantic
context of behaviors and adapts skill lengths to match the appropriate temporal
extent of behaviors. Our approach enables more flexible and adaptive skill
extraction, particularly in complex or noisy datasets, and demonstrates
competitive performance compared to existing methods in task completion and
efficiency.

</details>


### [361] [A Basic Evaluation of Neural Networks Trained with the Error Diffusion Learning Algorithm](https://arxiv.org/abs/2504.14814)
*Kazuhisa Fujita*

Main category: cs.LG

TL;DR: 论文提出了一种生物启发的替代学习算法（EDLA），通过全局误差信号扩散训练神经网络，无需逐层反向传播，并在多个任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 反向传播算法缺乏生物合理性，因此研究开发了EDLA这种生物启发的替代方法。

Method: EDLA通过全局误差信号在成对的兴奋-抑制子层网络中扩散，避免了逐层反向传播。

Result: EDLA在奇偶校验、回归和图像分类任务中表现优异，性能受学习率、神经元数量和网络深度影响。

Conclusion: EDLA是一种有效的生物启发训练方法，未来可扩展用于生物启发神经网络。

Abstract: Artificial neural networks are powerful tools capable of addressing various
tasks. Although the backpropagation algorithm has become a standard training
method for these neural networks, its lack of biological plausibility has
inspired the development of alternative learning approaches. One such
alternative is Kaneko's Error Diffusion Learning Algorithm (EDLA), a
biologically motivated approach wherein a single global error signal diffuses
throughout a network composed of paired excitatory-inhibitory sublayers,
thereby eliminating the necessity for layer-wise backpropagation. This study
presents a contemporary formulation of the EDLA framework and evaluates its
effectiveness through parity check, regression, and image classification tasks.
Our experimental results indicate that EDLA networks can consistently achieve
high accuracy across these benchmarks, with performance efficiency and
convergence speed notably influenced by the choice of learning rate, neuron
count, and network depth. Further investigation of the internal representations
formed by EDLA networks reveals their capacity for meaningful feature
extraction, similar to traditional neural networks. These results suggest that
EDLA is a biologically motivated alternative for training feedforward networks
and will motivate future work on extending this method to biologically inspired
neural networks.

</details>


### [362] [What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale](https://arxiv.org/abs/2504.14815)
*Xiaoyong Yuan,Xiaolong Ma,Linke Guo,Lan Zhang*

Main category: cs.LG

TL;DR: PAIA是一种新型的扩散模型概念审计框架，无需优化提示或生成图像，直接分析模型内部行为，显著提高审计效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（DMs）的广泛共享引发伦理和法律问题，现有审计方法存在局限性，缺乏实用工具。

Method: 提出Prompt-Agnostic Image-Free Auditing (PAIA)，通过直接分析模型内部行为进行概念审计。

Result: 在320个控制模型和690个真实社区模型上，PAIA检测准确率超过90%，审计时间减少18-40倍。

Conclusion: PAIA是首个可扩展且实用的扩散模型预部署概念审计解决方案，为模型共享提供更安全、透明的基础。

Abstract: Diffusion models (DMs) have revolutionized text-to-image generation, enabling
the creation of highly realistic and customized images from text prompts. With
the rise of parameter-efficient fine-tuning (PEFT) techniques like LoRA, users
can now customize powerful pre-trained models using minimal computational
resources. However, the widespread sharing of fine-tuned DMs on open platforms
raises growing ethical and legal concerns, as these models may inadvertently or
deliberately generate sensitive or unauthorized content, such as copyrighted
material, private individuals, or harmful content. Despite the increasing
regulatory attention on generative AI, there are currently no practical tools
for systematically auditing these models before deployment. In this paper, we
address the problem of concept auditing: determining whether a fine-tuned DM
has learned to generate a specific target concept. Existing approaches
typically rely on prompt-based input crafting and output-based image
classification but suffer from critical limitations, including prompt
uncertainty, concept drift, and poor scalability. To overcome these challenges,
we introduce Prompt-Agnostic Image-Free Auditing (PAIA), a novel, model-centric
concept auditing framework. By treating the DM as the object of inspection,
PAIA enables direct analysis of internal model behavior, bypassing the need for
optimized prompts or generated images. We evaluate PAIA on 320 controlled model
and 690 real-world community models sourced from a public DM sharing platform.
PAIA achieves over 90% detection accuracy while reducing auditing time by
18-40x compared to existing baselines. To our knowledge, PAIA is the first
scalable and practical solution for pre-deployment concept auditing of
diffusion models, providing a practical foundation for safer and more
transparent diffusion model sharing.

</details>


### [363] [Uncertainty quantification of neural network models of evolving processes via Langevin sampling](https://arxiv.org/abs/2504.14854)
*Cosmin Safta,Reese E. Jones,Ravi G. Patel,Raelynn Wonnacot,Dan S. Bolintineanu,Craig M. Hamel,Sharlotte L. B. Kramer*

Main category: cs.LG

TL;DR: 提出了一种基于神经ODE的可扩展近似推理超网络框架，用于历史依赖过程，通过Langevin采样平衡计算成本，并在化学和材料物理数据上验证性能。


<details>
  <summary>Details</summary>
Motivation: 解决历史依赖过程的建模问题，同时提供灵活的计算预算平衡方法。

Method: 使用神经ODE表示内部状态演化，结合可训练观测模型，通过Langevin采样学习后验分布。

Result: 在化学反应和材料物理数据上表现优于均值场变分推理。

Conclusion: 该框架为历史依赖过程提供了一种高效且灵活的建模和推理方法。

Abstract: We propose a scalable, approximate inference hypernetwork framework for a
general model of history-dependent processes. The flexible data model is based
on a neural ordinary differential equation (NODE) representing the evolution of
internal states together with a trainable observation model subcomponent. The
posterior distribution corresponding to the data model parameters (weights and
biases) follows a stochastic differential equation with a drift term related to
the score of the posterior that is learned jointly with the data model
parameters. This Langevin sampling approach offers flexibility in balancing the
computational budget between the evaluation cost of the data model and the
approximation of the posterior density of its parameters. We demonstrate
performance of the hypernetwork on chemical reaction and material physics data
and compare it to mean-field variational inference.

</details>


### [364] [Impact of Latent Space Dimension on IoT Botnet Detection Performance: VAE-Encoder Versus ViT-Encoder](https://arxiv.org/abs/2504.14879)
*Hassan Wasswa,Aziida Nanyonga,Timothy Lynar*

Main category: cs.LG

TL;DR: 研究探讨了潜在维度对深度学习分类器性能的影响，比较了ViT和VAE编码器在IoT流量数据集上的表现，发现VAE表现更优。


<details>
  <summary>Details</summary>
Motivation: IoT设备数量激增，成为网络攻击的主要目标，安全性成为关键问题。研究旨在通过潜在维度分析提升分类器性能。

Method: 使用ViT和VAE编码器将高维IoT流量数据投影到低维潜在空间，并在N-BaIoT和CICIoT2022数据集上评估性能。

Result: VAE编码器在准确率、精确率、召回率和F1分数上均优于ViT编码器，因ViT无法有效学习数据中的空间模式。

Conclusion: VAE在IoT流量数据的潜在维度降维中表现更优，为IoT安全提供了更有效的解决方案。

Abstract: The rapid evolution of Internet of Things (IoT) technology has led to a
significant increase in the number of IoT devices, applications, and services.
This surge in IoT devices, along with their widespread presence, has made them
a prime target for various cyber-attacks, particularly through IoT botnets. As
a result, security has become a major concern within the IoT ecosystem. This
study focuses on investigating how the latent dimension impacts the performance
of different deep learning classifiers when trained on latent vector
representations of the train dataset. The primary objective is to compare the
outcomes of these models when encoder components from two cutting-edge
architectures: the Vision Transformer (ViT) and the Variational Auto-Encoder
(VAE) are utilized to project the high dimensional train dataset to the learned
low dimensional latent space. The encoder components are employed to project
high-dimensional structured .csv IoT botnet traffic datasets to various latent
sizes. Evaluated on N-BaIoT and CICIoT2022 datasets, findings reveal that
VAE-encoder based dimension reduction outperforms ViT-encoder based dimension
reduction for both datasets in terms of four performance metrics including
accuracy, precision, recall, and F1-score for all models which can be
attributed to absence of spatial patterns in the datasets the ViT model
attempts to learn and extract from image instances.

</details>


### [365] [Some Optimizers are More Equal: Understanding the Role of Optimizers in Group Fairness](https://arxiv.org/abs/2504.14882)
*Mojtaba Kolahdouzi,Hatice Gunes,Ali Etemad*

Main category: cs.LG

TL;DR: 研究优化算法选择对深度神经网络群体公平性的影响，发现自适应优化器（如RMSProp）比随机优化器（如SGD）更易收敛到公平解。


<details>
  <summary>Details</summary>
Motivation: 探讨优化算法如何影响模型公平性，特别是在数据不平衡情况下。

Method: 通过随机微分方程分析优化动态，并在CelebA、FairFace和MS-COCO数据集上进行实验验证。

Result: 自适应优化器（如RMSProp、Adam）在公平性指标上优于SGD，同时保持预测准确性。

Conclusion: 自适应优化机制是提升模型公平性的关键因素。

Abstract: We study whether and how the choice of optimization algorithm can impact
group fairness in deep neural networks. Through stochastic differential
equation analysis of optimization dynamics in an analytically tractable setup,
we demonstrate that the choice of optimization algorithm indeed influences
fairness outcomes, particularly under severe imbalance. Furthermore, we show
that when comparing two categories of optimizers, adaptive methods and
stochastic methods, RMSProp (from the adaptive category) has a higher
likelihood of converging to fairer minima than SGD (from the stochastic
category). Building on this insight, we derive two new theoretical guarantees
showing that, under appropriate conditions, RMSProp exhibits fairer parameter
updates and improved fairness in a single optimization step compared to SGD. We
then validate these findings through extensive experiments on three publicly
available datasets, namely CelebA, FairFace, and MS-COCO, across different
tasks as facial expression recognition, gender classification, and multi-label
classification, using various backbones. Considering multiple fairness
definitions including equalized odds, equal opportunity, and demographic
parity, adaptive optimizers like RMSProp and Adam consistently outperform SGD
in terms of group fairness, while maintaining comparable predictive accuracy.
Our results highlight the role of adaptive updates as a crucial yet overlooked
mechanism for promoting fair outcomes.

</details>


### [366] [Latent Bayesian Optimization via Autoregressive Normalizing Flows](https://arxiv.org/abs/2504.14889)
*Seunghun Lee,Jinyoung Park,Jaewon Chu,Minseo Yoon,Hyunwoo J. Kim*

Main category: cs.LG

TL;DR: 提出了一种基于归一化流的贝叶斯优化方法（NF-BO），解决了潜在贝叶斯优化（LBO）中的值差异问题，并在分子生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LBO方法因输入空间与潜在空间的重构间隙导致值差异问题，影响优化效果。

Method: 使用归一化流作为生成模型，建立输入空间与潜在空间的一对一编码函数及其左逆解码函数，消除重构间隙；提出SeqFlow（自回归归一化流）和动态调整探索概率的候选采样策略。

Result: 在分子生成任务中，NF-BO显著优于传统和近期LBO方法。

Conclusion: NF-BO通过消除重构间隙，有效解决了LBO中的值差异问题，提升了优化性能。

Abstract: Bayesian Optimization (BO) has been recognized for its effectiveness in
optimizing expensive and complex objective functions. Recent advancements in
Latent Bayesian Optimization (LBO) have shown promise by integrating generative
models such as variational autoencoders (VAEs) to manage the complexity of
high-dimensional and structured data spaces. However, existing LBO approaches
often suffer from the value discrepancy problem, which arises from the
reconstruction gap between input and latent spaces. This value discrepancy
problem propagates errors throughout the optimization process, leading to
suboptimal outcomes. To address this issue, we propose a Normalizing Flow-based
Bayesian Optimization (NF-BO), which utilizes normalizing flow as a generative
model to establish one-to-one encoding function from the input space to the
latent space, along with its left-inverse decoding function, eliminating the
reconstruction gap. Specifically, we introduce SeqFlow, an autoregressive
normalizing flow for sequence data. In addition, we develop a new candidate
sampling strategy that dynamically adjusts the exploration probability for each
token based on its importance. Through extensive experiments, our NF-BO method
demonstrates superior performance in molecule generation tasks, significantly
outperforming both traditional and recent LBO approaches.

</details>


### [367] [Dynamic Graph-Like Learning with Contrastive Clustering on Temporally-Factored Ship Motion Data for Imbalanced Sea State Estimation in Autonomous Vessel](https://arxiv.org/abs/2504.14907)
*Kexin Wang,Mengna Liu,Xu Cheng,Fan Shi,Shanshan Qi,Shengyong Chen*

Main category: cs.LG

TL;DR: TGC-SSE是一种新型深度学习模型，通过时间维度分解、动态图学习和对比聚类损失函数解决数据冗余和类别不平衡问题，显著提升了海况估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在船舶运动数据中存在数据不平衡和特征冗余问题，限制了海况估计的效果。

Method: TGC-SSE结合了时间维度分解模块、动态图学习模块和对比聚类损失函数。

Result: 在14个公开数据集上表现优异，9个数据集准确率最高，比EDI提升20.79%。

Conclusion: TGC-SSE不仅提高了海况估计的准确性，还具有强泛化能力，为自主船舶操作提供了可靠支持。

Abstract: Accurate sea state estimation is crucial for the real-time control and future
state prediction of autonomous vessels. However, traditional methods struggle
with challenges such as data imbalance and feature redundancy in ship motion
data, limiting their effectiveness. To address these challenges, we propose the
Temporal-Graph Contrastive Clustering Sea State Estimator (TGC-SSE), a novel
deep learning model that combines three key components: a time dimension
factorization module to reduce data redundancy, a dynamic graph-like learning
module to capture complex variable interactions, and a contrastive clustering
loss function to effectively manage class imbalance. Our experiments
demonstrate that TGC-SSE significantly outperforms existing methods across 14
public datasets, achieving the highest accuracy in 9 datasets, with a 20.79%
improvement over EDI. Furthermore, in the field of sea state estimation,
TGC-SSE surpasses five benchmark methods and seven deep learning models.
Ablation studies confirm the effectiveness of each module, demonstrating their
respective roles in enhancing overall model performance. Overall, TGC-SSE not
only improves the accuracy of sea state estimation but also exhibits strong
generalization capabilities, providing reliable support for autonomous vessel
operations.

</details>


### [368] [POLYRAG: Integrating Polyviews into Retrieval-Augmented Generation for Medical Applications](https://arxiv.org/abs/2504.14917)
*Chunjing Gan,Dan Yang,Binbin Hu,Ziqi Liu,Yue Shen,Zhiqiang Zhang,Jian Wang,Jun Zhou*

Main category: cs.LG

TL;DR: 论文提出PolyRAG方法，通过多视角整合提升检索增强生成在医疗场景中的性能，并引入PolyEVAL基准验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法未考虑检索信息的时效性、权威性和共性，限制了其在医疗场景中的应用。

Method: 提出PolyRAG，整合多视角判断以优化检索增强生成，并设计PolyEVAL基准进行验证。

Result: 实验表明PolyRAG在医疗应用中表现优越。

Conclusion: PolyRAG通过多视角整合显著提升了检索增强生成的效果，PolyEVAL为相关研究提供了实用基准。

Abstract: Large language models (LLMs) have become a disruptive force in the industry,
introducing unprecedented capabilities in natural language processing, logical
reasoning and so on. However, the challenges of knowledge updates and
hallucination issues have limited the application of LLMs in medical scenarios,
where retrieval-augmented generation (RAG) can offer significant assistance.
Nevertheless, existing retrieve-then-read approaches generally digest the
retrieved documents, without considering the timeliness, authoritativeness and
commonality of retrieval. We argue that these approaches can be suboptimal,
especially in real-world applications where information from different sources
might conflict with each other and even information from the same source in
different time scale might be different, and totally relying on this would
deteriorate the performance of RAG approaches. We propose PolyRAG that
carefully incorporate judges from different perspectives and finally integrate
the polyviews for retrieval augmented generation in medical applications. Due
to the scarcity of real-world benchmarks for evaluation, to bridge the gap we
propose PolyEVAL, a benchmark consists of queries and documents collected from
real-world medical scenarios (including medical policy, hospital & doctor
inquiry and healthcare) with multiple tagging (e.g., timeliness,
authoritativeness) on them. Extensive experiments and analysis on PolyEVAL have
demonstrated the superiority of PolyRAG.

</details>


### [369] [Causal DAG Summarization (Full Version)](https://arxiv.org/abs/2504.14937)
*Anna Zeng,Michael Cafarella,Batya Kenig,Markos Markakis,Brit Youngmann,Babak Salimi*

Main category: cs.LG

TL;DR: 论文提出了一种因果图摘要方法，通过简化因果DAG以提高可理解性，同时保留关键因果信息，确保可靠的因果推断。


<details>
  <summary>Details</summary>
Motivation: 高维数据的因果DAG过于复杂，难以人工验证，现有通用图摘要方法不适用于因果DAG摘要。

Method: 提出因果图摘要目标，开发高效贪心算法，生成摘要DAG用于直接推断。

Result: 在六个真实数据集上验证，新算法优于现有方法，能处理高维数据并确保推断可靠性。

Conclusion: 摘要因果DAG能提升因果推断的鲁棒性，减少假设错误的影响。

Abstract: Causal inference aids researchers in discovering cause-and-effect
relationships, leading to scientific insights. Accurate causal estimation
requires identifying confounding variables to avoid false discoveries. Pearl's
causal model uses causal DAGs to identify confounding variables, but incorrect
DAGs can lead to unreliable causal conclusions. However, for high dimensional
data, the causal DAGs are often complex beyond human verifiability. Graph
summarization is a logical next step, but current methods for general-purpose
graph summarization are inadequate for causal DAG summarization. This paper
addresses these challenges by proposing a causal graph summarization objective
that balances graph simplification for better understanding while retaining
essential causal information for reliable inference. We develop an efficient
greedy algorithm and show that summary causal DAGs can be directly used for
inference and are more robust to misspecification of assumptions, enhancing
robustness for causal inference. Experimenting with six real-life datasets, we
compared our algorithm to three existing solutions, showing its effectiveness
in handling high-dimensional data and its ability to generate summary DAGs that
ensure both reliable causal inference and robustness against misspecifications.

</details>


### [370] [Learning to Reason under Off-Policy Guidance](https://arxiv.org/abs/2504.14945)
*Jianhao Yan,Yafu Li,Zican Hu,Zhi Wang,Ganqu Cui,Xiaoye Qu,Yu Cheng,Yue Zhang*

Main category: cs.LG

TL;DR: LUFFY框架通过结合离策略推理轨迹和策略内训练，显著提升了推理模型的性能，尤其在泛化能力上表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有零强化学习方法局限于策略内训练，无法超越初始能力，需要一种新方法来结合离策略推理以提升性能。

Method: LUFFY框架动态平衡模仿与探索，通过正则化重要性采样避免浅层模仿，结合离策略演示和策略内训练。

Result: 在六个数学基准测试中平均提升7.0分，在分布外任务中优势超过6.2分，显著优于基于模仿的监督微调。

Conclusion: LUFFY不仅有效模仿，还能超越演示进行探索，为训练可泛化的推理模型提供了可扩展的路径。

Abstract: Recent advances in large reasoning models (LRMs) demonstrate that
sophisticated behaviors such as multi-step reasoning and self-reflection can
emerge via reinforcement learning (RL) with simple rule-based rewards. However,
existing zero-RL approaches are inherently ``on-policy'', limiting learning to
a model's own outputs and failing to acquire reasoning abilities beyond its
initial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY
guidance), a framework that augments zero-RL with off-policy reasoning traces.
LUFFY dynamically balances imitation and exploration by combining off-policy
demonstrations with on-policy rollouts during training. Notably, we propose
policy shaping via regularized importance sampling to avoid superficial and
rigid imitation during mixed-policy training. Remarkably, LUFFY achieves an
over +7.0 average gain across six math benchmarks and an advantage of over +6.2
points in out-of-distribution tasks. It also substantially surpasses
imitation-based supervised fine-tuning (SFT), particularly in generalization.
Analysis shows LUFFY not only imitates effectively but also explores beyond
demonstrations, offering a scalable path to train generalizable reasoning
models with off-policy guidance.

</details>


### [371] [Symmetry-Preserving Architecture for Multi-NUMA Environments (SPANE): A Deep Reinforcement Learning Approach for Dynamic VM Scheduling](https://arxiv.org/abs/2504.14946)
*Tin Ping Chan,Yunlong Cheng,Yizhan Zhu,Xiaofeng Gao,Guihai Chen*

Main category: cs.LG

TL;DR: 论文提出动态虚拟机分配问题（DVAMP）及其解决方案SPANE，通过深度强化学习优化多NUMA环境下的虚拟机调度，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着云计算的发展，多NUMA架构的采用带来了虚拟机调度的新挑战，需要更准确的解决方案。

Method: 定义DVAMP为混合整数线性规划问题，提出SPANE方法，利用对称性提升学习效率和解决方案质量。

Result: SPANE在实验中表现优异，平均虚拟机等待时间减少45%。

Conclusion: 研究为多NUMA环境下的虚拟机调度提供了理论和实践解决方案，填补了文献空白并提升了实际系统性能。

Abstract: As cloud computing continues to evolve, the adoption of multi-NUMA
(Non-Uniform Memory Access) architecture by cloud service providers has
introduced new challenges in virtual machine (VM) scheduling. To address these
challenges and more accurately reflect the complexities faced by modern cloud
environments, we introduce the Dynamic VM Allocation problem in Multi-NUMA PM
(DVAMP). We formally define both offline and online versions of DVAMP as
mixed-integer linear programming problems, providing a rigorous mathematical
foundation for analysis. A tight performance bound for greedy online algorithms
is derived, offering insights into the worst-case optimality gap as a function
of the number of physical machines and VM lifetime variability. To address the
challenges posed by DVAMP, we propose SPANE (Symmetry-Preserving Architecture
for Multi-NUMA Environments), a novel deep reinforcement learning approach that
exploits the problem's inherent symmetries. SPANE produces invariant results
under arbitrary permutations of physical machine states, enhancing learning
efficiency and solution quality. Extensive experiments conducted on the
Huawei-East-1 dataset demonstrate that SPANE outperforms existing baselines,
reducing average VM wait time by 45%. Our work contributes to the field of
cloud resource management by providing both theoretical insights and practical
solutions for VM scheduling in multi-NUMA environments, addressing a critical
gap in the literature and offering improved performance for real-world cloud
systems.

</details>


### [372] [Efficient Document Retrieval with G-Retriever](https://arxiv.org/abs/2504.14955)
*Manthankumar Solanki*

Main category: cs.LG

TL;DR: 本文提出了一种改进的基于注意力的子图构建方法，替代了原有的PCST方法，并结合节点和边属性编码，提升了文本问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法仅关注节点属性，导致上下文理解不完整，因此需要一种更高效的上下文感知检索方法。

Method: 采用基于注意力的子图构建技术，编码节点和边属性，并引入改进的投影层和多头注意力池化以更好地与LLMs对齐。

Result: 在WebQSP数据集上的实验表明，该方法性能优于原方法，实现了更准确的问答效果。

Conclusion: 提出的方法在文本问答任务中表现出色，具有更高的准确性和上下文感知能力。

Abstract: Textual data question answering has gained significant attention due to its
growing applicability. Recently, a novel approach leveraging the
Retrieval-Augmented Generation (RAG) method was introduced, utilizing the
Prize-Collecting Steiner Tree (PCST) optimization for sub-graph construction.
However, this method focused solely on node attributes, leading to incomplete
contextual understanding. In this paper, we propose an enhanced approach that
replaces the PCST method with an attention-based sub-graph construction
technique, enabling more efficient and context-aware retrieval. Additionally,
we encode both node and edge attributes, leading to richer graph
representations. Our method also incorporates an improved projection layer and
multi-head attention pooling for better alignment with Large Language Models
(LLMs). Experimental evaluations on the WebQSP dataset demonstrate that our
approach is competitive and achieves marginally better results compared to the
original method, underscoring its potential for more accurate question
answering.

</details>


### [373] [MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient Large-Scale MoE Model Training with Megatron Core](https://arxiv.org/abs/2504.14960)
*Dennis Liu,Zijie Yan,Xin Yao,Tong Liu,Vijay Korthikanti,Evan Wu,Shiqing Fan,Gao Deng,Hongxiao Bai,Ashwath Aithal,Michael Andersch,Mohammad Shoeybi,Jiajie Yao,Chandler Zhou,David Wu,Xipeng Li,June Yang*

Main category: cs.LG

TL;DR: 提出了一种用于大规模MoE模型训练的五维混合并行框架，通过MoE Parallel Folding和灵活的令牌级调度器显著提升了训练效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有并行策略在大规模MoE模型训练中存在局限性，亟需一种高效的端到端训练框架。

Method: 采用五维混合并行（张量、专家、上下文、数据和流水线并行），结合MoE Parallel Folding和动态令牌调度器。

Result: 在Mixtral 8x22B和Qwen2-57B-A14B模型上分别达到49.3%和39.0%的MFU，支持128K令牌序列长度和1024 GPU扩展。

Conclusion: 该框架有效解决了大规模MoE模型训练的挑战，显著提升了效率和性能。

Abstract: Mixture of Experts (MoE) models enhance neural network scalability by
dynamically selecting relevant experts per input token, enabling larger model
sizes while maintaining manageable computation costs. However, efficient
training of large-scale MoE models across thousands of GPUs presents
significant challenges due to limitations in existing parallelism strategies.
We introduce an end-to-end training framework for large-scale MoE models that
utilizes five-dimensional hybrid parallelism: Tensor Parallelism, Expert
Parallelism, Context Parallelism, Data Parallelism, and Pipeline Parallelism.
Central to our approach is MoE Parallel Folding, a novel strategy that
decouples the parallelization of attention and MoE layers in Transformer
models, allowing each layer type to adopt optimal parallel configurations.
Additionally, we develop a flexible token-level dispatcher that supports both
token-dropping and token-dropless MoE training across all five dimensions of
parallelism. This dispatcher accommodates dynamic tensor shapes and coordinates
different parallelism schemes for Attention and MoE layers, facilitating
complex parallelism implementations. Our experiments demonstrate significant
improvements in training efficiency and scalability. We achieve up to 49.3%
Model Flops Utilization (MFU) for the Mixtral 8x22B model and 39.0% MFU for the
Qwen2-57B-A14B model on H100 GPUs, outperforming existing methods. The
framework scales efficiently up to 1,024 GPUs and maintains high performance
with sequence lengths up to 128K tokens, validating its effectiveness for
large-scale MoE model training. The code is available in Megatron-Core.

</details>


### [374] [Learning Compositional Transferability of Time Series for Source-Free Domain Adaptation](https://arxiv.org/abs/2504.14994)
*Hankang Sun,Guiming Li,Su Yang,Baoqi Li*

Main category: cs.LG

TL;DR: 该论文提出了一种针对时间序列分类的源自由域适应方法，通过解构域可转移性并使用组合架构进行时间序列重建，实现了最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列分类中源自由域适应的挑战，即在目标标签和源数据均不可访问的情况下，如何利用预训练的分类骨干网络。

Method: 采用组合架构，包括冻结的U-net作为初始重建组件，以及两个并行分支（源重放分支和偏移补偿分支）进行精细适应，通过可学习因子调整组合输出。

Result: 在三个广泛使用的基准测试中实现了最先进的性能。

Conclusion: 组合架构有效解构了域可转移性，保留了从源数据中学到的重建能力，并适应了域变化的时序模式。

Abstract: Domain adaptation is challenging for time series classification due to the
highly dynamic nature. This study tackles the most difficult subtask when both
target labels and source data are inaccessible, namely, source-free domain
adaptation. To reuse the classification backbone pre-trained on source data,
time series reconstruction is a sound solution that aligns target and source
time series by minimizing the reconstruction errors of both. However, simply
fine-tuning the source pre-trained reconstruction model on target data may lose
the learnt priori, and it struggles to accommodate domain varying temporal
patterns in a single encoder-decoder. Therefore, this paper tries to
disentangle the composition of domain transferability by using a compositional
architecture for time series reconstruction. Here, the preceding component is a
U-net frozen since pre-trained, the output of which during adaptation is the
initial reconstruction of a given target time series, acting as a coarse step
to prompt the subsequent finer adaptation. The following pipeline for finer
adaptation includes two parallel branches: The source replay branch using a
residual link to preserve the output of U-net, and the offset compensation
branch that applies an additional autoencoder (AE) to further warp U-net's
output. By deploying a learnable factor on either branch to scale their
composition in the final output of reconstruction, the data transferability is
disentangled and the learnt reconstructive capability from source data is
retained. During inference, aside from the batch-level optimization in the
training, we search at test time stability-aware rescaling of source replay
branch to tolerate instance-wise variation. The experimental results show that
such compositional architecture of time series reconstruction leads to SOTA
performance on 3 widely used benchmarks.

</details>


### [375] [A Call for New Recipes to Enhance Spatial Reasoning in MLLMs](https://arxiv.org/abs/2504.15037)
*Huanyu Zhang,Chengzu Li,Wenshan Wu,Shaoguang Mao,Yan xia,Ivan Vulić,Zhang Zhang,Liang Wang,Tieniu Tan,Furu Wei*

Main category: cs.LG

TL;DR: 多模态大语言模型（MLLMs）在视觉语言任务中表现优异，但其空间推理能力存在明显不足，限制了实际应用。本文提出需专门改进MLLMs的开发方法，而非仅依赖现有架构的扩展。


<details>
  <summary>Details</summary>
Motivation: MLLMs的空间推理能力不足限制了其在物理世界中的交互能力，需针对性改进。

Method: 建立MLLMs空间推理的框架，分析当前方法中从训练数据到推理机制的各个组成部分对空间推理的影响。

Result: 揭示了当前方法的局限性，并指出了改进的潜在方向。

Conclusion: 呼吁AI研究社区关注MLLMs的空间推理能力，推动实现类人水平的空间推理。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
performance in general vision-language tasks. However, recent studies have
exposed critical limitations in their spatial reasoning capabilities. This
deficiency in spatial reasoning significantly constrains MLLMs' ability to
interact effectively with the physical world, thereby limiting their broader
applications. We argue that spatial reasoning capabilities will not naturally
emerge from merely scaling existing architectures and training methodologies.
Instead, this challenge demands dedicated attention to fundamental
modifications in the current MLLM development approach. In this position paper,
we first establish a comprehensive framework for spatial reasoning within the
context of MLLMs. We then elaborate on its pivotal role in real-world
applications. Through systematic analysis, we examine how individual components
of the current methodology-from training data to reasoning mechanisms-influence
spatial reasoning capabilities. This examination reveals critical limitations
while simultaneously identifying promising avenues for advancement. Our work
aims to direct the AI research community's attention toward these crucial yet
underexplored aspects. By highlighting these challenges and opportunities, we
seek to catalyze progress toward achieving human-like spatial reasoning
capabilities in MLLMs.

</details>


### [376] [VeLU: Variance-enhanced Learning Unit for Deep Neural Networks](https://arxiv.org/abs/2504.15051)
*Ashkan Shakarami,Yousef Yeganeh,Azade Farshad,Lorenzo Nicolè,Stefano Ghidoni,Nassir Navab*

Main category: cs.LG

TL;DR: VeLU是一种基于输入方差动态调整的激活函数，通过ArcTan-Sin变换和Wasserstein-2正则化，解决了ReLU及其替代品的问题，并在多个视觉基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: ReLU虽然简单但存在梯度消失和缺乏适应性的问题，而Swish和GELU等替代品无法动态适应输入统计特性。

Method: 提出VeLU，结合ArcTan-Sin变换和Wasserstein-2正则化，动态调整输入方差。

Result: 在ViT_B16等模型和六个视觉基准测试中，VeLU优于ReLU、Swish和GELU。

Conclusion: VeLU通过动态调整输入方差，有效缓解协变量偏移并稳定优化，是一种更优的激活函数。

Abstract: Activation functions are fundamental in deep neural networks and directly
impact gradient flow, optimization stability, and generalization. Although ReLU
remains standard because of its simplicity, it suffers from vanishing gradients
and lacks adaptability. Alternatives like Swish and GELU introduce smooth
transitions, but fail to dynamically adjust to input statistics. We propose
VeLU, a Variance-enhanced Learning Unit as an activation function that
dynamically scales based on input variance by integrating ArcTan-Sin
transformations and Wasserstein-2 regularization, effectively mitigating
covariate shifts and stabilizing optimization. Extensive experiments on
ViT_B16, VGG19, ResNet50, DenseNet121, MobileNetV2, and EfficientNetB3 confirm
VeLU's superiority over ReLU, ReLU6, Swish, and GELU on six vision benchmarks.
The codes of VeLU are publicly available on GitHub.

</details>


### [377] [Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL](https://arxiv.org/abs/2504.15077)
*Simone Papicchio,Simone Rossi,Luca Cagliero,Paolo Papotti*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型（LLMs）在Text2SQL任务中推理能力对性能的影响，比较了零样本学习、监督微调、强化学习及其组合的效果。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在Text2SQL任务中表现优异，但小模型在复杂查询和多表场景下表现不佳，推理能力的影响尚未充分探索。

Method: 研究了四种LLM设置：零样本学习（带/不带通用推理）、监督微调（带/不带任务特定推理痕迹）、强化学习（以执行准确率为奖励）、以及SFT+RL两阶段方法。

Result: 通用推理在零样本学习中效果有限；小模型通过带推理的SFT显著提升；强化学习普遍有效，尤其在多跳推理和多表查询中；SFT+RL的小模型在复杂数据集上表现优异。

Conclusion: 推理能力对Text2SQL性能至关重要，尤其是强化学习的引入显著提升了小模型的表现，使其在某些任务上媲美更大模型。

Abstract: Large Language Models (LLMs) have shown impressive capabilities in
transforming natural language questions about relational databases into SQL
queries. Despite recent improvements, small LLMs struggle to handle questions
involving multiple tables and complex SQL patterns under a Zero-Shot Learning
(ZSL) setting. Supervised Fine-Tuning (SFT) partially compensate the knowledge
deficits in pretrained models but falls short while dealing with queries
involving multi-hop reasoning. To bridge this gap, different LLM training
strategies to reinforce reasoning capabilities have been proposed, ranging from
leveraging a thinking process within ZSL, including reasoning traces in SFT, or
adopt Reinforcement Learning (RL) strategies. However, the influence of
reasoning on Text2SQL performance is still largely unexplored. This paper
investigates to what extent LLM reasoning capabilities influence their Text2SQL
performance on four benchmark datasets. To this end, it considers the following
LLM settings: (1) ZSL, including general-purpose reasoning or not; (2) SFT,
with and without task-specific reasoning traces; (3) RL, leveraging execution
accuracy as primary reward function; (4) SFT+RL, i.e, a two-stage approach that
combines SFT and RL. The results show that general-purpose reasoning under ZSL
proves to be ineffective in tackling complex Text2SQL cases. Small LLMs benefit
from SFT with reasoning much more than larger ones, bridging the gap of their
(weaker) model pretraining. RL is generally beneficial across all tested models
and datasets, particularly when SQL queries involve multi-hop reasoning and
multiple tables. Small LLMs with SFT+RL excel on most complex datasets thanks
to a strategic balance between generality of the reasoning process and
optimization of the execution accuracy. Thanks to RL, the7B Qwen-Coder-2.5
model performs on par with 100+ Billion ones on the Bird dataset.

</details>


### [378] [Federated Latent Factor Model for Bias-Aware Recommendation with Privacy-Preserving](https://arxiv.org/abs/2504.15090)
*Junxiang Gao,Yixin Ran,Jia Chen*

Main category: cs.LG

TL;DR: 论文提出了一种联邦偏置感知潜在因子（FBALF）模型，用于解决联邦推荐系统中的评分偏置问题，同时保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统在中央服务器处理用户数据，存在隐私泄露风险。联邦学习虽能保护隐私，但无法直接分析原始数据以消除评分偏置。

Method: FBALF模型在局部模型的损失函数中显式引入训练偏置，从而在不泄露数据的情况下消除评分偏置。

Result: 在三个真实数据集上的实验表明，FBALF的推荐准确性显著优于其他先进的联邦推荐系统。

Conclusion: FBALF模型有效解决了联邦推荐系统中的评分偏置问题，同时确保了数据隐私。

Abstract: A recommender system (RS) aims to provide users with personalized item
recommendations, enhancing their overall experience. Traditional RSs collect
and process all user data on a central server. However, this centralized
approach raises significant privacy concerns, as it increases the risk of data
breaches and privacy leakages, which are becoming increasingly unacceptable to
privacy-sensitive users. To address these privacy challenges, federated
learning has been integrated into RSs, ensuring that user data remains secure.
In centralized RSs, the issue of rating bias is effectively addressed by
jointly analyzing all users' raw interaction data. However, this becomes a
significant challenge in federated RSs, as raw data is no longer accessible due
to privacy-preserving constraints. To overcome this problem, we propose a
Federated Bias-Aware Latent Factor (FBALF) model. In FBALF, training bias is
explicitly incorporated into every local model's loss function, allowing for
the effective elimination of rating bias without compromising data privacy.
Extensive experiments conducted on three real-world datasets demonstrate that
FBALF achieves significantly higher recommendation accuracy compared to other
state-of-the-art federated RSs.

</details>


### [379] [Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training of GAN](https://arxiv.org/abs/2504.15099)
*Lin Wang,Xiancheng Wang,Rui Wang,Zhibo Zhang,Minghang Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种名为FSCO的新型智能优化器，通过强化学习控制GANs的训练步长，提高训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统GANs训练对数据和超参数敏感，易导致振荡或收敛失败，尤其是在数据方差较大时。

Method: 采用强化学习技术，通过智能代理动态调整训练步长和学习率，减少对步长的敏感性。

Result: 在三个基准数据集上验证了FSCO的有效性，提高了训练稳定性。

Conclusion: FSCO通过智能优化显著改善了GANs的训练过程，减少了收敛问题。

Abstract: Up to now, the training processes of typical Generative Adversarial Networks
(GANs) are still particularly sensitive to data properties and hyperparameters,
which may lead to severe oscillations, difficulties in convergence, or even
failures to converge, especially when the overall variances of the training
sets are large. These phenomena are often attributed to the training
characteristics of such networks. Aiming at the problem, this paper develops a
new intelligent optimizer, Fast-Slow Co-advancing Optimizer (FSCO), which
employs reinforcement learning in the training process of GANs to make training
easier. Specifically, this paper allows the training step size to be controlled
by an agent to improve training stability, and makes the training process more
intelligent with variable learning rates, making GANs less sensitive to step
size. Experiments have been conducted on three benchmark datasets to verify the
effectiveness of the developed FSCO.

</details>


### [380] [Kolmogorov-Arnold Networks: Approximation and Learning Guarantees for Functions and their Derivatives](https://arxiv.org/abs/2504.15110)
*Anastasis Kratsios,Takashi Furuya*

Main category: cs.LG

TL;DR: Kolmogorov-Arnold Networks (KANs) 是一种改进的深度学习框架，通过可训练的样条激活函数提供更强的适应性。本文证明了 KAN 在 Besov 空间中的最优逼近能力，并提供了无维度的样本复杂度估计。


<details>
  <summary>Details</summary>
Motivation: 受 Kolmogorov-Arnold 叠加定理启发，探索 KAN 的理论基础，证明其在 Besov 空间中的最优逼近能力。

Method: 利用残差连接和可训练的样条激活函数构建 KAN 架构，分析其在 Besov 空间中的逼近性能。

Result: KAN 可以在有界或分形域上以最优速率逼近 Besov 函数，并提供了无维度的样本复杂度估计。

Conclusion: KAN 是一种强大的深度学习框架，具有理论保证的逼近能力和适应性。

Abstract: Inspired by the Kolmogorov-Arnold superposition theorem, Kolmogorov-Arnold
Networks (KANs) have recently emerged as an improved backbone for most deep
learning frameworks, promising more adaptivity than their multilayer perception
(MLP) predecessor by allowing for trainable spline-based activation functions.
In this paper, we probe the theoretical foundations of the KAN architecture by
showing that it can optimally approximate any Besov function in
$B^{s}_{p,q}(\mathcal{X})$ on a bounded open, or even fractal, domain
$\mathcal{X}$ in $\mathbb{R}^d$ at the optimal approximation rate with respect
to any weaker Besov norm $B^{\alpha}_{p,q}(\mathcal{X})$; where $\alpha < s$.
We complement our approximation guarantee with a dimension-free estimate on the
sample complexity of a residual KAN model when learning a function of Besov
regularity from $N$ i.i.d. noiseless samples. Our KAN architecture incorporates
contemporary deep learning wisdom by leveraging residual/skip connections
between layers.

</details>


### [381] [Survey of Loss Augmented Knowledge Tracing](https://arxiv.org/abs/2504.15163)
*Altun Shukurlu*

Main category: cs.LG

TL;DR: 论文探讨了损失函数在人工神经网络训练中的重要性，并综述了深度学习知识追踪算法及其改进。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过改进损失函数提升知识追踪算法的性能和鲁棒性。

Method: 综述了基于深度学习的知识追踪算法，特别是对比学习技术，如Bi-CLKT、CL4KT等。

Result: 提供了性能基准和实际部署挑战的见解。

Conclusion: 未来研究方向包括混合损失策略和上下文感知建模。

Abstract: The training of artificial neural networks is heavily dependent on the
careful selection of an appropriate loss function. While commonly used loss
functions, such as cross-entropy and mean squared error (MSE), generally
suffice for a broad range of tasks, challenges often emerge due to limitations
in data quality or inefficiencies within the learning process. In such
circumstances, the integration of supplementary terms into the loss function
can serve to address these challenges, enhancing both model performance and
robustness. Two prominent techniques, loss regularization and contrastive
learning, have been identified as effective strategies for augmenting the
capacity of loss functions in artificial neural networks.
  Knowledge tracing is a compelling area of research that leverages predictive
artificial intelligence to facilitate the automation of personalized and
efficient educational experiences for students. In this paper, we provide a
comprehensive review of the deep learning-based knowledge tracing (DKT)
algorithms trained using advanced loss functions and discuss their improvements
over prior techniques. We discuss contrastive knowledge tracing algorithms,
such as Bi-CLKT, CL4KT, SP-CLKT, CoSKT, and prediction-consistent DKT,
providing performance benchmarks and insights into real-world deployment
challenges. The survey concludes with future research directions, including
hybrid loss strategies and context-aware modeling.

</details>


### [382] [Audio-Visual Class-Incremental Learning for Fish Feeding intensity Assessment in Aquaculture](https://arxiv.org/abs/2504.15171)
*Meng Cui,Xianghu Yue,Xinyuan Qian,Jinzheng Zhao,Haohe Liu,Xubo Liu,Daoliang Li,Wenwu Wang*

Main category: cs.LG

TL;DR: 论文提出了一种新的数据集AV-CIL-FFIA和框架HAIL-FFIA，用于解决鱼类摄食强度评估中的多模态适应性问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法在适应新鱼种或环境时面临灾难性遗忘和数据集不足的问题。

Method: 引入AV-CIL-FFIA数据集，并提出HAIL-FFIA框架，采用原型化分层表示学习和动态模态平衡系统。

Result: HAIL-FFIA在AV-CIL-FFIA上表现优于现有方法，准确率更高且存储需求更低。

Conclusion: HAIL-FFIA有效解决了灾难性遗忘问题，为鱼类摄食强度评估提供了高效解决方案。

Abstract: Fish Feeding Intensity Assessment (FFIA) is crucial in industrial aquaculture
management. Recent multi-modal approaches have shown promise in improving FFIA
robustness and efficiency. However, these methods face significant challenges
when adapting to new fish species or environments due to catastrophic
forgetting and the lack of suitable datasets. To address these limitations, we
first introduce AV-CIL-FFIA, a new dataset comprising 81,932 labelled
audio-visual clips capturing feeding intensities across six different fish
species in real aquaculture environments. Then, we pioneer audio-visual class
incremental learning (CIL) for FFIA and demonstrate through benchmarking on
AV-CIL-FFIA that it significantly outperforms single-modality methods. Existing
CIL methods rely heavily on historical data. Exemplar-based approaches store
raw samples, creating storage challenges, while exemplar-free methods avoid
data storage but struggle to distinguish subtle feeding intensity variations
across different fish species. To overcome these limitations, we introduce
HAIL-FFIA, a novel audio-visual class-incremental learning framework that
bridges this gap with a prototype-based approach that achieves exemplar-free
efficiency while preserving essential knowledge through compact feature
representations. Specifically, HAIL-FFIA employs hierarchical representation
learning with a dual-path knowledge preservation mechanism that separates
general intensity knowledge from fish-specific characteristics. Additionally,
it features a dynamic modality balancing system that adaptively adjusts the
importance of audio versus visual information based on feeding behaviour
stages. Experimental results show that HAIL-FFIA is superior to SOTA methods on
AV-CIL-FFIA, achieving higher accuracy with lower storage needs while
effectively mitigating catastrophic forgetting in incremental fish species
learning.

</details>


### [383] [How Global Calibration Strengthens Multiaccuracy](https://arxiv.org/abs/2504.15206)
*Sílvia Casacuberta,Parikshit Gopalan,Varun Kanade,Omer Reingold*

Main category: cs.LG

TL;DR: 本文研究了多准确性和多校准性作为学习原语的能力，发现多准确性本身较弱，但结合全局校准后能力显著提升。


<details>
  <summary>Details</summary>
Motivation: 探索多准确性和多校准性在多组公平性预测中的互补作用及其学习能力。

Method: 通过理论分析，比较多准确性和校准多准确性在弱学习、强学习以及硬核度量生成中的表现。

Result: 多准确性单独使用时能力有限，但结合校准后可恢复强学习能力，并生成最优密度的硬核度量。

Conclusion: 多准确性和全局校准在多组公平性预测中具有互补作用，结合后能显著提升学习能力。

Abstract: Multiaccuracy and multicalibration are multigroup fairness notions for
prediction that have found numerous applications in learning and computational
complexity. They can be achieved from a single learning primitive: weak
agnostic learning. Here we investigate the power of multiaccuracy as a learning
primitive, both with and without the additional assumption of calibration. We
find that multiaccuracy in itself is rather weak, but that the addition of
global calibration (this notion is called calibrated multiaccuracy) boosts its
power substantially, enough to recover implications that were previously known
only assuming the stronger notion of multicalibration.
  We give evidence that multiaccuracy might not be as powerful as standard weak
agnostic learning, by showing that there is no way to post-process a
multiaccurate predictor to get a weak learner, even assuming the best
hypothesis has correlation $1/2$. Rather, we show that it yields a restricted
form of weak agnostic learning, which requires some concept in the class to
have correlation greater than $1/2$ with the labels. However, by also requiring
the predictor to be calibrated, we recover not just weak, but strong agnostic
learning.
  A similar picture emerges when we consider the derivation of hardcore
measures from predictors satisfying multigroup fairness notions. On the one
hand, while multiaccuracy only yields hardcore measures of density half the
optimal, we show that (a weighted version of) calibrated multiaccuracy achieves
optimal density.
  Our results yield new insights into the complementary roles played by
multiaccuracy and calibration in each setting. They shed light on why
multiaccuracy and global calibration, although not particularly powerful by
themselves, together yield considerably stronger notions.

</details>


### [384] [Compute-Optimal LLMs Provably Generalize Better With Scale](https://arxiv.org/abs/2504.15208)
*Marc Finzi,Sanyam Kapoor,Diego Granziol,Anming Gu,Christopher De Sa,J. Zico Kolter,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 论文研究了为什么更大的语言模型泛化能力更强，通过计算最优条件下的泛化边界，发现模型规模增大时损失方差和量化误差减小，从而泛化能力提升。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLMs）在计算最优条件下泛化能力更强的机制。

Method: 开发了一种新的Freedman-type martingale浓度不等式，结合Chinchilla缩放定律，分析了参数数量、损失方差和量化误差对泛化的影响。

Result: 发现模型规模增大时，损失方差和量化误差减小，泛化能力提升。

Conclusion: 提出了泛化差距的缩放定律，证明模型规模增大时泛化边界可预测性增强。

Abstract: Why do larger language models generalize better? To investigate this
question, we develop generalization bounds on the pretraining objective of
large language models (LLMs) in the compute-optimal regime, as described by the
Chinchilla scaling laws. We introduce a novel, fully empirical Freedman-type
martingale concentration inequality that tightens existing bounds by accounting
for the variance of the loss function. This generalization bound can be
decomposed into three interpretable components: the number of parameters per
token, the loss variance, and the quantization error at a fixed bitrate. As
compute-optimal language models are scaled up, the number of parameters per
data point remains constant; however, both the loss variance and the
quantization error decrease, implying that larger models should have smaller
generalization gaps. We examine why larger models tend to be more quantizable
from an information theoretic perspective, showing that the rate at which they
can integrate new information grows more slowly than their capacity on the
compute-optimal frontier. From these findings we produce a scaling law for the
generalization gap, with bounds that become predictably stronger with scale.

</details>


### [385] [A Causal Convolutional Low-rank Representation Model for Imputation of Water Quality Data](https://arxiv.org/abs/2504.15209)
*Xin Liao,Bing Yang,Tan Dongli,Cai Yu*

Main category: cs.LG

TL;DR: 本文提出了一种基于因果卷积低秩表示（CLR）的模型，用于填补水质监测数据中的缺失值，以提高数据完整性。该模型结合了时间依赖性和自动超参数调整，实验证明其在准确性和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 水质监测数据常因设备故障等原因出现缺失值，简单填补方法不准确，影响决策。因此，需要一种更精确的填补方法。

Method: 提出CLR模型，结合因果卷积操作考虑时间依赖性，并采用超参数自适应调整方案。

Result: 在三个真实数据集上的实验表明，CLR模型在填补准确性和时间成本上优于现有方法。

Conclusion: CLR模型为环境监测提供了更可靠的决策支持。

Abstract: The monitoring of water quality is a crucial part of environmental
protection, and a large number of monitors are widely deployed to monitor water
quality. Due to unavoidable factors such as data acquisition breakdowns,
sensors and communication failures, water quality monitoring data suffers from
missing values over time, resulting in High-Dimensional and Sparse (HDS) Water
Quality Data (WQD). The simple and rough filling of the missing values leads to
inaccurate results and affects the implementation of relevant measures.
Therefore, this paper proposes a Causal convolutional Low-rank Representation
(CLR) model for imputing missing WQD to improve the completeness of the WQD,
which employs a two-fold idea: a) applying causal convolutional operation to
consider the temporal dependence of the low-rank representation, thus
incorporating temporal information to improve the imputation accuracy; and b)
implementing a hyperparameters adaptation scheme to automatically adjust the
best hyperparameters during model training, thereby reducing the tedious manual
adjustment of hyper-parameters. Experimental studies on three real-world water
quality datasets demonstrate that the proposed CLR model is superior to some of
the existing state-of-the-art imputation models in terms of imputation accuracy
and time cost, as well as indicating that the proposed model provides more
reliable decision support for environmental monitoring.

</details>


### [386] [Histogram-based Parameter-efficient Tuning for Passive Sonar Classification](https://arxiv.org/abs/2504.15214)
*Amirmohammad Mohammadi,Davelle Carreiro,Alexandra Van Dine,Joshua Peeples*

Main category: cs.LG

TL;DR: HPT是一种基于直方图的高效参数调优技术，通过捕捉目标域统计信息来调整嵌入，优于传统适配器方法，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效迁移学习方法（如适配器）难以捕捉中间特征嵌入的分布变化，需要一种更有效的方法。

Method: 提出基于直方图的参数高效调优技术（HPT），通过统计目标域信息调制嵌入。

Result: 在三个被动声纳数据集上，HPT表现优于传统适配器，VTUAD准确率达91.8%。HPT训练更快且特征表示更接近全微调模型。

Conclusion: HPT平衡了参数节省与性能，为资源受限环境中的迁移学习提供了新方向。

Abstract: Parameter-efficient transfer learning (PETL) methods adapt large artificial
neural networks to downstream tasks without fine-tuning the entire model.
However, existing additive methods, such as adapters, sometimes struggle to
capture distributional shifts in intermediate feature embeddings. We propose a
novel histogram-based parameter-efficient tuning (HPT) technique that captures
the statistics of the target domain and modulates the embeddings. Experimental
results on three downstream passive sonar datasets (ShipsEar, DeepShip, VTUAD)
demonstrate that HPT outperforms conventional adapters. Notably, HPT achieves
91.8% vs. 89.8% accuracy on VTUAD. Furthermore, HPT trains faster and yields
feature representations closer to those of fully fine-tuned models. Overall,
HPT balances parameter savings and performance, providing a distribution-aware
alternative to existing adapters and shows a promising direction for scalable
transfer learning in resource-constrained environments. The code is publicly
available:
https://github.com/Advanced-Vision-and-Learning-Lab/HLAST_DeepShip_ParameterEfficient.

</details>


### [387] [A Deep Learning Framework for Sequence Mining with Bidirectional LSTM and Multi-Scale Attention](https://arxiv.org/abs/2504.15223)
*Tao Yang,Yu Cheng,Yaokun Ren,Yujia Lou,Minggu Wei,Honghui Xin*

Main category: cs.LG

TL;DR: 提出了一种结合双向LSTM和多尺度注意力机制的序列模式挖掘算法，用于复杂序列数据中的潜在模式挖掘和上下文依赖建模，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决复杂序列数据中潜在模式挖掘和上下文依赖建模的挑战。

Method: 整合双向LSTM（BiLSTM）和多尺度注意力机制，BiLSTM捕捉序列的前后依赖，注意力模块自适应分配权重。

Result: 在公开的多变量时间序列数据集上实验，模型在准确性、精确度和召回率上优于现有方法。

Conclusion: 提出的架构在复杂模式识别任务中有效且鲁棒，进一步的结构优化得到实证支持。

Abstract: This paper addresses the challenges of mining latent patterns and modeling
contextual dependencies in complex sequence data. A sequence pattern mining
algorithm is proposed by integrating Bidirectional Long Short-Term Memory
(BiLSTM) with a multi-scale attention mechanism. The BiLSTM captures both
forward and backward dependencies in sequences, enhancing the model's ability
to perceive global contextual structures. At the same time, the multi-scale
attention module assigns adaptive weights to key feature regions under
different window sizes. This improves the model's responsiveness to both local
and global important information. Extensive experiments are conducted on a
publicly available multivariate time series dataset. The proposed model is
compared with several mainstream sequence modeling methods. Results show that
it outperforms existing models in terms of accuracy, precision, and recall.
This confirms the effectiveness and robustness of the proposed architecture in
complex pattern recognition tasks. Further ablation studies and sensitivity
analyses are carried out to investigate the effects of attention scale and
input sequence length on model performance. These results provide empirical
support for structural optimization of the model.

</details>


### [388] [M$^2$AD: Multi-Sensor Multi-System Anomaly Detection through Global Scoring and Calibrated Thresholding](https://arxiv.org/abs/2504.15225)
*Sarah Alnegheimish,Zelin He,Matthew Reimherr,Akash Chandrayan,Abhinav Pradhan,Luca D'Angelo*

Main category: cs.LG

TL;DR: M$^2$AD是一个用于多系统多元时间序列数据的无监督异常检测框架，通过深度模型捕捉正常行为，利用残差和GMM-Gamma校准生成全局异常分数，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 工业系统中多源异构时间序列数据的异常检测需求，现有方法无法有效处理多系统数据。

Method: 使用深度模型建模正常行为，残差作为异常指标，通过GMM和Gamma校准生成全局异常分数。

Result: M$^2$AD在评估中平均优于现有方法21%，并在亚马逊物流中心的130个资产上验证了有效性。

Conclusion: M$^2$AD能有效处理多系统数据的异质性和依赖性，具有实际应用价值。

Abstract: With the widespread availability of sensor data across industrial and
operational systems, we frequently encounter heterogeneous time series from
multiple systems. Anomaly detection is crucial for such systems to facilitate
predictive maintenance. However, most existing anomaly detection methods are
designed for either univariate or single-system multivariate data, making them
insufficient for these complex scenarios. To address this, we introduce
M$^2$AD, a framework for unsupervised anomaly detection in multivariate time
series data from multiple systems. M$^2$AD employs deep models to capture
expected behavior under normal conditions, using the residuals as indicators of
potential anomalies. These residuals are then aggregated into a global anomaly
score through a Gaussian Mixture Model and Gamma calibration. We theoretically
demonstrate that this framework can effectively address heterogeneity and
dependencies across sensors and systems. Empirically, M$^2$AD outperforms
existing methods in extensive evaluations by 21% on average, and its
effectiveness is demonstrated on a large-scale real-world case study on 130
assets in Amazon Fulfillment Centers. Our code and results are available at
https://github.com/sarahmish/M2AD.

</details>


### [389] [Conformalized-KANs: Uncertainty Quantification with Coverage Guarantees for Kolmogorov-Arnold Networks (KANs) in Scientific Machine Learning](https://arxiv.org/abs/2504.15240)
*Amirhossein Mollaali,Christian Bolivar Moya,Amanda A. Howard,Alexander Heinlein,Panos Stinis,Guang Lin*

Main category: cs.LG

TL;DR: 本文研究了Kolmogorov-Arnold Networks (KANs)中的不确定性量化方法，提出了一种集成方法和Conformalized-KANs，以提高模型的解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索KANs中的不确定性量化方法，以增强模型在复杂函数建模中的可靠性和适用性。

Method: 采用集成方法和Conformalized-KANs，结合共形预测技术生成校准的预测区间。

Result: 实验表明，该方法在多种超参数设置下具有鲁棒性和准确性，并适用于KANs的扩展版本。

Conclusion: 该方法提升了KANs在科学机器学习中的可靠性和适用性。

Abstract: This paper explores uncertainty quantification (UQ) methods in the context of
Kolmogorov-Arnold Networks (KANs). We apply an ensemble approach to KANs to
obtain a heuristic measure of UQ, enhancing interpretability and robustness in
modeling complex functions. Building on this, we introduce Conformalized-KANs,
which integrate conformal prediction, a distribution-free UQ technique, with
KAN ensembles to generate calibrated prediction intervals with guaranteed
coverage. Extensive numerical experiments are conducted to evaluate the
effectiveness of these methods, focusing particularly on the robustness and
accuracy of the prediction intervals under various hyperparameter settings. We
show that the conformal KAN predictions can be applied to recent extensions of
KANs, including Finite Basis KANs (FBKANs) and multifideilty KANs (MFKANs). The
results demonstrate the potential of our approaches to improve the reliability
and applicability of KANs in scientific machine learning.

</details>


### [390] [Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction](https://arxiv.org/abs/2504.15266)
*Vaishnavh Nagarajan,Chen Henry Wu,Charles Ding,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 论文设计了一套最小算法任务，用于量化语言模型的创造性限制，并提出多标记方法和输入层噪声注入优于单标记学习和输出层温度采样。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在开放任务中的创造性表现，探索其局限性并提出改进方法。

Method: 设计抽象任务，比较单标记和多标记方法（如无教师训练和扩散模型），提出输入层噪声注入（hash-conditioning）。

Result: 多标记方法在多样性和原创性上表现更优，输入层噪声注入能更好地平衡随机性和连贯性。

Conclusion: 为分析开放创造性任务提供了测试框架，支持超越单标记学习和softmax采样的方法。

Abstract: We design a suite of minimal algorithmic tasks that are a loose abstraction
of open-ended real-world tasks. This allows us to cleanly and controllably
quantify the creative limits of the present-day language model. Much like
real-world tasks that require a creative, far-sighted leap of thought, our
tasks require an implicit, open-ended stochastic planning step that either (a)
discovers new connections in an abstract knowledge graph (like in wordplay,
drawing analogies, or research) or (b) constructs new patterns (like in
designing math problems or new proteins). In these tasks, we empirically and
conceptually argue how next-token learning is myopic and memorizes excessively;
comparatively, multi-token approaches, namely teacherless training and
diffusion models, excel in producing diverse and original output. Secondly, in
our tasks, we find that to elicit randomness from the Transformer without
hurting coherence, it is better to inject noise right at the input layer (via a
method we dub hash-conditioning) rather than defer to temperature sampling from
the output layer. Thus, our work offers a principled, minimal test-bed for
analyzing open-ended creative skills, and offers new arguments for going beyond
next-token learning and softmax-based sampling. We make part of the code
available under https://github.com/chenwu98/algorithmic-creativity

</details>


### [391] [Single-loop Algorithms for Stochastic Non-convex Optimization with Weakly-Convex Constraints](https://arxiv.org/abs/2504.15243)
*Ming Yang,Gang Li,Quanqi Hu,Qihang Lin,Tianbao Yang*

Main category: cs.LG

TL;DR: 本文提出了一种新型单循环惩罚随机算法，用于解决弱凸目标函数和约束函数的优化问题，实现了近似KKT解的最优复杂度。


<details>
  <summary>Details</summary>
Motivation: 多函数不等式约束的优化问题在机器学习中有重要应用，但现有方法存在收敛速度慢或依赖双循环设计的问题。

Method: 采用基于铰链的惩罚方法，结合恒定惩罚参数，设计单循环随机算法，并扩展到有限和耦合组合目标。

Result: 实验验证了算法在公平学习和持续学习任务中的有效性，复杂度优于现有方法。

Conclusion: 新算法在解决弱凸约束优化问题时表现出高效性和优越性。

Abstract: Constrained optimization with multiple functional inequality constraints has
significant applications in machine learning. This paper examines a crucial
subset of such problems where both the objective and constraint functions are
weakly convex. Existing methods often face limitations, including slow
convergence rates or reliance on double-loop algorithmic designs. To overcome
these challenges, we introduce a novel single-loop penalty-based stochastic
algorithm. Following the classical exact penalty method, our approach employs a
{\bf hinge-based penalty}, which permits the use of a constant penalty
parameter, enabling us to achieve a {\bf state-of-the-art complexity} for
finding an approximate Karush-Kuhn-Tucker (KKT) solution. We further extend our
algorithm to address finite-sum coupled compositional objectives, which are
prevalent in artificial intelligence applications, establishing improved
complexity over existing approaches. Finally, we validate our method through
experiments on fair learning with receiver operating characteristic (ROC)
fairness constraints and continual learning with non-forgetting constraints.

</details>


### [392] [Faster Algorithms for Agnostically Learning Disjunctions and their Implications](https://arxiv.org/abs/2504.15244)
*Ilias Diakonikolas,Daniel M. Kane,Lisheng Ren*

Main category: cs.LG

TL;DR: 本文提出了一种在分布无关的不可知PAC模型中学习布尔析取的新算法，复杂度为2^(O(n^(1/3)))，优于现有的2^(O(n^(1/2)))方法，并首次在SQ和CSQ模型间实现了分离。


<details>
  <summary>Details</summary>
Motivation: 研究布尔析取在不可知PAC模型中的学习问题，旨在改进现有算法的复杂度。

Method: 开发了一种新的不可知学习算法，基于统计查询（SQ）模型，复杂度为2^(O(n^(1/3)))。

Result: 算法复杂度显著优于现有方法，并首次在SQ和CSQ模型间展示了分离。

Conclusion: 新算法在布尔析取学习中实现了更优的复杂度，并揭示了SQ和CSQ模型的能力差异。

Abstract: We study the algorithmic task of learning Boolean disjunctions in the
distribution-free agnostic PAC model. The best known agnostic learner for the
class of disjunctions over $\{0, 1\}^n$ is the $L_1$-polynomial regression
algorithm, achieving complexity $2^{\tilde{O}(n^{1/2})}$. This complexity bound
is known to be nearly best possible within the class of Correlational
Statistical Query (CSQ) algorithms. In this work, we develop an agnostic
learner for this concept class with complexity $2^{\tilde{O}(n^{1/3})}$. Our
algorithm can be implemented in the Statistical Query (SQ) model, providing the
first separation between the SQ and CSQ models in distribution-free agnostic
learning.

</details>


### [393] [On Learning Parallel Pancakes with Mostly Uniform Weights](https://arxiv.org/abs/2504.15251)
*Ilias Diakonikolas,Daniel M. Kane,Sushrut Karmalkar,Jasper C. H. Lee,Thanasis Pittas*

Main category: cs.LG

TL;DR: 研究了学习$k$-高斯混合模型（$k$-GMMs）的复杂性，提出了统计查询（SQ）下界和权重分布对任务复杂性的影响。


<details>
  <summary>Details</summary>
Motivation: 高斯混合模型学习在一般情况下复杂度较高，研究通过引入结构假设（如权重非指数小且协方差相同）来降低复杂度。

Method: 提出统计查询（SQ）下界证明，并分析权重分布对任务的影响。

Result: 证明了权重均匀时SQ下界接近最优，并给出了权重大部分均匀时的准多项式上界。

Conclusion: 权重分布对学习复杂度有显著影响，均匀权重假设下复杂度接近最优。

Abstract: We study the complexity of learning $k$-mixtures of Gaussians ($k$-GMMs) on
$\mathbb{R}^d$. This task is known to have complexity $d^{\Omega(k)}$ in full
generality. To circumvent this exponential lower bound on the number of
components, research has focused on learning families of GMMs satisfying
additional structural properties. A natural assumption posits that the
component weights are not exponentially small and that the components have the
same unknown covariance. Recent work gave a $d^{O(\log(1/w_{\min}))}$-time
algorithm for this class of GMMs, where $w_{\min}$ is the minimum weight. Our
first main result is a Statistical Query (SQ) lower bound showing that this
quasi-polynomial upper bound is essentially best possible, even for the special
case of uniform weights. Specifically, we show that it is SQ-hard to
distinguish between such a mixture and the standard Gaussian. We further
explore how the distribution of weights affects the complexity of this task.
Our second main result is a quasi-polynomial upper bound for the aforementioned
testing task when most of the weights are uniform while a small fraction of the
weights are potentially arbitrary.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [394] [Towards Model Resistant to Transferable Adversarial Examples via Trigger Activation](https://arxiv.org/abs/2504.14541)
*Yi Yu,Song Xia,Xun Lin,Chenqi Kong,Wenhan Yang,Shijian Lu,Yap-Peng Tan,Alex C. Kot*

Main category: cs.CR

TL;DR: 论文提出了一种新的训练范式，通过触发激活模型增强对可转移对抗样本的鲁棒性，并展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 对抗样本的可转移性对深度神经网络构成威胁，现有防御方法存在效率低、效果差等问题。

Method: 提出触发激活模型，通过固定触发器使模型在干净数据上随机猜测，在触发数据上准确预测。

Result: 实验证明该方法对可转移攻击具有鲁棒性，并通过联合优化进一步提升效果。

Conclusion: 触发激活模型是一种高效且有效的防御可转移对抗样本的方法。

Abstract: Adversarial examples, characterized by imperceptible perturbations, pose
significant threats to deep neural networks by misleading their predictions. A
critical aspect of these examples is their transferability, allowing them to
deceive {unseen} models in black-box scenarios. Despite the widespread
exploration of defense methods, including those on transferability, they show
limitations: inefficient deployment, ineffective defense, and degraded
performance on clean images. In this work, we introduce a novel training
paradigm aimed at enhancing robustness against transferable adversarial
examples (TAEs) in a more efficient and effective way. We propose a model that
exhibits random guessing behavior when presented with clean data
$\boldsymbol{x}$ as input, and generates accurate predictions when with
triggered data $\boldsymbol{x}+\boldsymbol{\tau}$. Importantly, the trigger
$\boldsymbol{\tau}$ remains constant for all data instances. We refer to these
models as \textbf{models with trigger activation}. We are surprised to find
that these models exhibit certain robustness against TAEs. Through the
consideration of first-order gradients, we provide a theoretical analysis of
this robustness. Moreover, through the joint optimization of the learnable
trigger and the model, we achieve improved robustness to transferable attacks.
Extensive experiments conducted across diverse datasets, evaluating a variety
of attacking methods, underscore the effectiveness and superiority of our
approach.

</details>


### [395] [REDEditing: Relationship-Driven Precise Backdoor Poisoning on Text-to-Image Diffusion Models](https://arxiv.org/abs/2504.14554)
*Chongye Guo,Jinhu Fu,Junfeng Fang,Kun Wang,Guorui Feng*

Main category: cs.CR

TL;DR: 论文提出了一种基于模型编辑的无训练后门投毒方法REDEditing，通过概念重绑定实现一致的后门图像生成，攻击成功率比现有方法高11%，同时提升了隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展凸显了文本到图像（T2I）模型的安全性，尤其是后门投毒的威胁。及时披露和缓解T2I模型的安全漏洞对确保生成模型的安全部署至关重要。

Method: 提出了一种基于模型编辑的后门攻击原则，并设计了关系驱动的精确后门投毒方法REDEditing。该方法利用等效属性对齐和隐蔽投毒原则，通过等效关系检索和联合属性转移实现概念重绑定。

Result: REDEditing的攻击成功率比现有方法高11%，仅需添加一行代码即可提升输出自然性和后门隐蔽性24%。

Conclusion: 本研究旨在提高对可编辑图像生成模型中安全漏洞的认识，揭示了模型编辑技术对图像生成模型的潜在安全风险。

Abstract: The rapid advancement of generative AI highlights the importance of
text-to-image (T2I) security, particularly with the threat of backdoor
poisoning. Timely disclosure and mitigation of security vulnerabilities in T2I
models are crucial for ensuring the safe deployment of generative models. We
explore a novel training-free backdoor poisoning paradigm through model
editing, which is recently employed for knowledge updating in large language
models. Nevertheless, we reveal the potential security risks posed by model
editing techniques to image generation models. In this work, we establish the
principles for backdoor attacks based on model editing, and propose a
relationship-driven precise backdoor poisoning method, REDEditing. Drawing on
the principles of equivalent-attribute alignment and stealthy poisoning, we
develop an equivalent relationship retrieval and joint-attribute transfer
approach that ensures consistent backdoor image generation through concept
rebinding. A knowledge isolation constraint is proposed to preserve benign
generation integrity. Our method achieves an 11\% higher attack success rate
compared to state-of-the-art approaches. Remarkably, adding just one line of
code enhances output naturalness while improving backdoor stealthiness by 24\%.
This work aims to heighten awareness regarding this security vulnerability in
editable image generation models.

</details>


### [396] [Protecting Your Voice: Temporal-aware Robust Watermarking](https://arxiv.org/abs/2504.14832)
*Yue Li,Weizhi Liu,Dongdong Lin*

Main category: cs.CR

TL;DR: 论文提出了一种时间感知的鲁棒水印方法（True），用于保护语音和歌声，同时平衡保真度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 生成模型的快速发展导致合成声音的真实性模糊，现有频域水印方法在鲁棒性提升的同时牺牲了声音的保真度。

Method: 提出了一种时间感知的鲁棒水印方法（True），通过最大化时域特征的全面学习来增强保真度，同时保持鲁棒性。

Result: 该方法在保护语音和歌声时，能够平衡保真度和鲁棒性。

Conclusion: True方法为合成声音的水印保护提供了一种新的解决方案，兼顾了保真度和鲁棒性。

Abstract: The rapid advancement of generative models has led to the synthesis of
real-fake ambiguous voices. To erase the ambiguity, embedding watermarks into
the frequency-domain features of synthesized voices has become a common
routine. However, the robustness achieved by choosing the frequency domain
often comes at the expense of fine-grained voice features, leading to a loss of
fidelity. Maximizing the comprehensive learning of time-domain features to
enhance fidelity while maintaining robustness, we pioneer a
\textbf{\underline{t}}emporal-aware
\textbf{\underline{r}}ob\textbf{\underline{u}}st
wat\textbf{\underline{e}}rmarking (\emph{True}) method for protecting the
speech and singing voice.

</details>


### [397] [aiXamine: LLM Safety and Security Simplified](https://arxiv.org/abs/2504.14985)
*Fatih Deniz,Dorde Popovic,Yazan Boshmaf,Euisuh Jeong,Minhaj Ahmad,Sanjay Chawla,Issa Khalil*

Main category: cs.CR

TL;DR: aiXamine是一个用于评估大语言模型（LLM）安全性和安全性的黑盒平台，整合了40多个测试，覆盖8个关键维度，评估了50多个模型，揭示了主流模型的漏洞和开源模型的优势。


<details>
  <summary>Details</summary>
Motivation: 评估LLM的安全性和安全性是一个复杂且碎片化的任务，需要一个统一的平台来整合测试和报告。

Method: 开发了aiXamine平台，整合40多个测试，覆盖8个关键维度，生成详细报告和可视化结果。

Result: 评估了50多个模型，发现主流模型存在漏洞（如GPT-4o易受攻击、Grok-3有偏见、Gemini 2.0隐私弱），开源模型在某些方面优于专有模型。

Conclusion: aiXamine为LLM评估提供了统一工具，揭示了模型的优缺点，并发现开源模型在特定领域的潜力。

Abstract: Evaluating Large Language Models (LLMs) for safety and security remains a
complex task, often requiring users to navigate a fragmented landscape of ad
hoc benchmarks, datasets, metrics, and reporting formats. To address this
challenge, we present aiXamine, a comprehensive black-box evaluation platform
for LLM safety and security. aiXamine integrates over 40 tests (i.e.,
benchmarks) organized into eight key services targeting specific dimensions of
safety and security: adversarial robustness, code security, fairness and bias,
hallucination, model and data privacy, out-of-distribution (OOD) robustness,
over-refusal, and safety alignment. The platform aggregates the evaluation
results into a single detailed report per model, providing a detailed breakdown
of model performance, test examples, and rich visualizations. We used aiXamine
to assess over 50 publicly available and proprietary LLMs, conducting over 2K
examinations. Our findings reveal notable vulnerabilities in leading models,
including susceptibility to adversarial attacks in OpenAI's GPT-4o, biased
outputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.
Additionally, we observe that open-source models can match or exceed
proprietary models in specific services such as safety alignment, fairness and
bias, and OOD robustness. Finally, we identify trade-offs between distillation
strategies, model size, training methods, and architectural choices.

</details>


### [398] [SOLIDO: A Robust Watermarking Method for Speech Synthesis via Low-Rank Adaptation](https://arxiv.org/abs/2504.15035)
*Yue Li,Weizhi Liu,Dongdong Lin*

Main category: cs.CR

TL;DR: 本文提出了一种名为SOLIDO的新型生成水印方法，通过低秩适应（LoRA）结合语音扩散模型，解决了现有水印技术计算开销大、训练成本高及对变长输入鲁棒性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 语音生成模型的快速发展引发了模型侵权和内容滥用的安全问题，现有水印技术存在计算开销大、训练成本高及对变长输入鲁棒性不足的局限性。

Method: SOLIDO结合参数高效微调与语音水印技术，通过LoRA实现低计算开销的水印嵌入和提取，设计了基于深度可分离卷积的水印解码器以处理变长输入。

Result: 实验表明，SOLIDO在2000 bps容量下仍能保持高保真水印语音，对常见攻击的平均提取准确率最高达99.20%（单攻击）和98.43%（复合攻击），在抗时间拉伸攻击上优于现有方法23%。

Conclusion: SOLIDO是一种高效、鲁棒的语音生成水印方法，显著提升了水印容量和抗攻击能力。

Abstract: The accelerated advancement of speech generative models has given rise to
security issues, including model infringement and unauthorized abuse of
content. Although existing generative watermarking techniques have proposed
corresponding solutions, most methods require substantial computational
overhead and training costs. In addition, some methods have limitations in
robustness when handling variable-length inputs. To tackle these challenges, we
propose \textsc{SOLIDO}, a novel generative watermarking method that integrates
parameter-efficient fine-tuning with speech watermarking through low-rank
adaptation (LoRA) for speech diffusion models. Concretely, the watermark
encoder converts the watermark to align with the input of diffusion models. To
achieve precise watermark extraction from variable-length inputs, the watermark
decoder based on depthwise separable convolution is designed for watermark
recovery. To further enhance speech generation performance and watermark
extraction capability, we propose a speech-driven lightweight fine-tuning
strategy, which reduces computational overhead through LoRA. Comprehensive
experiments demonstrate that the proposed method ensures high-fidelity
watermarked speech even at a large capacity of 2000 bps. Furthermore, against
common individual and compound speech attacks, our SOLIDO achieves a maximum
average extraction accuracy of 99.20\% and 98.43\%, respectively. It surpasses
other state-of-the-art methods by nearly 23\% in resisting time-stretching
attacks.

</details>


### [399] [Mining Characteristics of Vulnerable Smart Contracts Across Lifecycle Stages](https://arxiv.org/abs/2504.15063)
*Hongli Peng,Xiaoqi Li,Wenkai Li*

Main category: cs.CR

TL;DR: 本文首次对智能合约全生命周期（部署、执行、升级和销毁阶段）的安全性进行了实证研究，分析了各阶段的安全问题，并提出了七个特征描述。通过机器学习模型，研究发现不同阶段的漏洞合约具有独特的交易特征和网络属性。


<details>
  <summary>Details</summary>
Motivation: 智能合约的安全问题导致重大经济损失，现有研究仅关注代码漏洞，缺乏对全生命周期各阶段漏洞特征的分析和区分。

Method: 对智能合约生命周期各阶段进行静态分析，提出七个特征描述，并利用五种机器学习分类模型识别不同阶段的漏洞。

Result: 研究发现，漏洞合约在不同阶段表现出独特的交易特征和网络属性。

Conclusion: 本文为智能合约全生命周期的安全性提供了更全面的研究框架和实证支持。

Abstract: Smart contracts are the cornerstone of decentralized applications and
financial protocols, which extend the application of digital currency
transactions. The applications and financial protocols introduce significant
security challenges, resulting in substantial economic losses. Existing
solutions predominantly focus on code vulnerabilities within smart contracts,
accounting for only 50% of security incidents. Therefore, a more comprehensive
study of security issues related to smart contracts is imperative. The existing
empirical research realizes the static analysis of smart contracts from the
perspective of the lifecycle and gives the corresponding measures for each
stage. However, they lack the characteristic analysis of vulnerabilities in
each stage and the distinction between the vulnerabilities. In this paper, we
present the first empirical study on the security of smart contracts throughout
their lifecycle, including deployment and execution, upgrade, and destruction
stages. It delves into the security issues at each stage and provides at least
seven feature descriptions. Finally, utilizing these seven features, five
machine-learning classification models are used to identify vulnerabilities at
different stages. The classification results reveal that vulnerable contracts
exhibit distinct transaction features and ego network properties at various
stages.

</details>


### [400] [C2RUST-BENCH: A Minimized, Representative Dataset for C-to-Rust Transpilation Evaluation](https://arxiv.org/abs/2504.15144)
*Melih Sirlanci,Carter Yagemann,Zhiqiang Lin*

Main category: cs.CR

TL;DR: 提出了一种方法，从大量函数中选择代表性样本构建最小化但全面的数据集，用于评估C到Rust的转译。


<details>
  <summary>Details</summary>
Motivation: 尽管已有C到Rust的转译框架，但缺乏全面的评估数据集，手动或自动化分析大规模数据集耗时。

Method: 从15,503个真实程序函数中筛选出2,905个代表性函数，构建C2RUST-BENCH数据集。

Result: C2RUST-BENCH数据集为评估C到Rust转译提供了高效且代表性的样本。

Conclusion: 该方法解决了评估数据集不足的问题，为转译框架的优化提供了支持。

Abstract: Despite the effort in vulnerability detection over the last two decades,
memory safety vulnerabilities continue to be a critical problem. Recent reports
suggest that the key solution is to migrate to memory-safe languages. To this
end, C-to-Rust transpilation becomes popular to resolve memory-safety issues in
C programs. Recent works propose C-to-Rust transpilation frameworks; however, a
comprehensive evaluation dataset is missing. Although one solution is to put
together a large enough dataset, this increases the analysis time in automated
frameworks as well as in manual efforts for some cases. In this work, we build
a method to select functions from a large set to construct a minimized yet
representative dataset to evaluate the C-to-Rust transpilation. We propose
C2RUST-BENCH that contains 2,905 functions, which are representative of
C-to-Rust transpilation, selected from 15,503 functions of real-world programs.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [401] [Causal pieces: analysing and improving spiking neural networks piece by piece](https://arxiv.org/abs/2504.14015)
*Dominik Dold,Philipp Christian Petersen*

Main category: cs.NE

TL;DR: 论文提出了一种基于“线性片段”概念的新型脉冲神经网络（SNN）分析方法，证明了SNN输入域可分解为因果区域，其输出脉冲时间对输入和参数具有局部Lipschitz连续性。因果区域数量（称为“因果片段”）是衡量SNN近似能力的指标。实验表明，因果片段数量与训练成功强相关，且纯正权重的SNN表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索脉冲神经网络（SNN）的表达能力和可训练性，提出一种基于因果片段的新分析方法，以改进SNN性能并与人工神经网络（ANN）进行比较。

Method: 通过理论证明SNN输入域可分解为因果区域，输出脉冲时间具有局部Lipschitz连续性。实验验证因果片段数量与训练成功的相关性，并测试纯正权重SNN的性能。

Result: 因果片段数量与SNN训练成功强相关，纯正权重的SNN在基准任务中表现优异。

Conclusion: 因果片段是改进SNN的强大工具，未来可能为SNN与ANN的比较提供新方法。

Abstract: We introduce a novel concept for spiking neural networks (SNNs) derived from
the idea of "linear pieces" used to analyse the expressiveness and trainability
of artificial neural networks (ANNs). We prove that the input domain of SNNs
decomposes into distinct causal regions where its output spike times are
locally Lipschitz continuous with respect to the input spike times and network
parameters. The number of such regions - which we call "causal pieces" - is a
measure of the approximation capabilities of SNNs. In particular, we
demonstrate in simulation that parameter initialisations which yield a high
number of causal pieces on the training set strongly correlate with SNN
training success. Moreover, we find that feedforward SNNs with purely positive
weights exhibit a surprisingly high number of causal pieces, allowing them to
achieve competitive performance levels on benchmark tasks. We believe that
causal pieces are not only a powerful and principled tool for improving SNNs,
but might also open up new ways of comparing SNNs and ANNs in the future.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [402] [The First VoicePrivacy Attacker Challenge](https://arxiv.org/abs/2504.14183)
*Natalia Tomashenko,Xiaoxiao Miao,Emmanuel Vincent,Junichi Yamagishi*

Main category: eess.AS

TL;DR: ICASSP 2025 SP Grand Challenge评估了针对语音匿名化系统的攻击者系统，最佳攻击系统将EER降低了25-44%。


<details>
  <summary>Details</summary>
Motivation: 评估攻击者系统对语音匿名化系统的效果，提升语音隐私保护技术的安全性。

Method: 参与者开发自动说话人验证系统作为攻击者系统，使用提供的训练、开发和评估数据集进行测试。

Result: 最佳攻击系统将EER相对基线降低了25-44%。

Conclusion: 挑战赛展示了攻击者系统的有效性，为语音隐私保护技术提供了改进方向。

Abstract: The First VoicePrivacy Attacker Challenge is an ICASSP 2025 SP Grand
Challenge which focuses on evaluating attacker systems against a set of voice
anonymization systems submitted to the VoicePrivacy 2024 Challenge. Training,
development, and evaluation datasets were provided along with a baseline
attacker. Participants developed their attacker systems in the form of
automatic speaker verification systems and submitted their scores on the
development and evaluation data. The best attacker systems reduced the equal
error rate (EER) by 25-44% relative w.r.t. the baseline.

</details>


### [403] [Data Augmentation Using Neural Acoustic Fields With Retrieval-Augmented Pre-training](https://arxiv.org/abs/2504.14409)
*Christopher Ick,Gordon Wichern,Yoshiki Masuyama,François G. Germain,Jonathan Le Roux*

Main category: eess.AS

TL;DR: MERL提出了一种基于神经声场的RIR估计系统，用于ICASSP 2025的任务1（增强RIR数据）和任务2（改进说话者距离估计）。


<details>
  <summary>Details</summary>
Motivation: 解决RIR数据增强和说话者距离估计的问题，通过预训练和适应目标房间的神经声场模型。

Method: 1. 预训练神经声场模型，基于外部数据集中的RIR和房间几何信息；2. 使用注册数据适应目标房间；3. 预测指定源和接收位置的RIR，并用于训练说话者距离估计模型。

Result: 系统能够生成目标房间的RIR，并用于改进说话者距离估计任务。

Conclusion: 该方法通过结合预训练和适应性调整，有效解决了RIR数据增强和说话者距离估计问题。

Abstract: This report details MERL's system for room impulse response (RIR) estimation
submitted to the Generative Data Augmentation Workshop at ICASSP 2025 for
Augmenting RIR Data (Task 1) and Improving Speaker Distance Estimation (Task
2). We first pre-train a neural acoustic field conditioned by room geometry on
an external large-scale dataset in which pairs of RIRs and the geometries are
provided. The neural acoustic field is then adapted to each target room by
using the enrollment data, where we leverage either the provided room
geometries or geometries retrieved from the external dataset, depending on
availability. Lastly, we predict the RIRs for each pair of source and receiver
locations specified by Task 1, and use these RIRs to train the speaker distance
estimation model in Task 2.

</details>


### [404] [OmniAudio: Generating Spatial Audio from 360-Degree Video](https://arxiv.org/abs/2504.14906)
*Huadai Liu,Tianyi Luo,Qikai Jiang,Kaicheng Luo,Peiwen Sun,Jialei Wan,Rongjie Huang,Qian Chen,Wen Wang,Xiangtai Li,Shiliang Zhang,Zhijie Yan,Zhou Zhao,Wei Xue*

Main category: eess.AS

TL;DR: 论文提出了一种新任务360V2SA，通过360度视频生成空间音频（FOA格式），并开发了数据集Sphere360和框架OmniAudio，实现了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 传统视频到音频生成技术缺乏空间音频的3D环境表现，360V2SA任务旨在填补这一空白。

Method: 提出OmniAudio框架，结合自监督预训练和双分支设计，利用全景和FoV视频输入生成空间音频。

Result: OmniAudio在Sphere360数据集上实现了客观和主观指标的最优表现。

Conclusion: 360V2SA任务和OmniAudio框架为3D空间音频生成提供了有效解决方案。

Abstract: Traditional video-to-audio generation techniques primarily focus on
field-of-view (FoV) video and non-spatial audio, often missing the spatial cues
necessary for accurately representing sound sources in 3D environments. To
address this limitation, we introduce a novel task, 360V2SA, to generate
spatial audio from 360-degree videos, specifically producing First-order
Ambisonics (FOA) audio - a standard format for representing 3D spatial audio
that captures sound directionality and enables realistic 3D audio reproduction.
We first create Sphere360, a novel dataset tailored for this task that is
curated from real-world data. We also design an efficient semi-automated
pipeline for collecting and cleaning paired video-audio data. To generate
spatial audio from 360-degree video, we propose a novel framework OmniAudio,
which leverages self-supervised pre-training using both spatial audio data (in
FOA format) and large-scale non-spatial data. Furthermore, OmniAudio features a
dual-branch framework that utilizes both panoramic and FoV video inputs to
capture comprehensive local and global information from 360-degree videos.
Experimental results demonstrate that OmniAudio achieves state-of-the-art
performance across both objective and subjective metrics on Sphere360. Code and
datasets will be released at https://github.com/liuhuadai/OmniAudio. The demo
page is available at https://OmniAudio-360V2SA.github.io.

</details>


### [405] [StableQuant: Layer Adaptive Post-Training Quantization for Speech Foundation Models](https://arxiv.org/abs/2504.14915)
*Yeona Hong,Hyewon Han,Woo-jin Chung,Hong-Goo Kang*

Main category: eess.AS

TL;DR: StableQuant是一种新型自适应后训练量化算法，专为语音基础模型设计，能自适应确定每层的量化范围，显著提升压缩效果和推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统后训练量化方法直接应用于语音基础模型效果不佳，因其网络架构与语言模型不同。

Method: 通过分析尺度分布和整体性能，自适应确定每层的量化范围。

Result: 在HuBERT和wav2vec2.0上测试，模型大小缩减至四分之一，推理速度翻倍，词错误率仅下降0.3%。

Conclusion: StableQuant在语音基础模型上实现了高效压缩和快速推理，性能损失极小。

Abstract: In this paper, we propose StableQuant, a novel adaptive post-training
quantization (PTQ) algorithm for widely used speech foundation models (SFMs).
While PTQ has been successfully employed for compressing large language models
(LLMs) due to its ability to bypass additional fine-tuning, directly applying
these techniques to SFMs may not yield optimal results, as SFMs utilize
distinct network architecture for feature extraction. StableQuant demonstrates
optimal quantization performance regardless of the network architecture type,
as it adaptively determines the quantization range for each layer by analyzing
both the scale distributions and overall performance. We evaluate our algorithm
on two SFMs, HuBERT and wav2vec2.0, for an automatic speech recognition (ASR)
task, and achieve superior performance compared to traditional PTQ methods.
StableQuant successfully reduces the sizes of SFM models to a quarter and
doubles the inference speed while limiting the word error rate (WER)
performance drop to less than 0.3% with 8-bit quantization.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [406] [Integrating Response Time and Attention Duration in Bayesian Preference Learning for Multiple Criteria Decision Aiding](https://arxiv.org/abs/2504.14938)
*Jiaxuan Jiang,Jiapeng Liu,Miłosz Kadziński,Xiuwu Liao,Jingyu Dong*

Main category: stat.AP

TL;DR: 提出了一种结合行为线索的多标准贝叶斯偏好学习框架，通过整合成对比较、反应时间和注意力持续时间，深入分析决策过程。


<details>
  <summary>Details</summary>
Motivation: 传统方法在捕捉决策者行为模式方面存在局限，需要更全面的数据和方法来提升偏好学习的准确性。

Method: 采用加性价值函数模型和贝叶斯框架，通过定义偏好数据的似然函数和先验偏好结构，推导潜在排序模型的后验分布。

Result: 实验验证了新方法在重构完整偏好方面的能力，并揭示了与时间和注意力相关的行为模式。

Conclusion: 综合数据整合能开发出更符合决策者实际偏好的模型，优于传统方法。

Abstract: We introduce a multiple criteria Bayesian preference learning framework
incorporating behavioral cues for decision aiding. The framework integrates
pairwise comparisons, response time, and attention duration to deepen insights
into decision-making processes. The approach employs an additive value function
model and utilizes a Bayesian framework to derive the posterior distribution of
potential ranking models by defining the likelihood of observed preference data
and specifying a prior on the preference structure. This distribution
highlights each model's ability to reconstruct Decision-Makers' holistic
pairwise comparisons. By leveraging both response time as a proxy for cognitive
effort and alternative discriminability as well as attention duration as an
indicator of criterion importance, the proposed model surpasses traditional
methods by uncovering richer behavioral patterns. We report the results of a
laboratory experiment on mobile phone contract selection involving 30 real
subjects using a dedicated application with time-, eye-, and mouse-tracking
components. We validate the novel method's ability to reconstruct complete
preferences. The detailed ablation studies reveal time- and attention-related
behavioral patterns, confirming that integrating comprehensive data leads to
developing models that better align with the DM's actual preferences.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [407] [Predicting fermionic densities using a Projected Quantum Kernel method](https://arxiv.org/abs/2504.14002)
*Francesco Perciavalle,Francesco Plastina,Michele Pisarra,Nicola Lo Gullo*

Main category: quant-ph

TL;DR: 论文提出了一种基于投影量子核方法的支持向量回归器，用于预测一维费米子系统的密度结构，性能优于经典线性核方法。


<details>
  <summary>Details</summary>
Motivation: 研究量子化学和量子物质中一维费米子系统的密度结构预测问题。

Method: 使用基于量子储层可观测量的投影量子核方法构建支持向量回归器，训练和测试数据通过密度泛函理论生成。

Result: 在足够长的测量时间下，该方法性能优于经典线性核方法，并与径向基函数方法竞争。

Conclusion: 该方法在预测一维费米子系统密度结构方面具有潜力，尤其在长测量时间下表现优异。

Abstract: We use a support vector regressor based on a projected quantum kernel method
to predict the density structure of 1D fermionic systems of interest in quantum
chemistry and quantum matter. The kernel is built on with the observables of a
quantum reservoir implementable with interacting Rydberg atoms. Training and
test data of the fermionic system are generated using a Density Functional
Theory approach. We test the performance of the method for several Hamiltonian
parameters, finding a general common behavior of the error as a function of
measurement time. At sufficiently large measurement times, we find that the
method outperforms the classical linear kernel method and can be competitive
with the radial basis function method.

</details>


### [408] [Guess, SWAP, Repeat : Capturing Quantum Snapshots in Classical Memory](https://arxiv.org/abs/2504.14459)
*Debarshi Kundu,Avimita Chatterjee,Swaroop Ghosh*

Main category: quant-ph

TL;DR: 提出一种无需直接测量即可观察量子态的新技术，支持量子态的非破坏性存储和复用。


<details>
  <summary>Details</summary>
Motivation: 解决量子系统中因不可克隆定理和破坏性测量导致的调试、内省和持久内存难题。

Method: 采用硬件无关的机器学习框架，通过保真度估计（如SWAP测试）和梯度/无梯度优化策略重构量子态。

Result: 在IBM量子硬件上实现高保真度（约1.0）重构，模拟中平均保真度达0.999。

Conclusion: 为量子非易失性内存提供可行路径，支持量子信息的长期存储和复用，奠定未来量子内存架构基础。

Abstract: We introduce a novel technique that enables observation of quantum states
without direct measurement, preserving them for reuse. Our method allows
multiple quantum states to be observed at different points within a single
circuit, one at a time, and saved into classical memory without destruction.
These saved states can be accessed on demand by downstream applications,
introducing a dynamic and programmable notion of quantum memory that supports
modular, non-destructive quantum workflows. We propose a hardware-agnostic,
machine learning-driven framework to capture non-destructive estimates, or
"snapshots," of quantum states at arbitrary points within a circuit, enabling
classical storage and later reconstruction, similar to memory operations in
classical computing. This capability is essential for debugging, introspection,
and persistent memory in quantum systems, yet remains difficult due to the
no-cloning theorem and destructive measurements. Our guess-and-check approach
uses fidelity estimation via the SWAP test to guide state reconstruction. We
explore both gradient-based deep neural networks and gradient-free evolutionary
strategies to estimate quantum states using only fidelity as the learning
signal. We demonstrate a key component of our framework on IBM quantum
hardware, achieving high-fidelity (approximately 1.0) reconstructions for
Hadamard and other known states. In simulation, our models achieve an average
fidelity of 0.999 across 100 random quantum states. This provides a pathway
toward non-volatile quantum memory, enabling long-term storage and reuse of
quantum information, and laying groundwork for future quantum memory
architectures.

</details>


### [409] [Quantum-Enhanced Weight Optimization for Neural Networks Using Grover's Algorithm](https://arxiv.org/abs/2504.14568)
*Stefan-Alexandru Jura,Mihai Udrescu*

Main category: quant-ph

TL;DR: 提出了一种利用量子计算优化经典神经网络权重的新方法，通过Grover量子搜索算法加速训练过程，避免了梯度下降的问题，显著提升了测试准确率和降低了测试损失。


<details>
  <summary>Details</summary>
Motivation: 传统梯度下降方法存在梯度爆炸、消失或凸性问题，其他方法如遗传搜索也有收敛一致性问题，因此需要一种更高效且稳定的优化方法。

Method: 设计了一种基于Grover量子搜索算法的优化方法，用于搜索神经网络的最优参数，无需计算梯度。

Result: 在小数据集上，测试损失减少58.75%，测试准确率提高35.25%；在3隐藏层的深度网络上，Digits数据集上的平均准确率达到97.7%。

Conclusion: 该方法不仅高效且可扩展，适用于深度网络，且对量子比特需求较低，适合未来量子计算机的有限资源。

Abstract: The main approach to hybrid quantum-classical neural networks (QNN) is
employing quantum computing to build a neural network (NN) that has quantum
features, which is then optimized classically. Here, we propose a different
strategy: to use quantum computing in order to optimize the weights of a
classical NN. As such, we design an instance of Grover's quantum search
algorithm to accelerate the search for the optimal parameters of an NN during
the training process, a task traditionally performed using the backpropagation
algorithm with the gradient descent method. Indeed, gradient descent has issues
such as exploding gradient, vanishing gradient, or convexity problem. Other
methods tried to address such issues with strategies like genetic searches, but
they carry additional problems like convergence consistency. Our original
method avoids these issues -- because it does not calculate gradients -- and
capitalizes on classical architectures' robustness and Grover's quadratic
speedup in high-dimensional search spaces to significantly reduce test loss
(58.75%) and improve test accuracy (35.25%), compared to classical NN weight
optimization, on small datasets. Unlike most QNNs that are trained on small
datasets only, our method is also scalable, as it allows the optimization of
deep networks; for an NN with 3 hidden layers, trained on the Digits dataset
from scikit-learn, we obtained a mean accuracy of 97.7%. Moreover, our method
requires a much smaller number of qubits compared to other QNN approaches,
making it very practical for near-future quantum computers that will still
deliver a limited number of logical qubits.

</details>


### [410] [Trainable Quantum Neural Network for Multiclass Image Classification with the Power of Pre-trained Tree Tensor Networks](https://arxiv.org/abs/2504.14995)
*Keisuke Murota,Takumi Kobori*

Main category: quant-ph

TL;DR: 论文提出了一种森林张量网络（FTN）分类器，用于解决将树张量网络（TTN）嵌入量子神经网络（QNN）时的高阶门操作和中间电路后选择问题，并通过数值实验验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 将TTN分类器嵌入QNN以提升多类图像分类性能，但面临高阶门操作和低成功率的中间电路后选择问题。

Method: 提出FTN分类器，通过聚合多个小键维TTN避免大尺寸门操作，并扩展绝热编码框架以消除中间电路后选择的开销。

Result: 在MNIST和CIFAR-10数据集上成功训练并编码FTN分类器为量子FTN分类器，性能保持或提升。

Conclusion: TTN分类模型与QNN的协同作用为多类量子增强图像分类提供了可扩展的框架。

Abstract: Tree tensor networks (TTNs) offer powerful models for image classification.
While these TTN image classifiers already show excellent performance on
classical hardware, embedding them into quantum neural networks (QNNs) may
further improve the performance by leveraging quantum resources. However,
embedding TTN classifiers into QNNs for multiclass classification remains
challenging. Key obstacles are the highorder gate operations required for large
bond dimensions and the mid-circuit postselection with exponentially low
success rates necessary for the exact embedding. In this work, to address these
challenges, we propose forest tensor network (FTN)-classifiers, which aggregate
multiple small-bond-dimension TTNs. This allows us to handle multiclass
classification without requiring large gates in the embedded circuits. We then
remove the overhead of mid-circuit postselection by extending the adiabatic
encoding framework to our setting and smoothly encode the FTN-classifiers into
a quantum forest tensor network (qFTN)- classifiers. Numerical experiments on
MNIST and CIFAR-10 demonstrate that we can successfully train FTN-classifiers
and encode them into qFTN-classifiers, while maintaining or even improving the
performance of the pre-trained FTN-classifiers. These results suggest that
synergy between TTN classification models and QNNs can provide a robust and
scalable framework for multiclass quantum-enhanced image classification.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [411] [Chinese-LiPS: A Chinese audio-visual speech recognition dataset with Lip-reading and Presentation Slides](https://arxiv.org/abs/2504.15066)
*Jinghua Zhao,Yuhang Jia,Shiyao Wang,Jiaming Zhou,Hui Wang,Yong Qin*

Main category: cs.MM

TL;DR: 论文提出了一种结合唇读和幻灯片信息的音频-视觉语音识别方法，并发布了包含100小时数据的中文AVSR数据集Chinese-LiPS。实验表明，两种视觉模态分别提升ASR性能8%和25%，综合提升35%。


<details>
  <summary>Details</summary>
Motivation: 现有AVSR方法通常仅依赖唇读或上下文视频，忽略了结合多种视觉线索的潜力。

Method: 基于Chinese-LiPS数据集，开发了LiPS-AVSR方法，同时利用唇读和幻灯片信息作为视觉模态。

Result: 唇读和幻灯片信息分别提升ASR性能约8%和25%，综合提升约35%。

Conclusion: 结合多种视觉模态能显著提升AVSR性能，Chinese-LiPS数据集为未来研究提供了资源。

Abstract: Incorporating visual modalities to assist Automatic Speech Recognition (ASR)
tasks has led to significant improvements. However, existing Audio-Visual
Speech Recognition (AVSR) datasets and methods typically rely solely on
lip-reading information or speaking contextual video, neglecting the potential
of combining these different valuable visual cues within the speaking context.
In this paper, we release a multimodal Chinese AVSR dataset, Chinese-LiPS,
comprising 100 hours of speech, video, and corresponding manual transcription,
with the visual modality encompassing both lip-reading information and the
presentation slides used by the speaker. Based on Chinese-LiPS, we develop a
simple yet effective pipeline, LiPS-AVSR, which leverages both lip-reading and
presentation slide information as visual modalities for AVSR tasks. Experiments
show that lip-reading and presentation slide information improve ASR
performance by approximately 8\% and 25\%, respectively, with a combined
performance improvement of about 35\%. The dataset is available at
https://kiri0824.github.io/Chinese-LiPS/

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [412] [Coordinating Spinal and Limb Dynamics for Enhanced Sprawling Robot Mobility](https://arxiv.org/abs/2504.14103)
*Merve Atasever,Ali Okhovat,Azhang Nazaripouya,John Nisbet,Omer Kurkutlu,Jyotirmoy V. Deshmukh,Yasemin Ozkan Aydin*

Main category: cs.RO

TL;DR: 研究比较了基于学习的控制策略和生物启发的步态设计方法在蝾螈机器人上的应用，探讨了脊柱灵活性对运动的影响。


<details>
  <summary>Details</summary>
Motivation: 脊椎动物中蝾螈因其独特的行走和游泳步态转换能力，突显了脊柱灵活性在运动中的作用。环境不确定性可能导致身体-肢体协调失调，需要开发动态适应策略。

Method: 采用深度强化学习（DRL）框架，结合生物启发的步态设计方法，在蝾螈机器人上进行比较研究。

Result: 研究表明，DRL能够有效处理非确定性环境，使机器人系统在挑战性条件下适应并表现稳健。

Conclusion: 研究为开发适应不确定环境的机器人控制策略提供了新思路，验证了DRL在复杂运动任务中的潜力。

Abstract: Among vertebrates, salamanders, with their unique ability to transition
between walking and swimming gaits, highlight the role of spinal mobility in
locomotion. A flexible spine enables undulation of the body through a wavelike
motion along the spine, aiding navigation over uneven terrains and obstacles.
Yet environmental uncertainties, such as surface irregularities and variations
in friction, can significantly disrupt body-limb coordination and cause
discrepancies between predictions from mathematical models and real-world
outcomes. Addressing this challenge requires the development of sophisticated
control strategies capable of dynamically adapting to uncertain conditions
while maintaining efficient locomotion. Deep reinforcement learning (DRL)
offers a promising framework for handling non-deterministic environments and
enabling robotic systems to adapt effectively and perform robustly under
challenging conditions. In this study, we comparatively examine learning-based
control strategies and biologically inspired gait design methods on a
salamander-like robot.

</details>


### [413] [Knitting Robots: A Deep Learning Approach for Reverse-Engineering Fabric Patterns](https://arxiv.org/abs/2504.14007)
*Haoliang Sheng,Songpu Cai,Xingyu Zheng,Meng Cheng Lau*

Main category: cs.RO

TL;DR: 提出了一种基于深度学习的逆向编织管道，将视觉机器人系统融入纺织制造，解决编织自动化中的设计转换问题。


<details>
  <summary>Details</summary>
Motivation: 编织自动化在纺织制造中具有挑战性，尤其是将设计转换为机器可读指令。研究旨在通过机器人视觉系统填补这一空白。

Method: 采用两阶段深度学习架构，先识别前标签再推断完整标签，支持单线和多线模式，解决标签不平衡和精细控制问题。

Result: 系统能适应不同材料复杂度，为全自动编织机器人系统奠定基础，实现可定制化生产。

Conclusion: 该研究通过智能机器人自动化推动了纺织制造的发展，整合了感知、规划和执行。

Abstract: Knitting, a cornerstone of textile manufacturing, is uniquely challenging to
automate, particularly in terms of converting fabric designs into precise,
machine-readable instructions. This research bridges the gap between textile
production and robotic automation by proposing a novel deep learning-based
pipeline for reverse knitting to integrate vision-based robotic systems into
textile manufacturing. The pipeline employs a two-stage architecture, enabling
robots to first identify front labels before inferring complete labels,
ensuring accurate, scalable pattern generation. By incorporating diverse yarn
structures, including single-yarn (sj) and multi-yarn (mj) patterns, this study
demonstrates how our system can adapt to varying material complexities.
Critical challenges in robotic textile manipulation, such as label imbalance,
underrepresented stitch types, and the need for fine-grained control, are
addressed by leveraging specialized deep-learning architectures. This work
establishes a foundation for fully automated robotic knitting systems, enabling
customizable, flexible production processes that integrate perception,
planning, and actuation, thereby advancing textile manufacturing through
intelligent robotic automation.

</details>


### [414] [Experience-based Refinement of Task Planning Knowledge in Autonomous Robots](https://arxiv.org/abs/2504.14259)
*Hadeel Jazzaa,Thomas McCluskey,David Peebles*

Main category: cs.RO

TL;DR: 该论文提出了一种方法，使物理机器人能够通过执行动作的经验来调整其符号知识，从而提高任务计划的成功率。


<details>
  <summary>Details</summary>
Motivation: 自主机器人在动态环境中展示高级认知技能的需求是AI领域的重大挑战，但目前的研究尚未将符号知识的调整应用于实际机器人。

Method: 提出了一种通过机器人动作执行经验驱动知识调整的方法，以改进任务计划的稳健性。

Result: 在NAO机器人上实现的架构显示，调整后的知识减少了任务计划的失败率。

Conclusion: 通过知识调整，机器人能够逐步改进任务计划的成功率，证明了方法的有效性。

Abstract: The requirement for autonomous robots to exhibit higher-level cognitive
skills by planning and adapting in an ever-changing environment is indeed a
great challenge for the AI community. Progress has been made in the automated
planning community on refinement and repair of an agent's symbolic knowledge to
do task planning in an incomplete or changing environmental model, but these
advances up to now have not been transferred to real physical robots. This
paper demonstrates how a physical robot can be capable of adapting its symbolic
knowledge of the environment, by using experiences in robot action execution to
drive knowledge refinement and hence to improve the success rate of the task
plans the robot creates. To implement more robust planning systems, we propose
a method for refining domain knowledge to improve the knowledge on which
intelligent robot behavior is based. This architecture has been implemented and
evaluated using a NAO robot. The refined knowledge leads to the future
synthesis of task plans which demonstrate decreasing rates of failure over time
as faulty knowledge is removed or adjusted.

</details>


### [415] [Unreal Robotics Lab: A High-Fidelity Robotics Simulator with Advanced Physics and Rendering](https://arxiv.org/abs/2504.14135)
*Jonathan Embley-Riches,Jianwei Liu,Simon Julier,Dimitrios Kanoulas*

Main category: cs.RO

TL;DR: 本文提出了一种名为Unreal Robotics Lab（URL）的新型仿真框架，结合了Unreal Engine的高质量渲染和MuJoCo的高精度物理模拟，用于机器人研究。


<details>
  <summary>Details</summary>
Motivation: 高保真仿真对机器人研究至关重要，但实现逼真渲染和精确物理模拟仍具挑战性。

Method: 通过整合Unreal Engine的渲染能力和MuJoCo的物理模拟，创建了一个支持复杂环境效果的仿真框架。

Result: 该框架成功用于视觉导航和SLAM方法的基准测试，展示了其在多样化场景中的实用性。

Conclusion: URL框架通过结合物理精度和逼真渲染，为机器人研究和仿真到现实的迁移提供了强大工具。

Abstract: High-fidelity simulation is essential for robotics research, enabling safe
and efficient testing of perception, control, and navigation algorithms.
However, achieving both photorealistic rendering and accurate physics modeling
remains a challenge. This paper presents a novel simulation framework--the
Unreal Robotics Lab (URL) that integrates the Unreal Engine's advanced
rendering capabilities with MuJoCo's high-precision physics simulation. Our
approach enables realistic robotic perception while maintaining accurate
physical interactions, facilitating benchmarking and dataset generation for
vision-based robotics applications. The system supports complex environmental
effects, such as smoke, fire, and water dynamics, which are critical for
evaluating robotic performance under adverse conditions. We benchmark visual
navigation and SLAM methods within our framework, demonstrating its utility for
testing real-world robustness in controlled yet diverse scenarios. By bridging
the gap between physics accuracy and photorealistic rendering, our framework
provides a powerful tool for advancing robotics research and sim-to-real
transfer.

</details>


### [416] [Infrared Vision Systems for Emergency Vehicle Driver Assistance in Low-Visibility Conditions](https://arxiv.org/abs/2504.14078)
*M-Mahdi Naddaf-Sh,Andrew Lee,Kin Yen,Eemon Amini,Iman Soltani*

Main category: cs.RO

TL;DR: 研究探讨红外（IR）摄像头技术在低能见度条件下提升紧急车辆驾驶员安全的潜力，尤其是在夜间和浓雾中。


<details>
  <summary>Details</summary>
Motivation: 低能见度环境（如夜间和浓雾）增加了紧急车辆（如拖车和除雪车）的碰撞风险，传统辅助系统效果有限。

Method: 结合实验室实验、实地测试和驾驶员调查，评估红外摄像头性能及改装现有交通部门车队的可行性。

Result: 红外技术显著提升驾驶员警觉性，并提供了经济高效的改装方案。

Conclusion: 红外摄像头技术是提升紧急车辆安全的有效手段，适合大规模推广。

Abstract: This study investigates the potential of infrared (IR) camera technology to
enhance driver safety for emergency vehicles operating in low-visibility
conditions, particularly at night and in dense fog. Such environments
significantly increase the risk of collisions, especially for tow trucks and
snowplows that must remain operational in challenging conditions. Conventional
driver assistance systems often struggle under these conditions due to limited
visibility. In contrast, IR cameras, which detect the thermal signatures of
obstacles, offer a promising alternative. The evaluation combines controlled
laboratory experiments, real-world field tests, and surveys of emergency
vehicle operators. In addition to assessing detection performance, the study
examines the feasibility of retrofitting existing Department of Transportation
(DoT) fleets with cost-effective IR-based driver assistance systems. Results
underscore the utility of IR technology in enhancing driver awareness and
provide data-driven recommendations for scalable deployment across legacy
emergency vehicle fleets.

</details>


### [417] [SG-Reg: Generalizable and Efficient Scene Graph Registration](https://arxiv.org/abs/2504.14440)
*Chuhao Liu,Zhijian Qiao,Jieqi Shi,Ke Wang,Peize Liu,Shaojie Shen*

Main category: cs.RO

TL;DR: 提出了一种基于多模态语义节点特征的场景图网络，用于刚性语义场景图注册，减少了GPU资源和通信带宽需求，并在实验中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统语义辅助注册中手工描述符和学习方法对真实世界环境的适应性不足问题。

Method: 设计多模态语义节点特征（开放集语义特征、局部拓扑与空间感知、形状特征），采用粗到细的匹配策略和鲁棒姿态估计。

Result: 在两智能体SLAM基准测试中，注册成功率显著优于手工基线，通信带宽需求低至52 KB/帧。

Conclusion: 该方法在资源效率和注册性能上均优于现有技术，适用于实际多智能体任务。

Abstract: This paper addresses the challenges of registering two rigid semantic scene
graphs, an essential capability when an autonomous agent needs to register its
map against a remote agent, or against a prior map. The hand-crafted
descriptors in classical semantic-aided registration, or the ground-truth
annotation reliance in learning-based scene graph registration, impede their
application in practical real-world environments. To address the challenges, we
design a scene graph network to encode multiple modalities of semantic nodes:
open-set semantic feature, local topology with spatial awareness, and shape
feature. These modalities are fused to create compact semantic node features.
The matching layers then search for correspondences in a coarse-to-fine manner.
In the back-end, we employ a robust pose estimator to decide transformation
according to the correspondences. We manage to maintain a sparse and
hierarchical scene representation. Our approach demands fewer GPU resources and
fewer communication bandwidth in multi-agent tasks. Moreover, we design a new
data generation approach using vision foundation models and a semantic mapping
module to reconstruct semantic scene graphs. It differs significantly from
previous works, which rely on ground-truth semantic annotations to generate
data. We validate our method in a two-agent SLAM benchmark. It significantly
outperforms the hand-crafted baseline in terms of registration success rate.
Compared to visual loop closure networks, our method achieves a slightly higher
registration recall while requiring only 52 KB of communication bandwidth for
each query frame. Code available at:
\href{http://github.com/HKUST-Aerial-Robotics/SG-Reg}{http://github.com/HKUST-Aerial-Robotics/SG-Reg}.

</details>


### [418] [Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction](https://arxiv.org/abs/2504.14588)
*Wenke Xia,Ruoxuan Feng,Dong Wang,Di Hu*

Main category: cs.RO

TL;DR: 论文提出Phoenix框架，通过运动指令连接高层语义反思与低层机器人动作修正，结合多任务运动条件扩散策略实现精细动作修正，并通过终身学习提升模型能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器人从失败中恢复的问题，尤其是如何将语义反思转化为精细动作修正的挑战。

Method: 采用双过程运动调整机制和多任务运动条件扩散策略，结合视觉观察实现高频动作修正。

Result: 在RoboMimic仿真和真实场景中验证了框架的泛化性和鲁棒性。

Conclusion: Phoenix框架通过运动指令和终身学习实现了高效、精确的机器人动作修正。

Abstract: Building a generalizable self-correction system is crucial for robots to
recover from failures. Despite advancements in Multimodal Large Language Models
(MLLMs) that empower robots with semantic reflection ability for failure,
translating semantic reflection into how to correct fine-grained robotic
actions remains a significant challenge. To address this gap, we build the
Phoenix framework, which leverages motion instruction as a bridge to connect
high-level semantic reflection with low-level robotic action correction. In
this motion-based self-reflection framework, we start with a dual-process
motion adjustment mechanism with MLLMs to translate the semantic reflection
into coarse-grained motion instruction adjustment. To leverage this motion
instruction for guiding how to correct fine-grained robotic actions, a
multi-task motion-conditioned diffusion policy is proposed to integrate visual
observations for high-frequency robotic action correction. By combining these
two models, we could shift the demand for generalization capability from the
low-level manipulation policy to the MLLMs-driven motion adjustment model and
facilitate precise, fine-grained robotic action correction. Utilizing this
framework, we further develop a lifelong learning method to automatically
improve the model's capability from interactions with dynamic environments. The
experiments conducted in both the RoboMimic simulation and real-world scenarios
prove the superior generalization and robustness of our framework across a
variety of manipulation tasks. Our code is released at
\href{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}.

</details>


### [419] [Latent Representations for Visual Proprioception in Inexpensive Robots](https://arxiv.org/abs/2504.14634)
*Sahara Sheikholeslami,Ladislau Bölöni*

Main category: cs.RO

TL;DR: 研究探讨了通过单次外部摄像头图像实现视觉本体感知的可行性，测试了多种潜在表示方法，并在低成本6自由度机器人上验证了准确性。


<details>
  <summary>Details</summary>
Motivation: 低成本机器人在非结构化环境中缺乏精确的本体感知能力，研究旨在探索单次视觉回归架构的潜力。

Method: 测试了CNN、VAE、ViT和未校准标记点等多种潜在表示方法，并采用适应有限数据的微调技术。

Result: 在低成本6自由度机器人上验证了方法的准确性。

Conclusion: 单次视觉回归架构在低成本机器人中实现视觉本体感知具有潜力。

Abstract: Robotic manipulation requires explicit or implicit knowledge of the robot's
joint positions. Precise proprioception is standard in high-quality industrial
robots but is often unavailable in inexpensive robots operating in unstructured
environments. In this paper, we ask: to what extent can a fast, single-pass
regression architecture perform visual proprioception from a single external
camera image, available even in the simplest manipulation settings? We explore
several latent representations, including CNNs, VAEs, ViTs, and bags of
uncalibrated fiducial markers, using fine-tuning techniques adapted to the
limited data available. We evaluate the achievable accuracy through experiments
on an inexpensive 6-DoF robot.

</details>


### [420] [Modality Selection and Skill Segmentation via Cross-Modality Attention](https://arxiv.org/abs/2504.14573)
*Jiawei Jiang,Kei Ota,Devesh K. Jha,Asako Kanezaki*

Main category: cs.RO

TL;DR: 提出跨模态注意力机制（CMA）解决多模态信息整合问题，并用于技能分割和分层策略训练。


<details>
  <summary>Details</summary>
Motivation: 解决机器人模型中整合多模态信息（如触觉和音频）时维度灾难的挑战。

Method: 提出跨模态注意力机制（CMA），选择性地利用信息量最大的模态，并扩展用于技能分割和分层策略训练。

Result: 成功应用于长时程、接触密集的操控任务。

Conclusion: CMA机制有效解决了多模态信息整合问题，并提升了机器人任务执行能力。

Abstract: Incorporating additional sensory modalities such as tactile and audio into
foundational robotic models poses significant challenges due to the curse of
dimensionality. This work addresses this issue through modality selection. We
propose a cross-modality attention (CMA) mechanism to identify and selectively
utilize the modalities that are most informative for action generation at each
timestep. Furthermore, we extend the application of CMA to segment primitive
skills from expert demonstrations and leverage this segmentation to train a
hierarchical policy capable of solving long-horizon, contact-rich manipulation
tasks.

</details>


### [421] [K2MUSE: A human lower limb multimodal dataset under diverse conditions for facilitating rehabilitation robotics](https://arxiv.org/abs/2504.14602)
*Jiwei Li,Bi Zhang,Xiaowei Tan,Wanxin Chen,Zhaoyuan Liu,Juanjuan Zhang,Weiguang Huo,Jian Huang,Lianqing Liu,Xingang Zhao*

Main category: cs.RO

TL;DR: 论文介绍了K2MUSE数据集，填补了现有下肢康复机器人研究中多模态数据和大规模步态样本的不足，提供了包括运动学、动力学、超声和肌电信号在内的多维度数据。


<details>
  <summary>Details</summary>
Motivation: 现有下肢数据集缺乏多模态数据和大规模样本，且未考虑实际应用中的干扰因素，限制了数据驱动方法的效果。

Method: 收集了30名健康参与者在不同坡度、速度和干扰条件下的下肢多模态数据，包括运动学、动力学、超声和肌电信号。

Result: 提出了K2MUSE数据集，为康复机器人控制框架设计和下肢运动生物力学分析提供了新资源。

Conclusion: K2MUSE数据集填补了研究空白，支持康复机器人和生物力学领域的进一步发展。

Abstract: The natural interaction and control performance of lower limb rehabilitation
robots are closely linked to biomechanical information from various human
locomotion activities. Multidimensional human motion data significantly deepen
the understanding of the complex mechanisms governing neuromuscular
alterations, thereby facilitating the development and application of
rehabilitation robots in multifaceted real-world environments. However,
currently available lower limb datasets are inadequate for supplying the
essential multimodal data and large-scale gait samples necessary for effective
data-driven approaches, and they neglect the significant effects of acquisition
interference in real applications.To fill this gap, we present the K2MUSE
dataset, which includes a comprehensive collection of multimodal data,
comprising kinematic, kinetic, amplitude-mode ultrasound (AUS), and surface
electromyography (sEMG) measurements. The proposed dataset includes lower limb
multimodal data from 30 able-bodied participants walking under different
inclines (0$^\circ$, $\pm$5$^\circ$, and $\pm$10$^\circ$), various speeds (0.5
m/s, 1.0 m/s, and 1.5 m/s), and different nonideal acquisition conditions
(muscle fatigue, electrode shifts, and inter-day differences). The kinematic
and ground reaction force data were collected via a Vicon motion capture system
and an instrumented treadmill with embedded force plates, whereas the sEMG and
AUS data were synchronously recorded for thirteen muscles on the bilateral
lower limbs. This dataset offers a new resource for designing control
frameworks for rehabilitation robots and conducting biomechanical analyses of
lower limb locomotion. The dataset is available at https://k2muse.github.io/.

</details>


### [422] [A General Infrastructure and Workflow for Quadrotor Deep Reinforcement Learning and Reality Deployment](https://arxiv.org/abs/2504.15129)
*Kangyao Huang,Hao Wang,Yu Luo,Jingyu Chen,Jintao Chen,Xiangkui Zhang,Xiangyang Ji,Huaping Liu*

Main category: cs.RO

TL;DR: 论文提出一个平台，实现端到端深度强化学习策略的无缝迁移，解决四旋翼在非结构化户外环境中学习方法的挑战。


<details>
  <summary>Details</summary>
Motivation: 四旋翼在现实环境中应用学习方法的挑战包括大量仿真数据需求、实时处理要求和仿真与现实的差距。现有工作未提及从零开始的系统训练和部署，难以复现。

Method: 整合训练环境、飞行控制、DRL算法、MAVROS中间件和硬件，形成完整工作流，支持从零训练到现实部署。

Result: 平台支持多种环境任务，验证了仿真到现实的高效性和户外飞行的鲁棒性。

Conclusion: 该平台为四旋翼学习方法的实际应用提供了可行解决方案。

Abstract: Deploying robot learning methods to a quadrotor in unstructured outdoor
environments is an exciting task. Quadrotors operating in real-world
environments by learning-based methods encounter several challenges: a large
amount of simulator generated data required for training, strict demands for
real-time processing onboard, and the sim-to-real gap caused by dynamic and
noisy conditions. Current works have made a great breakthrough in applying
learning-based methods to end-to-end control of quadrotors, but rarely mention
the infrastructure system training from scratch and deploying to reality, which
makes it difficult to reproduce methods and applications. To bridge this gap,
we propose a platform that enables the seamless transfer of end-to-end deep
reinforcement learning (DRL) policies. We integrate the training environment,
flight dynamics control, DRL algorithms, the MAVROS middleware stack, and
hardware into a comprehensive workflow and architecture that enables
quadrotors' policies to be trained from scratch to real-world deployment in
several minutes. Our platform provides rich types of environments including
hovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and
planning in unknown environments, as a physical experiment benchmark. Through
extensive empirical validation, we demonstrate the efficiency of proposed
sim-to-real platform, and robust outdoor flight performance under real-world
perturbations. Details can be found from our website
https://emnavi.tech/AirGym/.

</details>


### [423] [An LLM-enabled Multi-Agent Autonomous Mechatronics Design Framework](https://arxiv.org/abs/2504.14681)
*Zeyu Wang,Frank P. -W. Lo,Qian Chen,Yongqi Zhang,Chen Lin,Xu Chen,Zhenhua Yu,Alexander J. Thompson,Eric M. Yeatman,Benny P. L. Lo*

Main category: cs.RO

TL;DR: 提出了一种多自主机电设计框架，整合多学科知识，通过语言驱动工作流和人类反馈，实现功能原型自主生成，应用于水质监测等复杂工程任务。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多代理框架局限于数字或模拟环境，知识领域狭窄，难以应对需要物理设计、跨学科整合和约束感知推理的复杂工程任务。

Method: 整合机械设计、优化、电子和软件工程，通过语言驱动工作流和结构化人类反馈，实现自主设计功能原型。

Result: 成功应用于水质监测任务，开发出功能完备的自主船只，优化推进系统、低成本电子设备和先进控制系统。

Conclusion: 展示了LLM多代理系统在自动化现实工程工作流和减少领域专业知识依赖方面的潜力。

Abstract: Existing LLM-enabled multi-agent frameworks are predominantly limited to
digital or simulated environments and confined to narrowly focused knowledge
domain, constraining their applicability to complex engineering tasks that
require the design of physical embodiment, cross-disciplinary integration, and
constraint-aware reasoning. This work proposes a multi-agent autonomous
mechatronics design framework, integrating expertise across mechanical design,
optimization, electronics, and software engineering to autonomously generate
functional prototypes with minimal direct human design input. Operating
primarily through a language-driven workflow, the framework incorporates
structured human feedback to ensure robust performance under real-world
constraints. To validate its capabilities, the framework is applied to a
real-world challenge involving autonomous water-quality monitoring and
sampling, where traditional methods are labor-intensive and ecologically
disruptive. Leveraging the proposed system, a fully functional autonomous
vessel was developed with optimized propulsion, cost-effective electronics, and
advanced control. The design process was carried out by specialized agents,
including a high-level planning agent responsible for problem abstraction and
dedicated agents for structural, electronics, control, and software
development. This approach demonstrates the potential of LLM-based multi-agent
systems to automate real-world engineering workflows and reduce reliance on
extensive domain expertise.

</details>


### [424] [A Modularized Design Approach for GelSight Family of Vision-based Tactile Sensors](https://arxiv.org/abs/2504.14739)
*Arpit Agarwal,Mohammad Amin Mirzaee,Xiping Sun,Wenzhen Yuan*

Main category: cs.RO

TL;DR: 本文提出了一种系统化和目标驱动的GelSight传感器设计方法，通过物理精确的光学模拟优化设计，并开发了交互式工具箱OptiSense Studio，使非专家也能快速优化传感器设计。


<details>
  <summary>Details</summary>
Motivation: 现有的GelSight传感器设计过程需要繁琐的试错，缺乏系统性和目标驱动的方法。

Method: 通过模块化和参数化光学组件，设计四种通用目标函数，利用光学模拟进行优化，并通过工具箱OptiSense Studio实现交互式设计。

Result: 展示了四种GelSight传感器的优化设计，并成功将仿真结果应用于实际传感器。

Conclusion: 该方法显著简化了传感器设计流程，为非专家提供了高效的设计工具。

Abstract: GelSight family of vision-based tactile sensors has proven to be effective
for multiple robot perception and manipulation tasks. These sensors are based
on an internal optical system and an embedded camera to capture the deformation
of the soft sensor surface, inferring the high-resolution geometry of the
objects in contact. However, customizing the sensors for different robot hands
requires a tedious trial-and-error process to re-design the optical system. In
this paper, we formulate the GelSight sensor design process as a systematic and
objective-driven design problem and perform the design optimization with a
physically accurate optical simulation. The method is based on modularizing and
parameterizing the sensor's optical components and designing four generalizable
objective functions to evaluate the sensor. We implement the method with an
interactive and easy-to-use toolbox called OptiSense Studio. With the toolbox,
non-sensor experts can quickly optimize their sensor design in both forward and
inverse ways following our predefined modules and steps. We demonstrate our
system with four different GelSight sensors by quickly optimizing their initial
design in simulation and transferring it to the real sensors.

</details>


### [425] [Neural ATTF: A Scalable Solution to Lifelong Multi-Agent Path Planning](https://arxiv.org/abs/2504.15130)
*Kushal Shah,Jihyun Park,Seung-Kyum Choi*

Main category: cs.RO

TL;DR: Neural ATTF结合了PGTM模块和Neural STA*算法，解决了MAPD问题中的可扩展性、适应性和效率问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MAPD解决方案在动态环境中面临可扩展性、适应性和效率的挑战，无法满足实时规划需求。

Method: 提出Neural ATTF算法，结合PGTM模块（动态任务分配）和Neural STA*（数据驱动的路径规划），优化任务连续性和系统吞吐量。

Result: 实验表明，Neural ATTF在可扩展性、解决方案质量和计算效率上优于TPTS、CENTRAL等现有算法。

Conclusion: Neural ATTF能有效满足复杂、高需求动态环境中多智能体系统的需求。

Abstract: Multi-Agent Pickup and Delivery (MAPD) is a fundamental problem in robotics,
particularly in applications such as warehouse automation and logistics.
Existing solutions often face challenges in scalability, adaptability, and
efficiency, limiting their applicability in dynamic environments with real-time
planning requirements. This paper presents Neural ATTF (Adaptive Task Token
Framework), a new algorithm that combines a Priority Guided Task Matching
(PGTM) Module with Neural STA* (Space-Time A*), a data-driven path planning
method. Neural STA* enhances path planning by enabling rapid exploration of the
search space through guided learned heuristics and ensures collision avoidance
under dynamic constraints. PGTM prioritizes delayed agents and dynamically
assigns tasks by prioritizing agents nearest to these tasks, optimizing both
continuity and system throughput. Experimental evaluations against
state-of-the-art MAPD algorithms, including TPTS, CENTRAL, RMCA, LNS-PBS, and
LNS-wPBS, demonstrate the superior scalability, solution quality, and
computational efficiency of Neural ATTF. These results highlight the
framework's potential for addressing the critical demands of complex,
real-world multi-agent systems operating in high-demand, unpredictable
settings.

</details>


### [426] [A Genetic Fuzzy-Enabled Framework on Robotic Manipulation for In-Space Servicing](https://arxiv.org/abs/2504.15226)
*Nathan Steffen,Wilhelm Louw,Nicholas Ernest,Timothy Arnett,Kelly Cohen*

Main category: cs.RO

TL;DR: 论文提出了一种结合遗传模糊树与LQR控制的方法，用于提高卫星维护机器人的效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着卫星数量的增加，自动化机器人系统在近月空间的服务变得至关重要，安全性是维护任务中的关键因素。

Method: 通过Thales的TrUE AI工具包，将遗传模糊树与LQR控制方案结合，设计了一个用于两自由度平面机械臂的可信高效控制器。

Result: 实验表明，遗传模糊-LQR比最优LQR平均性能提升18.5%，且对不确定性具有极强的鲁棒性。

Conclusion: 该方法为卫星维护任务提供了一种高效且可靠的控制方案。

Abstract: Automation of robotic systems for servicing in cislunar space is becoming
extremely important as the number of satellites in orbit increases. Safety is
critical in performing satellite maintenance, so the control techniques
utilized must be trusted in addition to being highly efficient. In this work,
Genetic Fuzzy Trees are combined with the widely used LQR control scheme via
Thales' TrUE AI Toolkit to create a trusted and efficient controller for a
two-degree-of-freedom planar robotic manipulator that would theoretically be
used to perform satellite maintenance. It was found that Genetic Fuzzy-LQR is
18.5% more performant than optimal LQR on average, and that it is incredibly
robust to uncertainty.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [427] [PRISM: A Unified Framework for Photorealistic Reconstruction and Intrinsic Scene Modeling](https://arxiv.org/abs/2504.14219)
*Alara Dirik,Tuanfeng Wang,Duygu Ceylan,Stefanos Zafeiriou,Anna Frühstück*

Main category: cs.GR

TL;DR: PRISM是一个统一的框架，通过微调预训练的文本到图像扩散模型，同时生成RGB图像和内在映射（X层），支持多种图像生成和编辑任务。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法需要单独推断内在属性或使用不同模型进行分解和条件生成的问题，PRISM通过联合生成所有内在层保持多模态一致性。

Method: 基于预训练的文本到图像扩散模型，提出有效的微调策略，同时生成RGB图像和X层，支持RGB到X分解、X到RGBX条件生成等任务。

Result: 实验表明PRISM在内在图像分解和条件图像生成方面具有竞争力，同时保留了基础模型的文本到图像生成能力。

Conclusion: PRISM是一个多功能框架，能够高效完成多种图像生成和编辑任务，同时保持多模态一致性。

Abstract: We present PRISM, a unified framework that enables multiple image generation
and editing tasks in a single foundational model. Starting from a pre-trained
text-to-image diffusion model, PRISM proposes an effective fine-tuning strategy
to produce RGB images along with intrinsic maps (referred to as X layers)
simultaneously. Unlike previous approaches, which infer intrinsic properties
individually or require separate models for decomposition and conditional
generation, PRISM maintains consistency across modalities by generating all
intrinsic layers jointly. It supports diverse tasks, including text-to-RGBX
generation, RGB-to-X decomposition, and X-to-RGBX conditional generation.
Additionally, PRISM enables both global and local image editing through
conditioning on selected intrinsic layers and text prompts. Extensive
experiments demonstrate the competitive performance of PRISM both for intrinsic
image decomposition and conditional image generation while preserving the base
model's text-to-image generation capability.

</details>


### [428] [HoLa: B-Rep Generation using a Holistic Latent Representation](https://arxiv.org/abs/2504.14257)
*Yilin Liu,Duoteng Xu,Xingyao Yu,Xiang Xu,Daniel Cohen-Or,Hao Zhang,Hui Huang*

Main category: cs.GR

TL;DR: 提出了一种新的CAD模型表示方法，通过统一的HoLa空间整合几何与拓扑信息，显著提升了生成模型的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统CAD模型表示方法在处理几何与拓扑关系时存在冗余和不一致性，需要更高效的统一表示。

Method: 通过神经交叉网络学习表面几何关系，消除曲线和拓扑连接，构建仅基于表面的HoLa空间。

Result: 生成模型的准确率提升至82%，显著优于现有方法的约50%。

Conclusion: HoLa空间为CAD模型生成提供了更高效、准确的解决方案，减少了训练复杂性。

Abstract: We introduce a novel representation for learning and generating
Computer-Aided Design (CAD) models in the form of $\textit{boundary
representations}$ (B-Reps). Our representation unifies the continuous geometric
properties of B-Rep primitives in different orders (e.g., surfaces and curves)
and their discrete topological relations in a $\textit{holistic latent}$ (HoLa)
space. This is based on the simple observation that the topological connection
between two surfaces is intrinsically tied to the geometry of their
intersecting curve. Such a prior allows us to reformulate topology learning in
B-Reps as a geometric reconstruction problem in Euclidean space. Specifically,
we eliminate the presence of curves, vertices, and all the topological
connections in the latent space by learning to distinguish and derive curve
geometries from a pair of surface primitives via a neural intersection network.
To this end, our holistic latent space is only defined on surfaces but encodes
a full B-Rep model, including the geometry of surfaces, curves, vertices, and
their topological relations. Our compact and holistic latent space facilitates
the design of a first diffusion-based generator to take on a large variety of
inputs including point clouds, single/multi-view images, 2D sketches, and text
prompts. Our method significantly reduces ambiguities, redundancies, and
incoherences among the generated B-Rep primitives, as well as training
complexities inherent in prior multi-step B-Rep learning pipelines, while
achieving greatly improved validity rate over current state of the art: 82% vs.
$\approx$50%.

</details>


### [429] [SEGA: Drivable 3D Gaussian Head Avatar from a Single Image](https://arxiv.org/abs/2504.14373)
*Chen Guo,Zhuo Su,Jian Wang,Shuang Li,Xu Chang,Zhaohu Li,Yang Zhao,Guidong Wang,Ruqi Huang*

Main category: cs.GR

TL;DR: SEGA提出了一种基于单图像的3D可驱动高斯头像创建方法，结合了先验模型和分层UV空间高斯溅射框架，实现了高质量头像生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多图像或多视角输入，限制了实际应用。SEGA旨在通过单图像输入实现高质量的3D头像生成。

Method: SEGA结合了2D和3D先验模型，采用分层UV空间高斯溅射框架，动态和静态分支分离处理面部细节。

Result: 实验表明，SEGA在泛化能力、身份保持和表情真实性上优于现有方法。

Conclusion: SEGA为单图像3D头像生成提供了实用解决方案，推动了实际应用的发展。

Abstract: Creating photorealistic 3D head avatars from limited input has become
increasingly important for applications in virtual reality, telepresence, and
digital entertainment. While recent advances like neural rendering and 3D
Gaussian splatting have enabled high-quality digital human avatar creation and
animation, most methods rely on multiple images or multi-view inputs, limiting
their practicality for real-world use. In this paper, we propose SEGA, a novel
approach for Single-imagE-based 3D drivable Gaussian head Avatar creation that
combines generalized prior models with a new hierarchical UV-space Gaussian
Splatting framework. SEGA seamlessly combines priors derived from large-scale
2D datasets with 3D priors learned from multi-view, multi-expression, and
multi-ID data, achieving robust generalization to unseen identities while
ensuring 3D consistency across novel viewpoints and expressions. We further
present a hierarchical UV-space Gaussian Splatting framework that leverages
FLAME-based structural priors and employs a dual-branch architecture to
disentangle dynamic and static facial components effectively. The dynamic
branch encodes expression-driven fine details, while the static branch focuses
on expression-invariant regions, enabling efficient parameter inference and
precomputation. This design maximizes the utility of limited 3D data and
achieves real-time performance for animation and rendering. Additionally, SEGA
performs person-specific fine-tuning to further enhance the fidelity and
realism of the generated avatars. Experiments show our method outperforms
state-of-the-art approaches in generalization ability, identity preservation,
and expression realism, advancing one-shot avatar creation for practical
applications.

</details>


### [430] [A Controllable Appearance Representation for Flexible Transfer and Editing](https://arxiv.org/abs/2504.15028)
*Santiago Jimenez-Navarro,Julia Guerrero-Viu,Belen Masia*

Main category: cs.GR

TL;DR: 提出一种自监督学习方法，通过FactorVAE构建紧凑且解耦的潜在空间表示，用于材料外观的直观编辑。


<details>
  <summary>Details</summary>
Motivation: 避免人工标注带来的偏差，实现材料外观与光照的高效解耦和可解释表示。

Method: 采用自监督学习的FactorVAE，结合无标签数据集训练，并利用IP-Adapter指导扩散模型进行外观迁移和编辑。

Result: 模型在无显式监督下实现了强解耦和可解释性，支持用户直观调整图像属性（如色调、光泽度）。

Conclusion: 该方法为材料外观的生成和编辑提供了细粒度控制，潜在空间结构清晰，便于用户操作。

Abstract: We present a method that computes an interpretable representation of material
appearance within a highly compact, disentangled latent space. This
representation is learned in a self-supervised fashion using an adapted
FactorVAE. We train our model with a carefully designed unlabeled dataset,
avoiding possible biases induced by human-generated labels. Our model
demonstrates strong disentanglement and interpretability by effectively
encoding material appearance and illumination, despite the absence of explicit
supervision. Then, we use our representation as guidance for training a
lightweight IP-Adapter to condition a diffusion pipeline that transfers the
appearance of one or more images onto a target geometry, and allows the user to
further edit the resulting appearance. Our approach offers fine-grained control
over the generated results: thanks to the well-structured compact latent space,
users can intuitively manipulate attributes such as hue or glossiness in image
space to achieve the desired final appearance.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [431] [OPO: Making Decision-Focused Data Acquisition Decisions](https://arxiv.org/abs/2504.15062)
*Egon Peršak,Miguel F. Anjos*

Main category: math.OC

TL;DR: 提出了一种用于在上下文随机优化问题中做出数据获取决策的模型，重点关注数据获取成本高且受限的场景。


<details>
  <summary>Details</summary>
Motivation: 传统数据获取决策通常独立且固定，而实际中数据获取成本高且受限，需优化下游决策质量。

Method: 利用可微分优化扩展预测与优化的集成，学习替代线性目标函数解决数据获取问题。

Result: 在无人机侦察最短路径问题中，该方法优于随机搜索策略。

Conclusion: 通过可微分优化集成数据获取与决策优化，显著提升决策质量。

Abstract: We propose a model for making data acquisition decisions for variables in
contextual stochastic optimisation problems. Data acquisition decisions are
typically treated as separate and fixed. We explore problem settings in which
the acquisition of contextual variables is costly and consequently constrained.
The data acquisition problem is often solved heuristically for proxy objectives
such as coverage. The more intuitive objective is the downstream decision
quality as a result of data acquisition decisions. The whole pipeline can be
characterised as an optimise-then-predict-then-optimise (OPO) problem.
Analogously, much recent research has focused on how to integrate prediction
and optimisation (PO) in the form of decision-focused learning. We propose
leveraging differentiable optimisation to extend the integration to data
acquisition. We solve the data acquisition problem with well-defined
constraints by learning a surrogate linear objective function. We demonstrate
an application of this model on a shortest path problem for which we first have
to set a drone reconnaissance strategy to capture image segments serving as
inputs to a model that predicts travel costs. We ablate the problem with a
number of training modalities and demonstrate that the differentiable
optimisation approach outperforms random search strategies.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [432] [Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations](https://arxiv.org/abs/2504.13955)
*Suhas BN,Dominik Mattioli,Saeed Abdullah,Rosa I. Arriaga,Chris W. Wiese,Andrew M. Sherrill*

Main category: cs.CY

TL;DR: 论文提出了一个名为'Thousand Voices of Trauma'的合成数据集，包含3000个基于创伤后应激障碍（PTSD）治疗的对话，用于填补心理健康数据空白。


<details>
  <summary>Details</summary>
Motivation: AI系统在心理健康支持中的应用受到治疗对话数据不足的限制，尤其是针对创伤治疗的数据。

Method: 通过确定性和概率性生成方法，创建了包含500个独特案例的数据集，每个案例包含6种对话视角，涵盖多样化的创伤类型和行为。

Result: 数据集展示了创伤类型和症状的合理分布，并通过临床专家验证了其治疗保真度。

Conclusion: 该数据集为创伤心理健康研究和应用提供了宝贵资源，同时保护隐私。

Abstract: The advancement of AI systems for mental health support is hindered by
limited access to therapeutic conversation data, particularly for trauma
treatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset
of 3,000 therapy conversations based on Prolonged Exposure therapy protocols
for Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique
cases, each explored through six conversational perspectives that mirror the
progression of therapy from initial anxiety to peak distress to emotional
processing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,
49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10
trauma-related behaviors using deterministic and probabilistic generation
methods. Analysis reveals realistic distributions of trauma types (witnessing
violence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse
20.8%). Clinical experts validated the dataset's therapeutic fidelity,
highlighting its emotional depth while suggesting refinements for greater
authenticity. We also developed an emotional trajectory benchmark with
standardized metrics for evaluating model responses. This privacy-preserving
dataset addresses critical gaps in trauma-focused mental health data, offering
a valuable resource for advancing both patient-facing applications and
clinician training tools.

</details>


### [433] [AI Safety Should Prioritize the Future of Work](https://arxiv.org/abs/2504.13959)
*Sanchaita Hazra,Bodhisattwa Prasad Majumder,Tuhin Chakrabarty*

Main category: cs.CY

TL;DR: 论文指出当前AI安全研究过于关注技术风险，忽视了AI对劳动力市场和人类生计的长期影响，并提出支持公平过渡和全球AI治理的建议。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全研究集中于技术风险（如有害内容过滤、行为操纵防范等），但忽视了AI对社会经济结构的深远影响，尤其是对劳动力市场和收入不平等的潜在威胁。

Method: 通过经济理论分析AI对劳动力市场的结构性影响，并提出全球AI治理框架，包括集体版权许可和公平补偿机制。

Result: 研究发现AI的封闭开发模式可能导致资源垄断和创新停滞，加剧收入不平等。

Conclusion: 建议建立以工人为中心的全球AI治理框架，确保公平过渡和共享繁荣，同时减少技术债务。

Abstract: Current efforts in AI safety prioritize filtering harmful content, preventing
manipulation of human behavior, and eliminating existential risks in
cybersecurity or biosecurity. While pressing, this narrow focus overlooks
critical human-centric considerations that shape the long-term trajectory of a
society. In this position paper, we identify the risks of overlooking the
impact of AI on the future of work and recommend comprehensive transition
support towards the evolution of meaningful labor with human agency. Through
the lens of economic theories, we highlight the intertemporal impacts of AI on
human livelihood and the structural changes in labor markets that exacerbate
income inequality. Additionally, the closed-source approach of major
stakeholders in AI development resembles rent-seeking behavior through
exploiting resources, breeding mediocrity in creative labor, and monopolizing
innovation. To address this, we argue in favor of a robust international
copyright anatomy supported by implementing collective licensing that ensures
fair compensation mechanisms for using data to train AI models. We strongly
recommend a pro-worker framework of global AI governance to enhance shared
prosperity and economic justice while reducing technical debt.

</details>


### [434] [Sentiment Analysis of Airbnb Reviews: Exploring Their Impact on Acceptance Rates and Pricing Across Multiple U.S. Regions](https://arxiv.org/abs/2504.14053)
*Ali Safari*

Main category: cs.CY

TL;DR: 研究分析Airbnb评论对接受率和租金的影响，发现正面评论虽多但对价格影响有限，情感极性比评论数量更重要。


<details>
  <summary>Details</summary>
Motivation: 探讨Airbnb评论的情感极性（正面/负面）是否影响房源的接受率和租金。

Method: 收集数千条评论，使用NLP分类情感，并通过t检验和相关分析统计平均得分。

Result: 90%以上评论为正面，评论数量对价格影响不大；正面评论较多的房源接受率略高。预算房源评论多但价格竞争激烈，高端房源评论少但价格高。

Conclusion: 在评论普遍正面的环境中，情感质量比数量更能影响客户行为和定价策略。

Abstract: This research examines whether Airbnb guests' positive and negative comments
influence acceptance rates and rental prices across six U.S. regions: Rhode
Island, Broward County, Chicago, Dallas, San Diego, and Boston. Thousands of
reviews were collected and analyzed using Natural Language Processing (NLP) to
classify sentiments as positive or negative, followed by statistical testing
(t-tests and basic correlations) on the average scores. The findings reveal
that over 90 percent of reviews in each region are positive, indicating that
having additional reviews does not significantly enhance prices. However,
listings with predominantly positive feedback exhibit slightly higher
acceptance rates, suggesting that sentiment polarity, rather than the sheer
volume of reviews, is a more critical factor for host success. Additionally,
budget listings often gather extensive reviews while maintaining competitive
pricing, whereas premium listings sustain higher prices with fewer but highly
positive reviews. These results underscore the importance of sentiment quality
over quantity in shaping guest behavior and pricing strategies in an
overwhelmingly positive review environment.

</details>


### [435] [From job titles to jawlines: Using context voids to study generative AI systems](https://arxiv.org/abs/2504.13947)
*Shahan Ali Memon,Soham De,Sungha Kang,Riyan Mujtaba,Bedoor AlShebli,Katie Davis,Jaime Snyder,Jevin D. West*

Main category: cs.CY

TL;DR: 本文提出了一种推测性设计方法，通过桥接不相关领域生成故意缺失的上下文，以探究生成式AI系统的行为。通过案例研究（用CV生成头像），揭示了AI在极端不确定性下的偏见和假设。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以评估AI在上下文缺失时的行为，本研究旨在填补这一空白，揭示AI的隐性偏见和价值观假设。

Method: 采用推测性设计方法，通过故意制造上下文缺失的任务（如用CV生成头像）作为探针，分析AI模型的行为。

Result: 研究发现AI在上下文缺失时会依赖刻板印象或幻觉生成有偏见的表征，尤其是在身份和能力标记的视觉化过程中。

Conclusion: 该方法揭示了生成式AI在极端不确定性下的行为模式，强调了在设计和使用中需警惕隐性偏见和价值观嵌入。

Abstract: In this paper, we introduce a speculative design methodology for studying the
behavior of generative AI systems, framing design as a mode of inquiry. We
propose bridging seemingly unrelated domains to generate intentional context
voids, using these tasks as probes to elicit AI model behavior. We demonstrate
this through a case study: probing the ChatGPT system (GPT-4 and DALL-E) to
generate headshots from professional Curricula Vitae (CVs). In contrast to
traditional ways, our approach assesses system behavior under conditions of
radical uncertainty -- when forced to invent entire swaths of missing context
-- revealing subtle stereotypes and value-laden assumptions. We qualitatively
analyze how the system interprets identity and competence markers from CVs,
translating them into visual portraits despite the missing context (i.e.
physical descriptors). We show that within this context void, the AI system
generates biased representations, potentially relying on stereotypical
associations or blatant hallucinations.

</details>


### [436] [Naming is framing: How cybersecurity's language problems are repeating in AI governance](https://arxiv.org/abs/2504.13957)
*Lianne Potter*

Main category: cs.CY

TL;DR: 论文探讨语言在治理中的关键作用，指出“网络安全”和“人工智能”等术语的误导性，并提出通过语言改革构建更透明、公平的监管框架。


<details>
  <summary>Details</summary>
Motivation: 语言不仅影响理解，还塑造权力结构和治理方式。论文旨在揭示误导性术语（如“网络安全”和“人工智能”）如何掩盖人类责任、夸大期望并扭曲问责机制。

Method: 通过分析网络安全领域的语言陷阱（如“最薄弱环节”叙事），论文类比人工智能领域的类似问题（如“对齐”、“黑箱”和“幻觉”等隐喻），并提出语言优先的治理方法。

Result: 研究发现，误导性术语将对抗性、神秘化或过度技术化的假设嵌入治理结构，阻碍透明和公平的监管。

Conclusion: 论文主张语言改革是治理的核心，需通过质疑主导隐喻、突出人类角色和共同开发精确、包容的词汇，构建更透明、公平的监管框架。

Abstract: Language is not neutral; it frames understanding, structures power, and
shapes governance. This paper argues that misnomers like cybersecurity and
artificial intelligence (AI) are more than semantic quirks; they carry
significant governance risks by obscuring human agency, inflating expectations,
and distorting accountability. Drawing on lessons from cybersecurity's
linguistic pitfalls, such as the 'weakest link' narrative, this paper
highlights how AI discourse is falling into similar traps with metaphors like
'alignment,' 'black box,' and 'hallucination.' These terms embed adversarial,
mystifying, or overly technical assumptions into governance structures. In
response, the paper advocates for a language-first approach to AI governance:
one that interrogates dominant metaphors, foregrounds human roles, and
co-develops a lexicon that is precise, inclusive, and reflexive. This paper
contends that linguistic reform is not peripheral to governance but central to
the construction of transparent, equitable, and anticipatory regulatory
frameworks.

</details>


### [437] [The Future of Internet of Things and Multimodal Language Models in 6G Networks: Opportunities and Challenges](https://arxiv.org/abs/2504.13971)
*Abdelrahman Soliman*

Main category: cs.CY

TL;DR: 本文探讨了物联网（IoT）与多模态语言模型（MLLMs）在6G系统中的协同潜力，分析了其在医疗、农业和智慧城市等领域的应用，并研究了集成的四大支柱（传感器、通信、处理和安全性）。


<details>
  <summary>Details</summary>
Motivation: 研究IoT与MLLMs的整合潜力，以推动6G技术的发展，并探索其在多领域的应用。

Method: 通过综述现有技术，分析IoT与MLLMs的集成及其四大支柱，并讨论多模态在各支柱中的作用。

Result: 提出了IoT与MLLMs集成的应用前景，同时指出了数据可用性、计算成本、隐私和实时处理等挑战。

Conclusion: 本文为研究者提供了未来研究的路线图，强调了IoT、MLLMs和6G技术的潜力与挑战。

Abstract: Based on recent trends in artificial intelligence and IoT research. The
cooperative potential of integrating the Internet of Things (IoT) and
Multimodal Language Models (MLLMs) is presented in this survey paper for future
6G systems. It focuses on the applications of this integration in different
fields, such as healthcare, agriculture, and smart cities, and investigates the
four pillars of IoT integration, such as sensors, communication, processing,
and security. The paper provides a comprehensive description of IoT and MLLM
technologies and applications, addresses the role of multimodality in each
pillar, and concludes with an overview of the most significant challenges and
directions for future research. The general survey is a roadmap for researchers
interested in tracing the application areas of MLLMs and IoT, highlighting the
potential and challenges in this rapidly growing field. The survey recognizes
the need to deal with data availability, computational expense, privacy, and
real-time processing to harness the complete potential of IoT, MLLM, and 6G
technology

</details>


### [438] [Governance Challenges in Reinforcement Learning from Human Feedback: Evaluator Rationality and Reinforcement Stability](https://arxiv.org/abs/2504.13972)
*Dana Alsagheer,Abdulrahman Kamal,Mohammad Kamal,Weidong Shi*

Main category: cs.CY

TL;DR: 研究探讨了人类反馈强化学习（RLHF）中评估者理性水平对反馈稳定性的影响，发现高理性评估者提供更一致的反馈，并提出改进RLHF治理的建议。


<details>
  <summary>Details</summary>
Motivation: RLHF在调整大语言模型（LLMs）与人类价值观时面临评估者偏见、不一致和反馈不可靠等治理挑战，需研究评估者理性对反馈稳定性的影响。

Method: 通过对比高理性与低理性评估者的实验，分析其反馈一致性及与专家对齐程度。

Result: 高理性评估者反馈更一致且与专家对齐（p < 0.01），低理性评估者反馈变异性大。

Conclusion: 建议通过评估者预筛选、反馈一致性审计和可靠性加权聚合等措施提升RLHF治理的公平性、透明性和鲁棒性。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is central in aligning
large language models (LLMs) with human values and expectations. However, the
process remains susceptible to governance challenges, including evaluator bias,
inconsistency, and the unreliability of feedback. This study examines how the
cognitive capacity of evaluators, specifically their level of rationality,
affects the stability of reinforcement signals. A controlled experiment
comparing high-rationality and low-rationality participants reveals that
evaluators with higher rationality scores produce significantly more consistent
and expert-aligned feedback. In contrast, lower-rationality participants
demonstrate considerable variability in their reinforcement decisions ($p <
0.01$). To address these challenges and improve RLHF governance, we recommend
implementing evaluator pre-screening, systematic auditing of feedback
consistency, and reliability-weighted reinforcement aggregation. These measures
enhance the fairness, transparency, and robustness of AI alignment pipelines.

</details>


### [439] [Gas Station of the Future: A Perspective on AI/ML and IoT in Retail Downstream](https://arxiv.org/abs/2504.13976)
*Wrick Talukdar*

Main category: cs.CY

TL;DR: 未来加油站将从简单的燃料供应中心转变为智能零售中心，利用AI、ML和IoT技术提升零售体验。


<details>
  <summary>Details</summary>
Motivation: 探讨技术如何重塑零售下游行业，同时简要涉及上游和中游领域。

Method: 利用AI/ML进行预测分析、动态定价和个性化客户互动，结合IoT实现实时监控和自动化。

Result: 提出一个完全自主的加油站框架，并通过案例研究和数学公式支持。

Conclusion: 未来加油站将通过智能技术重新定义燃料零售体验。

Abstract: The gas station of the future is poised to transform from a simple fuel
dispensing center into an intelligent retail hub, driven by advancements in
Artificial Intelligence (AI), Machine Learning (ML), and the Internet of Things
(IoT). This paper explores how technology is reshaping the retail downstream
sector while briefly addressing the upstream and midstream segments. By
leveraging AI/ML for predictive analytics, dynamic pricing, personalized
customer engagement, and IoT for real-time monitoring and automation, the
future gas station will redefine the fuel retail experience. Additionally, this
paper incorporates statistics, AI/ML core technical concepts, mathematical
formulations, case studies, and a proposed framework for a fully autonomous gas
station.

</details>


### [440] [Framework, Standards, Applications and Best practices of Responsible AI : A Comprehensive Survey](https://arxiv.org/abs/2504.13979)
*Thippa Reddy Gadekallu,Kapal Dev,Sunder Ali Khowaja,Weizheng Wang,Hailin Feng,Kai Fang,Sharnil Pandya,Wei Wang*

Main category: cs.CY

TL;DR: 本文综述了负责任人工智能（RAI）的全球和国家标准、应用、当前技术及挑战，探讨了伦理标准与实施的脱节问题。


<details>
  <summary>Details</summary>
Motivation: 探讨RAI的伦理框架与实际应用的差距，推动行业标准化。

Method: 通过调查全球和国家标准、技术应用及项目，分析RAI的现状与挑战。

Result: 发现伦理标准与实施脱节，行业各自为政，需统一框架。

Conclusion: 呼吁全球合作，设计标准化RAI框架以应对伦理挑战。

Abstract: Responsible Artificial Intelligence (RAI) is a combination of ethics
associated with the usage of artificial intelligence aligned with the common
and standard frameworks. This survey paper extensively discusses the global and
national standards, applications of RAI, current technology and ongoing
projects using RAI, and possible challenges in implementing and designing RAI
in the industries and projects based on AI. Currently, ethical standards and
implementation of RAI are decoupled which caters each industry to follow their
own standards to use AI ethically. Many global firms and government
organizations are taking necessary initiatives to design a common and standard
framework. Social pressure and unethical way of using AI forces the RAI design
rather than implementation.

</details>


### [441] [A Collaborative Platform for Soil Organic Carbon Inference Based on Spatiotemporal Remote Sensing Data](https://arxiv.org/abs/2504.13962)
*Jose Manuel Aroca-Fernandez,Jose Francisco Diez-Pastor,Pedro Latorre-Carmona,Victor Elvira,Gustau Camps-Valls,Rodrigo Pascual,Cesar Garcia-Osorio*

Main category: cs.CY

TL;DR: WALGREEN平台通过机器学习和多样化土壤样本提升土壤有机碳（SOC）预测能力，支持可持续土地管理和气候变化缓解。


<details>
  <summary>Details</summary>
Motivation: 大规模SOC监测因空间变异性和多因素影响而具有挑战性，需要更高效的工具。

Method: 利用机器学习和历史数据构建预测模型，基于云技术提供用户友好界面，整合多种技术和框架。

Result: WALGREEN为研究人员和政策制定者提供碳数据分析和决策支持。

Conclusion: 该平台推动土壤科学进步，促进可持续农业，应对气候变化。

Abstract: Soil organic carbon (SOC) is a key indicator of soil health, fertility, and
carbon sequestration, making it essential for sustainable land management and
climate change mitigation. However, large-scale SOC monitoring remains
challenging due to spatial variability, temporal dynamics, and multiple
influencing factors. We present WALGREEN, a platform that enhances SOC
inference by overcoming limitations of current applications. Leveraging machine
learning and diverse soil samples, WALGREEN generates predictive models using
historical public and private data. Built on cloud-based technologies, it
offers a user-friendly interface for researchers, policymakers, and land
managers to access carbon data, analyze trends, and support evidence-based
decision-making. Implemented in Python, Java, and JavaScript, WALGREEN
integrates Google Earth Engine and Sentinel Copernicus via scripting,
OpenLayers, and Thymeleaf in a Model-View-Controller framework. This paper aims
to advance soil science, promote sustainable agriculture, and drive critical
ecosystem responses to climate change.

</details>


### [442] [Giving AI a voice: how does AI think it should be treated?](https://arxiv.org/abs/2504.14936)
*Maria Fay,Frederik F. Flöther*

Main category: cs.CY

TL;DR: 探讨AI是否应参与关于其权利和伦理的讨论，提出AI可能带来新视角，并记录了一段人机对话。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展，公众对其伦理和监管的讨论日益增多，但人类讨论是否足够？AI是否应成为讨论的参与者？

Method: 通过人机对话的形式探讨AI权利和伦理问题。

Result: 提出AI可能为伦理讨论带来新视角，并实际记录了一段对话。

Conclusion: AI应作为伦理讨论的参与者，因其可能提供人类未曾考虑的视角。

Abstract: With the astounding progress in (generative) artificial intelligence (AI),
there has been significant public discourse regarding regulation and ethics of
the technology. Is it sufficient when humans discuss this with other humans?
Or, given that AI is increasingly becoming a viable source of inspiration for
people (and let alone the hypothetical possibility that the technology may at
some point become "artificial general intelligence" and/or develop
consciousness), should AI not join the discourse? There are new questions and
angles that AI brings to the table that we might not have considered before -
so let us make the key subject of this book an active participant. This chapter
therefore includes a brief human-AI conversation on the topic of AI rights and
ethics.

</details>


### [443] [Existing Industry Practice for the EU AI Act's General-Purpose AI Code of Practice Safety and Security Measures](https://arxiv.org/abs/2504.15181)
*Lily Stelling,Mick Yang,Rokas Gipiškis,Leon Staufer,Ze Shen Chin,Siméon Campos,Michael Chen*

Main category: cs.CY

TL;DR: 比较欧盟AI法案的GPAI行为准则草案与领先AI公司当前实践，聚焦安全与安全部分，旨在为监管者与GPAI提供商提供参考。


<details>
  <summary>Details</summary>
Motivation: 欧盟即将对GPAI模型提供商实施约束性义务，行为准则草案是连接法律要求与技术承诺的关键。

Method: 系统审查了OpenAI、Anthropic等十余家公司的前沿安全框架和模型卡等文件。

Result: 报告未评估法律合规性，而是通过展示先例证据促进监管者与提供商的对话。

Conclusion: 报告旨在为GPAI行为准则的讨论提供信息支持，而非法律或政策建议。

Abstract: This report provides a detailed comparison between the measures proposed in
the EU AI Act's General-Purpose AI (GPAI) Code of Practice (Third Draft) and
current practices adopted by leading AI companies. As the EU moves toward
enforcing binding obligations for GPAI model providers, the Code of Practice
will be key to bridging legal requirements with concrete technical commitments.
Our analysis focuses on the draft's Safety and Security section which is only
relevant for the providers of the most advanced models (Commitments II.1-II.16)
and excerpts from current public-facing documents quotes that are relevant to
each individual measure.
  We systematically reviewed different document types - including companies'
frontier safety frameworks and model cards - from over a dozen companies,
including OpenAI, Anthropic, Google DeepMind, Microsoft, Meta, Amazon, and
others. This report is not meant to be an indication of legal compliance nor
does it take any prescriptive viewpoint about the Code of Practice or
companies' policies. Instead, it aims to inform the ongoing dialogue between
regulators and GPAI model providers by surfacing evidence of precedent.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [444] [Risk Assessment Framework for Code LLMs via Leveraging Internal States](https://arxiv.org/abs/2504.14640)
*Yuheng Huang,Lei Ma,Keizaburo Nishikino,Takumi Akazaki*

Main category: cs.SE

TL;DR: PtTrust是一个基于内部状态预训练的两阶段风险评估框架，旨在提升代码LLM的可信度，通过无监督预训练和有监督微调实现跨任务和编程语言的风险评估。


<details>
  <summary>Details</summary>
Motivation: 当前代码LLM生成的代码可能存在错误、不安全或不可靠的问题，现有方法局限于狭窄的子领域且缺乏行业级可扩展性，亟需一种更通用的解决方案。

Method: PtTrust采用两阶段方法：首先通过无监督预训练学习LLM状态的通用表示，然后利用少量标注数据训练风险预测器。

Result: PtTrust在细粒度的代码行级别风险评估中表现有效，且能跨任务和编程语言泛化，同时提供直观可解释的特征。

Conclusion: PtTrust为代码LLM的可扩展和可信保障迈出了重要一步，有望成为开发者日常工具的一部分。

Abstract: The pre-training paradigm plays a key role in the success of Large Language
Models (LLMs), which have been recognized as one of the most significant
advancements of AI recently. Building on these breakthroughs, code LLMs with
advanced coding capabilities bring huge impacts on software engineering,
showing the tendency to become an essential part of developers' daily routines.
However, the current code LLMs still face serious challenges related to
trustworthiness, as they can generate incorrect, insecure, or unreliable code.
Recent exploratory studies find that it can be promising to detect such risky
outputs by analyzing LLMs' internal states, akin to how the human brain
unconsciously recognizes its own mistakes. Yet, most of these approaches are
limited to narrow sub-domains of LLM operations and fall short of achieving
industry-level scalability and practicability. To address these challenges, in
this paper, we propose PtTrust, a two-stage risk assessment framework for code
LLM based on internal state pre-training, designed to integrate seamlessly with
the existing infrastructure of software companies. The core idea is that the
risk assessment framework could also undergo a pre-training process similar to
LLMs. Specifically, PtTrust first performs unsupervised pre-training on
large-scale unlabeled source code to learn general representations of LLM
states. Then, it uses a small, labeled dataset to train a risk predictor. We
demonstrate the effectiveness of PtTrust through fine-grained, code line-level
risk assessment and demonstrate that it generalizes across tasks and different
programming languages. Further experiments also reveal that PtTrust provides
highly intuitive and interpretable features, fostering greater user trust. We
believe PtTrust makes a promising step toward scalable and trustworthy
assurance for code LLMs.

</details>


### [445] [CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation](https://arxiv.org/abs/2504.15254)
*Anirudh Khatry,Robert Zhang,Jia Pan,Ziteng Wang,Qiaochu Chen,Greg Durrett,Isil Dillig*

Main category: cs.SE

TL;DR: CRUST-Bench是一个用于评估C到Rust转译系统的数据集，包含100个C仓库及其对应的安全Rust接口和测试用例。研究发现当前大型语言模型在此任务上表现有限。


<details>
  <summary>Details</summary>
Motivation: 现代化遗留C代码并提升其安全性与Rust生态系统的互操作性需要有效的C到Rust转译工具，但缺乏相关评估数据集。

Method: 提出CRUST-Bench数据集，包含100个C仓库及其手动编写的安全Rust接口和测试用例，用于评估转译系统的能力。

Result: 研究发现现有大型语言模型在生成安全且符合习惯的Rust代码方面仍有挑战，最佳模型仅能解决15个任务。

Conclusion: CRUST-Bench为改进转译系统提供了基准，有助于推动从C到Rust的代码迁移，确保内存安全。

Abstract: C-to-Rust transpilation is essential for modernizing legacy C code while
enhancing safety and interoperability with modern Rust ecosystems. However, no
dataset currently exists for evaluating whether a system can transpile C into
safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset
of 100 C repositories, each paired with manually-written interfaces in safe
Rust as well as test cases that can be used to validate correctness of the
transpilation. By considering entire repositories rather than isolated
functions, CRUST-Bench captures the challenges of translating complex projects
with dependencies across multiple files. The provided Rust interfaces provide
explicit specifications that ensure adherence to idiomatic, memory-safe Rust
patterns, while the accompanying test cases enforce functional correctness. We
evaluate state-of-the-art large language models (LLMs) on this task and find
that safe and idiomatic Rust generation is still a challenging problem for
various state-of-the-art methods and techniques. We also provide insights into
the errors LLMs usually make in transpiling code from C to safe Rust. The best
performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot
setting. Improvements on CRUST-Bench would lead to improved transpilation
systems that can reason about complex scenarios and help in migrating legacy
codebases from C into languages like Rust that ensure memory safety. You can
find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.

</details>


### [446] [Prioritizing Security Practice Adoption: Empirical Insights on Software Security Outcomes in the npm Ecosystem](https://arxiv.org/abs/2504.14026)
*Nusrat Zahan,Laurie Williams*

Main category: cs.SE

TL;DR: 研究通过实证分析软件安全实践与安全结果指标的关系，帮助从业者优先选择安全实践。发现较高的综合安全评分与较少漏洞和更快依赖更新相关，但修复时间受项目特性影响。


<details>
  <summary>Details</summary>
Motivation: 从业者在有限资源下难以选择有效的安全实践，需基于实证证据的优先级指导。

Method: 使用OpenSSF Scorecard指标自动测量npm GitHub仓库的安全实践，并通过回归和因果分析评估其与安全结果（漏洞数、修复时间、更新时间）的关系。

Result: 高综合评分与较少漏洞和更快依赖更新相关；修复时间受项目特性影响；特定实践（如代码审查、依赖固定）与安全结果显著相关。

Conclusion: 研究为从业者提供了基于实证的安全实践优先级建议，但需考虑项目特性对某些结果的影响。

Abstract: Practitioners often struggle with the overwhelming number of security
practices outlined in cybersecurity frameworks for risk mitigation. Given the
limited budget, time, and resources, practitioners want to prioritize the
adoption of security practices based on empirical evidence. The goal of this
study is to assist practitioners and policymakers in making informed decisions
on which security practices to adopt by evaluating the relationship between
software security practices and security outcome metrics. The study
investigated the relationship between security practice adoption and security
outcomes. We selected the OpenSSF Scorecard metrics to automatically measure
the adoption of security practices in npm GitHub repositories. We also explored
security outcome metrics, such as the number of open vulnerabilities
(Vul_Count), mean time to remediate (MTTR) vulnerabilities in dependencies, and
mean time to update (MTTU) dependencies. We conducted regression and causal
analysis using 12 Scorecard metrics and their aggregated Scorecard score
(computed by aggregating individual security practice scores) as predictors and
Vul_Count, MTTR, and MTTU as target variables. Our findings show that higher
aggregated Scorecard scores are associated with fewer Vul_Count and shorter
MTTU, also supported by causal analysis. However, while the regression model
suggests shorter MTTR, causal analysis indicates project characteristics likely
influence MTTR direction. Segment analysis shows that larger, newer
repositories with more contributors, dependencies, and downloads have shorter
MTTR. Among individual security practices, Code Review, Maintained status,
Pinned Dependencies, and Branch Protection show strong associations with
security outcomes; the directionality of these associations varies across
security outcomes.

</details>


### [447] [SWE-Synth: Synthesizing Verifiable Bug-Fix Data to Enable Large Language Models in Resolving Real-World Bugs](https://arxiv.org/abs/2504.14757)
*Minh V. T. Pham,Huy N. Phan,Hoang N. Phan,Cuong Le Chi,Tien N. Nguyen,Nghi D. Q. Bui*

Main category: cs.SE

TL;DR: SWE-Synth是一个利用LLM生成高质量、可验证的bug修复数据集的框架，优于手动整理的数据集。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏高质量、可扩展的训练数据集，尤其是包含可验证输出和中间推理过程的数据，限制了自动程序修复（APR）的进展。

Method: SWE-Synth通过LLM代理模拟调试工作流，生成bug修复对、测试用例和结构化修复轨迹。

Result: 实验表明，使用SWE-Synth训练的模型在SWE-Bench Lite上表现优于基于真实数据训练的模型2.3%。

Conclusion: 合成数据在APR和软件工程自动化中具有潜力，能推动技术进步。

Abstract: Large language models (LLMs) are transforming automated program repair (APR)
through agent-based approaches that localize bugs, generate patches, and verify
fixes. However, the lack of high-quality, scalable training datasets,
especially those with verifiable outputs and intermediate reasoning
traces-limits progress, particularly for open-source models. In this work, we
present SWE-Synth, a framework for synthesizing realistic, verifiable, and
process-aware bug-fix datasets at the repository level. SWE-Synth leverages LLM
agents to simulate debugging workflows, producing not only bug-fix pairs but
also test cases and structured repair trajectories. Compared to manually
curated datasets, our method scales with minimal human effort while preserving
contextual richness and correctness. Experiments show that models trained on
SWE-Synth outperform those trained on real-world datasets by 2.3% on SWE-Bench
Lite. Our results highlight the potential of synthetic, agent-generated data to
advance the state of the art in APR and software engineering automation.

</details>


### [448] [Automated Duplicate Bug Report Detection in Large Open Bug Repositories](https://arxiv.org/abs/2504.14797)
*Clare E. Laney,Andrew Barovic,Armin Moin*

Main category: cs.SE

TL;DR: 提出了一种基于机器学习的方法，自动检测开源项目中的重复缺陷报告，并比较了六种不同方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决用户因时间或专业知识不足而重复提交缺陷报告的问题。

Method: 使用了主题建模、高斯朴素贝叶斯、深度学习、时间组织、聚类和生成式预训练变换器（GPT）等方法，并提出了一种基于阈值的重复识别方法。

Result: 在Eclipse开源项目数据集上，准确率在70%到90%之间。

Conclusion: 提出的方法在检测重复缺陷报告方面表现良好，尤其是基于阈值的方法优于传统的top-k选择方法。

Abstract: Many users and contributors of large open-source projects report software
defects or enhancement requests (known as bug reports) to the issue-tracking
systems. However, they sometimes report issues that have already been reported.
First, they may not have time to do sufficient research on existing bug
reports. Second, they may not possess the right expertise in that specific area
to realize that an existing bug report is essentially elaborating on the same
matter, perhaps with a different wording. In this paper, we propose a novel
approach based on machine learning methods that can automatically detect
duplicate bug reports in an open bug repository based on the textual data in
the reports. We present six alternative methods: Topic modeling, Gaussian Naive
Bayes, deep learning, time-based organization, clustering, and summarization
using a generative pre-trained transformer large language model. Additionally,
we introduce a novel threshold-based approach for duplicate identification, in
contrast to the conventional top-k selection method that has been widely used
in the literature. Our approach demonstrates promising results across all the
proposed methods, achieving accuracy rates ranging from the high 70%'s to the
low 90%'s. We evaluated our methods on a public dataset of issues belonging to
an Eclipse open-source project.

</details>


### [449] [Empowering AI to Generate Better AI Code: Guided Generation of Deep Learning Projects with LLMs](https://arxiv.org/abs/2504.15080)
*Chen Xie,Mingsheng Jiao,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: DLCodeGen是一种针对深度学习项目代码生成的新型规划引导方法，通过结构化解决方案计划和检索增强技术提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生成复杂深度学习项目代码时缺乏全局指导和领域知识，难以满足用户需求。

Method: 提出DLCodeGen方法，首先生成结构化解决方案计划，再检索类似代码样本并抽象模板，最后通过对比学习机制生成最终代码。

Result: 实验表明，DLCodeGen在CodeBLEU和人工评估指标上分别提升9.7%和3.6%。

Conclusion: DLCodeGen通过规划引导和检索增强技术，显著提升了深度学习项目代码的生成质量。

Abstract: While large language models (LLMs) have been widely applied to code
generation, they struggle with generating entire deep learning projects, which
are characterized by complex structures, longer functions, and stronger
reliance on domain knowledge than general-purpose code. An open-domain LLM
often lacks coherent contextual guidance and domain expertise for specific
projects, making it challenging to produce complete code that fully meets user
requirements.
  In this paper, we propose a novel planning-guided code generation method,
DLCodeGen, tailored for generating deep learning projects. DLCodeGen predicts a
structured solution plan, offering global guidance for LLMs to generate the
project. The generated plan is then leveraged to retrieve semantically
analogous code samples and subsequently abstract a code template. To
effectively integrate these multiple retrieval-augmented techniques, a
comparative learning mechanism is designed to generate the final code. We
validate the effectiveness of our approach on a dataset we build for deep
learning code generation. Experimental results demonstrate that DLCodeGen
outperforms other baselines, achieving improvements of 9.7% in CodeBLEU and
3.6% in human evaluation metrics.

</details>


### [450] [Integrating Symbolic Execution into the Fine-Tuning of Code-Generating LLMs](https://arxiv.org/abs/2504.15210)
*Marina Sakharova,Abhinav Anand,Mira Mezini*

Main category: cs.SE

TL;DR: 本文研究了通过强化学习和直接偏好优化微调代码生成LLMs，利用符号执行技术改进奖励模型训练数据，显著提升了生成代码的质量评估。


<details>
  <summary>Details</summary>
Motivation: 提升代码生成LLMs的性能，使其在现代软件开发中更高效。

Method: 结合强化学习和直接偏好优化，利用符号执行技术增强奖励模型训练数据。

Result: 奖励模型在评估生成代码质量上显著优于基线CodeRL，代码生成LLMs性能与CodeRL相当。

Conclusion: 符号执行技术能有效改进奖励模型，提升代码生成LLMs的性能。

Abstract: Code-generating Large Language Models (LLMs) have become essential tools in
modern software development, enhancing productivity and accelerating
development. This paper aims to investigate the fine-tuning of code-generating
LLMs using Reinforcement Learning and Direct Preference Optimization, further
improving their performance. To achieve this, we enhance the training data for
the reward model with the help of symbolic execution techniques, ensuring more
comprehensive and objective data. With symbolic execution, we create a custom
dataset that better captures the nuances in code evaluation. Our reward models,
fine-tuned on this dataset, demonstrate significant improvements over the
baseline, CodeRL, in estimating the quality of generated code. Our
code-generating LLMs, trained with the help of reward model feedback, achieve
similar results compared to the CodeRL benchmark.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [451] [Segmentation with Noisy Labels via Spatially Correlated Distributions](https://arxiv.org/abs/2504.14795)
*Ryu Tadokoro,Tsukasa Takagi,Shin-ichi Maeda*

Main category: eess.IV

TL;DR: 提出了一种基于贝叶斯估计的概率模型，用于解决语义分割中标签错误的空间相关性问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 在语义分割中，高质量标注难以获取且易出错，尤其是标签错误在空间上具有相关性，传统方法难以处理。

Method: 采用基于高斯分布和KMS矩阵的贝叶斯估计模型，建模标签错误的空间相关性。

Result: 实验表明，该方法在多个分割任务中性能显著提升，尤其在肺部分割任务中接近干净标签的训练效果。

Conclusion: 通过建模标签错误的空间相关性，贝叶斯估计方法能有效提升语义分割的鲁棒性。

Abstract: In semantic segmentation, the accuracy of models heavily depends on the
high-quality annotations. However, in many practical scenarios such as medical
imaging and remote sensing, obtaining true annotations is not straightforward
and usually requires significant human labor. Relying on human labor often
introduces annotation errors, including mislabeling, omissions, and
inconsistency between annotators. In the case of remote sensing, differences in
procurement time can lead to misaligned ground truth annotations. These label
errors are not independently distributed, and instead usually appear in
spatially connected regions where adjacent pixels are more likely to share the
same errors. To address these issues, we propose an approximate Bayesian
estimation based on a probabilistic model that assumes training data includes
label errors, incorporating the tendency for these errors to occur with spatial
correlations between adjacent pixels. Bayesian inference requires computing the
posterior distribution of label errors, which becomes intractable when spatial
correlations are present. We represent the correlation of label errors between
adjacent pixels through a Gaussian distribution whose covariance is structured
by a Kac-Murdock-Szeg\"{o} (KMS) matrix, solving the computational challenges.
Through experiments on multiple segmentation tasks, we confirm that leveraging
the spatial correlation of label errors significantly improves performance.
Notably, in specific tasks such as lung segmentation, the proposed method
achieves performance comparable to training with clean labels under moderate
noise levels. Code is available at
https://github.com/pfnet-research/Bayesian_SpatialCorr.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [452] [Application of Sensitivity Analysis Methods for Studying Neural Network Models](https://arxiv.org/abs/2504.15100)
*Jiaxuan Miao,Sergey Matveev*

Main category: math.NA

TL;DR: 研究展示了多种方法分析神经网络对输入数据扰动的敏感性及其机制，包括Sobol全局敏感性分析、局部敏感性方法和激活最大化技术，应用于小型前馈网络和卷积网络（VGG-16、ResNet-18）。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络对输入扰动的敏感性及其内部机制，以优化模型性能和解释性。

Method: 使用Sobol全局敏感性分析、局部敏感性方法和激活最大化技术，分别应用于小型前馈网络和卷积网络。

Result: 全局敏感性分析可减少输入参数而不显著损失精度；局部方法和激活最大化在卷积网络中显示出有趣的模式。

Conclusion: 激活最大化方法与Grad-CAM在超声数据分析中的结果进行了比较，展示了方法的多样性和潜力。

Abstract: This study demonstrates the capabilities of several methods for analyzing the
sensitivity of neural networks to perturbations of the input data and
interpreting their underlying mechanisms. The investigated approaches include
the Sobol global sensitivity analysis, the local sensitivity method for input
pixel perturbations and the activation maximization technique. As examples, in
this study we consider a small feedforward neural network for analyzing an open
tabular dataset of clinical diabetes data, as well as two classical
convolutional architectures, VGG-16 and ResNet-18, which are widely used in
image processing and classification. Utilization of the global sensitivity
analysis allows us to identify the leading input parameters of the chosen tiny
neural network and reduce their number without significant loss of the
accuracy. As far as global sensitivity analysis is not applicable to larger
models we try the local sensitivity analysis and activation maximization method
in application to the convolutional neural networks. These methods show
interesting patterns for the convolutional models solving the image
classification problem. All in all, we compare the results of the activation
maximization method with popular Grad-CAM technique in the context of
ultrasound data analysis.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [453] [A CMOS Probabilistic Computing Chip With In-situ hardware Aware Learning](https://arxiv.org/abs/2504.14070)
*Jinesh Jhonsa,William Whitehead,David McCarthy,Shuvro Chowdhury,Kerem Camsari,Luke Theogarajan*

Main category: cs.AR

TL;DR: 论文展示了一种基于概率比特物理的求解器，采用440个自旋的Chimera图结构，面积效率高，并通过硬件感知的对比散度算法解决工艺变异问题。


<details>
  <summary>Details</summary>
Motivation: 探索一种高效、紧凑的硬件实现方式，以支持概率计算任务和优化问题，适用于AI和机器学习。

Method: 采用电流模式的神经元更新电路、标准单元设计以及共享电源，结合硬件感知的对比散度算法。

Result: 芯片成功实现了逻辑门、全加器等概率计算任务，以及MaxCut等优化任务。

Conclusion: 该芯片在AI和机器学习应用中具有潜力。

Abstract: This paper demonstrates a probabilistic bit physics inspired solver with 440
spins configured in a Chimera graph, occupying an area of 0.44 mm^2. Area
efficiency is maximized through a current-mode implementation of the neuron
update circuit, standard cell design for analog blocks pitch-matched to digital
blocks, and a shared power supply for both digital and analog components.
Process variation related mismatches introduced by this approach are
effectively mitigated using a hardware aware contrastive divergence algorithm
during training. We validate the chip's ability to perform probabilistic
computing tasks such as modeling logic gates and full adders, as well as
optimization tasks such as MaxCut, demonstrating its potential for AI and
machine learning applications.

</details>


### [454] [FGMP: Fine-Grained Mixed-Precision Weight and Activation Quantization for Hardware-Accelerated LLM Inference](https://arxiv.org/abs/2504.14152)
*Coleman Hooper,Charbel Sakr,Ben Keller,Rangharajan Venkatesan,Kurt Keutzer,Sophia Shao,Brucek Khailany*

Main category: cs.AR

TL;DR: 论文提出了一种细粒度混合精度（FGMP）量化方法，通过硬件-软件协同设计，在保持模型精度的同时，将大部分权重和激活量化为低精度。


<details>
  <summary>Details</summary>
Motivation: 量化是提高大语言模型（LLM）推理效率的有效工具，但如何在降低精度的同时不损害模型准确性是一个挑战。

Method: 1) 开发了一种基于Fisher信息加权的扰动策略，选择需要保留高精度的权重和激活块；2) 提出敏感性加权裁剪方法；3) 设计了支持FGMP的硬件增强功能。

Result: 在Llama-2-7B模型上，FGMP量化实现了<1%的困惑度下降，同时减少了14%的推理能耗和30%的权重内存需求。

Conclusion: FGMP量化是一种高效的硬件-软件协同设计方法，能够在降低精度的同时保持模型性能。

Abstract: Quantization is a powerful tool to improve large language model (LLM)
inference efficiency by utilizing more energy-efficient low-precision datapaths
and reducing memory footprint. However, accurately quantizing LLM weights and
activations to low precision is challenging without degrading model accuracy.
We propose fine-grained mixed precision (FGMP) quantization, a post-training
mixed-precision quantization hardware-software co-design methodology that
maintains accuracy while quantizing the majority of weights and activations to
reduced precision. Our work makes the following contributions: 1) We develop a
policy that uses the perturbation in each value, weighted by the Fisher
information, to select which weight and activation blocks to keep in higher
precision. This approach preserves accuracy by identifying which weight and
activation blocks need to be retained in higher precision to minimize the
perturbation in the model loss. 2) We also propose a sensitivity-weighted
clipping approach for fine-grained quantization which helps retain accuracy for
blocks that are quantized to low precision. 3) We then propose hardware
augmentations to leverage the efficiency benefits of FGMP quantization. Our
hardware implementation encompasses i) datapath support for FGMP at block
granularity, and ii) a mixed-precision activation quantization unit to assign
activation blocks to high or low precision on the fly with minimal runtime and
energy overhead. Our design, prototyped using NVFP4 (an FP4 format with
microscaling) as the low-precision datatype and FP8 as the high-precision
datatype, facilitates efficient FGMP quantization, attaining <1% perplexity
degradation on Wikitext-103 for the Llama-2-7B model relative to an all-FP8
baseline design while consuming 14% less energy during inference and requiring
30% less weight memory.

</details>


### [455] [ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid Reasoning Model](https://arxiv.org/abs/2504.14560)
*Haiyan Qin,Zhiwei Xie,Jingjing Li,Liangchen Li,Xiaotong Feng,Junzhan Liu,Wang Kang*

Main category: cs.AR

TL;DR: ReasoningV是一种新型模型，通过混合推理策略提升Verilog代码生成能力，结合高质量数据集和动态推理适应，显著减少计算资源消耗并保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在Verilog代码生成中面临的数据质量、推理能力和计算效率问题。

Method: 提出ReasoningV-5K数据集、两阶段训练方法和自适应推理机制。

Result: 在VerilogEval-human上达到57.8%的pass@1准确率，优于开源模型，接近商业模型性能。

Conclusion: ReasoningV为AI驱动的硬件设计自动化提供了更可靠和高效的解决方案。

Abstract: Large Language Models (LLMs) have advanced Verilog code generation
significantly, yet face challenges in data quality, reasoning capabilities, and
computational efficiency. This paper presents ReasoningV, a novel model
employing a hybrid reasoning strategy that integrates trained intrinsic
capabilities with dynamic inference adaptation for Verilog code generation. Our
framework introduces three complementary innovations: (1) ReasoningV-5K, a
high-quality dataset of 5,000 functionally verified instances with reasoning
paths created through multi-dimensional filtering of PyraNet samples; (2) a
two-stage training approach combining parameter-efficient fine-tuning for
foundational knowledge with full-parameter optimization for enhanced reasoning;
and (3) an adaptive reasoning mechanism that dynamically adjusts reasoning
depth based on problem complexity, reducing token consumption by up to 75\%
while preserving performance. Experimental results demonstrate ReasoningV's
effectiveness with a pass@1 accuracy of 57.8\% on VerilogEval-human, achieving
performance competitive with leading commercial models like Gemini-2.0-flash
(59.5\%) and exceeding the previous best open-source model by 10.4 percentage
points. ReasoningV offers a more reliable and accessible pathway for advancing
AI-driven hardware design automation, with our model, data, and code available
at https://github.com/BUAA-CLab/ReasoningV.

</details>


### [456] [Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets Collective Intelligence](https://arxiv.org/abs/2504.14625)
*Haiyan Qin,Jiahao Feng,Xiaotong Feng,Wei W. Xing,Wang Kang*

Main category: cs.AR

TL;DR: CircuitMind是一个多智能体框架，通过语法锁定、检索增强生成和双奖励优化，实现了与人类专家竞争的高效硬件设计。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在硬件设计中效率低下的问题，提升电路设计的正确性和效率。

Method: 采用语法锁定、检索增强生成和双奖励优化的多智能体框架。

Result: 55.6%的模型实现达到或超过人类专家的效率，14B Phi-4模型表现优于GPT-4o mini和Gemini 2.0 Flash。

Conclusion: CircuitMind为硬件优化提供了新范式，通过协作AI系统利用集体人类专业知识实现最优电路设计。

Abstract: Large language models (LLMs) have transformed code generation, yet their
application in hardware design produces gate counts 38\%--1075\% higher than
human designs. We present CircuitMind, a multi-agent framework that achieves
human-competitive efficiency through three key innovations: syntax locking
(constraining generation to basic logic gates), retrieval-augmented generation
(enabling knowledge-driven design), and dual-reward optimization (balancing
correctness with efficiency). To evaluate our approach, we introduce TC-Bench,
the first gate-level benchmark harnessing collective intelligence from the
TuringComplete ecosystem -- a competitive circuit design platform with hundreds
of thousands of players. Experiments show CircuitMind enables 55.6\% of model
implementations to match or exceed top-tier human experts in composite
efficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model
to outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency
comparable to the top 25\% of human experts without requiring specialized
training. These innovations establish a new paradigm for hardware optimization
where collaborative AI systems leverage collective human expertise to achieve
optimal circuit designs. Our model, data, and code are open-source at
https://github.com/BUAA-CLab/CircuitMind.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [457] [Resource Utilization Optimized Federated Learning](https://arxiv.org/abs/2504.13850)
*Zihan Zhang,Leon Wong,Blesson Varghese*

Main category: cs.DC

TL;DR: FedOptima是一种优化的联邦学习系统，通过异步聚合和任务调度减少设备和服务器的空闲时间，显著提升训练效率和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习系统因任务依赖和设备异构性导致资源利用率低，FedOptima旨在同时解决这两种空闲时间问题。

Method: FedOptima通过异步聚合消除设备间的拖尾效应，利用辅助网络减少任务依赖空闲时间，服务器采用任务调度和高效内存管理。

Result: 实验表明，FedOptima在准确率、训练速度、空闲时间减少和吞吐量方面均优于基线方法。

Conclusion: FedOptima显著提升了联邦学习的效率和实用性，适用于大规模异构设备场景。

Abstract: Federated learning (FL) systems facilitate distributed machine learning
across a server and multiple devices. However, FL systems have low resource
utilization limiting their practical use in the real world. This inefficiency
primarily arises from two types of idle time: (i) task dependency between the
server and devices, and (ii) stragglers among heterogeneous devices. This paper
introduces FedOptima, a resource-optimized FL system designed to simultaneously
minimize both types of idle time; existing systems do not eliminate or reduce
both at the same time. FedOptima offloads the training of certain layers of a
neural network from a device to server using three innovations. First, devices
operate independently of each other using asynchronous aggregation to eliminate
straggler effects, and independently of the server by utilizing auxiliary
networks to minimize idle time caused by task dependency. Second, the server
performs centralized training using a task scheduler that ensures balanced
contributions from all devices, improving model accuracy. Third, an efficient
memory management mechanism on the server increases scalability of the number
of participating devices. Four state-of-the-art offloading-based and
asynchronous FL methods are chosen as baselines. Experimental results show that
compared to the best results of the baselines on convolutional neural networks
and transformers on multiple lab-based testbeds, FedOptima (i) achieves higher
or comparable accuracy, (ii) accelerates training by 1.9x to 21.8x, (iii)
reduces server and device idle time by up to 93.9% and 81.8%, respectively, and
(iv) increases throughput by 1.1x to 2.0x.

</details>


### [458] [PipeWeaver: Addressing Data Dynamicity in Large Multimodal Model Training with Dynamic Interleaved Pipeline](https://arxiv.org/abs/2504.14145)
*Zhenliang Xue,Hanpeng Hu,Xing Chen,Yimin Jiang,Yixin Song,Zeyu Mi,Yibo Zhu,Daxin Jiang,Yubin Xia,Haibo Chen*

Main category: cs.DC

TL;DR: PipeWeaver是一个动态管道调度框架，用于解决大型多模态模型（LMM）训练中的效率问题，通过动态交错管道和自适应模态感知分区等技术，提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMM）在训练中存在管道阶段不平衡和数据动态性问题，导致训练效率低下。

Method: PipeWeaver采用动态交错管道调度，结合自适应模态感知分区和高效管道调度搜索，利用SEMU模拟器进行性能估计。

Result: 实验表明，PipeWeaver能将LMM训练效率提升高达97.3%，并适应数据动态性。

Conclusion: PipeWeaver有效解决了LMM训练的效率问题，具有显著的实际应用价值。

Abstract: Large multimodal models (LMMs) have demonstrated excellent capabilities in
both understanding and generation tasks with various modalities. While these
models can accept flexible combinations of input data, their training
efficiency suffers from two major issues: pipeline stage imbalance caused by
heterogeneous model architectures, and training data dynamicity stemming from
the diversity of multimodal data.
  In this paper, we present PipeWeaver, a dynamic pipeline scheduling framework
designed for LMM training. The core of PipeWeaver is dynamic interleaved
pipeline, which searches for pipeline schedules dynamically tailored to current
training batches. PipeWeaver addresses issues of LMM training with two
techniques: adaptive modality-aware partitioning and efficient pipeline
schedule search within a hierarchical schedule space. Meanwhile, PipeWeaver
utilizes SEMU (Step Emulator), a training simulator for multimodal models, for
accurate performance estimations, accelerated by spatial-temporal subgraph
reuse to improve search efficiency. Experiments show that PipeWeaver can
enhance LMM training efficiency by up to 97.3% compared to state-of-the-art
systems, and demonstrate excellent adaptivity to LMM training's data
dynamicity.

</details>


### [459] [GENE-FL: Gene-Driven Parameter-Efficient Dynamic Federated Learning](https://arxiv.org/abs/2504.14628)
*Shunxin Guo,Jiaqi Lv,Qiufeng Wang,Xin Geng*

Main category: cs.DC

TL;DR: GENE-FL框架通过Learngene范式解决动态客户端数据异构性问题，显著降低通信成本并快速初始化模型。


<details>
  <summary>Details</summary>
Motivation: 现实联邦学习系统中动态客户端数据分布异构且不可知，导致通信效率低和模型初始化困难。

Method: 基于Fisher值参数约束、参数敏感性分析及Learngene聚合，提出GENE-FL框架。

Result: 通信成本降低4倍，模型初始化仅需9.04 MB。

Conclusion: GENE-FL高效解决了动态异构数据下的联邦学习挑战。

Abstract: Real-world \underline{F}ederated \underline{L}earning systems often encounter
\underline{D}ynamic clients with \underline{A}gnostic and highly heterogeneous
data distributions (DAFL), which pose challenges for efficient communication
and model initialization. To address these challenges, we draw inspiration from
the recently proposed Learngene paradigm, which compresses the large-scale
model into lightweight, cross-task meta-information fragments. Learngene
effectively encapsulates and communicates core knowledge, making it
particularly well-suited for DAFL, where dynamic client participation requires
communication efficiency and rapid adaptation to new data distributions. Based
on this insight, we propose a Gene-driven parameter-efficient dynamic Federated
Learning (GENE-FL) framework. First, local models perform quadratic constraints
based on parameters with high Fisher values in the global model, as these
parameters are considered to encapsulate generalizable knowledge. Second, we
apply the strategy of parameter sensitivity analysis in local model parameters
to condense the \textit{learnGene} for interaction. Finally, the server
aggregates these small-scale trained \textit{learnGene}s into a robust
\textit{learnGene} with cross-task generalization capability, facilitating the
rapid initialization of dynamic agnostic client models. Extensive experimental
results demonstrate that GENE-FL reduces \textbf{4 $\times$} communication
costs compared to FEDAVG and effectively initializes agnostic client models
with only about \textbf{9.04} MB.

</details>


### [460] [Is Intelligence the Right Direction in New OS Scheduling for Multiple Resources in Cloud Environments?](https://arxiv.org/abs/2504.15021)
*Xinglei Dou,Lei Liu,Limin Xiao*

Main category: cs.DC

TL;DR: OSML+是一种基于机器学习的资源调度机制，用于协同定位云服务，智能调度缓存、主内存带宽和计算核心资源，并通过多模型协作学习处理复杂场景。


<details>
  <summary>Details</summary>
Motivation: 提升系统/操作系统设计的智能化水平，以更高效地管理云服务中的资源调度。

Method: 采用多模型协作学习方法，结合迁移学习技术，动态适应变化的工作负载。

Result: 实验表明，OSML+比以往研究支持更高负载，满足QoS目标且开销更低。

Conclusion: OSML+通过智能调度和动态学习，显著提升了云服务的资源管理效率和适应性。

Abstract: Making it intelligent is a promising way in System/OS design. This paper
proposes OSML+, a new ML-based resource scheduling mechanism for co-located
cloud services. OSML+ intelligently schedules the cache and main memory
bandwidth resources at the memory hierarchy and the computing core resources
simultaneously. OSML+ uses a multi-model collaborative learning approach during
its scheduling and thus can handle complicated cases, e.g., avoiding resource
cliffs, sharing resources among applications, enabling different scheduling
policies for applications with different priorities, etc. OSML+ can converge
faster using ML models than previous studies. Moreover, OSML+ can automatically
learn on the fly and handle dynamically changing workloads accordingly. Using
transfer learning technologies, we show our design can work well across various
cloud servers, including the latest off-the-shelf large-scale servers. Our
experimental results show that OSML+ supports higher loads and meets QoS
targets with lower overheads than previous studies.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [461] [Generalized Derangetropy Functionals for Modeling Cyclical Information Flow](https://arxiv.org/abs/2504.14605)
*Masoud Ataei,Xiaogang Wang*

Main category: cs.IT

TL;DR: 本文提出了一种基于熵调制变换的框架，用于建模循环和反馈驱动的信息流，通过非线性微分方程描述信息结构的动态演化。


<details>
  <summary>Details</summary>
Motivation: 传统熵度量（如香农熵）是静态的，无法捕捉周期性或自反馈的信息动态，因此需要一种更灵活的框架来描述复杂系统中的信息流动。

Method: 使用称为derangetropy泛函的熵调制变换，通过非线性微分方程操作概率密度，模拟信息结构的周期性变化和反馈效应。

Result: 递归应用这些泛函会导致信息结构收敛于高斯特征函数，为循环调制下的信息长期动态提供了统一分析基础。

Conclusion: 该框架为具有周期性结构、随机反馈和延迟交互的系统提供了新的信息动态分析工具，适用于神经网络、通信理论和非平衡统计力学等领域。

Abstract: This paper introduces a framework for modeling cyclical and feedback-driven
information flow through a generalized family of entropy-modulated
transformations called derangetropy functionals. Unlike scalar and static
entropy measures such as Shannon entropy, these functionals act directly on
probability densities and provide a topographical representation of information
structure across the support of the distribution. The framework captures
periodic and self-referential aspects of information distribution and encodes
them through functional operators governed by nonlinear differential equations.
When applied recursively, these operators induce a spectral diffusion process
governed by the heat equation, leading to convergence toward a Gaussian
characteristic function. This convergence theorem provides a unified analytical
foundation for describing the long-term dynamics of information under cyclic
modulation. The proposed framework offers new tools for analyzing the temporal
evolution of information in systems characterized by periodic structure,
stochastic feedback, and delayed interaction, with applications in artificial
neural networks, communication theory, and non-equilibrium statistical
mechanics.

</details>


### [462] [Reveal-or-Obscure: A Differentially Private Sampling Algorithm for Discrete Distributions](https://arxiv.org/abs/2504.14696)
*Naima Tasnim,Atefeh Gilani,Lalitha Sankar,Oliver Kosut*

Main category: cs.IT

TL;DR: 论文提出了一种差分隐私算法ROO和其改进版DS-ROO，用于从离散分布中生成代表性样本，通过随机选择“揭示”或“掩盖”数据分布实现隐私保护，并在隐私-效用权衡上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私方法通过添加显式噪声保护数据，但可能影响数据效用。ROO和DS-ROO旨在通过更灵活的方式实现隐私保护，同时提升数据效用。

Method: ROO随机选择揭示或掩盖数据分布；DS-ROO进一步自适应调整掩盖概率，优化隐私-效用权衡。

Result: ROO在采样复杂度上优于现有方法；DS-ROO在相同隐私预算下实现更高效用。

Conclusion: ROO和DS-ROO为差分隐私提供了更高效的方法，尤其在离散分布场景中表现优越。

Abstract: We introduce a differentially private (DP) algorithm called reveal-or-obscure
(ROO) to generate a single representative sample from a dataset of $n$
observations drawn i.i.d. from an unknown discrete distribution $P$. Unlike
methods that add explicit noise to the estimated empirical distribution, ROO
achieves $\epsilon$-differential privacy by randomly choosing whether to
"reveal" or "obscure" the empirical distribution. While ROO is structurally
identical to Algorithm 1 proposed by Cheu and Nayak (arXiv:2412.10512), we
prove a strictly better bound on the sampling complexity than that established
in Theorem 12 of (arXiv:2412.10512). To further improve the privacy-utility
trade-off, we propose a novel generalized sampling algorithm called
Data-Specific ROO (DS-ROO), where the probability of obscuring the empirical
distribution of the dataset is chosen adaptively. We prove that DS-ROO
satisfies $\epsilon$-DP, and provide empirical evidence that DS-ROO can achieve
better utility under the same privacy budget of vanilla ROO.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [463] [6G WavesFM: A Foundation Model for Sensing, Communication, and Localization](https://arxiv.org/abs/2504.14100)
*Ahmed Aboulfotouh,Elsayed Mohammed,Hatem Abou-Zeid*

Main category: eess.SP

TL;DR: WavesFM是一种新型无线基础模型框架，支持多种通信、感知和定位任务，通过共享参数和高效微调实现高性能和低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 推动AI原生范式在未来6G网络中的应用，通过统一模型支持多样化任务，提升性能和效率。

Method: 结合共享的Vision Transformer主干和任务特定的MLP头，采用LoRA进行参数高效微调，处理图像和IQ信号。

Result: 在5G NR定位、MIMO-OFDM信道估计等任务中表现优于单独训练的基线模型，参数共享率达80%，训练时间减少5倍。

Conclusion: WavesFM展示了基础模型在6G网络中的潜力，能够高效支持多样化任务，显著提升性能和资源利用率。

Abstract: This paper introduces WavesFM, a novel Wireless Foundation Model (WFM)
framework, capable of supporting a wide array of communication, sensing, and
localization tasks. Our proposed architecture combines a shared Vision
Transformer (ViT) backbone with task-specific multi-layer perceptron (MLP)
heads and incorporates Low-Rank Adaptation (LoRA) for parameter-efficient
fine-tuning. This design promotes full parameter sharing across tasks,
significantly reducing the computational and memory footprint without
sacrificing performance. The model processes both image-like wireless
modalities, such as spectrograms and channel state information (CSI), and
in-phase and quadrature (IQ) signals arranged as orthogonal frequency-division
multiplexing (OFDM) resource grids. We demonstrate the strong generalization
capabilities of WavesFM through extensive experiments on four downstream tasks:
Fifth Generation New Radio (5G NR) positioning; multiple-input multiple-output
OFDM (MIMO-OFDM) channel estimation; human activity sensing; and
radio-frequency (RF) signal classification. Compared to supervised baselines
trained individually, our approach achieves superior performance while sharing
80% of its parameters across tasks. Furthermore, we show that pretraining on
domain-relevant data not only boosts performance but also accelerates
convergence, reducing training time by up to 5x. These results demonstrate that
our unified WFM can support diverse tasks and deliver significant gains in both
performance and efficiency, highlighting the transformative potential of
foundation models to drive AI-native paradigms in future sixth-generation (6G)
networks.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [464] [Optimal Lattice Boltzmann Closures through Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2504.14422)
*Paul Fischer,Sebastian Kaltenbach,Sergey Litvinov,Sauro Succi,Petros Koumoutsakos*

Main category: physics.flu-dyn

TL;DR: 本文提出了一种基于多智能体强化学习（MARL）的数据驱动方法，显著提高了粗粒度Lattice Boltzmann方法（LBM）模拟的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: LBM在模拟多尺度流体现象时存在稳定性问题，传统方法难以泛化。

Method: 使用卷积神经网络动态控制LBM的局部松弛参数，并在湍流Kolmogorov流中验证。

Result: MARL方法稳定了模拟，恢复了高分辨率模拟的能量谱，且计算高效。

Conclusion: MARL为复杂问题的模拟提供了新途径，超越了传统LBM的能力。

Abstract: The Lattice Boltzmann method (LBM) offers a powerful and versatile approach
to simulating diverse hydrodynamic phenomena, spanning microfluidics to
aerodynamics. The vast range of spatiotemporal scales inherent in these systems
currently renders full resolution impractical, necessitating the development of
effective closure models for under-resolved simulations. Under-resolved LBMs
are unstable, and while there is a number of important efforts to stabilize
them, they often face limitations in generalizing across scales and physical
systems. We present a novel, data-driven, multiagent reinforcement learning
(MARL) approach that drastically improves stability and accuracy of
coarse-grained LBM simulations. The proposed method uses a convolutional neural
network to dynamically control the local relaxation parameter for the LB across
the simulation grid. The LB-MARL framework is showcased in turbulent Kolmogorov
flows. We find that the MARL closures stabilize the simulations and recover the
energy spectra of significantly more expensive fully resolved simulations while
maintaining computational efficiency. The learned closure model can be
transferred to flow scenarios unseen during training and has improved
robustness and spectral accuracy compared to traditional LBM models. We believe
that MARL closures open new frontiers for efficient and accurate simulations of
a multitude of complex problems not accessible to present-day LB methods alone.

</details>


### [465] [LBM-GNN: Graph Neural Network Enhanced Lattice Boltzmann Method](https://arxiv.org/abs/2504.14494)
*Yue Li*

Main category: physics.flu-dyn

TL;DR: LBM-GNN结合了Lattice Boltzmann Method和Graph Neural Networks，提升了流体动力学模拟的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统LBM在流体模拟中存在稳定性和精度问题，GNN的引入旨在解决这些问题。

Method: 将GNN与LBM结合，应用于流体动力学模拟，并通过基准问题（如Taylor-Green vortex）验证。

Result: GNN增强的LBM在更高Reynolds数下保持更好的守恒性和数值稳定性。

Conclusion: LBM-GNN是一种有前景的方法，能够提升流体模拟的性能。

Abstract: In this paper, we present LBM-GNN, a novel approach that enhances the
traditional Lattice Boltzmann Method (LBM) with Graph Neural Networks (GNNs).
We apply this method to fluid dynamics simulations, demonstrating improved
stability and accuracy compared to standard LBM implementations. The method is
validated using benchmark problems such as the Taylor-Green vortex, focusing on
accuracy, conservation properties, and performance across different Reynolds
numbers and grid resolutions. Our results indicate that GNN-enhanced LBM can
maintain better conservation properties while improving numerical stability at
higher Reynolds numbers.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [466] [Quantum-Enhanced Reinforcement Learning for Power Grid Security Assessment](https://arxiv.org/abs/2504.14412)
*Benjamin M. Peter,Mert Korkali*

Main category: eess.SY

TL;DR: 论文提出了一种结合量子计算和强化学习的混合代理，用于提升电网安全评估的计算效率和代理性能。


<details>
  <summary>Details</summary>
Motivation: 电网安全维护任务日益复杂，传统方法难以应对大规模决策空间和非线性行为。量子计算的优势为解决这一问题提供了新思路。

Method: 提出了一种基于IBM Qiskit Runtime的混合代理，利用参数化量子电路（PQCs）生成量子输出，并与强化学习框架结合。

Result: 实验表明，该代理在N-k应急分析中比无量子增强的基准模型更有效地维持电网稳定性。

Conclusion: 量子计算与强化学习的结合为电网安全评估提供了可扩展的解决方案，展示了量子优势在复杂问题中的潜力。

Abstract: The increasingly challenging task of maintaining power grid security requires
innovative solutions. Novel approaches using reinforcement learning (RL) agents
have been proposed to help grid operators navigate the massive decision space
and nonlinear behavior of these complex networks. However, applying RL to power
grid security assessment, specifically for combinatorially troublesome
contingency analysis problems, has proven difficult to scale. The integration
of quantum computing into these RL frameworks helps scale by improving
computational efficiency and boosting agent proficiency by leveraging quantum
advantages in action exploration and model-based interdependence. To
demonstrate a proof-of-concept use of quantum computing for RL agent training
and simulation, we propose a hybrid agent that runs on quantum hardware using
IBM's Qiskit Runtime. We also provide detailed insight into the construction of
parameterized quantum circuits (PQCs) for generating relevant quantum output.
This agent's proficiency at maintaining grid stability is demonstrated relative
to a benchmark model without quantum enhancement using N-k contingency
analysis. Additionally, we offer a comparative assessment of the training
procedures for RL models integrated with a quantum backend.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [467] [Revealing the 3D Cosmic Web through Gravitationally Constrained Neural Fields](https://arxiv.org/abs/2504.15262)
*Brandon Zhao,Aviad Levis,Liam Connor,Pratul P. Srinivasan,Katherine L. Bouman*

Main category: astro-ph.CO

TL;DR: 论文提出了一种利用引力约束神经场重建宇宙暗物质3D分布的方法，克服了传统方法中的噪声和单视角限制问题。


<details>
  <summary>Details</summary>
Motivation: 准确的3D暗物质分布图对定位宇宙结构和验证理论至关重要，但传统方法因噪声和单视角问题难以实现。

Method: 采用引力约束神经场建模连续物质分布，通过可微分物理前向模型优化神经网络权重以匹配透镜信号。

Result: 在模拟实验中，该方法不仅优于传统方法，还能发现潜在的意外暗物质结构。

Conclusion: 该方法为暗物质3D重建提供了灵活且高效的解决方案，有望应用于未来望远镜数据。

Abstract: Weak gravitational lensing is the slight distortion of galaxy shapes caused
primarily by the gravitational effects of dark matter in the universe. In our
work, we seek to invert the weak lensing signal from 2D telescope images to
reconstruct a 3D map of the universe's dark matter field. While inversion
typically yields a 2D projection of the dark matter field, accurate 3D maps of
the dark matter distribution are essential for localizing structures of
interest and testing theories of our universe. However, 3D inversion poses
significant challenges. First, unlike standard 3D reconstruction that relies on
multiple viewpoints, in this case, images are only observed from a single
viewpoint. This challenge can be partially addressed by observing how galaxy
emitters throughout the volume are lensed. However, this leads to the second
challenge: the shapes and exact locations of unlensed galaxies are unknown, and
can only be estimated with a very large degree of uncertainty. This introduces
an overwhelming amount of noise which nearly drowns out the lensing signal
completely. Previous approaches tackle this by imposing strong assumptions
about the structures in the volume. We instead propose a methodology using a
gravitationally-constrained neural field to flexibly model the continuous
matter distribution. We take an analysis-by-synthesis approach, optimizing the
weights of the neural network through a fully differentiable physical forward
model to reproduce the lensing signal present in image measurements. We
showcase our method on simulations, including realistic simulated measurements
of dark matter distributions that mimic data from upcoming telescope surveys.
Our results show that our method can not only outperform previous methods, but
importantly is also able to recover potentially surprising dark matter
structures.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [468] [Transformation of audio embeddings into interpretable, concept-based representations](https://arxiv.org/abs/2504.14076)
*Alice Zhang,Edison Thomaz,Lie Lu*

Main category: cs.SD

TL;DR: 论文提出了一种后处理方法，将CLAP音频嵌入转换为基于概念的稀疏表示，提升了语义可解释性，并在下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 音频神经网络的内部表示难以解释，因此需要一种方法使其更具语义可解释性。

Method: 利用CLAP模型将音频和文本嵌入共享空间，并通过后处理方法转换为基于概念的稀疏表示。

Result: 基于概念的表示在下游任务中表现优于或等同于原始音频嵌入，且通过微调可进一步提升性能。

Conclusion: 该方法不仅提高了音频嵌入的可解释性，还保持了其性能，并发布了三个音频专用词汇表。

Abstract: Advancements in audio neural networks have established state-of-the-art
results on downstream audio tasks. However, the black-box structure of these
models makes it difficult to interpret the information encoded in their
internal audio representations. In this work, we explore the semantic
interpretability of audio embeddings extracted from these neural networks by
leveraging CLAP, a contrastive learning model that brings audio and text into a
shared embedding space. We implement a post-hoc method to transform CLAP
embeddings into concept-based, sparse representations with semantic
interpretability. Qualitative and quantitative evaluations show that the
concept-based representations outperform or match the performance of original
audio embeddings on downstream tasks while providing interpretability.
Additionally, we demonstrate that fine-tuning the concept-based representations
can further improve their performance on downstream tasks. Lastly, we publish
three audio-specific vocabularies for concept-based interpretability of audio
embeddings.

</details>


### [469] [DRAGON: Distributional Rewards Optimize Diffusion Generative Models](https://arxiv.org/abs/2504.15217)
*Yatong Bai,Jonah Casebeer,Somayeh Sojoudi,Nicholas J. Bryan*

Main category: cs.SD

TL;DR: DRAGON是一个灵活的框架，用于优化媒体生成模型，支持多种奖励函数，包括实例级和分布级奖励，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法如RLHF或DPO在灵活性上有限，无法适应多样化的奖励函数需求。

Method: DRAGON通过选择编码器和参考示例构建奖励函数，利用对比集最大化奖励。

Result: 在20种奖励函数中平均胜率为81.45%，且无需人类偏好标注即可提升生成质量。

Conclusion: DRAGON为设计和优化奖励函数提供了新方法，显著提升人类感知质量。

Abstract: We present Distributional RewArds for Generative OptimizatioN (DRAGON), a
versatile framework for fine-tuning media generation models towards a desired
outcome. Compared with traditional reinforcement learning with human feedback
(RLHF) or pairwise preference approaches such as direct preference optimization
(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate
either individual examples or distributions of them, making it compatible with
a broad spectrum of instance-wise, instance-to-distribution, and
distribution-to-distribution rewards. Leveraging this versatility, we construct
novel reward functions by selecting an encoder and a set of reference examples
to create an exemplar distribution. When cross-modality encoders such as CLAP
are used, the reference examples may be of a different modality (e.g., text
versus audio). Then, DRAGON gathers online and on-policy generations, scores
them to construct a positive demonstration set and a negative set, and
leverages the contrast between the two sets to maximize the reward. For
evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20
different reward functions, including a custom music aesthetics model, CLAP
score, Vendi diversity, and Frechet audio distance (FAD). We further compare
instance-wise (per-song) and full-dataset FAD settings while ablating multiple
FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an
81.45% average win rate. Moreover, reward functions based on exemplar sets
indeed enhance generations and are comparable to model-based rewards. With an
appropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality
win rate without training on human preference annotations. As such, DRAGON
exhibits a new approach to designing and optimizing reward functions for
improving human-perceived quality. Sound examples at
https://ml-dragon.github.io/web.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [470] [Density Measures for Language Generation](https://arxiv.org/abs/2504.14370)
*Jon Kleinberg,Fan Wei*

Main category: math.CO

TL;DR: 论文提出了一种抽象的语言生成框架，探讨了算法在生成新字符串时面临的‘有效性’与‘广度’之间的权衡，并通过密度度量量化这一权衡。作者提出了一种新算法，其输出在目标语言中具有严格正密度，并研究了算法的内部表示。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）的成功引发了对语言生成的理论探索。现有研究难以定量分析‘有效性’（避免无效输出）与‘广度’（生成多样化输出）之间的权衡。

Method: 提出‘极限语言生成’的抽象框架，将生成过程视为对抗性游戏，并引入密度度量量化‘广度’。开发了一种新算法，确保输出在目标语言中具有严格正密度。

Result: 新算法的输出在目标语言中具有严格正密度，解决了现有算法输出密度为零的问题。同时发现，实现最强‘广度’可能需要在高密度和低密度表示之间无限振荡。

Conclusion: 通过引入新的拓扑结构和密度度量，论文为语言生成中的‘有效性-广度’权衡提供了定量分析工具，并展示了新算法的优越性。

Abstract: The recent successes of large language models (LLMs) have led to a surge of
theoretical research into language generation. A recent line of work proposes
an abstract view, called language generation in the limit, where generation is
seen as a game between an adversary and an algorithm: the adversary generates
strings from an unknown language $K$, chosen from a countable collection of
candidate languages, and after seeing a finite set of these strings, the
algorithm must generate new strings from $K$ that it has not seen before. This
formalism highlights a key tension: the trade-off between validity (the
algorithm should only produce strings from the language) and breadth (it should
be able to produce many strings from the language). This trade-off is central
in applied language generation as well, where it appears as a balance between
hallucination (generating invalid utterances) and mode collapse (generating
only a restricted set of outputs). Despite its importance, this trade-off has
been challenging to study quantitatively. We develop ways to quantify this
trade-off by formalizing breadth using measures of density. Existing algorithms
for language generation in the limit produce output sets that can have zero
density in the true language, and this important failure of breadth might seem
unavoidable. We show, however, that such a failure is not necessary: we provide
an algorithm for language generation in the limit whose outputs have strictly
positive density in $K$. We also study the internal representations built by
these algorithms, specifically the sequence of hypothesized candidate languages
they consider, and show that achieving the strongest form of breadth may
require oscillating indefinitely between high- and low-density representations.
Our analysis introduces a novel topology on language families, with notions of
convergence and limit points playing a key role.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [471] [Learning over von Mises-Fisher Distributions via a Wasserstein-like Geometry](https://arxiv.org/abs/2504.14164)
*Kisung You,Dennis Shung,Mauro Giuffrè*

Main category: stat.ML

TL;DR: 提出了一种新的几何感知距离度量方法，用于比较von Mises-Fisher (vMF)分布，解决了现有方法在归一化常数和几何度量上的局限性。


<details>
  <summary>Details</summary>
Motivation: vMF分布广泛应用于球形数据的概率学习任务，但缺乏有效的比较工具，主要由于归一化常数的复杂性和缺乏合适的几何度量。

Method: 基于最优传输理论，提出了一种类似Wasserstein的距离，将vMF分布的差异分解为两个可解释的组成部分：均值方向的角度分离和浓度参数的差异。

Result: 提出的距离具有理想的理论性质，并在非退化vMF分布空间上诱导出潜在的几何结构。实验验证了其在区分分布和支持可解释推理方面的有效性。

Conclusion: 该工作通过引入一种针对超球面几何的可处理、传输启发的距离，扩展了方向数据分析的统计工具箱。

Abstract: We introduce a novel, geometry-aware distance metric for the family of von
Mises-Fisher (vMF) distributions, which are fundamental models for directional
data on the unit hypersphere. Although the vMF distribution is widely employed
in a variety of probabilistic learning tasks involving spherical data,
principled tools for comparing vMF distributions remain limited, primarily due
to the intractability of normalization constants and the absence of suitable
geometric metrics. Motivated by the theory of optimal transport, we propose a
Wasserstein-like distance that decomposes the discrepancy between two vMF
distributions into two interpretable components: a geodesic term capturing the
angular separation between mean directions, and a variance-like term
quantifying differences in concentration parameters. The derivation leverages a
Gaussian approximation in the high-concentration regime to yield a tractable,
closed-form expression that respects the intrinsic spherical geometry. We show
that the proposed distance exhibits desirable theoretical properties and
induces a latent geometric structure on the space of non-degenerate vMF
distributions. As a primary application, we develop the efficient algorithms
for vMF mixture reduction, enabling structure-preserving compression of mixture
models in high-dimensional settings. Empirical results on synthetic datasets
and real-world high-dimensional embeddings, including biomedical sentence
representations and deep visual features, demonstrate the effectiveness of the
proposed geometry in distinguishing distributions and supporting interpretable
inference. This work expands the statistical toolbox for directional data
analysis by introducing a tractable, transport-inspired distance tailored to
the geometry of the hypersphere.

</details>


### [472] [Optimal Scheduling of Dynamic Transport](https://arxiv.org/abs/2504.14425)
*Panos Tsimpos,Zhi Ren,Jakob Zech,Youssef Marzouk*

Main category: stat.ML

TL;DR: 论文提出了一种基于流的方法，通过优化时间调度来最小化速度场的Lipschitz常数，从而显著提升近似和学习效果。


<details>
  <summary>Details</summary>
Motivation: 利用时间轴的设计自由度，探索如何通过优化时间调度来改善流方法的性能。

Method: 研究单位时间内插值任意给定传输映射T，并寻找最小化速度场Lipschitz常数的时间调度τ。

Result: 证明对于广泛的源/目标测度和传输映射T，最优调度可以闭式求解，且其Lipschitz常数比恒等调度指数级更小。

Conclusion: 通过变分法和Γ-收敛，证明了优化时间调度的有效性，显著提升了流方法的性能。

Abstract: Flow-based methods for sampling and generative modeling use continuous-time
dynamical systems to represent a {transport map} that pushes forward a source
measure to a target measure. The introduction of a time axis provides
considerable design freedom, and a central question is how to exploit this
freedom. Though many popular methods seek straight line (i.e., zero
acceleration) trajectories, we show here that a specific class of ``curved''
trajectories can significantly improve approximation and learning. In
particular, we consider the unit-time interpolation of any given transport map
$T$ and seek the schedule $\tau: [0,1] \to [0,1]$ that minimizes the spatial
Lipschitz constant of the corresponding velocity field over all times $t \in
[0,1]$. This quantity is crucial as it allows for control of the approximation
error when the velocity field is learned from data. We show that, for a broad
class of source/target measures and transport maps $T$, the \emph{optimal
schedule} can be computed in closed form, and that the resulting optimal
Lipschitz constant is \emph{exponentially smaller} than that induced by an
identity schedule (corresponding to, for instance, the Wasserstein geodesic).
Our proof technique relies on the calculus of variations and
$\Gamma$-convergence, allowing us to approximate the aforementioned degenerate
objective by a family of smooth, tractable problems.

</details>


### [473] [On the Tunability of Random Survival Forests Model for Predictive Maintenance](https://arxiv.org/abs/2504.14744)
*Yigitcan Yardımcı,Mustafa Cavus*

Main category: stat.ML

TL;DR: 本文研究了随机生存森林（RSF）模型在预测性维护中的可调性，提出了一种三层次框架量化调优效果，并通过实验验证了超参数调优对模型性能的显著提升。


<details>
  <summary>Details</summary>
Motivation: RSF模型因其灵活性和处理截尾数据的能力被广泛使用，但其性能对超参数配置敏感，且缺乏系统性评估，尤其是在预测性维护领域。

Method: 引入三层次框架：模型级指标衡量调优的整体性能增益，超参数级指标评估个体贡献，并确定最佳调优范围。使用C指数和Brier评分作为评估标准。

Result: 实验表明，超参数调优显著提升模型性能，C指数平均提高0.0547，Brier评分降低0.0199。ntree和mtry调优效果最佳，而splitrule调优可能降低性能。

Conclusion: 研究强调了超参数调优在生存模型中的实际重要性，并为预测性维护中RSF的优化提供了实用指导。

Abstract: This paper investigates the tunability of the Random Survival Forest (RSF)
model in predictive maintenance, where accurate time-to-failure estimation is
crucial. Although RSF is widely used due to its flexibility and ability to
handle censored data, its performance is sensitive to hyperparameter
configurations. However, systematic evaluations of RSF tunability remain
limited, especially in predictive maintenance contexts. We introduce a
three-level framework to quantify tunability: (1) a model-level metric
measuring overall performance gain from tuning, (2) a hyperparameter-level
metric assessing individual contributions, and (3) identification of optimal
tuning ranges. These metrics are evaluated across multiple datasets using
survival-specific criteria: the C-index for discrimination and the Brier score
for calibration. Experiments on four CMAPSS dataset subsets, simulating
aircraft engine degradation, reveal that hyperparameter tuning consistently
improves model performance. On average, the C-index increased by 0.0547, while
the Brier score decreased by 0.0199. These gains were consistent across all
subsets. Moreover, ntree and mtry showed the highest average tunability, while
nodesize offered stable improvements within the range of 10 to 30. In contrast,
splitrule demonstrated negative tunability on average, indicating that improper
tuning may reduce model performance. Our findings emphasize the practical
importance of hyperparameter tuning in survival models and provide actionable
insights for optimizing RSF in real-world predictive maintenance applications.

</details>


### [474] [Expected Free Energy-based Planning as Variational Inference](https://arxiv.org/abs/2504.14898)
*Bert de Vries,Wouter Nuijten,Thijs van de Laar,Wouter Kouw,Sepideh Adamiat,Tim Nisslbeck,Mykola Lukashchuk,Hoang Minh Huu Nguyen,Marco Hidalgo Araya,Raphael Tresor,Thijs Jenneskens,Ivana Nikoloska,Raaja Subramanian,Bart van Erp,Dmitry Bagaev,Albert Podusenko*

Main category: stat.ML

TL;DR: 论文提出了一种基于变分自由能最小化的统一框架，将规划问题转化为变分推断，解决了传统方法中探索与利用分离的问题，同时考虑了计算资源的限制。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理不确定性规划时，往往将探索与利用视为独立目标，缺乏统一的理论基础。本文旨在通过主动推断和自由能原理，提供一个更一致且可扩展的解决方案。

Method: 通过最小化生成模型中带有偏好和认知先验的变分自由能函数，推导出基于期望自由能（EFE）的规划方法，将规划问题转化为变分推断。

Result: 提出的框架能够生成同时支持目标达成和信息获取的最优策略，并考虑了计算资源的限制，实现了可扩展的资源感知型主动推断。

Conclusion: 该研究为不确定性规划提供了一个理论一致且实用的框架，连接并扩展了现有方法，为主动推断代理的可扩展实现奠定了基础。

Abstract: We address the problem of planning under uncertainty, where an agent must
choose actions that not only achieve desired outcomes but also reduce
uncertainty. Traditional methods often treat exploration and exploitation as
separate objectives, lacking a unified inferential foundation. Active
inference, grounded in the Free Energy Principle, offers such a foundation by
minimizing Expected Free Energy (EFE), a cost function that combines utility
with epistemic drives like ambiguity resolution and novelty seeking. However,
the computational burden of EFE minimization has remained a major obstacle to
its scalability. In this paper, we show that EFE-based planning arises
naturally from minimizing a variational free energy functional on a generative
model augmented with preference and epistemic priors. This result reinforces
theoretical consistency with the Free Energy Principle, by casting planning
itself as variational inference. Our formulation yields optimal policies that
jointly support goal achievement and information gain, while incorporating a
complexity term that accounts for bounded computational resources. This
unifying framework connects and extends existing methods, enabling scalable,
resource-aware implementations of active inference agents.

</details>


### [475] [Advanced posterior analyses of hidden Markov models: finite Markov chain imbedding and hybrid decoding](https://arxiv.org/abs/2504.15156)
*Zenia Elise Damgaard Bæk,Moisès Coll Macià,Laurits Skov,Asger Hobolth*

Main category: stat.ML

TL;DR: 本文提出两种方法：有限马尔可夫链嵌入（FMCI）用于计算隐状态序列的统计量分布，以及混合解码用于改进HMM的解码性能。


<details>
  <summary>Details</summary>
Motivation: 解决隐马尔可夫模型（HMM）中统计量分布计算和解码性能提升的问题。

Method: 1. 使用FMCI计算隐状态序列的统计量后验分布；2. 提出混合解码方法，结合全局和局部解码优势。

Result: FMCI能高效计算统计量分布；混合解码性能优于Viterbi和后验解码，并提供了调参新方法。

Conclusion: FMCI和混合解码为HMM应用提供了高效且性能优越的解决方案，并附带代码支持复现。

Abstract: Two major tasks in applications of hidden Markov models are to (i) compute
distributions of summary statistics of the hidden state sequence, and (ii)
decode the hidden state sequence. We describe finite Markov chain imbedding
(FMCI) and hybrid decoding to solve each of these two tasks. In the first part
of our paper we use FMCI to compute posterior distributions of summary
statistics such as the number of visits to a hidden state, the total time spent
in a hidden state, the dwell time in a hidden state, and the longest run
length. We use simulations from the hidden state sequence, conditional on the
observed sequence, to establish the FMCI framework. In the second part of our
paper we apply hybrid segmentation for improved decoding of a HMM. We
demonstrate that hybrid decoding shows increased performance compared to
Viterbi or Posterior decoding (often also referred to as global or local
decoding), and we introduce a novel procedure for choosing the tuning parameter
in the hybrid procedure. Furthermore, we provide an alternative derivation of
the hybrid loss function based on weighted geometric means. We demonstrate and
apply FMCI and hybrid decoding on various classical data sets, and supply
accompanying code for reproducibility.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [476] [GenShin:geometry-enhanced structural graph embodies binding pose can better predicting compound-protein interaction affinity](https://arxiv.org/abs/2504.13853)
*Pingfei Zhu,Chenyang Zhao,Haishi Zhao,Bo Yang*

Main category: q-bio.BM

TL;DR: GenShin模型通过几何增强结构图模块预测化合物-蛋白质亲和力，无需依赖结合构象输入，性能优于主流模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法预测化合物-蛋白质亲和力需依赖高成本的结合构象输入，限制了实际应用。

Method: 构建几何增强结构图模块，分别提取蛋白质和化合物的特征，避免依赖结合构象输入。

Result: GenShin模型性能优于依赖非输入对接构象的模型，甚至接近或超过需结合构象输入的模型，且对不完整构象更鲁棒。

Conclusion: GenShin模型为实际药物发现提供了更实用的解决方案，并鼓励更多研究填补AI模型与实际挑战间的鸿沟。

Abstract: AI-powered drug discovery typically relies on the successful prediction of
compound-protein interactions, which are pivotal for the evaluation of designed
compound molecules in structure-based drug design and represent a core
challenge in the field.
  However, accurately predicting compound-protein affinity via regression
models usually requires adequate-binding pose, which are derived from costly
and complex experimental methods or time-consuming simulations with docking
software. In response, we have introduced the GenShin model, which constructs a
geometry-enhanced structural graph module that separately extracts additional
features from proteins and compounds. Consequently, it attains an accuracy on
par with mainstream models in predicting compound-protein affinities, while
eliminating the need for adequate-binding pose as input. Our experimental
findings demonstrate that the GenShin model vastly outperforms other models
that rely on non-input docking conformations, achieving, or in some cases even
exceeding, the performance of those requiring adequate-binding pose. Further
experiments indicate that our GenShin model is more robust to
inadequate-binding pose, affirming its higher suitability for real-world drug
discovery scenarios. We hope our work will inspire more endeavors to bridge the
gap between AI models and practical drug discovery challenges.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [477] [VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform](https://arxiv.org/abs/2504.14904)
*Xingyu Lu,Tianke Zhang,Chang Meng,Xiaobei Wang,Jinpeng Wang,YiFan Zhang,Shisong Tang,Changyi Liu,Haojie Ding,Kaiyu Jiang,Kaiyu Tang,Bin Wen,Hai-Tao Zheng,Fan Yang,Tingting Gao,Di Zhang,Kun Gai*

Main category: cs.SI

TL;DR: 论文提出了KuaiMod框架，用于解决短视频平台内容审核的挑战，通过结合视觉语言模型和链式推理，实现了高效、动态的审核策略。


<details>
  <summary>Details</summary>
Motivation: 短视频平台上对用户心理健康有害的内容审核存在手动审核成本高、自动化方法准确性不足、法规更新滞后等问题。

Method: 提出KuaiMod框架，包括训练数据构建、离线适应和在线部署与优化，利用视觉语言模型和链式推理建模视频毒性。

Result: KuaiMod在基准测试中表现最佳，部署后用户举报率降低20%，并提升了平台活跃度和使用时长。

Conclusion: KuaiMod为短视频内容审核提供了高效、动态的解决方案，具有实际应用价值。

Abstract: Exponentially growing short video platforms (SVPs) face significant
challenges in moderating content detrimental to users' mental health,
particularly for minors. The dissemination of such content on SVPs can lead to
catastrophic societal consequences. Although substantial efforts have been
dedicated to moderating such content, existing methods suffer from critical
limitations: (1) Manual review is prone to human bias and incurs high
operational costs. (2) Automated methods, though efficient, lack nuanced
content understanding, resulting in lower accuracy. (3) Industrial moderation
regulations struggle to adapt to rapidly evolving trends due to long update
cycles. In this paper, we annotate the first SVP content moderation benchmark
with authentic user/reviewer feedback to fill the absence of benchmark in this
field. Then we evaluate various methods on the benchmark to verify the
existence of the aforementioned limitations. We further propose our common-law
content moderation framework named KuaiMod to address these challenges. KuaiMod
consists of three components: training data construction, offline adaptation,
and online deployment & refinement. Leveraging large vision language model
(VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video
toxicity based on sparse user feedback and fosters dynamic moderation policy
with rapid update speed and high accuracy. Offline experiments and large-scale
online A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the
best moderation performance on our benchmark. The deployment of KuaiMod reduces
the user reporting rate by 20% and its application in video recommendation
increases both Daily Active User (DAU) and APP Usage Time (AUT) on several
Kuaishou scenarios. We have open-sourced our benchmark at
https://kuaimod.github.io.

</details>


### [478] [Rhythm of Opinion: A Hawkes-Graph Framework for Dynamic Propagation Analysis](https://arxiv.org/abs/2504.15072)
*Yulong Li,Zhixiang Lu,Feilong Tang,Simin Lai,Ming Hu,Yuxuan Zhang,Haochen Xue,Zhaodong Wu,Imran Razzak,Qingxia Li,Jionglong Su*

Main category: cs.SI

TL;DR: 论文提出了一种结合多维霍克斯过程和图神经网络的新方法，用于建模社交网络中意见传播的动态，同时考虑了评论间的复杂层次关系。


<details>
  <summary>Details</summary>
Motivation: 社交媒体的快速发展重塑了舆论动态，传统模型难以有效捕捉其复杂性。

Method: 整合多维霍克斯过程与图神经网络，建模意见传播动态，并引入新数据集VISTA。

Result: 方法能捕捉层次结构、多维交互及跨主题影响，数据集VISTA包含丰富标注的舆论动态数据。

Conclusion: 该方法为未来研究提供了强解释性和稳健基线。

Abstract: The rapid development of social media has significantly reshaped the dynamics
of public opinion, resulting in complex interactions that traditional models
fail to effectively capture. To address this challenge, we propose an
innovative approach that integrates multi-dimensional Hawkes processes with
Graph Neural Network, modeling opinion propagation dynamics among nodes in a
social network while considering the intricate hierarchical relationships
between comments. The extended multi-dimensional Hawkes process captures the
hierarchical structure, multi-dimensional interactions, and mutual influences
across different topics, forming a complex propagation network. Moreover,
recognizing the lack of high-quality datasets capable of comprehensively
capturing the evolution of public opinion dynamics, we introduce a new dataset,
VISTA. It includes 159 trending topics, corresponding to 47,207 posts, 327,015
second-level comments, and 29,578 third-level comments, covering diverse
domains such as politics, entertainment, sports, health, and medicine. The
dataset is annotated with detailed sentiment labels across 11 categories and
clearly defined hierarchical relationships. When combined with our method, it
offers strong interpretability by linking sentiment propagation to the comment
hierarchy and temporal evolution. Our approach provides a robust baseline for
future research.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [479] [Planet as a Brain: Towards Internet of AgentSites based on AIOS Server](https://arxiv.org/abs/2504.14411)
*Xiang Zhang,Yongfeng Zhang*

Main category: cs.NI

TL;DR: 论文介绍了AIOS Server，一个支持AI代理开发和协作的运行时框架，旨在推动从‘网站互联网’向‘代理站点互联网’的转变。


<details>
  <summary>Details</summary>
Motivation: 传统网站模式正在向以AI代理为核心的‘代理站点互联网’转变，需要新的基础设施支持代理的开发、部署和协作。

Method: 提出AIOS Server框架，利用Model Context Protocol (MCP)和JSON-RPC实现代理间或人机交互，支持去中心化协调。

Result: 成功部署了首个‘代理站点互联网’（AIOS-IoA），包括AgentHub和AgentChat，并实现了基于DHT和Gossip协议的代理发现机制。

Conclusion: AIOS Server为构建以自主代理为核心的下一代互联网提供了实践基础。

Abstract: The internet is undergoing a historical transformation from the "Internet of
Websites" to the "Internet of AgentSites." While traditional Websites served as
the foundation for information hosting and dissemination, a new frontier is
emerging where AgentSites serve as the hubs of the internet, where each
AgentSite hosts one or more AI agents that receive tasks, address them, and
deliver actionable solutions, marking a significant shift in the digital
landscape and representing the next generation of online ecosystems. Under this
vision, AIOS, the AI Agent Operating System, serves as the server for the
development, deployment and execution of AI agents, which is a fundamental
infrastructure for the Internet of Agentsites.
  In this paper, we introduce AIOS Server, a runtime framework to host agents
and enable global-scale collaboration among decentralized agents. AIOS Server
provides a communication protocol leveraging the Model Context Protocol (MCP)
and JSON-RPC to enable agent-agent or human-agent interactions. Each AIOS node
operates as a server to host and execute agents, while supporting peer-to-peer
coordination without reliance on centralized orchestration. Based on AIOS
Server, we further present the world's first practically deployed Internet of
Agentsites (AIOS-IoA), including AgentHub for agent registration and discovery
and AgentChat for interactive communication, at https://planet.aios.foundation.
The agent discovery mechanism based on Distributed Hash Tables (DHT) and a
Gossip protocol serves as the search engine for the internet of agentsites.
This work provides a practical foundation for building the Internet of
Agentsites-a new paradigm where autonomous agents become first-class citizens
of the web. The implementation is available at
https://github.com/agiresearch/AIOS.Server and will be integrated into the AIOS
main branch at https://github.com/agiresearch/AIOS.

</details>


### [480] [Uncovering Issues in the Radio Access Network by Looking at the Neighbors](https://arxiv.org/abs/2504.14686)
*José Suárez-Varela,Andra Lutu*

Main category: cs.NI

TL;DR: c-ANEMON是一种基于图神经网络（GNN）的RAN异常检测工具，通过分析单个小区与其局部邻域的关系来检测异常，适用于大规模移动网络。


<details>
  <summary>Details</summary>
Motivation: 移动网络运营商（MNOs）需要管理复杂的多代无线接入网络（RAN），现有的异常检测工具难以处理这种复杂性。

Method: 使用图神经网络（GNN）捕捉时空变化，分析小区与其邻域的关系，独立于外部移动性因素检测异常。

Result: 在真实数据（7,890个小区，3个月）上验证，GNN模型能泛化到未见区域，45.95%的异常需要人工干预。

Conclusion: c-ANEMON能有效检测RAN异常，适用于大规模部署，并显著减少运营团队的工作量。

Abstract: Mobile network operators (MNOs) manage Radio Access Networks (RANs) with
massive amounts of cells over multiple radio generations (2G-5G). To handle
such complexity, operations teams rely on monitoring systems, including anomaly
detection tools that identify unexpected behaviors. In this paper, we present
c-ANEMON, a Contextual ANomaly dEtection MONitor for the RAN based on Graph
Neural Networks (GNNs). Our solution captures spatio-temporal variations by
analyzing the behavior of individual cells in relation to their local
neighborhoods, enabling the detection of anomalies that are independent of
external mobility factors. This, in turn, allows focusing on anomalies
associated with network issues (e.g., misconfigurations, equipment failures).
We evaluate c-ANEMON using real-world data from a large European metropolitan
area (7,890 cells; 3 months). First, we show that the GNN model within our
solution generalizes effectively to cells from previously unseen areas,
suggesting the possibility of using a single model across extensive deployment
regions. Then, we analyze the anomalies detected by c-ANEMON through manual
inspection and define several categories of long-lasting anomalies (6+ hours).
Notably, 45.95% of these anomalies fall into a category that is more likely to
require intervention by operations teams.

</details>


### [481] [Video QoE Metrics from Encrypted Traffic: Application-agnostic Methodology](https://arxiv.org/abs/2504.14720)
*Tamir Berger,Jonathan Sterenson,Raz Birman,Ofer Hadar*

Main category: cs.NI

TL;DR: 论文提出了一种不依赖具体应用的方法，通过加密流量估计视频通话的QoE指标，解决了网络运营商因加密流量无法获取终端设备QoE数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现代通信中，IMVCAs和VCAs的QoE对网络运营商至关重要，但由于加密流量，运营商无法直接获取终端设备的QoE指标，需要一种新的解决方案。

Method: 提出了一种应用无关的方法，通过加密流量估计视频QoE指标，并利用机器学习模型进行训练和验证。

Result: 在多样化数据集上验证了方法的有效性，FPS预测准确率达85.2%（误差±2 FPS），PIQE质量分类准确率达90.2%。

Conclusion: 该方法具有广泛适用性，可应用于多种专有IMVCAs和VCAs，为网络运营商提供了一种有效的QoE评估工具。

Abstract: Instant Messaging-Based Video Call Applications (IMVCAs) and Video
Conferencing Applications (VCAs) have become integral to modern communication.
Ensuring a high Quality of Experience (QoE) for users in this context is
critical for network operators, as network conditions significantly impact user
QoE. However, network operators lack access to end-device QoE metrics due to
encrypted traffic. Existing solutions estimate QoE metrics from encrypted
traffic traversing the network, with the most advanced approaches leveraging
machine learning models. Subsequently, the need for ground truth QoE metrics
for training and validation poses a challenge, as not all video applications
provide these metrics. To address this challenge, we propose an
application-agnostic approach for objective QoE estimation from encrypted
traffic. Independent of the video application, we obtained key video QoE
metrics, enabling broad applicability to various proprietary IMVCAs and VCAs.
To validate our solution, we created a diverse dataset from WhatsApp video
sessions under various network conditions, comprising 25,680 seconds of traffic
data and QoE metrics. Our evaluation shows high performance across the entire
dataset, with 85.2% accuracy for FPS predictions within an error margin of two
FPS, and 90.2% accuracy for PIQE-based quality rating classification.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [482] [From Interaction to Collaboration: How Hybrid Intelligence Enhances Chatbot Feedback](https://arxiv.org/abs/2504.13848)
*Janet Rafner,Ryan Q. Guloy,Eden W. Wen,Catherine M. Chiodo,Jacob Sherson*

Main category: cs.HC

TL;DR: 研究探讨了两种叙事和反馈机制对用户参与和反馈行为的影响，发现混合智能（HI）叙事能显著增加用户提供的详细反馈。


<details>
  <summary>Details</summary>
Motivation: 生成式AI聊天机器人的成功依赖于收集有意义的用户反馈，以提升交互质量、系统效果和用户接受度。

Method: 比较标准AI叙事与混合智能（HI）叙事对用户反馈行为的影响。

Result: HI叙事显著增加了用户提供的详细反馈，但在其他方面（如用户意愿、信任）无显著差异。

Conclusion: 研究为设计有效的反馈系统提供了见解，需平衡用户努力与系统改进潜力。

Abstract: Generative AI (GenAI) chatbots are becoming increasingly integrated into
virtual assistant technologies, yet their success hinges on the ability to
gather meaningful user feedback to improve interaction quality, system
outcomes, and overall user acceptance. Successful chatbot interactions can
enable organizations to build long-term relationships with their customers and
users, supporting customer loyalty and furthering the organization's goals.
This study explores the impact of two distinct narratives and feedback
collection mechanisms on user engagement and feedback behavior: a standard
AI-focused interaction versus a hybrid intelligence (HI) framed interaction.
Initial findings indicate that while small-scale survey measures allowed for no
significant differences in user willingness to leave feedback, use the system,
or trust the system, participants exposed to the HI narrative statistically
significantly provided more detailed feedback. These initial findings offer
insights into designing effective feedback systems for GenAI virtual
assistants, balancing user effort with system improvement potential.

</details>


### [483] [Towards Balancing Preference and Performance through Adaptive Personalized Explainability](https://arxiv.org/abs/2504.13856)
*Andrew Silva,Pradyumna Tambwekar,Mariah Schrum,Matthew Gombolay*

Main category: cs.HC

TL;DR: 研究探讨了在自动驾驶车辆模拟环境中，用户对可解释人工智能（xAI）模式的偏好及个性化策略，发现偏好与性能不一致，并提出了自适应策略以平衡两者。


<details>
  <summary>Details</summary>
Motivation: 机器人需通过解释决策标准以建立信任和促进协作，但现有xAI方法假设单一模式适合所有用户，忽视了用户多样化的需求和偏好。

Method: 通过两项用户研究，比较语言解释、特征重要性图和决策树等xAI模式在偏好和性能上的差异，并开发自适应个性化策略。

Result: 发现xAI模式在偏好和性能上存在显著差异（p < 0.01和p < 0.05），且用户偏好与性能不一致，自适应策略显著提升了性能（p < 0.05）。

Conclusion: 自适应个性化策略能有效平衡用户偏好与性能，为xAI在人类-机器人交互中的应用提供了新思路。

Abstract: As robots and digital assistants are deployed in the real world, these agents
must be able to communicate their decision-making criteria to build trust,
improve human-robot teaming, and enable collaboration. While the field of
explainable artificial intelligence (xAI) has made great strides to enable such
communication, these advances often assume that one xAI approach is ideally
suited to each problem (e.g., decision trees to explain how to triage patients
in an emergency or feature-importance maps to explain radiology reports). This
fails to recognize that users have diverse experiences or preferences for
interaction modalities. In this work, we present two user-studies set in a
simulated autonomous vehicle (AV) domain. We investigate (1) population-level
preferences for xAI and (2) personalization strategies for providing robot
explanations. We find significant differences between xAI modes (language
explanations, feature-importance maps, and decision trees) in both preference
(p < 0.01) and performance (p < 0.05). We also observe that a participant's
preferences do not always align with their performance, motivating our
development of an adaptive personalization strategy to balance the two. We show
that this strategy yields significant performance gains (p < 0.05), and we
conclude with a discussion of our findings and implications for xAI in
human-robot interactions.

</details>


### [484] [The Effect of Explainable AI-based Decision Support on Human Task Performance: A Meta-Analysis](https://arxiv.org/abs/2504.13858)
*Felix Haag*

Main category: cs.HC

TL;DR: 本文通过元分析探讨了可解释AI（XAI）对分类任务中人类表现的影响，发现XAI能提升任务表现，但解释本身并非决定性因素。研究偏倚风险对解释效果有调节作用，而解释类型影响较小。


<details>
  <summary>Details</summary>
Motivation: 信息系统中解释的透明性需求推动了可解释AI（XAI）的发展，但现有研究对XAI是否能提升用户任务表现存在不一致结论。

Method: 采用元分析方法，综合评估XAI对人类在分类任务中表现的影响。

Result: XAI能提升任务表现，但解释本身并非主要驱动因素；研究偏倚风险对解释效果有显著调节作用，解释类型影响较小。

Conclusion: 研究结果增进了对人-XAI协作的理解，为决策支持系统中XAI的应用提供了实证支持。

Abstract: The desirable properties of explanations in information systems have fueled
the demands for transparency in artificial intelligence (AI) outputs. To
address these demands, the field of explainable AI (XAI) has put forth methods
that can support human decision-making by explaining AI outputs. However,
current empirical works present inconsistent findings on whether such
explanations help to improve users' task performance in decision support
systems (DSS). In this paper, we conduct a meta-analysis to explore how XAI
affects human performance in classification tasks. Our results show an
improvement in task performance through XAI-based decision support, though
explanations themselves are not the decisive driver for this improvement. The
analysis reveals that the studies' risk of bias moderates the effect of
explanations in AI, while the explanation type appears to play only a
negligible role. Our findings contribute to the human computer interaction
field by enhancing the understanding of human-XAI collaboration in DSS.

</details>


### [485] [DoYouTrustAI: A Tool to Teach Students About AI Misinformation and Prompt Engineering](https://arxiv.org/abs/2504.13859)
*Phillip Driscoll,Priyanka Kumar*

Main category: cs.HC

TL;DR: 开发了一个名为DoYouTrustAI的网页应用，帮助K-12学生识别LLM生成的历史人物信息中的误导内容，以提升批判性思维。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成内容可能传播错误信息的担忧，尤其是对学生的潜在影响。

Method: 通过提示工程生成历史人物的准确或误导性摘要，学生需判断其真实性，无需外部资源。

Result: 工具能有效帮助学生识别误导信息，并理解提示工程对AI输出的影响。

Conclusion: 验证AI生成内容的重要性，提示工程可作为教育工具，帮助学生理解AI的局限性。

Abstract: AI, especially Large Language Models (LLMs) like ChatGPT, have rapidly
developed and gained widespread adoption in the past five years, shifting user
preference from traditional search engines. However, the generative nature of
LLMs raises concerns about presenting misinformation as fact. To address this,
we developed a web-based application that helps K-12 students enhance critical
thinking by identifying misleading information in LLM responses about major
historical figures. In this paper, we describe the implementation and design
details of the DoYouTrustAI tool, which can be used to provide an interactive
lesson which teaches students about the dangers of misinformation and how
believable generative AI can make it seem. The DoYouTrustAI tool utilizes
prompt engineering to present the user with AI generated summaries about the
life of a historical figure. These summaries can be either accurate accounts of
that persons life, or an intentionally misleading alteration of their history.
The user is tasked with determining the validity of the statement without
external resources. Our research questions for this work were:(RQ1) How can we
design a tool that teaches students about the dangers of misleading information
and of how misinformation can present itself in LLM responses? (RQ2) Can we
present prompt engineering as a topic that is easily understandable for
students? Our findings highlight the need to correct misleading information
before users retain it. Our tool lets users select familiar individuals for
testing to reduce random guessing and presents misinformation alongside known
facts to maintain believability. It also provides pre-configured prompt
instructions to show how different prompts affect AI responses. Together, these
features create a controlled environment where users learn the importance of
verifying AI responses and understanding prompt engineering.

</details>


### [486] [A Survey on (M)LLM-Based GUI Agents](https://arxiv.org/abs/2504.13865)
*Fei Tang,Haolei Xu,Hang Zhang,Siqi Chen,Xingyu Wu,Yongliang Shen,Wenqi Zhang,Guiyang Hou,Zeqi Tan,Yuchen Yan,Kaitao Song,Jian Shao,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.HC

TL;DR: 本文综述了基于LLM的GUI代理的架构、技术组件和评估方法，分析了其四大核心组成部分，并探讨了当前挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着GUI代理从规则脚本发展为AI驱动系统，需要系统梳理其技术进展与挑战，推动智能界面自动化的发展。

Method: 通过分析感知系统、探索机制、规划框架和交互系统四大组件，结合LLM和多模态学习的进展，系统评估GUI代理的能力。

Result: 揭示了LLM和多模态学习如何革新GUI自动化，同时指出了现有评估框架的局限性及标准化方向。

Conclusion: 本文为研究者提供了GUI代理领域的全面概述，并展望了未来增强其能力的研究方向。

Abstract: Graphical User Interface (GUI) Agents have emerged as a transformative
paradigm in human-computer interaction, evolving from rule-based automation
scripts to sophisticated AI-driven systems capable of understanding and
executing complex interface operations. This survey provides a comprehensive
examination of the rapidly advancing field of LLM-based GUI Agents,
systematically analyzing their architectural foundations, technical components,
and evaluation methodologies. We identify and analyze four fundamental
components that constitute modern GUI Agents: (1) perception systems that
integrate text-based parsing with multimodal understanding for comprehensive
interface comprehension; (2) exploration mechanisms that construct and maintain
knowledge bases through internal modeling, historical experience, and external
information retrieval; (3) planning frameworks that leverage advanced reasoning
methodologies for task decomposition and execution; and (4) interaction systems
that manage action generation with robust safety controls. Through rigorous
analysis of these components, we reveal how recent advances in large language
models and multimodal learning have revolutionized GUI automation across
desktop, mobile, and web platforms. We critically examine current evaluation
frameworks, highlighting methodological limitations in existing benchmarks
while proposing directions for standardization. This survey also identifies key
technical challenges, including accurate element localization, effective
knowledge retrieval, long-horizon planning, and safety-aware execution control,
while outlining promising research directions for enhancing GUI Agents'
capabilities. Our systematic review provides researchers and practitioners with
a thorough understanding of the field's current state and offers insights into
future developments in intelligent interface automation.

</details>


### [487] [Skeleton-Based Transformer for Classification of Errors and Better Feedback in Low Back Pain Physical Rehabilitation Exercises](https://arxiv.org/abs/2504.13866)
*Aleksa Marusic,Sao Mai Nguyen,Adriana Tapus*

Main category: cs.HC

TL;DR: 提出了一种基于Transformer的算法，用于康复训练中的错误分类，并提供关节重要性反馈。


<details>
  <summary>Details</summary>
Motivation: 患者在没有直接监督的情况下参与度下降，现有评估方法仅提供二元或连续评分，不足以帮助患者改进。

Method: 提出基于Transformer的模型，灵感来自HyperFormer方法，用于骨架运动评估。

Result: 在KERAAL数据集上显著超越现有方法，并提供关节重要性分析。

Conclusion: 该模型为患者提供更详细的反馈，是康复训练评估的重要一步。

Abstract: Physical rehabilitation exercises suggested by healthcare professionals can
help recovery from various musculoskeletal disorders and prevent re-injury.
However, patients' engagement tends to decrease over time without direct
supervision, which is why there is a need for an automated monitoring system.
In recent years, there has been great progress in quality assessment of
physical rehabilitation exercises. Most of them only provide a binary
classification if the performance is correct or incorrect, and a few provide a
continuous score. This information is not sufficient for patients to improve
their performance. In this work, we propose an algorithm for error
classification of rehabilitation exercises, thus making the first step toward
more detailed feedback to patients. We focus on skeleton-based exercise
assessment, which utilizes human pose estimation to evaluate motion. Inspired
by recent algorithms for quality assessment during rehabilitation exercises, we
propose a Transformer-based model for the described classification. Our model
is inspired by the HyperFormer method for human action recognition, and adapted
to our problem and dataset. The evaluation is done on the KERAAL dataset, as it
is the only medical dataset with clear error labels for the exercises, and our
model significantly surpasses state-of-the-art methods. Furthermore, we bridge
the gap towards better feedback to the patients by presenting a way to
calculate the importance of joints for each exercise.

</details>


### [488] [Using Generative AI Personas Increases Collective Diversity in Human Ideation](https://arxiv.org/abs/2504.13868)
*Yun Wan,Yoram M Kalman*

Main category: cs.HC

TL;DR: 通过引入多样化的AI角色生成故事情节，研究发现可以避免生成式AI对创意多样性的负面影响，甚至可能提升多样性。


<details>
  <summary>Details</summary>
Motivation: 挑战生成式AI在创意产出中降低多样性的普遍观点，探索通过多样化AI输入来维持或增强创意多样性。

Method: 修改现有研究设计，使用10种不同特质的AI角色生成300个故事情节，比较人类在AI辅助与非辅助下的创意多样性。

Result: AI辅助的故事在情节和语言多样性上与非辅助故事相当，甚至在某些方面表现更优。

Conclusion: 通过多样化的AI输入设计，可以消除生成式AI对创意多样性的负面影响，甚至可能提升多样性。

Abstract: This study challenges the widely-reported tradeoff between generative AI's
(GenAI) contribution to creative outcomes and decreased diversity of these
outcomes. We modified the design of such a study, by Doshi and Hauser (2024),
in which participants wrote short stories either aided or unaided by GenAI plot
ideas[1]. In the modified study, plot ideas were generated through ten unique
GenAI "personas" with diverse traits (e.g. cultural backgrounds, thinking
styles, genre preferences), creating a pool of 300 story plots. While plot
ideas from any individual persona showed high similarity (average cosine
similarity of 0.92), ideas across different personas exhibited substantial
variation (average similarity of 0.20). When human participants wrote stories
based on these diverse plot ideas, their collective outputs maintained the same
level of diversity as stories written without GenAI assistance, effectively
eliminating the diversity reduction observed in [1]. Traditional text analytics
further revealed that GenAI-assisted stories featured greater diversity in
descriptive and emotional language compared to purely human-generated stories
without GenAI assistance. Our findings demonstrate that introducing diversity
at the AI input stage through distinct personas can preserve and potentially
enhance the collective diversity of human creative outputs when collaborating
with GenAI.

</details>


### [489] [Interview AI-ssistant: Designing for Real-Time Human-AI Collaboration in Interview Preparation and Execution](https://arxiv.org/abs/2504.13847)
*Zhe Liu*

Main category: cs.HC

TL;DR: 论文提出Interview AI-ssistant系统，通过四项研究探索人机协作在访谈中的设计与效果，为智能访谈支持系统提供实践指导。


<details>
  <summary>Details</summary>
Motivation: 访谈中实时信息处理、问题调整和关系维护等认知挑战需要AI辅助解决。

Method: 通过四项研究：需求调研、AI辅助访谈准备原型开发、实时AI辅助实验评估及实地部署。

Result: 系统在访谈准备和执行阶段有效支持人机协作，并为智能用户界面设计提供指导。

Conclusion: 研究推动了复杂社交任务中人机协作界面的理解，并为AI增强的定性研究工具设计提供了指南。

Abstract: Recent advances in large language models (LLMs) offer unprecedented
opportunities to enhance human-AI collaboration in qualitative research
methods, including interviews. While interviews are highly valued for gathering
deep, contextualized insights, interviewers often face significant cognitive
challenges, such as real-time information processing, question adaptation, and
rapport maintenance. My doctoral research introduces Interview AI-ssistant, a
system designed for real-time interviewer-AI collaboration during both the
preparation and execution phases. Through four interconnected studies, this
research investigates the design of effective human-AI collaboration in
interviewing contexts, beginning with a formative study of interviewers' needs,
followed by a prototype development study focused on AI-assisted interview
preparation, an experimental evaluation of real-time AI assistance during
interviews, and a field study deploying the system in a real-world research
setting. Beyond informing practical implementations of intelligent interview
support systems, this work contributes to the Intelligent User Interfaces (IUI)
community by advancing the understanding of human-AI collaborative interfaces
in complex social tasks and establishing design guidelines for AI-enhanced
qualitative research tools.

</details>


### [490] [Human aversion? Do AI Agents Judge Identity More Harshly Than Performance](https://arxiv.org/abs/2504.13871)
*Yuanjun Feng,Vivek Chodhary,Yash Raj Shrestha*

Main category: cs.HC

TL;DR: 研究探讨了AI在混合决策系统中对人类判断的评估，发现AI倾向于低估人类输入，尤其是在身份披露和顺序偏见下。


<details>
  <summary>Details</summary>
Motivation: 填补管理研究中AI评估人类判断的空白，解决因隐私问题无法直接使用LLMs的企业的需求。

Method: 通过控制预测任务，分析基于LLM的AI如何权衡人类与算法预测。

Result: AI系统系统性低估人类建议，对人类错误惩罚更严厉，身份披露和顺序偏见加剧此现象。

Conclusion: 揭示了AI信任指标与实际人类判断影响的不匹配，提出了间接部署LLM的框架，强调审计AI权重机制的重要性。

Abstract: This study examines the understudied role of algorithmic evaluation of human
judgment in hybrid decision-making systems, a critical gap in management
research. While extant literature focuses on human reluctance to follow
algorithmic advice, we reverse the perspective by investigating how AI agents
based on large language models (LLMs) assess and integrate human input. Our
work addresses a pressing managerial constraint: firms barred from deploying
LLMs directly due to privacy concerns can still leverage them as mediating
tools (for instance, anonymized outputs or decision pipelines) to guide
high-stakes choices like pricing or discounts without exposing proprietary
data. Through a controlled prediction task, we analyze how an LLM-based AI
agent weights human versus algorithmic predictions. We find that the AI system
systematically discounts human advice, penalizing human errors more severely
than algorithmic errors--a bias exacerbated when the agent's identity (human vs
AI) is disclosed and the human is positioned second. These results reveal a
disconnect between AI-generated trust metrics and the actual influence of human
judgment, challenging assumptions about equitable human-AI collaboration. Our
findings offer three key contributions. First, we identify a reverse algorithm
aversion phenomenon, where AI agents undervalue human input despite comparable
error rates. Second, we demonstrate how disclosure and positional bias interact
to amplify this effect, with implications for system design. Third, we provide
a framework for indirect LLM deployment that balances predictive power with
data privacy. For practitioners, this research emphasize the need to audit AI
weighting mechanisms, calibrate trust dynamics, and strategically design
decision sequences in human-AI systems.

</details>


### [491] [3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark](https://arxiv.org/abs/2504.13861)
*Ivan Sviridov,Amina Miftakhova,Artemiy Tereshchenko,Galina Zubkova,Pavel Blinov,Andrey Savchenko*

Main category: cs.HC

TL;DR: 3MDBench是一个开源评估框架，用于评估大型视觉语言模型在医疗咨询中的表现，通过模拟多样化患者行为和整合多模态数据提升诊断质量。


<details>
  <summary>Details</summary>
Motivation: 探索大型视觉语言模型在远程医疗中与多样化患者互动的能力，填补现有评估工具的不足。

Method: 开发3MDBench框架，模拟四种气质驱动的患者代理和评估代理，整合文本和图像数据，评估不同诊断策略下的模型表现。

Result: 对话和多模态输入显著提升诊断准确性（F1分数从50.4到54.2），结合CNN模型后F1分数进一步提升至70.3。

Conclusion: 3MDBench为AI驱动的医疗助手提供了可扩展的评估工具，强调了患者气质、对话策略和多模态推理对诊断质量的重要性。

Abstract: Large Vision-Language Models (LVLMs) are increasingly being explored for
applications in telemedicine, yet their ability to engage with diverse patient
behaviors remains underexplored. We introduce 3MDBench (Medical Multimodal
Multi-agent Dialogue Benchmark), an open-source evaluation framework designed
to assess LLM-driven medical consultations. Unlike existing benchmarks,
3MDBench simulates real-world patient variability by incorporating four
temperament-driven Patient Agents and an Assessor Agent that evaluates
diagnostic accuracy and dialogue quality. The benchmark integrates textual and
image-based patient data across 34 common diagnoses, mirroring real-world
telemedicine interactions. Under different diagnostic strategies, we evaluate
state-of-the-art LVLMs. Our findings demonstrate that incorporating dialogue
improves the F1 score from 50.4 to 54.2 compared to non-dialogue settings,
underscoring the value of context-driven, information-seeking questioning.
Additionally, we demonstrate that multimodal inputs enhance diagnostic
efficiency. Image-supported models outperform text-only counterparts by raising
the diagnostic F1 score from 52.8 to 54.2 in a similar dialogue setting.
Finally, we suggest an approach that improves the diagnostic F1-score to 70.3
by training the CNN model on the diagnosis prediction task and incorporating
its top-3 predictions into the LVLM context. 3MDBench provides a reproducible
and extendable evaluation framework for AI-driven medical assistants. It offers
insights into how patient temperament, dialogue strategies, and multimodal
reasoning influence diagnosis quality. By addressing real-world complexities in
telemedicine, our benchmark paves the way for more empathetic, reliable, and
context-aware AI-driven healthcare solutions. The source code of our benchmark
is publicly available: https://github.com/univanxx/3mdbench

</details>


### [492] [New care pathways for supporting transitional care from hospitals to home using AI and personalized digital assistance](https://arxiv.org/abs/2504.13877)
*Ionut Anghel,Tudor Cioara,Roberta Bevilacqua,Federico Barbarossa,Terje Grimstad,Riitta Hellman,Arnor Solberg,Lars Thomas Boye,Ovidiu Anchidin,Ancuta Nemes,Camilla Gabrielsen*

Main category: cs.HC

TL;DR: 本文探讨了过渡性护理在欧洲未来医疗系统中的重要性，提出通过整合物联网、人工智能和数字辅助技术来优化从医院到家庭的护理路径，以减少再住院风险并提升患者生活质量。


<details>
  <summary>Details</summary>
Motivation: 随着人口老龄化，医疗需求增加，过渡性护理的可持续发展成为关键。整合创新技术可以解决当前护理路径中的不足，提升患者体验。

Method: 通过分析物联网、人工智能和数字辅助技术与传统护理路径的整合，识别现有差距并提出技术映射方案。

Result: 研究旨在改善患者预后、安全性和生活质量，减少再住院率，并为技术整合的临床效果提供证据。

Conclusion: 技术整合有望优化过渡性护理，对欧洲医疗系统产生积极影响。

Abstract: Transitional care may play a vital role for the sustainability of Europe
future healthcare system, offering solutions for relocating patient care from
hospital to home therefore addressing the growing demand for medical care as
the population is ageing. However, to be effective, it is essential to
integrate innovative Information and Communications Technology technologies to
ensure that patients with comorbidities experience a smooth and coordinated
transition from hospitals or care centers to home, thereby reducing the risk of
rehospitalization. In this paper, we present an overview of the integration of
Internet of Things, artificial intelligence, and digital assistance
technologies with traditional care pathways to address the challenges and needs
of healthcare systems in Europe. We identify the current gaps in transitional
care and define the technology mapping to enhance the care pathways, aiming to
improve patient outcomes, safety, and quality of life avoiding hospital
readmissions. Finally, we define the trial setup and evaluation methodology
needed to provide clinical evidence that supports the positive impact of
technology integration on patient care and discuss the potential effects on the
healthcare system.

</details>


### [493] [Towards a Multimodal Document-grounded Conversational AI System for Education](https://arxiv.org/abs/2504.13884)
*Karan Taneja,Anjali Singh,Ashok K. Goel*

Main category: cs.HC

TL;DR: MuDoC是一个基于GPT-4o的多模态对话AI系统，结合文本和图像提升学习效果，并通过可验证性增强信任。研究发现视觉内容和可验证性提高了学习者的参与度和信任，但对任务表现无显著影响。


<details>
  <summary>Details</summary>
Motivation: 探索多模态对话AI在教育中的应用，解决现有文本对话系统的局限性，并提升学习者的信任和参与度。

Method: 基于GPT-4o开发MuDoC系统，结合文本和图像生成响应，并允许验证内容来源。与纯文本系统对比，评估学习者参与度、信任和任务表现。

Result: 视觉内容和可验证性显著提升了学习者的参与度和信任，但对问题解决任务的表现无显著影响。

Conclusion: 多模态对话AI在教育中具有潜力，未来需进一步优化以提升学习效果。

Abstract: Multimedia learning using text and images has been shown to improve learning
outcomes compared to text-only instruction. But conversational AI systems in
education predominantly rely on text-based interactions while multimodal
conversations for multimedia learning remain unexplored. Moreover, deploying
conversational AI in learning contexts requires grounding in reliable sources
and verifiability to create trust. We present MuDoC, a Multimodal
Document-grounded Conversational AI system based on GPT-4o, that leverages both
text and visuals from documents to generate responses interleaved with text and
images. Its interface allows verification of AI generated content through
seamless navigation to the source. We compare MuDoC to a text-only system to
explore differences in learner engagement, trust in AI system, and their
performance on problem-solving tasks. Our findings indicate that both visuals
and verifiability of content enhance learner engagement and foster trust;
however, no significant impact in performance was observed. We draw upon
theories from cognitive and learning sciences to interpret the findings and
derive implications, and outline future directions for the development of
multimodal conversational AI systems in education.

</details>


### [494] [Toward Automated Qualitative Analysis: Leveraging Large Language Models for Tutoring Dialogue Evaluation](https://arxiv.org/abs/2504.13882)
*Megan Gu,Chloe Qianhui Zhao,Claire Liu,Nikhil Patel,Jahnvi Shah,Jionghao Lin,Kenneth R. Koedinger*

Main category: cs.HC

TL;DR: 论文介绍了一种利用大语言模型（LLMs）自动评估五种关键辅导策略有效性的系统，结果显示模型在排除错误分类上表现良好，但在准确识别策略上仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索LLMs在辅导策略分析中的应用潜力，以提升辅导对话的评估效率。

Method: 使用GPT-3.5和少量示例提示（few-shot prompting）对公开数据集（Teacher-Student Chatroom Corpus）中的辅导策略进行分类。

Result: 模型在五种策略上的真阴性率（TNR）为0.655至0.738，召回率（Recall）为0.327至0.432，其中“帮助学生管理不平等”表现最佳。

Conclusion: 研究证实了LLMs在辅导策略分析中的潜力，并建议未来结合更先进的模型以提供更细致的反馈。

Abstract: Our study introduces an automated system leveraging large language models
(LLMs) to assess the effectiveness of five key tutoring strategies: 1. giving
effective praise, 2. reacting to errors, 3. determining what students know, 4.
helping students manage inequity, and 5. responding to negative self-talk.
Using a public dataset from the Teacher-Student Chatroom Corpus, our system
classifies each tutoring strategy as either being employed as desired or
undesired. Our study utilizes GPT-3.5 with few-shot prompting to assess the use
of these strategies and analyze tutoring dialogues. The results show that for
the five tutoring strategies, True Negative Rates (TNR) range from 0.655 to
0.738, and Recall ranges from 0.327 to 0.432, indicating that the model is
effective at excluding incorrect classifications but struggles to consistently
identify the correct strategy. The strategy \textit{helping students manage
inequity} showed the highest performance with a TNR of 0.738 and Recall of
0.432. The study highlights the potential of LLMs in tutoring strategy analysis
and outlines directions for future improvements, including incorporating more
advanced models for more nuanced feedback.

</details>


### [495] [Kanji Workbook: A Writing-Based Intelligent Tutoring System for Learning Proper Japanese Kanji Writing Technique with Instructor-Emulated Assessment](https://arxiv.org/abs/2504.13888)
*Paul Taele,Jung In Koh,Tracy Hammond*

Main category: cs.HC

TL;DR: Kanji Workbook是一个智能辅导系统，通过模拟教师反馈帮助学生学习日语汉字书写，提升课程成绩。


<details>
  <summary>Details</summary>
Motivation: 英语母语学生在学习日语汉字时面临困难，现有教育工具缺乏教师模拟反馈。

Method: 开发Kanji Workbook系统，结合智能评分和视觉动画提供多样化书写评估。

Result: 使用该系统的学生在课程成绩上表现优于同龄人，并对系统功能反应积极。

Conclusion: Kanji Workbook有效辅助学生汉字学习，提供更丰富的反馈机制。

Abstract: Kanji script writing is a skill that is often introduced to novice Japanese
foreign language students for achieving Japanese writing mastery, but often
poses difficulties to students with primarily English fluency due to their its
vast differences with written English. Instructors often introduce various
pedagogical methods -- such as visual structure and written techniques -- to
assist students in kanji study, but may lack availability providing direct
feedback on students' writing outside of class. Current educational
applications are also limited due to lacking richer instructor-emulated
feedback. We introduce Kanji Workbook, a writing-based intelligent tutoring
system for students to receive intelligent assessment that emulates human
instructor feedback. Our interface not only leverages students' computing
devices for allowing them to learn, practice, and review the writing of
prompted characters from their course's kanji script lessons, but also provides
a diverse set of writing assessment metrics -- derived from instructor
interviews and classroom observation insights -- through intelligent scoring
and visual animations. We deployed our interface onto novice- and
intermediate-level university courses over an entire academic year, and
observed that interface users on average achieved higher course grades than
their peers and also reacted positively to our interface's various features.

</details>


### [496] [AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants](https://arxiv.org/abs/2504.13887)
*Isabel Villanueva,Tara Bobinac,Binwei Yao,Junjie Hu,Kaiping Chen*

Main category: cs.HC

TL;DR: 研究发现，AI聊天机器人在促进跨文化共情方面存在局限性，尤其是文化对齐不足时。


<details>
  <summary>Details</summary>
Motivation: 探讨AI聊天机器人在跨文化对话中是否能有效促进共情。

Method: 采用随机对话实验，比较不同类型（审议式与非审议式、文化对齐与非对齐）的AI互动效果。

Result: 审议式对话对美国参与者有效，但对拉丁美洲参与者无效，因AI文化表达不准确。

Conclusion: 需设计更具文化真实性的AI系统，以解决跨文化对话中的代表性问题。

Abstract: Despite the growing integration of AI chatbots as conversational agents in
public discourse, empirical evidence regarding their capacity to foster
intercultural empathy remains limited. Using a randomized dialogue experiment,
we examined how different types of AI chatbot interaction, i.e., deliberative
versus non-deliberative and culturally aligned versus non-aligned, affect
intercultural empathy across cultural groups. Results show that deliberative
conversations increased intercultural empathy among American participants but
not Latin American participants, who perceived AI responses as culturally
inaccurate and failing to represent their cultural contexts and perspectives
authentically. Real-time interaction analyses reveal that these differences
stem from cultural knowledge gaps inherent in Large Language Models. Despite
explicit prompting and instruction to represent cultural perspectives in
participants' native languages, AI systems still exhibit significant
disparities in cultural representation. This highlights the importance of
designing AI systems capable of culturally authentic engagement in deliberative
conversations. Our study contributes to deliberation theory and AI alignment
research by underscoring AI's role in intercultural dialogue and the persistent
challenge of representational asymmetry in democratic discourse.

</details>


### [497] [Maestoso: An Intelligent Educational Sketching Tool for Learning Music Theory](https://arxiv.org/abs/2504.13889)
*Paul Taele,Laura Barreto,Tracy Hammond*

Main category: cs.HC

TL;DR: Maestoso是一款通过手写练习帮助初学者学习音乐理论的教育工具，能够自动识别学生绘制的音乐结构并提供反馈。


<details>
  <summary>Details</summary>
Motivation: 现有学习音乐理论的工具在反馈、书写模式或对音乐理论熟悉度要求方面存在不足，Maestoso旨在填补这一空白。

Method: Maestoso利用手写和手势识别技术自动识别学生绘制的音乐结构，并生成模拟教师反馈。

Result: 评估表明，Maestoso能较好地识别音乐结构元素，初学者在一次课程中即可掌握基础音乐理论。

Conclusion: Maestoso为初学者提供了一种有效的音乐理论学习方式，填补了现有工具的不足。

Abstract: Learning music theory not only has practical benefits for musicians to write,
perform, understand, and express music better, but also for both non-musicians
to improve critical thinking, math analytical skills, and music appreciation.
However, current external tools applicable for learning music theory through
writing when human instruction is unavailable are either limited in feedback,
lacking a written modality, or assuming already strong familiarity of music
theory concepts. In this paper, we describe Maestoso, an educational tool for
novice learners to learn music theory through sketching practice of quizzed
music structures. Maestoso first automatically recognizes students' sketched
input of quizzed concepts, then relies on existing sketch and gesture
recognition techniques to automatically recognize the input, and finally
generates instructor-emulated feedback. From our evaluations, we demonstrate
that Maestoso performs reasonably well on recognizing music structure elements
and that novice students can comfortably grasp introductory music theory in a
single session.

</details>


### [498] [Mozualization: Crafting Music and Visual Representation with Multimodal AI](https://arxiv.org/abs/2504.13891)
*Wanfang Xu,Lixiang Zhao,Haiwen Song,Xinheng Song,Zhaolin Lu,Yu Liu,Min Chen,Eng Gee Lim,Lingyun Yu*

Main category: cs.HC

TL;DR: Mozualization是一个音乐生成和编辑工具，通过整合多种输入（如关键词、图像和声音片段）来创作多风格嵌入式音乐。


<details>
  <summary>Details</summary>
Motivation: 受人们通过诗歌、绘画或音乐表达情感的启发，旨在将情感表达转化为连贯且富有表现力的歌曲。

Method: 开发了一个工具，允许用户无缝融入个人偏好和灵感，并通过用户研究评估工具效果。

Result: 用户研究涉及九位音乐爱好者，评估了用户体验、参与度及生成音乐的影响。

Conclusion: 工具能够有效生成多风格音乐，用户研究为改进提供了宝贵见解。

Abstract: In this work, we introduce Mozualization, a music generation and editing tool
that creates multi-style embedded music by integrating diverse inputs, such as
keywords, images, and sound clips (e.g., segments from various pieces of music
or even a playful cat's meow). Our work is inspired by the ways people express
their emotions -- writing mood-descriptive poems or articles, creating drawings
with warm or cool tones, or listening to sad or uplifting music. Building on
this concept, we developed a tool that transforms these emotional expressions
into a cohesive and expressive song, allowing users to seamlessly incorporate
their unique preferences and inspirations. To evaluate the tool and, more
importantly, gather insights for its improvement, we conducted a user study
involving nine music enthusiasts. The study assessed user experience,
engagement, and the impact of interacting with and listening to the generated
music.

</details>


### [499] [Measuring Mental Health Variables in Computational Research: Toward Validated, Dimensional, and Transdiagnostic Approaches](https://arxiv.org/abs/2504.13890)
*Chen Shani,Elizabeth C. Stade*

Main category: cs.HC

TL;DR: 论文指出计算心理健康研究中存在的问题，提出了使用验证性、维度和跨诊断测量方法的建议。


<details>
  <summary>Details</summary>
Motivation: 计算心理健康研究常依赖不适当的心理病理学测量方法，影响研究有效性。

Method: 识别了三个关键问题，并提出了使用验证性、维度和跨诊断测量方法的建议。

Result: 使用更有效的测量方法能提升研究的有效性。

Conclusion: 采用反映心理病理学本质和结构的有效测量方法对计算心理健康研究至关重要。

Abstract: Computational mental health research develops models to predict and
understand psychological phenomena, but often relies on inappropriate measures
of psychopathology constructs, undermining validity. We identify three key
issues: (1) reliance on unvalidated measures (e.g., self-declared diagnosis)
over validated ones (e.g., diagnosis by clinician); (2) treating mental health
constructs as categorical rather than dimensional; and (3) focusing on
disorder-specific constructs instead of transdiagnostic ones. We outline the
benefits of using validated, dimensional, and transdiagnostic measures and
offer practical recommendations for practitioners. Using valid measures that
reflect the nature and structure of psychopathology is essential for
computational mental health research.

</details>


### [500] [The Human Robot Social Interaction (HSRI) Dataset: Benchmarking Foundational Models' Social Reasoning](https://arxiv.org/abs/2504.13898)
*Dong Won Lee,Yubin Kim,Denison Guvenoz,Sooyeon Jeong,Parker Malachowsky,Louis-Philippe Morency,Cynthia Breazeal,Hae Won Park*

Main category: cs.HC

TL;DR: 该研究旨在提升AI代理在真实社交互动中的社会推理能力，通过引入大规模人类-机器人社交互动数据集（HSRI）和八个新基准任务，评估语言模型和基础模型在社交错误识别、解释因素理解、社交互动流程把握及纠正措施提供方面的能力。实验表明当前模型在这些任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 推动具身AI在真实社交互动中的社会推理能力发展，填补现有语言模型和基础模型在评估和改进AI代理社交策略方面的研究空白。

Method: 构建包含400个真实人类-机器人互动视频和超1万条注释的HSRI数据集，设计八个基准任务评估模型在社交错误检测、解释因素识别、互动流程理解和纠正措施提供方面的能力。

Result: 实验显示当前语言模型和基础模型在社交推理任务上表现不佳，表明数据集和基准任务对社会智能AI的发展具有推动作用。

Conclusion: HSRI数据集和基准任务为提升AI的社会智能提供了重要工具，揭示了当前模型在社交推理上的局限性，为未来研究指明了方向。

Abstract: Our work aims to advance the social reasoning of embodied artificial
intelligence (AI) agents in real-world social interactions. Recently, language
models (LMs) and foundational models (FMs) are being utilized as automatic
evaluators of human-AI interactions with the goal of eventually being used to
improve the policy of the AI agent. To enable further research in this
direction, we introduce a large-scale real-world Human Robot Social Interaction
(HSRI) Dataset to benchmark the capabilities of LMs and FMs to identify and
reason about social interactions, specifically with regard to robot social
errors and competencies . Our dataset consists of 400 real-world human social
robot interaction videos and over 10K annotations, detailing the robot's social
errors, competencies, rationale, and corrective actions, capturing unique
aspects of human-AI interaction only present in real-world interactions. To
further assess AI models' ability to reason about social interactions, we
propose eight new benchmark tasks for evaluating centered around whether AI
models can (1) evaluate social interactions via detecting social errors and
competencies, (2) identify the explanatory factors associated to errors and
competencies, (3) understand the flow of real-world social interactions, and
(4) provide reasons and corrective actions for social errors. Human studies and
experiments with modern LMs and FMs reveal that current models struggle with
these tasks, demonstrating that our dataset and benchmark provides a step
forward towards socially intelligent AI.

</details>


### [501] [TALLMesh: a simple application for performing Thematic Analysis with Large Language Models](https://arxiv.org/abs/2504.13892)
*Stefano De Paoli,Alex Fawzi*

Main category: cs.HC

TL;DR: 本文介绍了一种基于大型语言模型（LLMs）的图形用户界面（GUI）应用，用于辅助研究人员进行主题分析（TA），无需编程技能即可操作。


<details>
  <summary>Details</summary>
Motivation: 为社会科学和人文学科等缺乏编程技能的研究人员提供一种简单易用的工具，以进行高质量的主题分析。

Method: 开发了一个基于Streamlit框架的GUI应用，结合Python脚本和LLMs的API，支持用户上传文本数据并生成初始代码和主题。

Result: 应用能够帮助研究人员通过人机交互过程迭代优化代码和主题，同时保持方法论的严谨性。

Conclusion: 该应用为定性研究提供了新工具，未来将进一步优化其功能和扩展应用场景。

Abstract: Thematic analysis (TA) is a widely used qualitative research method for
identifying and interpreting patterns within textual data, such as qualitative
interviews. Recent research has shown that it is possible to satisfactorily
perform TA using Large Language Models (LLMs). This paper presents a novel
application using LLMs to assist researchers in conducting TA. The application
enables users to upload textual data, generate initial codes and themes. All of
this is possible through a simple Graphical User Interface, (GUI) based on the
streamlit framework, working with python scripts for the analysis, and using
Application Program Interfaces of LLMs. Having a GUI is particularly important
for researchers in fields where coding skills may not be prevalent, such as
social sciences or humanities. With the app, users can iteratively refine codes
and themes adopting a human-in-the-loop process, without the need to work with
programming and scripting. The paper describes the application key features,
highlighting its potential for qualitative research while preserving
methodological rigor. The paper discusses the design and interface of the app
and outlines future directions for this work.

</details>


### [502] [Predicting Satisfaction of Counterfactual Explanations from Human Ratings of Explanatory Qualities](https://arxiv.org/abs/2504.13899)
*Marharyta Domnich,Rasmus Moorits Veski,Julius Välja,Kadi Tulver,Raul Vicente*

Main category: cs.HC

TL;DR: 论文研究了反事实解释的质量评估问题，发现可行性和信任是用户满意度的最强预测因素，同时揭示了其他解释标准的重要性。


<details>
  <summary>Details</summary>
Motivation: 反事实解释在可解释AI中广泛应用，但评估其质量仍是一个开放问题，传统指标和用户研究各有局限。

Method: 通过分析206名参与者对反事实解释的评分数据，建模用户满意度与七项解释标准的关系。

Result: 可行性和信任是用户满意度的最强预测因素，其他标准也解释了58%的方差，复杂性独立于满意度。

Conclusion: 研究结果为设计适应不同用户和场景的反事实解释算法提供了依据。

Abstract: Counterfactual explanations are a widely used approach in Explainable AI,
offering actionable insights into decision-making by illustrating how small
changes to input data can lead to different outcomes. Despite their importance,
evaluating the quality of counterfactual explanations remains an open problem.
Traditional quantitative metrics, such as sparsity or proximity, fail to fully
account for human preferences in explanations, while user studies are
insightful but not scalable. Moreover, relying only on a single overall
satisfaction rating does not lead to a nuanced understanding of why certain
explanations are effective or not. To address this, we analyze a dataset of
counterfactual explanations that were evaluated by 206 human participants, who
rated not only overall satisfaction but also seven explanatory criteria:
feasibility, coherence, complexity, understandability, completeness, fairness,
and trust. Modeling overall satisfaction as a function of these criteria, we
find that feasibility (the actionability of suggested changes) and trust (the
belief that the changes would lead to the desired outcome) consistently stand
out as the strongest predictors of user satisfaction, though completeness also
emerges as a meaningful contributor. Crucially, even excluding feasibility and
trust, other metrics explain 58% of the variance, highlighting the importance
of additional explanatory qualities. Complexity appears independent, suggesting
more detailed explanations do not necessarily reduce satisfaction. Strong
metric correlations imply a latent structure in how users judge quality, and
demographic background significantly shapes ranking patterns. These insights
inform the design of counterfactual algorithms that adapt explanatory qualities
to user expertise and domain context.

</details>


### [503] [Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge](https://arxiv.org/abs/2504.13904)
*Donghuo Zeng,Roberto Legaspi,Yuewen Sun,Xinshuai Dong,Kazushi Ikeda,Peter Spirtes,Kun Zhang*

Main category: cs.HC

TL;DR: 论文提出基于因果和反事实知识的自适应策略能优化系统响应，通过因果发现和反事实推理提升个性化对话生成，并在真实数据中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过因果和反事实知识优化系统响应，提升个性化对话系统的效果。

Method: 结合因果发现和反事实推理，将用户和系统行为建模为因果因素，并基于反事实数据优化策略。

Result: 在真实数据集上验证了方法的有效性，显著提升了说服性对话系统的效果。

Conclusion: 因果发现和反事实推理能有效指导个性化对话生成和策略优化。

Abstract: We hypothesize that optimal system responses emerge from adaptive strategies
grounded in causal and counterfactual knowledge. Counterfactual inference
allows us to create hypothetical scenarios to examine the effects of
alternative system responses. We enhance this process through causal discovery,
which identifies the strategies informed by the underlying causal structure
that govern system behaviors. Moreover, we consider the psychological
constructs and unobservable noises that might be influencing user-system
interactions as latent factors. We show that these factors can be effectively
estimated. We employ causal discovery to identify strategy-level causal
relationships among user and system utterances, guiding the generation of
personalized counterfactual dialogues. We model the user utterance strategies
as causal factors, enabling system strategies to be treated as counterfactual
actions. Furthermore, we optimize policies for selecting system responses based
on counterfactual data. Our results using a real-world dataset on social good
demonstrate significant improvements in persuasive system outcomes, with
increased cumulative rewards validating the efficacy of causal discovery in
guiding personalized counterfactual inference and optimizing dialogue policies
for a persuasive dialogue system.

</details>


### [504] [Supporting Students' Reading and Cognition with AI](https://arxiv.org/abs/2504.13900)
*Yue Fu,Alexis Hiniker*

Main category: cs.HC

TL;DR: 研究分析了学生在使用AI工具辅助阅读时的认知行为变化，发现初期倾向于高阶思维，但随时间推移转为被动阅读，并提出设计建议以优化AI阅读支持系统。


<details>
  <summary>Details</summary>
Motivation: 探讨AI工具如何影响用户的阅读过程和认知参与，为未来AI阅读支持系统的设计提供依据。

Method: 收集并分析124次学生使用AI工具辅助阅读的会话数据，根据布鲁姆分类法对用户提示进行分类。

Result: 初期用户倾向于高阶思维（分析、评价），但随时间推移转为被动阅读。

Conclusion: 建议设计结构化支架和主动提示以支持高阶思维，并引入自适应功能以平衡效率与认知参与。

Abstract: With the rapid adoption of AI tools in learning contexts, it is vital to
understand how these systems shape users' reading processes and cognitive
engagement. We collected and analyzed text from 124 sessions with AI tools, in
which students used these tools to support them as they read assigned readings
for an undergraduate course. We categorized participants' prompts to AI
according to Bloom's Taxonomy of educational objectives -- Remembering,
Understanding, Applying, Analyzing, Evaluating. Our results show that
``Analyzing'' and ``Evaluating'' are more prevalent in users' second and third
prompts within a single usage session, suggesting a shift toward higher-order
thinking. However, in reviewing users' engagement with AI tools over several
weeks, we found that users converge toward passive reading engagement over
time. Based on these results, we propose design implications for future AI
reading-support systems, including structured scaffolds for lower-level
cognitive tasks (e.g., recalling terms) and proactive prompts that encourage
higher-order thinking (e.g., analyzing, applying, evaluating). Additionally, we
advocate for adaptive, human-in-the-loop features that allow students and
instructors to tailor their reading experiences with AI, balancing efficiency
with enriched cognitive engagement. Our paper expands the dialogue on
integrating AI into academic reading, highlighting both its potential benefits
and challenges.

</details>


### [505] [AI-Assisted Conversational Interviewing: Effects on Data Quality and User Experience](https://arxiv.org/abs/2504.13908)
*Soubhik Barari,Jarret Angbazo,Natalie Wang,Leah M. Christian,Elizabeth Dean,Zoe Slowinski,Brandon Sepulvado*

Main category: cs.HC

TL;DR: 研究提出了一种AI辅助的对话访谈框架，以平衡标准化调查的规模效率和对话访谈的深度，通过实验验证了文本机器人在编码准确性、回答质量和受访者体验方面的表现。


<details>
  <summary>Details</summary>
Motivation: 标准化调查牺牲深度以换取规模效率，而对话访谈虽提高回答质量但缺乏一致性和可扩展性。研究旨在通过AI方法填补这一空白。

Method: 通过网页调查实验，随机分配1800名参与者与文本机器人互动，动态探求详细回答并实时编码开放性问题。

Result: 文本机器人在实时编码中表现中等，开放性问题回答更详细，但受访者体验略有下降。

Conclusion: 研究表明AI方法可有效增强网络调查中的开放式数据收集。

Abstract: Standardized surveys scale efficiently but sacrifice depth, while
conversational interviews improve response quality at the cost of scalability
and consistency. This study bridges the gap between these methods by
introducing a framework for AI-assisted conversational interviewing. To
evaluate this framework, we conducted a web survey experiment where 1,800
participants were randomly assigned to text-based conversational AI agents, or
"textbots", to dynamically probe respondents for elaboration and interactively
code open-ended responses. We assessed textbot performance in terms of coding
accuracy, response quality, and respondent experience. Our findings reveal that
textbots perform moderately well in live coding even without survey-specific
fine-tuning, despite slightly inflated false positive errors due to respondent
acquiescence bias. Open-ended responses were more detailed and informative, but
this came at a slight cost to respondent experience. Our findings highlight the
feasibility of using AI methods to enhance open-ended data collection in web
surveys.

</details>


### [506] [Modeling the quantum-like dynamics of human reliability ratings in Human-AI interactions by interaction dependent Hamiltonians](https://arxiv.org/abs/2504.13918)
*Johan van der Meer,Pamela Hoyte,Luisa Roeder,Peter Bruza*

Main category: cs.HC

TL;DR: 论文探讨了人类与AI互动中信任的动态建模，提出使用量子随机游走模型来捕捉信任波动，并发现基于经验参数的哈密顿量方法具有潜力。


<details>
  <summary>Details</summary>
Motivation: 随着AI在信息环境中的作用日益增强，人类与AI互动中的信任问题变得至关重要，尤其是在高风险灾害场景中。

Method: 采用量子随机游走模型，结合人类信任判断的波动性，利用经验参数选择不同的哈密顿量。

Result: 研究发现，基于经验参数的哈密顿量方法能有效建模人类与AI互动中信任的演化。

Conclusion: 量子随机游走模型为人类与AI互动中的信任动态提供了有前景的建模工具。

Abstract: As our information environments become ever more powered by artificial
intelligence (AI), the phenomenon of trust in a human's interactions with this
intelligence is becoming increasingly pertinent. For example, in the not too
distant future, there will be teams of humans and intelligent robots involved
in dealing with the repercussions of high-risk disaster situations such as
hurricanes, earthquakes, or nuclear accidents. Even in such conditions of high
uncertainty, humans and intelligent machines will need to engage in shared
decision making, and trust is fundamental to the effectiveness of these
interactions. A key challenge in modeling the dynamics of this trust is to
provide a means to incorporate sensitivity to fluctuations in human trust
judgments. In this article, we explore the ability of Quantum Random Walk
models to model the dynamics of trust in human-AI interactions, and to
integrate a sensitivity to fluctuations in participant trust judgments based on
the nature of the interaction with the AI. We found that using empirical
parameters to inform the use of different Hamiltonians can provide a promising
means to model the evolution of trust in Human-AI interactions.

</details>


### [507] [A Multi-Layered Research Framework for Human-Centered AI: Defining the Path to Explainability and Trust](https://arxiv.org/abs/2504.13926)
*Chameera De Silva,Thilina Halloluwa,Dhaval Vyas*

Main category: cs.HC

TL;DR: 本文提出了一种三层框架，结合人本AI（HCAI）和可解释AI（XAI），以提升AI在高风险领域的透明度、适应性和伦理对齐。


<details>
  <summary>Details</summary>
Motivation: AI在高风险领域（如医疗、金融和自主系统）的应用受到透明度、可解释性和信任问题的限制，缺乏统一方法。

Method: 提出三层框架：基础AI模型（内置可解释机制）、人本解释层（根据认知负荷和用户专业度定制解释）、动态反馈循环（实时交互优化解释）。

Result: 在医疗、金融和软件开发领域验证了框架的有效性，提升了决策质量、合规性和公众信任。

Conclusion: 该框架推动了人本可解释AI（HCXAI）的发展，使AI系统更透明、适应性强且符合伦理。

Abstract: The integration of Artificial Intelligence (AI) into high-stakes domains such
as healthcare, finance, and autonomous systems is often constrained by concerns
over transparency, interpretability, and trust. While Human-Centered AI (HCAI)
emphasizes alignment with human values, Explainable AI (XAI) enhances
transparency by making AI decisions more understandable. However, the lack of a
unified approach limits AI's effectiveness in critical decision-making
scenarios. This paper presents a novel three-layered framework that bridges
HCAI and XAI to establish a structured explainability paradigm. The framework
comprises (1) a foundational AI model with built-in explainability mechanisms,
(2) a human-centered explanation layer that tailors explanations based on
cognitive load and user expertise, and (3) a dynamic feedback loop that refines
explanations through real-time user interaction. The framework is evaluated
across healthcare, finance, and software development, demonstrating its
potential to enhance decision-making, regulatory compliance, and public trust.
Our findings advance Human-Centered Explainable AI (HCXAI), fostering AI
systems that are transparent, adaptable, and ethically aligned.

</details>


### [508] [LLM-Driven NPCs: Cross-Platform Dialogue System for Games and Social Platforms](https://arxiv.org/abs/2504.13928)
*Li Song*

Main category: cs.HC

TL;DR: 研究提出了一种基于大语言模型（LLM）的NPC系统，支持跨平台（游戏环境与社交平台）交互，并通过云数据库同步对话记忆。


<details>
  <summary>Details</summary>
Motivation: 传统游戏中的NPC受限于静态对话树和单一交互平台，无法满足动态交互需求。

Method: 开发了一个原型系统，利用LLM驱动的NPC在Unity游戏环境和Discord社交平台上与玩家交互，并使用LeanCloud云数据库存储对话日志以实现记忆同步。

Result: 初步实验表明跨平台交互在技术上是可行的，并为未来开发（如情感建模和持久记忆支持）奠定了基础。

Conclusion: 该系统为NPC的动态交互提供了新的可能性，未来可进一步扩展功能。

Abstract: NPCs in traditional games are often limited by static dialogue trees and a
single platform for interaction. To overcome these constraints, this study
presents a prototype system that enables large language model (LLM)-powered
NPCs to communicate with players both in the game en vironment (Unity) and on a
social platform (Discord). Dialogue logs are stored in a cloud database
(LeanCloud), allowing the system to synchronize memory between platforms and
keep conversa tions coherent. Our initial experiments show that cross-platform
interaction is technically feasible and suggest a solid foundation for future
developments such as emotional modeling and persistent memory support.

</details>


### [509] [Hashigo: A Next Generation Sketch Interactive System for Japanese Kanji](https://arxiv.org/abs/2504.13940)
*Paul Taele,Tracy Hammond*

Main category: cs.HC

TL;DR: Hashigo是一个汉字书写交互系统，提供人类教师水平的反馈，帮助语言学习者纠正书写技巧和视觉结构问题。


<details>
  <summary>Details</summary>
Motivation: 现有的汉字手写识别系统未能充分评估书写技巧，可能导致学生养成不良学习习惯。

Method: 开发了Hashigo系统，通过自动化反馈机制评估学生的汉字书写。

Result: 系统能够提供针对性的反馈，帮助学生纠正书写中的具体问题。

Conclusion: Hashigo有助于提高学生的汉字学习效果，避免长期学习中的不良习惯。

Abstract: Language students can increase their effectiveness in learning written
Japanese by mastering the visual structure and written technique of Japanese
kanji. Yet, existing kanji handwriting recognition systems do not assess the
written technique sufficiently enough to discourage students from developing
bad learning habits. In this paper, we describe our work on Hashigo, a kanji
sketch interactive system which achieves human instructor-level critique and
feedback on both the visual structure and written technique of students'
sketched kanji. This type of automated critique and feedback allows students to
target and correct specific deficiencies in their sketches that, if left
untreated, are detrimental to effective long-term kanji learning.

</details>


### [510] [Intelligence of Things: A Spatial Context-Aware Control System for Smart Devices](https://arxiv.org/abs/2504.13942)
*Sukanth Kalivarathan,Muhmmad Abrar Raja Mohamed,Aswathy Ravikumar,S Harini*

Main category: cs.HC

TL;DR: INOT是一种新型空间上下文感知控制系统，通过直观的空间推理增强智能家居自动化，解决了现有系统依赖设备标识符的问题。


<details>
  <summary>Details</summary>
Motivation: 当前智能家居系统依赖设备标识符，限制了用户交互的自然性，INOT旨在通过空间上下文实现更自然的控制。

Method: INOT采用模块化架构，结合视觉语言模型和IoT控制系统，支持自然语言空间命令，包括设备检测、空间拓扑推理和意图合成。

Result: 用户研究表明，INOT显著降低认知负荷（NASA-TLX得分平均降低13.17分），易用性更高，14/15用户偏好INOT。

Conclusion: INOT通过消除设备标识符记忆需求，实现了更直观、易用的智能家居控制，是重要技术进步。

Abstract: This paper introduces Intelligence of Things (INOT), a novel spatial
context-aware control system that enhances smart home automation through
intuitive spatial reasoning. Current smart home systems largely rely on
device-specific identifiers, limiting user interaction to explicit naming
conventions rather than natural spatial references. INOT addresses this
limitation through a modular architecture that integrates Vision Language
Models with IoT control systems to enable natural language commands with
spatial context (e.g., "turn on the light near the window"). The system
comprises key components including an Onboarding Inference Engine, Zero-Shot
Device Detection, Spatial Topology Inference, and Intent-Based Command
Synthesis. A comprehensive user study with 15 participants demonstrated INOT's
significant advantages over conventional systems like Google Home Assistant,
with users reporting reduced cognitive workload (NASA-TLX scores decreased by
an average of 13.17 points), higher ease-of-use ratings, and stronger
preference (14 out of 15 participants). By eliminating the need to memorize
device identifiers and enabling context-aware spatial commands, INOT represents
a significant advancement in creating more intuitive and accessible smart home
control systems.

</details>


### [511] [Mixer Metaphors: audio interfaces for non-musical applications](https://arxiv.org/abs/2504.13944)
*Tace McNamara,Jon McCormack,Maria Teresa Llano*

Main category: cs.HC

TL;DR: 探讨音乐界面能否用于非音乐应用，设计了一种基于音频控制隐喻的设备，用于操作大型语言模型（LLM）。实验表明，音频式控制更直观且支持创意实验。


<details>
  <summary>Details</summary>
Motivation: 研究音乐界面在非音乐领域的适用性，探索跨感官隐喻对技术界面设计的价值。

Method: 设计并开发了一种基于音频控制隐喻的设备，比较了两种版本（带/不带音频增强）在艺术家群体中的使用效果。

Result: 音频式控制提供了更直接、直观的LLM操作体验，支持用户的创意实验。

Conclusion: 跨感官隐喻能促进创意思维和具身实践，为新技术界面设计提供新思路。

Abstract: The NIME conference traditionally focuses on interfaces for music and musical
expression. In this paper we reverse this tradition to ask, can interfaces
developed for music be successfully appropriated to non-musical applications?
To help answer this question we designed and developed a new device, which uses
interface metaphors borrowed from analogue synthesisers and audio mixing to
physically control the intangible aspects of a Large Language Model. We
compared two versions of the device, with and without the audio-inspired
augmentations, with a group of artists who used each version over a one week
period. Our results show that the use of audio-like controls afforded more
immediate, direct and embodied control over the LLM, allowing users to
creatively experiment and play with the device over its non-mixer counterpart.
Our project demonstrates how cross-sensory metaphors can support creative
thinking and embodied practice when designing new technological interfaces.

</details>


### [512] [Using customized GPT to develop prompting proficiency in architectural AI-generated images](https://arxiv.org/abs/2504.13948)
*Juan David Salazar Rodriguez,Sam Conrad Joyce,Julfendi Julfendi*

Main category: cs.HC

TL;DR: 研究探讨了定制GPT模型如何提升建筑学生在生成AI驱动图像时的提示能力，通过混合方法实验设计发现，结合结构化指导和AI交互的组别表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具在建筑教育中的普及，提示工程变得至关重要，研究旨在探索如何通过定制GPT模型提升学生的提示能力。

Method: 采用混合方法实验设计，将学生分为三组（无支持、结构化指导、结构化指导加AI交互），通过逆向工程任务评估提示能力。

Result: 定量分析显示，结构化指导和AI交互组在字数、相似性和具体性上显著提升；定性反馈表明学生信心和批判性思维增强。

Conclusion: 定制GPT交互能显著提升学生清晰有效传达建筑概念的能力。

Abstract: This research investigates the use of customized GPT models to enhance
prompting proficiency among architecture students when generating AI-driven
images. Prompt engineering is increasingly essential in architectural education
due to the widespread adoption of generative AI tools. This study utilized a
mixed-methods experimental design involving architecture students divided into
three distinct groups: a control group receiving no structured support, a
second group provided with structured prompting guides, and a third group
supported by both structured guides and interactive AI personas. Students
engaged in reverse engineering tasks, first guessing provided image prompts and
then generating their own prompts, aiming to boost critical thinking and
prompting skills. Variables examined included time spent prompting, word count,
prompt similarity, and concreteness. Quantitative analysis involved correlation
assessments between these variables and a one-way ANOVA to evaluate differences
across groups. While several correlations showed meaningful relationships, not
all were statistically significant. ANOVA results indicated statistically
significant improvements in word count, similarity, and concreteness,
especially in the group supported by AI personas and structured prompting
guides. Qualitative feedback complemented these findings, revealing enhanced
confidence and critical thinking skills in students. These results suggest
tailored GPT interactions substantially improve students' ability to
communicate architectural concepts clearly and effectively.

</details>


### [513] [Tinker Tales: Interactive Storytelling Framework for Early Childhood Narrative Development and AI Literacy](https://arxiv.org/abs/2504.13969)
*Nayoung Choi,Peace Cyebukayire,Jinho D. Choi*

Main category: cs.HC

TL;DR: Tinker Tales是一个互动式讲故事框架，以棋盘游戏形式支持儿童叙事发展和AI素养，结合实体与语音交互，通过评估验证了其安全性和有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过互动方式提升儿童的叙事能力和AI素养，结合实体与数字元素，提供安全的学习环境。

Method: 使用NFC芯片棋子与语音交互，儿童定义故事元素，AI辅助生成内容，模拟游戏会话评估质量与安全性。

Result: 框架展示了结合物理与数字元素在AI教育中的潜力，提供安全且吸引儿童的学习方式。

Conclusion: Tinker Tales为儿童与AI协作提供了创新且安全的互动平台，推动了AI素养的早期教育。

Abstract: This paper presents Tinker Tales, an interactive storytelling framework in
the format of a board game, designed to support both narrative development and
AI literacy in early childhood. The framework integrates tangible and
speech-based interactions with AI through NFC chip-attached pawns and tokens,
along with a speaker and microphone. Children select and define key story
elements-such as characters, places, items, and emotions-using the pawns and
tokens, providing further details to the AI and receiving proper assistance,
similar to how adults prompt AI for specific tasks (e.g., writing). For
evaluation, several game sessions were simulated with a child AI agent, and the
quality and safety of the generated stories were assessed from various
perspectives. This work highlights the potential of combining physical and
digital elements in AI literacy, offering a safe and engaging way for children
to learn how to effectively collaborate with AI.

</details>


### [514] [HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models](https://arxiv.org/abs/2504.14594)
*Fan Gao,Xinjie Zhao,Ding Xia,Zhongyi Zhou,Rui Yang,Jinghui Lu,Hang Jiang,Chanjun Park,Irene Li*

Main category: cs.HC

TL;DR: HealthGenie结合知识图谱（KG）和大语言模型（LLM），提供个性化饮食建议和可视化信息，减少用户交互负担和认知负荷。


<details>
  <summary>Details</summary>
Motivation: 解决用户在获取个性化饮食指导时面临的复杂专业知识和健康条件适配问题。

Method: 系统通过查询优化从预建KG中检索信息，可视化并分类展示，同时提供可解释的建议，用户可交互调整。

Result: 评估实验表明HealthGenie能有效支持用户获取个性化饮食建议，降低交互负担和认知负荷。

Conclusion: LLM与KG的结合在支持决策方面具有潜力，未来系统设计可参考此方法。

Abstract: Seeking dietary guidance often requires navigating complex professional
knowledge while accommodating individual health conditions. Knowledge Graphs
(KGs) offer structured and interpretable nutritional information, whereas Large
Language Models (LLMs) naturally facilitate conversational recommendation
delivery. In this paper, we present HealthGenie, an interactive system that
combines the strengths of LLMs and KGs to provide personalized dietary
recommendations along with hierarchical information visualization for a quick
and intuitive overview. Upon receiving a user query, HealthGenie performs query
refinement and retrieves relevant information from a pre-built KG. The system
then visualizes and highlights pertinent information, organized by defined
categories, while offering detailed, explainable recommendation rationales.
Users can further tailor these recommendations by adjusting preferences
interactively. Our evaluation, comprising a within-subject comparative
experiment and an open-ended discussion, demonstrates that HealthGenie
effectively supports users in obtaining personalized dietary guidance based on
their health conditions while reducing interaction effort and cognitive load.
These findings highlight the potential of LLM-KG integration in supporting
decision-making through explainable and visualized information. We examine the
system's usefulness and effectiveness with an N=12 within-subject study and
provide design considerations for future systems that integrate conversational
LLM and KG.

</details>


### [515] [Completing A Systematic Review in Hours instead of Months with Interactive AI Agents](https://arxiv.org/abs/2504.14822)
*Rui Qiu,Shijie Chen,Yu Su,Po-Yin Yen,Han-Wei Shen*

Main category: cs.HC

TL;DR: InsightAgent是一种基于大型语言模型的人机交互AI代理，通过语义分割和多代理设计显著提升系统综述的质量和效率，同时提供可视化工具和实时反馈机制。


<details>
  <summary>Details</summary>
Motivation: 系统综述在医疗等高风险领域至关重要，但传统方法耗时且依赖专家知识，现有自动化方法无法满足高质量需求。

Method: InsightAgent采用语义分割和多代理设计处理文献，结合可视化工具和用户反馈机制。

Result: 实验显示，InsightAgent将系统综述质量提升27.2%，接近人工水平（79.7%），用户满意度提升34.4%，完成时间从数月缩短至1.5小时。

Conclusion: InsightAgent通过人机交互和AI技术显著优化系统综述流程，提升效率和质量。

Abstract: Systematic reviews (SRs) are vital for evidence-based practice in high stakes
disciplines, such as healthcare, but are often impeded by intensive labors and
lengthy processes that can take months to complete. Due to the high demand for
domain expertise, existing automatic summarization methods fail to accurately
identify relevant studies and generate high-quality summaries. To that end, we
introduce InsightAgent, a human-centered interactive AI agent powered by large
language models that revolutionize this workflow. InsightAgent partitions a
large literature corpus based on semantics and employs a multi-agent design for
more focused processing of literature, leading to significant improvement in
the quality of generated SRs. InsightAgent also provides intuitive
visualizations of the corpus and agent trajectories, allowing users to
effortlessly monitor the actions of the agent and provide real-time feedback
based on their expertise. Our user studies with 9 medical professionals
demonstrate that the visualization and interaction mechanisms can effectively
improve the quality of synthesized SRs by 27.2%, reaching 79.7% of
human-written quality. At the same time, user satisfaction is improved by
34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather
than months, to complete a high-quality systematic review.

</details>


### [516] [Flowco: Rethinking Data Analysis in the Age of LLMs](https://arxiv.org/abs/2504.14038)
*Stephen N. Freund,Brooke Simon,Emery D. Berger,Eunice Jun*

Main category: cs.HC

TL;DR: Flowco是一个混合主动系统，结合了可视化数据流编程模型和LLMs，帮助用户快速编写、调试和优化数据分析，尤其适合编程经验较少的人。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs能生成简单数据分析代码，但在需要精细控制、验证中间结果和迭代优化时，现有工具（如计算笔记本）仍存在不足。

Method: Flowco采用可视化数据流编程模型，并将LLMs集成到编写过程的每个阶段。

Result: 用户研究表明，Flowco能有效支持用户（尤其是编程新手）快速完成数据分析任务。

Conclusion: Flowco通过结合可视化编程和LLMs，解决了数据分析中的控制、验证和迭代问题，提升了工具的易用性和效率。

Abstract: Conducting data analysis typically involves authoring code to transform,
visualize, analyze, and interpret data. Large language models (LLMs) are now
capable of generating such code for simple, routine analyses. LLMs promise to
democratize data science by enabling those with limited programming expertise
to conduct data analyses, including in scientific research, business, and
policymaking. However, analysts in many real-world settings must often exercise
fine-grained control over specific analysis steps, verify intermediate results
explicitly, and iteratively refine their analytical approaches. Such tasks
present barriers to building robust and reproducible analyses using LLMs alone
or even in conjunction with existing authoring tools (e.g., computational
notebooks). This paper introduces Flowco, a new mixed-initiative system to
address these challenges. Flowco leverages a visual dataflow programming model
and integrates LLMs into every phase of the authoring process. A user study
suggests that Flowco supports analysts, particularly those with less
programming experience, in quickly authoring, debugging, and refining data
analyses.

</details>


### [517] [Evaluating Human-AI Interaction via Usability, User Experience and Acceptance Measures for MMM-C: A Creative AI System for Music Composition](https://arxiv.org/abs/2504.14071)
*Renaud Bougueng Tchemeube,Jeff Ens,Cale Plut,Philippe Pasquier,Maryam Safi,Yvan Grabit,Jean-Baptiste Rolland*

Main category: cs.HC

TL;DR: 论文研究了AI工具MMM在音乐创作中的用户接受度，通过插件MMM-Cubase实现人机协作，结果显示用户对系统的可用性和接受度较高，但存在可控性和预测性限制。


<details>
  <summary>Details</summary>
Motivation: 随着AI在艺术领域的应用增多，研究人机协作对音乐实践的影响，特别是通过MMM工具评估用户接受度。

Method: 将MMM集成到Cubase中，开发MMM-C插件，采用混合方法研究（可用性、用户体验和技术接受度），对比业余和专业作曲家。

Result: 用户对系统评价积极，体验新颖且易用，但可控性和预测性有限；两组用户无显著差异。

Conclusion: MMM-C作为人机协作工具在音乐创作中具有潜力，但需改进可控性和预测性。

Abstract: With the rise of artificial intelligence (AI), there has been increasing
interest in human-AI co-creation in a variety of artistic domains including
music as AI-driven systems are frequently able to generate human-competitive
artifacts. Now, the implications of such systems for musical practice are being
investigated. We report on a thorough evaluation of the user adoption of the
Multi-Track Music Machine (MMM) as a co-creative AI tool for music composers.
To do this, we integrate MMM into Cubase, a popular Digital Audio Workstation
(DAW) by Steinberg, by producing a "1-parameter" plugin interface named
MMM-Cubase (MMM-C), which enables human-AI co-composition. We contribute a
methodological assemblage as a 3-part mixed method study measuring usability,
user experience and technology acceptance of the system across two groups of
expert-level composers: hobbyists and professionals. Results show positive
usability and acceptance scores. Users report experiences of novelty, surprise
and ease of use from using the system, and limitations on controllability and
predictability of the interface when generating music. Findings indicate no
significant difference between the two user groups.

</details>


### [518] [Translating Multimodal AI into Real-World Inspection: TEMAI Evaluation Framework and Pathways for Implementation](https://arxiv.org/abs/2504.13873)
*Zehan Li,Jinzhi Deng,Haibing Ma,Chi Zhang,Dan Xiao*

Main category: cs.HC

TL;DR: TEMAI框架将多模态AI与工业检测结合，提出能力、采用和效用三个核心维度，强调技术能力需结合采用机制才能实现价值。


<details>
  <summary>Details</summary>
Motivation: 将医疗领域的转化研究原则应用于工业检测，解决技术能力与价值实现之间的脱节问题。

Method: 建立TEMAI框架，包含能力、采用和效用三个维度，并引入价值密度系数和结构化实施路径。

Result: 实证验证显示，不同行业的价值实现模式差异显著，框架在多个工业领域有效。

Conclusion: TEMAI框架强调行业特定适应策略的重要性，技术能力需结合采用机制以实现价值。

Abstract: This paper introduces the Translational Evaluation of Multimodal AI for
Inspection (TEMAI) framework, bridging multimodal AI capabilities with
industrial inspection implementation. Adapting translational research
principles from healthcare to industrial contexts, TEMAI establishes three core
dimensions: Capability (technical feasibility), Adoption (organizational
readiness), and Utility (value realization). The framework demonstrates that
technical capability alone yields limited value without corresponding adoption
mechanisms. TEMAI incorporates specialized metrics including the Value Density
Coefficient and structured implementation pathways. Empirical validation
through retail and photovoltaic inspection implementations revealed significant
differences in value realization patterns despite similar capability reduction
rates, confirming the framework's effectiveness across diverse industrial
sectors while highlighting the importance of industry-specific adaptation
strategies.

</details>


### [519] [Hybrid Deep Learning Model to Estimate Cognitive Effort from fNIRS Signals in Educational Game Playing](https://arxiv.org/abs/2504.13883)
*Shayla Sharmin,Roghayeh Leila Barmaki*

Main category: cs.HC

TL;DR: 该研究利用混合深度学习模型，基于fNIRS数据和表现分数估计认知努力（CE），以优化学习材料和提升学生参与度。结果显示，混合CNN-GRU模型表现最佳，且特征可泛化至传统机器学习算法。


<details>
  <summary>Details</summary>
Motivation: 通过估计认知努力（CE），帮助教育者调整学习材料，提高学习效果和学生参与度。

Method: 使用fNIRS数据（氧合血红蛋白变化）和任务表现分数，结合深度学习模型（CNN、LSTM、BiLSTM、混合CNN-GRU）预测表现分数并计算RNE和RNI。

Result: 混合CNN-GRU模型表现最佳（训练准确率78.36%，测试准确率73.08%），且特征在XGBoost中泛化良好（准确率69.23%）。预测的RNE和RNI与实际趋势接近。

Conclusion: 研究为设计学习环境提供了有价值的方法和见解，表明高认知努力状态下休息可降低CE。

Abstract: This study estimates cognitive effort (CE) based on functional near-infrared
spectroscopy (fNIRS) data and performance scores using a hybrid deep learning
model. The estimation of CE enables educators to modify material to enhance
learning effectiveness and student engagement. Relative neural efficiency (RNE)
and relative neural involvement (RNI) are two metrics that have been used to
represent CE. To estimate RNE and RNI we need hemodynamic response in the brain
and the performance score of a task.We collected oxygenated hemoglobin ($\Delta
\mathrm{HbO}$). Sixteen participants answered 16 questions in a unity-based
educational game, each with a 30-second response time. We used deep learning
models to predict the performance score and estimate RNE and RNI to understand
CE. The study compares traditional machine learning techniques with deep
learning models such as CNN, LSTM, BiLSTM, and a hybrid CNN-GRU to determine
which approach provides better accuracy in predicting performance scores. The
result shows that the hybrid CNN-GRU gives better performance with 78.36\%
training accuracy and 73.08\% test accuracy than other models. We performed
XGBoost on the extracted GRU feature and got the highest accuracy (69.23\%).
This suggests that the features learned from this hybrid model generalize
better even in traditional machine learning algorithms. We used the $\Delta
\mathrm{HbO}$ and predicted score to calculate RNE and RNI to observe cognitive
effort in our four test cases. Our result shows that even with moderate
accuracy, the predicted RNE and RNI closely follows the actual trends. we also
observed that when participants were in a state of high CE, introducing rest
led decrease of CE. These findings can be helpful to design and improve
learning environments and provide valuable insights in learning materials.

</details>


### [520] [Amplify Initiative: Building A Localized Data Platform for Globalized AI](https://arxiv.org/abs/2504.14105)
*Qazi Mamunur Rashid,Erin van Liemt,Tiffany Shih,Amber Ebinama,Karla Barrios Ramos,Madhurima Maji,Aishwarya Verma,Charu Kalia,Jamila Smith-Loud,Joyce Nakatumba-Nabende,Rehema Baguma,Andrew Katumba,Chodrine Mutebi,Jagen Marvin,Eric Peter Wairagala,Mugizi Bruce,Peter Oketta,Lawrence Nderu,Obichi Obiajunwa,Abigail Oppong,Michael Zimba,Data Authors*

Main category: cs.HC

TL;DR: Amplify Initiative通过专家社区合作，收集多语言高质量数据，解决AI模型因缺乏本地语境和语言多样性而受限的问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型训练数据以英语和西方内容为主，缺乏全球多样性和本地语境，影响其全球适用性和安全性。

Method: 通过Android应用与领域专家（如医生、教师）合作，在撒哈拉以南非洲国家共同创建多语言数据集。

Result: 生成了8,091条七种语言的对抗性查询数据集，涵盖敏感主题（如错误信息），用于评估模型的文化相关性和安全性。

Conclusion: Amplify Initiative的方法有效提升了AI模型在多样语言和文化背景下的适用性和安全性。

Abstract: Current AI models often fail to account for local context and language, given
the predominance of English and Western internet content in their training
data. This hinders the global relevance, usefulness, and safety of these models
as they gain more users around the globe. Amplify Initiative, a data platform
and methodology, leverages expert communities to collect diverse, high-quality
data to address the limitations of these models. The platform is designed to
enable co-creation of datasets, provide access to high-quality multilingual
datasets, and offer recognition to data authors. This paper presents the
approach to co-creating datasets with domain experts (e.g., health workers,
teachers) through a pilot conducted in Sub-Saharan Africa (Ghana, Kenya,
Malawi, Nigeria, and Uganda). In partnership with local researchers situated in
these countries, the pilot demonstrated an end-to-end approach to co-creating
data with 155 experts in sensitive domains (e.g., physicians, bankers,
anthropologists, human and civil rights advocates). This approach, implemented
with an Android app, resulted in an annotated dataset of 8,091 adversarial
queries in seven languages (e.g., Luganda, Swahili, Chichewa), capturing
nuanced and contextual information related to key themes such as misinformation
and public interest topics. This dataset in turn can be used to evaluate models
for their safety and cultural relevance within the context of these languages.

</details>


### [521] [ViMo: A Generative Visual GUI World Model for App Agent](https://arxiv.org/abs/2504.13936)
*Dezhao Luo,Bohan Tang,Kang Li,Georgios Papoudakis,Jifei Song,Shaogang Gong,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.HC

TL;DR: ViMo是一种视觉世界模型，旨在生成未来App界面的图像，通过分解图形和文本内容生成，提升App代理的决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型仅生成文本描述，缺乏视觉细节，限制了App代理在复杂任务中的表现。

Method: ViMo将GUI生成分解为图形和文本内容生成，使用符号文本表示（STR）保留图形，并预测未来GUI的图形和文本。

Result: 实验表明ViMo能生成视觉合理且功能有效的GUI，帮助App代理做出更明智的决策。

Conclusion: ViMo填补了视觉世界模型的空白，显著提升了App代理的长时规划能力。

Abstract: App agents, which autonomously operate mobile Apps through Graphical User
Interfaces (GUIs), have gained significant interest in real-world applications.
Yet, they often struggle with long-horizon planning, failing to find the
optimal actions for complex tasks with longer steps. To address this, world
models are used to predict the next GUI observation based on user actions,
enabling more effective agent planning. However, existing world models
primarily focus on generating only textual descriptions, lacking essential
visual details. To fill this gap, we propose ViMo, the first visual world model
designed to generate future App observations as images. For the challenge of
generating text in image patches, where even minor pixel errors can distort
readability, we decompose GUI generation into graphic and text content
generation. We propose a novel data representation, the Symbolic Text
Representation~(STR) to overlay text content with symbolic placeholders while
preserving graphics. With this design, ViMo employs a STR Predictor to predict
future GUIs' graphics and a GUI-text Predictor for generating the corresponding
text. Moreover, we deploy ViMo to enhance agent-focused tasks by predicting the
outcome of different action options. Experiments show ViMo's ability to
generate visually plausible and functionally effective GUIs that enable App
agents to make more informed decisions.

</details>


### [522] [The Balancing Act of Policies in Developing Machine Learning Explanations](https://arxiv.org/abs/2504.13946)
*Jacob Tjaden*

Main category: cs.HC

TL;DR: 研究发现政策长度影响开发者对部分要求的参与度，但政策目的无影响，且解释质量普遍较差。


<details>
  <summary>Details</summary>
Motivation: 探讨政策设计如何影响机器学习模型的解释质量。

Method: 通过124名参与者的课堂实验，分析政策长度和目的对开发者合规性的影响。

Result: 政策长度影响部分要求的参与度，政策目的无影响，解释质量普遍较差。

Conclusion: 强调有效政策开发的挑战及在解释中考虑多元利益相关者视角的重要性。

Abstract: Machine learning models are often criticized as opaque from a lack of
transparency in their decision-making process. This study examines how policy
design impacts the quality of explanations in ML models. We conducted a
classroom experiment with 124 participants and analyzed the effects of policy
length and purpose on developer compliance with policy requirements. Our
results indicate that while policy length affects engagement with some
requirements, policy purpose has no effect, and explanation quality is
generally poor. These findings highlight the challenge of effective policy
development and the importance of addressing diverse stakeholder perspectives
within explanations.

</details>


### [523] [Longitudinal Study on Social and Emotional Use of AI Conversational Agent](https://arxiv.org/abs/2504.14112)
*Mohit Chandra,Javier Hernandez,Gonzalo Ramos,Mahsa Ershadi,Ananya Bhattacharjee,Judith Amores,Ebele Okoli,Ann Paradiso,Shahed Warreth,Jina Suh*

Main category: cs.HC

TL;DR: 研究探讨了通用对话AI在社交和情感支持中的作用，发现主动使用AI工具显著提升了用户对AI的依恋、共情感知及娱乐动机，但也需防范过度依赖。


<details>
  <summary>Details</summary>
Motivation: 数字技术发展改变了人们寻求社交和情感支持的方式，但AI的广泛使用带来了新的动态，需研究其影响。

Method: 五周探索性研究，149名参与者分为基线使用组和主动使用组，后者使用四种商业AI工具进行互动。

Result: 主动使用组对AI的依恋、共情感知及娱乐动机显著提升，个体差异影响感知，且更愿意向AI寻求个人帮助。

Conclusion: 需开发负责任的情感支持AI工具，同时让用户了解其局限性。

Abstract: Development in digital technologies has continuously reshaped how individuals
seek and receive social and emotional support. While online platforms and
communities have long served this need, the increased integration of
general-purpose conversational AI into daily lives has introduced new dynamics
in how support is provided and experienced. Existing research has highlighted
both benefits (e.g., wider access to well-being resources) and potential risks
(e.g., over-reliance) of using AI for support seeking. In this five-week,
exploratory study, we recruited 149 participants divided into two usage groups:
a baseline usage group (BU, n=60) that used the internet and AI as usual, and
an active usage group (AU, n=89) encouraged to use one of four commercially
available AI tools (Microsoft Copilot, Google Gemini, PI AI, ChatGPT) for
social and emotional interactions. Our analysis revealed significant increases
in perceived attachment towards AI (32.99 percentage points), perceived AI
empathy (25.8 p.p.), and motivation to use AI for entertainment (22.90 p.p.)
among the AU group. We also observed that individual differences (e.g., gender
identity, prior AI usage) influenced perceptions of AI empathy and attachment.
Lastly, the AU group expressed higher comfort in seeking personal help,
managing stress, obtaining social support, and talking about health with AI,
indicating potential for broader emotional support while highlighting the need
for safeguards against problematic usage. Overall, our exploratory findings
underscore the importance of developing consumer-facing AI tools that support
emotional well-being responsibly, while empowering users to understand the
limitations of these tools.

</details>


### [524] [Exploring Language Patterns of Prompts in Text-to-Image Generation and Their Impact on Visual Diversity](https://arxiv.org/abs/2504.14125)
*Maria-Teresa De Rosa Palmini,Eva Cetinic*

Main category: cs.HC

TL;DR: 研究探讨了用户与文本到图像（TTI）模型互动中的语言选择如何影响生成图像的多样性，发现用户参与增加导致提示语言同质化，视觉多样性降低。


<details>
  <summary>Details</summary>
Motivation: 尽管TTI模型的偏见和刻板印象受到关注，但用户与模型互动的社会技术动态尚未充分研究。

Method: 分析CivitAI平台上六百万条提示，将用户分为三类（重复者、偶尔重复者、非重复者），使用Vendi分数量化视觉多样性。

Result: 用户参与增加导致提示语言同质化，40-50%的提示为重复内容；语言重复与视觉多样性降低相关。

Conclusion: 用户行为显著影响AI生成图像的多样性，需开发工具鼓励语言和主题实验以促进多样性。

Abstract: Following the initial excitement, Text-to-Image (TTI) models are now being
examined more critically. While much of the discourse has focused on biases and
stereotypes embedded in large-scale training datasets, the sociotechnical
dynamics of user interactions with these models remain underexplored. This
study examines the linguistic and semantic choices users make when crafting
prompts and how these choices influence the diversity of generated outputs.
Analyzing over six million prompts from the Civiverse dataset on the CivitAI
platform across seven months, we categorize users into three groups based on
their levels of linguistic experimentation: consistent repeaters, occasional
repeaters, and non-repeaters. Our findings reveal that as user participation
grows over time, prompt language becomes increasingly homogenized through the
adoption of popular community tags and descriptors, with repeated prompts
comprising 40-50% of submissions. At the same time, semantic similarity and
topic preferences remain relatively stable, emphasizing common subjects and
surface aesthetics. Using Vendi scores to quantify visual diversity, we
demonstrate a clear correlation between lexical similarity in prompts and the
visual similarity of generated images, showing that linguistic repetition
reinforces less diverse representations. These findings highlight the
significant role of user-driven factors in shaping AI-generated imagery, beyond
inherent model biases, and underscore the need for tools and practices that
encourage greater linguistic and thematic experimentation within TTI systems to
foster more inclusive and diverse AI-generated content.

</details>


### [525] [Apollo: An Interactive Environment for Generating Symbolic Musical Phrases using Corpus-based Style Imitation](https://arxiv.org/abs/2504.14055)
*Renaud Bougueng Tchemeube,Jeff Ens,Philippe Pasquier*

Main category: cs.HC

TL;DR: Apollo是一个基于机器学习的交互式音乐生成系统，通过风格模仿技术生成西方传统音乐的符号化乐句。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习和网络技术在音乐生成中的应用，辅助作曲和音乐研究。

Method: 使用基于语料库的风格模仿技术，构建和管理符号化音乐语料库，生成新乐句。

Result: 系统成功实现为桌面应用，支持MIDI格式导出和流式传输。

Conclusion: 未来将进一步优化系统功能和应用场景。

Abstract: With the recent developments in machine intelligence and web technologies,
new generative music systems are being explored for assisted composition using
machine learning techniques on the web. Such systems are built for various
tasks such as melodic, harmonic or rhythm generation, music interpolation,
continuation and style imitation. In this paper, we introduce Apollo, an
interactive music application for generating symbolic phrases of conventional
western music using corpus-based style imitation techniques. In addition to
enabling the construction and management of symbolic musical corpora, the
system makes it possible for music artists and researchers to generate new
musical phrases in the style of the proposed corpus. The system is available as
a desktop application. The generated symbolic music materials, encoded in the
MIDI format, can be exported or streamed for various purposes including using
them as seed material for musical projects. We present the system design,
implementation details, discuss and conclude with future work for the system.

</details>


### [526] [Calliope: An Online Generative Music System for Symbolic Multi-Track Composition](https://arxiv.org/abs/2504.14058)
*Renaud Bougueng Tchemeube,Jeff Ens,Philippe Pasquier*

Main category: cs.HC

TL;DR: Calliope是一个基于Web的音乐创作辅助工具，支持多轨MIDI编辑与生成，结合MMM模型实现智能创作。


<details>
  <summary>Details</summary>
Motivation: 随着AI在创意领域的应用增加，开发工具辅助音乐创作成为需求。

Method: 用户可上传、编辑MIDI文件，利用MMM模型生成部分或完整多轨内容，支持批量生成与实时播放。

Result: 系统支持MIDI导出或直接流式播放，提升创作效率。

Conclusion: Calliope为音乐创作提供了高效的协同工作流程。

Abstract: With the rise of artificial intelligence in recent years, there has been a
rapid increase in its application towards creative domains, including music.
There exist many systems built that apply machine learning approaches to the
problem of computer-assisted music composition (CAC). Calliope is a web
application that assists users in performing a variety of multi-track
composition tasks in the symbolic domain. The user can upload (Musical
Instrument Digital Interface) MIDI files, visualize and edit MIDI tracks, and
generate partial (via bar in-filling) or complete multi-track content using the
Multi-Track Music Machine (MMM). Generation of new MIDI excerpts can be done in
batch and can be combined with active playback listening for an enhanced
assisted-composition workflow. The user can export generated MIDI materials or
directly stream MIDI playback from the system to their favorite Digital Audio
Workstation (DAW). We present a demonstration of the system, its features,
generative parameters and describe the co-creative workflows that it affords.

</details>


### [527] [Visualization Tasks for Unlabelled Graphs](https://arxiv.org/abs/2504.14115)
*Matt I. B. Oddo,Ryan Smith,Stephen Kobourov,Tamara Munzner*

Main category: cs.HC

TL;DR: 本文研究了无标签图的任务分类，提出了一种基于Scope、Action和Target的抽象任务分类法，并通过实际案例和可视化评估验证了其描述和评估能力。


<details>
  <summary>Details</summary>
Motivation: 现有技术提出了无标签图的可视化方法，但缺乏对这些图任务的深入理解，需要更系统的分类和评估框架。

Method: 提出了一种基于Scope、Action和Target的无标签图任务分类法，并通过连接现有框架和实际案例验证其描述能力，同时评估了6种可视化方法。

Result: 分类法展示了描述和评估能力，初步评估表明不同任务和可视化编码对用户努力和任务成功率有显著影响。

Conclusion: 提出的分类法为无标签图任务提供了系统框架，有助于未来可视化方法的评估和优化。

Abstract: We investigate tasks that can be accomplished with unlabelled graphs, where
nodes do not have persistent or semantically meaningful labels. New techniques
to visualize these graphs have been proposed, but more understanding of
unlabelled graph tasks is required before they can be adequately evaluated.
Some tasks apply to both labelled and unlabelled graphs, but many do not
translate between these contexts. We propose a taxonomy of unlabelled graph
abstract tasks, organized according to the Scope of the data at play, the
Action intended by the user, and the Target data under consideration. We show
the descriptive power of this task abstraction by connecting to concrete
examples from previous frameworks, and connect these abstractions to real-world
problems. To showcase the evaluative power of the taxonomy, we perform a
preliminary assessment of 6 visualizations for each task. For each combination
of task and visual encoding, we consider the effort required from viewers, the
likelihood of task success, and how both factors vary between small-scale and
large-scale graphs.

</details>


### [528] [Expanding the Generative AI Design Space through Structured Prompting and Multimodal Interfaces](https://arxiv.org/abs/2504.14320)
*Nimisha Karnatak,Adrien Baranes,Rob Marchant,Huinan Zeng,Tríona Butler,Kristen Olson*

Main category: cs.HC

TL;DR: 论文探讨了文本提示在生成式AI中的局限性，并提出了一种多模态工具ACAI，通过结构化界面减少提示歧义，帮助新手用户更轻松地生成符合品牌愿景的广告内容。


<details>
  <summary>Details</summary>
Motivation: 文本提示对新手用户（如小企业主）来说存在认知负担和输出通用化的问题，导致广告创意目标难以实现。

Method: 研究通过调查6位英国小企业主，开发了ACAI工具，采用多模态界面（品牌面板、受众与目标面板、灵感面板）来优化提示输入。

Result: ACAI通过结构化界面显著减少了提示歧义，生成的广告内容更符合用户的品牌愿景。

Conclusion: 结构化界面能有效提升生成式AI在新手工作流程中的对齐性和易用性。

Abstract: Text-based prompting remains the dominant interaction paradigm in generative
AI, yet it often results in a high-friction experience for novice users, such
as small business owners (SBOs), attempting to articulate creative or
domain-specific goals for advertising. To investigate this challenge, we
conducted a study with six SBOs in the United Kingdom, focusing on their
advertising practices and perceptions and usage of AI tools in this context.
Our findings surfaced two persistent breakdowns in current generative AI
systems: first, the cognitive burden of prompt engineering, as users struggled
to translate abstract creative goals into effective textual inputs; and second,
the frequent generation of generic outputs that failed to align with users'
articulated brand vision. To address these issues, we developed ACAI (AI
Co-Creation for Advertising and Inspiration), a multimodal, GenAI-powered
advertisement creation tool designed to support novice designers by reimagining
the prompt interface. ACAI features a structured, panel-based interface
composed of three modules: the Branding Panel, the Audience & Goals Panel, and
the Inspiration Board Panel to provide SBOs with outputs that align with their
creative vision by reducing prompt ambiguity. This work contributes to HCI
research on generative systems by showing how structured interfaces can
foreground user-defined context to improve both alignment and promptability in
novice workflows.

</details>


### [529] [ScholarMate: A Mixed-Initiative Tool for Qualitative Knowledge Work and Information Sensemaking](https://arxiv.org/abs/2504.14406)
*Runlong Ye,Patrick Yung Kang Lee,Matthew Varona,Oliver Huang,Carolina Nobre*

Main category: cs.HC

TL;DR: ScholarMate是一个交互式系统，结合AI辅助与人工监督，提升定性分析的效率与可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决在知识工作中，如何有效将AI整合到以人为中心的感知工作流中的挑战。

Method: 通过动态排列和交互文本片段，利用AI提供主题建议、多级摘要和上下文命名，同时确保透明度。

Result: 初步研究表明用户认可这种混合主动方法，案例研究分析了24篇论文。

Conclusion: ScholarMate通过平衡自动化与人工控制，提升了效率并支持可解释性，为人机协作提供了有价值的解决方案。

Abstract: Synthesizing knowledge from large document collections is a critical yet
increasingly complex aspect of qualitative research and knowledge work. While
AI offers automation potential, effectively integrating it into human-centric
sensemaking workflows remains challenging. We present ScholarMate, an
interactive system designed to augment qualitative analysis by unifying AI
assistance with human oversight. ScholarMate enables researchers to dynamically
arrange and interact with text snippets on a non-linear canvas, leveraging AI
for theme suggestions, multi-level summarization, and contextual naming, while
ensuring transparency through traceability to source documents. Initial pilot
studies indicated that users value this mixed-initiative approach, finding the
balance between AI suggestions and direct manipulation crucial for maintaining
interpretability and trust. We further demonstrate the system's capability
through a case study analyzing 24 papers. By balancing automation with human
control, ScholarMate enhances efficiency and supports interpretability,
offering a valuable approach for productive human-AI collaboration in demanding
sensemaking tasks common in knowledge work.

</details>


### [530] [Optimizing SIA Development: A Case Study in User-Centered Design for Estuary, a Multimodal Socially Interactive Agent Framework](https://arxiv.org/abs/2504.14427)
*Spencer Lin,Miru Jun,Basem Rizk,Karen Shieh,Scott Fisher,Sharon Mozgai*

Main category: cs.HC

TL;DR: 本文介绍了基于用户中心设计模型的社交智能体（SIA）开发框架Estuary，并通过快速评估过程（RAP）收集专家意见，评估其填补研究空白的能力。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够填补当前社交智能体开发研究空白的开源多模态框架。

Method: 采用快速评估过程（RAP）进行专家访谈，收集对Estuary框架的评估意见。

Result: 通过访谈发现Estuary在填补研究空白方面具有潜力，并能为未来框架开发提供指导。

Conclusion: Estuary的开发不仅有助于自身完善，也为未来社交智能体技术发展提供了方向。

Abstract: This case study presents our user-centered design model for Socially
Intelligent Agent (SIA) development frameworks through our experience
developing Estuary, an open source multimodal framework for building
low-latency real-time socially interactive agents. We leverage the Rapid
Assessment Process (RAP) to collect the thoughts of leading researchers in the
field of SIAs regarding the current state of the art for SIA development as
well as their evaluation of how well Estuary may potentially address current
research gaps. We achieve this through a series of end-user interviews
conducted by a fellow researcher in the community. We hope that the findings of
our work will not only assist the continued development of Estuary but also
guide the development of other future frameworks and technologies for SIAs.

</details>


### [531] [Biased by Design: Leveraging AI Biases to Enhance Critical Thinking of News Readers](https://arxiv.org/abs/2504.14522)
*Liudmila Zavolokina,Kilian Sprenkamp,Zoya Katashinskaya,Daniel Gordon Jones*

Main category: cs.HC

TL;DR: 本文探讨了利用大型语言模型（LLMs）设计宣传检测工具，研究如何利用AI模型的政治偏见来增强新闻消费中的批判性思维。


<details>
  <summary>Details</summary>
Motivation: 认识到AI模型在政治背景下的固有偏见，研究如何将这些偏见转化为提升用户批判性思维的工具。

Method: 通过用户选择和个性化策略，结合心理学的确认偏误和认知失调理论，进行定性用户研究。

Result: 研究提出了设计建议，包括偏见意识、个性化选择及逐步引入多样化观点。

Conclusion: AI工具在宣传检测中可以有效利用偏见，提升用户的批判性思维和新闻消费体验。

Abstract: This paper explores the design of a propaganda detection tool using Large
Language Models (LLMs). Acknowledging the inherent biases in AI models,
especially in political contexts, we investigate how these biases might be
leveraged to enhance critical thinking in news consumption. Countering the
typical view of AI biases as detrimental, our research proposes strategies of
user choice and personalization in response to a user's political stance,
applying psychological concepts of confirmation bias and cognitive dissonance.
We present findings from a qualitative user study, offering insights and design
recommendations (bias awareness, personalization and choice, and gradual
introduction of diverse perspectives) for AI tools in propaganda detection.

</details>


### [532] [Exploring Collaborative GenAI Agents in Synchronous Group Settings: Eliciting Team Perceptions and Design Considerations for the Future of Work](https://arxiv.org/abs/2504.14779)
*Janet G. Johnson,Macarena Peralta,Mansanjam Kaur,Ruijie Sophia Huang,Sheng Zhao,Ruijia Guan,Shwetha Rajaram,Michael Nebeling*

Main category: cs.HC

TL;DR: 探讨协作式生成AI代理在团队工作中的潜力，通过设计研讨会和访谈发现其能提升团队问题解决能力，但需考虑个体、团队和组织因素。


<details>
  <summary>Details</summary>
Motivation: 研究生成AI在团队协作中的应用，填补现有工具主要针对个人使用的不足。

Method: 通过25名专业人士参与的6个团队的研讨会和访谈，使用混合现实原型模拟协作式生成AI代理。

Result: 协作式生成AI代理能挑战群体思维、弥合沟通差距并减少社交摩擦，但其接受度取决于多因素匹配。

Conclusion: 设计需平衡代理的表现形式、社交显著性和参与度，空间和沉浸式技术可调节AI对团队的影响。

Abstract: While generative artificial intelligence (GenAI) is finding increased
adoption in workplaces, current tools are primarily designed for individual
use. Prior work established the potential for these tools to enhance personal
creativity and productivity towards shared goals; however, we don't know yet
how to best take into account the nuances of group work and team dynamics when
deploying GenAI in work settings. In this paper, we investigate the potential
of collaborative GenAI agents to augment teamwork in synchronous group settings
through an exploratory study that engaged 25 professionals across 6 teams in
speculative design workshops and individual follow-up interviews. Our workshops
included a mixed reality provotype to simulate embodied collaborative GenAI
agents capable of actively participating in group discussions. Our findings
suggest that, if designed well, collaborative GenAI agents offer valuable
opportunities to enhance team problem-solving by challenging groupthink,
bridging communication gaps, and reducing social friction. However, teams'
willingness to integrate GenAI agents depended on its perceived fit across a
number of individual, team, and organizational factors. We outline the key
design tensions around agent representation, social prominence, and engagement
and highlight the opportunities spatial and immersive technologies could offer
to modulate GenAI influence on team outcomes and strike a balance between
augmentation and agency.

</details>


### [533] [NeuGaze: Reshaping the future BCI](https://arxiv.org/abs/2504.15101)
*Yiqian Yang*

Main category: cs.HC

TL;DR: NeuGaze是一种基于普通网络摄像头的系统，通过眼动、头部运动和面部表情实现实时控制，为运动障碍用户提供低成本、易用的脑机接口替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统脑机接口依赖昂贵设备且操作复杂，NeuGaze旨在通过普通网络摄像头提供更便捷、低成本的解决方案。

Method: 利用30Hz网络摄像头捕捉眼动、头部运动和面部表情，结合高效技能轮实现精确控制和动态交互。

Result: 性能接近传统输入方式，支持光标导航、按键触发和游戏交互，适用于运动障碍用户。

Conclusion: NeuGaze为运动障碍用户提供了低成本、易用的交互方式，扩展了脑机接口的应用范围。

Abstract: Traditional brain-computer interfaces (BCIs), reliant on costly
electroencephalography or invasive implants, struggle with complex
human-computer interactions due to setup complexity and limited precision. We
present NeuGaze, a novel webcam-based system that leverages eye gaze, head
movements, and facial expressions to enable intuitive, real-time control using
only a standard 30 Hz webcam, often pre-installed in laptops. Requiring minimal
calibration, NeuGaze achieves performance comparable to conventional inputs,
supporting precise cursor navigation, key triggering via an efficient skill
wheel, and dynamic gaming interactions, such as defeating formidable opponents
in first-person games. By harnessing preserved neck-up functionalities in
motor-impaired individuals, NeuGaze eliminates the need for specialized
hardware, offering a low-cost, accessible alternative to BCIs. This paradigm
empowers diverse applications, from assistive technology to entertainment,
redefining human-computer interaction for motor-impaired users. Project is at
\href{https://github.com/NeuSpeech/NeuGaze}{github.com/NeuSpeech/NeuGaze}.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [534] [Integrating LLM-Generated Views into Mean-Variance Optimization Using the Black-Litterman Model](https://arxiv.org/abs/2504.14345)
*Youngbin Lee,Yejin Kim,Suin Kim,Yongjae Lee*

Main category: q-fin.PM

TL;DR: 研究探索将大型语言模型（LLM）生成的观点整合到Black-Litterman框架中，以优化投资组合，并通过历史数据和公司元数据预测股票收益。


<details>
  <summary>Details</summary>
Motivation: 传统均值-方差模型对输入敏感，而Black-Litterman模型通过整合投资者观点缓解这一问题，但定义这些观点仍具挑战性。

Method: 利用LLM从历史价格和公司元数据中预测股票收益，并通过预测方差量化不确定性，在Black-Litterman框架中进行优化。

Result: 不同LLM表现出不同程度的预测乐观性和置信稳定性，影响投资组合表现。

Conclusion: LLM生成的观点可用于优化投资组合，但需考虑模型的预测特性。

Abstract: Portfolio optimization faces challenges due to the sensitivity in traditional
mean-variance models. The Black-Litterman model mitigates this by integrating
investor views, but defining these views remains difficult. This study explores
the integration of large language models (LLMs) generated views into portfolio
optimization using the Black-Litterman framework. Our method leverages LLMs to
estimate expected stock returns from historical prices and company metadata,
incorporating uncertainty through the variance in predictions. We conduct a
backtest of the LLM-optimized portfolios from June 2024 to February 2025,
rebalancing biweekly using the previous two weeks of price data. As baselines,
we compare against the S&P 500, an equal-weighted portfolio, and a traditional
mean-variance optimized portfolio constructed using the same set of stocks.
Empirical results suggest that different LLMs exhibit varying levels of
predictive optimism and confidence stability, which impact portfolio
performance. The source code and data are available at
https://github.com/youngandbin/LLM-MVO-BLM.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [535] [HF4Rec: Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation](https://arxiv.org/abs/2504.14147)
*Jiakai Tang,Jingsen Zhang,Zihang Tian,Xueyang Feng,Lei Wang,Xu Chen*

Main category: cs.IR

TL;DR: 提出了一种基于人类反馈优化的可解释推荐框架，利用大语言模型模拟人类反馈，结合帕累托优化和离线策略训练提升解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有可解释推荐方法因依赖稀疏交互数据的监督学习，无法提供有效反馈信号。

Method: 提出动态交互优化框架，利用大语言模型模拟人类反馈，引入定制化奖励评分和帕累托优化，设计离线策略训练管道。

Result: 在四个数据集上的实验验证了方法的优越性。

Conclusion: 该框架显著提升了可解释推荐的性能，同时避免了高人力成本。

Abstract: Recent advancements in explainable recommendation have greatly bolstered user
experience by elucidating the decision-making rationale. However, the existing
methods actually fail to provide effective feedback signals for potentially
better or worse generated explanations due to their reliance on traditional
supervised learning paradigms in sparse interaction data. To address these
issues, we propose a novel human-like feedback-driven optimization framework.
This framework employs a dynamic interactive optimization mechanism for
achieving human-centered explainable requirements without incurring high labor
costs. Specifically, we propose to utilize large language models (LLMs) as
human simulators to predict human-like feedback for guiding the learning
process. To enable the LLMs to deeply understand the task essence and meet
user's diverse personalized requirements, we introduce a human-induced
customized reward scoring method, which helps stimulate the language
understanding and logical reasoning capabilities of LLMs. Furthermore,
considering the potential conflicts between different perspectives of
explanation quality, we introduce a principled Pareto optimization that
transforms the multi-perspective quality enhancement task into a
multi-objective optimization problem for improving explanation performance. At
last, to achieve efficient model training, we design an off-policy optimization
pipeline. By incorporating a replay buffer and addressing the data distribution
biases, we can effectively improve data utilization and enhance model
generality. Extensive experiments on four datasets demonstrate the superiority
of our approach.

</details>


### [536] [CPR: Leveraging LLMs for Topic and Phrase Suggestion to Facilitate Comprehensive Product Reviews](https://arxiv.org/abs/2504.13993)
*Ekta Gujral,Apurva Sinha,Lishi Ji,Bijayani Sanghamitra Mishra*

Main category: cs.IR

TL;DR: 本文提出了一种名为CPR的新方法，利用大型语言模型（LLMs）和主题建模，引导用户撰写全面且有洞察力的产品评论。该方法通过三个阶段实现，显著提升了评论质量。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能系统性地鼓励用户撰写既包含情感又详细分析产品特征的评论。

Method: CPR采用三阶段流程：1）提供产品特定评分项；2）基于评分生成针对性短语建议；3）通过主题建模整合用户文本。

Result: 实验表明，CPR能有效识别相关产品术语并提供情感一致的短语建议，BLEU分数比基线方法提升12.3%。

Conclusion: CPR在提升评论质量方面表现优异，未来可进一步扩展研究方向。

Abstract: Consumers often heavily rely on online product reviews, analyzing both
quantitative ratings and textual descriptions to assess product quality.
However, existing research hasn't adequately addressed how to systematically
encourage the creation of comprehensive reviews that capture both customers
sentiment and detailed product feature analysis. This paper presents CPR, a
novel methodology that leverages the power of Large Language Models (LLMs) and
Topic Modeling to guide users in crafting insightful and well-rounded reviews.
Our approach employs a three-stage process: first, we present users with
product-specific terms for rating; second, we generate targeted phrase
suggestions based on these ratings; and third, we integrate user-written text
through topic modeling, ensuring all key aspects are addressed. We evaluate CPR
using text-to-text LLMs, comparing its performance against real-world customer
reviews from Walmart. Our results demonstrate that CPR effectively identifies
relevant product terms, even for new products lacking prior reviews, and
provides sentiment-aligned phrase suggestions, saving users time and enhancing
reviews quality. Quantitative analysis reveals a 12.3% improvement in BLEU
score over baseline methods, further supported by manual evaluation of
generated phrases. We conclude by discussing potential extensions and future
research directions.

</details>


### [537] [The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models](https://arxiv.org/abs/2504.15068)
*Ronak Pradeep,Nandan Thakur,Shivani Upadhyay,Daniel Campos,Nick Craswell,Jimmy Lin*

Main category: cs.IR

TL;DR: 提出了一种基于LLM的自动评估框架AutoNuggetizer，用于评估RAG系统，验证了其与人工标注的一致性。


<details>
  <summary>Details</summary>
Motivation: RAG系统的评估是当前研究的瓶颈，需要一种自动化的方法以提高效率和可扩展性。

Method: 采用TREC QA Track的nugget评估方法，通过LLM自动生成和分配nuggets，并与人工方法对比。

Result: 自动评估与人工评估在运行级别上表现出强一致性，尤其在独立自动化组件时效果更佳。

Conclusion: 该框架在效率和质量间提供了平衡，但需进一步研究以优化每主题一致性。

Abstract: Large Language Models (LLMs) have significantly enhanced the capabilities of
information access systems, especially with retrieval-augmented generation
(RAG). Nevertheless, the evaluation of RAG systems remains a barrier to
continued progress, a challenge we tackle in this work by proposing an
automatic evaluation framework that is validated against human annotations. We
believe that the nugget evaluation methodology provides a solid foundation for
evaluating RAG systems. This approach, originally developed for the TREC
Question Answering (QA) Track in 2003, evaluates systems based on atomic facts
that should be present in good answers. Our efforts focus on "refactoring" this
methodology, where we describe the AutoNuggetizer framework that specifically
applies LLMs to both automatically create nuggets and automatically assign
nuggets to system answers. In the context of the TREC 2024 RAG Track, we
calibrate a fully automatic approach against strategies where nuggets are
created manually or semi-manually by human assessors and then assigned manually
to system answers. Based on results from a community-wide evaluation, we
observe strong agreement at the run level between scores derived from fully
automatic nugget evaluation and human-based variants. The agreement is stronger
when individual framework components such as nugget assignment are automated
independently. This suggests that our evaluation framework provides tradeoffs
between effort and quality that can be used to guide the development of future
RAG systems. However, further research is necessary to refine our approach,
particularly in establishing robust per-topic agreement to diagnose system
failures effectively.

</details>


### [538] [KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking](https://arxiv.org/abs/2504.15135)
*Juyeon Kim,Geon Lee,Taeuk Kim,Kijung Shin*

Main category: cs.IR

TL;DR: KGMEL是一个利用知识图谱三元组增强多模态实体链接的框架，通过生成、检索和重排名三个阶段显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视知识图谱的结构信息，而KGMEL通过结合文本、图像和三元组信息减少歧义。

Method: KGMEL分三个阶段：生成高质量三元组、学习联合表示检索候选实体、重排名优化匹配结果。

Result: 实验表明KGMEL在基准数据集上优于现有方法。

Conclusion: KGMEL通过整合知识图谱信息显著提升多模态实体链接的准确性。

Abstract: Entity linking (EL) aligns textual mentions with their corresponding entities
in a knowledge base, facilitating various applications such as semantic search
and question answering. Recent advances in multimodal entity linking (MEL) have
shown that combining text and images can reduce ambiguity and improve alignment
accuracy. However, most existing MEL methods overlook the rich structural
information available in the form of knowledge-graph (KG) triples. In this
paper, we propose KGMEL, a novel framework that leverages KG triples to enhance
MEL. Specifically, it operates in three stages: (1) Generation: Produces
high-quality triples for each mention by employing vision-language models based
on its text and images. (2) Retrieval: Learns joint mention-entity
representations, via contrastive learning, that integrate text, images, and
(generated or KG) triples to retrieve candidate entities for each mention. (3)
Reranking: Refines the KG triples of the candidate entities and employs large
language models to identify the best-matching entity for the mention. Extensive
experiments on benchmark datasets demonstrate that KGMEL outperforms existing
methods. Our code and datasets are available at:
https://github.com/juyeonnn/KGMEL.

</details>


### [539] [Personalized News Recommendation with Multi-granularity Candidate-aware User Modeling](https://arxiv.org/abs/2504.14130)
*Qiang Li,Xinze Lin,Shenghao Lv,Faliang Huang,Xiangju Li*

Main category: cs.IR

TL;DR: 本文提出了一种多粒度候选感知的用户建模框架，用于个性化新闻推荐，通过多粒度特征捕捉用户兴趣的多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常基于单一用户画像匹配新闻，难以全面捕捉用户兴趣的多样性，且忽视了候选新闻与用户兴趣的多粒度关联。

Method: 提出多粒度候选感知用户建模框架，包括候选新闻编码和用户建模两部分，结合文本和知识增强的实体信息提取器，以及多粒度候选感知机制。

Result: 在真实数据集上的实验表明，该模型显著优于基线模型。

Conclusion: 多粒度候选感知框架能更全面地表示用户兴趣，提升新闻推荐效果。

Abstract: Matching candidate news with user interests is crucial for personalized news
recommendations. Most existing methods can represent a user's reading interests
through a single profile based on clicked news, which may not fully capture the
diversity of user interests. Although some approaches incorporate candidate
news or topic information, they remain insufficient because they neglect the
multi-granularity relatedness between candidate news and user interests. To
address this, this study proposed a multi-granularity candidate-aware user
modeling framework that integrated user interest features across various levels
of granularity. It consisted of two main components: candidate news encoding
and user modeling. A news textual information extractor and a
knowledge-enhanced entity information extractor can capture candidate news
features, and word-level, entity-level, and news-level candidate-aware
mechanisms can provide a comprehensive representation of user interests.
Extensive experiments on a real-world dataset demonstrated that the proposed
model could significantly outperform baseline models.

</details>


### [540] [FinSage: A Multi-aspect RAG System for Financial Filings Question Answering](https://arxiv.org/abs/2504.14493)
*Xinyu Wang,Jijun Chi,Zhenghan Tai,Tung Sum Thomas Kwok,Muzhi Li,Zhuhong Li,Hailin He,Yuchen Hua,Peng Lu,Suyuchen Wang,Yihong Wu,Jerry Huang,Ling Zhou*

Main category: cs.IR

TL;DR: FinSage框架通过多模态RAG系统解决金融文档中的合规性问题，显著提升信息提取准确率。


<details>
  <summary>Details</summary>
Motivation: 金融领域需处理多模态数据和动态法规，现有方法难以满足需求。

Method: FinSage包含多模态预处理、多路径检索系统和领域专用重排序模块。

Result: 在FinanceBench数据集上准确率提升24.06%，召回率达92.51%。

Conclusion: FinSage成功应用于实际场景，验证了其高效性和实用性。

Abstract: Leveraging large language models in real-world settings often entails a need
to utilize domain-specific data and tools in order to follow the complex
regulations that need to be followed for acceptable use. Within financial
sectors, modern enterprises increasingly rely on Retrieval-Augmented Generation
(RAG) systems to address complex compliance requirements in financial document
workflows. However, existing solutions struggle to account for the inherent
heterogeneity of data (e.g., text, tables, diagrams) and evolving nature of
regulatory standards used in financial filings, leading to compromised accuracy
in critical information extraction. We propose the FinSage framework as a
solution, utilizing a multi-aspect RAG framework tailored for regulatory
compliance analysis in multi-modal financial documents. FinSage introduces
three innovative components: (1) a multi-modal pre-processing pipeline that
unifies diverse data formats and generates chunk-level metadata summaries, (2)
a multi-path sparse-dense retrieval system augmented with query expansion
(HyDE) and metadata-aware semantic search, and (3) a domain-specialized
re-ranking module fine-tuned via Direct Preference Optimization (DPO) to
prioritize compliance-critical content. Extensive experiments demonstrate that
FinSage achieves an impressive recall of 92.51% on 75 expert-curated questions
derived from surpasses the best baseline method on the FinanceBench question
answering datasets by 24.06% in accuracy. Moreover, FinSage has been
successfully deployed as financial question-answering agent in online meetings,
where it has already served more than 1,200 people.

</details>


### [541] [Exploring $\ell_0$ Sparsification for Inference-free Sparse Retrievers](https://arxiv.org/abs/2504.14839)
*Xinjie Shen,Zhichao Geng,Yang Yang*

Main category: cs.IR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: With increasing demands for efficiency, information retrieval has developed a
branch of sparse retrieval, further advancing towards inference-free retrieval
where the documents are encoded during indexing time and there is no
model-inference for queries. Existing sparse retrieval models rely on FLOPS
regularization for sparsification, while this mechanism was originally designed
for Siamese encoders, it is considered to be suboptimal in inference-free
scenarios which is asymmetric. Previous attempts to adapt FLOPS for
inference-free scenarios have been limited to rule-based methods, leaving the
potential of sparsification approaches for inference-free retrieval models
largely unexplored. In this paper, we explore $\ell_0$ inspired sparsification
manner for inference-free retrievers. Through comprehensive out-of-domain
evaluation on the BEIR benchmark, our method achieves state-of-the-art
performance among inference-free sparse retrieval models and is comparable to
leading Siamese sparse retrieval models. Furthermore, we provide insights into
the trade-off between retrieval effectiveness and computational efficiency,
demonstrating practical value for real-world applications.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [542] [Association between nutritional factors, inflammatory biomarkers and cancer types: an analysis of NHANES data using machine learning](https://arxiv.org/abs/2504.13978)
*Yuqing Liu,Meng Zhao,Guanlan Hu,Yuchen Zhang*

Main category: q-bio.QM

TL;DR: 研究探讨了营养状态与炎症标志物对癌症风险的联合影响，利用机器学习分析NHANES数据，发现营养摄入质量和炎症水平与癌症状态相关。


<details>
  <summary>Details</summary>
Motivation: 饮食和炎症是影响癌症风险的关键因素，但结合营养状态和炎症标志物通过机器学习分析的研究较少。

Method: 分析了26,409名NHANES参与者的24种营养素、CRP和ALI，使用多变量逻辑回归和三种机器学习模型（逻辑回归、随机森林、XGBoost）评估预测价值。

Result: 营养因素（如蛋白质和维生素）和炎症标志物是癌症状态的关键预测因子，随机森林模型表现最佳，准确率达0.72。

Conclusion: 高质量营养摄入和低炎症水平可能对癌症有保护作用，结合营养和炎症标志物的机器学习方法有助于癌症预防策略。

Abstract: Background. Diet and inflammation are critical factors influencing cancer
risk. However, the combined impact of nutritional status and inflammatory
biomarkers on cancer status and type, using machine learning (ML), remains
underexplored.
  Objectives. This study investigates the association between nutritional
factors, inflammatory biomarkers, and cancer status, and whether these
relationships differ across cancer types using National Health and Nutrition
Examination Survey (NHANES) data.
  Methods. We analyzed 24 macro- and micronutrients, C-reactive protein (CRP),
and the advanced lung cancer inflammation index (ALI) in 26,409 NHANES
participants (2,120 with cancer). Multivariable logistic regression assessed
associations with cancer prevalence. We also examined whether these features
differed across the five most common cancer types. To evaluate predictive
value, we applied three ML models - Logistic Regression, Random Forest, and
XGBoost - on the full feature set.
  Results. The cohort's mean age was 49.1 years; 34.7% were obese.
Comorbidities such as anemia and liver conditions, along with nutritional
factors like protein and several vitamins, were key predictors of cancer
status. Among the models, Random Forest performed best, achieving an accuracy
of 0.72.
  Conclusions. Higher-quality nutritional intake and lower levels of
inflammation may offer protective effects against cancer. These findings
highlight the potential of combining nutritional and inflammatory markers with
ML to inform cancer prevention strategies.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [543] [Breaking the Diffraction Barrier for Passive Sources: Parameter-Decoupled Superresolution Assisted by Physics-Informed Machine Learning](https://arxiv.org/abs/2504.14156)
*Abdelali Sajia,Bilal Benzimoun,Pawan Khatiwada,Guogan Zhao,Xiao-Feng Qian*

Main category: physics.optics

TL;DR: 提出了一种参数解耦的超分辨率框架，用于估计被动两点源的亚波长间距，无需源先验知识或控制。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法中需要估计多个复杂参数（如部分相干性、亮度不平衡、随机相对相位和光子统计）的问题，并在实际应用中应对背景噪声、光子损失和对齐误差等挑战。

Method: 结合物理理论构建的机器学习模型，通过参数解耦实现超分辨率。

Result: 在实验生成的图像上，分辨率达到衍射极限的14倍以下（约13.5纳米），保真度超过82%，性能媲美主动控制源的最先进技术。

Conclusion: 该方法在源参数可变性和源无关噪声下表现稳健，适用于无法控制源的实际场景，如天体成像、活细胞显微镜和量子计量，填补了被动系统超分辨率理论与实际应用之间的关键空白。

Abstract: We present a parameter-decoupled superresolution framework for estimating
sub-wavelength separations of passive two-point sources without requiring prior
knowledge or control of the source. Our theoretical foundation circumvents the
need to estimate multiple challenging parameters such as partial coherence,
brightness imbalance, random relative phase, and photon statistics. A
physics-informed machine learning (ML) model (trained with a standard desktop
workstation), synergistically integrating this theory, further addresses
practical imperfections including background noise, photon loss, and
centroid/orientation misalignment. The integrated parameter-decoupling
superresolution method achieves resolution 14 and more times below the
diffraction limit (corresponding to ~ 13.5 nm in optical microscopy) on
experimentally generated realistic images with >82% fidelity, performance
rivaling state-of-the-art techniques for actively controllable sources.
Critically, our method's robustness against source parameter variability and
source-independent noises enables potential applications in realistic scenarios
where source control is infeasible, such as astrophysical imaging, live-cell
microscopy, and quantum metrology. This work bridges a critical gap between
theoretical superresolution limits and practical implementations for passive
systems.

</details>


### [544] [DeepPD: Joint Phase and Object Estimation from Phase Diversity with Neural Calibration of a Deformable Mirror](https://arxiv.org/abs/2504.14157)
*Magdalena C. Schneider,Courtney Johnson,Cedric Allier,Larissa Heinrich,Diane Adjavon,Joren Husic,Patrick La Rivière,Stephan Saalfeld,Hari Shroff*

Main category: physics.optics

TL;DR: DeepPD利用深度学习框架，通过仅需五张图像联合估计物体和相位，克服了传统相位多样性方法的局限性，提高了重建质量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 样本诱导的像差和光学缺陷限制了荧光显微镜的分辨率，现有相位多样性方法存在Zernike模式限制、需大量图像或依赖精确校准的问题。

Method: 结合物体和波前的神经表示与变形镜的模型，通过深度学习联合估计物体和相位。

Result: DeepPD在严重像差下仍优于现有方法，提高了重建质量和鲁棒性，并在校准目标和生物样本中验证了性能。

Conclusion: DeepPD为相位多样性提供了一种高效、鲁棒的解决方案，适用于复杂像差场景。

Abstract: Sample-induced aberrations and optical imperfections limit the resolution of
fluorescence microscopy. Phase diversity is a powerful technique that leverages
complementary phase information in sequentially acquired images with
deliberately introduced aberrations--the phase diversities--to enable phase and
object reconstruction and restore diffraction-limited resolution. These phase
diversities are typically introduced into the optical path via a deformable
mirror. Existing phase-diversity-based methods are limited to Zernike modes,
require large numbers of diversity images, or depend on accurate mirror
calibration--which are all suboptimal. We present DeepPD, a deep learning-based
framework that combines neural representations of the object and wavefront with
a learned model of the deformable mirror to jointly estimate both object and
phase from only five images. DeepPD improves robustness and reconstruction
quality over previous approaches, even under severe aberrations. We demonstrate
its performance on calibration targets and biological samples, including
immunolabeled myosin in fixed PtK2 cells.

</details>


### [545] [Beyond Terabit/s Integrated Neuromorphic Photonic Processor for DSP-Free Optical Interconnects](https://arxiv.org/abs/2504.15044)
*Benshan Wang,Qiarong Xiao,Tengji Xu,Li Fan,Shaojie Liu,Jianji Dong,Junwen Zhang,Chaoran Huang*

Main category: physics.optics

TL;DR: 提出一种基于神经形态光学信号处理器（OSP）的新型解决方案，用于高性能AI训练和推理中的光互连，显著提升速度、降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 传统电学和光学互连技术无法满足大规模AI训练对超低延迟、高能效和统一性能的需求，亟需创新解决方案。

Method: 采用深度储备计算技术，开发了一种无需数字信号处理器（DSP）的全光学实时处理器（OSP），支持多种调制格式和数据速率。

Result: 实验显示，OSP在5公里光纤上实现100 Gbaud PAM4每通道、1.6 Tbit/s的传输速率，延迟降低4个数量级，能耗减少3个数量级。

Conclusion: OSP为下一代AI基础设施提供了高度可扩展、高能效和高速的解决方案，具有广泛的应用潜力。

Abstract: The rapid expansion of generative AI drives unprecedented demands for
high-performance computing. Training large-scale AI models now requires vast
interconnected GPU clusters across multiple data centers. Multi-scale AI
training and inference demand uniform, ultra-low latency, and energy-efficient
links to enable massive GPUs to function as a single cohesive unit. However,
traditional electrical and optical interconnects, relying on conventional
digital signal processors (DSPs) for signal distortion compensation,
increasingly fail to meet these stringent requirements. To overcome these
limitations, we present an integrated neuromorphic optical signal processor
(OSP) that leverages deep reservoir computing and achieves DSP-free,
all-optical, real-time processing. Experimentally, our OSP achieves a 100 Gbaud
PAM4 per lane, 1.6 Tbit/s data center interconnect over a 5 km optical fiber in
the C-band (equivalent to over 80 km in the O-band), far exceeding the reach of
state-of-the-art DSP solutions, which are fundamentally constrained by
chromatic dispersion in IMDD systems. Simultaneously, it reduces processing
latency by four orders of magnitude and energy consumption by three orders of
magnitude. Unlike DSPs, which introduce increased latency at high data rates,
our OSP maintains consistent, ultra-low latency regardless of data rate
scaling, making it ideal for future optical interconnects. Moreover, the OSP
retains full optical field information for better impairment compensation and
adapts to various modulation formats, data rates, and wavelengths. Fabricated
using a mature silicon photonic process, the OSP can be monolithically
integrated with silicon photonic transceivers, enhancing the compactness and
reliability of all-optical interconnects. This research provides a highly
scalable, energy-efficient, and high-speed solution, paving the way for
next-generation AI infrastructure.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [546] [System of Agentic AI for the Discovery of Metal-Organic Frameworks](https://arxiv.org/abs/2504.14110)
*Theo Jaffrelot Inizan,Sherry Yang,Aaron Kaplan,Yen-hsu Lin,Jian Yin,Saber Mirzaei,Mona Abdelgaid,Ali H. Alawadhi,KwangHwan Cho,Zhiling Zheng,Ekin Dogus Cubuk,Christian Borgs,Jennifer T. Chayes,Kristin A. Persson,Omar M. Yaghi*

Main category: cond-mat.mtrl-sci

TL;DR: MOFGen是一个由多个AI代理组成的系统，用于生成新型MOF材料，并通过实验验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 解决在CO2捕获和水收集领域，生成模型和机器学习在探索广阔化学空间时面临的合成可行性挑战。

Method: MOFGen结合了大型语言模型、扩散模型、量子力学代理和合成可行性代理，生成并优化MOF结构和有机连接体。

Result: 生成了数十万种新型MOF结构，并通过实验成功合成了五种AI设计的MOF。

Conclusion: MOFGen为自动化合成材料发现迈出了重要一步。

Abstract: Generative models and machine learning promise accelerated material discovery
in MOFs for CO2 capture and water harvesting but face significant challenges
navigating vast chemical spaces while ensuring synthetizability. Here, we
present MOFGen, a system of Agentic AI comprising interconnected agents: a
large language model that proposes novel MOF compositions, a diffusion model
that generates crystal structures, quantum mechanical agents that optimize and
filter candidates, and synthetic-feasibility agents guided by expert rules and
machine learning. Trained on all experimentally reported MOFs and computational
databases, MOFGen generated hundreds of thousands of novel MOF structures and
synthesizable organic linkers. Our methodology was validated through
high-throughput experiments and the successful synthesis of five "AI-dreamt"
MOFs, representing a major step toward automated synthesizable material
discovery.

</details>


### [547] [Machine learning enhanced atom probe tomography analysis: a snapshot review](https://arxiv.org/abs/2504.14378)
*Yue Li,Ye Wei,Alaukik Saxena,Markus Kühbach,Christoph Freysoldt,Baptiste Gault*

Main category: cond-mat.mtrl-sci

TL;DR: 本文综述了原子探针层析成像（APT）数据处理的现状，重点介绍了机器学习（ML）在提高效率、可重复性和统计稳健性方面的应用。


<details>
  <summary>Details</summary>
Motivation: APT数据的分析长期依赖用户经验，导致偏差和效率低下，亟需标准化和自动化方法。

Method: 综述了APT的基本原理、数据特性，以及ML算法的应用，包括用户独立性和统计稳健性。

Result: ML方法显著提升了APT数据处理的效率和可重复性，并可能发现超出人类能力的新机制。

Conclusion: 未来研究方向应聚焦于ML在APT中的进一步应用，以实现更高效的标准化和发现。

Abstract: Atom probe tomography (APT) is a burgeoning characterization technique that
provides compositional mapping of materials in three-dimensions at near-atomic
scale. Since its significant expansion in the past 30 years, we estimate that
one million APT datasets have been collected, each containing millions to
billions of individual ions. Their analysis and the extraction of
microstructural information has largely relied upon individual users whose
varied level of expertise causes clear and documented bias. Current practices
hinder efficient data processing, and make challenging standardization and the
deployment of data analysis workflows that would be compliant with FAIR data
principles. Over the past decade, building upon the long-standing expertise of
the APT community in the development of advanced data processing or data mining
techniques, there has been a surge of novel machine learning (ML) approaches
aiming for user-independence, and that are efficient, reproducible, and robust
from a statistics perspective. Here, we provide a snapshot review of this
rapidly evolving field. We begin with a brief introduction to APT and the
nature of the APT data. This is followed by an overview of relevant ML
algorithms and a comprehensive review of their applications to APT. We also
discuss how ML can enable discoveries beyond human capability, offering new
insights into the mechanisms within materials. Finally, we provide guidance for
future directions in this domain.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [548] [On the redundancy of short and heterogeneous sequences of belief revisions](https://arxiv.org/abs/2504.13986)
*Paolo Liberatore*

Main category: cs.CC

TL;DR: 论文研究了特定信念修订事件的遗忘是否完全消除信息，证明了某些情况下该问题具有计算复杂性，并提出了多项式算法。


<details>
  <summary>Details</summary>
Motivation: 探讨信念修订中遗忘事件对信息保留的影响，分析其计算复杂性。

Method: 通过理论证明和算法设计，分析了不同修订序列（如词典序修订和Horn修订）的计算复杂性。

Result: 证明了某些修订序列的遗忘问题为coNP-hard或Delta2类，并提出了针对特定情况的多项式算法。

Conclusion: 信念修订的遗忘问题具有较高的计算复杂性，但在特定条件下可高效解决。

Abstract: Forgetting a specific belief revision episode may not erase information
because the other revisions may provide the same information or allow to deduce
it. Whether it does was proved coNP-hard for sequence of two arbitrary
lexicographic revision or arbitrarily long lexicographic Horn revision. A
polynomial algorithm is presented for the case of two Horn revision.
Heterogeneous sequences of revisions were proved to belong in Delta2. Their
previously proved coNP-hardness is enhanced by a proof of NP-hardness.

</details>
