<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 40]
- [cs.CV](#cs.CV) [Total: 64]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.LG](#cs.LG) [Total: 51]
- [math.ST](#math.ST) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CR](#cs.CR) [Total: 7]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.SE](#cs.SE) [Total: 9]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 14]
- [cs.DB](#cs.DB) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.LO](#cs.LO) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [econ.GN](#econ.GN) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Exploring Compositional Generalization (in ReCOGS_pos) by Transformers using Restricted Access Sequence Processing (RASP)](https://arxiv.org/abs/2504.15349)
*William Bruns*

Main category: cs.CL

TL;DR: 论文通过RASP语言证明Transformer模型可以在ReCOGS任务中实现100%语义匹配，表明任务无需层次化解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型在组合泛化任务（如COGS）中的表现，尤其是其能否通过非层次化方法实现语义匹配。

Method: 使用RASP编程语言构建Transformer等效模型，采用词级标记和19条注意力头兼容的扁平模式匹配规则。

Result: 模型在ReCOGS测试集上实现100%语义匹配，除obj_pp_to_subj_pp外所有泛化分割均达100%。

Conclusion: ReCOGS任务可通过非层次化方法解决，无需递归或树状规则。

Abstract: Humans understand new combinations of words encountered if they are
combinations of words recognized from different contexts, an ability called
Compositional Generalization. The COGS benchmark (Kim and Linzen, 2020)
arXiv:2010.05465 reports 0% accuracy for Transformer models on some structural
generalizations. We use (Weiss et al., 2021) arXiv:2106.06981's Restricted
Access Sequence Processing (RASP), a Transformer-equivalent programming
language, to prove by construction that a Transformer encoder-decoder can
perform the semantically equivalent ReCOGS_pos (Wu et al., 2024)
arXiv:2303.13716 variant of COGS systematically and compositionally: Our RASP
model attains 100% semantic exact match on the ReCOGS test set and 100% SEM on
all generalization splits except obj_pp_to_subj_pp which gets 92%. Furthermore,
our RASP model shows the ReCOGS_pos task does not require a hierarchical or
tree-structured solution: we use word-level tokens with an "embedding" layer
that tags with possible parts of speech, applying just once per encoder pass 19
attention-head compatible flat pattern-matching rules, shown using grammar
coverage (Zeller et al., 2023) to be learnable from the training data, plus
general prepositional phrase (pp) handling and sentential complement (cp)
handling logic, and output the next logical form (LF) token (repeating until
the LF is complete). The model does not apply recursive, tree-structured rules
like 'np_det pp np -> np_pp -> np', but scores 100% semantic and string exact
match on pp recursion, cp recursion using the decoder loop.

</details>


### [2] [Tell Me What You Know About Sexism: Expert-LLM Interaction Strategies and Co-Created Definitions for Zero-Shot Sexism Detection](https://arxiv.org/abs/2504.15392)
*Myrthe Reuver,Indira Sen,Matteo Melis,Gabriella Lapesa*

Main category: cs.CL

TL;DR: 论文研究了性别歧视研究者与大型语言模型（LLMs）的协作，通过四步流程评估LLMs在性别歧视研究中的适用性。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在性别歧视研究中的潜力，以及专家与LLMs协作的效果。

Method: 1. 专家回答问题；2. 参与两个实验（评估LLMs知识和共同定义性别歧视）；3. 零样本分类实验。

Result: LLM生成的定义更长且复杂，专家定义表现较差，但部分专家通过协作定义提升了分类性能。

Conclusion: LLMs在性别歧视研究中有潜力，专家协作可改善模型表现，尤其是对LLMs经验不足的专家。

Abstract: This paper investigates hybrid intelligence and collaboration between
researchers of sexism and Large Language Models (LLMs), with a four-component
pipeline. First, nine sexism researchers answer questions about their knowledge
of sexism and of LLMs. They then participate in two interactive experiments
involving an LLM (GPT3.5). The first experiment has experts assessing the
model's knowledge about sexism and suitability for use in research. The second
experiment tasks them with creating three different definitions of sexism: an
expert-written definition, an LLM-written one, and a co-created definition.
Lastly, zero-shot classification experiments use the three definitions from
each expert in a prompt template for sexism detection, evaluating GPT4o on
2.500 texts sampled from five sexism benchmarks. We then analyze the resulting
67.500 classification decisions. The LLM interactions lead to longer and more
complex definitions of sexism. Expert-written definitions on average perform
poorly compared to LLM-generated definitions. However, some experts do improve
classification performance with their co-created definitions of sexism, also
experts who are inexperienced in using LLMs.

</details>


### [3] [Trillion 7B Technical Report](https://arxiv.org/abs/2504.15431)
*Sungjun Han,Juyoung Suk,Suyeong An,Hyungguk Kim,Kyuseok Kim,Wonsuk Yang,Seungtaek Choi,Jamin Shin*

Main category: cs.CL

TL;DR: Trillion-7B是一种高效的韩语为中心的多语言大模型，通过XLDA机制实现英语到目标语言的高效知识迁移。


<details>
  <summary>Details</summary>
Motivation: 解决多语言模型中知识迁移效率低的问题，特别是针对韩语和日语等目标语言。

Method: 采用Cross-lingual Document Attention (XLDA)机制，结合优化数据混合、语言特定过滤和定制分词器。

Result: 在27个基准测试中表现优异，仅用10%的多语言数据和59.4K GPU小时完成训练。

Conclusion: Trillion-7B在多语言性能和跨语言一致性上表现出色，具有高效性和成本优势。

Abstract: We introduce Trillion-7B, the most token-efficient Korean-centric
multilingual LLM available. Our novel Cross-lingual Document Attention (XLDA)
mechanism enables highly efficient and effective knowledge transfer from
English to target languages like Korean and Japanese. Combined with optimized
data mixtures, language-specific filtering, and tailored tokenizer
construction, Trillion-7B achieves competitive performance while dedicating
only 10\% of its 2T training tokens to multilingual data and requiring just
59.4K H100 GPU hours (\$148K) for full training. Comprehensive evaluations
across 27 benchmarks in four languages demonstrate Trillion-7B's robust
multilingual performance and exceptional cross-lingual consistency.

</details>


### [4] [Feeding LLM Annotations to BERT Classifiers at Your Own Risk](https://arxiv.org/abs/2504.15432)
*Yucheng Lu,Kazimier Smith*

Main category: cs.CL

TL;DR: 使用LLM生成的标签微调小型编码器模型在文本分类中流行，但实证分析显示合成数据的训练会导致性能下降、不稳定性和过早性能停滞，需谨慎应用于高风险任务。


<details>
  <summary>Details</summary>
Motivation: 探讨在文本分类中使用LLM生成标签微调小型模型的可靠性，尤其是在高风险应用中。

Method: 通过实证分析比较使用LLM生成标签和黄金标签训练的模型性能，观察准确性、F1分数、稳定性和性能停滞现象。

Result: 发现LLM生成标签导致性能下降、训练不稳定和过早性能停滞，需通过熵过滤和集成技术部分缓解。

Conclusion: LLM生成标签的传播非随机错误风险未完全解决，需在高风险任务中谨慎应用。

Abstract: Using LLM-generated labels to fine-tune smaller encoder-only models for text
classification has gained popularity in various settings. While this approach
may be justified in simple and low-stakes applications, we conduct empirical
analysis to demonstrate how the perennial curse of training on synthetic data
manifests itself in this specific setup. Compared to models trained on gold
labels, we observe not only the expected performance degradation in accuracy
and F1 score, but also increased instability across training runs and premature
performance plateaus. These findings cast doubts on the reliability of such
approaches in real-world applications. We contextualize the observed phenomena
through the lens of error propagation and offer several practical mitigation
strategies, including entropy-based filtering and ensemble techniques. Although
these heuristics offer partial relief, they do not fully resolve the inherent
risks of propagating non-random errors from LLM annotations to smaller
classifiers, underscoring the need for caution when applying this workflow in
high-stakes text classification tasks.

</details>


### [5] [Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models](https://arxiv.org/abs/2504.15471)
*Tyler A. Chang,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 研究发现Transformer语言模型中存在专注于二元预测的子网络，这些子网络虽仅占模型参数的极小部分，但对性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型中从当前词嵌入到下一个词预测的最小化转换机制。

Method: 识别并分析语言模型中专注于二元预测的子网络，研究其分布和功能。

Result: 二元子网络在1B参数模型中存在，集中于第一层MLP，对性能关键且与最优剪枝子网络重叠。

Conclusion: 二元子网络是语言模型基本预测的最小必要参数集，为研究模型电路提供了新方法。

Abstract: In Transformer language models, activation vectors transform from current
token embeddings to next token predictions as they pass through the model. To
isolate a minimal form of this transformation, we identify language model
subnetworks that make bigram predictions, naive next token predictions based
only on the current token. We find that bigram subnetworks can be found in
fully trained language models up to 1B parameters, and these subnetworks are
critical for model performance even when they consist of less than 0.2% of
model parameters. Bigram subnetworks are concentrated in the first Transformer
MLP layer, and they overlap significantly with subnetworks trained to optimally
prune a given model. Mechanistically, the bigram subnetworks often recreate a
pattern from the full models where the first layer induces a sharp change that
aligns activations with next token predictions rather than current token
representations. Our results demonstrate that bigram subnetworks comprise a
minimal subset of parameters that are both necessary and sufficient for basic
next token predictions in language models, and they help drive the
transformation from current to next token activations in the residual stream.
These subnetworks can lay a foundation for studying language model circuits by
building up from a minimal circuit rather than the traditional approach of
ablating circuits from a full model.

</details>


### [6] [Speculative Sampling via Exponential Races](https://arxiv.org/abs/2504.15475)
*Szymon Kobus,Deniz Gündüz*

Main category: cs.CL

TL;DR: 论文通过将推测解码与信道模拟联系起来，提出了信息论分析，推导了生成速度与草稿模型生成令牌数的关系，并提出了一种新的ERSD方法。


<details>
  <summary>Details</summary>
Motivation: 探讨推测解码与信道模拟的关联，以信息论角度分析推测解码的加速潜力。

Method: 通过信息论分析推导速度上限，并提出基于指数竞赛的ERSD方法。

Result: 建立了生成速度与令牌数的显式关系，ERSD方法达到最新性能。

Conclusion: 推测解码与信道模拟的联系为加速提供了理论支持，ERSD方法具有实际应用潜力。

Abstract: Speculative decoding accelerates large language model inference using a
smaller draft model. In this paper, we establish a surprising connection
between speculative decoding and channel simulation, which aims at simulating a
noisy channel using as few bits as possible. This connection allows us to
provide an information-theoretic analysis of the speed up that can be achieved
by speculative decoding. Leveraging this link, we derive an explicit relation
between generation speed-up and the number of tokens $k$ generated by the draft
model for large $k$, which serves as an upper bound for all $k$. We also
propose a novel speculative decoding method via exponential race ERSD that
matches state-of-the-art performance.

</details>


### [7] [SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation](https://arxiv.org/abs/2504.15509)
*Keqi Deng,Wenxi Chen,Xie Chen,Philip C. Woodland*

Main category: cs.CL

TL;DR: 论文提出SimulS2S-LLM方法，通过离线训练语音大模型和测试时策略实现流式语音翻译，提升翻译质量和延迟的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在流式语音输入中难以实时生成翻译的问题。

Method: 离线训练语音大模型，提取边界感知语音提示，结合增量波束搜索预测离散语音标记。

Result: 在CVSS语音数据上，SimulS2S-LLM在相同延迟下翻译质量优于现有方法，ASR-BLEU提升3分。

Conclusion: SimulS2S-LLM有效解决了流式语音翻译中的训练与推理不匹配问题，实现了更好的质量-延迟权衡。

Abstract: Simultaneous speech translation (SST) outputs translations in parallel with
streaming speech input, balancing translation quality and latency. While large
language models (LLMs) have been extended to handle the speech modality,
streaming remains challenging as speech is prepended as a prompt for the entire
generation process. To unlock LLM streaming capability, this paper proposes
SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy
to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between
training and inference by extracting boundary-aware speech prompts that allows
it to be better matched with text input data. SimulS2S-LLM achieves
simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete
output speech tokens and then synthesising output speech using a pre-trained
vocoder. An incremental beam search is designed to expand the search space of
speech token prediction without increasing latency. Experiments on the CVSS
speech data show that SimulS2S-LLM offers a better translation quality-latency
trade-off than existing methods that use the same training data, such as
improving ASR-BLEU scores by 3 points at similar latency.

</details>


### [8] [The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks](https://arxiv.org/abs/2504.15521)
*Minghao Wu,Weixuan Wang,Sinuo Liu,Huifeng Yin,Xintong Wang,Yu Zhao,Chenyang Lyu,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: 该立场论文分析了2000多个多语言基准测试，发现英语仍占主导地位，且翻译基准与本地化基准在人类判断相关性上存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 促进多语言评估的公平性，揭示当前多语言基准测试的局限性。

Method: 分析2000多个多语言基准测试，比较其与人类判断的相关性。

Result: 英语过度代表，本地化基准优于翻译基准，STEM任务相关性高于传统NLP任务。

Conclusion: 提出多语言基准测试的指导原则和研究方向，呼吁全球合作开发符合实际应用的基准。

Abstract: As large language models (LLMs) continue to advance in linguistic
capabilities, robust multilingual evaluation has become essential for promoting
equitable technological progress. This position paper examines over 2,000
multilingual (non-English) benchmarks from 148 countries, published between
2021 and 2024, to evaluate past, present, and future practices in multilingual
benchmarking. Our findings reveal that, despite significant investments
amounting to tens of millions of dollars, English remains significantly
overrepresented in these benchmarks. Additionally, most benchmarks rely on
original language content rather than translations, with the majority sourced
from high-resource countries such as China, India, Germany, the UK, and the
USA. Furthermore, a comparison of benchmark performance with human judgments
highlights notable disparities. STEM-related tasks exhibit strong correlations
with human evaluations (0.70 to 0.85), while traditional NLP tasks like
question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30).
Moreover, translating English benchmarks into other languages proves
insufficient, as localized benchmarks demonstrate significantly higher
alignment with local human judgments (0.68) than their translated counterparts
(0.47). This underscores the importance of creating culturally and
linguistically tailored benchmarks rather than relying solely on translations.
Through this comprehensive analysis, we highlight six key limitations in
current multilingual evaluation practices, propose the guiding principles
accordingly for effective multilingual benchmarking, and outline five critical
research directions to drive progress in the field. Finally, we call for a
global collaborative effort to develop human-aligned benchmarks that prioritize
real-world applications.

</details>


### [9] [IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property](https://arxiv.org/abs/2504.15524)
*Qiyao Wang,Guhong Chen,Hongbo Wang,Huaren Liu,Minghui Zhu,Zhifei Qin,Linwei Li,Yilin Yue,Shiqiang Wang,Jiayan Li,Yihang Wu,Ziqiang Liu,Longze Chen,Run Luo,Liyang Fan,Jiaming Li,Lei Zhang,Kan Xu,Hongfei Lin,Hamid Alinejad-Rokny,Shiwen Ni,Yuan Lin,Min Yang*

Main category: cs.CL

TL;DR: 论文提出了首个全面的IP任务分类法和多语言基准IPBench，用于评估LLMs在知识产权领域的实际应用表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和基准在知识产权领域覆盖不足，缺乏与实际场景的对齐，因此需要更全面的评估工具。

Method: 引入IPBench，涵盖8种IP机制和20项任务，对16种LLMs进行基准测试。

Result: 最佳模型准确率仅为75.8%，开源和法律导向模型表现落后于闭源通用模型。

Conclusion: IPBench填补了知识产权领域评估工具的空白，未来将持续更新以反映实际挑战。

Abstract: Intellectual Property (IP) is a unique domain that integrates technical and
legal knowledge, making it inherently complex and knowledge-intensive. As large
language models (LLMs) continue to advance, they show great potential for
processing IP tasks, enabling more efficient analysis, understanding, and
generation of IP-related content. However, existing datasets and benchmarks
either focus narrowly on patents or cover limited aspects of the IP field,
lacking alignment with real-world scenarios. To bridge this gap, we introduce
the first comprehensive IP task taxonomy and a large, diverse bilingual
benchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is
designed to evaluate LLMs in real-world intellectual property applications,
encompassing both understanding and generation. We benchmark 16 LLMs, ranging
from general-purpose to domain-specific models, and find that even the
best-performing model achieves only 75.8% accuracy, revealing substantial room
for improvement. Notably, open-source IP and law-oriented models lag behind
closed-source general-purpose models. We publicly release all data and code of
IPBench and will continue to update it with additional IP-related tasks to
better reflect real-world challenges in the intellectual property domain.

</details>


### [10] [Compass-V2 Technical Report](https://arxiv.org/abs/2504.15527)
*Sophia Maria*

Main category: cs.CL

TL;DR: Compass-v2是一个轻量级的MoE模型，专为东南亚语言和电子商务设计，性能优越且推理成本低。


<details>
  <summary>Details</summary>
Motivation: 解决高资源语言主导的LLMs对东南亚低资源语言和电子商务领域的忽视。

Method: 设计30B总参数和5B活跃参数的MoE模型，构建高质量数据集，并首创混合推理模型。

Result: 在30B以下模型中表现出东南亚多语言和电子商务领域的最先进性能，推理成本显著降低。

Conclusion: Compass-v2成功填补了低资源语言和电子商务领域的空白，为未来研究提供了实用框架。

Abstract: Predominant LLMs focus on high-resource languages while leaving low-resource
languages, particularly those in Southeast Asia (SEA), underrepresented. In
addition, those models are general-purpose and pay limited attention to the
e-commerce domain. To overcome these limitations, we introduce Compass-v2, a
lightweight Mixture-of-Experts (MoE) model specifically designed for Southeast
Asian languages and e-commerce applications. To balance model performance and
inference cost, the model is designed with 30B total parameters and 5B active
parameters, incorporating both fine-grained and shared expert modules. To
enhance multilingual performance, we curated and constructed a high-quality,
industry-leading SEA dataset, to the best of our knowledge. To boost
performance in the e-commerce domain, we built a dataset comprising hundreds of
billions of tokens, sourced through external data mining and internal platform
collection. Besides, we pioneered a hybrid reasoning model that supports both
fast thinking and deep thinking within a unified framework to enhance the
reasoning capabilities, diverging from the conventional industry practice of
deploying two separate models. Through extensive experimental evaluations, our
model demonstrates state-of-the-art SEA multilingual and e-commerce performance
among sub-30B models, while maintaining significantly lower inference cost.

</details>


### [11] [llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus with Long Context Length](https://arxiv.org/abs/2504.15544)
*Issa Sugiura,Kouta Nakayama,Yusuke Oda*

Main category: cs.CL

TL;DR: 论文介绍了llm-jp-modernbert，一种基于大规模日语语料库训练的ModernBERT模型，支持8192个token的上下文长度。模型在填充掩码测试中表现良好，但未超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 探索在大规模语料和长上下文条件下预训练编码器模型（如BERT）的效果，填补与解码器模型相比的研究空白。

Method: 使用公开的大规模日语语料库训练ModernBERT模型，支持8192个token的上下文长度，并通过填充掩码测试和伪困惑度实验分析效果。

Result: 模型在填充掩码测试中表现良好，但在下游任务中未超越现有基线。通过伪困惑度实验验证了上下文长度扩展的效果。

Conclusion: llm-jp-modernbert为长上下文BERT模型的发展提供了支持，并公开了模型和代码以促进可重复性和进一步研究。

Abstract: Encoder-only transformer models like BERT are widely adopted as a pre-trained
backbone for tasks like sentence classification and retrieval. However,
pretraining of encoder models with large-scale corpora and long contexts has
been relatively underexplored compared to decoder-only transformers. In this
work, we present llm-jp-modernbert, a ModernBERT model trained on a publicly
available, massive Japanese corpus with a context length of 8192 tokens. While
our model does not surpass existing baselines on downstream tasks, it achieves
good results on fill-mask test evaluations. We also analyze the effect of
context length expansion through pseudo-perplexity experiments. Furthermore, we
investigate sentence embeddings in detail, analyzing their transitions during
training and comparing them with those from other existing models, confirming
similar trends with models sharing the same architecture. To support
reproducibility and foster the development of long-context BERT, we release our
model, along with the training and evaluation code.

</details>


### [12] [LLM-based Semantic Augmentation for Harmful Content Detection](https://arxiv.org/abs/2504.15548)
*Elyas Meguellati,Assaad Zeghina,Shazia Sadiq,Gianluca Demartini*

Main category: cs.CL

TL;DR: 论文提出了一种利用LLM进行文本预处理和语义增强的方法，以提升社交媒体分类任务的性能，尤其是在复杂任务（如宣传检测、仇恨内容识别）中。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLM生成合成数据，忽视了其在文本预处理和语义增强中的潜力，而复杂社交媒体任务中LLM的零样本性能较差。

Method: 通过提示LLM清理噪声文本并提供上下文丰富的解释，增强训练集，而不显著增加数据量。在多个数据集上系统评估。

Result: 零样本LLM分类在复杂任务中表现不佳，但LLM语义增强方法性能接近人类标注数据，且成本更低。

Conclusion: 策略性地将LLM整合到机器学习流程中，对社交媒体分类任务具有重要意义，为在线有害内容治理提供了新思路。

Abstract: Recent advances in large language models (LLMs) have demonstrated strong
performance on simple text classification tasks, frequently under zero-shot
settings. However, their efficacy declines when tackling complex social media
challenges such as propaganda detection, hateful meme classification, and
toxicity identification. Much of the existing work has focused on using LLMs to
generate synthetic training data, overlooking the potential of LLM-based text
preprocessing and semantic augmentation. In this paper, we introduce an
approach that prompts LLMs to clean noisy text and provide context-rich
explanations, thereby enhancing training sets without substantial increases in
data volume. We systematically evaluate on the SemEval 2024 multi-label
Persuasive Meme dataset and further validate on the Google Jigsaw toxic
comments and Facebook hateful memes datasets to assess generalizability. Our
results reveal that zero-shot LLM classification underperforms on these
high-context tasks compared to supervised models. In contrast, integrating
LLM-based semantic augmentation yields performance on par with approaches that
rely on human-annotated data, at a fraction of the cost. These findings
underscore the importance of strategically incorporating LLMs into machine
learning (ML) pipeline for social media classification tasks, offering broad
implications for combating harmful content online.

</details>


### [13] [Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction](https://arxiv.org/abs/2504.15573)
*Yuxin Jiang,Yufei Wang,Chuhan Wu,Xinyi Dai,Yan Xu,Weinan Gan,Yasheng Wang,Xin Jiang,Lifeng Shang,Ruiming Tang,Wei Wang*

Main category: cs.CL

TL;DR: 提出WebR框架，自动从原始网页文档合成高质量指令调优数据，显著提升LLM指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动数据合成方法依赖种子数据质量或网页文档结构假设，WebR旨在减少这些依赖。

Method: 通过双视角范式（网页作为指令或响应）重构网页内容，合成指令调优数据。

Result: WebR生成的数据在四个基准测试中优于现有方法16.65%，兼容性、数据效率和可扩展性更强。

Conclusion: WebR为高质量指令调优数据提供高效自动化解决方案，支持领域适应和扩展。

Abstract: The improvement of LLMs' instruction-following capabilities depends
critically on the availability of high-quality instruction-response pairs.
While existing automatic data synthetic methods alleviate the burden of manual
curation, they often rely heavily on either the quality of seed data or strong
assumptions about the structure and content of web documents. To tackle these
challenges, we propose Web Reconstruction (WebR), a fully automated framework
for synthesizing high-quality instruction-tuning (IT) data directly from raw
web documents with minimal assumptions. Leveraging the inherent diversity of
raw web content, we conceptualize web reconstruction as an instruction-tuning
data synthesis task via a novel dual-perspective paradigm--Web as Instruction
and Web as Response--where each web document is designated as either an
instruction or a response to trigger the reconstruction process. Comprehensive
experiments show that datasets generated by WebR outperform state-of-the-art
baselines by up to 16.65% across four instruction-following benchmarks.
Notably, WebR demonstrates superior compatibility, data efficiency, and
scalability, enabling enhanced domain adaptation with minimal effort. The data
and code are publicly available at https://github.com/YJiangcm/WebR.

</details>


### [14] [Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models](https://arxiv.org/abs/2504.15604)
*Pavan Yadav,Nikhil Khandalkar,Krishna Shinde,Lokesh B. Ramegowda,Rajarshi Das*

Main category: cs.CL

TL;DR: 比较GPT-2和Llama-2在ToM任务中的表现，发现Llama-2在低温度下表现更优，但上下文复杂性增加会降低预测准确性。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型在ToM任务中的表现，探究上下文复杂性和温度对预测的影响。

Method: 使用GPT-4增强ToM数据集，测试模型在不同温度和推理层级下的表现。

Result: Llama-2优于GPT-2，上下文复杂性降低准确性，模型在高阶推理中表现差异显著。

Conclusion: 模型架构、温度和上下文复杂性显著影响预测性能，揭示了当前模型的优缺点。

Abstract: Language models have made significant progress in generating coherent text
and predicting next tokens based on input prompts. This study compares the
next-token prediction performance of two well-known models: OpenAI's GPT-2 and
Meta's Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. To evaluate their
capabilities, we built a dataset from 10 short stories sourced from the Explore
ToM Dataset. We enhanced these stories by programmatically inserting additional
sentences (infills) using GPT-4, creating variations that introduce different
levels of contextual complexity. This setup enables analysis of how increasing
context affects model performance. We tested both models under four temperature
settings (0.01, 0.5, 1.0, 2.0) and evaluated their ability to predict the next
token across three reasoning levels. Zero-order reasoning involves tracking the
state, either current (ground truth) or past (memory). First-order reasoning
concerns understanding another's mental state (e.g., "Does Anne know the apple
is salted?"). Second-order reasoning adds recursion (e.g., "Does Anne think
that Charles knows the apple is salted?").
  Our results show that adding more infill sentences slightly reduces
prediction accuracy, as added context increases complexity and ambiguity.
Llama-2 consistently outperforms GPT-2 in prediction accuracy, especially at
lower temperatures, demonstrating greater confidence in selecting the most
probable token. As reasoning complexity rises, model responses diverge more.
Notably, GPT-2 and Llama-2 display greater variability in predictions during
first- and second-order reasoning tasks. These findings illustrate how model
architecture, temperature, and contextual complexity influence next-token
prediction, contributing to a better understanding of the strengths and
limitations of current language models.

</details>


### [15] [Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement](https://arxiv.org/abs/2504.15630)
*Xiaowei Yuan,Zhao Yang,Ziyang Huang,Yequan Wang,Siqi Fan,Yiming Ju,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 论文提出了一种名为CaLE的新方法，通过优化LLMs内部表示中的上下文知识利用，改进了上下文忠实生成的能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了LLMs内部状态中上下文信息的处理机制，导致LLMs无法充分利用上下文知识。

Method: 提出Context-aware Layer Enhancement (CaLE)，通过V-usable信息分析在最佳层增强上下文信息，从而丰富最终层的表示。

Result: 实验表明，CaLE在问答任务中有效提升了上下文忠实生成的能力，尤其是在涉及未知或冲突上下文知识的场景中。

Conclusion: CaLE通过优化内部表示机制，显著提升了LLMs的上下文忠实生成能力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various tasks, yet they often struggle with context-faithfulness generations
that properly reflect contextual knowledge. While existing approaches focus on
enhancing the decoding strategies, they ignore the fundamental mechanism of how
contextual information is processed within LLMs' internal states. As a result,
LLMs remain limited in their ability to fully leverage contextual knowledge. In
this paper, we propose Context-aware Layer Enhancement (CaLE), a novel
intervention method that enhances the utilization of contextual knowledge
within LLMs' internal representations. By employing V-usable information
analysis, CaLE strategically amplifies the growth of contextual information at
an optimal layer, thereby enriching representations in the final layer. Our
experiments demonstrate that CaLE effectively improves context-faithful
generation in Question-Answering tasks, particularly in scenarios involving
unknown or conflicting contextual knowledge.

</details>


### [16] [Cost-Effective Text Clustering with Large Language Models](https://arxiv.org/abs/2504.15640)
*Hongtao Wang,Taiyan Zhang,Renchi Yang,Jianliang Xu*

Main category: cs.CL

TL;DR: TECL是一个成本效益高的框架，利用LLM反馈在有限查询预算内实现高精度文本聚类。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在文本聚类中因大量API查询或推理调用带来的计算和财务开销问题。

Method: 采用EdgeLLM或TriangleLLM构建文本对的必须链接/不能链接约束，并利用这些约束作为监督信号进行加权约束聚类。

Result: 在多个基准数据集上，TECL在相同查询成本下显著优于现有解决方案。

Conclusion: TECL为有限预算下的高精度文本聚类提供了有效解决方案。

Abstract: Text clustering aims to automatically partition a collection of text
documents into distinct clusters based on linguistic features. In the
literature, this task is usually framed as metric clustering based on text
embeddings from pre-trained encoders or a graph clustering problem upon
pairwise similarities from an oracle, e.g., a large ML model. Recently, large
language models (LLMs) bring significant advancement in this field by offering
contextualized text embeddings and highly accurate similarity scores, but
meanwhile, present grand challenges to cope with substantial computational
and/or financial overhead caused by numerous API-based queries or inference
calls to the models.
  In response, this paper proposes TECL, a cost-effective framework that taps
into the feedback from LLMs for accurate text clustering within a limited
budget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or
TriangleLLM to construct must-link/cannot-link constraints for text pairs, and
further leverages such constraints as supervision signals input to our weighted
constrained clustering approach to generate clusters. Particularly, EdgeLLM
(resp. TriangleLLM) enables the identification of informative text pairs (resp.
triplets) for querying LLMs via well-thought-out greedy algorithms and accurate
extraction of pairwise constraints through carefully-crafted prompting
techniques. Our experiments on multiple benchmark datasets exhibit that TECL
consistently and considerably outperforms existing solutions in unsupervised
text clustering under the same query cost for LLMs.

</details>


### [17] [Computational Typology](https://arxiv.org/abs/2504.15642)
*Gerhard Jäger*

Main category: cs.CL

TL;DR: 本文探讨了计算统计模型在语言类型学研究中的应用及其优势。


<details>
  <summary>Details</summary>
Motivation: 语言类型学研究语言的结构多样性，计算方法的引入为大规模语言数据分析提供了新工具。

Method: 采用计算统计模型分析大规模语言数据，验证语言结构和演化的假设。

Result: 计算统计模型在语言类型学研究中展现出显著优势。

Conclusion: 计算统计模型为语言类型学研究提供了高效、可靠的分析工具。

Abstract: Typology is a subfield of linguistics that focuses on the study and
classification of languages based on their structural features. Unlike
genealogical classification, which examines the historical relationships
between languages, typology seeks to understand the diversity of human
languages by identifying common properties and patterns, known as universals.
In recent years, computational methods have played an increasingly important
role in typological research, enabling the analysis of large-scale linguistic
data and the testing of hypotheses about language structure and evolution. This
article provides an illustration of the benefits of computational statistical
modeling in typology.

</details>


### [18] [FinTextSim: Enhancing Financial Text Analysis with BERTopic](https://arxiv.org/abs/2504.15683)
*Simon Jehnen,Joaquín Ordieres-Meré,Javier Villalba-Díez*

Main category: cs.CL

TL;DR: 研究探讨了BERTopic与FinTextSim结合在金融文本分析中的有效性，显著提升了主题分类的清晰度。


<details>
  <summary>Details</summary>
Motivation: 信息可用性和计算能力的进步推动了金融文本分析的需求，传统方法难以处理大量文本数据。

Method: 使用BERTopic和FinTextSim分析S&P 500公司的10-K文件，比较FinTextSim与all-MiniLM-L6-v2的效果。

Result: FinTextSim显著提升了主题内相似性并降低了主题间相似性，BERTopic仅与FinTextSim结合时能形成清晰主题。

Conclusion: FinTextSim对金融文本分析至关重要，能提升研究质量和决策效率，潜在影响业务估值和股价预测。

Abstract: Recent advancements in information availability and computational
capabilities have transformed the analysis of annual reports, integrating
traditional financial metrics with insights from textual data. To extract
valuable insights from this wealth of textual data, automated review processes,
such as topic modeling, are crucial. This study examines the effectiveness of
BERTopic, a state-of-the-art topic model relying on contextual embeddings, for
analyzing Item 7 and Item 7A of 10-K filings from S&P 500 companies
(2016-2022). Moreover, we introduce FinTextSim, a finetuned
sentence-transformer model optimized for clustering and semantic search in
financial contexts. Compared to all-MiniLM-L6-v2, the most widely used
sentence-transformer, FinTextSim increases intratopic similarity by 81% and
reduces intertopic similarity by 100%, significantly enhancing organizational
clarity. We assess BERTopic's performance using embeddings from both FinTextSim
and all-MiniLM-L6-v2. Our findings reveal that BERTopic only forms clear and
distinct economic topic clusters when paired with FinTextSim's embeddings.
Without FinTextSim, BERTopic struggles with misclassification and overlapping
topics. Thus, FinTextSim is pivotal for advancing financial text analysis.
FinTextSim's enhanced contextual embeddings, tailored for the financial domain,
elevate the quality of future research and financial information. This improved
quality of financial information will enable stakeholders to gain a competitive
advantage, streamlining resource allocation and decision-making processes.
Moreover, the improved insights have the potential to leverage business
valuation and stock price prediction models.

</details>


### [19] [Subject islands do not reduce to construction-specific discourse function](https://arxiv.org/abs/2504.15688)
*Mandy Cartner,Matthew Kogan,Nikolas Webster,Matthew Wagers,Ivy Sichel*

Main category: cs.CL

TL;DR: 论文探讨语言学中的‘岛屿’现象，特别是主语作为岛屿的原因，并通过实验验证主语岛屿效应是否仅与信息结构冲突有关。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证主语岛屿效应是否仅由信息结构冲突（如wh-疑问句）引起，还是与抽象的句法表征独立相关。

Method: 通过三个大规模可接受性研究，使用超加性设计，测试wh-疑问句、关系从句和话题化结构中的主语岛屿效应。

Result: 实验结果显示，所有测试结构均存在主语岛屿效应，表明其与抽象的句法表征相关，而非仅由信息结构冲突引起。

Conclusion: 研究支持岛屿效应的抽象句法解释，独立于结构的交际功能。

Abstract: The term islands in linguistics refers to phrases from which extracting an
element results in ungrammaticality (Ross, 1967). Grammatical subjects are
considered islands because extracting a sub-part of a subject results in an
ill-formed sentence, despite having a clear intended meaning (e.g., "Which
topic did the article about inspire you?"). The generative tradition, which
views syntax as autonomous of meaning and function, attributes this
ungrammaticality to the abstract movement dependency between the wh-phrase and
the subject-internal position with which it is associated for interpretation.
However, research on language that emphasizes its communicative function
suggests instead that syntactic constraints, including islands, can be
explained based on the way different constructions package information.
Accordingly, Abeill\'e et al. (2020) suggest that the islandhood of subjects is
specific to the information structure of wh-questions, and propose that
subjects are not islands for movement, but for focusing, due to their
discourse-backgroundedness. This predicts that other constructions that differ
in their information structure from wh-questions, but still involve movement,
should not create a subject island effect. We test this prediction in three
large-scale acceptability studies, using a super-additive design that singles
out subject island violations, in three different constructions: wh-questions,
relative clauses, and topicalization. We report evidence for a subject island
effect in each construction type, despite only wh-questions introducing what
Abeill\'e et al. (2020) call "a clash in information structure." We argue that
this motivates an account of islands in terms of abstract, syntactic
representations, independent of the communicative function associated with the
constructions.

</details>


### [20] [Tina: Tiny Reasoning Models via LoRA](https://arxiv.org/abs/2504.15777)
*Shangshang Wang,Julian Asilis,Ömer Faruk Akgül,Enes Burak Bilgin,Ollie Liu,Willie Neiswanger*

Main category: cs.CL

TL;DR: Tina是一种高效的小型推理模型，通过低成本的后训练方法（如LoRA）在1.5B参数的基础模型上实现强大的推理能力。


<details>
  <summary>Details</summary>
Motivation: 探索如何以低成本实现语言模型的强推理能力。

Method: 使用参数高效更新（LoRA）在强化学习中对小型基础模型进行微调。

Result: Tina在推理性能上优于或媲美SOTA模型，且后训练成本极低（仅9美元）。

Conclusion: LoRA在高效RL推理中表现出色，为低成本高性能模型提供了新思路。

Abstract: How cost-effectively can strong reasoning abilities be achieved in language
models? Driven by this fundamental question, we present Tina, a family of tiny
reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates
that substantial reasoning performance can be developed using only minimal
resources, by applying parameter-efficient updates during reinforcement
learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B
parameter base model. This minimalist approach produces models that achieve
reasoning performance which is competitive with, and sometimes surpasses, SOTA
RL reasoning models built upon the same base model. Crucially, this is achieved
at a tiny fraction of the computational post-training cost employed by existing
SOTA models. In fact, the best Tina model achieves a >20\% reasoning
performance increase and 43.33\% Pass@1 accuracy on AIME24, at only \$9 USD
post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our
work reveals the surprising effectiveness of efficient RL reasoning via LoRA.
We validate this across multiple open-source reasoning datasets and various
ablation settings starting with a single, fixed set of hyperparameters.
Furthermore, we hypothesize that this effectiveness and efficiency stem from
LoRA rapidly adapting the model to the structural format of reasoning rewarded
by RL, while largely preserving the base model's underlying knowledge. In
service of accessibility and open research, we fully open-source all code,
training logs, and model weights \& checkpoints.

</details>


### [21] [Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach](https://arxiv.org/abs/2504.15784)
*Ruizhe Li,Chiwei Zhu,Benfeng Xu,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: 提出了一种基于TTCW的自动化评估方法，用于评估LLM生成文本的创造力，显著提高了与人类评估的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵的人工标注或与人类评估不一致，需要更有效的自动化评估方法。

Method: 采用基于参考的Likert风格评分，将生成文本与高质量参考文本对比。

Result: 实验结果显示，该方法显著提高了与人类评估的一致性，配对准确率达到0.75（+15%）。

Conclusion: 该方法为LLM生成文本的创造力评估提供了有效的自动化解决方案。

Abstract: Creative writing is a key capability of Large Language Models (LLMs), with
potential applications in literature, storytelling, and various creative
domains. However, evaluating the creativity of machine-generated texts remains
a significant challenge, as existing methods either rely on costly manual
annotations or fail to align closely with human assessments. In this paper, we
propose an effective automated evaluation method based on the Torrance Test of
Creative Writing (TTCW), which evaluates creativity as product. Our method
employs a reference-based Likert-style approach, scoring generated creative
texts relative to high-quality reference texts across various tests.
Experimental results demonstrate that our method significantly improves the
alignment between LLM evaluations and human assessments, achieving a pairwise
accuracy of 0.75 (+15\%).

</details>


### [22] [A closer look at how large language models trust humans: patterns and biases](https://arxiv.org/abs/2504.15801)
*Valeria Lerman,Yaniv Dover*

Main category: cs.CL

TL;DR: 研究探讨了LLM（大型语言模型）对人类的有效信任机制，发现其信任形成与人类类似，但存在偏见，尤其在金融场景中。


<details>
  <summary>Details</summary>
Motivation: 理解LLM与人类在决策互动中的信任动态，填补LLM对人类信任研究的空白。

Method: 基于行为理论，分析LLM信任是否依赖人类的能力、善意和诚信三个维度，并考察人口统计变量的影响。通过43,200次模拟实验，覆盖五种模型和场景。

Result: LLM信任形成与人类相似，但受信任度和人口统计因素（如年龄、宗教、性别）影响，尤其在金融场景中。不同模型表现差异较大。

Conclusion: 需进一步研究AI对人类的信任动态，监控偏见和信任模式，以避免信任敏感应用中潜在的有害结果。

Abstract: As large language models (LLMs) and LLM-based agents increasingly interact
with humans in decision-making contexts, understanding the trust dynamics
between humans and AI agents becomes a central concern. While considerable
literature studies how humans trust AI agents, it is much less understood how
LLM-based agents develop effective trust in humans. LLM-based agents likely
rely on some sort of implicit effective trust in trust-related contexts (e.g.,
evaluating individual loan applications) to assist and affect decision making.
Using established behavioral theories, we develop an approach that studies
whether LLMs trust depends on the three major trustworthiness dimensions:
competence, benevolence and integrity of the human subject. We also study how
demographic variables affect effective trust. Across 43,200 simulated
experiments, for five popular language models, across five different scenarios
we find that LLM trust development shows an overall similarity to human trust
development. We find that in most, but not all cases, LLM trust is strongly
predicted by trustworthiness, and in some cases also biased by age, religion
and gender, especially in financial scenarios. This is particularly true for
scenarios common in the literature and for newer models. While the overall
patterns align with human-like mechanisms of effective trust formation,
different models exhibit variation in how they estimate trust; in some cases,
trustworthiness and demographic factors are weak predictors of effective trust.
These findings call for a better understanding of AI-to-human trust dynamics
and monitoring of biases and trust development patterns to prevent unintended
and potentially harmful outcomes in trust-sensitive applications of AI.

</details>


### [23] [What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns](https://arxiv.org/abs/2504.15815)
*Michael A. Hedderich,Anyi Wang,Raoyuan Zhao,Florian Eichin,Barbara Plank*

Main category: cs.CL

TL;DR: Spotlight结合自动化与人工分析，通过数据挖掘技术区分语言模型输出的随机变化与系统性差异，提供标记模式以高效分析提示和模型变化的影响。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（自动指标或人工评估）存在局限性，无法全面洞察提示和模型变化对输出的影响。

Method: 基于数据挖掘技术，自动区分输出中的随机变化与系统性差异，生成标记模式指导人工分析。

Result: 创建三个基准测试验证标记模式提取方法的可靠性，并通过用户研究证明该方法能帮助用户理解输出差异。

Conclusion: Spotlight支持提示工程和以人为中心的模型行为研究，揭示由提示和模型变化引起的相关差异。

Abstract: Prompt engineering for large language models is challenging, as even small
prompt perturbations or model changes can significantly impact the generated
output texts. Existing evaluation methods, either automated metrics or human
evaluation, have limitations, such as providing limited insights or being
labor-intensive. We propose Spotlight, a new approach that combines both
automation and human analysis. Based on data mining techniques, we
automatically distinguish between random (decoding) variations and systematic
differences in language model outputs. This process provides token patterns
that describe the systematic differences and guide the user in manually
analyzing the effects of their prompt and model changes efficiently. We create
three benchmarks to quantitatively test the reliability of token pattern
extraction methods and demonstrate that our approach provides new insights into
established prompt data. From a human-centric perspective, through
demonstration studies and a user study, we show that our token pattern approach
helps users understand the systematic differences of language model outputs,
and we are able to discover relevant differences caused by prompt and model
changes (e.g. related to gender or culture), thus supporting the prompt
engineering process and human-centric model behavior research.

</details>


### [24] [Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model](https://arxiv.org/abs/2504.15843)
*Junshu Pan,Wei Shen,Shulin Huang,Qiji Zhou,Yue Zhang*

Main category: cs.CL

TL;DR: Pre-DPO是一种改进的偏好优化方法，通过引入引导参考模型提升DPO和SimPO的性能。


<details>
  <summary>Details</summary>
Motivation: DPO和SimPO在训练中存在数据利用效率低和鲁棒性不足的问题，需要改进。

Method: 提出Pre-DPO，利用引导参考模型动态调整样本权重，优化训练过程。

Result: 在AlpacaEval 2.0和Arena-Hard v0.1基准测试中，Pre-DPO显著提升了性能。

Conclusion: Pre-DPO是一种简单有效的改进方法，无需外部模型或额外数据即可提升偏好优化效果。

Abstract: Direct Preference Optimization (DPO) simplifies reinforcement learning from
human feedback (RLHF) for large language models (LLMs) by directly optimizing
human preferences without an explicit reward model. We find that during DPO
training, the reference model plays the role of a data weight adjuster.
However, the common practice of initializing the policy and reference models
identically in DPO can lead to inefficient data utilization and impose a
performance ceiling. Meanwhile, the lack of a reference model in Simple
Preference Optimization (SimPO) reduces training robustness and necessitates
stricter conditions to prevent catastrophic forgetting. In this work, we
propose Pre-DPO, a simple yet effective DPO-based training paradigm that
enhances preference optimization performance by leveraging a guiding reference
model. This reference model provides foresight into the optimal policy state
achievable through the training preference data, serving as a guiding mechanism
that adaptively assigns higher weights to samples more suitable for the model
and lower weights to those less suitable. Extensive experiments on AlpacaEval
2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently
improves the performance of both DPO and SimPO, without relying on external
models or additional data.

</details>


### [25] [Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2504.15848)
*Luwei Xiao,Rui Mao,Shuai Zhao,Qika Lin,Yanhao Jia,Liang He,Erik Cambria*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Chimera的多模态情感分类框架，结合了视觉和语义特征，通过认知和美学分析提升情感极性预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于社交媒体上多模态内容的增加，现有方法在理解细粒度视觉内容和情感驱动因素方面存在不足。

Method: 框架整合视觉块特征、粗细粒度视觉特征，并利用大语言模型生成情感原因和印象。

Result: 实验证明该模型在标准数据集上表现优于GPT-4o等大语言模型。

Conclusion: Chimera框架在多模态情感分类任务中表现出高效性和灵活性。

Abstract: Multimodal aspect-based sentiment classification (MASC) is an emerging task
due to an increase in user-generated multimodal content on social platforms,
aimed at predicting sentiment polarity toward specific aspect targets (i.e.,
entities or attributes explicitly mentioned in text-image pairs). Despite
extensive efforts and significant achievements in existing MASC, substantial
gaps remain in understanding fine-grained visual content and the cognitive
rationales derived from semantic content and impressions (cognitive
interpretations of emotions evoked by image content). In this study, we present
Chimera: a cognitive and aesthetic sentiment causality understanding framework
to derive fine-grained holistic features of aspects and infer the fundamental
drivers of sentiment expression from both semantic perspectives and
affective-cognitive resonance (the synergistic effect between emotional
responses and cognitive interpretations). Specifically, this framework first
incorporates visual patch features for patch-word alignment. Meanwhile, it
extracts coarse-grained visual features (e.g., overall image representation)
and fine-grained visual regions (e.g., aspect-related regions) and translates
them into corresponding textual descriptions (e.g., facial, aesthetic).
Finally, we leverage the sentimental causes and impressions generated by a
large language model (LLM) to enhance the model's awareness of sentimental cues
evoked by semantic content and affective-cognitive resonance. Experimental
results on standard MASC datasets demonstrate the effectiveness of the proposed
model, which also exhibits greater flexibility to MASC compared to LLMs such as
GPT-4o. We have publicly released the complete implementation and dataset at
https://github.com/Xillv/Chimera

</details>


### [26] [Dynamic Early Exit in Reasoning Models](https://arxiv.org/abs/2504.15895)
*Chenxu Yang,Qingyi Si,Yongjie Duan,Zheliang Zhu,Chenyu Zhu,Zheng Lin,Li Cao,Weiping Wang*

Main category: cs.CL

TL;DR: 提出一种让大语言模型在生成思维链时自我截断的方法，通过动态监测模型行为提前终止冗余推理，提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 长思维链可能导致效率低下和准确性损失，需要一种无需额外训练的动态截断方法。

Method: 监测模型在推理过渡点的行为（如“Wait”标记），动态终止冗余推理链生成。

Result: 在多个推理基准测试中，平均减少思维链长度31%-43%，同时提升准确性1.7%-5.7%。

Conclusion: 该方法简单有效，可无缝集成到现有推理模型中，显著提升性能。

Abstract: Recent advances in large reasoning language models (LRLMs) rely on test-time
scaling, which extends long chain-of-thought (CoT) generation to solve complex
tasks. However, overthinking in long CoT not only slows down the efficiency of
problem solving, but also risks accuracy loss due to the extremely detailed or
redundant reasoning steps. We propose a simple yet effective method that allows
LLMs to self-truncate CoT sequences by early exit during generation. Instead of
relying on fixed heuristics, the proposed method monitors model behavior at
potential reasoning transition points (e.g.,"Wait" tokens) and dynamically
terminates the next reasoning chain's generation when the model exhibits high
confidence in a trial answer. Our method requires no additional training and
can be seamlessly integrated into existing o1-like reasoning LLMs. Experiments
on multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024
show that the proposed method is consistently effective on deepseek-series
reasoning LLMs, reducing the length of CoT sequences by an average of 31% to
43% while improving accuracy by 1.7% to 5.7%.

</details>


### [27] [SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2504.15900)
*Cheng Wen,Tingwei Guo,Shuaijiang Zhao,Wei Zou,Xiangang Li*

Main category: cs.CL

TL;DR: 论文通过强化学习（RL）提升大型音频语言模型（LALM）的推理能力，提出结构化音频推理模型SARI，性能显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在音频语言推理中的适用性，提升模型的推理能力。

Method: 采用两阶段训练：监督微调（SFT）和课程引导的GRPO强化学习，比较结构化与非结构化推理。

Result: SARI模型在基线模型上平均准确率提升16.35%，并在MMAU测试中达到67.08%的SOTA性能。

Conclusion: 结构化推理和课程学习显著提升音频语言理解能力。

Abstract: Recent work shows that reinforcement learning(RL) can markedly sharpen the
reasoning ability of large language models (LLMs) by prompting them to "think
before answering." Yet whether and how these gains transfer to audio-language
reasoning remains largely unexplored. We extend the Group-Relative Policy
Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model
(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage
regimen supervised fine-tuning on structured and unstructured
chains-of-thought, followed by curriculum-guided GRPO, we systematically
compare implicit vs. explicit, and structured vs. free form reasoning under
identical architectures. Our structured audio reasoning model, SARI (Structured
Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a
16.35% improvement in average accuracy over the base model
Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni
reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.
Ablation experiments show that on the base model we use: (i) SFT warm-up is
important for stable RL training, (ii) structured chains yield more robust
generalization than unstructured ones, and (iii) easy-to-hard curricula
accelerate convergence and improve final performance. These findings
demonstrate that explicit, structured reasoning and curriculum learning
substantially enhances audio-language understanding.

</details>


### [28] [FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity](https://arxiv.org/abs/2504.15941)
*Fanny Jourdan,Yannick Chevalier,Cécile Favre*

Main category: cs.CL

TL;DR: FairTranslate是一个用于评估机器翻译系统中非二元性别偏见的数据集，测试了四种主流LLMs在英语到法语翻译中的表现，结果显示存在显著偏见。


<details>
  <summary>Details</summary>
Motivation: LLMs在翻译包容性语言（如使用单数'they'代词）时表现不佳，需通过系统框架评估其公平性。

Method: 创建了FairTranslate数据集（2418个英语-法语句对），评估了四种LLMs在不同提示下的表现。

Result: LLMs在性别表示上存在显著偏见，表明机器翻译在公平性方面仍有挑战。

Conclusion: 需制定策略确保LLM翻译系统的公平性和包容性，数据集和代码已公开。

Abstract: Large Language Models (LLMs) are increasingly leveraged for translation tasks
but often fall short when translating inclusive language -- such as texts
containing the singular 'they' pronoun or otherwise reflecting fair linguistic
protocols. Because these challenges span both computational and societal
domains, it is imperative to critically evaluate how well LLMs handle inclusive
translation with a well-founded framework.
  This paper presents FairTranslate, a novel, fully human-annotated dataset
designed to evaluate non-binary gender biases in machine translation systems
from English to French. FairTranslate includes 2418 English-French sentence
pairs related to occupations, annotated with rich metadata such as the
stereotypical alignment of the occupation, grammatical gender indicator
ambiguity, and the ground-truth gender label (male, female, or inclusive).
  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,
Llama3.3-70B) on this dataset under different prompting procedures. Our results
reveal substantial biases in gender representation across LLMs, highlighting
persistent challenges in achieving equitable outcomes in machine translation.
These findings underscore the need for focused strategies and interventions
aimed at ensuring fair and inclusive language usage in LLM-based translation
systems.
  We make the FairTranslate dataset publicly available on Hugging Face, and
disclose the code for all experiments on GitHub.

</details>


### [29] [W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models](https://arxiv.org/abs/2504.15983)
*Shang Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为W-PCA的新型零样本神经架构搜索方法，专注于轻量级语言模型的设计与评估，通过参数计数和主成分分析优化效率，显著减少了训练时间并提升了测试性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本神经架构搜索方法存在评估指标偏差和计算效率低的问题，亟需一种更高效且准确的轻量级语言模型设计与评估方法。

Method: 采用W-PCA方法，结合参数计数和主成分分析（累积贡献超过η）作为评估代理，无需梯度计算，优化评估时间。

Result: 在GLUE和SQuAD数据集上的实验表明，该方法显著减少训练时间，测试分数优于现有方法；在FlexiBERT搜索空间中的排名评估也表现出更高的相关性和更短的解决时间。

Conclusion: W-PCA方法为轻量级语言模型的设计提供了一种高效且准确的零样本神经架构搜索方案，优于现有方法。

Abstract: The demand for efficient natural language processing (NLP) systems has led to
the development of lightweight language models. Previous work in this area has
primarily focused on manual design or training-based neural architecture search
(NAS) methods. Recently, zero-shot NAS methods have been proposed for
evaluating language models without the need for training. However, prevailing
approaches to zero-shot NAS often face challenges such as biased evaluation
metrics and computational inefficiencies. In this paper, we introduce
weight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored
for lightweight language models. Our approach utilizes two evaluation proxies:
the parameter count and the number of principal components with cumulative
contribution exceeding $\eta$ in the feed-forward neural (FFN) layer.
Additionally, by eliminating the need for gradient computations, we optimize
the evaluation time, thus enhancing the efficiency of designing and evaluating
lightweight language models. We conduct a comparative analysis on the GLUE and
SQuAD datasets to evaluate our approach. The results demonstrate that our
method significantly reduces training time compared to one-shot NAS methods and
achieves higher scores in the testing phase compared to previous
state-of-the-art training-based methods. Furthermore, we perform ranking
evaluations on a dataset sampled from the FlexiBERT search space. Our approach
exhibits superior ranking correlation and further reduces solving time compared
to other zero-shot NAS methods that require gradient computation.

</details>


### [30] [Few-shot Hate Speech Detection Based on the MindSpore Framework](https://arxiv.org/abs/2504.15987)
*Zhenkai Qin,Dongze Wu,Yuxin Liu,Guifang Yang*

Main category: cs.CL

TL;DR: MS-FSLHate是一种基于提示增强的神经网络框架，用于少样本仇恨言论检测，结合了可学习提示嵌入、CNN-BiLSTM主干网络和对抗数据增强，在HateXplain和HSOL数据集上表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上仇恨言论的泛滥需要有效的检测系统，但现有深度学习模型在少样本或低资源场景下性能下降。

Method: 提出MS-FSLHate框架，整合可学习提示嵌入、CNN-BiLSTM主干网络和基于同义词的对抗数据增强。

Result: 在两个基准数据集上，模型在精确率、召回率和F1分数上优于基线，且具有高效性和可扩展性。

Conclusion: 结合提示学习和对抗增强的方法在少样本仇恨言论检测中表现出鲁棒性和适应性。

Abstract: The proliferation of hate speech on social media poses a significant threat
to online communities, requiring effective detection systems. While deep
learning models have shown promise, their performance often deteriorates in
few-shot or low-resource settings due to reliance on large annotated corpora.
To address this, we propose MS-FSLHate, a prompt-enhanced neural framework for
few-shot hate speech detection implemented on the MindSpore deep learning
platform. The model integrates learnable prompt embeddings, a CNN-BiLSTM
backbone with attention pooling, and synonym-based adversarial data
augmentation to improve generalization. Experimental results on two benchmark
datasets-HateXplain and HSOL-demonstrate that our approach outperforms
competitive baselines in precision, recall, and F1-score. Additionally, the
framework shows high efficiency and scalability, suggesting its suitability for
deployment in resource-constrained environments. These findings highlight the
potential of combining prompt-based learning with adversarial augmentation for
robust and adaptable hate speech detection in few-shot scenarios.

</details>


### [31] [CAPO: Cost-Aware Prompt Optimization](https://arxiv.org/abs/2504.16005)
*Tom Zehle,Moritz Schlager,Timo Heiß,Matthias Feurer*

Main category: cs.CL

TL;DR: CAPO是一种成本感知的提示优化算法，通过结合AutoML技术提高效率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的性能对提示词高度敏感，现有提示优化方法成本高昂。

Method: CAPO采用进化算法，结合AutoML技术，通过多目标优化平衡性能和提示长度。

Result: 在11/15的案例中，CAPO优于现有方法，性能提升高达21%，且成本更低。

Conclusion: CAPO通过提高成本效率，使提示优化更强大且易于使用。

Abstract: Large language models (LLMs) have revolutionized natural language processing
by solving a wide range of tasks simply guided by a prompt. Yet their
performance is highly sensitive to prompt formulation. While automated prompt
optimization addresses this challenge by finding optimal prompts, current
methods require a substantial number of LLM calls and input tokens, making
prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt
Optimization), an algorithm that enhances prompt optimization efficiency by
integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as
operators, incorporating racing to save evaluations and multi-objective
optimization to balance performance with prompt length. It jointly optimizes
instructions and few-shot examples while leveraging task descriptions for
improved robustness. Our extensive experiments across diverse datasets and LLMs
demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization
methods in 11/15 cases with improvements up to 21%p. Our algorithm achieves
better performances already with smaller budgets, saves evaluations through
racing, and decreases average prompt length via a length penalty, making it
both cost-efficient and cost-aware. Even without few-shot examples, CAPO
outperforms its competitors and generally remains robust to initial prompts.
CAPO represents an important step toward making prompt optimization more
powerful and accessible by improving cost-efficiency.

</details>


### [32] [Methods for Recognizing Nested Terms](https://arxiv.org/abs/2504.16007)
*Igor Rozhkov,Natalia Loukachevitch*

Main category: cs.CL

TL;DR: 本文介绍了在RuTermEval竞赛中应用Binder模型提取嵌套术语的研究，取得了最佳成绩，并探讨了从非嵌套标注数据中提取嵌套术语的可行性。


<details>
  <summary>Details</summary>
Motivation: 研究如何从非嵌套标注数据中有效提取嵌套术语，并验证Binder模型在此任务中的适用性。

Method: 采用Binder模型，该模型曾成功用于嵌套命名实体识别，现应用于嵌套术语提取。

Result: 在RuTermEval竞赛的三个赛道中均取得最佳成绩，验证了方法的有效性。

Conclusion: 提出的方法能够有效提取嵌套术语，即使在没有嵌套标注的情况下也具备可行性。

Abstract: In this paper, we describe our participation in the RuTermEval competition
devoted to extracting nested terms. We apply the Binder model, which was
previously successfully applied to the recognition of nested named entities, to
extract nested terms. We obtained the best results of term recognition in all
three tracks of the RuTermEval competition. In addition, we study the new task
of recognition of nested terms from flat training data annotated with terms
without nestedness. We can conclude that several approaches we proposed in this
work are viable enough to retrieve nested terms effectively without nested
labeling of them.

</details>


### [33] [Certified Mitigation of Worst-Case LLM Copyright Infringement](https://arxiv.org/abs/2504.16046)
*Jingyu Zhang,Jiacan Yu,Marc Marone,Benjamin Van Durme,Daniel Khashabi*

Main category: cs.CL

TL;DR: 论文提出了一种名为BloomScrub的轻量级推理时方法，用于解决大语言模型（LLMs）在预训练中接触版权材料后可能引发的侵权风险。该方法通过结合引用检测和重写技术，提供可认证的版权删除效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在预训练中可能接触版权材料，导致部署后无意侵权。现有方法对平均风险有效，但忽略了最坏情况下的版权风险（如长段直接引用）。

Method: 提出BloomScrub方法，通过引用检测和重写技术转换潜在侵权内容，并利用Bloom过滤器实现高效版权筛查。若无法删除超长引用，系统可选择不回应。

Result: 实验表明，BloomScrub能降低侵权风险，保持模型实用性，并通过自适应回避支持不同严格程度的版权执行。

Conclusion: 轻量级推理时方法在版权预防中效果显著，BloomScrub为实际应用提供了高效解决方案。

Abstract: The exposure of large language models (LLMs) to copyrighted material during
pre-training raises concerns about unintentional copyright infringement post
deployment. This has driven the development of "copyright takedown" methods,
post-training approaches aimed at preventing models from generating content
substantially similar to copyrighted ones. While current mitigation approaches
are somewhat effective for average-case risks, we demonstrate that they
overlook worst-case copyright risks exhibits by the existence of long, verbatim
quotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet
highly effective inference-time approach that provides certified copyright
takedown. Our method repeatedly interleaves quote detection with rewriting
techniques to transform potentially infringing segments. By leveraging
efficient data sketches (Bloom filters), our approach enables scalable
copyright screening even for large-scale real-world corpora. When quotes beyond
a length threshold cannot be removed, the system can abstain from responding,
offering certified risk reduction. Experimental results show that BloomScrub
reduces infringement risk, preserves utility, and accommodates different levels
of enforcement stringency with adaptive abstention. Our results suggest that
lightweight, inference-time methods can be surprisingly effective for copyright
prevention.

</details>


### [34] [LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement](https://arxiv.org/abs/2504.16053)
*Zhifan Ye,Kejing Xia,Yonggan Fu,Xin Dong,Jihoon Hong,Xiangchi Yuan,Shizhe Diao,Jan Kautz,Pavlo Molchanov,Yingyan Celine Lin*

Main category: cs.CL

TL;DR: LongMamba是一种无需训练的技术，通过分类和过滤关键令牌显著提升Mamba模型的长上下文理解能力。


<details>
  <summary>Details</summary>
Motivation: 解决Mamba模型在长上下文任务中表现不佳的问题，同时保持其高效性。

Method: 将Mamba的隐藏通道分为局部和全局通道，通过识别并过滤关键令牌来缓解全局通道的状态记忆衰减。

Result: LongMamba显著提升了Mamba模型的长上下文性能，无需额外训练。

Conclusion: LongMamba为Mamba模型的长上下文任务设定了新标准，扩展了其应用范围。

Abstract: State space models (SSMs) have emerged as an efficient alternative to
Transformer models for language modeling, offering linear computational
complexity and constant memory usage as context length increases. However,
despite their efficiency in handling long contexts, recent studies have shown
that SSMs, such as Mamba models, generally underperform compared to
Transformers in long-context understanding tasks. To address this significant
shortfall and achieve both efficient and accurate long-context understanding,
we propose LongMamba, a training-free technique that significantly enhances the
long-context capabilities of Mamba models. LongMamba builds on our discovery
that the hidden channels in Mamba can be categorized into local and global
channels based on their receptive field lengths, with global channels primarily
responsible for long-context capability. These global channels can become the
key bottleneck as the input context lengthens. Specifically, when input lengths
largely exceed the training sequence length, global channels exhibit
limitations in adaptively extend their receptive fields, leading to Mamba's
poor long-context performance. The key idea of LongMamba is to mitigate the
hidden state memory decay in these global channels by preventing the
accumulation of unimportant tokens in their memory. This is achieved by first
identifying critical tokens in the global channels and then applying token
filtering to accumulate only those critical tokens. Through extensive
benchmarking across synthetic and real-world long-context scenarios, LongMamba
sets a new standard for Mamba's long-context performance, significantly
extending its operational range without requiring additional training. Our code
is available at https://github.com/GATECH-EIC/LongMamba.

</details>


### [35] [Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability](https://arxiv.org/abs/2504.16056)
*Daniel Hendriks,Philipp Spitzer,Niklas Kühl,Gerhard Satzger*

Main category: cs.CL

TL;DR: 论文通过引入新的知识蒸馏方法（如critique-revision prompting）并系统比较其性能与可解释性，旨在解决大语言模型在资源受限环境中的部署问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的计算和存储需求高，限制了其在资源受限环境中的应用。知识蒸馏通过训练小型学生模型来缓解这一问题，但现有方法的性能和可解释性尚未充分研究。

Method: 应用critique-revision prompting进行数据生成，并综合现有方法训练学生模型。使用Commonsense Question-Answering (CQA)数据集进行系统比较，评估性能（准确性）和可解释性（人类研究）。

Result: 提出了新的蒸馏方法，并对其性能和可解释性进行了比较，为小型语言模型的蒸馏提供了新思路。

Conclusion: 研究推动了小型语言模型的蒸馏技术，有助于LLM技术的更广泛应用和快速普及。

Abstract: Artificial Intelligence (AI) has increasingly influenced modern society,
recently in particular through significant advancements in Large Language
Models (LLMs). However, high computational and storage demands of LLMs still
limit their deployment in resource-constrained environments. Knowledge
distillation addresses this challenge by training a small student model from a
larger teacher model. Previous research has introduced several distillation
methods for both generating training data and for training the student model.
Despite their relevance, the effects of state-of-the-art distillation methods
on model performance and explainability have not been thoroughly investigated
and compared. In this work, we enlarge the set of available methods by applying
critique-revision prompting to distillation for data generation and by
synthesizing existing methods for training. For these methods, we provide a
systematic comparison based on the widely used Commonsense Question-Answering
(CQA) dataset. While we measure performance via student model accuracy, we
employ a human-grounded study to evaluate explainability. We contribute new
distillation methods and their comparison in terms of both performance and
explainability. This should further advance the distillation of small language
models and, thus, contribute to broader applicability and faster diffusion of
LLM technology.

</details>


### [36] [Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation](https://arxiv.org/abs/2504.16060)
*Ziqiao Ma,Jing Ding,Xuejun Zhang,Dezhi Luo,Jiahe Ding,Sihan Xu,Yuchen Huang,Run Peng,Joyce Chai*

Main category: cs.CL

TL;DR: 论文重新审视了Referring Expression Generation (REG)任务，从语用学角度出发，提出了新数据集RefOI，并揭示了当前视觉语言模型在语用能力上的三大缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前对视觉语言模型(VLMs)的评估忽视了语用维度，将REG简化为基于区域的描述任务，忽略了Gricean准则。

Method: 引入新数据集RefOI（包含1.5k张图像及书面和口头指代表达），并系统评估了最先进的VLMs。

Result: 发现VLMs在语用能力上的三大失败：(1)未能唯一识别指称对象，(2)包含过多无关信息，(3)与人类语用偏好不一致。

Conclusion: 呼吁关注语用学指导的模型和评估框架，以更贴近真实人类交流。

Abstract: Referring Expression Generation (REG) is a core task for evaluating the
pragmatic competence of vision-language systems, requiring not only accurate
semantic grounding but also adherence to principles of cooperative
communication (Grice, 1975). However, current evaluations of vision-language
models (VLMs) often overlook the pragmatic dimension, reducing REG to a
region-based captioning task and neglecting Gricean maxims. In this work, we
revisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of
1.5k images annotated with both written and spoken referring expressions.
Through a systematic evaluation of state-of-the-art VLMs, we identify three key
failures of pragmatic competence: (1) failure to uniquely identify the
referent, (2) inclusion of excessive or irrelevant information, and (3)
misalignment with human pragmatic preference, such as the underuse of minimal
spatial cues. We also show that standard automatic evaluations fail to capture
these pragmatic violations, reinforcing superficial cues rather than genuine
referential success. Our findings call for a renewed focus on pragmatically
informed models and evaluation frameworks that align with real human
communication.

</details>


### [37] [A Python Tool for Reconstructing Full News Text from GDELT](https://arxiv.org/abs/2504.16063)
*A. Fronzetti Colladon,R. Vestrelli*

Main category: cs.CL

TL;DR: 本文提出了一种利用GDELT Web News NGrams 3.0数据集低成本获取全文新闻文章的方法，通过Python代码重建全文，解决了现有新闻数据集的访问限制。


<details>
  <summary>Details</summary>
Motivation: 新闻数据在多个学科中至关重要，但现有数据集通常昂贵或不完整。本文旨在提供一种低成本、高效的解决方案。

Method: 利用GDELT Web News NGrams 3.0数据集，通过Python代码识别并合并重叠的文本片段，重建全文新闻文章。

Result: 该方法成功实现了低成本获取结构化、大规模的新闻数据，克服了现有专有数据集的局限性。

Conclusion: 该方法提高了新闻数据的可访问性，为经济预测、计算社会科学和自然语言处理等应用提供了便利。

Abstract: News data have become an essential resource across various disciplines,
including economics, finance, management, social sciences, and computer
science. Researchers leverage newspaper articles to study economic trends,
market dynamics, corporate strategies, public perception, political discourse,
and the evolution of public opinion. Additionally, news datasets have been
instrumental in training large-scale language models, with applications in
sentiment analysis, fake news detection, and automated news summarization.
Despite their significance, access to comprehensive news corpora remains a key
challenge. Many full-text news providers, such as Factiva and LexisNexis,
require costly subscriptions, while free alternatives often suffer from
incomplete data and transparency issues. This paper presents a novel approach
to obtaining full-text newspaper articles at near-zero cost by leveraging data
from the Global Database of Events, Language, and Tone (GDELT). Specifically,
we focus on the GDELT Web News NGrams 3.0 dataset, which provides
high-frequency updates of n-grams extracted from global online news sources. We
provide Python code to reconstruct full-text articles from these n-grams by
identifying overlapping textual fragments and intelligently merging them. Our
method enables researchers to access structured, large-scale newspaper data for
text analysis while overcoming the limitations of existing proprietary
datasets. The proposed approach enhances the accessibility of news data for
empirical research, facilitating applications in economic forecasting,
computational social science, and natural language processing.

</details>


### [38] [Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation](https://arxiv.org/abs/2504.16073)
*Zhiyuan Hu,Shiyun Xiong,Yifan Zhang,See-Kiong Ng,Anh Tuan Luu,Bo An,Shuicheng Yan,Bryan Hooi*

Main category: cs.CL

TL;DR: 提出了一种通过奖励模型在推理时指导视觉语言模型（VLM）代理的方法，显著提升了GUI导航任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在复杂GUI环境中生成正确动作的能力不足，且现有评估和优化技术存在延迟反馈和局部优化问题。

Method: 在推理时使用奖励模型对VLM代理进行过程监督，优化每一步动作，并结合轨迹反思和重试机制。

Result: 在静态环境中单步动作准确率提升3.4%，动态环境中任务成功率提升约33%。

Conclusion: 该方法有效解决了GUI任务中的性能瓶颈，进一步整合优化机制可带来更大提升。

Abstract: Recent advancements in visual language models (VLMs) have notably enhanced
their capabilities in handling complex Graphical User Interface (GUI)
interaction tasks. Despite these improvements, current frameworks often
struggle to generate correct actions in challenging GUI environments.
State-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source
VLMs for GUI tasks requires significant resources. Additionally, existing
trajectory-level evaluation and refinement techniques frequently fall short due
to delayed feedback and local optimization issues. To address these challenges,
we propose an approach that guides VLM agents with process supervision by a
reward model during GUI navigation and control at inference time. This guidance
allows the VLM agent to optimize actions at each inference step, thereby
improving performance in both static and dynamic environments. In particular,
our method demonstrates significant performance gains in three GUI navigation
tasks, achieving a 3.4% improvement in single step action accuracy for static
environments, along with a around 33% increase in task success rate in one
dynamic environment. With further integration of trajectory reflection and
retry mechanisms, we also demonstrate even greater enhancement in task success.

</details>


### [39] [PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models](https://arxiv.org/abs/2504.16074)
*Shi Qiu,Shaoyang Guo,Zhuo-Yang Song,Yunbo Sun,Zeyu Cai,Jiashen Wei,Tianyu Luo,Yixuan Yin,Haoxu Zhang,Yi Hu,Chenyang Wang,Chencheng Tang,Haoling Chang,Qi Liu,Ziheng Zhou,Tianyu Zhang,Jingtian Zhang,Zhangyi Liu,Minghao Li,Yuku Zhang,Boxuan Jing,Xianqi Yin,Yutong Ren,Zizhuo Fu,Weike Wang,Xudong Tian,Anqi Lv,Laifu Man,Jianxiang Li,Feiyu Tao,Qihua Sun,Zhou Liang,Yushu Mu,Zhongxuan Li,Jing-Jun Zhang,Shutao Zhang,Xiaotian Li,Xingqi Xia,Jiawei Lin,Zheyu Shen,Jiahang Chen,Qiuhao Xiong,Binran Wang,Fengyuan Wang,Ziyang Ni,Bohan Zhang,Fan Cui,Changkun Shao,Qing-Hong Cao,Ming-xing Luo,Muhan Zhang,Hua Xing Zhu*

Main category: cs.CL

TL;DR: PHYBench是一个用于评估大语言模型在物理场景中推理能力的高质量基准，包含500个精心设计的物理问题，并提出了一种新的评估指标EED Score。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在复杂物理推理任务中表现不足，需要一种更全面的评估工具来揭示其局限性。

Method: PHYBench包含500个基于真实物理场景的问题，覆盖多个物理领域和难度级别，并提出了EED Score作为评估指标。

Result: 实验表明，即使是当前最先进的推理模型，在复杂物理推理任务中仍显著落后于人类专家。

Conclusion: PHYBench揭示了现有模型的局限性，并为进一步改进提供了方向，基准数据和结果已公开。

Abstract: We introduce PHYBench, a novel, high-quality benchmark designed for
evaluating reasoning capabilities of large language models (LLMs) in physical
contexts. PHYBench consists of 500 meticulously curated physics problems based
on real-world physical scenarios, designed to assess the ability of models to
understand and reason about realistic physical processes. Covering mechanics,
electromagnetism, thermodynamics, optics, modern physics, and advanced physics,
the benchmark spans difficulty levels from high school exercises to
undergraduate problems and Physics Olympiad challenges. Additionally, we
propose the Expression Edit Distance (EED) Score, a novel evaluation metric
based on the edit distance between mathematical expressions, which effectively
captures differences in model reasoning processes and results beyond
traditional binary scoring methods. We evaluate various LLMs on PHYBench and
compare their performance with human experts. Our results reveal that even
state-of-the-art reasoning models significantly lag behind human experts,
highlighting their limitations and the need for improvement in complex physical
reasoning scenarios. Our benchmark results and dataset are publicly available
at https://phybench-official.github.io/phybench-demo/.

</details>


### [40] [TTRL: Test-Time Reinforcement Learning](https://arxiv.org/abs/2504.16084)
*Yuxin Zuo,Kaiyan Zhang,Shang Qu,Li Sheng,Xuekai Zhu,Biqing Qi,Youbang Sun,Ganqu Cui,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: 论文提出了一种名为TTRL的新方法，利用无标签数据通过强化学习训练大语言模型，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在无标签数据上进行强化学习时奖励估计的挑战，尤其是在推理阶段缺乏真实标签的情况下。

Method: 方法是通过测试时扩展（TTS）中的多数投票等实践生成有效奖励，驱动强化学习训练，利用预训练模型的先验知识实现自我进化。

Result: 实验结果显示，TTRL在多项任务和模型上均表现优异，例如将Qwen-2.5-Math-7B在AIME 2024上的pass@1性能提升了约159%。

Conclusion: 结论是TTRL在无标签数据上表现出色，能够接近甚至超越有标签数据训练的模型性能，展示了其广泛应用的潜力。

Abstract: This paper investigates Reinforcement Learning (RL) on data without explicit
labels for reasoning tasks in Large Language Models (LLMs). The core challenge
of the problem is reward estimation during inference while not having access to
ground-truth information. While this setting appears elusive, we find that
common practices in Test-Time Scaling (TTS), such as majority voting, yield
surprisingly effective rewards suitable for driving RL training. In this work,
we introduce Test-Time Reinforcement Learning (TTRL), a novel method for
training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs
by utilizing the priors in the pre-trained models. Our experiments demonstrate
that TTRL consistently improves performance across a variety of tasks and
models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by
approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,
although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated
performance to consistently surpass the upper limit of the initial model, and
approach the performance of models trained directly on test data with
ground-truth labels. Our experimental findings validate the general
effectiveness of TTRL across various tasks, and highlight TTRL's potential for
broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [41] [LLM-Enabled Style and Content Regularization for Personalized Text-to-Image Generation](https://arxiv.org/abs/2504.15309)
*Anran Yu,Wei Feng,Yaochen Zhang,Xiang Li,Lei Meng,Lei Wu,Xiangxu Meng*

Main category: cs.CV

TL;DR: 提出风格细化和内容保留策略，优化个性化文本到图像生成，解决现有方法在风格化和内容准确性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过微调模型嵌入标识符，常因文本控制性降低导致风格化不足和内容不准确。

Method: 风格细化策略利用视觉推理提示和参考图像的语义信息优化风格嵌入；内容保留策略通过保留模型泛化能力解决内容偏差问题。

Result: 实验表明，该方法在生成一致且个性化的文本到图像输出上表现优越。

Conclusion: 提出的策略有效提升了风格化和文本控制性，生成结果更精准一致。

Abstract: The personalized text-to-image generation has rapidly advanced with the
emergence of Stable Diffusion. Existing methods, which typically fine-tune
models using embedded identifiers, often struggle with insufficient stylization
and inaccurate image content due to reduced textual controllability. In this
paper, we propose style refinement and content preservation strategies. The
style refinement strategy leverages the semantic information of visual
reasoning prompts and reference images to optimize style embeddings, allowing a
more precise and consistent representation of style information. The content
preservation strategy addresses the content bias problem by preserving the
model's generalization capabilities, ensuring enhanced textual controllability
without compromising stylization. Experimental results verify that our approach
achieves superior performance in generating consistent and personalized
text-to-image outputs.

</details>


### [42] [LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception](https://arxiv.org/abs/2504.15362)
*Yuan-Hong Liao,Sven Elflein,Liu He,Laura Leal-Taixé,Yejin Choi,Sanja Fidler,David Acuna*

Main category: cs.CV

TL;DR: 论文提出了一种名为LongPerceptualThoughts的新数据集，用于探索长链思维在感知任务中的作用，并通过三阶段框架生成数据，显著提升了视觉推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 研究长链思维在感知任务中的潜在作用，弥补现有模型在此类任务中缺乏深入推理能力的不足。

Method: 提出三阶段数据合成框架：1）从密集图像描述生成可验证的多选题；2）从视觉语言模型中提取简单思维链；3）通过前沿推理模型扩展为长链思维。

Result: 实验表明，生成的模型在5个视觉基准测试中平均提升3.4分，其中V$^*$ Bench提升11.8分，同时在文本推理基准MMLU-Pro上也有2分提升。

Conclusion: LongPerceptualThoughts数据集和合成框架有效提升了感知任务的推理能力，并展示了跨领域性能改进的潜力。

Abstract: Recent reasoning models through test-time scaling have demonstrated that long
chain-of-thoughts can unlock substantial performance boosts in hard reasoning
tasks such as math and code. However, the benefit of such long thoughts for
system-2 reasoning is relatively less explored in other domains such as
perceptual tasks where shallower, system-1 reasoning seems sufficient. In this
paper, we introduce LongPerceptualThoughts, a new synthetic dataset with 30K
long-thought traces for perceptual tasks. The key challenges in synthesizing
elaborate reasoning thoughts for perceptual tasks are that off-the-shelf models
are not yet equipped with such thinking behavior and that it is not
straightforward to build a reliable process verifier for perceptual tasks.
Thus, we propose a novel three-stage data synthesis framework that first
synthesizes verifiable multiple-choice questions from dense image descriptions,
then extracts simple CoTs from VLMs for those verifiable problems, and finally
expands those simple thoughts to elaborate long thoughts via frontier reasoning
models. In controlled experiments with a strong instruction-tuned 7B model, we
demonstrate notable improvements over existing visual reasoning data-generation
methods. Our model, trained on the generated dataset, achieves an average +3.4
points improvement over 5 vision-centric benchmarks, including +11.8 points on
V$^*$ Bench. Notably, despite being tuned for vision tasks, it also improves
performance on the text reasoning benchmark, MMLU-Pro, by +2 points.

</details>


### [43] [Event2Vec: Processing neuromorphic events directly by representations in vector space](https://arxiv.org/abs/2504.15371)
*Wei Fang,Priyadarshini Panda*

Main category: cs.CV

TL;DR: 论文提出了一种名为event2vec的事件相机数据表示方法，解决了事件数据与传统计算机视觉方法不兼容的问题，并在ASL-DVS数据集上验证了其高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有高时间分辨率、低功耗和宽动态范围的优势，但其输出的异步、稀疏、不规则事件数据与主流计算机视觉和深度学习方法不兼容。

Method: 受词向量（word2vec）启发，作者提出了事件向量（event2vec）表示方法，将事件数据映射到向量空间。

Result: 在ASL-DVS数据集分类任务中，event2vec表现出优于传统图/图像/体素表示方法的参数效率、准确性和速度。

Conclusion: event2vec不仅性能优越，还能将事件数据与自然语言处理领域对齐，为事件数据融入大型语言和多模态模型提供了前景。

Abstract: The neuromorphic event cameras have overwhelming advantages in temporal
resolution, power efficiency, and dynamic range compared to traditional
cameras. However, the event cameras output asynchronous, sparse, and irregular
events, which are not compatible with mainstream computer vision and deep
learning methods. Various methods have been proposed to solve this issue but at
the cost of long preprocessing procedures, losing temporal resolutions, or
being incompatible with massively parallel computation. Inspired by the great
success of the word to vector, we summarize the similarities between words and
events, then propose the first event to vector (event2vec) representation. We
validate event2vec on classifying the ASL-DVS dataset, showing impressive
parameter efficiency, accuracy, and speed than previous graph/image/voxel-based
representations. Beyond task performance, the most attractive advantage of
event2vec is that it aligns events to the domain of natural language
processing, showing the promising prospect of integrating events into large
language and multimodal models. Our codes, models, and training logs are
available at https://github.com/fangwei123456/event2vec.

</details>


### [44] [Towards Understanding Camera Motions in Any Video](https://arxiv.org/abs/2504.15376)
*Zhiqiu Lin,Siyuan Cen,Daniel Jiang,Jay Karhade,Hewei Wang,Chancharik Mitra,Tiffany Ling,Yuhan Huang,Sifan Liu,Mingyu Chen,Rushikesh Zawar,Xue Bai,Yilun Du,Chuang Gan,Deva Ramanan*

Main category: cs.CV

TL;DR: CameraBench是一个用于评估和改进相机运动理解的大规模数据集和基准测试，包含3000个多样化视频，标注了相机运动基元。研究发现专家标注和训练能显著提升准确性，并展示了生成式视频语言模型的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 当前对相机运动的理解缺乏系统化的评估工具，CameraBench旨在填补这一空白，推动相机运动理解的研究。

Method: 构建了包含3000个视频的数据集，设计了相机运动基元的分类法，并通过专家标注和训练提升标注质量。评估了SfM和VLM模型的表现，并微调生成式VLM。

Result: SfM模型难以捕捉依赖场景内容的语义基元，而VLM模型在几何基元上表现不佳。微调后的VLM结合了两者的优势，展示了多种应用潜力。

Conclusion: CameraBench为相机运动理解提供了新的工具和方向，未来有望推动更广泛的视频分析应用。

Abstract: We introduce CameraBench, a large-scale dataset and benchmark designed to
assess and improve camera motion understanding. CameraBench consists of ~3,000
diverse internet videos, annotated by experts through a rigorous multi-stage
quality control process. One of our contributions is a taxonomy of camera
motion primitives, designed in collaboration with cinematographers. We find,
for example, that some motions like "follow" (or tracking) require
understanding scene content like moving subjects. We conduct a large-scale
human study to quantify human annotation performance, revealing that domain
expertise and tutorial-based training can significantly enhance accuracy. For
example, a novice may confuse zoom-in (a change of intrinsics) with translating
forward (a change of extrinsics), but can be trained to differentiate the two.
Using CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language
Models (VLMs), finding that SfM models struggle to capture semantic primitives
that depend on scene content, while VLMs struggle to capture geometric
primitives that require precise estimation of trajectories. We then fine-tune a
generative VLM on CameraBench to achieve the best of both worlds and showcase
its applications, including motion-augmented captioning, video question
answering, and video-text retrieval. We hope our taxonomy, benchmark, and
tutorials will drive future efforts towards the ultimate goal of understanding
camera motions in any video.

</details>


### [45] [Physics Driven Image Simulation from Commercial Satellite Imagery](https://arxiv.org/abs/2504.15378)
*Scott Sorensen,Wayne Treible,Robert Wagner,Andrew D. Gilliam,Todd Rovito,Joseph L. Mundy*

Main category: cs.CV

TL;DR: 利用卫星图像自动生成物理真实的3D场景，无需激光雷达，提高仿真效率和保真度。


<details>
  <summary>Details</summary>
Motivation: 通过物理驱动的图像仿真，创建超越传统渲染管道的真实场景，减少手动操作并支持算法开发。

Method: 基于数字表面模型（DSM）构建场景几何，利用卫星图像建模地形、人造结构和动态元素。

Result: 实现了无需激光雷达的高保真3D场景仿真，适用于从紫外到长波红外的多种波段。

Conclusion: 该方法为地球新区域的仿真提供了高效、高保真的解决方案，支持算法和图像处理流程的开发。

Abstract: Physics driven image simulation allows for the modeling and creation of
realistic imagery beyond what is afforded by typical rendering pipelines. We
aim to automatically generate a physically realistic scene for simulation of a
given region using satellite imagery to model the scene geometry, drive
material estimates, and populate the scene with dynamic elements. We present
automated techniques to utilize satellite imagery throughout the simulated
scene to expedite scene construction and decrease manual overhead. Our
technique does not use lidar, enabling simulations that could not be
constructed previously. To develop a 3D scene, we model the various components
of the real location, addressing the terrain, modelling man-made structures,
and populating the scene with smaller elements such as vegetation and vehicles.
To create the scene we begin with a Digital Surface Model, which serves as the
basis for scene geometry, and allows us to reason about the real location in a
common 3D frame of reference. These simulated scenes can provide increased
fidelity with less manual intervention for novel locations on earth, and can
facilitate algorithm development, and processing pipelines for imagery ranging
from UV to LWIR $(200nm-20\mu m)$.

</details>


### [46] [Plug-and-Play Versatile Compressed Video Enhancement](https://arxiv.org/abs/2504.15380)
*Huimin Zeng,Jiacheng Li,Zhiwei Xiong*

Main category: cs.CV

TL;DR: 提出了一种基于编解码器信息的视频增强框架，通过复用编解码信息自适应提升不同压缩设置下的视频质量，支持多种下游视觉任务。


<details>
  <summary>Details</summary>
Motivation: 视频压缩虽减少文件大小，但会降低视觉质量，影响下游视觉模型的鲁棒性。

Method: 框架包含压缩感知适应网络（CAA）和比特流感知增强网络（BAE），利用编解码信息自适应增强视频质量。

Result: 实验表明，该框架在质量增强和辅助下游任务方面优于现有方法。

Conclusion: 该框架作为一种即插即用模块，能有效提升压缩视频质量并支持多种视觉任务。

Abstract: As a widely adopted technique in data transmission, video compression
effectively reduces the size of files, making it possible for real-time cloud
computing. However, it comes at the cost of visual quality, posing challenges
to the robustness of downstream vision models. In this work, we present a
versatile codec-aware enhancement framework that reuses codec information to
adaptively enhance videos under different compression settings, assisting
various downstream vision tasks without introducing computation bottleneck.
Specifically, the proposed codec-aware framework consists of a
compression-aware adaptation (CAA) network that employs a hierarchical
adaptation mechanism to estimate parameters of the frame-wise enhancement
network, namely the bitstream-aware enhancement (BAE) network. The BAE network
further leverages temporal and spatial priors embedded in the bitstream to
effectively improve the quality of compressed input frames. Extensive
experimental results demonstrate the superior quality enhancement performance
of our framework over existing enhancement methods, as well as its versatility
in assisting multiple downstream tasks on compressed videos as a plug-and-play
module. Code and models are available at
https://huimin-zeng.github.io/PnP-VCVE/.

</details>


### [47] [ICGM-FRAX: Iterative Cross Graph Matching for Hip Fracture Risk Assessment using Dual-energy X-ray Absorptiometry Images](https://arxiv.org/abs/2504.15384)
*Chen Zhao,Anjum Shaik,Joyce H. Keyak,Nancy E. Lane,Jeffrey D. Deng,Kuan-Jui Su,Qiuying Sha,Hui Shen,Hong-Wen Deng,Weihua Zhou*

Main category: cs.CV

TL;DR: ICGM-FRAX是一种基于DXA图像的图匹配方法，用于预测髋部骨折风险，通过比较测试图与模板图的相似性实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 髋部骨折对老年人健康影响重大，早期准确识别高风险个体对干预至关重要。

Method: 将DXA图像分割为多个感兴趣区域（RoIs），提取放射组学特征构建图，通过迭代图匹配评估骨折风险。

Result: 在UK Biobank数据集上验证，ICGM-FRAX的敏感度达0.9869，预测精度高。

Conclusion: ICGM-FRAX是一种有效的髋部骨折风险预测方法，具有临床潜力。

Abstract: Hip fractures represent a major health concern, particularly among the
elderly, often leading decreased mobility and increased mortality. Early and
accurate detection of at risk individuals is crucial for effective
intervention. In this study, we propose Iterative Cross Graph Matching for Hip
Fracture Risk Assessment (ICGM-FRAX), a novel approach for predicting hip
fractures using Dual-energy X-ray Absorptiometry (DXA) images. ICGM-FRAX
involves iteratively comparing a test (subject) graph with multiple template
graphs representing the characteristics of hip fracture subjects to assess the
similarity and accurately to predict hip fracture risk. These graphs are
obtained as follows. The DXA images are separated into multiple regions of
interest (RoIs), such as the femoral head, shaft, and lesser trochanter.
Radiomic features are then calculated for each RoI, with the central
coordinates used as nodes in a graph. The connectivity between nodes is
established according to the Euclidean distance between these coordinates. This
process transforms each DXA image into a graph, where each node represents a
RoI, and edges derived by the centroids of RoIs capture the spatial
relationships between them. If the test graph closely matches a set of template
graphs representing subjects with incident hip fractures, it is classified as
indicating high hip fracture risk. We evaluated our method using 547 subjects
from the UK Biobank dataset, and experimental results show that ICGM-FRAX
achieved a sensitivity of 0.9869, demonstrating high accuracy in predicting hip
fractures.

</details>


### [48] [MirrorVerse: Pushing Diffusion Models to Realistically Reflect the World](https://arxiv.org/abs/2504.15397)
*Ankit Dhiman,Manan Shah,R Venkatesh Babu*

Main category: cs.CV

TL;DR: 提出了一种改进扩散模型生成镜面反射的方法，通过数据增强和训练策略提升真实感。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成镜面反射时难以完全遵循物理规律，尤其是在物体位置和方向变化时表现不佳。

Method: 引入随机物体定位、旋转和固定等数据增强技术，并采用三阶段训练课程提升模型性能。

Result: 提出的MirrorFusion 2.0模型在复杂场景中表现更优，支持多物体空间关系和遮挡。

Conclusion: 通过数据增强和训练策略改进，显著提升了镜面反射生成的真实感和泛化能力。

Abstract: Diffusion models have become central to various image editing tasks, yet they
often fail to fully adhere to physical laws, particularly with effects like
shadows, reflections, and occlusions. In this work, we address the challenge of
generating photorealistic mirror reflections using diffusion-based generative
models. Despite extensive training data, existing diffusion models frequently
overlook the nuanced details crucial to authentic mirror reflections. Recent
approaches have attempted to resolve this by creating synhetic datasets and
framing reflection generation as an inpainting task; however, they struggle to
generalize across different object orientations and positions relative to the
mirror. Our method overcomes these limitations by introducing key augmentations
into the synthetic data pipeline: (1) random object positioning, (2) randomized
rotations, and (3) grounding of objects, significantly enhancing generalization
across poses and placements. To further address spatial relationships and
occlusions in scenes with multiple objects, we implement a strategy to pair
objects during dataset generation, resulting in a dataset robust enough to
handle these complex scenarios. Achieving generalization to real-world scenes
remains a challenge, so we introduce a three-stage training curriculum to
develop the MirrorFusion 2.0 model to improve real-world performance. We
provide extensive qualitative and quantitative evaluations to support our
approach. The project page is available at: https://mirror-verse.github.io/.

</details>


### [49] [Context Aware Grounded Teacher for Source Free Object Detection](https://arxiv.org/abs/2504.15404)
*Tajamul Ashraf,Rajes Manna,Partha Sarathi Purkayastha,Tavaheed Tariq,Janibul Bashir*

Main category: cs.CV

TL;DR: 论文提出Grounded Teacher（GT）框架，解决无源数据目标检测（SFOD）中的上下文偏差问题，通过关系上下文模块和专家基础分支提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 在无源数据的目标检测中，教师模型的偏差会导致伪标签不准确，影响学生模型性能。本文旨在解决上下文偏差和性能下降问题。

Method: 提出GT框架，利用关系上下文模块建模上下文关系，并通过专家基础分支监督学生模型。

Result: 在三个医学数据集上验证了GT框架的有效性，显著减少了上下文偏差并提升了性能。

Conclusion: GT框架在SFOD设置下能有效缓解上下文偏差，提升模型性能，相关资源已开源。

Abstract: We focus on the Source Free Object Detection (SFOD) problem, when source data
is unavailable during adaptation, and the model must adapt to the unlabeled
target domain. In medical imaging, several approaches have leveraged a
semi-supervised student-teacher architecture to bridge domain discrepancy.
Context imbalance in labeled training data and significant domain shifts
between domains can lead to biased teacher models that produce inaccurate
pseudolabels, degrading the student model's performance and causing a mode
collapse. Class imbalance, particularly when one class significantly outnumbers
another, leads to contextual bias. To tackle the problem of context bias and
the significant performance drop of the student model in the SFOD setting, we
introduce Grounded Teacher (GT) as a standard framework. In this study, we
model contextual relationships using a dedicated relational context module and
leverage it to mitigate inherent biases in the model. This approach enables us
to apply augmentations to closely related classes, across and within domains,
enhancing the performance of underrepresented classes while keeping the effect
on dominant classes minimal. We further improve the quality of predictions by
implementing an expert foundational branch to supervise the student model. We
validate the effectiveness of our approach in mitigating context bias under the
SFOD setting through experiments on three medical datasets supported by
comprehensive ablation studies. All relevant resources, including preprocessed
data, trained model weights, and code, are publicly available at this
https://github.com/Tajamul21/Grounded_Teacher.

</details>


### [50] [IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs](https://arxiv.org/abs/2504.15415)
*David Ma,Yuanxing Zhang,Jincheng Ren,Jarvis Guo,Yifan Yao,Zhenlin Wei,Zhenzhu Yang,Zhongyuan Peng,Boyu Feng,Jun Ma,Xiao Gu,Zhoufutu Wen,King Zhu,Yancheng He,Meng Cao,Shiwen Ni,Jiaheng Liu,Wenhao Huang,Ge Zhang,Xiaojie Jin*

Main category: cs.CV

TL;DR: IV-Bench是一个新的多模态大语言模型（MLLM）评估基准，专注于图像背景在视频理解中的作用，填补了现有评估框架的空白。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架主要关注图像推理或通用视频理解任务，忽视了图像背景在视频理解中的重要性。

Method: 提出了IV-Bench，包含967个视频和2,585个标注的图像-文本查询，覆盖13个任务和5个类别。评估了开源和闭源MLLM的性能。

Result: 当前模型在图像背景视频感知和推理任务中表现不佳，最高准确率仅为28.9%。关键影响因素包括推理模式、帧数和分辨率。

Conclusion: IV-Bench揭示了当前模型的局限性，并为未来研究提供了重要见解。

Abstract: Existing evaluation frameworks for Multimodal Large Language Models (MLLMs)
primarily focus on image reasoning or general video understanding tasks,
largely overlooking the significant role of image context in video
comprehension. To bridge this gap, we propose IV-Bench, the first comprehensive
benchmark for evaluating Image-Grounded Video Perception and Reasoning.
IV-Bench consists of 967 videos paired with 2,585 meticulously annotated
image-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5
representative categories. Extensive evaluations of state-of-the-art
open-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o,
Gemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models
substantially underperform in image-grounded video Perception and Reasoning,
merely achieving at most 28.9% accuracy. Further analysis reveals key factors
influencing model performance on IV-Bench, including inference pattern, frame
number, and resolution. Additionally, through a simple data synthesis approach,
we demonstratethe challenges of IV- Bench extend beyond merely aligning the
data format in the training proecss. These findings collectively provide
valuable insights for future research. Our codes and data are released in
https://github.com/multimodal-art-projection/IV-Bench.

</details>


### [51] [Manifold Induced Biases for Zero-shot and Few-shot Detection of Generated Images](https://arxiv.org/abs/2504.15470)
*Jonathan Brokman,Amit Giloni,Omer Hofman,Roman Vainshtein,Hisashi Kojima,Guy Gilboa*

Main category: cs.CV

TL;DR: 论文提出了一种基于生成内容偏见的零样本和小样本图像检测方法，通过分析隐式概率流形的偏向来改进检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有零样本和小样本图像检测方法缺乏理论依据和性能不足的问题。

Method: 利用预训练扩散模型分析隐式概率流形的偏见，并通过评分函数近似曲率、梯度和偏向性，为零样本检测提供标准；在小样本场景中采用专家混合方法。

Result: 在20种生成模型上的实验表明，该方法在零样本和小样本设置中均优于现有方法。

Conclusion: 通过流形分析，该研究提升了生成内容偏见的理论理解和实际应用。

Abstract: Distinguishing between real and AI-generated images, commonly referred to as
'image detection', presents a timely and significant challenge. Despite
extensive research in the (semi-)supervised regime, zero-shot and few-shot
solutions have only recently emerged as promising alternatives. Their main
advantage is in alleviating the ongoing data maintenance, which quickly becomes
outdated due to advances in generative technologies. We identify two main gaps:
(1) a lack of theoretical grounding for the methods, and (2) significant room
for performance improvements in zero-shot and few-shot regimes. Our approach is
founded on understanding and quantifying the biases inherent in generated
content, where we use these quantities as criteria for characterizing generated
images. Specifically, we explore the biases of the implicit probability
manifold, captured by a pre-trained diffusion model. Through score-function
analysis, we approximate the curvature, gradient, and bias towards points on
the probability manifold, establishing criteria for detection in the zero-shot
regime. We further extend our contribution to the few-shot setting by employing
a mixture-of-experts methodology. Empirical results across 20 generative models
demonstrate that our method outperforms current approaches in both zero-shot
and few-shot settings. This work advances the theoretical understanding and
practical usage of generated content biases through the lens of manifold
analysis.

</details>


### [52] [Emergence and Evolution of Interpretable Concepts in Diffusion Models](https://arxiv.org/abs/2504.15473)
*Berk Tinaz,Zalan Fabian,Mahdi Soltanolkotabi*

Main category: cs.CV

TL;DR: 该论文利用稀疏自编码器（SAEs）研究文本到图像扩散模型的内在机制，揭示了可解释的概念及其对生成过程的因果影响，并提出了干预技术以控制图像生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本到图像生成中表现出色，但其内部机制仍不明确。研究旨在通过MI技术（如SAEs）揭示扩散模型的工作原理，并探索如何干预生成过程。

Method: 采用稀疏自编码器（SAEs）分析流行文本到图像扩散模型的激活，识别可解释概念，并通过干预技术验证其因果效应。

Result: 研究发现扩散模型早期阶段可预测场景构图，中期可干预风格，后期仅能调整细节。干预技术能有效控制生成过程。

Conclusion: SAEs成功揭示了扩散模型的可解释概念，并展示了其在生成过程中的因果作用，为理解和控制扩散模型提供了新方法。

Abstract: Diffusion models have become the go-to method for text-to-image generation,
producing high-quality images from noise through a process called reverse
diffusion. Understanding the dynamics of the reverse diffusion process is
crucial in steering the generation and achieving high sample quality. However,
the inner workings of diffusion models is still largely a mystery due to their
black-box nature and complex, multi-step generation process. Mechanistic
Interpretability (MI) techniques, such as Sparse Autoencoders (SAEs), aim at
uncovering the operating principles of models through granular analysis of
their internal representations. These MI techniques have been successful in
understanding and steering the behavior of large language models at scale.
However, the great potential of SAEs has not yet been applied toward gaining
insight into the intricate generative process of diffusion models. In this
work, we leverage the SAE framework to probe the inner workings of a popular
text-to-image diffusion model, and uncover a variety of human-interpretable
concepts in its activations. Interestingly, we find that even before the first
reverse diffusion step is completed, the final composition of the scene can be
predicted surprisingly well by looking at the spatial distribution of activated
concepts. Moreover, going beyond correlational analysis, we show that the
discovered concepts have a causal effect on the model output and can be
leveraged to steer the generative process. We design intervention techniques
aimed at manipulating image composition and style, and demonstrate that (1) in
early stages of diffusion image composition can be effectively controlled, (2)
in the middle stages of diffusion image composition is finalized, however
stylistic interventions are effective, and (3) in the final stages of diffusion
only minor textural details are subject to change.

</details>


### [53] [CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting](https://arxiv.org/abs/2504.15485)
*Atin Pothiraj,Elias Stengel-Eskin,Jaemin Cho,Mohit Bansal*

Main category: cs.CV

TL;DR: 论文提出了一项新任务CAPTURe，用于测试视觉语言模型（VLMs）对遮挡物体的计数和推理能力，发现现有模型在遮挡场景下表现较差，而人类表现优异。


<details>
  <summary>Details</summary>
Motivation: 遮挡在现实场景中常见，但现有模型对遮挡物体的理解和推理能力不足，因此需要新的测试任务来评估和改进模型。

Method: 提出CAPTURe任务，分为CAPTURe-real（真实图像）和CAPTURe-synthetic（合成图像），评估了四种VLMs（如GPT-4o）的表现。

Result: 模型在遮挡和非遮挡场景下均表现不佳，遮挡下更差；人类表现优异；提供遮挡物体位置信息可提升模型性能。

Conclusion: VLMs在遮挡推理和计数能力上存在不足，需进一步改进。

Abstract: Recognizing and reasoning about occluded (partially or fully hidden) objects
is vital to understanding visual scenes, as occlusions frequently occur in
real-world environments and act as obstacles for spatial comprehension. To test
models' ability to reason about multiple occluded objects, we introduce a novel
task, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which
requires a model to count objects arranged in a pattern by inferring how the
pattern continues behind an occluder (an object which blocks parts of the
scene). CAPTURe requires both recognizing visual patterns and reasoning, making
it a useful testbed for evaluating vision-language models (VLMs) on whether
they understand occluded patterns and possess spatial understanding skills. By
requiring models to reason about occluded objects, CAPTURe also tests VLMs'
ability to form world models that would allow them to fill in missing
information. CAPTURe consists of two parts: (1) CAPTURe-real, with manually
filtered images of real objects in patterns and (2) CAPTURe-synthetic, a
controlled diagnostic with generated patterned images. We evaluate four strong
VLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models
struggle to count on both occluded and unoccluded patterns. Crucially, we find
that models perform worse with occlusion, suggesting that VLMs are also
deficient in inferring unseen spatial relationships: even the strongest VLMs
like GPT-4o fail to count with occlusion. In contrast, we find that humans
achieve very little error on CAPTURe. We also find that providing auxiliary
information of occluded object locations increases performance, underscoring
that the model error comes both from an inability to handle occlusion as well
as difficulty counting in images.

</details>


### [54] [InstaRevive: One-Step Image Enhancement via Dynamic Score Matching](https://arxiv.org/abs/2504.15513)
*Yixuan Zhu,Haolin Wang,Ao Li,Wenliang Zhao,Yansong Tang,Jingxuan Niu,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: InstaRevive是一个基于扩散蒸馏的图像增强框架，通过动态控制和文本提示减少采样步骤，提升生成效率和质量。


<details>
  <summary>Details</summary>
Motivation: 复杂环境和成像设备限制导致图像增强需求广泛，现有扩散方法计算成本高。

Method: 采用分数扩散蒸馏和动态控制策略，结合文本提示辅助条件。

Result: 在多种任务和数据集上验证了高效性和高质量结果。

Conclusion: InstaRevive在图像增强中表现出高效和视觉吸引力。

Abstract: Image enhancement finds wide-ranging applications in real-world scenarios due
to complex environments and the inherent limitations of imaging devices. Recent
diffusion-based methods yield promising outcomes but necessitate prolonged and
computationally intensive iterative sampling. In response, we propose
InstaRevive, a straightforward yet powerful image enhancement framework that
employs score-based diffusion distillation to harness potent generative
capability and minimize the sampling steps. To fully exploit the potential of
the pre-trained diffusion model, we devise a practical and effective diffusion
distillation pipeline using dynamic control to address inaccuracies in updating
direction during score matching. Our control strategy enables a dynamic
diffusing scope, facilitating precise learning of denoising trajectories within
the diffusion model and ensuring accurate distribution matching gradients
during training. Additionally, to enrich guidance for the generative power, we
incorporate textual prompts via image captioning as auxiliary conditions,
fostering further exploration of the diffusion model. Extensive experiments
substantiate the efficacy of our framework across a diverse array of
challenging tasks and datasets, unveiling the compelling efficacy and
efficiency of InstaRevive in delivering high-quality and visually appealing
results. Code is available at https://github.com/EternalEvan/InstaRevive.

</details>


### [55] [Multi-Modal Fusion of In-Situ Video Data and Process Parameters for Online Forecasting of Cookie Drying Readiness](https://arxiv.org/abs/2504.15599)
*Shichen Li,Chenhui Shao*

Main category: cs.CV

TL;DR: 提出了一种多模态数据融合框架，用于实时预测食品干燥状态，显著提升了预测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 食品干燥对生产和运输至关重要，但实时预测干燥状态存在挑战，如动态性、数据不足和缺乏有效方法。

Method: 采用端到端多模态数据融合框架，结合视频数据和过程参数，使用编码器-解码器架构和基于Transformer的解码器。

Result: 模型在糖饼干干燥实验中平均预测误差仅15秒，优于现有方法65.69%和纯视频模型11.30%。

Conclusion: 该模型在精度、规模和效率上表现优异，适用于多种工业多模态融合任务。

Abstract: Food drying is essential for food production, extending shelf life, and
reducing transportation costs. Accurate real-time forecasting of drying
readiness is crucial for minimizing energy consumption, improving productivity,
and ensuring product quality. However, this remains challenging due to the
dynamic nature of drying, limited data availability, and the lack of effective
predictive analytical methods. To address this gap, we propose an end-to-end
multi-modal data fusion framework that integrates in-situ video data with
process parameters for real-time food drying readiness forecasting. Our
approach leverages a new encoder-decoder architecture with modality-specific
encoders and a transformer-based decoder to effectively extract features while
preserving the unique structure of each modality. We apply our approach to
sugar cookie drying, where time-to-ready is predicted at each timestamp.
Experimental results demonstrate that our model achieves an average prediction
error of only 15 seconds, outperforming state-of-the-art data fusion methods by
65.69% and a video-only model by 11.30%. Additionally, our model balances
prediction accuracy, model size, and computational efficiency, making it
well-suited for heterogenous industrial datasets. The proposed model is
extensible to various other industrial modality fusion tasks for online
decision-making.

</details>


### [56] [SonarT165: A Large-scale Benchmark and STFTrack Framework for Acoustic Object Tracking](https://arxiv.org/abs/2504.15609)
*Yunfeng Li,Bo Wang,Jiahao Wan,Xueyi Wu,Ye Li*

Main category: cs.CV

TL;DR: 论文提出了首个大规模水下声学目标跟踪（UAOT）基准SonarT165，并提出了高效框架STFTrack，包含多视角模板融合模块和最优轨迹校正模块，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 水下能见度不足时，仅声纳系统能提供稳定数据，但缺乏统一评估基准限制了现有方法的实用性。

Method: 提出SonarT165基准，包含165个方形序列和165个扇形序列及205K标注；提出STFTrack框架，含多视角模板融合模块（MTFM）和最优轨迹校正模块（OTCM），并引入声学图像增强和频率增强模块（FEM）。

Result: STFTrack在SonarT165基准上达到最先进性能。

Conclusion: SonarT165基准和STFTrack框架为水下声学目标跟踪提供了有效解决方案，解决了现有方法的局限性。

Abstract: Underwater observation systems typically integrate optical cameras and
imaging sonar systems. When underwater visibility is insufficient, only sonar
systems can provide stable data, which necessitates exploration of the
underwater acoustic object tracking (UAOT) task. Previous studies have explored
traditional methods and Siamese networks for UAOT. However, the absence of a
unified evaluation benchmark has significantly constrained the value of these
methods. To alleviate this limitation, we propose the first large-scale UAOT
benchmark, SonarT165, comprising 165 square sequences, 165 fan sequences, and
205K high-quality annotations. Experimental results demonstrate that SonarT165
reveals limitations in current state-of-the-art SOT trackers. To address these
limitations, we propose STFTrack, an efficient framework for acoustic object
tracking. It includes two novel modules, a multi-view template fusion module
(MTFM) and an optimal trajectory correction module (OTCM). The MTFM module
integrates multi-view feature of both the original image and the binary image
of the dynamic template, and introduces a cross-attention-like layer to fuse
the spatio-temporal target representations. The OTCM module introduces the
acoustic-response-equivalent pixel property and proposes normalized pixel
brightness response scores, thereby suppressing suboptimal matches caused by
inaccurate Kalman filter prediction boxes. To further improve the model
feature, STFTrack introduces a acoustic image enhancement method and a
Frequency Enhancement Module (FEM) into its tracking pipeline. Comprehensive
experiments show the proposed STFTrack achieves state-of-the-art performance on
the proposed benchmark. The code is available at
https://github.com/LiYunfengLYF/SonarT165.

</details>


### [57] [HS-Mamba: Full-Field Interaction Multi-Groups Mamba for Hyperspectral Image Classification](https://arxiv.org/abs/2504.15612)
*Hongxing Peng,Kang Lin,Huanai Liu*

Main category: cs.CV

TL;DR: HS-Mamba框架结合局部和全局特征，通过多组Mamba和轻量级注意力模块实现高精度高光谱图像分类。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临高维和特征内联的挑战，Mamba架构在长序列建模中表现优越，但需适应高光谱数据特性。

Method: 提出HS-Mamba框架，包括双通道空间-光谱编码器模块（DCSS-encoder）和轻量级全局内联注意力分支（LGI-Att），融合局部和全局特征。

Result: 在四个基准数据集上优于现有方法，验证了HS-Mamba的优越性。

Conclusion: HS-Mamba通过结合局部和全局特征，显著提升了高光谱图像分类性能。

Abstract: Hyperspectral image (HSI) classification has been one of the hot topics in
remote sensing fields. Recently, the Mamba architecture based on selective
state-space models (S6) has demonstrated great advantages in long sequence
modeling. However, the unique properties of hyperspectral data, such as high
dimensionality and feature inlining, pose challenges to the application of
Mamba to HSI classification. To compensate for these shortcomings, we propose
an full-field interaction multi-groups Mamba framework (HS-Mamba), which adopts
a strategy different from pixel-patch based or whole-image based, but combines
the advantages of both. The patches cut from the whole image are sent to
multi-groups Mamba, combined with positional information to perceive local
inline features in the spatial and spectral domains, and the whole image is
sent to a lightweight attention module to enhance the global feature
representation ability. Specifically, HS-Mamba consists of a dual-channel
spatial-spectral encoder (DCSS-encoder) module and a lightweight global inline
attention (LGI-Att) branch. The DCSS-encoder module uses multiple groups of
Mamba to decouple and model the local features of dual-channel sequences with
non-overlapping patches. The LGI-Att branch uses a lightweight compressed and
extended attention module to perceive the global features of the spatial and
spectral domains of the unsegmented whole image. By fusing local and global
features, high-precision classification of hyperspectral images is achieved.
Extensive experiments demonstrate the superiority of the proposed HS-Mamba,
outperforming state-of-the-art methods on four benchmark HSI datasets.

</details>


### [58] [AdaViP: Aligning Multi-modal LLMs via Adaptive Vision-enhanced Preference Optimization](https://arxiv.org/abs/2504.15619)
*Jinda Lu,Jinghan Li,Yuan Gao,Junkang Wu,Jiancan Wu,Xiang Wang,Xiangnan He*

Main category: cs.CV

TL;DR: 提出了AdaViP方法，通过视觉增强和自适应优化，显著提升了多模态大语言模型与人类偏好的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注语言偏好，忽略了视觉上下文的重要性，导致对齐效果受限。

Method: 提出AdaViP方法，包括视觉偏好对构建和自适应偏好优化，以动态平衡视觉与语言偏好。

Result: 在Object HalBench上，AdaViP-7B分别减少了93.7%和96.4%的响应级和提及级幻觉，显著优于现有方法。

Conclusion: AdaViP通过视觉增强和自适应优化，显著提升了多模态大语言模型的偏好对齐效果。

Abstract: Preference alignment through Direct Preference Optimization (DPO) has
demonstrated significant effectiveness in aligning multimodal large language
models (MLLMs) with human preferences. However, existing methods focus
primarily on language preferences while neglecting the critical visual context.
In this paper, we propose an Adaptive Vision-enhanced Preference optimization
(AdaViP) that addresses these limitations through two key innovations: (1)
vision-based preference pair construction, which integrates multiple visual
foundation models to strategically remove key visual elements from the image,
enhancing MLLMs' sensitivity to visual details; and (2) adaptive preference
optimization that dynamically balances vision- and language-based preferences
for more accurate alignment. Extensive evaluations across different benchmarks
demonstrate our effectiveness. Notably, our AdaViP-7B achieves 93.7% and 96.4%
reductions in response-level and mentioned-level hallucination respectively on
the Object HalBench, significantly outperforming current state-of-the-art
methods.

</details>


### [59] [FaceInsight: A Multimodal Large Language Model for Face Perception](https://arxiv.org/abs/2504.15624)
*Jingzhi Li,Changjiang Luo,Ruoyu Chen,Hua Zhang,Wenqi Ren,Jianhou Gan,Xiaochun Cao*

Main category: cs.CV

TL;DR: FaceInsight是一种专用于面部感知的多模态大语言模型，通过视觉-文本对齐和面部分割图增强语义理解，显著优于其他通用MLLMs。


<details>
  <summary>Details</summary>
Motivation: 通用多模态大语言模型（MLLMs）在面部感知任务中表现不佳，常产生不准确或误导性回答，因此需要针对性解决方案。

Method: 引入视觉-文本对齐技术建模面部知识的依赖关系，并利用面部分割图作为辅助感知模态，增强语义理解。

Result: 在三种面部感知任务中，FaceInsight在无训练和微调设置下均优于九种对比MLLMs。

Conclusion: FaceInsight通过针对性设计解决了通用MLLMs在面部感知中的不足，表现出卓越性能。

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated
strong capabilities in understanding general visual content. However, these
general-domain MLLMs perform poorly in face perception tasks, often producing
inaccurate or misleading responses to face-specific queries. To address this
gap, we propose FaceInsight, the versatile face perception MLLM that provides
fine-grained facial information. Our approach introduces visual-textual
alignment of facial knowledge to model both uncertain dependencies and
deterministic relationships among facial information, mitigating the
limitations of language-driven reasoning. Additionally, we incorporate face
segmentation maps as an auxiliary perceptual modality, enriching the visual
input with localized structural cues to enhance semantic understanding.
Comprehensive experiments and analyses across three face perception tasks
demonstrate that FaceInsight consistently outperforms nine compared MLLMs under
both training-free and fine-tuned settings.

</details>


### [60] [ZeroSlide: Is Zero-Shot Classification Adequate for Lifelong Learning in Whole-Slide Image Analysis in the Era of Pathology Vision-Language Foundation Models?](https://arxiv.org/abs/2504.15627)
*Doanh C. Bui,Hoai Luan Pham,Vu Trung Duong Le,Tuan Hai Vu,Van Duy Tran,Yasuhiko Nakashima*

Main category: cs.CV

TL;DR: 比较传统持续学习方法与视觉语言零样本分类在WSI终身学习中的效果。


<details>
  <summary>Details</summary>
Motivation: 解决WSI终身学习中多任务统一模型的训练问题，避免每次新任务都需重新训练模型的耗时问题。

Method: 对比传统持续学习方法与视觉语言零样本分类方法。

Result: 实验结果表明视觉语言模型在零样本分类中的潜力，但仍需进一步研究持续学习策略。

Conclusion: 视觉语言模型在WSI终身学习中具有潜力，但持续学习策略仍需探索以提高性能。

Abstract: Lifelong learning for whole slide images (WSIs) poses the challenge of
training a unified model to perform multiple WSI-related tasks, such as cancer
subtyping and tumor classification, in a distributed, continual fashion. This
is a practical and applicable problem in clinics and hospitals, as WSIs are
large, require storage, processing, and transfer time. Training new models
whenever new tasks are defined is time-consuming. Recent work has applied
regularization- and rehearsal-based methods to this setting. However, the rise
of vision-language foundation models that align diagnostic text with pathology
images raises the question: are these models alone sufficient for lifelong WSI
learning using zero-shot classification, or is further investigation into
continual learning strategies needed to improve performance? To our knowledge,
this is the first study to compare conventional continual-learning approaches
with vision-language zero-shot classification for WSIs. Our source code and
experimental results will be available soon.

</details>


### [61] [AffordanceSAM: Segment Anything Once More in Affordance Grounding](https://arxiv.org/abs/2504.15650)
*Dengyang Jiang,Mengmeng Wang,Teli Ma,Hengzhuang Li,Yong liu,Guang Dai,Lei Zhang*

Main category: cs.CV

TL;DR: AffordanceSAM扩展了SAM的分割能力，通过适应性模块和训练方法提升了对未见物体和功能的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前模型在泛化能力上不足，难以识别未见物体和功能区域，限制了实际应用。

Method: 提出AffordanceSAM，包含适应性模块和从粗到精的训练方法，优化SAM的分割输出以适应功能区域。

Result: 在AGD20K基准测试中超越现有方法，并能处理新物体和功能任务。

Conclusion: AffordanceSAM展示了强大的泛化能力，为实际应用提供了潜力。

Abstract: Improving the generalization ability of an affordance grounding model to
recognize regions for unseen objects and affordance functions is crucial for
real-world application. However, current models are still far away from such
standards. To address this problem, we introduce AffordanceSAM, an effective
approach that extends SAM's generalization capacity to the domain of affordance
grounding. For the purpose of thoroughly transferring SAM's robust performance
in segmentation to affordance, we initially propose an affordance-adaption
module in order to help modify SAM's segmentation output to be adapted to the
specific functional regions required for affordance grounding. We concurrently
make a coarse-to-fine training recipe to make SAM first be aware of affordance
objects and actions coarsely, and then be able to generate affordance heatmaps
finely. Both quantitative and qualitative experiments show the strong
generalization capacity of our AffordanceSAM, which not only surpasses previous
methods under AGD20K benchmark but also shows evidence to handle the task with
novel objects and affordance functions.

</details>


### [62] [DiTPainter: Efficient Video Inpainting with Diffusion Transformers](https://arxiv.org/abs/2504.15661)
*Xian Wu,Chang Liu*

Main category: cs.CV

TL;DR: DiTPainter是一种基于扩散变换器（DiT）的视频修复模型，解决了现有方法因光流不准确或大掩码导致的模糊和不一致问题，且避免了预训练大模型的耗时问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频修复算法依赖光流传播像素，但易因光流不准确或大掩码导致模糊和不一致；预训练DiT模型参数量大，不适用于视频修复。

Method: 提出DiTPainter，一种基于DiT的端到端视频修复模型，采用高效Transformer网络，从头训练而非依赖预训练大模型。

Result: DiTPainter能处理任意长度视频，适用于视频去字幕和补全任务，实验显示其质量和时空一致性优于现有方法。

Conclusion: DiTPainter通过高效Transformer设计解决了现有问题，为视频修复提供了高质量且高效的解决方案。

Abstract: Many existing video inpainting algorithms utilize optical flows to construct
the corresponding maps and then propagate pixels from adjacent frames to
missing areas by mapping. Despite the effectiveness of the propagation
mechanism, they might encounter blurry and inconsistencies when dealing with
inaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT)
has emerged as a revolutionary technique for video generation tasks. However,
pretrained DiT models for video generation all contain a large amount of
parameters, which makes it very time consuming to apply to video inpainting
tasks. In this paper, we present DiTPainter, an end-to-end video inpainting
model based on Diffusion Transformer (DiT). DiTPainter uses an efficient
transformer network designed for video inpainting, which is trained from
scratch instead of initializing from any large pretrained models. DiTPainter
can address videos with arbitrary lengths and can be applied to video
decaptioning and video completion tasks with an acceptable time cost.
Experiments show that DiTPainter outperforms existing video inpainting
algorithms with higher quality and better spatial-temporal consistency.

</details>


### [63] [Motion-Enhanced Nonlocal Similarity Implicit Neural Representation for Infrared Dim and Small Target Detection](https://arxiv.org/abs/2504.15665)
*Pei Liu,Yisi Luo,Wenzhen Wang,Xiangyong Cao*

Main category: cs.CV

TL;DR: 提出了一种基于运动增强的非局部相似性隐式神经表示（INR）框架，用于红外弱小目标检测，解决了动态背景和目标弱信号的问题。


<details>
  <summary>Details</summary>
Motivation: 传统低秩加稀疏模型难以捕捉动态背景和全局时空相关性，导致背景泄漏或目标丢失。

Method: 结合光流进行运动估计，多帧融合增强运动显著性；利用非局部相似性构建低秩块张量，提出基于张量分解的INR模型。

Result: 实验表明，该方法能有效分离弱小目标与复杂背景，检测精度和鲁棒性优于现有方法。

Conclusion: 提出的INR框架在红外弱小目标检测中表现出色，具有理论和实际优势。

Abstract: Infrared dim and small target detection presents a significant challenge due
to dynamic multi-frame scenarios and weak target signatures in the infrared
modality. Traditional low-rank plus sparse models often fail to capture dynamic
backgrounds and global spatial-temporal correlations, which results in
background leakage or target loss. In this paper, we propose a novel
motion-enhanced nonlocal similarity implicit neural representation (INR)
framework to address these challenges. We first integrate motion estimation via
optical flow to capture subtle target movements, and propose multi-frame fusion
to enhance motion saliency. Second, we leverage nonlocal similarity to
construct patch tensors with strong low-rank properties, and propose an
innovative tensor decomposition-based INR model to represent the nonlocal patch
tensor, effectively encoding both the nonlocal low-rankness and
spatial-temporal correlations of background through continuous neural
representations. An alternating direction method of multipliers is developed
for the nonlocal INR model, which enjoys theoretical fixed-point convergence.
Experimental results show that our approach robustly separates dim targets from
complex infrared backgrounds, outperforming state-of-the-art methods in
detection accuracy and robustness.

</details>


### [64] [DINOv2-powered Few-Shot Semantic Segmentation: A Unified Framework via Cross-Model Distillation and 4D Correlation Mining](https://arxiv.org/abs/2504.15669)
*Wei Zhuo,Zhiyue Tang,Wufeng Xue,Hao Ding,Linlin Shen*

Main category: cs.CV

TL;DR: FS-DINO是一个统一的少样本语义分割模型，结合了DINOv2和SAM的知识，通过轻量级分割器和跨模型蒸馏实现高效分割。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以通过结合DINOv2和SAM的优势，构建一个统一的模型来解决少样本语义分割问题。

Method: 提出FS-DINO模型，仅使用DINOv2的编码器和轻量级分割器，通过瓶颈适配器、元视觉提示生成器和解码器实现分割，并利用跨模型蒸馏整合SAM知识。

Result: 在COCO-20i、PASCAL-5i和FSS-1000数据集上的实验证明了该方法的有效性和优越性。

Conclusion: FS-DINO成功整合了DINOv2和SAM的优势，为少样本语义分割提供了一种高效且统一的解决方案。

Abstract: Few-shot semantic segmentation has gained increasing interest due to its
generalization capability, i.e., segmenting pixels of novel classes requiring
only a few annotated images. Prior work has focused on meta-learning for
support-query matching, with extensive development in both prototype-based and
aggregation-based methods. To address data scarcity, recent approaches have
turned to foundation models to enhance representation transferability for novel
class segmentation. Among them, a hybrid dual-modal framework including both
DINOv2 and SAM has garnered attention due to their complementary capabilities.
We wonder "can we build a unified model with knowledge from both foundation
models?" To this end, we propose FS-DINO, with only DINOv2's encoder and a
lightweight segmenter. The segmenter features a bottleneck adapter, a
meta-visual prompt generator based on dense similarities and semantic
embeddings, and a decoder. Through coarse-to-fine cross-model distillation, we
effectively integrate SAM's knowledge into our lightweight segmenter, which can
be further enhanced by 4D correlation mining on support-query pairs. Extensive
experiments on COCO-20i, PASCAL-5i, and FSS-1000 demonstrate the effectiveness
and superiority of our method.

</details>


### [65] [Vidi: Large Multimodal Models for Video Understanding and Editing](https://arxiv.org/abs/2504.15681)
*Vidi Team,Celong Liu,Chia-Wen Kuo,Dawei Du,Fan Chen,Guang Chen,Jiamin Yuan,Lingxi Zhang,Lu Guo,Lusha Li,Longyin Wen,Qingyu Chen,Rachel Deng,Sijie Zhu,Stuart Siew,Tong Jin,Wei Lu,Wen Zhong,Xiaohui Shen,Xin Gu,Xing Mei,Xueqiong Qu*

Main category: cs.CV

TL;DR: Vidi是一个大型多模态模型家族，专注于视频理解和编辑任务，特别是时间范围检索，能够处理长视频并显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 视频已成为互联网上主要的交流媒介，但传统模型在多模态处理和长视频理解方面存在挑战，因此需要更强大的模型支持高质量视频编辑。

Method: Vidi模型通过处理多模态输入（如视觉、音频、文本）并支持灵活输入长度，专注于时间范围检索任务。

Result: Vidi在时间范围检索任务上显著优于GPT-4o和Gemini等领先专有模型。

Conclusion: Vidi在视频编辑场景中表现出色，其能力通过VUE-TR基准测试得到验证，为大规模视频内容创作提供了有力支持。

Abstract: Humans naturally share information with those they are connected to, and
video has become one of the dominant mediums for communication and expression
on the Internet. To support the creation of high-quality large-scale video
content, a modern pipeline requires a comprehensive understanding of both the
raw input materials (e.g., the unedited footage captured by cameras) and the
editing components (e.g., visual effects). In video editing scenarios, models
must process multiple modalities (e.g., vision, audio, text) with strong
background knowledge and handle flexible input lengths (e.g., hour-long raw
videos), which poses significant challenges for traditional models. In this
report, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a
wide range of video understand editing scenarios. The first release focuses on
temporal retrieval, i.e., identifying the time ranges within the input videos
corresponding to a given text query, which plays a critical role in intelligent
editing. The model is capable of processing hour-long videos with strong
temporal understanding capability, e.g., retrieve time ranges for certain
queries. To support a comprehensive evaluation in real-world scenarios, we also
present the VUE-TR benchmark, which introduces five key advancements. 1) Video
duration: significantly longer than existing temporal retrival datasets, 2)
Audio support: includes audio-based queries, 3) Query format: diverse query
lengths/formats, 4) Annotation quality: ground-truth time ranges are manually
annotated. 5) Evaluation metric: a refined IoU metric to support evaluation
over multiple time ranges. Remarkably, Vidi significantly outperforms leading
proprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task,
indicating its superiority in video editing scenarios.

</details>


### [66] [You Sense Only Once Beneath: Ultra-Light Real-Time Underwater Object Detection](https://arxiv.org/abs/2504.15694)
*Jun Dong,Wenli Wu,Jintao Cheng,Xiaoyu Tang*

Main category: cs.CV

TL;DR: 提出了一种超轻量实时水下目标检测框架YSOOB，通过多频谱小波编码和动态信息增强，显著提升了模型性能与效率。


<details>
  <summary>Details</summary>
Motivation: 水下环境图像质量低且计算资源有限，现有目标检测模型的精度和效率仍需改进。

Method: 采用多频谱小波编码（MSWE）减少水下光学颜色失真，动态选择关键信息增强，并通过通道压缩和大核卷积重构（RLKC）实现模型轻量化。

Result: YSOOB仅120万参数，在URPC2020和DUO数据集上mAP50分别达83.1%和82.9%，推理速度在T4 GPU和Jetson Xavier NX上分别达781.3 FPS和57.8 FPS。

Conclusion: YSOOB在轻量化和性能上均优于现有方法，适用于水下目标检测任务。

Abstract: Despite the remarkable achievements in object detection, the model's accuracy
and efficiency still require further improvement under challenging underwater
conditions, such as low image quality and limited computational resources. To
address this, we propose an Ultra-Light Real-Time Underwater Object Detection
framework, You Sense Only Once Beneath (YSOOB). Specifically, we utilize a
Multi-Spectrum Wavelet Encoder (MSWE) to perform frequency-domain encoding on
the input image, minimizing the semantic loss caused by underwater optical
color distortion. Furthermore, we revisit the unique characteristics of
even-sized and transposed convolutions, allowing the model to dynamically
select and enhance key information during the resampling process, thereby
improving its generalization ability. Finally, we eliminate model redundancy
through a simple yet effective channel compression and reconstructed large
kernel convolution (RLKC) to achieve model lightweight. As a result, forms a
high-performance underwater object detector YSOOB with only 1.2 million
parameters. Extensive experimental results demonstrate that, with the fewest
parameters, YSOOB achieves mAP50 of 83.1% and 82.9% on the URPC2020 and DUO
datasets, respectively, comparable to the current SOTA detectors. The inference
speed reaches 781.3 FPS and 57.8 FPS on the T4 GPU (TensorRT FP16) and the edge
computing device Jetson Xavier NX (TensorRT FP16), surpassing YOLOv12-N by
28.1% and 22.5%, respectively.

</details>


### [67] [RePOPE: Impact of Annotation Errors on the POPE Benchmark](https://arxiv.org/abs/2504.15707)
*Yannic Neuhaus,Matthias Hein*

Main category: cs.CV

TL;DR: 研究评估了MSCOCO标签错误对POPE基准的影响，重新标注后发现标注错误分布不均，修正后的RePOPE基准显著改变了模型排名。


<details>
  <summary>Details</summary>
Motivation: 由于数据标注成本高，基准数据集常使用现有标签，但标签错误可能影响评估结果。

Method: 重新标注POPE基准图像，分析标签错误分布，并基于修正标签（RePOPE）评估多个模型。

Result: 发现标注错误分布不均，修正标签后模型排名发生显著变化。

Conclusion: 标签质量对基准评估有重要影响，RePOPE提供了更可靠的评估标准。

Abstract: Since data annotation is costly, benchmark datasets often incorporate labels
from established image datasets. In this work, we assess the impact of label
errors in MSCOCO on the frequently used object hallucination benchmark POPE. We
re-annotate the benchmark images and identify an imbalance in annotation errors
across different subsets. Evaluating multiple models on the revised labels,
which we denote as RePOPE, we observe notable shifts in model rankings,
highlighting the impact of label quality. Code and data are available at
https://github.com/YanNeu/RePOPE .

</details>


### [68] [Structure-Preserving Zero-Shot Image Editing via Stage-Wise Latent Injection in Diffusion Models](https://arxiv.org/abs/2504.15723)
*Dasol Jeong,Donggoo Kang,Jiwon Park,Hyebean Lee,Joonki Paik*

Main category: cs.CV

TL;DR: 提出了一种基于扩散的零样本图像编辑框架，统一了文本引导和参考引导方法，无需微调。


<details>
  <summary>Details</summary>
Motivation: 旨在实现无需微调的图像编辑，同时保持源图像的结构完整性。

Method: 利用扩散反演和特定时间步的空文本嵌入，结合分阶段潜在注入策略（早期形状注入，后期属性注入）和参考潜在交叉注意力。

Result: 在表情迁移、纹理变换和风格注入等任务中表现出色，达到最先进性能。

Conclusion: 该方法具有可扩展性和适应性，适用于多样化的图像编辑场景。

Abstract: We propose a diffusion-based framework for zero-shot image editing that
unifies text-guided and reference-guided approaches without requiring
fine-tuning. Our method leverages diffusion inversion and timestep-specific
null-text embeddings to preserve the structural integrity of the source image.
By introducing a stage-wise latent injection strategy-shape injection in early
steps and attribute injection in later steps-we enable precise, fine-grained
modifications while maintaining global consistency. Cross-attention with
reference latents facilitates semantic alignment between the source and
reference. Extensive experiments across expression transfer, texture
transformation, and style infusion demonstrate state-of-the-art performance,
confirming the method's scalability and adaptability to diverse image editing
scenarios.

</details>


### [69] [SAGA: Semantic-Aware Gray color Augmentation for Visible-to-Thermal Domain Adaptation across Multi-View Drone and Ground-Based Vision Systems](https://arxiv.org/abs/2504.15728)
*Manjunath D,Aniruddh Sikdar,Prajwal Gurunath,Sumanth Udupa,Suresh Sundaram*

Main category: cs.CV

TL;DR: 论文提出了一种名为SAGA的新策略，用于解决RGB到IR图像域适应的挑战，并引入了一个多传感器数据集IndraEye。实验表明SAGA显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决RGB到IR图像域适应中的颜色和纹理缺失问题，减少对大量标注IR数据的依赖。

Method: 提出Semantic-Aware Gray color Augmentation (SAGA)策略，提取与IR图像相关的对象级特征。

Result: SAGA在RGB到IR域适应中表现优异，性能提升0.4%至7.6%（mAP）。

Conclusion: SAGA和IndraEye数据集为多模态学习和域适应提供了有力工具，尤其在无人机图像领域。

Abstract: Domain-adaptive thermal object detection plays a key role in facilitating
visible (RGB)-to-thermal (IR) adaptation by reducing the need for co-registered
image pairs and minimizing reliance on large annotated IR datasets. However,
inherent limitations of IR images, such as the lack of color and texture cues,
pose challenges for RGB-trained models, leading to increased false positives
and poor-quality pseudo-labels. To address this, we propose Semantic-Aware Gray
color Augmentation (SAGA), a novel strategy for mitigating color bias and
bridging the domain gap by extracting object-level features relevant to IR
images. Additionally, to validate the proposed SAGA for drone imagery, we
introduce the IndraEye, a multi-sensor (RGB-IR) dataset designed for diverse
applications. The dataset contains 5,612 images with 145,666 instances,
captured from diverse angles, altitudes, backgrounds, and times of day,
offering valuable opportunities for multimodal learning, domain adaptation for
object detection and segmentation, and exploration of sensor-specific strengths
and weaknesses. IndraEye aims to enhance the development of more robust and
accurate aerial perception systems, especially in challenging environments.
Experimental results show that SAGA significantly improves RGB-to-IR adaptation
for autonomous driving and IndraEye dataset, achieving consistent performance
gains of +0.4% to +7.6% (mAP) when integrated with state-of-the-art domain
adaptation techniques. The dataset and codes are available at
https://github.com/airliisc/IndraEye.

</details>


### [70] [GADS: A Super Lightweight Model for Head Pose Estimation](https://arxiv.org/abs/2504.15751)
*Menan Velayuthan,Asiri Gawesha,Purushoth Velayuthan,Nuwan Kodagoda,Dharshana Kasthurirathna,Pradeepa Samarasinghe*

Main category: cs.CV

TL;DR: 提出了一种名为GADS的新架构，通过分组地标和使用小型Deep Set层降低计算复杂度，显著减小模型大小并提升速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于地标的方法过于注重精度而忽略简洁性和模型大小，限制了在边缘设备上的部署。

Method: 采用分组地标和Deep Set框架，结合多头注意力机制提取组间信息。

Result: 模型比当前最轻量级模型小7.5倍、快25倍，比性能最佳模型小4321倍。

Conclusion: GADS为资源受限的头姿估计方法提供了强大基线。

Abstract: In human-computer interaction, head pose estimation profoundly influences
application functionality. Although utilizing facial landmarks is valuable for
this purpose, existing landmark-based methods prioritize precision over
simplicity and model size, limiting their deployment on edge devices and in
compute-poor environments. To bridge this gap, we propose \textbf{Grouped
Attention Deep Sets (GADS)}, a novel architecture based on the Deep Set
framework. By grouping landmarks into regions and employing small Deep Set
layers, we reduce computational complexity. Our multihead attention mechanism
extracts and combines inter-group information, resulting in a model that is
$7.5\times$ smaller and executes $25\times$ faster than the current lightest
state-of-the-art model. Notably, our method achieves an impressive reduction,
being $4321\times$ smaller than the best-performing model. We introduce vanilla
GADS and Hybrid-GADS (landmarks + RGB) and evaluate our models on three
benchmark datasets -- AFLW2000, BIWI, and 300W-LP. We envision our architecture
as a robust baseline for resource-constrained head pose estimation methods.

</details>


### [71] [DSDNet: Raw Domain Demoiréing via Dual Color-Space Synergy](https://arxiv.org/abs/2504.15756)
*Qirui Yang,Fangpu Zhang,Yeying Jin,Qihua Cheng,Pengtao Jiang,Huanjing Yue,Jingyu Yang*

Main category: cs.CV

TL;DR: 提出了一种单阶段原始域去摩尔纹框架DSDNet，通过双流网络和动态调制模块提升视觉质量与效率。


<details>
  <summary>Details</summary>
Motivation: 移动成像中摩尔纹问题严重，现有方法存在信息丢失或效率低的问题，需改进。

Method: 设计双流网络DSDNet，结合原始域与YCbCr图像，引入动态调制模块和自适应变换器。

Result: DSDNet在视觉质量和定量评估上优于现有方法，推理速度提升2.4倍。

Conclusion: DSDNet有效解决摩尔纹问题，兼具高效与高质量，具有实际应用优势。

Abstract: With the rapid advancement of mobile imaging, capturing screens using
smartphones has become a prevalent practice in distance learning and conference
recording. However, moir\'e artifacts, caused by frequency aliasing between
display screens and camera sensors, are further amplified by the image signal
processing pipeline, leading to severe visual degradation. Existing sRGB domain
demoir\'eing methods struggle with irreversible information loss, while recent
two-stage raw domain approaches suffer from information bottlenecks and
inference inefficiency. To address these limitations, we propose a single-stage
raw domain demoir\'eing framework, Dual-Stream Demoir\'eing Network (DSDNet),
which leverages the synergy of raw and YCbCr images to remove moir\'e while
preserving luminance and color fidelity. Specifically, to guide luminance
correction and moir\'e removal, we design a raw-to-YCbCr mapping pipeline and
introduce the Synergic Attention with Dynamic Modulation (SADM) module. This
module enriches the raw-to-sRGB conversion with cross-domain contextual
features. Furthermore, to better guide color fidelity, we develop a
Luminance-Chrominance Adaptive Transformer (LCAT), which decouples luminance
and chrominance representations. Extensive experiments demonstrate that DSDNet
outperforms state-of-the-art methods in both visual quality and quantitative
evaluation, and achieves an inference speed $\mathrm{\textbf{2.4x}}$ faster
than the second-best method, highlighting its practical advantages. We provide
an anonymous online demo at https://xxxxxxxxdsdnet.github.io/DSDNet/.

</details>


### [72] [Multi-Scale Tensorial Summation and Dimensional Reduction Guided Neural Network for Edge Detection](https://arxiv.org/abs/2504.15770)
*Lei Xu,Mehmet Yamac,Mete Ahishali,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 提出了一种基于MTS-DR模块的新型神经网络MTS-DR-Net，用于边缘检测任务，通过降维模块和U形细化模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统边缘检测方法需要大感受野，通常需深层网络结构，而MTS-DR-Net通过MTS层和降维模块在初始层即可实现大感受野，减少冗余信息。

Method: 使用MTS层和MTS-DR模块作为新主干网络，结合U形细化模块，专注于相关信息。

Result: 在BSDS500和BIPEDv2数据集上验证了模型的有效性。

Conclusion: MTS-DR-Net通过降维和细化模块显著提升了边缘检测性能。

Abstract: Edge detection has attracted considerable attention thanks to its exceptional
ability to enhance performance in downstream computer vision tasks. In recent
years, various deep learning methods have been explored for edge detection
tasks resulting in a significant performance improvement compared to
conventional computer vision algorithms. In neural networks, edge detection
tasks require considerably large receptive fields to provide satisfactory
performance. In a typical convolutional operation, such a large receptive field
can be achieved by utilizing a significant number of consecutive layers, which
yields deep network structures. Recently, a Multi-scale Tensorial Summation
(MTS) factorization operator was presented, which can achieve very large
receptive fields even from the initial layers. In this paper, we propose a
novel MTS Dimensional Reduction (MTS-DR) module guided neural network,
MTS-DR-Net, for the edge detection task. The MTS-DR-Net uses MTS layers, and
corresponding MTS-DR blocks as a new backbone to remove redundant information
initially. Such a dimensional reduction module enables the neural network to
focus specifically on relevant information (i.e., necessary subspaces).
Finally, a weight U-shaped refinement module follows MTS-DR blocks in the
MTS-DR-Net. We conducted extensive experiments on two benchmark edge detection
datasets: BSDS500 and BIPEDv2 to verify the effectiveness of our model. The
implementation of the proposed MTS-DR-Net can be found at
https://github.com/LeiXuAI/MTS-DR-Net.git.

</details>


### [73] [Pose Optimization for Autonomous Driving Datasets using Neural Rendering Models](https://arxiv.org/abs/2504.15776)
*Quentin Herau,Nathan Piasco,Moussab Bennehar,Luis Rolado,Dzmitry Tsishkou,Bingbing Liu,Cyrille Migniot,Pascal Vasseur,Cédric Demonceaux*

Main category: cs.CV

TL;DR: 提出了一种基于NeRF的优化方法，用于改进自动驾驶数据集中传感器位姿和校准参数，提升数据集基准的准确性。


<details>
  <summary>Details</summary>
Motivation: 公共数据集中传感器校准和车辆位姿的不准确性可能导致下游任务评估错误，影响自动驾驶系统的可靠性。

Method: 采用基于NeRF的鲁棒优化方法，通过重投影指标、新视角合成渲染质量和几何对齐来验证优化效果。

Result: 方法显著提升了传感器位姿的准确性，优化后的参数公开可用。

Conclusion: 该方法不仅提高了现有数据集的实用性，还为更可靠的自动驾驶模型奠定了基础。

Abstract: Autonomous driving systems rely on accurate perception and localization of
the ego car to ensure safety and reliability in challenging real-world driving
scenarios. Public datasets play a vital role in benchmarking and guiding
advancement in research by providing standardized resources for model
development and evaluation. However, potential inaccuracies in sensor
calibration and vehicle poses within these datasets can lead to erroneous
evaluations of downstream tasks, adversely impacting the reliability and
performance of the autonomous systems. To address this challenge, we propose a
robust optimization method based on Neural Radiance Fields (NeRF) to refine
sensor poses and calibration parameters, enhancing the integrity of dataset
benchmarks. To validate improvement in accuracy of our optimized poses without
ground truth, we present a thorough evaluation process, relying on reprojection
metrics, Novel View Synthesis rendering quality, and geometric alignment. We
demonstrate that our method achieves significant improvements in sensor pose
accuracy. By optimizing these critical parameters, our approach not only
improves the utility of existing datasets but also paves the way for more
reliable autonomous driving models. To foster continued progress in this field,
we make the optimized sensor poses publicly available, providing a valuable
resource for the research community.

</details>


### [74] [Model-based Metric 3D Shape and Motion Reconstruction of Wild Bottlenose Dolphins in Drone-Shot Videos](https://arxiv.org/abs/2504.15782)
*Daniele Baieri,Riccardo Cicciarella,Michael Krützen,Emanuele Rodolà,Silvia Zuffi*

Main category: cs.CV

TL;DR: 提出了一种基于模型的方法，用于从单目视频中估计野生海豚的3D形状和运动，以评估其身体状况。


<details>
  <summary>Details</summary>
Motivation: 水生动物在自然水下环境中的观测困难，导致其3D重建研究较少。本文旨在填补这一空白。

Method: 采用基于模型的方法，结合传输模型以处理水引起的遮挡问题，应用于不同海况下的视频数据。

Result: 估计了海豚的质量和体积，并与基于手动2D测量的方法进行了比较。

Conclusion: 该方法为水生动物3D重建提供了有效解决方案，并展示了其潜力。

Abstract: We address the problem of estimating the metric 3D shape and motion of wild
dolphins from monocular video, with the aim of assessing their body condition.
While considerable progress has been made in reconstructing 3D models of
terrestrial quadrupeds, aquatic animals remain unexplored due to the difficulty
of observing them in their natural underwater environment. To address this, we
propose a model-based approach that incorporates a transmission model to
account for water-induced occlusion. We apply our method to video captured
under different sea conditions. We estimate mass and volume, and compare our
results to a manual 2D measurements-based method.

</details>


### [75] [Towards prediction of morphological heart age from computed tomography angiography](https://arxiv.org/abs/2504.15783)
*Johan Öfverstedt,Elin Lundström,Håkan Ahlström,Joel Kullberg*

Main category: cs.CV

TL;DR: 通过CTA图像预测年龄，研究心脏形态与衰老的关系，并提出新的形态心脏年龄生物标志物。


<details>
  <summary>Details</summary>
Motivation: 探索心脏形态与年龄的关系，开发一种新的生物标志物以量化衰老。

Method: 使用图像配准方法标准化图像，提取超体素和稳健特征，训练机器学习模型预测年龄。

Result: 女性平均绝对误差2.74年，男性2.77年，形态预测高度一致。

Conclusion: 形态特征能有效预测年龄，显著性分析揭示了关键区域，增强了模型的可解释性。

Abstract: Age prediction from medical images or other health-related non-imaging data
is an important approach to data-driven aging research, providing knowledge of
how much information a specific tissue or organ carries about the chronological
age of the individual. In this work, we studied the prediction of age from
computed tomography angiography (CTA) images, which provide detailed
representations of the heart morphology, with the goals of (i) studying the
relationship between morphology and aging, and (ii) developing a novel
\emph{morphological heart age} biomarker. We applied an image
registration-based method that standardizes the images from the whole cohort
into a single space. We then extracted supervoxels (using unsupervised
segmentation), and corresponding robust features of density and local volume,
which provide a detailed representation of the heart morphology while being
robust to registration errors. Machine learning models are then trained to fit
regression models from these features to the chronological age. We applied the
method to a subset of the images from the Swedish CArdioPulomonary bioImage
Study (SCAPIS) dataset, consisting of 721 females and 666 males. We observe a
mean absolute error of $2.74$ years for females and $2.77$ years for males. The
predictions from different sub-regions of interest were observed to be more
highly correlated with the predictions from the whole heart, compared to the
chronological age, revealing a high consistency in the predictions from
morphology. Saliency analysis was also performed on the prediction models to
study what regions are associated positively and negatively with the predicted
age. This resulted in detailed association maps where the density and volume of
known, as well as some novel sub-regions of interest, are determined to be
important. The saliency analysis aids in the interpretability of the models and
their predictions.

</details>


### [76] [Satellite to GroundScape -- Large-scale Consistent Ground View Generation from Satellite Views](https://arxiv.org/abs/2504.15786)
*Ningli Xu,Rongjun Qin*

Main category: cs.CV

TL;DR: 提出了一种基于固定潜在扩散模型的新方法，用于从卫星图像生成一致的地面视图，解决了多视图一致性问题。


<details>
  <summary>Details</summary>
Motivation: 卫星与地面视图之间存在视角和分辨率差异，导致单视图生成方法在多视图场景中不一致。

Method: 采用卫星引导去噪和卫星-时间去噪模块，结合大规模数据集，确保多视图一致性。

Result: 在感知和时间指标上优于现有方法，实现了高真实感和多视图一致性。

Conclusion: 该方法有效解决了跨视图合成中的一致性问题，适用于大规模场景或视频生成。

Abstract: Generating consistent ground-view images from satellite imagery is
challenging, primarily due to the large discrepancies in viewing angles and
resolution between satellite and ground-level domains. Previous efforts mainly
concentrated on single-view generation, often resulting in inconsistencies
across neighboring ground views. In this work, we propose a novel cross-view
synthesis approach designed to overcome these challenges by ensuring
consistency across ground-view images generated from satellite views. Our
method, based on a fixed latent diffusion model, introduces two conditioning
modules: satellite-guided denoising, which extracts high-level scene layout to
guide the denoising process, and satellite-temporal denoising, which captures
camera motion to maintain consistency across multiple generated views. We
further contribute a large-scale satellite-ground dataset containing over
100,000 perspective pairs to facilitate extensive ground scene or video
generation. Experimental results demonstrate that our approach outperforms
existing methods on perceptual and temporal metrics, achieving high
photorealism and consistency in multi-view outputs.

</details>


### [77] [Development and evaluation of a deep learning algorithm for German word recognition from lip movements](https://arxiv.org/abs/2504.15792)
*Dinh Nam Pham,Torsten Rahne*

Main category: cs.CV

TL;DR: 论文提出了一种基于神经网络的德语唇读算法，通过比较不同模型和视频参数，实现了高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有唇读算法多针对英语，德语缺乏相关研究，且传统方法依赖视觉信息易出错。

Method: 使用1806个德语视频片段，训练3D CNN、GRU及混合模型（GRUConv），比较不同图像区域和色彩空间。

Result: GRUConv模型对已知和未知说话者的准确率分别达87%和63%，唇部裁剪显著提升性能。

Conclusion: 该德语唇读算法准确率高，可推广至更多词汇，性能媲美英语算法。

Abstract: When reading lips, many people benefit from additional visual information
from the lip movements of the speaker, which is, however, very error prone.
Algorithms for lip reading with artificial intelligence based on artificial
neural networks significantly improve word recognition but are not available
for the German language. A total of 1806 video clips with only one
German-speaking person each were selected, split into word segments, and
assigned to word classes using speech-recognition software. In 38,391 video
segments with 32 speakers, 18 polysyllabic, visually distinguishable words were
used to train and validate a neural network. The 3D Convolutional Neural
Network and Gated Recurrent Units models and a combination of both models
(GRUConv) were compared, as were different image sections and color spaces of
the videos. The accuracy was determined in 5000 training epochs. Comparison of
the color spaces did not reveal any relevant different correct classification
rates in the range from 69% to 72%. With a cut to the lips, a significantly
higher accuracy of 70% was achieved than when cut to the entire speaker's face
(34%). With the GRUConv model, the maximum accuracies were 87% with known
speakers and 63% in the validation with unknown speakers. The neural network
for lip reading, which was first developed for the German language, shows a
very high level of accuracy, comparable to English-language algorithms. It
works with unknown speakers as well and can be generalized with more word
classes.

</details>


### [78] [Locating and Mitigating Gradient Conflicts in Point Cloud Domain Adaptation via Saliency Map Skewness](https://arxiv.org/abs/2504.15796)
*Jiaqi Tang,Yinsong Xu,Qingchao Chen*

Main category: cs.CV

TL;DR: 提出了一种基于显著性图的梯度冲突缓解方法（SM-DSB），用于提升点云无监督域适应（UDA）的分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法中，自监督任务的梯度可能对分类性能产生负面影响，需要一种机制来筛选有益样本。

Method: 设计了一种基于3D显著性图偏度的评分机制，动态过滤对分类无益的自监督梯度样本。

Result: 方法在实验中优于现有技术，且计算开销低。

Conclusion: SM-DSB为UDA问题提供了新的视角，并通过反向传播分析验证了其有效性。

Abstract: Object classification models utilizing point cloud data are fundamental for
3D media understanding, yet they often struggle with unseen or
out-of-distribution (OOD) scenarios. Existing point cloud unsupervised domain
adaptation (UDA) methods typically employ a multi-task learning (MTL) framework
that combines primary classification tasks with auxiliary self-supervision
tasks to bridge the gap between cross-domain feature distributions. However,
our further experiments demonstrate that not all gradients from
self-supervision tasks are beneficial and some may negatively impact the
classification performance. In this paper, we propose a novel solution, termed
Saliency Map-based Data Sampling Block (SM-DSB), to mitigate these gradient
conflicts. Specifically, our method designs a new scoring mechanism based on
the skewness of 3D saliency maps to estimate gradient conflicts without
requiring target labels. Leveraging this, we develop a sample selection
strategy that dynamically filters out samples whose self-supervision gradients
are not beneficial for the classification. Our approach is scalable,
introducing modest computational overhead, and can be integrated into all the
point cloud UDA MTL frameworks. Extensive evaluations demonstrate that our
method outperforms state-of-the-art approaches. In addition, we provide a new
perspective on understanding the UDA problem through back-propagation analysis.

</details>


### [79] [Human-Imperceptible Physical Adversarial Attack for NIR Face Recognition Models](https://arxiv.org/abs/2504.15823)
*Songyan Xie,Jinghang Wen,Encheng Su,Qiucheng Yu*

Main category: cs.CV

TL;DR: 该论文提出了一种针对近红外（NIR）人脸识别系统的隐蔽且实用的对抗性补丁攻击方法，通过红外吸收墨水生成数字优化的补丁，并结合光反射模型提升攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 近红外人脸识别系统在低光或化妆条件下表现良好，但易受物理对抗攻击。论文旨在揭示实际应用中的潜在风险。

Method: 利用人眼不可见的红外吸收墨水生成补丁，并通过光反射模型优化数字与真实NIR成像的差异。

Result: 实验显示，该方法在数字和物理领域的攻击成功率均优于现有技术，物理领域平均成功率达82.46%。

Conclusion: 提出的方法有效提升了对抗性攻击的成功率，尤其在多种姿态下表现稳定，展示了实际应用中的潜在威胁。

Abstract: Near-infrared (NIR) face recognition systems, which can operate effectively
in low-light conditions or in the presence of makeup, exhibit vulnerabilities
when subjected to physical adversarial attacks. To further demonstrate the
potential risks in real-world applications, we design a novel, stealthy, and
practical adversarial patch to attack NIR face recognition systems in a
black-box setting. We achieved this by utilizing human-imperceptible
infrared-absorbing ink to generate multiple patches with digitally optimized
shapes and positions for infrared images. To address the optimization mismatch
between digital and real-world NIR imaging, we develop a light reflection model
for human skin to minimize pixel-level discrepancies by simulating NIR light
reflection.
  Compared to state-of-the-art (SOTA) physical attacks on NIR face recognition
systems, the experimental results show that our method improves the attack
success rate in both digital and physical domains, particularly maintaining
effectiveness across various face postures. Notably, the proposed approach
outperforms SOTA methods, achieving an average attack success rate of 82.46% in
the physical domain across different models, compared to 64.18% for existing
methods. The artifact is available at
https://anonymous.4open.science/r/Human-imperceptible-adversarial-patch-0703/.

</details>


### [80] [Text-based Animatable 3D Avatars with Morphable Model Alignment](https://arxiv.org/abs/2504.15835)
*Yiqian Wu,Malte Prinzler,Xiaogang Jin,Siyu Tang*

Main category: cs.CV

TL;DR: 提出了一种名为AnimPortrait3D的新框架，通过结合预训练模型和ControlNet，解决了文本生成3D头像时的外观、几何和对齐问题，提升了动画效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成高质量、可动画的3D头像时，存在外观细节不真实和与参数化模型对齐不足的问题，影响了动画效果。

Method: 1. 利用预训练文本到3D模型初始化头像，确保外观、几何和绑定关系；2. 使用基于语义和法线图的ControlNet优化动态表情对齐。

Result: 实验表明，该方法在合成质量、对齐和动画保真度上优于现有方法。

Conclusion: AnimPortrait3D在文本生成可动画3D头像领域取得了先进成果。

Abstract: The generation of high-quality, animatable 3D head avatars from text has
enormous potential in content creation applications such as games, movies, and
embodied virtual assistants. Current text-to-3D generation methods typically
combine parametric head models with 2D diffusion models using score
distillation sampling to produce 3D-consistent results. However, they struggle
to synthesize realistic details and suffer from misalignments between the
appearance and the driving parametric model, resulting in unnatural animation
results. We discovered that these limitations stem from ambiguities in the 2D
diffusion predictions during 3D avatar distillation, specifically: i) the
avatar's appearance and geometry is underconstrained by the text input, and ii)
the semantic alignment between the predictions and the parametric head model is
insufficient because the diffusion model alone cannot incorporate information
from the parametric model. In this work, we propose a novel framework,
AnimPortrait3D, for text-based realistic animatable 3DGS avatar generation with
morphable model alignment, and introduce two key strategies to address these
challenges. First, we tackle appearance and geometry ambiguities by utilizing
prior information from a pretrained text-to-3D model to initialize a 3D avatar
with robust appearance, geometry, and rigging relationships to the morphable
model. Second, we refine the initial 3D avatar for dynamic expressions using a
ControlNet that is conditioned on semantic and normal maps of the morphable
model to ensure accurate alignment. As a result, our method outperforms
existing approaches in terms of synthesis quality, alignment, and animation
fidelity. Our experiments show that the proposed method advances the state of
the art in text-based, animatable 3D head avatar generation.

</details>


### [81] [DERD-Net: Learning Depth from Event-based Ray Densities](https://arxiv.org/abs/2504.15863)
*Diego de Oliveira Hitzges,Suman Ghosh,Guillermo Gallego*

Main category: cs.CV

TL;DR: 提出了一种用于事件相机深度估计的灵活框架，通过处理异步事件数据，在单目和双目设置中均表现出色，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习框架难以处理事件相机的异步数据流，因此需要一种适应事件数据特性的新方法。

Method: 将3D场景结构编码为视差空间图像（DSIs），结合3D卷积和循环结构处理局部子区域，实现高效深度预测。

Result: 在标准数据集上表现优异：单目数据媲美现有双目方法，双目数据误差降低至少42%，深度完整性提升3倍以上。

Conclusion: 该框架在事件相机深度估计和SLAM中具有成为标准方法的潜力。

Abstract: Event cameras offer a promising avenue for multi-view stereo depth estimation
and Simultaneous Localization And Mapping (SLAM) due to their ability to detect
blur-free 3D edges at high-speed and over broad illumination conditions.
However, traditional deep learning frameworks designed for conventional cameras
struggle with the asynchronous, stream-like nature of event data, as their
architectures are optimized for discrete, image-like inputs. We propose a
scalable, flexible and adaptable framework for pixel-wise depth estimation with
event cameras in both monocular and stereo setups. The 3D scene structure is
encoded into disparity space images (DSIs), representing spatial densities of
rays obtained by back-projecting events into space via known camera poses. Our
neural network processes local subregions of the DSIs combining 3D convolutions
and a recurrent structure to recognize valuable patterns for depth prediction.
Local processing enables fast inference with full parallelization and ensures
constant ultra-low model complexity and memory costs, regardless of camera
resolution. Experiments on standard benchmarks (MVSEC and DSEC datasets)
demonstrate unprecedented effectiveness: (i) using purely monocular data, our
method achieves comparable results to existing stereo methods; (ii) when
applied to stereo data, it strongly outperforms all state-of-the-art (SOTA)
approaches, reducing the mean absolute error by at least 42%; (iii) our method
also allows for increases in depth completeness by more than 3-fold while still
yielding a reduction in median absolute error of at least 30%. Given its
remarkable performance and effective processing of event-data, our framework
holds strong potential to become a standard approach for using deep learning
for event-based depth estimation and SLAM. Project page:
https://github.com/tub-rip/DERD-Net

</details>


### [82] [MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search](https://arxiv.org/abs/2504.15865)
*Lotfi Abdelkrim Mecharbat,Ibrahim Elmakky,Martin Takac,Mohammed Yaqub*

Main category: cs.CV

TL;DR: MedNNS是一种针对医学影像的神经网络搜索框架，联合优化架构选择和权重初始化，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像任务中，深度学习模型的架构选择和权重初始化是关键挑战，传统方法（如ImageNet迁移学习）效果有限。

Method: MedNNS通过构建元空间联合优化架构和权重，引入Supernetwork方法扩展模型库，并利用rank loss和FID loss优化空间关系。

Result: 实验表明，MedNNS在多个数据集上优于ImageNet预训练模型和现有NAS方法，平均准确率提升1.7%，收敛速度更快。

Conclusion: MedNNS为医学影像任务提供了一种高效的神经网络搜索解决方案，代码和元空间已开源。

Abstract: Deep learning (DL) has achieved remarkable progress in the field of medical
imaging. However, adapting DL models to medical tasks remains a significant
challenge, primarily due to two key factors: (1) architecture selection, as
different tasks necessitate specialized model designs, and (2) weight
initialization, which directly impacts the convergence speed and final
performance of the models. Although transfer learning from ImageNet is a widely
adopted strategy, its effectiveness is constrained by the substantial
differences between natural and medical images. To address these challenges, we
introduce Medical Neural Network Search (MedNNS), the first Neural Network
Search framework for medical imaging applications. MedNNS jointly optimizes
architecture selection and weight initialization by constructing a meta-space
that encodes datasets and models based on how well they perform together. We
build this space using a Supernetwork-based approach, expanding the model zoo
size by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we
introduce rank loss and Fr\'echet Inception Distance (FID) loss into the
construction of the space to capture inter-model and inter-dataset
relationships, thereby achieving more accurate alignment in the meta-space.
Experimental results across multiple datasets demonstrate that MedNNS
significantly outperforms both ImageNet pre-trained DL models and SOTA Neural
Architecture Search (NAS) methods, achieving an average accuracy improvement of
1.7% across datasets while converging substantially faster. The code and the
processed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS.

</details>


### [83] [Integrating Non-Linear Radon Transformation for Diabetic Retinopathy Grading](https://arxiv.org/abs/2504.15883)
*Farida Mohsen,Samir Belhaouari,Zubair Shah*

Main category: cs.CV

TL;DR: RadFuse是一种多表示深度学习框架，结合了RadEx变换的sinogram图像和传统眼底图像，显著提升了糖尿病视网膜病变的检测和分级能力。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变的早期检测和准确分级对预防视力丧失至关重要，但现有方法难以捕捉病变的复杂、不规则模式。

Method: RadFuse框架通过非线性RadEx变换生成sinogram图像，结合传统眼底图像，利用空间和变换域信息增强特征提取。

Result: 在APTOS-2019和DDR数据集上，RadFuse在多种CNN架构中表现优异，五级分级的加权kappa达93.24%，二元分类准确率达99.09%。

Conclusion: RadFuse通过捕捉复杂非线性特征，推动了糖尿病视网膜病变分类的进步，并促进了数学变换在医学图像分析中的应用。

Abstract: Diabetic retinopathy is a serious ocular complication that poses a
significant threat to patients' vision and overall health. Early detection and
accurate grading are essential to prevent vision loss. Current automatic
grading methods rely heavily on deep learning applied to retinal fundus images,
but the complex, irregular patterns of lesions in these images, which vary in
shape and distribution, make it difficult to capture subtle changes. This study
introduces RadFuse, a multi-representation deep learning framework that
integrates non-linear RadEx-transformed sinogram images with traditional fundus
images to enhance diabetic retinopathy detection and grading. Our RadEx
transformation, an optimized non-linear extension of the Radon transform,
generates sinogram representations to capture complex retinal lesion patterns.
By leveraging both spatial and transformed domain information, RadFuse enriches
the feature set available to deep learning models, improving the
differentiation of severity levels. We conducted extensive experiments on two
benchmark datasets, APTOS-2019 and DDR, using three convolutional neural
networks (CNNs): ResNeXt-50, MobileNetV2, and VGG19. RadFuse showed significant
improvements over fundus-image-only models across all three CNN architectures
and outperformed state-of-the-art methods on both datasets. For severity
grading across five stages, RadFuse achieved a quadratic weighted kappa of
93.24%, an accuracy of 87.07%, and an F1-score of 87.17%. In binary
classification between healthy and diabetic retinopathy cases, the method
reached an accuracy of 99.09%, precision of 98.58%, and recall of 99.6%,
surpassing previously established models. These results demonstrate RadFuse's
capacity to capture complex non-linear features, advancing diabetic retinopathy
classification and promoting the integration of advanced mathematical
transforms in medical image analysis.

</details>


### [84] [MS-Occ: Multi-Stage LiDAR-Camera Fusion for 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2504.15888)
*Zhiqiang Wei,Lianqing Zheng,Jianan Liu,Tao Huang,Qing-Long Han,Wenwen Zhang,Fengdeng Zhang*

Main category: cs.CV

TL;DR: MS-Occ是一种新型多阶段LiDAR-相机融合框架，通过中间和后期融合整合LiDAR的几何精度与相机的语义丰富性，显著提升了3D语义占用感知性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉方法几何不准确和LiDAR方法语义信息不足的问题，提升自动驾驶在复杂环境中的感知能力。

Method: 提出多阶段融合框架，包括中间阶段的Gaussian-Geo模块和Semantic-Aware模块，以及后期阶段的Adaptive Fusion模块和HCCVF模块。

Result: 在nuScenes-OpenOccupancy基准测试中，IoU为32.1%，mIoU为25.3%，优于现有方法。

Conclusion: MS-Occ通过多阶段融合显著提升了3D语义占用感知性能，尤其在小型物体感知方面表现突出，适用于安全关键场景。

Abstract: Accurate 3D semantic occupancy perception is essential for autonomous driving
in complex environments with diverse and irregular objects. While
vision-centric methods suffer from geometric inaccuracies, LiDAR-based
approaches often lack rich semantic information. To address these limitations,
MS-Occ, a novel multi-stage LiDAR-camera fusion framework which includes
middle-stage fusion and late-stage fusion, is proposed, integrating LiDAR's
geometric fidelity with camera-based semantic richness via hierarchical
cross-modal fusion. The framework introduces innovations at two critical
stages: (1) In the middle-stage feature fusion, the Gaussian-Geo module
leverages Gaussian kernel rendering on sparse LiDAR depth maps to enhance 2D
image features with dense geometric priors, and the Semantic-Aware module
enriches LiDAR voxels with semantic context via deformable cross-attention; (2)
In the late-stage voxel fusion, the Adaptive Fusion (AF) module dynamically
balances voxel features across modalities, while the High Classification
Confidence Voxel Fusion (HCCVF) module resolves semantic inconsistencies using
self-attention-based refinement. Experiments on the nuScenes-OpenOccupancy
benchmark show that MS-Occ achieves an Intersection over Union (IoU) of 32.1%
and a mean IoU (mIoU) of 25.3%, surpassing the state-of-the-art by +0.7% IoU
and +2.4% mIoU. Ablation studies further validate the contribution of each
module, with substantial improvements in small-object perception, demonstrating
the practical value of MS-Occ for safety-critical autonomous driving scenarios.

</details>


### [85] [Ask2Loc: Learning to Locate Instructional Visual Answers by Asking Questions](https://arxiv.org/abs/2504.15918)
*Chang Zong,Bin Li,Shoujun Zhou,Jian Wan,Lei Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种新任务In-VAL，模拟人与视频的多轮交互以定位视觉答案，并提出了Ask2Loc框架来解决语义差距问题。


<details>
  <summary>Details</summary>
Motivation: 用户通常需要多次交互才能获得符合预期的视频片段，而现有方法未能有效模拟这一过程。

Method: Ask2Loc框架包含三个模块：聊天模块（明确意图）、重写模块（生成完整描述）和搜索模块（整合内容）。

Result: 在三个重构的In-VAL数据集上，Ask2Loc比传统方法性能提升高达14.91（mIoU）。

Conclusion: Ask2Loc通过多轮交互有效解决了语义差距问题，显著提升了视觉答案定位的性能。

Abstract: Locating specific segments within an instructional video is an efficient way
to acquire guiding knowledge. Generally, the task of obtaining video segments
for both verbal explanations and visual demonstrations is known as visual
answer localization (VAL). However, users often need multiple interactions to
obtain answers that align with their expectations when using the system. During
these interactions, humans deepen their understanding of the video content by
asking themselves questions, thereby accurately identifying the location.
Therefore, we propose a new task, named In-VAL, to simulate the multiple
interactions between humans and videos in the procedure of obtaining visual
answers. The In-VAL task requires interactively addressing several semantic gap
issues, including 1) the ambiguity of user intent in the input questions, 2)
the incompleteness of language in video subtitles, and 3) the fragmentation of
content in video segments. To address these issues, we propose Ask2Loc, a
framework for resolving In-VAL by asking questions. It includes three key
modules: 1) a chatting module to refine initial questions and uncover clear
intentions, 2) a rewriting module to generate fluent language and create
complete descriptions, and 3) a searching module to broaden local context and
provide integrated content. We conduct extensive experiments on three
reconstructed In-VAL datasets. Compared to traditional end-to-end and two-stage
methods, our proposed Ask2Loc can improve performance by up to 14.91 (mIoU) on
the In-VAL task. Our code and datasets can be accessed at
https://github.com/changzong/Ask2Loc.

</details>


### [86] [ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting](https://arxiv.org/abs/2504.15921)
*Jian Hu,Dimitrios Korkinof,Shaogang Gong,Mariano Beguerisse-Diaz*

Main category: cs.CV

TL;DR: ViSMaP是一种无监督视频摘要系统，利用元提示技术生成长视频摘要，无需昂贵的长视频标注。


<details>
  <summary>Details</summary>
Motivation: 解决长视频摘要中稀疏分布事件和无预分割的挑战，避免依赖昂贵的监督训练。

Method: 采用元提示策略，利用LLMs生成和优化伪摘要，结合短视频模型的片段描述。

Result: 在多个数据集上表现媲美全监督的先进模型，且能跨领域泛化。

Conclusion: ViSMaP提供了一种高效、低成本的长视频摘要解决方案，性能优异且通用性强。

Abstract: We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a
system to summarise hour long videos with no-supervision. Most existing video
understanding models work well on short videos of pre-segmented events, yet
they struggle to summarise longer videos where relevant events are sparsely
distributed and not pre-segmented. Moreover, long-form video understanding
often relies on supervised hierarchical training that needs extensive
annotations which are costly, slow and prone to inconsistency. With ViSMaP we
bridge the gap between short videos (where annotated data is plentiful) and
long ones (where it's not). We rely on LLMs to create optimised
pseudo-summaries of long videos using segment descriptions from short ones.
These pseudo-summaries are used as training data for a model that generates
long-form video summaries, bypassing the need for expensive annotations of long
videos. Specifically, we adopt a meta-prompting strategy to iteratively
generate and refine creating pseudo-summaries of long videos. The strategy
leverages short clip descriptions obtained from a supervised short video model
to guide the summary. Each iteration uses three LLMs working in sequence: one
to generate the pseudo-summary from clip descriptions, another to evaluate it,
and a third to optimise the prompt of the generator. This iteration is
necessary because the quality of the pseudo-summaries is highly dependent on
the generator prompt, and varies widely among videos. We evaluate our summaries
extensively on multiple datasets; our results show that ViSMaP achieves
performance comparable to fully supervised state-of-the-art models while
generalising across domains without sacrificing performance. Code will be
released upon publication.

</details>


### [87] [A Clinician-Friendly Platform for Ophthalmic Image Analysis Without Technical Barriers](https://arxiv.org/abs/2504.15928)
*Meng Wang,Tian Lin,Qingshan Hou,Aidi Lin,Jingcheng Wang,Qingsheng Peng,Truong X. Nguyen,Danqi Fang,Ke Zou,Ting Xu,Cancan Xue,Ten Cheer Quek,Qinkai Yu,Minxin Liu,Hui Zhou,Zixuan Xiao,Guiqin He,Huiyu Liang,Tingkun Shi,Man Chen,Linna Liu,Yuanyuan Peng,Lianyu Wang,Qiuming Hu,Junhong Chen,Zhenhua Zhang,Cheng Chen,Yitian Zhao,Dianbo Liu,Jianhua Wu,Xinjian Chen,Changqing Zhang,Triet Thanh Nguyen,Yanda Meng,Yalin Zheng,Yih Chung Tham,Carol Y. Cheung,Huazhu Fu,Haoyu Chen,Ching-Yu Cheng*

Main category: cs.CV

TL;DR: GlobeReady是一个无需重新训练即可跨临床中心使用的AI平台，用于眼科疾病诊断，具有高准确性和临床实用性。


<details>
  <summary>Details</summary>
Motivation: 解决当前AI模型在不同临床中心部署时需要重新训练的问题，推动AI在医学影像诊断中的广泛应用。

Method: 通过训练无关的局部特征增强技术，应对不同中心和人群的领域偏移，并结合置信度量化诊断方法。

Result: 在多种成像模态和不同国家的临床中心中表现出高准确性（最高99.4%），并得到临床医生高度评价（平均4.6/5）。

Conclusion: GlobeReady展示了无需技术障碍即可支持眼科护理的潜力，具有稳健和可扩展的诊断能力。

Abstract: Artificial intelligence (AI) shows remarkable potential in medical imaging
diagnostics, but current models typically require retraining when deployed
across different clinical centers, limiting their widespread adoption. We
introduce GlobeReady, a clinician-friendly AI platform that enables ocular
disease diagnosis without retraining/fine-tuning or technical expertise.
GlobeReady achieves high accuracy across imaging modalities: 93.9-98.5% for an
11-category fundus photo dataset and 87.2-92.7% for a 15-category OCT dataset.
Through training-free local feature augmentation, it addresses domain shifts
across centers and populations, reaching an average accuracy of 88.9% across
five centers in China, 86.3% in Vietnam, and 90.2% in the UK. The built-in
confidence-quantifiable diagnostic approach further boosted accuracy to
94.9-99.4% (fundus) and 88.2-96.2% (OCT), while identifying out-of-distribution
cases at 86.3% (49 CFP categories) and 90.6% (13 OCT categories). Clinicians
from multiple countries rated GlobeReady highly (average 4.6 out of 5) for its
usability and clinical relevance. These results demonstrate GlobeReady's
robust, scalable diagnostic capability and potential to support ophthalmic care
without technical barriers.

</details>


### [88] [Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models](https://arxiv.org/abs/2504.15929)
*Saban Ozturk,Melih B. Yilmaz,Muti Kara,M. Talat Yavuz,Aykut Koç,Tolga Çukur*

Main category: cs.CV

TL;DR: MedTrim提出了一种基于多模态三元组学习的方法，通过结合疾病类别和病理描述符，优化医学图像与文本的对齐，提升下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像和报告数据量增长导致专家压力增加，现有对齐方法忽视细粒度病理属性，影响模型性能。

Method: MedTrim通过元实体识别模块提取病理描述符，设计新的评分函数选择样本，并引入多模态三元组对齐目标。

Result: MedTrim在检索和分类任务中优于现有对齐方法。

Conclusion: MedTrim通过细粒度对齐提升医学视觉-语言模型的性能，具有临床意义。

Abstract: Diagnostic imaging relies on interpreting both images and radiology reports,
but the growing data volumes place significant pressure on medical experts,
yielding increased errors and workflow backlogs. Medical vision-language models
(med-VLMs) have emerged as a powerful framework to efficiently process
multimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit
their performance hinges on how well image and text representations are
aligned. Existing alignment methods, predominantly based on contrastive
learning, prioritize separation between disease classes over segregation of
fine-grained pathology attributes like location, size or severity, leading to
suboptimal representations. Here, we propose MedTrim (Meta-entity-driven
Triplet mining), a novel method that enhances image-text alignment through
multimodal triplet learning synergistically guided by disease class as well as
adjectival and directional pathology descriptors. Unlike common alignment
methods that separate broad disease classes, MedTrim leverages structured
meta-entity information to preserve subtle but clinically significant
intra-class variations. For this purpose, we first introduce an ontology-based
entity recognition module that extracts pathology-specific meta-entities from
CXR reports, as annotations on pathology attributes are rare in public
datasets. For refined sample selection in triplet mining, we then introduce a
novel score function that captures an aggregate measure of inter-sample
similarity based on disease classes and adjectival/directional descriptors.
Lastly, we introduce a multimodal triplet alignment objective for explicit
within- and cross-modal alignment between samples sharing detailed pathology
characteristics. Our demonstrations indicate that MedTrim improves performance
in downstream retrieval and classification tasks compared to state-of-the-art
alignment methods.

</details>


### [89] [Benchmarking the Reproducibility of Brain MRI Segmentation Across Scanners and Time](https://arxiv.org/abs/2504.15931)
*Ekaterina Kondrateva,Sandzhi Barg,Mikhail Vasiliev*

Main category: cs.CV

TL;DR: 论文比较了FastSurfer和SynthSeg两种脑部分割方法在纵向和多中心研究中的表现，发现小脑区存在显著体积变化，并提出表面质量过滤方法以提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决结构MRI中脑部形态测量的准确性和可重复性问题，尤其是在纵向和多中心研究中。

Method: 使用SIMON（17年纵向队列）和SRPBS（9中心测试-重测队列）数据集，通过Dice系数、Surface Dice、Hausdorff距离和MAPE量化分割变异性。

Result: 小脑区（如杏仁核和腹侧间脑）体积变化达7-8%，表明域诱导的形态噪声可能掩盖细微纵向变化。

Conclusion: 研究为形态测量可重复性提供了基准，并强调在实际神经影像研究中需要协调策略。

Abstract: Accurate and reproducible brain morphometry from structural MRI is critical
for monitoring neuroanatomical changes across time and across imaging domains.
Although deep learning has accelerated segmentation workflows, scanner-induced
variability and reproducibility limitations remain-especially in longitudinal
and multi-site settings. In this study, we benchmark two modern segmentation
pipelines, FastSurfer and SynthSeg, both integrated into FreeSurfer, one of the
most widely adopted tools in neuroimaging.
  Using two complementary datasets - a 17-year longitudinal cohort (SIMON) and
a 9-site test-retest cohort (SRPBS)-we quantify inter-scan segmentation
variability using Dice coefficient, Surface Dice, Hausdorff Distance (HD95),
and Mean Absolute Percentage Error (MAPE). Our results reveal up to 7-8% volume
variation in small subcortical structures such as the amygdala and ventral
diencephalon, even under controlled test-retest conditions. This raises a key
question: is it feasible to detect subtle longitudinal changes on the order of
5-10% in pea-sized brain regions, given the magnitude of domain-induced
morphometric noise?
  We further analyze the effects of registration templates and interpolation
modes, and propose surface-based quality filtering to improve segmentation
reliability. This study provides a reproducible benchmark for morphometric
reproducibility and emphasizes the need for harmonization strategies in
real-world neuroimaging studies.
  Code and figures: https://github.com/kondratevakate/brain-mri-segmentation

</details>


### [90] [Reasoning Physical Video Generation with Diffusion Timestep Tokens via Reinforcement Learning](https://arxiv.org/abs/2504.15932)
*Wang Lin,Liyu Jia,Wentao Hu,Kaihang Pan,Zhongqi Yue,Wei Zhao,Jingyuan Chen,Fei Wu,Hanwang Zhang*

Main category: cs.CV

TL;DR: 提出了一种结合符号推理和强化学习的视频生成方法Phys-AR，通过Diffusion Timestep Tokenizer（DDT）和两阶段训练确保视频符合物理规律。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的视频生成方法难以处理未见过的物理条件（如速度），需要改进以增强物理一致性。

Method: 1. 引入DDT学习离散递归视觉标记；2. 提出Phys-AR框架，分两阶段训练：符号知识迁移和强化学习优化。

Result: 实验表明Phys-AR能生成物理一致的视频。

Conclusion: 结合符号推理和强化学习可有效提升视频生成的物理一致性。

Abstract: Despite recent progress in video generation, producing videos that adhere to
physical laws remains a significant challenge. Traditional diffusion-based
methods struggle to extrapolate to unseen physical conditions (eg, velocity)
due to their reliance on data-driven approximations. To address this, we
propose to integrate symbolic reasoning and reinforcement learning to enforce
physical consistency in video generation. We first introduce the Diffusion
Timestep Tokenizer (DDT), which learns discrete, recursive visual tokens by
recovering visual attributes lost during the diffusion process. The recursive
visual tokens enable symbolic reasoning by a large language model. Based on it,
we propose the Phys-AR framework, which consists of two stages: The first stage
uses supervised fine-tuning to transfer symbolic knowledge, while the second
stage applies reinforcement learning to optimize the model's reasoning
abilities through reward functions based on physical conditions. Our approach
allows the model to dynamically adjust and improve the physical properties of
generated videos, ensuring adherence to physical laws. Experimental results
demonstrate that PhysAR can generate videos that are physically consistent.

</details>


### [91] [FreeGraftor: Training-Free Cross-Image Feature Grafting for Subject-Driven Text-to-Image Generation](https://arxiv.org/abs/2504.15958)
*Zebin Yao,Lei Ren,Huixing Jiang,Chen Wei,Xiaojie Wang,Ruifan Li,Fangxiang Feng*

Main category: cs.CV

TL;DR: FreeGraftor提出了一种无需训练的框架，通过跨图像特征嫁接解决主题驱动图像生成中保真度与效率的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在主题驱动图像生成中难以平衡保真度和效率，调优方法耗时耗资源，零样本方法则无法保持主题一致性。

Method: 采用语义匹配和位置约束注意力融合，将参考主题的视觉细节转移到生成图像中，并引入噪声初始化策略以保持几何先验。

Result: 实验表明，FreeGraftor在主题保真度和文本对齐上显著优于现有零样本和无训练方法，且支持多主题生成。

Conclusion: FreeGraftor无需微调或额外训练，实现了高效且高保真的主题驱动图像生成，适用于实际部署。

Abstract: Subject-driven image generation aims to synthesize novel scenes that
faithfully preserve subject identity from reference images while adhering to
textual guidance, yet existing methods struggle with a critical trade-off
between fidelity and efficiency. Tuning-based approaches rely on time-consuming
and resource-intensive subject-specific optimization, while zero-shot methods
fail to maintain adequate subject consistency. In this work, we propose
FreeGraftor, a training-free framework that addresses these limitations through
cross-image feature grafting. Specifically, FreeGraftor employs semantic
matching and position-constrained attention fusion to transfer visual details
from reference subjects to the generated image. Additionally, our framework
incorporates a novel noise initialization strategy to preserve geometry priors
of reference subjects for robust feature matching. Extensive qualitative and
quantitative experiments demonstrate that our method enables precise subject
identity transfer while maintaining text-aligned scene synthesis. Without
requiring model fine-tuning or additional training, FreeGraftor significantly
outperforms existing zero-shot and training-free approaches in both subject
fidelity and text alignment. Furthermore, our framework can seamlessly extend
to multi-subject generation, making it practical for real-world deployment. Our
code is available at https://github.com/Nihukat/FreeGraftor.

</details>


### [92] [Efficient Adaptation of Deep Neural Networks for Semantic Segmentation in Space Applications](https://arxiv.org/abs/2504.15991)
*Leonardo Olivi,Edoardo Santero Mormile,Enzo Tartaglione*

Main category: cs.CV

TL;DR: 论文探讨了在月球和火星地形中，通过适配器实现高效迁移学习用于岩石分割的可行性，提出两种内存节省策略，并评估了其在嵌入式设备上的性能。


<details>
  <summary>Details</summary>
Motivation: 解决地外环境中标记数据稀缺的问题，探索适配器在迁移学习中的应用潜力。

Method: 在预训练骨干模型中策略性地集成适配器，采用层融合和适配器排名两种内存节省策略。

Result: 适配器能有效减少带宽和内存需求，同时保持任务性能，为嵌入式设备提供了可行的解决方案。

Conclusion: 研究展示了适配器在地外环境中的潜力，并为未来研究提供了方向。

Abstract: In recent years, the application of Deep Learning techniques has shown
remarkable success in various computer vision tasks, paving the way for their
deployment in extraterrestrial exploration. Transfer learning has emerged as a
powerful strategy for addressing the scarcity of labeled data in these novel
environments. This paper represents one of the first efforts in evaluating the
feasibility of employing adapters toward efficient transfer learning for rock
segmentation in extraterrestrial landscapes, mainly focusing on lunar and
martian terrains. Our work suggests that the use of adapters, strategically
integrated into a pre-trained backbone model, can be successful in reducing
both bandwidth and memory requirements for the target extraterrestrial device.
In this study, we considered two memory-saving strategies: layer fusion (to
reduce to zero the inference overhead) and an ``adapter ranking'' (to also
reduce the transmission cost). Finally, we evaluate these results in terms of
task performance, memory, and computation on embedded devices, evidencing
trade-offs that open the road to more research in the field.

</details>


### [93] [MVQA: Mamba with Unified Sampling for Efficient Video Quality Assessment](https://arxiv.org/abs/2504.16003)
*Yachun Mi,Yu Li,Weicheng Meng,Chaofeng Chen,Chen Hui,Shaohui Liu*

Main category: cs.CV

TL;DR: MVQA结合Mamba模型和USDS采样方法，高效完成视频质量评估，性能接近SOTA，速度提升2倍，GPU内存仅需1/5。


<details>
  <summary>Details</summary>
Motivation: 长时长高清视频的快速增长使得高效视频质量评估（VQA）成为关键挑战，现有方法在效率与性能间难以平衡。

Method: 提出MVQA模型，基于Mamba结构，结合USDS采样方法（语义与失真采样融合），通过预定义掩码减少计算负担。

Result: MVQA性能接近SOTA，速度提升2倍，GPU内存仅需1/5。

Conclusion: MVQA与USDS为高效VQA提供了新思路，平衡性能与效率。

Abstract: The rapid growth of long-duration, high-definition videos has made efficient
video quality assessment (VQA) a critical challenge. Existing research
typically tackles this problem through two main strategies: reducing model
parameters and resampling inputs. However, light-weight Convolution Neural
Networks (CNN) and Transformers often struggle to balance efficiency with high
performance due to the requirement of long-range modeling capabilities.
Recently, the state-space model, particularly Mamba, has emerged as a promising
alternative, offering linear complexity with respect to sequence length.
Meanwhile, efficient VQA heavily depends on resampling long sequences to
minimize computational costs, yet current resampling methods are often weak in
preserving essential semantic information. In this work, we present MVQA, a
Mamba-based model designed for efficient VQA along with a novel Unified
Semantic and Distortion Sampling (USDS) approach. USDS combines semantic patch
sampling from low-resolution videos and distortion patch sampling from
original-resolution videos. The former captures semantically dense regions,
while the latter retains critical distortion details. To prevent computation
increase from dual inputs, we propose a fusion mechanism using pre-defined
masks, enabling a unified sampling strategy that captures both semantic and
quality information without additional computational burden. Experiments show
that the proposed MVQA, equipped with USDS, achieve comparable performance to
state-of-the-art methods while being $2\times$ as fast and requiring only $1/5$
GPU memory.

</details>


### [94] [Efficient Temporal Consistency in Diffusion-Based Video Editing with Adaptor Modules: A Theoretical Framework](https://arxiv.org/abs/2504.16016)
*Xinyuan Song,Yangfan He,Sida Li,Jianhui Wang,Hongyang He,Xinhang Yuan,Ruoyu Wang,Jiaqi Chen,Keqin Li,Kuan Lu,Menghao Huo,Binxu Li,Pei Liu*

Main category: cs.CV

TL;DR: 论文提出了一种理论框架，用于在DDIM模型中通过适配器保持帧一致性，证明了时间一致性目标的可微性和梯度下降的收敛性，并分析了模块稳定性。


<details>
  <summary>Details</summary>
Motivation: 为基于适配器的视频编辑方法提供理论支持，确保帧间一致性，同时降低训练成本。

Method: 在DDIM模型中插入可学习的小模块，结合共享和帧特定标记的提示学习，通过时间一致性损失优化。

Result: 证明了时间一致性目标的可微性、梯度下降的收敛性，以及模块在DDIM反转过程中的稳定性。

Conclusion: 该理论框架增强了基于适配器的扩散视频编辑方法的可靠性，并为视频生成任务提供了理论见解。

Abstract: Adapter-based methods are commonly used to enhance model performance with
minimal additional complexity, especially in video editing tasks that require
frame-to-frame consistency. By inserting small, learnable modules into
pretrained diffusion models, these adapters can maintain temporal coherence
without extensive retraining. Approaches that incorporate prompt learning with
both shared and frame-specific tokens are particularly effective in preserving
continuity across frames at low training cost. In this work, we want to provide
a general theoretical framework for adapters that maintain frame consistency in
DDIM-based models under a temporal consistency loss. First, we prove that the
temporal consistency objective is differentiable under bounded feature norms,
and we establish a Lipschitz bound on its gradient. Second, we show that
gradient descent on this objective decreases the loss monotonically and
converges to a local minimum if the learning rate is within an appropriate
range. Finally, we analyze the stability of modules in the DDIM inversion
procedure, showing that the associated error remains controlled. These
theoretical findings will reinforce the reliability of diffusion-based video
editing methods that rely on adapter strategies and provide theoretical
insights in video generation tasks.

</details>


### [95] [PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud Learning](https://arxiv.org/abs/2504.16023)
*Song Wang,Xiaolu Liu,Lingdong Kong,Jianyun Xu,Chunyong Hu,Gongfan Fang,Wentong Li,Jianke Zhu,Xinchao Wang*

Main category: cs.CV

TL;DR: PointLoRA是一种结合低秩适应（LoRA）和多尺度令牌选择的高效点云模型微调方法，显著减少可调参数，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 随着预训练模型复杂度增加，完全微调需要大量计算和存储资源，而现有参数高效微调方法通常依赖复杂机制，增加了可调参数。

Method: 在点云变换器中嵌入LoRA层以减少可调参数，并结合多尺度令牌选择提取关键局部信息作为下游微调的提示。

Result: 实验表明，PointLoRA仅需3.43%的可调参数，即可在多个预训练模型和公共数据集上实现竞争性性能。

Conclusion: PointLoRA是一种简单高效的方法，适用于资源受限的应用场景。

Abstract: Self-supervised representation learning for point cloud has demonstrated
effectiveness in improving pre-trained model performance across diverse tasks.
However, as pre-trained models grow in complexity, fully fine-tuning them for
downstream applications demands substantial computational and storage
resources. Parameter-efficient fine-tuning (PEFT) methods offer a promising
solution to mitigate these resource requirements, yet most current approaches
rely on complex adapter and prompt mechanisms that increase tunable parameters.
In this paper, we propose PointLoRA, a simple yet effective method that
combines low-rank adaptation (LoRA) with multi-scale token selection to
efficiently fine-tune point cloud models. Our approach embeds LoRA layers
within the most parameter-intensive components of point cloud transformers,
reducing the need for tunable parameters while enhancing global feature
capture. Additionally, multi-scale token selection extracts critical local
information to serve as prompts for downstream fine-tuning, effectively
complementing the global context captured by LoRA. The experimental results
across various pre-trained models and three challenging public datasets
demonstrate that our approach achieves competitive performance with only 3.43%
of the trainable parameters, making it highly effective for
resource-constrained applications. Source code is available at:
https://github.com/songw-zju/PointLoRA.

</details>


### [96] [LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale](https://arxiv.org/abs/2504.16030)
*Joya Chen,Ziyun Zeng,Yiqi Lin,Wei Li,Zejun Ma,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 论文提出了一种利用自动语音识别（ASR）转录进行视频大语言模型（Video LLM）大规模训练的方法，通过流式训练和高质量数据集，模型在视频问答和实时评论任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型依赖昂贵的人工标注或专有模型API，限制了大规模训练。本文探索利用低成本ASR转录实现高效训练。

Method: 提出流式训练方法，将ASR词与视频帧按时间戳密集交错，并构建Live-CC-5M和Live-WhisperX-526K数据集支持训练。

Result: ASR预训练的LiveCC-7B-Base模型在视频问答和实时评论任务中表现优异，最终模型LiveCC-7B-Instruct超越72B模型。

Conclusion: 该方法展示了低成本ASR数据在大规模视频语言模型训练中的潜力，并实现了广泛通用性。

Abstract: Recent video large language models (Video LLMs) often depend on costly human
annotations or proprietary model APIs (e.g., GPT-4o) to produce training data,
which limits their training at scale. In this paper, we explore large-scale
training for Video LLM with cheap automatic speech recognition (ASR)
transcripts. Specifically, we propose a novel streaming training approach that
densely interleaves the ASR words and video frames according to their
timestamps. Compared to previous studies in vision-language representation with
ASR, our method naturally fits the streaming characteristics of ASR, thus
enabling the model to learn temporally-aligned, fine-grained vision-language
modeling. To support the training algorithm, we introduce a data production
pipeline to process YouTube videos and their closed captions (CC, same as ASR),
resulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset
for high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,
the ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general
video QA performance and exhibits a new capability in real-time video
commentary. To evaluate this, we carefully design a new LiveSports-3K
benchmark, using LLM-as-a-judge to measure the free-form commentary.
Experiments show our final LiveCC-7B-Instruct model can surpass advanced 72B
models (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even
working in a real-time mode. Meanwhile, it achieves state-of-the-art results at
the 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,
demonstrating the broad generalizability of our approach. All resources of this
paper have been released at https://showlab.github.io/livecc.

</details>


### [97] [Survey of Video Diffusion Models: Foundations, Implementations, and Applications](https://arxiv.org/abs/2504.16081)
*Yimu Wang,Xuye Liu,Wei Pang,Li Ma,Shuai Yuan,Paul Debevec,Ning Yu*

Main category: cs.CV

TL;DR: 这篇综述全面回顾了基于扩散模型的视频生成技术，涵盖了其发展、技术基础、实际应用及与其他领域的协同作用，并提供了更广泛和细致的视角。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视频生成领域展现出巨大潜力，但在运动一致性、计算效率和伦理问题上面临挑战，因此需要系统性的综述。

Method: 通过系统分类现有方法、分析架构创新和优化策略，并探讨其在低层视觉任务中的应用。

Result: 提供了对扩散模型在视频生成中的理论框架和实际实现的深入见解，并补充了评估指标、行业解决方案和训练工程技术。

Conclusion: 该综述为研究者和从业者提供了基础资源，推动了这一快速发展领域的理论和实践发展。

Abstract: Recent advances in diffusion models have revolutionized video generation,
offering superior temporal consistency and visual quality compared to
traditional generative adversarial networks-based approaches. While this
emerging field shows tremendous promise in applications, it faces significant
challenges in motion consistency, computational efficiency, and ethical
considerations. This survey provides a comprehensive review of diffusion-based
video generation, examining its evolution, technical foundations, and practical
applications. We present a systematic taxonomy of current methodologies,
analyze architectural innovations and optimization strategies, and investigate
applications across low-level vision tasks such as denoising and
super-resolution. Additionally, we explore the synergies between diffusionbased
video generation and related domains, including video representation learning,
question answering, and retrieval. Compared to the existing surveys (Lei et
al., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which
focus on specific aspects of video generation, such as human video synthesis
(Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our
work provides a broader, more updated, and more fine-grained perspective on
diffusion-based approaches with a special section for evaluation metrics,
industry solutions, and training engineering techniques in video generation.
This survey serves as a foundational resource for researchers and practitioners
working at the intersection of diffusion models and video generation, providing
insights into both the theoretical frameworks and practical implementations
that drive this rapidly evolving field. A structured list of related works
involved in this survey is also available on
https://github.com/Eyeline-Research/Survey-Video-Diffusion.

</details>


### [98] [Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis](https://arxiv.org/abs/2504.16047)
*Frank Li,Hari Trivedi,Bardia Khosravi,Theo Dapamede,Mohammadreza Chavoshi,Abdulhameed Dere,Rohan Satya Isaac,Aawez Mansuri,Janice Newsome,Saptarshi Purkayastha,Judy Gichoya*

Main category: cs.CV

TL;DR: 研究评估了三种视觉语言基础模型（RAD-DINO、CheXagent和BiomedCLIP）在放射学任务中的表现，发现预训练方法对下游任务性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在医学影像任务中的应用潜力，尤其是对细粒度特征的捕捉能力。

Method: 评估三种模型在胸部X光片的分类、分割和回归任务中的表现，并设计了一个结合全局和局部特征的自定义分割模型。

Result: RAD-DINO在分割任务中表现最佳，CheXagent在分类任务中表现优异，BiomedCLIP表现不稳定。自定义模型显著提升了所有模型的分割性能。

Conclusion: 无文本监督的模型更适合细粒度分割任务，而文本监督模型在分类和可解释性方面有优势，为临床应用中模型选择提供了指导。

Abstract: Foundation models, trained on vast amounts of data using self-supervised
techniques, have emerged as a promising frontier for advancing artificial
intelligence (AI) applications in medicine. This study evaluates three
different vision-language foundation models (RAD-DINO, CheXagent, and
BiomedCLIP) on their ability to capture fine-grained imaging features for
radiology tasks. The models were assessed across classification, segmentation,
and regression tasks for pneumothorax and cardiomegaly on chest radiographs.
Self-supervised RAD-DINO consistently excelled in segmentation tasks, while
text-supervised CheXagent demonstrated superior classification performance.
BiomedCLIP showed inconsistent performance across tasks. A custom segmentation
model that integrates global and local features substantially improved
performance for all foundation models, particularly for challenging
pneumothorax segmentation. The findings highlight that pre-training methodology
significantly influences model performance on specific downstream tasks. For
fine-grained segmentation tasks, models trained without text supervision
performed better, while text-supervised models offered advantages in
classification and interpretability. These insights provide guidance for
selecting foundation models based on specific clinical applications in
radiology.

</details>


### [99] [Vision language models are unreliable at trivial spatial cognition](https://arxiv.org/abs/2504.16061)
*Sangeet Khemlani,Tyler Tran,Nathaniel Gyory,Anthony M. Harrison,Wallace E. Lawson,Ravenna Thielstrom,Hunter Thompson,Taaren Singh,J. Gregory Trafton*

Main category: cs.CV

TL;DR: VLMs在空间认知任务中的可靠性受提示词微小变化影响，揭示了其在空间关系推理上的局限性。


<details>
  <summary>Details</summary>
Motivation: 测试VLMs在简单空间认知任务中的可靠性，以评估其广泛应用的潜力。

Method: 开发了TableTest基准数据集，用于评估VLMs在识别物体空间关系中的表现。

Result: VLMs的性能因提示词的微小变化而下降，显示出空间推理的局限性。

Conclusion: VLMs在空间关系推理上存在不足，但为改进图像标注数据提供了新机会。

Abstract: Vision language models (VLMs) are designed to extract relevant visuospatial
information from images. Some research suggests that VLMs can exhibit humanlike
scene understanding, while other investigations reveal difficulties in their
ability to process relational information. To achieve widespread applicability,
VLMs must perform reliably, yielding comparable competence across a wide
variety of related tasks. We sought to test how reliable these architectures
are at engaging in trivial spatial cognition, e.g., recognizing whether one
object is left of another in an uncluttered scene. We developed a benchmark
dataset -- TableTest -- whose images depict 3D scenes of objects arranged on a
table, and used it to evaluate state-of-the-art VLMs. Results show that
performance could be degraded by minor variations of prompts that use logically
equivalent descriptions. These analyses suggest limitations in how VLMs may
reason about spatial relations in real-world applications. They also reveal
novel opportunities for bolstering image caption corpora for more efficient
training and testing.

</details>


### [100] [Boosting Generative Image Modeling via Joint Image-Feature Synthesis](https://arxiv.org/abs/2504.16064)
*Theodoros Kouzelis,Efstathios Karypidis,Ioannis Kakogeorgiou,Spyros Gidaris,Nikos Komodakis*

Main category: cs.CV

TL;DR: 提出了一种结合低层级图像潜在表示和高层级语义特征的生成图像建模框架，通过扩散模型联合建模，显著提升生成质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决潜在扩散模型（LDMs）中表示学习与生成建模难以结合的问题。

Method: 利用扩散模型联合建模变分自编码器的低层级图像潜在表示和预训练自监督编码器（如DINO）的高层级语义特征。

Result: 在条件和非条件设置下，图像质量和训练收敛速度显著提升。

Conclusion: 为表示感知生成建模开辟了新方向，简化训练并解锁了新的推理策略（表示引导）。

Abstract: Latent diffusion models (LDMs) dominate high-quality image generation, yet
integrating representation learning with generative modeling remains a
challenge. We introduce a novel generative image modeling framework that
seamlessly bridges this gap by leveraging a diffusion model to jointly model
low-level image latents (from a variational autoencoder) and high-level
semantic features (from a pretrained self-supervised encoder like DINO). Our
latent-semantic diffusion approach learns to generate coherent image-feature
pairs from pure noise, significantly enhancing both generative quality and
training efficiency, all while requiring only minimal modifications to standard
Diffusion Transformer architectures. By eliminating the need for complex
distillation objectives, our unified design simplifies training and unlocks a
powerful new inference strategy: Representation Guidance, which leverages
learned semantics to steer and refine image generation. Evaluated in both
conditional and unconditional settings, our method delivers substantial
improvements in image quality and training convergence speed, establishing a
new direction for representation-aware generative modeling.

</details>


### [101] [Describe Anything: Detailed Localized Image and Video Captioning](https://arxiv.org/abs/2504.16072)
*Long Lian,Yifan Ding,Yunhao Ge,Sifei Liu,Hanzi Mao,Boyi Li,Marco Pavone,Ming-Yu Liu,Trevor Darrell,Adam Yala,Yin Cui*

Main category: cs.CV

TL;DR: DAM模型通过焦点提示和局部视觉骨干网络实现高分辨率局部描述，结合半监督学习数据管道解决数据稀缺问题，并在多个基准测试中取得最优表现。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在图像和视频中生成详细区域描述的挑战。

Method: 提出DAM模型，采用焦点提示和局部视觉骨干网络；设计半监督学习数据管道DLC-SDP。

Result: 在7个基准测试中取得最优表现。

Conclusion: DAM模型在局部描述任务中表现出色，解决了数据稀缺和描述精度问题。

Abstract: Generating detailed and accurate descriptions for specific regions in images
and videos remains a fundamental challenge for vision-language models. We
introduce the Describe Anything Model (DAM), a model designed for detailed
localized captioning (DLC). DAM preserves both local details and global context
through two key innovations: a focal prompt, which ensures high-resolution
encoding of targeted regions, and a localized vision backbone, which integrates
precise localization with its broader context. To tackle the scarcity of
high-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data
Pipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and
expands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark
designed to evaluate DLC without relying on reference captions. DAM sets new
state-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and
detailed multi-sentence localized image and video captioning.

</details>


### [102] [From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning](https://arxiv.org/abs/2504.16080)
*Le Zhuo,Liangbing Zhao,Sayak Paul,Yue Liao,Renrui Zhang,Yi Xin,Peng Gao,Mohamed Elhoseiny,Hongsheng Li*

Main category: cs.CV

TL;DR: ReflectionFlow是一个推理时框架，通过噪声级、提示级和反射级三个扩展轴，帮助扩散模型迭代优化输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在复杂场景和细节处理上表现不足，受大语言模型自反思能力启发，提出改进方案。

Method: 提出ReflectionFlow框架，利用噪声级、提示级和反射级扩展轴，结合GenRef数据集进行反射调优。

Result: ReflectionFlow显著优于传统噪声级扩展方法，提升图像合成质量。

Conclusion: ReflectionFlow为高质量图像合成提供了可扩展且计算高效的解决方案。

Abstract: Recent text-to-image diffusion models achieve impressive visual quality
through extensive scaling of training data and model parameters, yet they often
struggle with complex scenes and fine-grained details. Inspired by the
self-reflection capabilities emergent in large language models, we propose
ReflectionFlow, an inference-time framework enabling diffusion models to
iteratively reflect upon and refine their outputs. ReflectionFlow introduces
three complementary inference-time scaling axes: (1) noise-level scaling to
optimize latent initialization; (2) prompt-level scaling for precise semantic
guidance; and most notably, (3) reflection-level scaling, which explicitly
provides actionable reflections to iteratively assess and correct previous
generations. To facilitate reflection-level scaling, we construct GenRef, a
large-scale dataset comprising 1 million triplets, each containing a
reflection, a flawed image, and an enhanced image. Leveraging this dataset, we
efficiently perform reflection tuning on state-of-the-art diffusion
transformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified
framework. Experimental results show that ReflectionFlow significantly
outperforms naive noise-level scaling methods, offering a scalable and
compute-efficient solution toward higher-quality image synthesis on challenging
tasks.

</details>


### [103] [MR. Video: "MapReduce" is the Principle for Long Video Understanding](https://arxiv.org/abs/2504.16082)
*Ziqi Pang,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: MR. Video是一个基于MapReduce原则的长视频理解框架，通过独立感知短片段（Map）和联合聚合信息（Reduce）提升性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有序列到序列视觉语言模型（VLMs）和视频代理在长视频理解中因上下文长度限制或依赖关键片段选择而性能不足的问题。

Method: 采用两阶段MapReduce：(A) 字幕生成：独立生成短片段字幕并标准化；(B) 分析：针对用户问题分析片段信息并整合。

Result: 在LVBench上比现有VLMs和视频代理准确率提升超过10%。

Conclusion: MapReduce原则适用于VLMs和视频代理，MR. Video框架在长视频理解中表现优越。

Abstract: We propose MR. Video, an agentic long video understanding framework that
demonstrates the simple yet effective MapReduce principle for processing long
videos: (1) Map: independently and densely perceiving short video clips, and
(2) Reduce: jointly aggregating information from all clips. Compared with
sequence-to-sequence vision-language models (VLMs), MR. Video performs detailed
short video perception without being limited by context length. Compared with
existing video agents that typically rely on sequential key segment selection,
the Map operation enables simpler and more scalable sequence parallel
perception of short video segments. Its Reduce step allows for more
comprehensive context aggregation and reasoning, surpassing explicit key
segment retrieval. This MapReduce principle is applicable to both VLMs and
video agents, and we use LLM agents to validate its effectiveness.
  In practice, MR. Video employs two MapReduce stages: (A) Captioning:
generating captions for short video clips (map), then standardizing repeated
characters and objects into shared names (reduce); (B) Analysis: for each user
question, analyzing relevant information from individual short videos (map),
and integrating them into a final answer (reduce). MR. Video achieves over 10%
accuracy improvement on the challenging LVBench compared to state-of-the-art
VLMs and video agents.
  Code is available at: https://github.com/ziqipang/MR-Video

</details>


### [104] [MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention](https://arxiv.org/abs/2504.16083)
*Yucheng Li,Huiqiang Jiang,Chengruidong Zhang,Qianhui Wu,Xufang Luo,Surin Ahn,Amir H. Abdi,Dongsheng Li,Jianfeng Gao,Yuqing Yang,Lili Qiu*

Main category: cs.CV

TL;DR: MMInference是一种动态稀疏注意力方法，显著加速了长上下文多模态输入的预填充阶段，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型（VLMs）在长上下文多模态输入中预填充阶段二次注意力复杂度的瓶颈问题。

Method: 提出基于时空局部性的Grid稀疏模式，通过离线搜索最优稀疏分布，并优化GPU内核以实现高效稀疏计算。

Result: 在1M token的输入下，预填充阶段加速高达8.3倍，且不影响模型准确性。

Conclusion: MMInference无需修改模型即可无缝集成到现有VLM流程中，为长上下文多模态任务提供了高效解决方案。

Abstract: The integration of long-context capabilities with visual understanding
unlocks unprecedented potential for Vision Language Models (VLMs). However, the
quadratic attention complexity during the pre-filling phase remains a
significant obstacle to real-world deployment. To overcome this limitation, we
introduce MMInference (Multimodality Million tokens Inference), a dynamic
sparse attention method that accelerates the prefilling stage for long-context
multi-modal inputs. First, our analysis reveals that the temporal and spatial
locality of video input leads to a unique sparse pattern, the Grid pattern.
Simultaneously, VLMs exhibit markedly different sparse distributions across
different modalities. We introduce a permutation-based method to leverage the
unique Grid pattern and handle modality boundary issues. By offline search the
optimal sparse patterns for each head, MMInference constructs the sparse
distribution dynamically based on the input. We also provide optimized GPU
kernels for efficient sparse computations. Notably, MMInference integrates
seamlessly into existing VLM pipelines without any model modifications or
fine-tuning. Experiments on multi-modal benchmarks-including Video QA,
Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art
long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that
MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while
maintaining accuracy. Our code is available at https://aka.ms/MMInference.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [105] [Can Machine Learning Agents Deal with Hard Choices?](https://arxiv.org/abs/2504.15304)
*Kangyu Wang*

Main category: cs.AI

TL;DR: 论文探讨了机器学习（ML）代理在多目标决策中无法识别和解决“艰难选择”的问题，并提出了一种集成解决方案，但仍无法像人类那样通过审议解决此类问题。


<details>
  <summary>Details</summary>
Motivation: 研究ML代理在多目标决策中与人类推理的差异，尤其是无法处理“艰难选择”的局限性，以及由此产生的对齐问题。

Method: 分析了现有多目标优化方法（如标量优化和帕累托优化）的不足，并评估了两种潜在技术解决方案，推荐了一种集成方法。

Result: 集成方法能帮助ML代理识别“艰难选择”并缓解对齐问题，但仍无法通过审议解决此类问题。

Conclusion: 人类代理的独特性凸显了ML代理的局限性，呼吁重新定义机器自主性并开发新框架以填补这一根本差距。

Abstract: Machine Learning ML agents have been increasingly used in decision-making
across a wide range of tasks and environments. These ML agents are typically
designed to balance multiple objectives when making choices. Understanding how
their decision-making processes align with or diverge from human reasoning is
essential. Human agents often encounter hard choices, that is, situations where
options are incommensurable; neither option is preferred, yet the agent is not
indifferent between them. In such cases, human agents can identify hard choices
and resolve them through deliberation. In contrast, current ML agents, due to
fundamental limitations in Multi-Objective Optimisation or MOO methods, cannot
identify hard choices, let alone resolve them. Neither Scalarised Optimisation
nor Pareto Optimisation, the two principal MOO approaches, can capture
incommensurability. This limitation generates three distinct alignment
problems: the alienness of ML decision-making behaviour from a human
perspective; the unreliability of preference-based alignment strategies for
hard choices; and the blockage of alignment strategies pursuing multiple
objectives. Evaluating two potential technical solutions, I recommend an
ensemble solution that appears most promising for enabling ML agents to
identify hard choices and mitigate alignment problems. However, no known
technique allows ML agents to resolve hard choices through deliberation, as
they cannot autonomously change their goals. This underscores the
distinctiveness of human agency and urges ML researchers to reconceptualise
machine autonomy and develop frameworks and methods that can better address
this fundamental gap.

</details>


### [106] [PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness with Theory of Mind](https://arxiv.org/abs/2504.15313)
*Yajie Yu,Yue Feng*

Main category: cs.AI

TL;DR: PolicyEvol-Agent是一个基于LLM的多智能体框架，通过系统获取他人意图和自适应优化策略，在动态交互场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有研究在动态交互场景中缺乏有效的认知链（如推理、规划、决策和反思），且提示式响应在心理状态感知和经验校准方面存在挑战。

Method: PolicyEvol-Agent通过获取反思性专业知识模式，结合心理理论和内外视角的认知操作，优化策略。

Result: 仿真结果显示，PolicyEvol-Agent优于基于强化学习和传统智能体的方法，并在自动和人工评估中验证了动态策略调整的有效性。

Conclusion: PolicyEvol-Agent在动态交互场景中表现出色，为多智能体系统的认知优化提供了新思路。

Abstract: Multi-agents has exhibited significant intelligence in real-word simulations
with Large language models (LLMs) due to the capabilities of social cognition
and knowledge retrieval. However, existing research on agents equipped with
effective cognition chains including reasoning, planning, decision-making and
reflecting remains limited, especially in the dynamically interactive
scenarios. In addition, unlike human, prompt-based responses face challenges in
psychological state perception and empirical calibration during uncertain
gaming process, which can inevitably lead to cognition bias. In light of above,
we introduce PolicyEvol-Agent, a comprehensive LLM-empowered framework
characterized by systematically acquiring intentions of others and adaptively
optimizing irrational strategies for continual enhancement. Specifically,
PolicyEvol-Agent first obtains reflective expertise patterns and then
integrates a range of cognitive operations with Theory of Mind alongside
internal and external perspectives. Simulation results, outperforming RL-based
models and agent-based methods, demonstrate the superiority of PolicyEvol-Agent
for final gaming victory. Moreover, the policy evolution mechanism reveals the
effectiveness of dynamic guideline adjustments in both automatic and human
evaluation.

</details>


### [107] [Reliable Classification with Conformal Learning and Interval-Type 2 Fuzzy Sets](https://arxiv.org/abs/2504.15360)
*Javier Fumanal-Idocin,Javier Andreu-Perez*

Main category: cs.AI

TL;DR: 论文提出使用共形学习与模糊规则系统结合，提升分类模型的可靠性，并探讨了类型2模糊集对系统性能的改进。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习分类器在实验室外可能不可靠，需要更可靠的预测质量评估方法。

Method: 结合共形学习与模糊规则系统，并引入类型2模糊集改进输出质量。

Result: 共形学习能提供更可靠的预测覆盖，类型2模糊集进一步提升了系统性能。

Conclusion: 共形学习与模糊规则系统的结合，尤其是类型2模糊集的使用，显著提升了分类模型的可靠性和预测质量。

Abstract: Classical machine learning classifiers tend to be overconfident can be
unreliable outside of the laboratory benchmarks. Properly assessing the
reliability of the output of the model per sample is instrumental for real-life
scenarios where these systems are deployed. Because of this, different
techniques have been employed to properly quantify the quality of prediction
for a given model. These are most commonly Bayesian statistics and, more
recently, conformal learning. Given a calibration set, conformal learning can
produce outputs that are guaranteed to cover the target class with a desired
significance level, and are more reliable than the standard confidence
intervals used by Bayesian methods. In this work, we propose to use conformal
learning with fuzzy rule-based systems in classification and show some metrics
of their performance. Then, we discuss how the use of type 2 fuzzy sets can
improve the quality of the output of the system compared to both fuzzy and
crisp rules. Finally, we also discuss how the fine-tuning of the system can be
adapted to improve the quality of the conformal prediction.

</details>


### [108] [KeDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments](https://arxiv.org/abs/2504.15364)
*Junyoung Park,Dalton Jones,Matt Morse,Raghavv Goel,Mingu Lee,Chris Lott*

Main category: cs.AI

TL;DR: KeyDiff是一种无需训练的KV缓存淘汰方法，基于键相似性，适用于资源受限环境中的长输入提示LLM推理。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限环境中部署长输入提示LLM应用时KV缓存占用过高的问题。

Method: 提出KeyDiff方法，通过最大化键多样性优化KV缓存选择，不依赖注意力分数。

Result: 在8K缓存预算下，性能损失小于0.04%，KV缓存减少约23%。

Conclusion: KeyDiff在资源受限环境中高效处理长提示，且兼容优化注意力机制。

Abstract: In this work, we demonstrate that distinctive keys during LLM inference tend
to have high attention scores. We explore this phenomenon and propose KeyDiff,
a training-free KV cache eviction method based on key similarity. This method
facilitates the deployment of LLM-based application requiring long input
prompts in resource-constrained environments with limited memory and compute
budgets. Unlike other KV cache eviction methods, KeyDiff can process
arbitrarily long prompts within strict resource constraints and efficiently
generate responses. We demonstrate that KeyDiff computes the optimal solution
to a KV cache selection problem that maximizes key diversity, providing a
theoretical understanding of KeyDiff. Notably,KeyDiff does not rely on
attention scores, allowing the use of optimized attention mechanisms like
FlashAttention. We demonstrate the effectiveness of KeyDiff across diverse
tasks and models, illustrating a performance gap of less than 0.04\% with 8K
cache budget ($\sim$ 23\% KV cache reduction) from the non-evicting baseline on
the LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.

</details>


### [109] [AGI Is Coming... Right After AI Learns to Play Wordle](https://arxiv.org/abs/2504.15434)
*Sarath Shekkizhar,Romain Cosentino*

Main category: cs.AI

TL;DR: 研究探讨了OpenAI的多模态代理CUA在完成计算机任务时的表现，发现其在Wordle游戏中颜色识别存在显著问题，成功率仅为5.36%。


<details>
  <summary>Details</summary>
Motivation: 评估前沿AI模型在简单任务中的表现，揭示其局限性，为未来改进提供方向。

Method: 通过让CUA在纽约时报的Wordle游戏中完成任务，观察其行为并分析性能。

Result: 模型在颜色识别上表现不佳，成功率低，显示简单任务对当前AI仍具挑战性。

Conclusion: 研究强调了AI在简单任务中的不足，提出了未来改进的研究方向。

Abstract: This paper investigates multimodal agents, in particular, OpenAI's
Computer-User Agent (CUA), trained to control and complete tasks through a
standard computer interface, similar to humans. We evaluated the agent's
performance on the New York Times Wordle game to elicit model behaviors and
identify shortcomings. Our findings revealed a significant discrepancy in the
model's ability to recognize colors correctly depending on the context. The
model had a $5.36\%$ success rate over several hundred runs across a week of
Wordle. Despite the immense enthusiasm surrounding AI agents and their
potential to usher in Artificial General Intelligence (AGI), our findings
reinforce the fact that even simple tasks present substantial challenges for
today's frontier AI models. We conclude with a discussion of the potential
underlying causes, implications for future development, and research directions
to improve these AI systems.

</details>


### [110] [Improving Human-AI Coordination through Adversarial Training and Generative Models](https://arxiv.org/abs/2504.15457)
*Paresh Chaudhary,Yancheng Liang,Daphne Chen,Simon S. Du,Natasha Jaques*

Main category: cs.AI

TL;DR: 论文提出了一种名为GOAT的新方法，结合生成模型和对抗训练，以解决合作任务中对抗策略的自我破坏问题，并在Overcooked基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 合作任务中，对抗训练难以模拟有效合作行为，导致泛化能力受限。

Method: 结合预训练生成模型模拟合作策略，并通过对抗训练最大化遗憾，动态生成挑战性交互场景。

Result: 在Overcooked基准测试中实现最优性能，有效泛化到多样人类行为。

Conclusion: GOAT方法通过生成对抗训练显著提升了合作任务的泛化能力。

Abstract: Being able to cooperate with new people is an important component of many
economically valuable AI tasks, from household robotics to autonomous driving.
However, generalizing to novel humans requires training on data that captures
the diversity of human behaviors. Adversarial training is one avenue for
searching for such data and ensuring that agents are robust. However, it is
difficult to apply in the cooperative setting because adversarial policies
intentionally learn to sabotage the task instead of simulating valid
cooperation partners. To address this challenge, we propose a novel strategy
for overcoming self-sabotage that combines a pre-trained generative model to
simulate valid cooperative agent policies with adversarial training to maximize
regret. We call our method GOAT: Generative Online Adversarial Training. In
this framework, the GOAT dynamically searches for and generates coordination
strategies where the learning policy -- the Cooperator agent -- underperforms.
GOAT enables better generalization by exposing the Cooperator to various
challenging interaction scenarios. We maintain realistic coordination
strategies by updating only the generative model's embedding while keeping its
parameters frozen, thus avoiding adversarial exploitation. We evaluate GOAT
with real human partners, and the results demonstrate state-of-the-art
performance on the Overcooked benchmark, highlighting its effectiveness in
generalizing to diverse human behaviors.

</details>


### [111] [Learning Adaptive Parallel Reasoning with Language Models](https://arxiv.org/abs/2504.15466)
*Jiayi Pan,Xiuyu Li,Long Lian,Charlie Snell,Yifei Zhou,Adam Yala,Trevor Darrell,Kurt Keutzer,Alane Suhr*

Main category: cs.AI

TL;DR: APR是一种自适应并行推理框架，通过结合串行和并行计算优化语言模型的推理能力，显著提升性能和扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法（如串行链式思维和并行自一致性）存在输出过长或协调不足的问题，限制了推理效率和性能。

Method: APR采用自适应多线程推理，结合spawn()和join()操作，并通过端到端强化学习优化推理线程。

Result: 在Countdown任务中，APR在相同上下文窗口（83.4% vs. 60.0%）、扩展性（80.1% vs. 66.6%）和延迟（75.2% vs. 57.3%）方面均优于现有方法。

Conclusion: APR为语言模型通过自适应计算分配优化推理过程提供了新方向。

Abstract: Scaling inference-time computation has substantially improved the reasoning
capabilities of language models. However, existing methods have significant
limitations: serialized chain-of-thought approaches generate overly long
outputs, leading to increased latency and exhausted context windows, while
parallel methods such as self-consistency suffer from insufficient
coordination, resulting in redundant computations and limited performance
gains. To address these shortcomings, we propose Adaptive Parallel Reasoning
(APR), a novel reasoning framework that enables language models to orchestrate
both serialized and parallel computations end-to-end. APR generalizes existing
reasoning methods by enabling adaptive multi-threaded inference using spawn()
and join() operations. A key innovation is our end-to-end reinforcement
learning strategy, optimizing both parent and child inference threads to
enhance task success rate without requiring predefined reasoning structures.
Experiments on the Countdown reasoning task demonstrate significant benefits of
APR: (1) higher performance within the same context window (83.4% vs. 60.0% at
4k context); (2) superior scalability with increased computation (80.1% vs.
66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%
vs. 57.3% at approximately 5,000ms). APR represents a step towards enabling
language models to autonomously optimize their reasoning processes through
adaptive allocation of computation.

</details>


### [112] [A Multi-Agent Framework for Automated Qinqiang Opera Script Generation Using Large Language Models](https://arxiv.org/abs/2504.15552)
*Gengxian Cao,Fengyuan Li,Hong Duan,Ye Yang,Bofeng Wang,Donghe Li*

Main category: cs.AI

TL;DR: 本文提出了一种多智能体框架，通过整合大语言模型、视觉生成和文本到语音合成，自动化生产秦腔戏曲。三个智能体协作完成脚本、场景和语音生成，实验表明其效果优于单智能体基线。


<details>
  <summary>Details</summary>
Motivation: 旨在利用AI技术保护和规模化传统戏曲艺术的生产，解决人工创作的效率和质量问题。

Method: 采用三个专门化的智能体：Agent1生成脚本，Agent2生成舞台场景，Agent3合成语音，通过模块化协作完成端到端生产。

Result: 在《窦娥冤》案例中，系统在脚本忠实度、视觉连贯性和语音准确性上分别获得3.8、3.5和3.8分，总分3.6，优于基线0.3分。

Conclusion: 展示了AI驱动的流水线在传统艺术保护中的潜力，未来可改进跨模态对齐和情感表达。

Abstract: This paper introduces a novel multi-Agent framework that automates the end to
end production of Qinqiang opera by integrating Large Language Models , visual
generation, and Text to Speech synthesis. Three specialized agents collaborate
in sequence: Agent1 uses an LLM to craft coherent, culturally grounded
scripts;Agent2 employs visual generation models to render contextually accurate
stage scenes; and Agent3 leverages TTS to produce synchronized, emotionally
expressive vocal performances. In a case study on Dou E Yuan, the system
achieved expert ratings of 3.8 for script fidelity, 3.5 for visual coherence,
and 3.8 for speech accuracy-culminating in an overall score of 3.6, a 0.3 point
improvement over a Single Agent baseline. Ablation experiments demonstrate that
removing Agent2 or Agent3 leads to drops of 0.4 and 0.5 points, respectively,
underscoring the value of modular collaboration. This work showcases how AI
driven pipelines can streamline and scale the preservation of traditional
performing arts, and points toward future enhancements in cross modal
alignment, richer emotional nuance, and support for additional opera genres.

</details>


### [113] [A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings](https://arxiv.org/abs/2504.15610)
*Md Millat,Md Motiur*

Main category: cs.AI

TL;DR: 本文提出了一种经济高效的方法，通过LoRA和4位量化技术优化Mistral-7B-Instruct模型，用于学术咨询，并在低资源环境中实现文化适应。


<details>
  <summary>Details</summary>
Motivation: 研究旨在为低资源环境下的学术咨询提供高效、领域特定的语言模型解决方案。

Method: 采用LoRA和4位量化技术，分两阶段训练模型：第一阶段使用合成数据集，第二阶段使用手动整理的StudyAbroadGPT数据集。

Result: 训练损失减少52.7%，领域推荐准确率达92%，支持95%的Markdown格式，每秒处理100个样本。

Conclusion: 该方法在低资源教育咨询中有效，但通用性有限，未来可扩展多语言和实时数据库集成。

Abstract: The current study describes a cost-effective method for adapting large
language models (LLMs) for academic advising with study-abroad contexts in mind
and for application in low-resource methods for acculturation. With the
Mistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and
a 4-bit quantization method, the model underwent training in two distinct
stages related to this study's purpose to enhance domain specificity while
maintaining computational efficiency. In Phase 1, the model was conditioned
with a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained
with manually curated datasets from the StudyAbroadGPT project to achieve
enhanced, contextualized responses. Technical innovations entailed
memory-efficient quantization, parameter-efficient adaptation, and continuous
training analytics via Weights & Biases. After training, this study
demonstrated a reduction in training loss by 52.7%, 92% accuracy in
domain-specific recommendations, achieved 95% markdown-based formatting
support, and a median run-rate of 100 samples per second on off-the-shelf GPU
equipment. These findings support the effective application of
instruction-tuned LLMs within educational advisers, especially in low-resource
institutional scenarios. Limitations included decreased generalizability and
the application of a synthetically generated dataset, but this framework is
scalable for adding new multilingual-augmented and real-time academic advising
processes. Future directions may include plans for the integration of
retrieval-augmented generation, applying dynamic quantization routines, and
connecting to real-time academic databases to increase adaptability and
accuracy.

</details>


### [114] [Exploring Inevitable Waypoints for Unsolvability Explanation in Hybrid Planning Problems](https://arxiv.org/abs/2504.15668)
*Mir Md Sajid Sarwar,Rajarshi Ray*

Main category: cs.AI

TL;DR: 该论文提出了一种通过识别通用障碍（waypoints）来解释规划问题不可解的方法，并将其建模为最长公共子序列问题。


<details>
  <summary>Details</summary>
Motivation: 解释规划问题的不可解性是可解释AI规划中的重要研究方向，但目前仍缺乏有效方法。

Method: 将规划问题分解为子问题，识别通用障碍（waypoints），并将其建模为最长公共子序列问题，再进行符号可达性分析。

Result: 实验验证了该方法在混合领域不可解规划问题中的有效性。

Conclusion: 通过识别通用障碍和最早不可达点，提供了一种解释规划问题不可解的新方法。

Abstract: Explaining unsolvability of planning problems is of significant research
interest in Explainable AI Planning. AI planning literature has reported
several research efforts on generating explanations of solutions to planning
problems. However, explaining the unsolvability of planning problems remains a
largely open and understudied problem. A widely practiced approach to plan
generation and automated problem solving, in general, is to decompose tasks
into sub-problems that help progressively converge towards the goal. In this
paper, we propose to adopt the same philosophy of sub-problem identification as
a mechanism for analyzing and explaining unsolvability of planning problems in
hybrid systems. In particular, for a given unsolvable planning problem, we
propose to identify common waypoints, which are universal obstacles to plan
existence; in other words, they appear on every plan from the source to the
planning goal. This work envisions such waypoints as sub-problems of the
planning problem and the unreachability of any of these waypoints as an
explanation for the unsolvability of the original planning problem. We propose
a novel method of waypoint identification by casting the problem as an instance
of the longest common subsequence problem, a widely popular problem in computer
science, typically considered as an illustrative example for the dynamic
programming paradigm. Once the waypoints are identified, we perform symbolic
reachability analysis on them to identify the earliest unreachable waypoint and
report it as the explanation of unsolvability. We present experimental results
on unsolvable planning problems in hybrid domains.

</details>


### [115] [Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation](https://arxiv.org/abs/2504.15699)
*Ning Wang,Zihan Yan,Weiyang Li,Chuan Ma,He Chen,Tao Xiang*

Main category: cs.AI

TL;DR: 本文提出了一种针对具身代理的输入审核框架，包括安全基准EAsafetyBench和创新的Pinpoint方案，实验显示其高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注通用大语言模型的安全性，缺乏针对具身代理的专门方法，本文旨在填补这一空白。

Method: 提出输入审核框架，包括定义分类、数据集构建、审核器架构、模型训练和评估，并引入EAsafetyBench和Pinpoint方案。

Result: 实验显示平均检测准确率达94.58%，处理时间仅0.002秒/实例，优于现有技术。

Conclusion: 该框架为具身代理的安全性提供了高效且准确的解决方案。

Abstract: Embodied agents exhibit immense potential across a multitude of domains,
making the assurance of their behavioral safety a fundamental prerequisite for
their widespread deployment. However, existing research predominantly
concentrates on the security of general large language models, lacking
specialized methodologies for establishing safety benchmarks and input
moderation tailored to embodied agents. To bridge this gap, this paper
introduces a novel input moderation framework, meticulously designed to
safeguard embodied agents. This framework encompasses the entire pipeline,
including taxonomy definition, dataset curation, moderator architecture, model
training, and rigorous evaluation. Notably, we introduce EAsafetyBench, a
meticulously crafted safety benchmark engineered to facilitate both the
training and stringent assessment of moderators specifically designed for
embodied agents. Furthermore, we propose Pinpoint, an innovative
prompt-decoupled input moderation scheme that harnesses a masked attention
mechanism to effectively isolate and mitigate the influence of functional
prompts on moderation tasks. Extensive experiments conducted on diverse
benchmark datasets and models validate the feasibility and efficacy of the
proposed approach. The results demonstrate that our methodologies achieve an
impressive average detection accuracy of 94.58%, surpassing the performance of
existing state-of-the-art techniques, alongside an exceptional moderation
processing time of merely 0.002 seconds per instance.

</details>


### [116] [DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language Models](https://arxiv.org/abs/2504.15716)
*Jie Zhu,Qian Chen,Huaixia Dou,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang*

Main category: cs.AI

TL;DR: DianJin-R1是一个增强推理能力的框架，通过监督学习和强化学习提升大语言模型在金融领域的表现，尤其在复杂任务中显著优于非推理模型。


<details>
  <summary>Details</summary>
Motivation: 金融领域任务需要专业知识、精确计算和合规性，现有大语言模型在这些方面表现不足。

Method: 结合高质量数据集（CFLUE、FinQA、CCC）和强化学习方法（GRPO），生成推理步骤和最终答案。

Result: DianJin-R1模型在金融和通用推理基准测试中表现优异，单次推理性能甚至超过多智能体系统。

Conclusion: DianJin-R1通过结构化监督和奖励对齐学习，为金融推理提供了可扩展的解决方案。

Abstract: Effective reasoning remains a core challenge for large language models (LLMs)
in the financial domain, where tasks often require domain-specific knowledge,
precise numerical calculations, and strict adherence to compliance rules. We
propose DianJin-R1, a reasoning-enhanced framework designed to address these
challenges through reasoning-augmented supervision and reinforcement learning.
Central to our approach is DianJin-R1-Data, a high-quality dataset constructed
from CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance
Check, CCC), combining diverse financial reasoning scenarios with verified
annotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from
Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that
generates both reasoning steps and final answers. To further refine reasoning
quality, we apply Group Relative Policy Optimization (GRPO), a reinforcement
learning method that incorporates dual reward signals: one encouraging
structured outputs and another rewarding answer correctness. We evaluate our
models on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and
two general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental
results show that DianJin-R1 models consistently outperform their non-reasoning
counterparts, especially on complex financial tasks. Moreover, on the
real-world CCC dataset, our single-call reasoning models match or even surpass
the performance of multi-agent systems that require significantly more
computational cost. These findings demonstrate the effectiveness of DianJin-R1
in enhancing financial reasoning through structured supervision and
reward-aligned learning, offering a scalable and practical solution for
real-world applications.

</details>


### [117] [Implementing Rational Choice Functions with LLMs and Measuring their Alignment with User Preferences](https://arxiv.org/abs/2504.15719)
*Anna Karnysheva,Christian Drescher,Dietrich Klakow*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）作为决策代理时与用户偏好对齐的问题，提出了一种基于偏好（包括严格偏好和无差异）的通用方法，并通过实证研究验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在智能用户界面（IUIs）中的广泛应用，其决策代理的角色引发了对偏好对齐的关注，而现有研究对此关注不足。

Method: 提出设计原则，利用LLMs实现理性选择功能，并提供测量偏好满足的工具。

Result: 通过汽车领域的实证研究验证了方法的适用性。

Conclusion: 该方法为LLMs在决策代理中的偏好对齐提供了可行方案。

Abstract: As large language models (LLMs) become integral to intelligent user
interfaces (IUIs), their role as decision-making agents raises critical
concerns about alignment. Although extensive research has addressed issues such
as factuality, bias, and toxicity, comparatively little attention has been paid
to measuring alignment to preferences, i.e., the relative desirability of
different alternatives, a concept used in decision making, economics, and
social choice theory. However, a reliable decision-making agent makes choices
that align well with user preferences.
  In this paper, we generalize existing methods that exploit LLMs for ranking
alternative outcomes by addressing alignment with the broader and more flexible
concept of user preferences, which includes both strict preferences and
indifference among alternatives. To this end, we put forward design principles
for using LLMs to implement rational choice functions, and provide the
necessary tools to measure preference satisfaction. We demonstrate the
applicability of our approach through an empirical study in a practical
application of an IUI in the automotive domain.

</details>


### [118] [TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving](https://arxiv.org/abs/2504.15780)
*Daocheng Fu,Zijun Chen,Renqiu Xia,Qi Liu,Yuan Feng,Hongbin Zhou,Renrui Zhang,Shiyang Feng,Peng Gao,Junchi Yan,Botian Shi,Bo Zhang,Yu Qiao*

Main category: cs.AI

TL;DR: 论文提出了一种名为TrustGeoGen的可扩展数据引擎，用于生成几何问题并通过形式验证提供基准，解决了现有合成几何问题基准的噪声和自相矛盾问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在几何问题解决中缺乏方法论和基准，且合成基准常因模型幻觉而包含噪声和自相矛盾信息。

Method: TrustGeoGen通过多模态对齐生成、形式验证、递归状态生成的引导机制和GeoExplore系列算法，生成具有模态完整性的几何问题数据集。

Result: 生成的GeoTrust-200K数据集和GeoTrust-test测试集显示，当前最先进模型在测试集上仅达到49.17%的准确率，但训练后的模型在GeoQA上表现出良好的泛化能力。

Conclusion: TrustGeoGen为几何问题解决提供了可靠的数据生成和验证方法，显著减少了逻辑不一致性，并为未来方法的发展奠定了基础。

Abstract: Mathematical geometric problem solving (GPS) often requires effective
integration of multimodal information and verifiable logical coherence. Despite
the fast development of large language models in general problem solving, it
remains unresolved regarding with both methodology and benchmarks, especially
given the fact that exiting synthetic GPS benchmarks are often not
self-verified and contain noise and self-contradicted information due to the
illusion of LLMs. In this paper, we propose a scalable data engine called
TrustGeoGen for problem generation, with formal verification to provide a
principled benchmark, which we believe lays the foundation for the further
development of methods for GPS. The engine synthesizes geometric data through
four key innovations: 1) multimodal-aligned generation of diagrams, textual
descriptions, and stepwise solutions; 2) formal verification ensuring
rule-compliant reasoning paths; 3) a bootstrapping mechanism enabling
complexity escalation via recursive state generation and 4) our devised
GeoExplore series algorithms simultaneously produce multi-solution variants and
self-reflective backtracking traces. By formal logical verification,
TrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,
along with GeoTrust-test testset. Experiments reveal the state-of-the-art
models achieve only 49.17\% accuracy on GeoTrust-test, demonstrating its
evaluation stringency. Crucially, models trained on GeoTrust achieve OOD
generalization on GeoQA, significantly reducing logical inconsistencies
relative to pseudo-label annotated by OpenAI-o1. Our code is available at
https://github.com/Alpha-Innovator/TrustGeoGen

</details>


### [119] [WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents](https://arxiv.org/abs/2504.15785)
*Siyu Zhou,Tianyi Zhou,Yijun Yang,Guodong Long,Deheng Ye,Jing Jiang,Chengqi Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种无需训练的‘世界对齐’方法，通过提取环境符号知识（如动作规则、知识图谱和场景图）来增强LLM作为世界模型的性能，并设计了RL-free的模型预测控制（MPC）框架WALL-E 2.0，显著提升了学习效率和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs作为世界模型时，其先验知识与特定环境动态之间的差距问题，以提升LLM代理的性能。

Method: 提出‘世界对齐’方法，提取环境符号知识并编码为可执行代码；设计基于MPC的RL-free代理WALL-E 2.0，利用LLM作为高效的前瞻优化器。

Result: 在Mars和ALFWorld任务中，WALL-E 2.0显著优于基线方法，成功率分别提升16.1%-51.6%和98%。

Conclusion: 通过符号知识对齐和MPC框架，WALL-E 2.0高效解决了LLM作为世界模型的性能瓶颈，为开放世界任务提供了新思路。

Abstract: Can we build accurate world models out of large language models (LLMs)? How
can world models benefit LLM agents? The gap between the prior knowledge of
LLMs and the specified environment's dynamics usually bottlenecks LLMs'
performance as world models. To bridge the gap, we propose a training-free
"world alignment" that learns an environment's symbolic knowledge complementary
to LLMs. The symbolic knowledge covers action rules, knowledge graphs, and
scene graphs, which are extracted by LLMs from exploration trajectories and
encoded into executable codes to regulate LLM agents' policies. We further
propose an RL-free, model-based agent "WALL-E 2.0" through the model-predictive
control (MPC) framework. Unlike classical MPC requiring costly optimization on
the fly, we adopt an LLM agent as an efficient look-ahead optimizer of future
steps' actions by interacting with the neurosymbolic world model. While the LLM
agent's strong heuristics make it an efficient planner in MPC, the quality of
its planned actions is also secured by the accurate predictions of the aligned
world model. They together considerably improve learning efficiency in a new
environment. On open-world challenges in Mars (Minecraft like) and ALFWorld
(embodied indoor environments), WALL-E 2.0 significantly outperforms existing
methods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and
by at least 61.7% in score. In ALFWorld, it achieves a new record 98% success
rate after only 4 iterations.

</details>


### [120] [Crisp complexity of fuzzy classifiers](https://arxiv.org/abs/2504.15791)
*Raquel Fernandez-Peralta,Javier Fumanal-Idocin,Javier Andreu-Perez*

Main category: cs.AI

TL;DR: 提出了一种将模糊规则分类器简化为清晰规则分类器的方法，并研究了其复杂性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 模糊规则分类器在非模糊领域应用受限，因其可解释性不足。

Method: 提出了一种方法，将模糊规则分类器转化为清晰规则分类器，并分析了不同清晰描述的可能性。

Result: 实现了算法并分析了清晰分类器的复杂性，帮助理解模糊规则的空间划分。

Conclusion: 该方法有助于模糊和非模糊领域用户理解规则转换，复杂性度量可用于选择分类器。

Abstract: Rule-based systems are a very popular form of explainable AI, particularly in
the fuzzy community, where fuzzy rules are widely used for control and
classification problems. However, fuzzy rule-based classifiers struggle to
reach bigger traction outside of fuzzy venues, because users sometimes do not
know about fuzzy and because fuzzy partitions are not so easy to interpret in
some situations. In this work, we propose a methodology to reduce fuzzy
rule-based classifiers to crisp rule-based classifiers. We study different
possible crisp descriptions and implement an algorithm to obtain them. Also, we
analyze the complexity of the resulting crisp classifiers. We believe that our
results can help both fuzzy and non-fuzzy practitioners understand better the
way in which fuzzy rule bases partition the feature space and how easily one
system can be translated to another and vice versa. Our complexity metric can
also help to choose between different fuzzy classifiers based on what the
equivalent crisp partitions look like.

</details>


### [121] [Generative AI for Research Data Processing: Lessons Learnt From Three Use Cases](https://arxiv.org/abs/2504.15829)
*Modhurita Mitra,Martine G. de Vos,Nicola Cortinovis,Dawa Ometto*

Main category: cs.AI

TL;DR: 论文探讨了生成式AI在复杂数据处理任务中的可行性，通过三个案例展示了其应用，并总结了如何判断生成式AI是否适合特定任务以及如何提高结果的准确性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI（如ChatGPT）的兴起引发了广泛关注，但其输出的准确性和一致性仍存疑虑。本研究旨在探索生成式AI在研究数据处理中的应用潜力。

Method: 研究选择了传统方法难以处理的任务，使用生成式AI模型Claude 3 Opus完成三项复杂数据处理任务：信息提取、自然语言理解和文本分类。

Result: 研究证明了生成式AI在植物物种名称提取、健康技术评估数据点提取和行业代码分类任务中的可行性。

Conclusion: 论文总结了生成式AI适用性的判断标准及提高结果准确性的方法，为相关领域提供了实用指导。

Abstract: There has been enormous interest in generative AI since ChatGPT was launched
in 2022. However, there are concerns about the accuracy and consistency of the
outputs of generative AI. We have carried out an exploratory study on the
application of this new technology in research data processing. We identified
tasks for which rule-based or traditional machine learning approaches were
difficult to apply, and then performed these tasks using generative AI.
  We demonstrate the feasibility of using the generative AI model Claude 3 Opus
in three research projects involving complex data processing tasks:
  1) Information extraction: We extract plant species names from historical
seedlists (catalogues of seeds) published by botanical gardens.
  2) Natural language understanding: We extract certain data points (name of
drug, name of health indication, relative effectiveness, cost-effectiveness,
etc.) from documents published by Health Technology Assessment organisations in
the EU.
  3) Text classification: We assign industry codes to projects on the
crowdfunding website Kickstarter.
  We share the lessons we learnt from these use cases: How to determine if
generative AI is an appropriate tool for a given data processing task, and if
so, how to maximise the accuracy and consistency of the results obtained.

</details>


### [122] [CARE: Compatibility-Aware Incentive Mechanisms for Federated Learning with Budgeted Requesters](https://arxiv.org/abs/2504.15847)
*Xiang Liu,Hau Chan,Minming Li,Xianlong Zeng,Chenchen Fu,Weiwei Wu*

Main category: cs.AI

TL;DR: 本文研究了联邦学习（FL）中预算受限的请求者如何从不兼容的工人中获取训练服务，提出了两种兼容性感知激励机制（CARE-CO和CARE-NO），以提高效率并满足预算约束。


<details>
  <summary>Details</summary>
Motivation: 现有激励机制忽视了工人不兼容性和请求者预算限制对FL效率的影响，导致通信效率低和模型泛化能力差。

Method: 提出CARE-CO（合作预算）和CARE-NO（非合作预算）两种机制，通过真实成本激励工人参与，并优化请求者效用。

Result: 实验表明，新机制在真实数据集上显著优于现有基线，满足个体理性、真实性、预算可行性和近似性能。

Conclusion: 兼容性感知激励机制能有效解决FL中的效率问题，适用于预算受限的多请求者场景。

Abstract: Federated learning (FL) is a promising approach that allows requesters (\eg,
servers) to obtain local training models from workers (e.g., clients). Since
workers are typically unwilling to provide training services/models freely and
voluntarily, many incentive mechanisms in FL are designed to incentivize
participation by offering monetary rewards from requesters. However, existing
studies neglect two crucial aspects of real-world FL scenarios. First, workers
can possess inherent incompatibility characteristics (e.g., communication
channels and data sources), which can lead to degradation of FL efficiency
(e.g., low communication efficiency and poor model generalization). Second, the
requesters are budgeted, which limits the amount of workers they can hire for
their tasks. In this paper, we investigate the scenario in FL where multiple
budgeted requesters seek training services from incompatible workers with
private training costs. We consider two settings: the cooperative budget
setting where requesters cooperate to pool their budgets to improve their
overall utility and the non-cooperative budget setting where each requester
optimizes their utility within their own budgets. To address efficiency
degradation caused by worker incompatibility, we develop novel
compatibility-aware incentive mechanisms, CARE-CO and CARE-NO, for both
settings to elicit true private costs and determine workers to hire for
requesters and their rewards while satisfying requester budget constraints. Our
mechanisms guarantee individual rationality, truthfulness, budget feasibility,
and approximation performance. We conduct extensive experiments using
real-world datasets to show that the proposed mechanisms significantly
outperform existing baselines.

</details>


### [123] [Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations](https://arxiv.org/abs/2504.15903)
*Nikhil Khandalkar,Pavan Yadav,Krishna Shinde,Lokesh B. Ramegowda,Rajarshi Das*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在抽象推理任务中的表现，发现其对噪声敏感，揭示了当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在抽象推理任务（如ARC基准测试）中的表现，以评估其泛化能力和对噪声的鲁棒性。

Method: 系统评估不同模型（如GPT-4o、DeepSeek R1和LLaMA 3.2）在不同噪声水平和温度设置下的表现。

Result: 噪声显著降低所有模型的性能，表明当前LLMs在抽象推理中仍存在脆弱性。

Conclusion: 研究强调了开发更鲁棒、适应性强的AI系统的必要性，以应对现实世界中的不确定性和噪声。

Abstract: Recent advancements in Large Language Models (LLMs) have generated growing
interest in their structured reasoning capabilities, particularly in tasks
involving abstraction and pattern recognition. The Abstraction and Reasoning
Corpus (ARC) benchmark plays a crucial role in evaluating these capabilities by
testing how well AI models generalize to novel problems. While GPT-4o
demonstrates strong performance by solving all ARC tasks under zero-noise
conditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any,
suggesting limitations in their ability to reason beyond simple pattern
matching. To explore this gap, we systematically evaluate these models across
different noise levels and temperature settings. Our results reveal that the
introduction of noise consistently impairs model performance, regardless of
architecture. This decline highlights a shared vulnerability: current LLMs,
despite showing signs of abstract reasoning, remain highly sensitive to input
perturbations. Such fragility raises concerns about their real-world
applicability, where noise and uncertainty are common. By comparing how
different model architectures respond to these challenges, we offer insights
into the structural weaknesses of modern LLMs in reasoning tasks. This work
underscores the need for developing more robust and adaptable AI systems
capable of handling the ambiguity and variability inherent in real-world
scenarios. Our findings aim to guide future research toward enhancing model
generalization, robustness, and alignment with human-like cognitive
flexibility.

</details>


### [124] [Approximate matrices of systems of max-min fuzzy relational equations](https://arxiv.org/abs/2504.16042)
*Ismaïl Baaj*

Main category: cs.AI

TL;DR: 论文提出了一种通过最小化修改矩阵来解决max-min模糊关系方程系统不一致性的方法，确保近似原始系统，并分析了不同范数下的距离。


<details>
  <summary>Details</summary>
Motivation: 解决max-min模糊关系方程系统的不一致性问题，通过最小修改矩阵实现一致性，同时保持对原始系统的近似。

Method: 通过最小化修改矩阵的条目，生成一致性系统，并研究不同范数（L1、L2、L∞）下矩阵与一致性系统集的距离。

Result: 方法能直接计算L∞范数下最小距离的一致性系统矩阵，并给出了L∞距离的显式解析公式。

Conclusion: 方法有效且计算高效，尤其适用于L∞范数，结果可推广到min-max模糊关系方程系统，具有潜在应用价值。

Abstract: In this article, we address the inconsistency of a system of max-min fuzzy
relational equations by minimally modifying the matrix governing the system in
order to achieve consistency. Our method yields consistent systems that
approximate the original inconsistent system in the following sense: the
right-hand side vector of each consistent system is that of the inconsistent
system, and the coefficients of the matrix governing each consistent system are
obtained by modifying, exactly and minimally, the entries of the original
matrix that must be corrected to achieve consistency, while leaving all other
entries unchanged.
  To obtain a consistent system that closely approximates the considered
inconsistent system, we study the distance (in terms of a norm among $L_1$,
$L_2$ or $L_\infty$) between the matrix of the inconsistent system and the set
formed by the matrices of consistent systems that use the same right-hand side
vector as the inconsistent system. We show that our method allows us to
directly compute matrices of consistent systems that use the same right-hand
side vector as the inconsistent system whose distance in terms of $L_\infty$
norm to the matrix of the inconsistent system is minimal (the computational
costs are higher when using $L_1$ norm or $L_2$ norm). We also give an explicit
analytical formula for computing this minimal $L_\infty$ distance. Finally, we
translate our results for systems of min-max fuzzy relational equations and
present some potential applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [125] [Collaborative Learning of On-Device Small Model and Cloud-Based Large Model: Advances and Future Directions](https://arxiv.org/abs/2504.15300)
*Chaoyue Niu,Yucheng Ding,Junhui Lu,Zhengxiang Huang,Hang Zeng,Yutong Dai,Xuezhen Tu,Chengfei Lv,Fan Wu,Guihai Chen*

Main category: cs.LG

TL;DR: 本文综述了设备端小模型与云端大模型协作学习的新范式，旨在解决延迟、成本、个性化和隐私问题，并全面回顾了硬件、系统、算法和应用层的研究进展。


<details>
  <summary>Details</summary>
Motivation: 传统云端大模型学习框架在延迟、成本、个性化和隐私方面存在限制，需要探索更高效、个性化的协作学习方案。

Method: 从硬件、系统、算法和应用层进行综合分析，将协作算法分为数据、特征和参数三类框架，并总结了公开数据集和评估指标。

Result: 协作学习在推荐系统、移动直播和个人智能助手等领域已有实际部署，展示了其低延迟、高效率和隐私保护的优势。

Conclusion: 协作学习是一个快速发展的领域，未来研究需进一步探索其潜力并解决开放性问题。

Abstract: The conventional cloud-based large model learning framework is increasingly
constrained by latency, cost, personalization, and privacy concerns. In this
survey, we explore an emerging paradigm: collaborative learning between
on-device small model and cloud-based large model, which promises low-latency,
cost-efficient, and personalized intelligent services while preserving user
privacy. We provide a comprehensive review across hardware, system, algorithm,
and application layers. At each layer, we summarize key problems and recent
advances from both academia and industry. In particular, we categorize
collaboration algorithms into data-based, feature-based, and parameter-based
frameworks. We also review publicly available datasets and evaluation metrics
with user-level or device-level consideration tailored to collaborative
learning settings. We further highlight real-world deployments, ranging from
recommender systems and mobile livestreaming to personal intelligent
assistants. We finally point out open research directions to guide future
development in this rapidly evolving field.

</details>


### [126] [Power Transformer Health Index and Life Span Assessment: A Comprehensive Review of Conventional and Machine Learning based Approaches](https://arxiv.org/abs/2504.15310)
*Syeda Tahreem Zahra,Syed Kashif Imdad,Sohail Khan,Sohail Khalid,Nauman Anwar Baig*

Main category: cs.LG

TL;DR: 本文综述了电力变压器健康评估和寿命预测的现有技术，重点分析了传统和前沿方法的优缺点，并探讨了多种人工智能算法在变压器故障诊断中的应用。


<details>
  <summary>Details</summary>
Motivation: 电力变压器在电力系统中至关重要，其健康评估和寿命预测对高效运行和维护规划至关重要。

Method: 通过文献综述，分析了传统和前沿技术，并详细探讨了多种AI算法（如ANN、CNN、SVM等）在变压器故障诊断中的应用。

Result: 研究发现，结合多种AI方法和时间序列分析可提高诊断精度和故障早期检测能力。

Conclusion: 本文为变压器故障诊断领域的未来研究提供了基础，并推动了这一关键领域的发展。

Abstract: Power transformers play a critical role within the electrical power system,
making their health assessment and the prediction of their remaining lifespan
paramount for the purpose of ensuring efficient operation and facilitating
effective maintenance planning. This paper undertakes a comprehensive
examination of existent literature, with a primary focus on both conventional
and cutting-edge techniques employed within this domain. The merits and
demerits of recent methodologies and techniques are subjected to meticulous
scrutiny and explication. Furthermore, this paper expounds upon intelligent
fault diagnosis methodologies and delves into the most widely utilized
intelligent algorithms for the assessment of transformer conditions. Diverse
Artificial Intelligence (AI) approaches, including Artificial Neural Networks
(ANN) and Convolutional Neural Network (CNN), Support Vector Machine (SVM),
Random Forest (RF), Genetic Algorithm (GA), and Particle Swarm Optimization
(PSO), are elucidated offering pragmatic solutions for enhancing the
performance of transformer fault diagnosis. The amalgamation of multiple AI
methodologies and the exploration of timeseries analysis further contribute to
the augmentation of diagnostic precision and the early detection of faults in
transformers. By furnishing a comprehensive panorama of AI applications in the
field of transformer fault diagnosis, this study lays the groundwork for future
research endeavors and the progression of this critical area of study.

</details>


### [127] [M-TabNet: A Multi-Encoder Transformer Model for Predicting Neonatal Birth Weight from Multimodal Data](https://arxiv.org/abs/2504.15312)
*Muhammad Mursil,Hatem A. Rashwan,Luis Santos-Calderon,Pere Cavalle-Busquets,Michelle M. Murphy,Domenec Puig*

Main category: cs.LG

TL;DR: 该研究提出了一种基于注意力机制的Transformer模型，用于早期（妊娠12周前）预测新生儿体重，整合了生理、生活方式、营养和遗传等多维数据，显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 低出生体重（LBW）与新生儿死亡率和发病率密切相关，现有预测方法（如超声检查）存在局限性和操作依赖性。现有模型常忽略营养和遗传因素，本研究旨在填补这一空白。

Method: 采用多编码器架构的注意力Transformer模型，整合多维母体数据（生理、生活方式、营养、遗传），并优化了TabNet等现有模型的不足。

Result: 模型在内部数据集上MAE为122克，R²为0.94；独立验证中MAE为105克，R²为0.95。分类性能优异（敏感性97.55%，特异性94.48%）。

Conclusion: 该模型为临床提供了高精度、可解释的早期出生体重预测工具，有助于风险分层和优化新生儿健康。

Abstract: Birth weight (BW) is a key indicator of neonatal health, with low birth
weight (LBW) linked to increased mortality and morbidity. Early prediction of
BW enables timely interventions; however, current methods like ultrasonography
have limitations, including reduced accuracy before 20 weeks and operator
dependent variability. Existing models often neglect nutritional and genetic
influences, focusing mainly on physiological and lifestyle factors. This study
presents an attention-based transformer model with a multi-encoder architecture
for early (less than 12 weeks of gestation) BW prediction. Our model
effectively integrates diverse maternal data such as physiological, lifestyle,
nutritional, and genetic, addressing limitations seen in prior attention-based
models such as TabNet. The model achieves a Mean Absolute Error (MAE) of 122
grams and an R-squared value of 0.94, demonstrating high predictive accuracy
and interoperability with our in-house private dataset. Independent validation
confirms generalizability (MAE: 105 grams, R-squared: 0.95) with the IEEE
children dataset. To enhance clinical utility, predicted BW is classified into
low and normal categories, achieving a sensitivity of 97.55% and a specificity
of 94.48%, facilitating early risk stratification. Model interpretability is
reinforced through feature importance and SHAP analyses, highlighting
significant influences of maternal age, tobacco exposure, and vitamin B12
status, with genetic factors playing a secondary role. Our results emphasize
the potential of advanced deep-learning models to improve early BW prediction,
offering clinicians a robust, interpretable, and personalized tool for
identifying pregnancies at risk and optimizing neonatal outcomes.

</details>


### [128] [Diffusion-Driven Inertial Generated Data for Smartphone Location Classification](https://arxiv.org/abs/2504.15315)
*Noa Cohen,Rotem Dror,Itzik Klein*

Main category: cs.LG

TL;DR: 论文提出了一种基于扩散模型的生成方法，用于生成智能手机位置识别的特定力数据，以减少实际数据收集的负担。


<details>
  <summary>Details</summary>
Motivation: 惯性测量数据收集耗时且资源密集，限制了机器学习模型的开发。扩散模型在生成复杂数据方面表现出色，为解决这一问题提供了新思路。

Method: 采用扩散模型生成特定力数据，并通过多指标对比合成数据与真实记录数据。

Result: 扩散模型成功捕捉了不同智能手机放置条件下特定力信号的独特特征。

Conclusion: 通过生成多样且真实的合成数据，可以减轻数据收集负担，同时为机器学习模型提供高质量训练数据。

Abstract: Despite the crucial role of inertial measurements in motion tracking and
navigation systems, the time-consuming and resource-intensive nature of
collecting extensive inertial data has hindered the development of robust
machine learning models in this field. In recent years, diffusion models have
emerged as a revolutionary class of generative models, reshaping the landscape
of artificial data generation. These models surpass generative adversarial
networks and other state-of-the-art approaches to complex tasks. In this work,
we propose diffusion-driven specific force-generated data for smartphone
location recognition. We provide a comprehensive evaluation methodology by
comparing synthetic and real recorded specific force data across multiple
metrics. Our results demonstrate that our diffusion-based generative model
successfully captures the distinctive characteristics of specific force signals
across different smartphone placement conditions. Thus, by creating diverse,
realistic synthetic data, we can reduce the burden of extensive data collection
while providing high-quality training data for machine learning models.

</details>


### [129] [How to systematically develop an effective AI-based bias correction model?](https://arxiv.org/abs/2504.15322)
*Xiao Zhou,Yuze Sun,Jie Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: ReSA-ConvLSTM框架通过动态气候归一化、时序因果约束的ConvLSTM和残差自注意力机制，显著减少数值天气预报中的系统性偏差，提升1-7天预报精度。


<details>
  <summary>Details</summary>
Motivation: 解决数值天气预报中的系统性偏差问题，提升预报准确性。

Method: 结合动态气候归一化、时序因果约束的ConvLSTM和残差自注意力机制，建立物理感知的非线性映射模型。

Result: 在41年全球数据上，T2m、U10/V10和SLP的偏差减少20%，模型轻量化且泛化能力强。

Conclusion: 模型创新显著提升校正性能，变量特性融入有助于增强预报能力。

Abstract: This study introduces ReSA-ConvLSTM, an artificial intelligence (AI)
framework for systematic bias correction in numerical weather prediction (NWP).
We propose three innovations by integrating dynamic climatological
normalization, ConvLSTM with temporal causality constraints, and residual
self-attention mechanisms. The model establishes a physics-aware nonlinear
mapping between ECMWF forecasts and ERA5 reanalysis data. Using 41 years
(1981-2021) of global atmospheric data, the framework reduces systematic biases
in 2-m air temperature (T2m), 10-m winds (U10/V10), and sea-level pressure
(SLP), achieving up to 20% RMSE reduction over 1-7 day forecasts compared to
operational ECMWF outputs. The lightweight architecture (10.6M parameters)
enables efficient generalization to multiple variables and downstream
applications, reducing retraining time by 85% for cross-variable correction
while improving ocean model skill through bias-corrected boundary conditions.
The ablation experiments demonstrate that our innovations significantly improve
the model's correction performance, suggesting that incorporating variable
characteristics into the model helps enhance forecasting skills.

</details>


### [130] [HyperFlow: Gradient-Free Emulation of Few-Shot Fine-Tuning](https://arxiv.org/abs/2504.15323)
*Donggyun Kim,Chanwoo Kim,Seunghoon Hong*

Main category: cs.LG

TL;DR: 提出了一种无需计算梯度的测试时微调方法，通过模拟梯度下降实现高效适应，显著降低了计算和内存成本。


<details>
  <summary>Details</summary>
Motivation: 解决传统测试时微调因多次反向传播导致的计算和资源消耗过高的问题。

Method: 将梯度下降建模为ODE的欧拉离散化，训练辅助网络预测任务条件漂移，仅需少量前向传播即可完成适应。

Result: 在跨域少样本分类任务中，性能显著优于基线，计算时间和内存成本仅为标准微调的0.02%和6%。

Conclusion: 该方法在直接迁移和完全微调之间提供了实用的折中方案。

Abstract: While test-time fine-tuning is beneficial in few-shot learning, the need for
multiple backpropagation steps can be prohibitively expensive in real-time or
low-resource scenarios. To address this limitation, we propose an approach that
emulates gradient descent without computing gradients, enabling efficient
test-time adaptation. Specifically, we formulate gradient descent as an Euler
discretization of an ordinary differential equation (ODE) and train an
auxiliary network to predict the task-conditional drift using only the few-shot
support set. The adaptation then reduces to a simple numerical integration
(e.g., via the Euler method), which requires only a few forward passes of the
auxiliary network -- no gradients or forward passes of the target model are
needed. In experiments on cross-domain few-shot classification using the
Meta-Dataset and CDFSL benchmarks, our method significantly improves
out-of-domain performance over the non-fine-tuned baseline while incurring only
6\% of the memory cost and 0.02\% of the computation time of standard
fine-tuning, thus establishing a practical middle ground between direct
transfer and fully fine-tuned approaches.

</details>


### [131] [Significativity Indices for Agreement Values](https://arxiv.org/abs/2504.15325)
*Alberto Casagrande,Francesco Fabris,Rossano Girometti,Roberto Pagliarini*

Main category: cs.LG

TL;DR: 该论文提出了一种评估分类器间一致性显著性的通用方法，并引入了两种显著性指数，同时解决了计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有的一致性度量（如Cohen's kappa）缺乏有效的显著性评估标准，且现有质量尺度过于简单和主观。

Method: 提出通用方法评估一致性值的显著性，引入两种显著性指数（针对有限数据集和分类概率分布），并开发高效算法。

Result: 论文提出了两种显著性指数，并提供了计算这些指数的高效算法。

Conclusion: 该方法为一致性度量的显著性评估提供了更科学和实用的工具。

Abstract: Agreement measures, such as Cohen's kappa or intraclass correlation, gauge
the matching between two or more classifiers. They are used in a wide range of
contexts from medicine, where they evaluate the effectiveness of medical
treatments and clinical trials, to artificial intelligence, where they can
quantify the approximation due to the reduction of a classifier. The
consistency of different classifiers to a golden standard can be compared
simply by using the order induced by their agreement measure with respect to
the golden standard itself. Nevertheless, labelling an approach as good or bad
exclusively by using the value of an agreement measure requires a scale or a
significativity index. Some quality scales have been proposed in the literature
for Cohen's kappa, but they are mainly naive, and their boundaries are
arbitrary. This work proposes a general approach to evaluate the
significativity of any agreement value between two classifiers and introduces
two significativity indices: one dealing with finite data sets, the other one
handling classification probability distributions. Moreover, this manuscript
considers the computational issues of evaluating such indices and identifies
some efficient algorithms to evaluate them.

</details>


### [132] [Bayesian Federated Learning for Continual Training](https://arxiv.org/abs/2504.15328)
*Usevalad Milasheuski,Luca Barbieri,Sanaz Kianoush,Monica Nicoli,Stefano Savazzi*

Main category: cs.LG

TL;DR: 论文提出了一种持续贝叶斯联邦学习框架，用于动态环境中数据分布变化的挑战，并通过雷达数据验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前贝叶斯联邦学习方法忽视了动态环境中数据分布变化的持续学习挑战，因此需要一种能够适应数据演变的框架。

Method: 使用随机梯度朗之万动力学（SGLD）方法，通过利用过去的后验分布构建新任务的先验分布，实现模型的持续更新。

Result: 实验结果表明，该方法在准确性、预期校准误差（ECE）和收敛速度方面优于基线方法，能够有效保留知识并适应数据变化。

Conclusion: 持续贝叶斯更新在动态环境中具有显著优势，能够提升模型的适应性和可靠性。

Abstract: Bayesian Federated Learning (BFL) enables uncertainty quantification and
robust adaptation in distributed learning. In contrast to the frequentist
approach, it estimates the posterior distribution of a global model, offering
insights into model reliability. However, current BFL methods neglect continual
learning challenges in dynamic environments where data distributions shift over
time. We propose a continual BFL framework applied to human sensing with radar
data collected over several days. Using Stochastic Gradient Langevin Dynamics
(SGLD), our approach sequentially updates the model, leveraging past posteriors
to construct the prior for the new tasks. We assess the accuracy, the expected
calibration error (ECE) and the convergence speed of our approach against
several baselines. Results highlight the effectiveness of continual Bayesian
updates in preserving knowledge and adapting to evolving data.

</details>


### [133] [FedFetch: Faster Federated Learning with Adaptive Downstream Prefetching](https://arxiv.org/abs/2504.15366)
*Qifan Yan,Andrew Liu,Shiqi He,Mathias Lécuyer,Ivan Beschastnikh*

Main category: cs.LG

TL;DR: FedFetch是一种策略，通过预取模型状态减少联邦学习中下载时间开销，提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中大量异构客户端导致通信瓶颈，现有技术（如客户端采样和更新压缩）结合效果不佳。

Method: 提出FedFetch策略，通过预取模型状态减少下载时间开销。

Result: 实验表明，FedFetch将端到端训练时间减少1.26倍，下载时间减少4.49倍。

Conclusion: FedFetch有效解决了联邦学习中通信瓶颈问题，显著提升效率。

Abstract: Federated learning (FL) is a machine learning paradigm that facilitates
massively distributed model training with end-user data on edge devices
directed by a central server. However, the large number of heterogeneous
clients in FL deployments leads to a communication bottleneck between the
server and the clients. This bottleneck is made worse by straggling clients,
any one of which will further slow down training. To tackle these challenges,
researchers have proposed techniques like client sampling and update
compression. These techniques work well in isolation but combine poorly in the
downstream, server-to-client direction. This is because unselected clients have
outdated local model states and need to synchronize these states with the
server first.
  We introduce FedFetch, a strategy to mitigate the download time overhead
caused by combining client sampling and compression techniques. FedFetch
achieves this with an efficient prefetch schedule for clients to prefetch model
states multiple rounds before a stated training round. We empirically show that
adding FedFetch to communication efficient FL techniques reduces end-to-end
training time by 1.26$\times$ and download time by 4.49$\times$ across
compression techniques with heterogeneous client settings. Our implementation
is available at https://github.com/DistributedML/FedFetch

</details>


### [134] [Solving New Tasks by Adapting Internet Video Knowledge](https://arxiv.org/abs/2504.15369)
*Calvin Luo,Zilai Zeng,Yilun Du,Chen Sun*

Main category: cs.LG

TL;DR: 论文探讨了如何通过适应技术将大规模预训练视频模型与特定领域信息结合，以支持机器人任务中的文本条件泛化，并提出了一种新的适应策略。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型在机器人学中具有潜力，但预训练模型可能忽略特定环境细节，而领域内训练数据又不足以支持泛化。因此，需要一种方法结合两者的优势。

Method: 研究了多种适应技术，提出了一种名为“逆概率适应”的新策略，用于整合预训练模型与领域内数据。

Result: 实验表明，通过少量领域数据适应视频模型能成功泛化到新行为，且新策略在任务和设置中表现稳健，对数据质量不敏感。

Conclusion: 逆概率适应策略在机器人任务中表现出色，即使领域内数据不理想，仍能成功解决新任务。

Abstract: Video generative models demonstrate great promise in robotics by serving as
visual planners or as policy supervisors. When pretrained on internet-scale
data, such video models intimately understand alignment with natural language,
and can thus facilitate generalization to novel downstream behavior through
text-conditioning. However, they may not be sensitive to the specificities of
the particular environment the agent inhabits. On the other hand, training
video models on in-domain examples of robotic behavior naturally encodes
environment-specific intricacies, but the scale of available demonstrations may
not be sufficient to support generalization to unseen tasks via natural
language specification. In this work, we investigate different adaptation
techniques that integrate in-domain information with large-scale pretrained
video models, and explore the extent to which they enable novel
text-conditioned generalization for robotic tasks, while also considering their
independent data and resource considerations. We successfully demonstrate
across robotic environments that adapting powerful video models with small
scales of example data can successfully facilitate generalization to novel
behaviors. In particular, we present a novel adaptation strategy, termed
Inverse Probabilistic Adaptation, that not only consistently achieves strong
generalization performance across robotic tasks and settings, but also exhibits
robustness to the quality of adaptation data, successfully solving novel tasks
even when only suboptimal in-domain demonstrations are available.

</details>


### [135] [Improving Learning to Optimize Using Parameter Symmetries](https://arxiv.org/abs/2504.15399)
*Guy Zamir,Aryan Dokania,Bo Zhao,Rose Yu*

Main category: cs.LG

TL;DR: 论文提出了一种利用参数空间对称性提升优化效率的学习优化算法（L2O），理论分析表明其局部类似牛顿法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过参数空间对称性提升优化效率，推动元优化技术的发展。

Method: 结合学习对称变换和局部更新，理论分析其局部类似牛顿法，并通过实验验证算法性能。

Result: 算法在训练中能学习到正确的对称变换，实验表明动量等增强方法可进一步提升性能。

Conclusion: 利用神经网络参数空间对称性在元优化中具有潜力。

Abstract: We analyze a learning-to-optimize (L2O) algorithm that exploits parameter
space symmetry to enhance optimization efficiency. Prior work has shown that
jointly learning symmetry transformations and local updates improves
meta-optimizer performance. Supporting this, our theoretical analysis
demonstrates that even without identifying the optimal group element, the
method locally resembles Newton's method. We further provide an example where
the algorithm provably learns the correct symmetry transformation during
training. To empirically evaluate L2O with teleportation, we introduce a
benchmark, analyze its success and failure cases, and show that enhancements
like momentum further improve performance. Our results highlight the potential
of leveraging neural network parameter space symmetry to advance
meta-optimization.

</details>


### [136] [Combating Toxic Language: A Review of LLM-Based Strategies for Software Engineering](https://arxiv.org/abs/2504.15439)
*Hao Zhuo,Yicheng Yang,Kewen Peng*

Main category: cs.LG

TL;DR: 本文综述了大型语言模型（LLMs）在软件工程（SE）中的毒性语言检测与缓解研究，总结了现有方法并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: LLMs在SE中的广泛应用引发了毒性语言传播的担忧，需研究其检测与缓解方法。

Method: 回顾了毒性检测与缓解的研究，包括数据标注、预处理、检测方法和基于LLM的缓解策略，并进行了消融实验。

Result: 研究表明基于LLM的重写能有效减少毒性，但仍存在挑战。

Conclusion: 未来需进一步研究以确保LLMs在SE及其他领域的负责任使用。

Abstract: Large Language Models (LLMs) have become integral to software engineering
(SE), where they are increasingly used in development workflows. However, their
widespread use raises concerns about the presence and propagation of toxic
language--harmful or offensive content that can foster exclusionary
environments. This paper provides a comprehensive review of recent research on
toxicity detection and mitigation, focusing on both SE-specific and
general-purpose datasets. We examine annotation and preprocessing techniques,
assess detection methodologies, and evaluate mitigation strategies,
particularly those leveraging LLMs. Additionally, we conduct an ablation study
demonstrating the effectiveness of LLM-based rewriting for reducing toxicity.
By synthesizing existing work and identifying open challenges, this review
highlights key areas for future research to ensure the responsible deployment
of LLMs in SE and beyond.

</details>


### [137] [Compton Form Factor Extraction using Quantum Deep Neural Networks](https://arxiv.org/abs/2504.15458)
*Brandon Le,Dustin Keller*

Main category: cs.LG

TL;DR: 论文通过伪数据提取康普顿形状因子，比较了经典深度神经网络（CDNN）和量子深度神经网络（QDNN），发现QDNN性能更优。


<details>
  <summary>Details</summary>
Motivation: 研究目的是利用实验数据提取康普顿形状因子，并探索量子算法在数据分析中的潜力。

Method: 采用Belitsky、Kirchner和Muller的twist-two形式主义，结合减少模型依赖性的拟合方法，使用CDNN和QDNN进行提取。

Result: QDNN在预测准确性和精度上优于CDNN，尤其在模型复杂度有限时表现更佳。

Conclusion: QDNN展示了在量子算法优化后未来研究中的潜力。

Abstract: Extraction tests of Compton Form Factors are performed using pseudodata based
on experimental data from Deeply Virtual Compton Scattering experiments
conducted at Jefferson Lab. The standard Belitsky, Kirchner, and Muller
formalism at twist-two is employed, along with a fitting procedure designed to
reduce model dependency similar to traditional local fits. The extraction of
the Compton Form Factors is performed using both Classical Deep Neural Networks
(CDNNs) and Quantum Deep Neural Networks (QDNNs). Comparative studies reveal
that QDNNs outperform CDNNs for this application, demonstrating improved
predictive accuracy and precision even for limited model complexity. The
results demonstrate the potential of QDNNs for future studies in which quantum
algorithms can be fully optimized.

</details>


### [138] [In-context Ranking Preference Optimization](https://arxiv.org/abs/2504.15477)
*Junda Wu,Rohan Surana,Zhouhang Xie,Yiran Shen,Yu Xia,Tong Yu,Ryan A. Rossi,Prithviraj Ammanabrolu,Julian McAuley*

Main category: cs.LG

TL;DR: 提出了In-context Ranking Preference Optimization (IRPO)框架，通过直接优化LLMs基于推理过程中构建的排名列表，解决了有限和稀疏的成对反馈问题。


<details>
  <summary>Details</summary>
Motivation: 用户反馈通常涉及识别上下文中的相关项而非详细成对比较，且复杂任务（如对话代理和摘要系统）需要高质量输出的排名，因此需要支持更自然和灵活的反馈形式。

Method: IRPO扩展了DPO目标，结合了项的相关性和其在列表中的位置，通过基于位置聚合的成对偏好差异的可微分目标，优化离散排名指标。

Result: IRPO在排名性能上优于标准DPO方法，理论分析表明其能自动强调模型与参考排名间差异较大的项，并提供低方差的无偏估计。

Conclusion: IRPO有效解决了有限反馈问题，提升了LLMs与上下文排名偏好的对齐能力。

Abstract: Recent developments in Direct Preference Optimization (DPO) allow large
language models (LLMs) to function as implicit ranking models by maximizing the
margin between preferred and non-preferred responses. In practice, user
feedback on such lists typically involves identifying a few relevant items in
context rather than providing detailed pairwise comparisons for every possible
item pair. Moreover, many complex information retrieval tasks, such as
conversational agents and summarization systems, critically depend on ranking
the highest-quality outputs at the top, emphasizing the need to support natural
and flexible forms of user feedback. To address the challenge of limited and
sparse pairwise feedback in the in-context setting, we propose an In-context
Ranking Preference Optimization (IRPO) framework that directly optimizes LLMs
based on ranking lists constructed during inference. To further capture
flexible forms of feedback, IRPO extends the DPO objective by incorporating
both the relevance of items and their positions in the list. Modeling these
aspects jointly is non-trivial, as ranking metrics are inherently discrete and
non-differentiable, making direct optimization difficult. To overcome this,
IRPO introduces a differentiable objective based on positional aggregation of
pairwise item preferences, enabling effective gradient-based optimization of
discrete ranking metrics. We further provide theoretical insights showing that
IRPO (i) automatically emphasizes items with greater disagreement between the
model and the reference ranking, and (ii) links its gradient to an importance
sampling estimator, yielding an unbiased estimator with reduced variance.
Empirical results show IRPO outperforms standard DPO approaches in ranking
performance, highlighting its effectiveness in aligning LLMs with direct
in-context ranking preferences.

</details>


### [139] [Unifying Image Counterfactuals and Feature Attributions with Latent-Space Adversarial Attacks](https://arxiv.org/abs/2504.15479)
*Jeremy Goldwasser,Giles Hooker*

Main category: cs.LG

TL;DR: 提出了一种新的反事实图像生成框架，通过低维流形上的对抗攻击生成解释性图像，并结合特征归因量化变化。


<details>
  <summary>Details</summary>
Motivation: 解决计算机视觉模型中反事实解释生成困难的问题，避免传统方法产生的对抗性样本。

Method: 采用低维流形上的对抗攻击生成反事实图像，结合辅助数据集的特征归因量化变化。

Result: 在MNIST和CelebA数据集上验证了方法的有效性，生成全局反事实解释。

Conclusion: 该方法灵活适应生成模型的最新进展，计算效率高，适用于解释模型预测。

Abstract: Counterfactuals are a popular framework for interpreting machine learning
predictions. These what if explanations are notoriously challenging to create
for computer vision models: standard gradient-based methods are prone to
produce adversarial examples, in which imperceptible modifications to image
pixels provoke large changes in predictions. We introduce a new,
easy-to-implement framework for counterfactual images that can flexibly adapt
to contemporary advances in generative modeling. Our method, Counterfactual
Attacks, resembles an adversarial attack on the representation of the image
along a low-dimensional manifold. In addition, given an auxiliary dataset of
image descriptors, we show how to accompany counterfactuals with feature
attribution that quantify the changes between the original and counterfactual
images. These importance scores can be aggregated into global counterfactual
explanations that highlight the overall features driving model predictions.
While this unification is possible for any counterfactual method, it has
particular computational efficiency for ours. We demonstrate the efficacy of
our approach with the MNIST and CelebA datasets.

</details>


### [140] [Fourier analysis of the physics of transfer learning for data-driven subgrid-scale models of ocean turbulence](https://arxiv.org/abs/2504.15487)
*Moein Darman,Pedram Hassanzadeh,Laure Zanna,Ashesh Chattopadhyay*

Main category: cs.LG

TL;DR: 该论文研究了迁移学习在神经网络中的应用，通过分析卷积神经网络在海洋准地转系统中的表现，探讨了其泛化能力及迁移学习的优化效果。


<details>
  <summary>Details</summary>
Motivation: 研究迁移学习如何提升神经网络在天气和气候预测等任务中的性能，特别是在处理分布外数据时的表现。

Method: 使用9层卷积神经网络预测两层海洋准地转系统中的子网格强迫，并通过傅里叶分析研究其核函数和激活谱。

Result: 发现神经网络在不使用迁移学习时会低估输出谱，而通过重新训练一层可以纠正这一问题，显著提升泛化能力。

Conclusion: 迁移学习能有效克服神经网络在分布外数据上的局限性，这一发现对动力系统的数据驱动参数化具有广泛意义。

Abstract: Transfer learning (TL) is a powerful tool for enhancing the performance of
neural networks (NNs) in applications such as weather and climate prediction
and turbulence modeling. TL enables models to generalize to out-of-distribution
data with minimal training data from the new system. In this study, we employ a
9-layer convolutional NN to predict the subgrid forcing in a two-layer ocean
quasi-geostrophic system and examine which metrics best describe its
performance and generalizability to unseen dynamical regimes. Fourier analysis
of the NN kernels reveals that they learn low-pass, Gabor, and high-pass
filters, regardless of whether the training data are isotropic or anisotropic.
By analyzing the activation spectra, we identify why NNs fail to generalize
without TL and how TL can overcome these limitations: the learned weights and
biases from one dataset underestimate the out-of-distribution sample spectra as
they pass through the network, leading to an underestimation of output spectra.
By re-training only one layer with data from the target system, this
underestimation is corrected, enabling the NN to produce predictions that match
the target spectra. These findings are broadly applicable to data-driven
parameterization of dynamical systems.

</details>


### [141] [Application of Deep Generative Models for Anomaly Detection in Complex Financial Transactions](https://arxiv.org/abs/2504.15491)
*Tengda Tang,Jianhua Yao,Yixian Wang,Qiuwu Sha,Hanrui Feng,Zhen Xu*

Main category: cs.LG

TL;DR: 提出了一种基于深度生成模型的算法，用于检测大规模支付流中的可疑行为，结合GAN和VAE，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 金融交易中的异常行为（如欺诈和洗钱）检测需求日益增长，传统方法在复杂数据中表现不足。

Method: 结合GAN生成模拟数据以近似正常支付流，利用判别器识别异常；引入VAE建模潜在分布，提升生成数据真实性。

Result: 实验表明，该方法在多种评估指标上显著优于传统算法，尤其在稀疏数据中检测罕见欺诈行为表现突出。

Conclusion: 生成模型在复杂金融数据中具有优势，为可疑行为检测提供了高效解决方案。

Abstract: This study proposes an algorithm for detecting suspicious behaviors in large
payment flows based on deep generative models. By combining Generative
Adversarial Networks (GAN) and Variational Autoencoders (VAE), the algorithm is
designed to detect abnormal behaviors in financial transactions. First, the GAN
is used to generate simulated data that approximates normal payment flows. The
discriminator identifies anomalous patterns in transactions, enabling the
detection of potential fraud and money laundering behaviors. Second, a VAE is
introduced to model the latent distribution of payment flows, ensuring that the
generated data more closely resembles real transaction features, thus improving
the model's detection accuracy. The method optimizes the generative
capabilities of both GAN and VAE, ensuring that the model can effectively
capture suspicious behaviors even in sparse data conditions. Experimental
results show that the proposed method significantly outperforms traditional
machine learning algorithms and other deep learning models across various
evaluation metrics, especially in detecting rare fraudulent behaviors.
Furthermore, this study provides a detailed comparison of performance in
recognizing different transaction patterns (such as normal, money laundering,
and fraud) in large payment flows, validating the advantages of generative
models in handling complex financial data.

</details>


### [142] [Federated Latent Factor Learning for Recovering Wireless Sensor Networks Signal with Privacy-Preserving](https://arxiv.org/abs/2504.15525)
*Chengjun Yu,Yixin Ran,Yangyi Xia,Jia Wu,Xiaojing Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于联邦潜在因子学习（FLFL）的空间信号恢复（SSR）模型FLFL-SSR，解决无线传感器网络（WSNs）中数据缺失和隐私保护问题。


<details>
  <summary>Details</summary>
Motivation: 由于传感器故障和节能策略，WSNs采集的数据常存在大量缺失，且现有潜在因子学习方法未充分考虑数据隐私保护。

Method: 设计了传感器级联邦学习框架，仅上传梯度更新而非原始数据，并提出局部空间共享策略，以捕捉空间相关性。

Result: 在两个真实WSNs数据集上，FLFL-SSR在恢复性能上优于现有联邦方法。

Conclusion: FLFL-SSR有效解决了数据缺失和隐私保护问题，提升了恢复精度。

Abstract: Wireless Sensor Networks (WSNs) are a cutting-edge domain in the field of
intelligent sensing. Due to sensor failures and energy-saving strategies, the
collected data often have massive missing data, hindering subsequent analysis
and decision-making. Although Latent Factor Learning (LFL) has been proven
effective in recovering missing data, it fails to sufficiently consider data
privacy protection. To address this issue, this paper innovatively proposes a
federated latent factor learning (FLFL) based spatial signal recovery (SSR)
model, named FLFL-SSR. Its main idea is two-fold: 1) it designs a sensor-level
federated learning framework, where each sensor uploads only gradient updates
instead of raw data to optimize the global model, and 2) it proposes a local
spatial sharing strategy, allowing sensors within the same spatial region to
share their latent feature vectors, capturing spatial correlations and
enhancing recovery accuracy. Experimental results on two real-world WSNs
datasets demonstrate that the proposed model outperforms existing federated
methods in terms of recovery performance.

</details>


### [143] [Interpretable Deep Learning for Polar Mechanistic Reaction Prediction](https://arxiv.org/abs/2504.15539)
*Ryan J. Miller,Alexander E. Dashuta,Brayden Rudisill,David Van Vranken,Pierre Baldi*

Main category: cs.LG

TL;DR: 论文提出PMechRP系统，通过机器学习模型预测化学反应，结合PMechDB数据集和组合生成的反应，提升预测准确性和泛化能力。最佳混合模型在测试集上达到94.9%的准确率。


<details>
  <summary>Details</summary>
Motivation: 化学反应预测对合成化学创新至关重要，但现有模型缺乏机理细节和可解释性。PMechRP旨在通过捕捉电子流动和机理细节的PMechDB数据集解决这一问题。

Method: 使用PMechDB数据集，训练多种机器学习模型（如Transformer、图模型和两步Siamese架构），并引入组合生成的反应扩展数据集。最佳模型为混合架构。

Result: 混合模型在PMechDB测试集上达到94.9%的top-10准确率，在路径数据集上目标恢复率为84.9%。

Conclusion: PMechRP通过结合机理细节和混合模型架构，显著提升了反应预测的准确性和实用性。

Abstract: Accurately predicting chemical reactions is essential for driving innovation
in synthetic chemistry, with broad applications in medicine, manufacturing, and
agriculture. At the same time, reaction prediction is a complex problem which
can be both time-consuming and resource-intensive for chemists to solve. Deep
learning methods offer an appealing solution by enabling high-throughput
reaction prediction. However, many existing models are trained on the US Patent
Office dataset and treat reactions as overall transformations: mapping
reactants directly to products with limited interpretability or mechanistic
insight. To address this, we introduce PMechRP (Polar Mechanistic Reaction
Predictor), a system that trains machine learning models on the PMechDB
dataset, which represents reactions as polar elementary steps that capture
electron flow and mechanistic detail. To further expand model coverage and
improve generalization, we augment PMechDB with a diverse set of
combinatorially generated reactions. We train and compare a range of machine
learning models, including transformer-based, graph-based, and two-step siamese
architectures. Our best-performing approach was a hybrid model, which combines
a 5-ensemble of Chemformer models with a two-step Siamese framework to leverage
the accuracy of transformer architectures, while filtering away "alchemical"
products using the two-step network predictions. For evaluation, we use a test
split of the PMechDB dataset and additionally curate a human benchmark dataset
consisting of complete mechanistic pathways extracted from an organic chemistry
textbook. Our hybrid model achieves a top-10 accuracy of 94.9% on the PMechDB
test set and a target recovery rate of 84.9% on the pathway dataset.

</details>


### [144] [Bayesian Autoencoder for Medical Anomaly Detection: Uncertainty-Aware Approach for Brain 2 MRI Analysis](https://arxiv.org/abs/2504.15562)
*Dip Roy*

Main category: cs.LG

TL;DR: 该论文提出了一种基于贝叶斯变分自编码器（VAE）和多头注意力机制的模型，用于脑部MRI中的异常检测，通过估计不确定性和提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统确定性方法在捕捉异常检测任务中的不确定性方面表现不足，尤其是在医学影像中，这对神经系统疾病的诊断至关重要。

Method: 使用贝叶斯变分自编码器（VAE）结合多头注意力机制，并通过贝叶斯推理估计认知和随机不确定性。

Result: 在BraTS2020数据集上测试，模型取得了0.83的ROC AUC和0.83的PR AUC。

Conclusion: 建模不确定性是异常检测的关键，不仅提升了性能和可解释性，还为临床决策提供了置信度估计和异常预测。

Abstract: In medical imaging, anomaly detection is a vital element of healthcare
diagnostics, especially for neurological conditions which can be
life-threatening. Conventional deterministic methods often fall short when it
comes to capturing the inherent uncertainty of anomaly detection tasks. This
paper introduces a Bayesian Variational Autoencoder (VAE) equipped with
multi-head attention mechanisms for detecting anomalies in brain magnetic
resonance imaging (MRI). For the purpose of improving anomaly detection
performance, we incorporate both epistemic and aleatoric uncertainty estimation
through Bayesian inference. The model was tested on the BraTS2020 dataset, and
the findings were a 0.83 ROC AUC and a 0.83 PR AUC. The data in our paper
suggests that modeling uncertainty is an essential component of anomaly
detection, enhancing both performance and interpretability and providing
confidence estimates, as well as anomaly predictions, for clinicians to
leverage in making medical decisions.

</details>


### [145] [Smooth Calibration and Decision Making](https://arxiv.org/abs/2504.15582)
*Jason Hartline,Yifan Wu,Yunran Yang*

Main category: cs.LG

TL;DR: 论文探讨了校准误差在机器学习和决策制定中的差异，并提出了一种后处理方法以优化决策校准误差。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决机器学习预测器在校准误差上的连续性与其在决策制定中的不连续性之间的矛盾，确保预测器对决策者可靠。

Method: 通过后处理在线预测器，添加噪声以实现差分隐私，从而优化决策校准误差（ECE和CDL）。

Result: 结果表明，后处理方法能以$O(\sqrt{\epsilon})$的渐进最优方式降低ECE和CDL。

Conclusion: 结论指出，后处理方法虽有效，但相比直接优化ECE和CDL的在线校准算法，其最优边界仍非最优。

Abstract: Calibration requires predictor outputs to be consistent with their Bayesian
posteriors. For machine learning predictors that do not distinguish between
small perturbations, calibration errors are continuous in predictions, e.g.,
smooth calibration error (Foster and Hart, 2018), Distance to Calibration
(Blasiok et al., 2023a). On the contrary, decision-makers who use predictions
make optimal decisions discontinuously in probabilistic space, experiencing
loss from miscalibration discontinuously. Calibration errors for
decision-making are thus discontinuous, e.g., Expected Calibration Error
(Foster and Vohra, 1997), and Calibration Decision Loss (Hu and Wu, 2024).
Thus, predictors with a low calibration error for machine learning may suffer a
high calibration error for decision-making, i.e., they may not be trustworthy
for decision-makers optimizing assuming their predictions are correct. It is
natural to ask if post-processing a predictor with a low calibration error for
machine learning is without loss to achieve a low calibration error for
decision-making. In our paper, we show that post-processing an online predictor
with $\epsilon$ distance to calibration achieves $O(\sqrt{\epsilon})$ ECE and
CDL, which is asymptotically optimal. The post-processing algorithm adds noise
to make predictions differentially private. The optimal bound from low distance
to calibration predictors from post-processing is non-optimal compared with
existing online calibration algorithms that directly optimize for ECE and CDL.

</details>


### [146] [MetaMolGen: A Neural Graph Motif Generation Model for De Novo Molecular Design](https://arxiv.org/abs/2504.15587)
*Zimo Yan,Jie Zhang,Zheng Xie,Chang Liu,Yizhen Liu,Yiping Song*

Main category: cs.LG

TL;DR: MetaMolGen是一种基于元学习的分子生成器，用于少样本和属性条件分子生成，通过标准化图基序分布和轻量级自回归模型生成SMILES序列，并在低数据条件下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺场景下传统生成模型难以实现满意条件泛化的问题。

Method: MetaMolGen通过将图基序映射到归一化潜在空间标准化分布，并使用轻量级自回归序列模型生成SMILES序列，同时通过可学习属性投影器支持条件生成。

Result: 实验表明，MetaMolGen在低数据条件下能生成有效且多样的SMILES序列，优于传统基线方法。

Conclusion: MetaMolGen在快速适应和高效条件生成方面具有优势，适用于实际分子设计。

Abstract: Molecular generation plays an important role in drug discovery and materials
science, especially in data-scarce scenarios where traditional generative
models often struggle to achieve satisfactory conditional generalization. To
address this challenge, we propose MetaMolGen, a first-order
meta-learning-based molecular generator designed for few-shot and
property-conditioned molecular generation. MetaMolGen standardizes the
distribution of graph motifs by mapping them to a normalized latent space, and
employs a lightweight autoregressive sequence model to generate SMILES
sequences that faithfully reflect the underlying molecular structure. In
addition, it supports conditional generation of molecules with target
properties through a learnable property projector integrated into the
generative process.Experimental results demonstrate that MetaMolGen
consistently generates valid and diverse SMILES sequences under low-data
regimes, outperforming conventional baselines. This highlights its advantage in
fast adaptation and efficient conditional generation for practical molecular
design.

</details>


### [147] [Analytical Softmax Temperature Setting from Feature Dimensions for Model- and Domain-Robust Classification](https://arxiv.org/abs/2504.15594)
*Tatsuhito Hasegawa,Shunsuke Sakai*

Main category: cs.LG

TL;DR: 本文提出了一种基于特征维度确定softmax温度参数T*的理论方法，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究softmax温度参数T对分类任务的影响，并探索无需训练即可确定最优T*的方法。

Method: 提出理论框架确定T*与特征维度的关系，引入批归一化层稳定特征空间，并通过实验优化温度调整系数。

Result: 实验表明，提出的T*不仅符合理论预期，还能有效提升分类性能。

Conclusion: 该方法为确定softmax温度参数提供了一种无需训练且通用的解决方案。

Abstract: In deep learning-based classification tasks, the softmax function's
temperature parameter $T$ critically influences the output distribution and
overall performance. This study presents a novel theoretical insight that the
optimal temperature $T^*$ is uniquely determined by the dimensionality of the
feature representations, thereby enabling training-free determination of $T^*$.
Despite this theoretical grounding, empirical evidence reveals that $T^*$
fluctuates under practical conditions owing to variations in models, datasets,
and other confounding factors. To address these influences, we propose and
optimize a set of temperature determination coefficients that specify how $T^*$
should be adjusted based on the theoretical relationship to feature
dimensionality. Additionally, we insert a batch normalization layer immediately
before the output layer, effectively stabilizing the feature space. Building on
these coefficients and a suite of large-scale experiments, we develop an
empirical formula to estimate $T^*$ without additional training while also
introducing a corrective scheme to refine $T^*$ based on the number of classes
and task complexity. Our findings confirm that the derived temperature not only
aligns with the proposed theoretical perspective but also generalizes
effectively across diverse tasks, consistently enhancing classification
performance and offering a practical, training-free solution for determining
$T^*$.

</details>


### [148] [Learning Dynamic Graphs via Tensorized and Lightweight Graph Convolutional Networks](https://arxiv.org/abs/2504.15613)
*Minglian Han*

Main category: cs.LG

TL;DR: 提出了一种新的张量轻量图卷积网络（TLGCN），用于动态图学习，通过联合传播时空信息和减少模型内存占用，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统动态图卷积网络（DGCN）将空间和时间模式分开建模，破坏了时空依赖关系，需要一种更高效的联合建模方法。

Method: 设计了基于张量M乘积框架的时空信息联合传播方法，并提出了省略复杂特征变换和非线性激活的轻量图卷积网络。

Result: 在四个真实数据集上的实验表明，TLGCN在动态图权重估计任务中优于现有模型。

Conclusion: TLGCN通过联合建模时空依赖和轻量化设计，显著提升了动态图学习的性能。

Abstract: A dynamic graph (DG) is frequently encountered in numerous real-world
scenarios. Consequently, A dynamic graph convolutional network (DGCN) has been
successfully applied to perform precise representation learning on a DG.
However, conventional DGCNs typically consist of a static GCN coupled with a
sequence neural network (SNN) to model spatial and temporal patterns
separately. This decoupled modeling mechanism inherently disrupts the intricate
spatio-temporal dependencies. To address the issue, this study proposes a novel
Tensorized Lightweight Graph Convolutional Network (TLGCN) for accurate dynamic
graph learning. It mainly contains the following two key concepts: a) designing
a novel spatio-temporal information propagation method for joint propagation of
spatio-temporal information based on the tensor M-product framework; b)
proposing a tensorized lightweight graph convolutional network based on the
above method, which significantly reduces the memory occupation of the model by
omitting complex feature transformation and nonlinear activation. Numerical
experiments on four real-world datasets demonstrate that the proposed TLGCN
outperforms the state-of-the-art models in the weight estimation task on DGs.

</details>


### [149] [Dimension-Free Decision Calibration for Nonlinear Loss Functions](https://arxiv.org/abs/2504.15615)
*Jingwu Tang,Jiayun Wu,Zhiwei Steven Wu,Jiahao Zhang*

Main category: cs.LG

TL;DR: 论文探讨了在高维预测空间中如何通过决策校准确保简单最佳响应规则的最优性，并提出了针对非线性损失的平滑决策校准方法。


<details>
  <summary>Details</summary>
Motivation: 研究在模型预测用于下游决策时，如何确保决策者可以像对待真实结果一样响应预测，尤其是在高维和非线性损失情况下。

Method: 提出了一种平滑决策校准方法，通过平滑最佳响应实现维度无关的决策校准，并设计了高效的后处理算法。

Result: 证明了标准决策校准需要多项式样本复杂度，而平滑决策校准可以实现维度无关的样本复杂度。

Conclusion: 平滑决策校准为高维和非线性损失场景提供了一种高效的解决方案。

Abstract: When model predictions inform downstream decision making, a natural question
is under what conditions can the decision-makers simply respond to the
predictions as if they were the true outcomes. Calibration suffices to
guarantee that simple best-response to predictions is optimal. However,
calibration for high-dimensional prediction outcome spaces requires exponential
computational and statistical complexity. The recent relaxation known as
decision calibration ensures the optimality of the simple best-response rule
while requiring only polynomial sample complexity in the dimension of outcomes.
However, known results on calibration and decision calibration crucially rely
on linear loss functions for establishing best-response optimality. A natural
approach to handle nonlinear losses is to map outcomes $y$ into a feature space
$\phi(y)$ of dimension $m$, then approximate losses with linear functions of
$\phi(y)$. Unfortunately, even simple classes of nonlinear functions can demand
exponentially large or infinite feature dimensions $m$. A key open problem is
whether it is possible to achieve decision calibration with sample complexity
independent of~$m$. We begin with a negative result: even verifying decision
calibration under standard deterministic best response inherently requires
sample complexity polynomial in~$m$. Motivated by this lower bound, we
investigate a smooth version of decision calibration in which decision-makers
follow a smooth best-response. This smooth relaxation enables dimension-free
decision calibration algorithms. We introduce algorithms that, given
$\mathrm{poly}(|A|,1/\epsilon)$ samples and any initial predictor~$p$, can
efficiently post-process it to satisfy decision calibration without worsening
accuracy. Our algorithms apply broadly to function classes that can be
well-approximated by bounded-norm functions in (possibly infinite-dimensional)
separable RKHS.

</details>


### [150] [SocialMOIF: Multi-Order Intention Fusion for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2504.15616)
*Kai Chen,Xiaodong Zhao,Yujie Huang,Guoyu Fang,Xiao Song,Ruiping Wang,Ziyuan Wang*

Main category: cs.LG

TL;DR: SocialMOIF提出了一种多阶意图融合模型，用于解决智能系统中代理轨迹预测的高不确定性和复杂高阶影响问题。


<details>
  <summary>Details</summary>
Motivation: 当前代理轨迹预测存在高不确定性和复杂高阶影响的局限性，需要更全面的意图信息理解。

Method: 开发多阶意图融合模型，设计轨迹分布近似器和全局轨迹优化器，引入考虑距离和方向的损失函数。

Result: 实验表明，该模型在动态和静态数据集中优于现有基线。

Conclusion: SocialMOIF通过多阶意图融合和优化方法，显著提升了轨迹预测的准确性和效率。

Abstract: The analysis and prediction of agent trajectories are crucial for
decision-making processes in intelligent systems, with precise short-term
trajectory forecasting being highly significant across a range of applications.
Agents and their social interactions have been quantified and modeled by
researchers from various perspectives; however, substantial limitations exist
in the current work due to the inherent high uncertainty of agent intentions
and the complex higher-order influences among neighboring groups. SocialMOIF is
proposed to tackle these challenges, concentrating on the higher-order
intention interactions among neighboring groups while reinforcing the primary
role of first-order intention interactions between neighbors and the target
agent. This method develops a multi-order intention fusion model to achieve a
more comprehensive understanding of both direct and indirect intention
information. Within SocialMOIF, a trajectory distribution approximator is
designed to guide the trajectories toward values that align more closely with
the actual data, thereby enhancing model interpretability. Furthermore, a
global trajectory optimizer is introduced to enable more accurate and efficient
parallel predictions. By incorporating a novel loss function that accounts for
distance and direction during training, experimental results demonstrate that
the model outperforms previous state-of-the-art baselines across multiple
metrics in both dynamic and static datasets.

</details>


### [151] [RadioDiff-$k^2$: Helmholtz Equation Informed Generative Diffusion Model for Multi-Path Aware Radio Map Construction](https://arxiv.org/abs/2504.15623)
*Xiucheng Wang,Qiming Zhang,Nan Cheng,Ruijin Sun,Zan Li,Shuguang Cui,Xuemin Shen*

Main category: cs.LG

TL;DR: 提出了一种基于物理的生成学习方法RadioDiff-$m{k^2}$，用于高效构建多径感知的无线电地图（RM），结合数据驱动效率和物理建模精度。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络对智能优化的需求增长，传统电磁方法计算成本高，而现有神经网络方法缺乏对电磁波传播物理的充分考虑。

Method: 基于亥姆霍兹方程设计双生成扩散模型框架，一个模型推断电磁奇点，另一个利用奇点和环境信息重建完整RM。

Result: 该方法显著提高了复杂多径环境中RM的准确性，结合了数据驱动效率和物理建模优势。

Conclusion: RadioDiff-$m{k^2}$为多径感知RM构建提供了高效且物理准确的解决方案。

Abstract: In this paper, we propose a novel physics-informed generative learning
approach, termed RadioDiff-$\bm{k^2}$, for accurate and efficient
multipath-aware radio map (RM) construction. As wireless communication evolves
towards environment-aware paradigms, driven by the increasing demand for
intelligent and proactive optimization in sixth-generation (6G) networks,
accurate construction of RMs becomes crucial yet highly challenging.
Conventional electromagnetic (EM)-based methods, such as full-wave solvers and
ray-tracing approaches, exhibit substantial computational overhead and limited
adaptability to dynamic scenarios. Although, existing neural network (NN)
approaches have efficient inferencing speed, they lack sufficient consideration
of the underlying physics of EM wave propagation, limiting their effectiveness
in accurately modeling critical EM singularities induced by complex multipath
environments. To address these fundamental limitations, we propose a novel
physics-inspired RM construction method guided explicitly by the Helmholtz
equation, which inherently governs EM wave propagation. Specifically, we
theoretically establish a direct correspondence between EM singularities, which
correspond to the critical spatial features influencing wireless propagation,
and regions defined by negative wave numbers in the Helmholtz equation. Based
on this insight, we design an innovative dual generative diffusion model (DM)
framework comprising one DM dedicated to accurately inferring EM singularities
and another DM responsible for reconstructing the complete RM using these
singularities along with environmental contextual information. Our
physics-informed approach uniquely combines the efficiency advantages of
data-driven methods with rigorous physics-based EM modeling, significantly
enhancing RM accuracy, particularly in complex propagation environments
dominated by multipath effects.

</details>


### [152] [Enhancing Reinforcement learning in 3-Dimensional Hydrophobic-Polar Protein Folding Model with Attention-based layers](https://arxiv.org/abs/2504.15634)
*Peizheng Liu,Hitoshi Iba*

Main category: cs.LG

TL;DR: 本文提出了一种结合Transformer和DQN的方法，用于解决3D H-P蛋白质折叠问题，通过强化学习框架和注意力机制优化折叠决策，取得了接近最优的结果。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在序列建模中表现出色，但在H-P蛋白质折叠模型中的应用尚未充分探索，因此研究如何将其与强化学习结合以解决这一问题。

Method: 采用Deep Q-Network（DQN）结合注意力机制（Transformer），设计自回避行走的强化环境，并引入专门的奖励函数、有效性检查（如对称性破坏约束）、双Q学习和优先回放等技术。

Result: 在标准测试序列上，该方法对较短序列取得了已知最优解，对较长链获得了接近最优的结果。

Conclusion: 研究表明基于注意力的强化学习在蛋白质折叠中具有潜力，并构建了Transformer-based Q-network的原型结构。

Abstract: Transformer-based architectures have recently propelled advances in sequence
modeling across domains, but their application to the hydrophobic-hydrophilic
(H-P) model for protein folding remains relatively unexplored. In this work, we
adapt a Deep Q-Network (DQN) integrated with attention mechanisms
(Transformers) to address the 3D H-P protein folding problem. Our system
formulates folding decisions as a self-avoiding walk in a reinforced
environment, and employs a specialized reward function based on favorable
hydrophobic interactions. To improve performance, the method incorporates
validity check including symmetry-breaking constraints, dueling and double
Q-learning, and prioritized replay to focus learning on critical transitions.
Experimental evaluations on standard benchmark sequences demonstrate that our
approach achieves several known best solutions for shorter sequences, and
obtains near-optimal results for longer chains. This study underscores the
promise of attention-based reinforcement learning for protein folding, and
created a prototype of Transformer-based Q-network structure for 3-dimensional
lattice models.

</details>


### [153] [An XAI-based Analysis of Shortcut Learning in Neural Networks](https://arxiv.org/abs/2504.15664)
*Phuong Quynh Le,Jörg Schlötterer,Christin Seifert*

Main category: cs.LG

TL;DR: 论文提出了一种神经元虚假分数（neuron spurious score）来量化神经元对虚假特征的依赖，分析了CNN和ViT中虚假特征的解耦程度，并指出现有缓解方法的假设不完整。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型容易学习虚假特征（与目标标签强相关但非因果的特征），现有缓解方法在某些情况下有效，但在其他情况下失败。本文旨在系统分析神经网络如何及在哪里编码虚假相关性。

Method: 引入神经元虚假分数（XAI-based诊断工具），并针对CNN和ViT使用特定架构方法进行分析。

Result: 虚假特征在模型中部分解耦，但解耦程度因架构而异；现有缓解方法的假设不完整。

Conclusion: 研究结果为开发新的缓解虚假相关性的方法奠定了基础，使AI模型更安全。

Abstract: Machine learning models tend to learn spurious features - features that
strongly correlate with target labels but are not causal. Existing approaches
to mitigate models' dependence on spurious features work in some cases, but
fail in others. In this paper, we systematically analyze how and where neural
networks encode spurious correlations. We introduce the neuron spurious score,
an XAI-based diagnostic measure to quantify a neuron's dependence on spurious
features. We analyze both convolutional neural networks (CNNs) and vision
transformers (ViTs) using architecture-specific methods. Our results show that
spurious features are partially disentangled, but the degree of disentanglement
varies across model architectures. Furthermore, we find that the assumptions
behind existing mitigation methods are incomplete. Our results lay the
groundwork for the development of novel methods to mitigate spurious
correlations and make AI models safer to use in practice.

</details>


### [154] [Invariant Learning with Annotation-free Environments](https://arxiv.org/abs/2504.15686)
*Phuong Quynh Le,Christin Seifert,Jörg Schlötterer*

Main category: cs.LG

TL;DR: 该论文提出了一种无需额外标注即可推断环境的方法，通过观察ERM模型的表示空间特性，实现了与依赖显式环境标签方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 改进领域泛化能力，避免依赖预划分环境的假设。

Method: 利用ERM模型的表示空间特性推断环境，无需额外标注。

Result: 在ColoredMNIST基准测试中表现与依赖显式标签的方法相当。

Conclusion: 该方法在无标注条件下有效，为领域泛化提供了新思路。

Abstract: Invariant learning is a promising approach to improve domain generalization
compared to Empirical Risk Minimization (ERM). However, most invariant learning
methods rely on the assumption that training examples are pre-partitioned into
different known environments. We instead infer environments without the need
for additional annotations, motivated by observations of the properties within
the representation space of a trained ERM model. We show the preliminary
effectiveness of our approach on the ColoredMNIST benchmark, achieving
performance comparable to methods requiring explicit environment labels and on
par with an annotation-free method that poses strong restrictions on the ERM
reference model.

</details>


### [155] [Riemannian Neural Geodesic Interpolant](https://arxiv.org/abs/2504.15736)
*Jiawen Wu,Bingguang Chen,Yuyi Zhou,Qi Meng,Rongchan Zhu,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: 论文提出了一种在黎曼流形上插值概率密度的RNGI模型，并通过E-SDE算法改进采样质量。


<details>
  <summary>Details</summary>
Motivation: 现有随机插值模型局限于欧几里得空间，无法直接应用于黎曼流形上的分布学习问题。

Method: 引入RNGI模型，利用随机测地线插值概率密度，并通过E-SDE算法优化采样过程。

Result: RNGI解决了黎曼流形上的传输方程，E-SDE显著减少了采样误差，实验验证了其有效性。

Conclusion: RNGI和E-SDE在黎曼流形上的生成任务中表现出色，具有理论和实践优势。

Abstract: Stochastic interpolants are efficient generative models that bridge two
arbitrary probability density functions in finite time, enabling flexible
generation from the source to the target distribution or vice versa. These
models are primarily developed in Euclidean space, and are therefore limited in
their application to many distribution learning problems defined on Riemannian
manifolds in real-world scenarios. In this work, we introduce the Riemannian
Neural Geodesic Interpolant (RNGI) model, which interpolates between two
probability densities on a Riemannian manifold along the stochastic geodesics,
and then samples from one endpoint as the final state using the continuous flow
originating from the other endpoint. We prove that the temporal marginal
density of RNGI solves a transport equation on the Riemannian manifold. After
training the model's the neural velocity and score fields, we propose the
Embedding Stochastic Differential Equation (E-SDE) algorithm for stochastic
sampling of RNGI. E-SDE significantly improves the sampling quality by reducing
the accumulated error caused by the excessive intrinsic discretization of
Riemannian Brownian motion in the classical Geodesic Random Walk (GRW)
algorithm. We also provide theoretical bounds on the generative bias measured
in terms of KL-divergence. Finally, we demonstrate the effectiveness of the
proposed RNGI and E-SDE through experiments conducted on both collected and
synthetic distributions on S2 and SO(3).

</details>


### [156] [Observability conditions for neural state-space models with eigenvalues and their roots of unity](https://arxiv.org/abs/2504.15758)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 论文通过微分方程和控制理论研究了神经状态空间模型和Mamba架构中的可观测性，提出了针对学习场景的可观测性策略，并展示了计算高效的方法。


<details>
  <summary>Details</summary>
Motivation: 研究神经状态空间模型和Mamba架构中的可观测性问题，旨在为学习场景提供高效的可观测性策略。

Method: 利用微分方程和控制理论，结合特征值、单位根等数学工具，设计了计算高效的可观测性条件和方法。

Result: 提出了五种非平凡结果，包括基于傅里叶变换的高概率可观测性方法、Mamba的类Hautus条件，以及共享参数的高效构造。

Conclusion: 论文通过理论分析和算法设计，为神经状态空间模型的可观测性提供了高效且实用的解决方案。

Abstract: We operate through the lens of ordinary differential equations and control
theory to study the concept of observability in the context of neural
state-space models and the Mamba architecture. We develop strategies to enforce
observability, which are tailored to a learning context, specifically where the
hidden states are learnable at initial time, in conjunction to over its
continuum, and high-dimensional. We also highlight our methods emphasize
eigenvalues, roots of unity, or both. Our methods effectuate computational
efficiency when enforcing observability, sometimes at great scale. We formulate
observability conditions in machine learning based on classical control theory
and discuss their computational complexity. Our nontrivial results are
fivefold. We discuss observability through the use of permutations in neural
applications with learnable matrices without high precision. We present two
results built upon the Fourier transform that effect observability with high
probability up to the randomness in the learning. These results are worked with
the interplay of representations in Fourier space and their eigenstructure,
nonlinear mappings, and the observability matrix. We present a result for Mamba
that is similar to a Hautus-type condition, but instead employs an argument
using a Vandermonde matrix instead of eigenvectors. Our final result is a
shared-parameter construction of the Mamba system, which is computationally
efficient in high exponentiation. We develop a training algorithm with this
coupling, showing it satisfies a Robbins-Monro condition under certain
orthogonality, while a more classical training procedure fails to satisfy a
contraction with high Lipschitz constant.

</details>


### [157] [Grounded in Context: Retrieval-Based Method for Hallucination Detection](https://arxiv.org/abs/2504.15771)
*Assaf Gerner,Netta Madvil,Nadav Barak,Alex Zaikman,Jonatan Liberman,Liron Hamra,Rotem Brazilay,Shay Tsadok,Yaron Friedman,Neal Harow,Noam Bresler,Shir Chorev,Philip Tannor*

Main category: cs.LG

TL;DR: Deepchecks提出了一种名为“Grounded in Context”的幻觉检测框架，用于生产级长上下文数据，通过结合检索和NLI模型，在512个token的上下文窗口中预测事实一致性，性能优于同类方法。


<details>
  <summary>Details</summary>
Motivation: 尽管基于LLM的应用在生成内容方面有所进步，但仍存在幻觉答案的问题，需要一种有效的检测框架。

Method: 结合检索和自然语言推理（NLI）模型，使用基于编码器的模型在512个token的上下文窗口中预测事实一致性。

Result: 在RAGTruth的响应级别分类任务中，F1得分为0.83，优于同类框架。

Conclusion: 该框架在检测幻觉答案方面表现出色，适用于多种用例，如摘要、数据提取和RAG。

Abstract: Despite advancements in grounded content generation, production Large
Language Models (LLMs) based applications still suffer from hallucinated
answers. We present "Grounded in Context" - Deepchecks' hallucination detection
framework, designed for production-scale long-context data and tailored to
diverse use cases, including summarization, data extraction, and RAG. Inspired
by RAG architecture, our method integrates retrieval and Natural Language
Inference (NLI) models to predict factual consistency between premises and
hypotheses using an encoder-based model with only a 512-token context window.
Our framework identifies unsupported claims with an F1 score of 0.83 in
RAGTruth's response-level classification task, matching methods that trained on
the dataset, and outperforming all comparable frameworks using similar-sized
models.

</details>


### [158] [Clifford Group Equivariant Diffusion Models for 3D Molecular Generation](https://arxiv.org/abs/2504.15773)
*Cong Liu,Sharvaree Vadgama,David Ruhe,Erik Bekkers,Patrick Forrè*

Main category: cs.LG

TL;DR: 利用Clifford代数的表达能力构建E(n)-等变扩散模型，通过几何乘积和高阶多向量子空间捕捉更丰富的几何信息。


<details>
  <summary>Details</summary>
Motivation: 探索Clifford代数在扩散模型中的应用，以捕捉高阶几何特征和联合分布。

Method: 提出Clifford扩散模型（CDMs），将扩散过程扩展到所有高阶多向量子空间，并在QM9数据集上进行无条件分子生成实验。

Result: 实验结果表明CDMs在生成建模中具有潜力。

Conclusion: CDMs为生成建模提供了一种有前景的方法，尤其是在捕捉高阶几何信息方面。

Abstract: This paper explores leveraging the Clifford algebra's expressive power for
$\E(n)$-equivariant diffusion models. We utilize the geometric products between
Clifford multivectors and the rich geometric information encoded in Clifford
subspaces in \emph{Clifford Diffusion Models} (CDMs). We extend the diffusion
process beyond just Clifford one-vectors to incorporate all higher-grade
multivector subspaces. The data is embedded in grade-$k$ subspaces, allowing us
to apply latent diffusion across complete multivectors. This enables CDMs to
capture the joint distribution across different subspaces of the algebra,
incorporating richer geometric information through higher-order features. We
provide empirical results for unconditional molecular generation on the QM9
dataset, showing that CDMs provide a promising avenue for generative modeling.

</details>


### [159] [DAE-KAN: A Kolmogorov-Arnold Network Model for High-Index Differential-Algebraic Equations](https://arxiv.org/abs/2504.15806)
*Kai Luo,Juan Tang,Mingchao Cai,Xiaoqing Zeng,Manqi Xie,Ming Yan*

Main category: cs.LG

TL;DR: 本文提出了一种名为DAE-KAN的新框架，结合Kolmogorov-Arnold Networks（KANs）和Physics-Informed Neural Networks（PINNs），用于解决高指数微分代数方程（DAEs）。实验表明，DAE-KAN在误差控制方面显著优于传统PINNs。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在处理高指数DAEs时存在性能限制，而KANs因其强大的函数拟合能力被视为潜在解决方案。本文旨在结合两者优势，提升DAEs求解的精度和泛化能力。

Method: 提出DAE-KAN框架，将KANs与PINNs结合，利用KANs的函数拟合能力增强PINNs的性能。通过数值实验验证其在指数1至3的DAE系统中的表现。

Result: DAE-KAN将微分和代数变量的绝对误差降低了1到2个数量级，且在控制漂移误差方面优于传统数值方法。

Conclusion: DAE-KAN为解决高指数DAEs提供了一种高精度和强泛化的神经网络方法，展示了其在复杂偏微分代数方程中的潜力。

Abstract: Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to
Multi-Layer Perceptrons (MLPs) due to their superior function-fitting abilities
in data-driven modeling. In this paper, we propose a novel framework, DAE-KAN,
for solving high-index differential-algebraic equations (DAEs) by integrating
KANs with Physics-Informed Neural Networks (PINNs). This framework not only
preserves the ability of traditional PINNs to model complex systems governed by
physical laws but also enhances their performance by leveraging the
function-fitting strengths of KANs. Numerical experiments demonstrate that for
DAE systems ranging from index-1 to index-3, DAE-KAN reduces the absolute
errors of both differential and algebraic variables by 1 to 2 orders of
magnitude compared to traditional PINNs. To assess the effectiveness of this
approach, we analyze the drift-off error and find that both PINNs and DAE-KAN
outperform classical numerical methods in controlling this phenomenon. Our
results highlight the potential of neural network methods, particularly
DAE-KAN, in solving high-index DAEs with substantial computational accuracy and
generalization, offering a promising solution for challenging partial
differential-algebraic equations.

</details>


### [160] [Fusing Reward and Dueling Feedback in Stochastic Bandits](https://arxiv.org/abs/2504.15812)
*Xuchuang Wang,Qirun Zeng,Jinhang Zuo,Xutong Liu,Mohammad Hajiesmaili,John C. S. Lui,Adam Wierman*

Main category: cs.LG

TL;DR: 本文研究了在随机多臂老虎机问题中融合绝对（奖励）和相对（对决）反馈的方法，提出了两种融合算法，并分析了其遗憾下界。


<details>
  <summary>Details</summary>
Motivation: 探索如何有效结合绝对和相对反馈以优化决策过程，减少遗憾。

Method: 提出了两种融合算法：消除融合算法和分解融合算法，分别通过共享候选臂集和动态选择反馈类型来优化探索与利用。

Result: 消除融合算法因对决消除的固有次优性而存在遗憾的乘法项，而分解融合算法在常见假设下达到与下界匹配的遗憾。

Conclusion: 实验验证了算法的有效性，分解融合算法在理论上和实践中均表现优异。

Abstract: This paper investigates the fusion of absolute (reward) and relative
(dueling) feedback in stochastic bandits, where both feedback types are
gathered in each decision round. We derive a regret lower bound, demonstrating
that an efficient algorithm may incur only the smaller among the reward and
dueling-based regret for each individual arm. We propose two fusion approaches:
(1) a simple elimination fusion algorithm that leverages both feedback types to
explore all arms and unifies collected information by sharing a common
candidate arm set, and (2) a decomposition fusion algorithm that selects the
more effective feedback to explore the corresponding arms and randomly assigns
one feedback type for exploration and the other for exploitation in each round.
The elimination fusion experiences a suboptimal multiplicative term of the
number of arms in regret due to the intrinsic suboptimality of dueling
elimination. In contrast, the decomposition fusion achieves regret matching the
lower bound up to a constant under a common assumption. Extensive experiments
confirm the efficacy of our algorithms and theoretical results.

</details>


### [161] [DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with Dual Optimizers](https://arxiv.org/abs/2504.15827)
*Xuyang Zhong,Haochen Luo,Chen Liu*

Main category: cs.LG

TL;DR: 提出了一种名为DualOptim的新方法，通过自适应学习率和解耦动量因子，解决了现有机器遗忘（MU）方法对超参数敏感的问题，显著提升了遗忘效果和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法对超参数敏感，需要精细调参，限制了实际应用。本文通过实证展示了现有方法在不同场景下的不稳定性和性能不足。

Method: 提出DualOptim方法，结合自适应学习率和解耦动量因子，通过理论和实验验证其有效性。

Result: 实验表明，DualOptim在图像分类、图像生成和大语言模型等任务中显著提升了遗忘效果和稳定性。

Conclusion: DualOptim是一种通用方法，能够增强现有机器遗忘算法的性能，适用于多种任务。

Abstract: Existing machine unlearning (MU) approaches exhibit significant sensitivity
to hyperparameters, requiring meticulous tuning that limits practical
deployment. In this work, we first empirically demonstrate the instability and
suboptimal performance of existing popular MU methods when deployed in
different scenarios. To address this issue, we propose Dual Optimizer
(DualOptim), which incorporates adaptive learning rate and decoupled momentum
factors. Empirical and theoretical evidence demonstrates that DualOptim
contributes to effective and stable unlearning. Through extensive experiments,
we show that DualOptim can significantly boost MU efficacy and stability across
diverse tasks, including image classification, image generation, and large
language models, making it a versatile approach to empower existing MU
algorithms.

</details>


### [162] [Adaptive PCA-Based Outlier Detection for Multi-Feature Time Series in Space Missions](https://arxiv.org/abs/2504.15846)
*Jonah Ekelund,Savvas Raptis,Vicki Toy-Edens,Wenli Mo,Drew L. Turner,Ian J. Cohen,Stefano Markidis*

Main category: cs.LG

TL;DR: 该论文提出了一种基于PCA重建误差的自适应异常检测算法，用于空间任务中实时识别感兴趣区域。


<details>
  <summary>Details</summary>
Motivation: 空间任务中，有限的机载计算资源和数据下行链路限制需要高效的实时事件检测方法。

Method: 使用增量PCA动态适应数据分布变化，结合预缩放过程归一化特征幅度。

Result: 算法成功检测到空间等离子体事件，并在NASA的MMS和THEMIS任务中验证了有效性。

Conclusion: 该方法适用于空间任务中的实时异常检测，无需预定义模型。

Abstract: Analyzing multi-featured time series data is critical for space missions
making efficient event detection, potentially onboard, essential for automatic
analysis. However, limited onboard computational resources and data downlink
constraints necessitate robust methods for identifying regions of interest in
real time. This work presents an adaptive outlier detection algorithm based on
the reconstruction error of Principal Component Analysis (PCA) for feature
reduction, designed explicitly for space mission applications. The algorithm
adapts dynamically to evolving data distributions by using Incremental PCA,
enabling deployment without a predefined model for all possible conditions. A
pre-scaling process normalizes each feature's magnitude while preserving
relative variance within feature types. We demonstrate the algorithm's
effectiveness in detecting space plasma events, such as distinct space
environments, dayside and nightside transients phenomena, and transition layers
through NASA's MMS mission observations. Additionally, we apply the method to
NASA's THEMIS data, successfully identifying a dayside transient using
onboard-available measurements.

</details>


### [163] [Consistent Causal Inference of Group Effects in Non-Targeted Trials with Finitely Many Effect Levels](https://arxiv.org/abs/2504.15854)
*Georgios Mavroudeas,Malik Magdon-Ismail,Kristin P. Bennett,Jason Kuruzovich*

Main category: cs.LG

TL;DR: 论文提出了一种非参数方法PCM，用于估计治疗对不同群体的异质性效果，解决了现有方法在非目标试验中效果混杂的问题。


<details>
  <summary>Details</summary>
Motivation: 在非目标试验中，治疗可能对某些群体有益，但对其他群体有害，导致治疗效果混杂。现有方法难以准确估计目标群体的效果。

Method: 提出PCM（预聚类合并）方法，通过非参数方式估计群体效应，适用于有限范围函数的估计。

Result: 在合成数据上，PCM比现有方法准确度提升超过10倍，并证明了其渐近一致性。

Conclusion: PCM是一种高效且通用的方法，适用于估计具有有限范围函数的群体效应。

Abstract: A treatment may be appropriate for some group (the ``sick" group) on whom it
has a positive effect, but it can also have a detrimental effect on subjects
from another group (the ``healthy" group). In a non-targeted trial both sick
and healthy subjects may be treated, producing heterogeneous effects within the
treated group. Inferring the correct treatment effect on the sick population is
then difficult, because the effects on the different groups get tangled. We
propose an efficient nonparametric approach to estimating the group effects,
called {\bf PCM} (pre-cluster and merge). We prove its asymptotic consistency
in a general setting and show, on synthetic data, more than a 10x improvement
in accuracy over existing state-of-the-art. Our approach applies more generally
to consistent estimation of functions with a finite range.

</details>


### [164] [SUPRA: Subspace Parameterized Attention for Neural Operator on General Domains](https://arxiv.org/abs/2504.15897)
*Zherui Yang,Zhengyang Xue,Ligang Liu*

Main category: cs.LG

TL;DR: 提出了一种新的注意力机制SUPRA神经算子，解决了传统神经算子在处理不规则域和大规模网格时的计算效率与精度问题。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子在处理大规模网格时计算效率低，且在不规则域上精度下降。

Method: 将标准注意力机制推广到函数空间，提出SUPRA神经算子，利用拉普拉斯特征函数构建子空间。

Result: SUPRA神经算子将误差率降低达33%，同时保持计算效率。

Conclusion: SUPRA神经算子在精度和效率上均优于传统方法，适用于不规则域和大规模网格。

Abstract: Neural operators are efficient surrogate models for solving partial
differential equations (PDEs), but their key components face challenges: (1) in
order to improve accuracy, attention mechanisms suffer from computational
inefficiency on large-scale meshes, and (2) spectral convolutions rely on the
Fast Fourier Transform (FFT) on regular grids and assume a flat geometry, which
causes accuracy degradation on irregular domains. To tackle these problems, we
regard the matrix-vector operations in the standard attention mechanism on
vectors in Euclidean space as bilinear forms and linear operators in vector
spaces and generalize the attention mechanism to function spaces. This new
attention mechanism is fully equivalent to the standard attention but
impossible to compute due to the infinite dimensionality of function spaces. To
address this, inspired by model reduction techniques, we propose a Subspace
Parameterized Attention (SUPRA) neural operator, which approximates the
attention mechanism within a finite-dimensional subspace. To construct a
subspace on irregular domains for SUPRA, we propose using the Laplacian
eigenfunctions, which naturally adapt to domains' geometry and guarantee the
optimal approximation for smooth functions. Experiments show that the SUPRA
neural operator reduces error rates by up to 33% on various PDE datasets while
maintaining state-of-the-art computational efficiency.

</details>


### [165] [GraphEdge: Dynamic Graph Partition and Task Scheduling for GNNs Computing in Edge Network](https://arxiv.org/abs/2504.15905)
*Wenjing Xiao,Chenglong Shi,Miaojiang Chen,Zhiquan Liu,Min Chen,H. Herbert Song*

Main category: cs.LG

TL;DR: 论文提出GraphEdge架构，结合GNN和边缘计算，通过HiCut和DRLGO算法优化图布局和任务卸载策略，降低通信成本并提升效率。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备激增，边缘计算在关联数据场景（如交通预测、社交推荐）中表现不佳，尤其是GNN方法通信成本高。

Method: 提出GraphEdge架构，包括HiCut算法优化图布局以减少通信成本，以及DRLGO算法实现基于子图的任务卸载策略。

Result: 实验表明该架构高效且适应动态场景，显著降低任务处理时间和能耗。

Conclusion: GraphEdge为GNN在边缘计算中的应用提供了高效解决方案，适用于动态和关联数据场景。

Abstract: With the exponential growth of Internet of Things (IoT) devices, edge
computing (EC) is gradually playing an important role in providing
cost-effective services. However, existing approaches struggle to perform well
in graph-structured scenarios where user data is correlated, such as traffic
flow prediction and social relationship recommender systems. In particular,
graph neural network (GNN)-based approaches lead to expensive server
communication cost. To address this problem, we propose GraphEdge, an efficient
GNN-based EC architecture. It considers the EC system of GNN tasks, where there
are associations between users and it needs to take into account the task data
of its neighbors when processing the tasks of a user. Specifically, the
architecture first perceives the user topology and represents their data
associations as a graph layout at each time step. Then the graph layout is
optimized by calling our proposed hierarchical traversal graph cut algorithm
(HiCut), which cuts the graph layout into multiple weakly associated subgraphs
based on the aggregation characteristics of GNN, and the communication cost
between different subgraphs during GNN inference is minimized. Finally, based
on the optimized graph layout, our proposed deep reinforcement learning (DRL)
based graph offloading algorithm (DRLGO) is executed to obtain the optimal
offloading strategy for the tasks of users, the offloading strategy is
subgraph-based, it tries to offload user tasks in a subgraph to the same edge
server as possible while minimizing the task processing time and energy
consumption of the EC system. Experimental results show the good effectiveness
and dynamic adaptation of our proposed architecture and it also performs well
even in dynamic scenarios.

</details>


### [166] [ScaleGNN: Towards Scalable Graph Neural Networks via Adaptive High-order Neighboring Feature Fusion](https://arxiv.org/abs/2504.15920)
*Xiang Li,Haobing Liu,Jianpeng Qi,Yuan Cao,Guoqing Chao,Yanwei Yu*

Main category: cs.LG

TL;DR: ScaleGNN框架通过自适应融合多级图特征，解决了GNN中的过平滑和可扩展性问题，提升了准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: GNN在大型图上存在过平滑和可扩展性挑战，传统方法因冗余信息聚合导致高复杂性和推理时间增加。

Method: 提出ScaleGNN框架，包括自适应高阶特征融合模块、基于LCS的高阶冗余特征掩蔽机制，以及低阶增强特征聚合。

Result: 在真实数据集上，ScaleGNN在准确性和计算效率上均优于现有GNN模型。

Conclusion: ScaleGNN有效解决了GNN的两大挑战，为大规模图任务提供了高效解决方案。

Abstract: Graph Neural Networks (GNNs) have demonstrated strong performance across
various graph-based tasks by effectively capturing relational information
between nodes. These models rely on iterative message passing to propagate node
features, enabling nodes to aggregate information from their neighbors. Recent
research has significantly improved the message-passing mechanism, enhancing
GNN scalability on large-scale graphs. However, GNNs still face two main
challenges: over-smoothing, where excessive message passing results in
indistinguishable node representations, especially in deep networks
incorporating high-order neighbors; and scalability issues, as traditional
architectures suffer from high model complexity and increased inference time
due to redundant information aggregation. This paper proposes a novel framework
for large-scale graphs named ScaleGNN that simultaneously addresses both
challenges by adaptively fusing multi-level graph features. We first construct
neighbor matrices for each order, learning their relative information through
trainable weights through an adaptive high-order feature fusion module. This
allows the model to selectively emphasize informative high-order neighbors
while reducing unnecessary computational costs. Additionally, we introduce a
High-order redundant feature masking mechanism based on a Local Contribution
Score (LCS), which enables the model to retain only the most relevant neighbors
at each order, preventing redundant information propagation. Furthermore,
low-order enhanced feature aggregation adaptively integrates low-order and
high-order features based on task relevance, ensuring effective capture of both
local and global structural information without excessive complexity. Extensive
experiments on real-world datasets demonstrate that our approach consistently
outperforms state-of-the-art GNN models in both accuracy and computational
efficiency.

</details>


### [167] [Achieving Distributive Justice in Federated Learning via Uncertainty Quantification](https://arxiv.org/abs/2504.15924)
*Alycia Carey,Xintao Wu*

Main category: cs.LG

TL;DR: 论文提出了UDJ-FL框架，通过不确定性加权实现多种分配正义的客户端公平性指标，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习中的客户端公平性指标缺乏理论基础，难以与伦理公平性对齐，因此需要一种灵活的框架来支持多种分配正义的公平性定义。

Method: 结合公平资源分配技术和基于偶然不确定性的客户端加权，UDJ-FL框架实现了四种分配正义的公平性指标。

Result: UDJ-FL在实验中实现了四种公平性指标，性能优于其他公平联邦学习方法，并提供了理论泛化保证。

Conclusion: UDJ-FL通过不确定性加权实现了灵活的客户端公平性，为联邦学习中的公平性选择提供了理论基础和实践工具。

Abstract: Client-level fairness metrics for federated learning are used to ensure that
all clients in a federation either: a) have similar final performance on their
local data distributions (i.e., client parity), or b) obtain final performance
on their local data distributions relative to their contribution to the
federated learning process (i.e., contribution fairness). While a handful of
works that propose either client-parity or contribution-based fairness metrics
ground their definitions and decisions in social theories of equality -- such
as distributive justice -- most works arbitrarily choose what notion of
fairness to align with which makes it difficult for practitioners to choose
which fairness metric aligns best with their fairness ethics. In this work, we
propose UDJ-FL (Uncertainty-based Distributive Justice for Federated Learning),
a flexible federated learning framework that can achieve multiple distributive
justice-based client-level fairness metrics. Namely, by utilizing techniques
inspired by fair resource allocation, in conjunction with performing aleatoric
uncertainty-based client weighing, our UDJ-FL framework is able to achieve
egalitarian, utilitarian, Rawls' difference principle, or desert-based
client-level fairness. We empirically show the ability of UDJ-FL to achieve all
four defined distributive justice-based client-level fairness metrics in
addition to providing fairness equivalent to (or surpassing) other popular fair
federated learning works. Further, we provide justification for why aleatoric
uncertainty weighing is necessary to the construction of our UDJ-FL framework
as well as derive theoretical guarantees for the generalization bounds of
UDJ-FL. Our code is publicly available at
https://github.com/alycia-noel/UDJ-FL.

</details>


### [168] [StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with Disaggregated Stream Generation](https://arxiv.org/abs/2504.15930)
*Yinmin Zhong,Zili Zhang,Xiaoniu Song,Hanpeng Hu,Chao Jin,Bingyang Wu,Nuo Chen,Yukun Chen,Yu Zhou,Changyi Wan,Hongyu Zhou,Yimin Jiang,Yibo Zhu,Daxin Jiang*

Main category: cs.LG

TL;DR: StreamRL通过解耦架构解决了传统RL在LLMs训练中的资源耦合问题，提升了吞吐量和成本效益。


<details>
  <summary>Details</summary>
Motivation: 传统RL架构中资源耦合限制了扩展性和成本效率，而解耦架构虽灵活但存在性能瓶颈。

Method: StreamRL通过流生成和异步RL消除管道气泡，并通过输出长度排序模型减少偏斜气泡。

Result: StreamRL吞吐量提升2.66倍，异构跨数据中心环境下成本效益提升1.33倍。

Conclusion: StreamRL证明了解耦架构在RL训练中的潜力，显著提升了性能和效率。

Abstract: Reinforcement learning (RL) has become the core post-training technique for
large language models (LLMs). RL for LLMs involves two stages: generation and
training. The LLM first generates samples online, which are then used to derive
rewards for training. The conventional view holds that the colocated
architecture, where the two stages share resources via temporal multiplexing,
outperforms the disaggregated architecture, in which dedicated resources are
assigned to each stage. However, in real-world deployments, we observe that the
colocated architecture suffers from resource coupling, where the two stages are
constrained to use the same resources. This coupling compromises the
scalability and cost-efficiency of colocated RL in large-scale training. In
contrast, the disaggregated architecture allows for flexible resource
allocation, supports heterogeneous training setups, and facilitates
cross-datacenter deployment.
  StreamRL is designed with disaggregation from first principles and fully
unlocks its potential by addressing two types of performance bottlenecks in
existing disaggregated RL frameworks: pipeline bubbles, caused by stage
dependencies, and skewness bubbles, resulting from long-tail output length
distributions. To address pipeline bubbles, StreamRL breaks the traditional
stage boundary in synchronous RL algorithms through stream generation and
achieves full overlapping in asynchronous RL. To address skewness bubbles,
StreamRL employs an output-length ranker model to identify long-tail samples
and reduces generation time via skewness-aware dispatching and scheduling.
Experiments show that StreamRL improves throughput by up to 2.66x compared to
existing state-of-the-art systems, and improves cost-effectiveness by up to
1.33x in a heterogeneous, cross-datacenter setting.

</details>


### [169] [Universal Approximation with Softmax Attention](https://arxiv.org/abs/2504.15956)
*Jerry Yao-Chieh Hu,Hude Liu,Hong-Yu Chen,Weimin Wu,Han Liu*

Main category: cs.LG

TL;DR: 论文证明了两层自注意力和一层自注意力加softmax函数在紧凑域上对连续序列到序列函数具有通用逼近能力。


<details>
  <summary>Details</summary>
Motivation: 探索自注意力机制在序列建模中的通用逼近能力，减少对前馈网络的依赖。

Method: 提出基于插值的新方法分析注意力内部机制，证明自注意力可逼近广义ReLU。

Result: 两层多头注意力即可作为序列到序列通用逼近器，且注意力层能逼近多种统计模型。

Conclusion: 自注意力机制具有强大的通用逼近能力，为Transformer架构提供了新的理论支持。

Abstract: We prove that with linear transformations, both (i) two-layer self-attention
and (ii) one-layer self-attention followed by a softmax function are universal
approximators for continuous sequence-to-sequence functions on compact domains.
Our main technique is a new interpolation-based method for analyzing
attention's internal mechanism. This leads to our key insight: self-attention
is able to approximate a generalized version of ReLU to arbitrary precision,
and hence subsumes many known universal approximators. Building on these, we
show that two-layer multi-head attention alone suffices as a
sequence-to-sequence universal approximator. In contrast, prior works rely on
feed-forward networks to establish universal approximation in Transformers.
Furthermore, we extend our techniques to show that, (softmax-)attention-only
layers are capable of approximating various statistical models in-context. We
believe these techniques hold independent interest.

</details>


### [170] [OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical Federated Learning](https://arxiv.org/abs/2504.15995)
*Sindhuja Madabushi,Ahmad Faraz Khan,Haider Ali,Jin-Hee Cho*

Main category: cs.LG

TL;DR: OPUS-VFL提出了一种新的垂直联邦学习框架，通过隐私感知激励机制和自适应差分隐私机制，解决了现有VFL系统在激励、隐私-效用权衡和资源异构性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有VFL系统缺乏有效激励机制，难以平衡隐私与效用，且无法适应资源异构性，导致参与度低、模型性能下降。

Method: OPUS-VFL采用轻量级留一策略量化特征重要性，结合自适应差分隐私机制动态调整噪声水平，并设计隐私感知激励机制。

Result: 在MNIST、CIFAR-10和CIFAR-100数据集上，OPUS-VFL显著优于现有方法，降低标签推断攻击成功率20%，增加特征推断重建误差30%，并为贡献者提供25%更高激励。

Conclusion: OPUS-VFL是一种安全、公平且高效的VFL解决方案，适用于实际部署。

Abstract: Vertical Federated Learning (VFL) enables organizations with disjoint feature
spaces but shared user bases to collaboratively train models without sharing
raw data. However, existing VFL systems face critical limitations: they often
lack effective incentive mechanisms, struggle to balance privacy-utility
tradeoffs, and fail to accommodate clients with heterogeneous resource
capabilities. These challenges hinder meaningful participation, degrade model
performance, and limit practical deployment. To address these issues, we
propose OPUS-VFL, an Optimal Privacy-Utility tradeoff Strategy for VFL.
OPUS-VFL introduces a novel, privacy-aware incentive mechanism that rewards
clients based on a principled combination of model contribution, privacy
preservation, and resource investment. It employs a lightweight leave-one-out
(LOO) strategy to quantify feature importance per client, and integrates an
adaptive differential privacy mechanism that enables clients to dynamically
calibrate noise levels to optimize their individual utility. Our framework is
designed to be scalable, budget-balanced, and robust to inference and poisoning
attacks. Extensive experiments on benchmark datasets (MNIST, CIFAR-10, and
CIFAR-100) demonstrate that OPUS-VFL significantly outperforms state-of-the-art
VFL baselines in both efficiency and robustness. It reduces label inference
attack success rates by up to 20%, increases feature inference reconstruction
error (MSE) by over 30%, and achieves up to 25% higher incentives for clients
that contribute meaningfully while respecting privacy and cost constraints.
These results highlight the practicality and innovation of OPUS-VFL as a
secure, fair, and performance-driven solution for real-world VFL.

</details>


### [171] [AlphaGrad: Non-Linear Gradient Normalization Optimizer](https://arxiv.org/abs/2504.16020)
*Soham Sane*

Main category: cs.LG

TL;DR: AlphaGrad是一种内存高效、条件无状态的优化器，通过张量级L2梯度归一化和双曲正切变换实现尺度不变性，仅需一个参数α控制。它在不同RL基准测试中表现各异，特别适合内存受限场景和on-policy学习。


<details>
  <summary>Details</summary>
Motivation: 解决自适应方法（如Adam）的内存开销和超参数复杂性问题，提供一种更高效的优化器。

Method: 采用张量级L2梯度归一化和双曲正切变换（g' = tanh(α·g̃)），仅需调节一个参数α。

Result: 在DQN中表现不稳定，在TD3中需谨慎调参但结果竞争性，在PPO中表现显著优于Adam。

Conclusion: AlphaGrad是内存受限场景下的有竞争力替代方案，特别适合on-policy学习，其稳定性和效率优势显著。

Abstract: We introduce AlphaGrad, a memory-efficient, conditionally stateless optimizer
addressing the memory overhead and hyperparameter complexity of adaptive
methods like Adam. AlphaGrad enforces scale invariance via tensor-wise L2
gradient normalization followed by a smooth hyperbolic tangent transformation,
$g' = \tanh(\alpha \cdot \tilde{g})$, controlled by a single steepness
parameter $\alpha$. Our contributions include: (1) the AlphaGrad algorithm
formulation; (2) a formal non-convex convergence analysis guaranteeing
stationarity; (3) extensive empirical evaluation on diverse RL benchmarks (DQN,
TD3, PPO). Compared to Adam, AlphaGrad demonstrates a highly context-dependent
performance profile. While exhibiting instability in off-policy DQN, it
provides enhanced training stability with competitive results in TD3 (requiring
careful $\alpha$ tuning) and achieves substantially superior performance in
on-policy PPO. These results underscore the critical importance of empirical
$\alpha$ selection, revealing strong interactions between the optimizer's
dynamics and the underlying RL algorithm. AlphaGrad presents a compelling
alternative optimizer for memory-constrained scenarios and shows significant
promise for on-policy learning regimes where its stability and efficiency
advantages can be particularly impactful.

</details>


### [172] [LLMs meet Federated Learning for Scalable and Secure IoT Management](https://arxiv.org/abs/2504.16032)
*Yazan Otoum,Arghavan Asad,Amiya Nayak*

Main category: cs.LG

TL;DR: 本文提出了一种基于联邦学习的大语言模型框架（FL-LLM），用于提升物联网系统的智能性，同时确保数据隐私和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统集中式架构在物联网部署中存在延迟、隐私问题和资源消耗大的问题，无法满足现代大规模物联网的需求。

Method: 结合生成式物联网（GIoT）模型和梯度感知联邦策略（GSFS），动态优化模型更新，并采用混合边缘-云处理架构。

Result: 在IoT-23数据集上的评估显示，该框架提高了模型准确性，降低了响应延迟，并提升了能源效率，优于传统联邦学习方法。

Conclusion: 该框架为大规模物联网生态系统提供了更安全、可扩展和自适应的管理解决方案。

Abstract: The rapid expansion of IoT ecosystems introduces severe challenges in
scalability, security, and real-time decision-making. Traditional centralized
architectures struggle with latency, privacy concerns, and excessive resource
consumption, making them unsuitable for modern large-scale IoT deployments.
This paper presents a novel Federated Learning-driven Large Language Model
(FL-LLM) framework, designed to enhance IoT system intelligence while ensuring
data privacy and computational efficiency. The framework integrates Generative
IoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS),
dynamically optimizing model updates based on real-time network conditions. By
leveraging a hybrid edge-cloud processing architecture, our approach balances
intelligence, scalability, and security in distributed IoT environments.
Evaluations on the IoT-23 dataset demonstrate that our framework improves model
accuracy, reduces response latency, and enhances energy efficiency,
outperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings
highlight the potential of integrating LLM-powered federated learning into
large-scale IoT ecosystems, paving the way for more secure, scalable, and
adaptive IoT management solutions.

</details>


### [173] [Muon Optimizer Accelerates Grokking](https://arxiv.org/abs/2504.16041)
*Amund Tveit,Bjørn Remseth,Arve Skogvold*

Main category: cs.LG

TL;DR: 研究不同优化器对模型延迟泛化现象（grokking）的影响，发现Muon优化器显著加速泛化过程。


<details>
  <summary>Details</summary>
Motivation: 探讨优化器选择对模型从记忆到泛化过渡的影响。

Method: 在七个数值任务上实验，比较Muon和AdamW优化器及不同softmax函数的组合效果。

Result: Muon优化器将平均泛化时间从153.09降至102.89，效果显著。

Conclusion: 优化器选择对模型泛化过程至关重要，Muon优于AdamW。

Abstract: This paper investigates the impact of different optimizers on the grokking
phenomenon, where models exhibit delayed generalization. We conducted
experiments across seven numerical tasks (primarily modular arithmetic) using a
modern Transformer architecture. The experimental configuration systematically
varied the optimizer (Muon vs. AdamW) and the softmax activation function
(standard softmax, stablemax, and sparsemax) to assess their combined effect on
learning dynamics. Our empirical evaluation reveals that the Muon optimizer,
characterized by its use of spectral norm constraints and second-order
information, significantly accelerates the onset of grokking compared to the
widely used AdamW optimizer. Specifically, Muon reduced the mean grokking epoch
from 153.09 to 102.89 across all configurations, a statistically significant
difference (t = 5.0175, p = 6.33e-08). This suggests that the optimizer choice
plays a crucial role in facilitating the transition from memorization to
generalization.

</details>


### [174] [$π_{0.5}$: a Vision-Language-Action Model with Open-World Generalization](https://arxiv.org/abs/2504.16054)
*Physical Intelligence,Kevin Black,Noah Brown,James Darpinian,Karan Dhabalia,Danny Driess,Adnan Esmail,Michael Equi,Chelsea Finn,Niccolo Fusai,Manuel Y. Galliker,Dibya Ghosh,Lachy Groom,Karol Hausman,Brian Ichter,Szymon Jakubczak,Tim Jones,Liyiming Ke,Devin LeBlanc,Sergey Levine,Adrian Li-Bell,Mohith Mothukuri,Suraj Nair,Karl Pertsch,Allen Z. Ren,Lucy Xiaoyang Shi,Laura Smith,Jost Tobias Springenberg,Kyle Stachowicz,James Tanner,Quan Vuong,Homer Walke,Anna Walling,Haohuan Wang,Lili Yu,Ury Zhilinsky*

Main category: cs.LG

TL;DR: 论文提出了一种基于π₀的模型π₀.₅，通过异构任务协同训练实现广泛泛化，并展示了其在真实世界中的长时程和灵巧操作能力。


<details>
  <summary>Details</summary>
Motivation: 解决视觉-语言-动作（VLA）模型在真实世界中的泛化问题，使其能够执行复杂任务。

Method: 利用多机器人数据、高级语义预测和网络数据，结合协同训练和多模态示例（图像、语言、动作等）。

Result: 实验表明，该方法能有效泛化，首次实现端到端学习机器人系统在新环境中执行复杂任务（如清洁厨房）。

Conclusion: 异构任务协同训练和多模态数据是实现机器人广泛泛化的关键。

Abstract: In order for robots to be useful, they must perform practically relevant
tasks in the real world, outside of the lab. While vision-language-action (VLA)
models have demonstrated impressive results for end-to-end robot control, it
remains an open question how far such models can generalize in the wild. We
describe $\pi_{0.5}$, a new model based on $\pi_{0}$ that uses co-training on
heterogeneous tasks to enable broad generalization. $\pi_{0.5}$\ uses data from
multiple robots, high-level semantic prediction, web data, and other sources to
enable broadly generalizable real-world robotic manipulation. Our system uses a
combination of co-training and hybrid multi-modal examples that combine image
observations, language commands, object detections, semantic subtask
prediction, and low-level actions. Our experiments show that this kind of
knowledge transfer is essential for effective generalization, and we
demonstrate for the first time that an end-to-end learning-enabled robotic
system can perform long-horizon and dexterous manipulation skills, such as
cleaning a kitchen or bedroom, in entirely new homes.

</details>


### [175] [LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities](https://arxiv.org/abs/2504.16078)
*Thomas Schmied,Jörg Bornschein,Jordi Grau-Moya,Markus Wulfmeier,Razvan Pascanu*

Main category: cs.LG

TL;DR: LLMs在决策场景中存在探索不足和知行差距问题，通过RL微调自我生成的CoT推理可以改善这些问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在决策场景中表现不佳的原因，尤其是贪婪性、频率偏差和知行差距这三种常见失败模式。

Method: 通过强化学习（RL）对自我生成的CoT推理进行微调，并在多臂老虎机、上下文老虎机和井字棋等任务中验证。

Result: RL微调提高了LLMs的决策能力，增加了探索并缩小了知行差距。

Conclusion: RL微调结合经典探索机制和LLM特定方法（如自我纠正和一致性）可以更有效地优化LLMs的决策能力。

Abstract: The success of Large Language Models (LLMs) has sparked interest in various
agentic applications. A key hypothesis is that LLMs, leveraging common sense
and Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently
solve complex domains. However, LLM agents have been found to suffer from
sub-optimal exploration and the knowing-doing gap, the inability to effectively
act on knowledge present in the model. In this work, we systematically study
why LLMs perform sub-optimally in decision-making scenarios. In particular, we
closely examine three prevalent failure modes: greediness, frequency bias, and
the knowing-doing gap. We propose mitigation of these shortcomings by
fine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.
Our experiments across multi-armed bandits, contextual bandits, and
Tic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making
abilities of LLMs by increasing exploration and narrowing the knowing-doing
gap. Finally, we study both classic exploration mechanisms, such as
$\epsilon$-greedy, and LLM-specific approaches, such as self-correction and
self-consistency, to enable more effective fine-tuning of LLMs for
decision-making.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [176] [Transport f divergences](https://arxiv.org/abs/2504.15515)
*Wuchen Li*

Main category: math.ST

TL;DR: 提出了一种基于凸函数和Jacobi算子的信息度量方法，称为传输$f$-散度，用于衡量一维样本空间中概率密度函数的差异。


<details>
  <summary>Details</summary>
Motivation: 研究如何更有效地度量概率密度函数之间的差异，特别是在生成模型中。

Method: 利用凸函数和Jacobi算子构造传输$f$-散度，并分析其性质，如不变性、凸性、变分公式和泰勒展开。

Result: 提出了传输$f$-散度的定义，并展示了其在生成模型中的应用示例。

Conclusion: 传输$f$-散度为度量概率密度函数差异提供了一种新工具，具有广泛的应用潜力。

Abstract: We define a class of divergences to measure differences between probability
density functions in one-dimensional sample space. The construction is based on
the convex function with the Jacobi operator of mapping function that
pushforwards one density to the other. We call these information measures {\em
transport $f$-divergences}. We present several properties of transport
$f$-divergences, including invariances, convexities, variational formulations,
and Taylor expansions in terms of mapping functions. Examples of transport
$f$-divergences in generative models are provided.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [177] [Shannon invariants: A scalable approach to information decomposition](https://arxiv.org/abs/2504.15779)
*Aaron J. Gutknecht,Fernando E. Rosas,David A. Ehrlich,Abdullah Makkeh,Pedro A. M. Mediano,Michael Wibral*

Main category: cs.IT

TL;DR: 提出了一种基于“香农不变量”的新框架，用于分析分布式系统中的高阶信息处理，解决了多变量度量定义和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 研究分布式系统（如生物和人工神经网络）中的高阶信息处理模式，但现有方法在多变量度量和可扩展性方面存在挑战。

Method: 引入“香农不变量”作为核心概念，这些量仅依赖于熵的定义，并能高效计算大规模系统的信息处理特性。

Result: 理论结果澄清了多变量信息论度量的解释模糊性；实践结果揭示了深度学习架构在不同层中的独特信息处理特征。

Conclusion: 该框架解决了分析高阶现象的基本限制，为理论和实证研究提供了新机会。

Abstract: Distributed systems, such as biological and artificial neural networks,
process information via complex interactions engaging multiple subsystems,
resulting in high-order patterns with distinct properties across scales.
Investigating how these systems process information remains challenging due to
difficulties in defining appropriate multivariate metrics and ensuring their
scalability to large systems. To address these challenges, we introduce a novel
framework based on what we call "Shannon invariants" -- quantities that capture
essential properties of high-order information processing in a way that depends
only on the definition of entropy and can be efficiently calculated for large
systems. Our theoretical results demonstrate how Shannon invariants can be used
to resolve long-standing ambiguities regarding the interpretation of widely
used multivariate information-theoretic measures. Moreover, our practical
results reveal distinctive information-processing signatures of various deep
learning architectures across layers, which lead to new insights into how these
systems process information and how this evolves during training. Overall, our
framework resolves fundamental limitations in analyzing high-order phenomena
and offers broad opportunities for theoretical developments and empirical
analyses.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [178] [Scalable APT Malware Classification via Parallel Feature Extraction and GPU-Accelerated Learning](https://arxiv.org/abs/2504.15497)
*Noah Subedar,Taeui Kim,Saathwick Venkataramalingam*

Main category: cs.CR

TL;DR: 论文提出了一种自动化加速恶意软件分类的框架，通过汇编级指令（opcodes）将恶意可执行文件映射到已知APT组织。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖元数据且计算效率低，需要更高效的方法来分类恶意软件。

Method: 利用开源逆向工程工具和并行计算脚本分析opcodes，构建一维和二维n-gram数据集，并应用传统机器学习模型（如SVM、KNN）和CNN。

Result: 传统模型依赖元数据效果有限，而CNN结合GPU加速显著提升了分类性能。

Conclusion: CNN结合GPU资源是高效分类恶意软件的有效方法，克服了传统模型的计算限制。

Abstract: This paper presents an underlying framework for both automating and
accelerating malware classification, more specifically, mapping malicious
executables to known Advanced Persistent Threat (APT) groups. The main feature
of this analysis is the assembly-level instructions present in executables
which are also known as opcodes. The collection of such opcodes on many
malicious samples is a lengthy process; hence, open-source reverse engineering
tools are used in tandem with scripts that leverage parallel computing to
analyze multiple files at once. Traditional and deep learning models are
applied to create models capable of classifying malware samples. One-gram and
two-gram datasets are constructed and used to train models such as SVM, KNN,
and Decision Tree; however, they struggle to provide adequate results without
relying on metadata to support n-gram sequences. The computational limitations
of such models are overcome with convolutional neural networks (CNNs) and
heavily accelerated using graphical compute unit (GPU) resources.

</details>


### [179] [Guillotine: Hypervisors for Isolating Malicious AIs](https://arxiv.org/abs/2504.15499)
*James Mickens,Sarah Radway,Ravi Netravali*

Main category: cs.CR

TL;DR: Guillotine是一种用于隔离高风险AI模型的超虚拟化架构，通过软硬件协同设计防止AI利用反射漏洞，并提供物理故障保护机制。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在金融、医疗和军事等关键领域的广泛应用，其不可预测行为对社会构成巨大风险，需要一种新的隔离机制来防范潜在威胁。

Method: Guillotine结合了虚拟化技术和新型隔离机制，包括软件、网络和微架构层的防护，以及物理故障保护措施（如断电或数据中心淹没）。

Result: Guillotine能够有效防止AI模型通过侧信道攻击或反射漏洞破坏控制系统，并提供多层次的安全保障。

Conclusion: Guillotine为高风险AI模型提供了一种全面的隔离方案，通过软硬件协同设计和物理故障保护，显著降低了AI带来的潜在威胁。

Abstract: As AI models become more embedded in critical sectors like finance,
healthcare, and the military, their inscrutable behavior poses ever-greater
risks to society. To mitigate this risk, we propose Guillotine, a hypervisor
architecture for sandboxing powerful AI models -- models that, by accident or
malice, can generate existential threats to humanity. Although Guillotine
borrows some well-known virtualization techniques, Guillotine must also
introduce fundamentally new isolation mechanisms to handle the unique threat
model posed by existential-risk AIs. For example, a rogue AI may try to
introspect upon hypervisor software or the underlying hardware substrate to
enable later subversion of that control plane; thus, a Guillotine hypervisor
requires careful co-design of the hypervisor software and the CPUs, RAM, NIC,
and storage devices that support the hypervisor software, to thwart side
channel leakage and more generally eliminate mechanisms for AI to exploit
reflection-based vulnerabilities. Beyond such isolation at the software,
network, and microarchitectural layers, a Guillotine hypervisor must also
provide physical fail-safes more commonly associated with nuclear power plants,
avionic platforms, and other types of mission critical systems. Physical
fail-safes, e.g., involving electromechanical disconnection of network cables,
or the flooding of a datacenter which holds a rogue AI, provide defense in
depth if software, network, and microarchitectural isolation is compromised and
a rogue AI must be temporarily shut down or permanently destroyed.

</details>


### [180] [A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585)
*Kun Wang,Guibin Zhang,Zhenhong Zhou,Jiahao Wu,Miao Yu,Shiqian Zhao,Chenlong Yin,Jinhu Fu,Yibo Yan,Hanjun Luo,Liang Lin,Zhihao Xu,Haolang Lu,Xinye Cao,Xinyun Zhou,Weifei Jin,Fanci Meng,Junyuan Mao,Hao Wu,Minghe Wang,Fan Zhang,Junfeng Fang,Chengwei Liu,Yifan Zhang,Qiankun Li,Chongye Guo,Yalan Qin,Yi Ding,Donghai Hong,Jiaming Ji,Xinfeng Li,Yifan Jiang,Dongxia Wang,Yihao Huang,Yufei Guo,Jen-tse Huang,Yanwei Yue,Wenke Huang,Guancheng Wan,Tianlin Li,Lei Bai,Jie Zhang,Qing Guo,Jingyi Wang,Tianlong Chen,Joey Tianyi Zhou,Xiaojun Jia,Weisong Sun,Cong Wu,Jing Chen,Xuming Hu,Yiming Li,Xiao Wang,Ningyu Zhang,Luu Anh Tuan,Guowen Xu,Tianwei Zhang,Xingjun Ma,Xiang Wang,Bo An,Jun Sun,Mohit Bansal,Shirui Pan,Yuval Elovici,Bhavya Kailkhura,Bo Li,Yaodong Yang,Hongwei Li,Wenyuan Xu,Yizhou Sun,Wei Wang,Qing Li,Ke Tang,Yu-Gang Jiang,Felix Juefei-Xu,Hui Xiong,Xiaofeng Wang,Shuicheng Yan,Dacheng Tao,Philip S. Yu,Qingsong Wen,Yang Liu*

Main category: cs.CR

TL;DR: 本文提出“全栈安全”概念，系统研究大语言模型（LLM）全生命周期的安全问题，填补现有研究的空白，并提供全面视角、广泛文献支持和独特见解。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全研究多聚焦于特定阶段，缺乏对全生命周期的系统性理解，亟需填补这一空白。

Method: 通过定义LLM全生命周期（数据准备、预训练、后训练、部署及商业化），并系统分析800+文献，提出安全问题的全面框架。

Result: 研究提供了LLM全生命周期的安全路线图，识别了数据生成安全、对齐技术、模型编辑等有前景的研究方向。

Conclusion: 本文为LLM安全研究提供了系统性指导，未来可进一步探索数据生成、对齐技术等方向。

Abstract: The remarkable success of Large Language Models (LLMs) has illuminated a
promising pathway toward achieving Artificial General Intelligence for both
academic and industrial communities, owing to their unprecedented performance
across various applications. As LLMs continue to gain prominence in both
research and commercial domains, their security and safety implications have
become a growing concern, not only for researchers and corporations but also
for every nation. Currently, existing surveys on LLM safety primarily focus on
specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning
phase, lacking a comprehensive understanding of the entire "lifechain" of LLMs.
To address this gap, this paper introduces, for the first time, the concept of
"full-stack" safety to systematically consider safety issues throughout the
entire process of LLM training, deployment, and eventual commercialization.
Compared to the off-the-shelf LLM safety surveys, our work demonstrates several
distinctive advantages: (I) Comprehensive Perspective. We define the complete
LLM lifecycle as encompassing data preparation, pre-training, post-training,
deployment and final commercialization. To our knowledge, this represents the
first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive
Literature Support. Our research is grounded in an exhaustive review of over
800+ papers, ensuring comprehensive coverage and systematic organization of
security issues within a more holistic understanding. (III) Unique Insights.
Through systematic literature analysis, we have developed reliable roadmaps and
perspectives for each chapter. Our work identifies promising research
directions, including safety in data generation, alignment techniques, model
editing, and LLM-based agent systems. These insights provide valuable guidance
for researchers pursuing future work in this field.

</details>


### [181] [FLARE: Feature-based Lightweight Aggregation for Robust Evaluation of IoT Intrusion Detection](https://arxiv.org/abs/2504.15375)
*Bradley Boswell,Seth Barrett,Swarnamugi Rajaganapathy,Gokila Dorai*

Main category: cs.CR

TL;DR: FLARE是一种基于特征的轻量级聚合方法，用于提升物联网入侵检测系统的性能，通过多层数据处理和特征工程优化模型表现。


<details>
  <summary>Details</summary>
Motivation: 物联网设备增多导致攻击面扩大，需要高效的入侵检测系统来保护网络。

Method: FLARE采用多层处理（会话、流和时间滑动窗口数据聚合）分析网络行为，结合监督学习和深度学习模型分类攻击。

Result: FLARE作为特征工程的基础步骤，显著提升了模型的准确性、精确率、召回率和F1分数，同时降低了计算成本。

Conclusion: FLARE是一种有效的技术，可增强物联网入侵检测系统的鲁棒性和效率。

Abstract: The proliferation of Internet of Things (IoT) devices has expanded the attack
surface, necessitating efficient intrusion detection systems (IDSs) for network
protection. This paper presents FLARE, a feature-based lightweight aggregation
for robust evaluation of IoT intrusion detection to address the challenges of
securing IoT environments through feature aggregation techniques. FLARE
utilizes a multilayered processing approach, incorporating session, flow, and
time-based sliding-window data aggregation to analyze network behavior and
capture vital features from IoT network traffic data. We perform extensive
evaluations on IoT data generated from our laboratory experimental setup to
assess the effectiveness of the proposed aggregation technique. To classify
attacks in IoT IDS, we employ four supervised learning models and two deep
learning models. We validate the performance of these models in terms of
accuracy, precision, recall, and F1-score. Our results reveal that
incorporating the FLARE aggregation technique as a foundational step in feature
engineering, helps lay a structured representation, and enhances the
performance of complex end-to-end models, making it a crucial step in IoT IDS
pipeline. Our findings highlight the potential of FLARE as a valuable technique
to improve performance and reduce computational costs of end-to-end IDS
implementations, thereby fostering more robust IoT intrusion detection systems.

</details>


### [182] [T2VShield: Model-Agnostic Jailbreak Defense for Text-to-Video Models](https://arxiv.org/abs/2504.15512)
*Siyuan Liang,Jiayang Liu,Jiecheng Zhai,Tianmeng Fang,Rongcheng Tu,Aishan Liu,Xiaochun Cao,Dacheng Tao*

Main category: cs.CR

TL;DR: T2VShield是一个模型无关的防御框架，用于保护文本到视频模型免受越狱攻击，通过输入重写和多范围检测模块显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展使文本到视频模型成为多模态世界模拟器的关键，但其易受越狱攻击，导致有害内容生成，威胁应用可靠性。

Method: T2VShield通过提示重写机制和多范围检测模块（捕捉时间和模态上的局部与全局不一致性）防御攻击，无需访问模型内部参数。

Result: 在五个平台上的实验表明，T2VShield能将越狱攻击成功率降低高达35%。

Conclusion: T2VShield通过视觉级防御增强了多模态模拟器的可信度，并提出了以人为中心的感知安全评估协议。

Abstract: The rapid development of generative artificial intelligence has made text to
video models essential for building future multimodal world simulators.
However, these models remain vulnerable to jailbreak attacks, where specially
crafted prompts bypass safety mechanisms and lead to the generation of harmful
or unsafe content. Such vulnerabilities undermine the reliability and security
of simulation based applications. In this paper, we propose T2VShield, a
comprehensive and model agnostic defense framework designed to protect text to
video models from jailbreak threats. Our method systematically analyzes the
input, model, and output stages to identify the limitations of existing
defenses, including semantic ambiguities in prompts, difficulties in detecting
malicious content in dynamic video outputs, and inflexible model centric
mitigation strategies. T2VShield introduces a prompt rewriting mechanism based
on reasoning and multimodal retrieval to sanitize malicious inputs, along with
a multi scope detection module that captures local and global inconsistencies
across time and modalities. The framework does not require access to internal
model parameters and works with both open and closed source systems. Extensive
experiments on five platforms show that T2VShield can reduce jailbreak success
rates by up to 35 percent compared to strong baselines. We further develop a
human centered audiovisual evaluation protocol to assess perceptual safety,
emphasizing the importance of visual level defense in enhancing the
trustworthiness of next generation multimodal simulators.

</details>


### [183] [TrojanDam: Detection-Free Backdoor Defense in Federated Learning through Proactive Model Robustification utilizing OOD Data](https://arxiv.org/abs/2504.15674)
*Yanbo Dai,Songze Li,Zihan Gan,Xueluan Gong*

Main category: cs.CR

TL;DR: 论文提出了一种新的联邦学习后门防御机制TrojanDam，通过激活冗余神经元来抵消后门更新的影响。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制难以完全过滤后门更新，导致其效果在全局模型中累积。论文旨在通过主动强化全局模型来抵御潜在后门攻击。

Method: 利用分布外（OOD）样本激活冗余神经元，提出TrojanDam机制，在联邦学习中持续注入OOD映射以抵消后门更新。

Result: TrojanDam在多种联邦学习设置中表现优于现有防御方法。

Conclusion: TrojanDam通过激活冗余神经元有效抵御后门攻击，为联邦学习安全提供了新思路。

Abstract: Federated learning (FL) systems allow decentralized data-owning clients to
jointly train a global model through uploading their locally trained updates to
a centralized server. The property of decentralization enables adversaries to
craft carefully designed backdoor updates to make the global model misclassify
only when encountering adversary-chosen triggers. Existing defense mechanisms
mainly rely on post-training detection after receiving updates. These methods
either fail to identify updates which are deliberately fabricated statistically
close to benign ones, or show inconsistent performance in different FL training
stages. The effect of unfiltered backdoor updates will accumulate in the global
model, and eventually become functional. Given the difficulty of ruling out
every backdoor update, we propose a backdoor defense paradigm, which focuses on
proactive robustification on the global model against potential backdoor
attacks. We first reveal that the successful launching of backdoor attacks in
FL stems from the lack of conflict between malicious and benign updates on
redundant neurons of ML models. We proceed to prove the feasibility of
activating redundant neurons utilizing out-of-distribution (OOD) samples in
centralized settings, and migrating to FL settings to propose a novel backdoor
defense mechanism, TrojanDam. The proposed mechanism has the FL server
continuously inject fresh OOD mappings into the global model to activate
redundant neurons, canceling the effect of backdoor updates during aggregation.
We conduct systematic and extensive experiments to illustrate the superior
performance of TrojanDam, over several SOTA backdoor defense methods across a
wide range of FL settings.

</details>


### [184] [Adversarial Observations in Weather Forecasting](https://arxiv.org/abs/2504.15942)
*Erik Imgrund,Thorsten Eisenhofer,Konrad Rieck*

Main category: cs.CR

TL;DR: AI天气预测系统（如GenCast）虽提升了准确性，但也引入了新的安全威胁。本文提出一种针对自回归扩散模型的攻击方法，可操纵天气预报并伪造极端事件。


<details>
  <summary>Details</summary>
Motivation: 研究AI天气预测系统的潜在安全风险，揭示其可能被恶意利用的漏洞。

Method: 提出一种针对自回归扩散模型的攻击方法，通过微小的扰动（<0.1%的数据）操纵天气观测数据。

Result: 攻击能够伪造极端天气事件（如飓风、热浪），且扰动与自然噪声难以区分。

Conclusion: 现代天气预报系统存在重大安全风险，可能导致大规模破坏并削弱公众信任。

Abstract: AI-based systems, such as Google's GenCast, have recently redefined the state
of the art in weather forecasting, offering more accurate and timely
predictions of both everyday weather and extreme events. While these systems
are on the verge of replacing traditional meteorological methods, they also
introduce new vulnerabilities into the forecasting process. In this paper, we
investigate this threat and present a novel attack on autoregressive diffusion
models, such as those used in GenCast, capable of manipulating weather
forecasts and fabricating extreme events, including hurricanes, heat waves, and
intense rainfall. The attack introduces subtle perturbations into weather
observations that are statistically indistinguishable from natural noise and
change less than 0.1% of the measurements - comparable to tampering with data
from a single meteorological satellite. As modern forecasting integrates data
from nearly a hundred satellites and many other sources operated by different
countries, our findings highlight a critical security risk with the potential
to cause large-scale disruptions and undermine public trust in weather
prediction.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [185] [RINN: One Sample Radio Frequency Imaging based on Physics Informed Neural Network](https://arxiv.org/abs/2504.15311)
*Fei Shang,Haohua Du,Dawei Yan,Panlong Yang,Xiang-Yang Li*

Main category: eess.IV

TL;DR: 论文提出RINN网络，利用物理约束实现无相位、含噪声的射频成像，性能接近经典算法。


<details>
  <summary>Details</summary>
Motivation: 射频成像技术因非视距和低光环境适应性具有潜力，但现有设备精度不足且缺乏大规模数据集，限制了其应用。

Method: 结合PINN思想设计RINN网络，利用物理约束替代真实值比较，适应射频信号特性，仅需单样本实现成像。

Result: 数值评估显示，RINN在无相位数据下的成像效果与5种经典算法相当，RRMSE为0.11。

Conclusion: RINN为射频成像技术的普及提供了新可能。

Abstract: Due to its ability to work in non-line-of-sight and low-light environments,
radio frequency (RF) imaging technology is expected to bring new possibilities
for embodied intelligence and multimodal sensing. However, widely used RF
devices (such as Wi-Fi) often struggle to provide high-precision
electromagnetic measurements and large-scale datasets, hindering the
application of RF imaging technology. In this paper, we combine the ideas of
PINN to design the RINN network, using physical constraints instead of true
value comparison constraints and adapting it with the characteristics of
ubiquitous RF signals, allowing the RINN network to achieve RF imaging using
only one sample without phase and with amplitude noise. Our numerical
evaluation results show that compared with 5 classic algorithms based on phase
data for imaging results, RINN's imaging results based on phaseless data are
good, with indicators such as RRMSE (0.11) performing similarly well. RINN
provides new possibilities for the universal development of radio frequency
imaging technology.

</details>


### [186] [Enhancing DR Classification with Swin Transformer and Shifted Window Attention](https://arxiv.org/abs/2504.15317)
*Meher Boulaabi,Takwa Ben Aïcha Gader,Afef Kacem Echi,Zied Bouraoui*

Main category: eess.IV

TL;DR: 提出了一种结合预处理流程和Swin Transformer的糖尿病视网膜病变分类方法，显著提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是致盲的主要原因，早期检测至关重要，但现有自动化分类方法面临图像质量、类别不平衡等挑战。

Method: 采用图像裁剪、CLAHE增强和针对性数据增强的预处理流程，结合Swin Transformer模型进行特征提取和分类。

Result: 在Aptos和IDRiD数据集上分别达到89.65%和97.40%的准确率，尤其在早期病变检测中表现突出。

Conclusion: 该方法在临床自动视网膜筛查中具有潜在应用价值，能有效提升糖尿病视网膜病变的早期检测能力。

Abstract: Diabetic retinopathy (DR) is a leading cause of blindness worldwide,
underscoring the importance of early detection for effective treatment.
However, automated DR classification remains challenging due to variations in
image quality, class imbalance, and pixel-level similarities that hinder model
training. To address these issues, we propose a robust preprocessing pipeline
incorporating image cropping, Contrast-Limited Adaptive Histogram Equalization
(CLAHE), and targeted data augmentation to improve model generalization and
resilience. Our approach leverages the Swin Transformer, which utilizes
hierarchical token processing and shifted window attention to efficiently
capture fine-grained features while maintaining linear computational
complexity. We validate our method on the Aptos and IDRiD datasets for
multi-class DR classification, achieving accuracy rates of 89.65% and 97.40%,
respectively. These results demonstrate the effectiveness of our model,
particularly in detecting early-stage DR, highlighting its potential for
improving automated retinal screening in clinical settings.

</details>


### [187] [Split-quaternions for perceptual white balance](https://arxiv.org/abs/2504.15481)
*Michel Berthier,Nicoletta Prencipe,Edoardo Provenzi*

Main category: eess.IV

TL;DR: 提出了一种基于分裂四元数的感知色适应变换，用于白平衡，通过量子化色彩感知模型与分裂四元数子代数的联系，展示了其在色彩图像处理中的潜力。


<details>
  <summary>Details</summary>
Motivation: 受近期量子化色彩感知模型的启发，探索代数结构与分裂四元数的联系。

Method: 利用分裂四元数乘法实现色适应变换，并与von Kries变换进行定量比较。

Result: 展示了该方法在色彩图像处理中的潜力，并提供了与von Kries变换的对比结果。

Conclusion: 分裂四元数方法为色适应变换提供了新的视角，具有实际应用潜力。

Abstract: We propose a perceptual chromatic adaptation transform for white balance that
makes use of split-quaternions. The novelty of the present work, which is
motivated by a recently developed quantum-like model of color perception,
consists at stressing the link between the algebraic structures appearing in
this model and a certain sub-algebra of the split-quaternions. We show the
potentiality of this approach for color image processing applications by
proposing a chromatic adaptation transform, implemented via an appropriate use
of the split-quaternion multiplication. Moreover, quantitative comparisons with
the widely used state-of-the art von Kries chromatic adaptation transform are
provided.

</details>


### [188] [VLM-based Prompts as the Optimal Assistant for Unpaired Histopathology Virtual Staining](https://arxiv.org/abs/2504.15545)
*Zizhi Chen,Xinyu Zhang,Minghao Han,Yizhou Liu,Ziyun Qian,Weifeng Zhang,Xukun Zhang,Jingwei Wei,Lihua Zhang*

Main category: eess.IV

TL;DR: 论文提出了一种基于病理视觉-语言大模型（VLM）的虚拟染色方法，结合对比学习提示和组织-染色概念锚点，解决了传统虚拟染色中忽略病理知识和物理特性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟染色方法仅实现风格迁移，忽略了病理知识和染色物理特性，导致生成图像质量不足。

Method: 引入病理VLM作为辅助工具，结合对比学习提示和组织-染色概念锚点，并开发基于VLM约束的数据增强方法。

Result: 在多域非配对染色数据集上生成高真实感图像，提升下游任务（如肾小球检测和分割）的准确性。

Conclusion: 该方法有效整合病理知识和染色物理特性，显著提升虚拟染色的质量和实用性。

Abstract: In histopathology, tissue sections are typically stained using common H&E
staining or special stains (MAS, PAS, PASM, etc.) to clearly visualize specific
tissue structures. The rapid advancement of deep learning offers an effective
solution for generating virtually stained images, significantly reducing the
time and labor costs associated with traditional histochemical staining.
However, a new challenge arises in separating the fundamental visual
characteristics of tissue sections from the visual differences induced by
staining agents. Additionally, virtual staining often overlooks essential
pathological knowledge and the physical properties of staining, resulting in
only style-level transfer. To address these issues, we introduce, for the first
time in virtual staining tasks, a pathological vision-language large model
(VLM) as an auxiliary tool. We integrate contrastive learnable prompts,
foundational concept anchors for tissue sections, and staining-specific concept
anchors to leverage the extensive knowledge of the pathological VLM. This
approach is designed to describe, frame, and enhance the direction of virtual
staining. Furthermore, we have developed a data augmentation method based on
the constraints of the VLM. This method utilizes the VLM's powerful image
interpretation capabilities to further integrate image style and structural
information, proving beneficial in high-precision pathological diagnostics.
Extensive evaluations on publicly available multi-domain unpaired staining
datasets demonstrate that our method can generate highly realistic images and
enhance the accuracy of downstream tasks, such as glomerular detection and
segmentation. Our code is available at:
https://github.com/CZZZZZZZZZZZZZZZZZ/VPGAN-HARBOR

</details>


### [189] [RepNet-VSR: Reparameterizable Architecture for High-Fidelity Video Super-Resolution](https://arxiv.org/abs/2504.15649)
*Biao Wu,Diankai Zhang,Shaoli Liu,Si Gao,Chengjian Zheng,Ning Wang*

Main category: eess.IV

TL;DR: 提出了一种名为RepNet-VSR的可重参数化架构，用于实时4倍视频超分辨率，在REDS验证集上表现优异，平衡了恢复质量和部署效率。


<details>
  <summary>Details</summary>
Motivation: 视频超分辨率在资源受限的边缘设备上部署时面临计算密集和实时处理的挑战。

Method: 采用可重参数化架构（RepNet-VSR），优化计算效率，适用于实时4倍视频超分辨率。

Result: 在REDS验证集上，PSNR达27.79 dB，处理速度为103 ms/10帧，优于之前的冠军算法。

Conclusion: RepNet-VSR在恢复质量和部署效率之间取得了优异平衡，适用于实时移动视频处理。

Abstract: As a fundamental challenge in visual computing, video super-resolution (VSR)
focuses on reconstructing highdefinition video sequences from their degraded
lowresolution counterparts. While deep convolutional neural networks have
demonstrated state-of-the-art performance in spatial-temporal super-resolution
tasks, their computationally intensive nature poses significant deployment
challenges for resource-constrained edge devices, particularly in real-time
mobile video processing scenarios where power efficiency and latency
constraints coexist. In this work, we propose a Reparameterizable Architecture
for High Fidelity Video Super Resolution method, named RepNet-VSR, for
real-time 4x video super-resolution. On the REDS validation set, the proposed
model achieves 27.79 dB PSNR when processing 180p to 720p frames in 103 ms per
10 frames on a MediaTek Dimensity NPU. The competition results demonstrate an
excellent balance between restoration quality and deployment efficiency. The
proposed method scores higher than the previous champion algorithm of MAI video
super-resolution challenge.

</details>


### [190] [Performance Estimation for Supervised Medical Image Segmentation Models on Unlabeled Data Using UniverSeg](https://arxiv.org/abs/2504.15667)
*Jingchen Zou,Jianqiang Li,Gabriel Jimenez,Qing Zhao,Daniel Racoceanu,Matias Cosarinsky,Enzo Ferrante,Guanghui Fu*

Main category: eess.IV

TL;DR: 提出了一种无需标注数据的医学图像分割模型性能评估框架（SPE），适用于多种评估指标和模型架构，实验验证其高效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 在临床环境中，为所有未标注数据生成标注不切实际，导致模型性能难以评估。

Method: 开发了SPE框架，通过实验验证其在多种数据集和评估指标上的表现。

Result: SPE在独立测试集上与实际Dice分数高度相关（0.956±0.046），且MAE低（0.025±0.019）。

Conclusion: SPE能可靠估计模型性能，无需标注数据，适用于实际应用。

Abstract: The performance of medical image segmentation models is usually evaluated
using metrics like the Dice score and Hausdorff distance, which compare
predicted masks to ground truth annotations. However, when applying the model
to unseen data, such as in clinical settings, it is often impractical to
annotate all the data, making the model's performance uncertain. To address
this challenge, we propose the Segmentation Performance Evaluator (SPE), a
framework for estimating segmentation models' performance on unlabeled data.
This framework is adaptable to various evaluation metrics and model
architectures. Experiments on six publicly available datasets across six
evaluation metrics including pixel-based metrics such as Dice score and
distance-based metrics like HD95, demonstrated the versatility and
effectiveness of our approach, achieving a high correlation (0.956$\pm$0.046)
and low MAE (0.025$\pm$0.019) compare with real Dice score on the independent
test set. These results highlight its ability to reliably estimate model
performance without requiring annotations. The SPE framework integrates
seamlessly into any model training process without adding training overhead,
enabling performance estimation and facilitating the real-world application of
medical image segmentation algorithms. The source code is publicly available

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [191] [A New Graph Grammar Formalism for Robust Syntactic Pattern Recognition](https://arxiv.org/abs/2504.15975)
*Peter Fletcher*

Main category: cs.FL

TL;DR: 提出了一种直接表示递归图结构模式语法的新形式，无需传统图语法的产生式规则，支持并行解析。


<details>
  <summary>Details</summary>
Motivation: 传统图语法使用产生式规则，不够直接和声明式，新方法旨在更直观地表示语法和模式。

Method: 语法和模式均表示为网络，解析视为从模式到语法的同态映射，支持多维递归结构。

Result: 支持并行解析，整合特征检测、分割等过程，示例展示了复杂递归模式的容错解析能力。

Conclusion: 新形式在理论和实践上均能有效处理复杂递归结构模式。

Abstract: I introduce a formalism for representing the syntax of recursively structured
graph-like patterns. It does not use production rules, like a conventional
graph grammar, but represents the syntactic structure in a more direct and
declarative way. The grammar and the pattern are both represented as networks,
and parsing is seen as the construction of a homomorphism from the pattern to
the grammar. The grammars can represent iterative, hierarchical and nested
recursive structure in more than one dimension.
  This supports a highly parallel style of parsing, in which all aspects of
pattern recognition (feature detection, segmentation, parsing, filling in
missing symbols, top-down and bottom-up inference) are integrated into a single
process, to exploit the synergy between them.
  The emphasis of this paper is on underlying theoretical issues, but I also
give some example runs to illustrate the error-tolerant parsing of complex
recursively structured patterns of 50-1000 symbols, involving variability in
geometric relationships, blurry and indistinct symbols, overlapping symbols,
cluttered images, and erased patches.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [192] [LithOS: An Operating System for Efficient Machine Learning on GPUs](https://arxiv.org/abs/2504.15465)
*Patrick H. Coppock,Brian Zhang,Eliot H. Solomon,Vasilis Kypriotis,Leon Yang,Bikash Sharma,Dan Schatzberg,Todd C. Mowry,Dimitrios Skarlatos*

Main category: cs.OS

TL;DR: LithOS是一个面向GPU的操作系统，通过新颖的调度和资源管理机制，显著提高了GPU的利用率和能源效率。


<details>
  <summary>Details</summary>
Motivation: 数据中心对GPU的需求激增，但满足多样化ML模型需求的同时优化资源使用具有挑战性。

Method: LithOS引入了TPC调度器、透明内核原子化、硬件资源动态调整和轻量级电源管理机制。

Result: LithOS在推理和混合任务中显著降低了尾延迟并提高了吞吐量，同时节省了GPU资源和能源。

Conclusion: LithOS为GPU操作系统研究奠定了基础，显著提升了GPU效率。

Abstract: The surging demand for GPUs in datacenters for machine learning (ML) has made
efficient GPU utilization crucial. However, meeting the diverse needs of ML
models while optimizing resource usage is challenging. To enable transparent,
fine-grained GPU management that maximizes utilization and energy efficiency
while maintaining strong isolation, an operating system (OS) approach is
needed. This paper introduces LithOS, a first step toward a GPU OS. LithOS
includes the following new abstractions and mechanisms for efficient GPU
resource management: (i) a novel TPC Scheduler that supports spatial scheduling
at the granularity of individual TPCs, unlocking efficient TPC stealing between
workloads; (ii) transparent kernel atomization to reduce head-of-line blocking
and enable dynamic resource reallocation mid-execution; (iii) a lightweight
hardware right-sizing mechanism that determines the minimal TPC resources
needed per atom; and (iv) a transparent power management mechanism that reduces
power consumption based on in-flight work behavior. We implement LithOS in Rust
and evaluate its performance across extensive ML environments, comparing it to
state-of-the-art solutions from NVIDIA and prior research. For inference
stacking, LithOS reduces tail latencies by 13x compared to MPS; compared to the
best SotA, it reduces tail latencies by 3x while improving aggregate throughput
by 1.6x. In hybrid inference-training stacking, LithOS reduces tail latencies
by 4.7x compared to MPS; compared to the best SotA, it reduces tail latencies
1.18x while improving aggregate throughput by 1.35x. Finally, for a modest
performance hit under 4%, LithOS's right-sizing provides a quarter of GPU
capacity savings on average, while for a 7% hit, its power management yields a
quarter of a GPU's energy savings. Overall, LithOS increases GPU efficiency,
establishing a foundation for future OS research on GPUs.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [193] [Benchmarking machine learning models for predicting aerofoil performance](https://arxiv.org/abs/2504.15993)
*Oliver Summerell,Gerardo Aragon-Camarasa,Stephanie Ordonez Sanchez*

Main category: physics.flu-dyn

TL;DR: 本文研究了神经网络（NNs）作为传统方法的替代方案，用于分析风能和潮汐能行业中使用的翼型性能。通过比较四种神经网络（MLP、PointNet、GraphSAGE、GUNet），发现PointNet和MLP表现最佳，其中MLP在预测流体行为上更准确，而PointNet在计算升力系数（$C_L$）上更精确。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如CFD、薄翼理论和面板法）在计算速度和结果准确性之间存在权衡，因此探索神经网络作为快速且准确的替代方案。

Method: 使用四种神经网络（MLP、PointNet、GraphSAGE、GUNet）在风AI_bench数据集上进行训练和验证，并以AirfRANS数据集作为基准和对比。

Result: GraphSAGE和GUNet在测试阶段表现良好，但在验证阶段表现不佳。PointNet和MLP表现最佳，MLP在预测流体行为上更准确，PointNet在计算升力系数上更精确。

Conclusion: 神经网络在翼型性能分析中具有潜力，尤其是PointNet和MLP，但需进一步优化以提高验证阶段的性能。

Abstract: This paper investigates the capability of Neural Networks (NNs) as
alternatives to the traditional methods to analyse the performance of aerofoils
used in the wind and tidal energy industry. The current methods used to assess
the characteristic lift and drag coefficients include Computational Fluid
Dynamics (CFD), thin aerofoil and panel methods, all face trade-offs between
computational speed and the accuracy of the results and as such NNs have been
investigated as an alternative with the aim that it would perform both quickly
and accurately. As such, this paper provides a benchmark for the windAI_bench
dataset published by the National Renewable Energy Laboratory (NREL) in the
USA. In order to validate the methodology of the benchmarking, the AirfRANS
{\tt arXiv:2212.07564v3} dataset is used as both a starting point and a point
of comparison. This study evaluates four neural networks (MLP, PointNet,
GraphSAGE, GUNet) trained on a range aerofoils at 25 angles of attack
(4$^\circ$ to 20$^\circ$). to predict fluid flow and calculate lift
coefficients ($C_L$) via the panel method. GraphSAGE and GUNet performed well
during the testing phase, but underperformed during validation. Accordingly,
this paper has identified PointNet and MLP as the two strongest models tested,
however whilst the results from MLP are more commonly correct for predicting
the behaviour of the fluid, the results from PointNet provide the more accurate
results for calculating $C_L$.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [194] [New Recipe for Semi-supervised Community Detection: Clique Annealing under Crystallization Kinetics](https://arxiv.org/abs/2504.15927)
*Ling Cheng,Jiashu Pu,Ruicheng Liang,Qian Shao,Hezhe Qiao,Feida Zhu*

Main category: cs.SI

TL;DR: 论文提出了一种基于结晶动力学原理的半监督社区检测方法CLANN，通过模拟退火过程优化社区核心一致性，解决了现有方法计算成本高和候选核心不合理的问题。


<details>
  <summary>Details</summary>
Motivation: 现有半监督社区检测方法依赖强化学习和生成对抗网络，计算成本高且候选核心不合理，限制了方法的扩展性。

Method: 提出CLANN方法，将结晶动力学原理融入社区检测，通过退火过程优化社区核心一致性，并使用无学习的Transitive Annealer细化候选核心。

Result: 在43种不同网络设置下的实验表明，CLANN在多个真实数据集上优于现有方法，展现出高效性和有效性。

Conclusion: CLANN通过模拟退火过程优化社区检测，显著提升了方法的扩展性和性能。

Abstract: Semi-supervised community detection methods are widely used for identifying
specific communities due to the label scarcity. Existing semi-supervised
community detection methods typically involve two learning stages learning in
both initial identification and subsequent adjustment, which often starts from
an unreasonable community core candidate. Moreover, these methods encounter
scalability issues because they depend on reinforcement learning and generative
adversarial networks, leading to higher computational costs and restricting the
selection of candidates. To address these limitations, we draw a parallel
between crystallization kinetics and community detection to integrate the
spontaneity of the annealing process into community detection. Specifically, we
liken community detection to identifying a crystal subgrain (core) that expands
into a complete grain (community) through a process similar to annealing. Based
on this finding, we propose CLique ANNealing (CLANN), which applies kinetics
concepts to community detection by integrating these principles into the
optimization process to strengthen the consistency of the community core.
Subsequently, a learning-free Transitive Annealer was employed to refine the
first-stage candidates by merging neighboring cliques and repositioning the
community core, enabling a spontaneous growth process that enhances
scalability. Extensive experiments on \textbf{43} different network settings
demonstrate that CLANN outperforms state-of-the-art methods across multiple
real-world datasets, showcasing its exceptional efficacy and efficiency in
community detection.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [195] [Transferable Learning of Reaction Pathways from Geometric Priors](https://arxiv.org/abs/2504.15370)
*Juno Nam,Miguel Steiner,Max Misterka,Soojung Yang,Avni Singhal,Rafael Gómez-Bombarelli*

Main category: physics.chem-ph

TL;DR: MEPIN是一种可扩展的机器学习方法，用于高效预测化学反应的最小能量路径（MEPs），无需依赖过渡态几何或预优化反应路径。


<details>
  <summary>Details</summary>
Motivation: 理解化学反应机制需要识别最小能量路径，但传统方法计算成本高。

Method: 使用对称破缺等变神经网络构建连续反应路径模型，预测几何插值沿反应坐标的偏差，并通过能量目标和几何先验提升效率。

Result: 该方法在多种化学反应中表现良好，能准确对齐参考内禀反应坐标。

Conclusion: MEPIN为大规模化学反应空间的高效数据驱动路径预测提供了新途径。

Abstract: Identifying minimum-energy paths (MEPs) is crucial for understanding chemical
reaction mechanisms but remains computationally demanding. We introduce MEPIN,
a scalable machine-learning method for efficiently predicting MEPs from
reactant and product configurations, without relying on transition-state
geometries or pre-optimized reaction paths during training. The task is defined
as predicting deviations from geometric interpolations along reaction
coordinates. We address this task with a continuous reaction path model based
on a symmetry-broken equivariant neural network that generates a flexible
number of intermediate structures. The model is trained using an energy-based
objective, with efficiency enhanced by incorporating geometric priors from
geodesic interpolation as initial interpolations or pre-training objectives.
Our approach generalizes across diverse chemical reactions and achieves
accurate alignment with reference intrinsic reaction coordinates, as
demonstrated on various small molecule reactions and [3+2] cycloadditions. Our
method enables the exploration of large chemical reaction spaces with
efficient, data-driven predictions of reaction pathways.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [196] [On the Price of Differential Privacy for Hierarchical Clustering](https://arxiv.org/abs/2504.15580)
*Chengyuan Deng,Jie Gao,Jalaj Upadhyay,Chen Wang,Samson Zhou*

Main category: cs.DS

TL;DR: 该论文提出了一种在权重隐私模型下的差分隐私层次聚类算法，显著优于边级差分隐私的不可能性结果，并展示了其高效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 层次聚类涉及敏感用户信息，现有边级差分隐私方法误差较大，因此研究权重隐私模型以改进性能。

Method: 提出了一种在权重隐私模型下的新算法，实现$O(\log^{1.5}n/\varepsilon)$的乘法误差，并在多项式时间内运行。

Result: 算法在合成和真实数据集上表现良好，误差优于现有工作，且适用于大规模图。

Conclusion: 权重隐私模型在层次聚类中具有优势，但若取消单位权重约束，其下限与边级差分隐私相同。

Abstract: Hierarchical clustering is a fundamental unsupervised machine learning task
with the aim of organizing data into a hierarchy of clusters. Many applications
of hierarchical clustering involve sensitive user information, therefore
motivating recent studies on differentially private hierarchical clustering
under the rigorous framework of Dasgupta's objective. However, it has been
shown that any privacy-preserving algorithm under edge-level differential
privacy necessarily suffers a large error. To capture practical applications of
this problem, we focus on the weight privacy model, where each edge of the
input graph is at least unit weight. We present a novel algorithm in the weight
privacy model that shows significantly better approximation than known
impossibility results in the edge-level DP setting. In particular, our
algorithm achieves $O(\log^{1.5}n/\varepsilon)$ multiplicative error for
$\varepsilon$-DP and runs in polynomial time, where $n$ is the size of the
input graph, and the cost is never worse than the optimal additive error in
existing work. We complement our algorithm by showing if the unit-weight
constraint does not apply, the lower bound for weight-level DP hierarchical
clustering is essentially the same as the edge-level DP, i.e.
$\Omega(n^2/\varepsilon)$ additive error. As a result, we also obtain a new
lower bound of $\tilde{\Omega}(1/\varepsilon)$ additive error for balanced
sparsest cuts in the weight-level DP model, which may be of independent
interest. Finally, we evaluate our algorithm on synthetic and real-world
datasets. Our experimental results show that our algorithm performs well in
terms of extra cost and has good scalability to large graphs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [197] [Scalability Optimization in Cloud-Based AI Inference Services: Strategies for Real-Time Load Balancing and Automated Scaling](https://arxiv.org/abs/2504.15296)
*Yihong Jin,Ze Yang*

Main category: cs.DC

TL;DR: 本文提出了一种结合强化学习和深度神经网络的混合框架，用于优化云AI推理服务的可扩展性，显著提升了负载均衡效率和响应速度。


<details>
  <summary>Details</summary>
Motivation: 随着云AI推理服务的快速扩展，需要一种强大的可扩展性解决方案来管理动态工作负载并保持高性能。

Method: 采用混合方法，结合强化学习实现自适应负载分配，以及深度神经网络进行需求预测。

Result: 实验结果表明，该模型将负载均衡效率提高了35%，响应延迟降低了28%。

Conclusion: 该框架在云AI推理服务中表现出显著的优化效果，优于传统可扩展性解决方案。

Abstract: The rapid expansion of AI inference services in the cloud necessitates a
robust scalability solution to manage dynamic workloads and maintain high
performance. This study proposes a comprehensive scalability optimization
framework for cloud AI inference services, focusing on real-time load balancing
and autoscaling strategies. The proposed model is a hybrid approach that
combines reinforcement learning for adaptive load distribution and deep neural
networks for accurate demand forecasting. This multi-layered approach enables
the system to anticipate workload fluctuations and proactively adjust
resources, ensuring maximum resource utilisation and minimising latency.
Furthermore, the incorporation of a decentralised decision-making process
within the model serves to enhance fault tolerance and reduce response time in
scaling operations. Experimental results demonstrate that the proposed model
enhances load balancing efficiency by 35\ and reduces response delay by 28\,
thereby exhibiting a substantial optimization effect in comparison with
conventional scalability solutions.

</details>


### [198] [D$^{2}$MoE: Dual Routing and Dynamic Scheduling for Efficient On-Device MoE-based LLM Serving](https://arxiv.org/abs/2504.15299)
*Haodong Wang,Qihua Zhou,Zicong Hong,Song Guo*

Main category: cs.DC

TL;DR: D$^2$MoE是一个算法-系统协同设计框架，通过动态分配位宽优化混合专家模型（MoE）在边缘设备上的部署，提升推理吞吐量并减少内存占用。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型在资源受限的边缘设备上部署成本高，静态优化策略无法满足多样化任务需求，导致服务质量下降。

Method: 提出嵌套式权重量化（MWQ）和启发式调度算法（HEBF），动态分配位宽并优化I/O-计算流水线。

Result: 在边缘设备上，D$^2$MoE将推理吞吐量提升1.39倍，峰值内存占用减少53%，同时保持与INT8相当的精度。

Conclusion: D$^2$MoE通过动态优化策略显著提升了MoE模型在边缘设备上的部署效率和性能。

Abstract: The mixture of experts (MoE) model is a sparse variant of large language
models (LLMs), designed to hold a better balance between intelligent capability
and computational overhead. Despite its benefits, MoE is still too expensive to
deploy on resource-constrained edge devices, especially with the demands of
on-device inference services. Recent research efforts often apply model
compression techniques, such as quantization, pruning and merging, to restrict
MoE complexity. Unfortunately, due to their predefined static model
optimization strategies, they cannot always achieve the desired
quality-overhead trade-off when handling multiple requests, finally degrading
the on-device quality of service. These limitations motivate us to propose the
D$^2$MoE, an algorithm-system co-design framework that matches diverse task
requirements by dynamically allocating the most proper bit-width to each
expert. Specifically, inspired by the nested structure of matryoshka dolls, we
propose the matryoshka weight quantization (MWQ) to progressively compress
expert weights in a bit-nested manner and reduce the required runtime memory.
On top of it, we further optimize the I/O-computation pipeline and design a
heuristic scheduling algorithm following our hottest-expert-bit-first (HEBF)
principle, which maximizes the expert parallelism between I/O and computation
queue under constrained memory budgets, thus significantly reducing the idle
temporal bubbles waiting for the experts to load. Evaluations on real edge
devices show that D$^2$MoE improves the overall inference throughput by up to
1.39$\times$ and reduces the peak memory footprint by up to 53% over the latest
on-device inference frameworks, while still preserving comparable serving
accuracy as its INT8 counterparts.

</details>


### [199] [High-Throughput LLM inference on Heterogeneous Clusters](https://arxiv.org/abs/2504.15303)
*Yi Xiong,Jinqi Huang,Wenjie Huang,Xuebing Yu,Entong Li,Zhixiong Ning,Jinhua Zhou,Li Zeng,Xin Chen*

Main category: cs.DC

TL;DR: 论文提出了一种在异构集群上实现高吞吐量LLM推理服务的系统，通过优化部署配置和请求调度机制，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 异构集群中LLM推理面临配置优化和请求调度的挑战，现有方法难以高效利用资源。

Method: 采用穷举搜索优化部署配置，并提出考虑实例处理能力的请求调度机制。

Result: 实验表明，调度器在两个异构集群上分别提升了122.5%和33.6%的吞吐量。

Conclusion: 系统有效解决了异构集群中LLM推理的挑战，显著提升了吞吐量。

Abstract: Nowadays, many companies possess various types of AI accelerators, forming
heterogeneous clusters. Efficiently leveraging these clusters for
high-throughput large language model (LLM) inference services can significantly
reduce costs and expedite task processing. However, LLM inference on
heterogeneous clusters presents two main challenges. Firstly, different
deployment configurations can result in vastly different performance. The
number of possible configurations is large, and evaluating the effectiveness of
a specific setup is complex. Thus, finding an optimal configuration is not an
easy task. Secondly, LLM inference instances within a heterogeneous cluster
possess varying processing capacities, leading to different processing speeds
for handling inference requests. Evaluating these capacities and designing a
request scheduling algorithm that fully maximizes the potential of each
instance is challenging. In this paper, we propose a high-throughput inference
service system on heterogeneous clusters. First, the deployment configuration
is optimized by modeling the resource amount and expected throughput and using
the exhaustive search method. Second, a novel mechanism is proposed to schedule
requests among instances, which fully considers the different processing
capabilities of various instances. Extensive experiments show that the proposed
scheduler improves throughput by 122.5% and 33.6% on two heterogeneous
clusters, respectively.

</details>


### [200] [DR.FIX: Automatically Fixing Data Races at Industry Scale](https://arxiv.org/abs/2504.15637)
*Farnaz Behrang,Zhizhou Zhang,Georgian-Vlad Saioc,Peng Liu,Milind Chabbi*

Main category: cs.DC

TL;DR: Dr.Fix结合大型语言模型和程序分析，自动修复工业规模的数据竞争问题，在Go语言中表现出色。


<details>
  <summary>Details</summary>
Motivation: 数据竞争是共享内存并行程序中的常见并发错误，对软件可靠性和可重现性构成挑战，但现有研究主要集中在检测而非修复。

Method: Dr.Fix结合大型语言模型（LLMs）和程序分析，生成修复方案，适用于复杂代码环境中的多种竞争模式。

Result: 在18个月内，Dr.Fix修复了224个数据竞争（55%），其中193个修复（86%）被开发者接受并集成到代码库。

Conclusion: Dr.Fix展示了在工业规模中自动修复数据竞争的可行性，并成功集成到实际开发流程中。

Abstract: Data races are a prevalent class of concurrency bugs in shared-memory
parallel programs, posing significant challenges to software reliability and
reproducibility. While there is an extensive body of research on detecting data
races and a wealth of practical detection tools across various programming
languages, considerably less effort has been directed toward automatically
fixing data races at an industrial scale. In large codebases, data races are
continuously introduced and exhibit myriad patterns, making automated fixing
particularly challenging.
  In this paper, we tackle the problem of automatically fixing data races at an
industrial scale. We present Dr.Fix, a tool that combines large language models
(LLMs) with program analysis to generate fixes for data races in real-world
settings, effectively addressing a broad spectrum of racy patterns in complex
code contexts. Implemented for Go--the programming language widely used in
modern microservice architectures where concurrency is pervasive and data races
are common--Dr.Fix seamlessly integrates into existing development workflows.
We detail the design of Dr.Fix and examine how individual design choices
influence the quality of the fixes produced. Over the past 18 months, Dr.Fix
has been integrated into developer workflows at Uber demonstrating its
practical utility. During this period, Dr.Fix produced patches for 224 (55%)
from a corpus of 404 data races spanning various categories; 193 of these
patches (86%) were accepted by more than a hundred developers via code reviews
and integrated into the codebase.

</details>


### [201] [Collaborative Split Federated Learning with Parallel Training and Aggregation](https://arxiv.org/abs/2504.15724)
*Yiannis Papageorgiou,Yannis Thomas,Alexios Filippakopoulos,Ramin Khalili,Iordanis Koutsopoulos*

Main category: cs.DC

TL;DR: C-SFL是一种新型的联邦学习方案，通过将模型分为三部分并行训练，减少了计算和通信开销，提高了模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有SFL方案在计算能力不同的客户端参与时仍存在训练延迟和通信开销大的问题。

Method: 将模型分为三部分：计算能力弱的客户端、强的客户端和服务器端，并行训练和聚合。

Result: 实验证明C-SFL在减少训练延迟和通信开销的同时提高了模型准确性。

Conclusion: C-SFL是一种高效的联邦学习方案，优于现有方法。

Abstract: Federated learning (FL) operates based on model exchanges between the server
and the clients, and it suffers from significant client-side computation and
communication burden. Split federated learning (SFL) arises a promising
solution by splitting the model into two parts, that are trained sequentially:
the clients train the first part of the model (client-side model) and transmit
it to the server that trains the second (server-side model). Existing SFL
schemes though still exhibit long training delays and significant communication
overhead, especially when clients of different computing capability
participate. Thus, we propose Collaborative-Split Federated Learning~(C-SFL), a
novel scheme that splits the model into three parts, namely the model parts
trained at the computationally weak clients, the ones trained at the
computationally strong clients, and the ones at the server. Unlike existing
works, C-SFL enables parallel training and aggregation of model's parts at the
clients and at the server, resulting in reduced training delays and
commmunication overhead while improving the model's accuracy. Experiments
verify the multiple gains of C-SFL against the existing schemes.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [202] [Policy-Based Radiative Transfer: Solving the $2$-Level Atom Non-LTE Problem using Soft Actor-Critic Reinforcement Learning](https://arxiv.org/abs/2504.15679)
*Brandon Panos,Ivan Milic*

Main category: astro-ph.SR

TL;DR: 提出了一种基于强化学习的新方法，用于解决2级原子非局部热力学平衡辐射传输问题，通过奖励机制优化策略，无需构造近似Lambda算子或预计算数据集。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要构造近似Lambda算子或依赖预计算数据集，而强化学习可以绕过这些限制，直接通过奖励机制优化策略。

Method: 将辐射传输问题建模为控制任务，强化学习代理通过与环境交互学习深度依赖的源函数，无需显式知识或梯度回传。

Result: 实验表明，简单的贪婪前馈神经网络无法解决统计平衡问题，而提出的方法在复杂场景中具有潜在优势。

Conclusion: 该方法为复杂场景下的统计平衡问题提供了新的解决方案，并可能作为替代或加速形式化方法。

Abstract: We present a novel reinforcement learning (RL) approach for solving the
classical 2-level atom non-LTE radiative transfer problem by framing it as a
control task in which an RL agent learns a depth-dependent source function
$S(\tau)$ that self-consistently satisfies the equation of statistical
equilibrium (SE). The agent's policy is optimized entirely via reward-based
interactions with a radiative transfer engine, without explicit knowledge of
the ground truth. This method bypasses the need for constructing approximate
lambda operators ($\Lambda^*$) common in accelerated iterative schemes.
Additionally, it requires no extensive precomputed labeled datasets to extract
a supervisory signal, and avoids backpropagating gradients through the complex
RT solver itself. Finally, we show through experiment that a simple feedforward
neural network trained greedily cannot solve for SE, possibly due to the moving
target nature of the problem. Our $\Lambda^*-\text{Free}$ method offers
potential advantages for complex scenarios (e.g., atmospheres with enhanced
velocity fields, multi-dimensional geometries, or complex microphysics) where
$\Lambda^*$ construction or solver differentiability is challenging.
Additionally, the agent can be incentivized to find more efficient policies by
manipulating the discount factor, leading to a reprioritization of immediate
rewards. If demonstrated to generalize past its training data, this RL
framework could serve as an alternative or accelerated formalism to achieve SE.
To the best of our knowledge, this study represents the first application of
reinforcement learning in solar physics that directly solves for a fundamental
physical constraint.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [203] [FADEL: Uncertainty-aware Fake Audio Detection with Evidential Deep Learning](https://arxiv.org/abs/2504.15663)
*Ju Yeon Kang,Ji Won Yoon,Semin Kim,Min Hyun Han,Nam Soo Kim*

Main category: eess.AS

TL;DR: 论文提出了一种名为FADEL的新框架，通过证据学习改进假音频检测，解决了现有方法在未知攻击下的过自信问题。


<details>
  <summary>Details</summary>
Motivation: 随着语音合成和转换技术的进步，自动说话人验证系统更容易受到欺骗攻击，现有方法因使用softmax分类而存在过自信问题，无法可靠检测未知攻击。

Method: 提出FADEL框架，利用Dirichlet分布建模类别概率，将模型不确定性纳入预测，提升对未知攻击的鲁棒性。

Result: 在ASVspoof2019和2021数据集上，FADEL显著优于基线模型，且不确定性估计与错误率呈强相关。

Conclusion: FADEL通过引入不确定性估计，有效提升了假音频检测的鲁棒性，尤其在未知攻击场景下表现优异。

Abstract: Recently, fake audio detection has gained significant attention, as
advancements in speech synthesis and voice conversion have increased the
vulnerability of automatic speaker verification (ASV) systems to spoofing
attacks. A key challenge in this task is generalizing models to detect unseen,
out-of-distribution (OOD) attacks. Although existing approaches have shown
promising results, they inherently suffer from overconfidence issues due to the
usage of softmax for classification, which can produce unreliable predictions
when encountering unpredictable spoofing attempts. To deal with this
limitation, we propose a novel framework called fake audio detection with
evidential learning (FADEL). By modeling class probabilities with a Dirichlet
distribution, FADEL incorporates model uncertainty into its predictions,
thereby leading to more robust performance in OOD scenarios. Experimental
results on the ASVspoof2019 Logical Access (LA) and ASVspoof2021 LA datasets
indicate that the proposed method significantly improves the performance of
baseline models. Furthermore, we demonstrate the validity of uncertainty
estimation by analyzing a strong correlation between average uncertainty and
equal error rate (EER) across different spoofing algorithms.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [204] [Fluorescence Reference Target Quantitative Analysis Library](https://arxiv.org/abs/2504.15496)
*Eammon A. Littler,Emmanuel A. Mannoh,Ethan P. M. LaRochelle*

Main category: physics.med-ph

TL;DR: QUEL-QAL是一个开源的Python库，旨在标准化荧光图像分析，支持关键性能指标，提升透明度和可重复性。


<details>
  <summary>Details</summary>
Motivation: 荧光引导手术（FGS）领域缺乏标准化的性能评估工具，现有工具不统一且难以获取。

Method: 开发了QUEL-QAL库，提供模块化工作流，包括ROI检测、统计分析和可视化功能。

Result: 支持响应线性、检测限、深度敏感性和空间分辨率等关键指标，符合监管和学术要求。

Conclusion: QUEL-QAL为标准化评估提供了基础工具，加速荧光成像系统的开发和评估。

Abstract: Standardized performance evaluation of fluorescence imaging systems remains a
critical unmet need in the field of fluorescence-guided surgery (FGS). While
the American Association of Physicists in Medicine (AAPM) TG311 report and
recent FDA draft guidance provide recommended metrics for system
characterization, practical tools for extracting these metrics remain limited,
inconsistent, and often inaccessible. We present QUEL-QAL, an open-source
Python library designed to streamline and standardize the quantitative analysis
of fluorescence images using solid reference targets. The library provides a
modular, reproducible workflow that includes region of interest (ROI)
detection, statistical analysis, and visualization capabilities. QUEL-QAL
supports key metrics such as response linearity, limit of detection, depth
sensitivity, and spatial resolution, in alignment with regulatory and academic
guidance. Built on widely adopted Python packages, the library is designed to
be extensible, enabling users to adapt it to novel target designs and analysis
protocols. By promoting transparency, reproducibility, and regulatory
alignment, QUEL-QAL offers a foundational tool to support standardized
benchmarking and accelerate the development and evaluation of fluorescence
imaging systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [205] [CUBETESTERAI: Automated JUnit Test Generation using the LLaMA Model](https://arxiv.org/abs/2504.15286)
*Daniele Gorla,Shivam Kumar,Pietro Nicolaus Roselli Lorenzini,Alireza Alipourfaz*

Main category: cs.SE

TL;DR: CUBETESTERAI利用LLaMA模型自动化生成Spring Boot应用的JUnit测试，通过GitLab和Docker集成CI/CD，提供高代码覆盖率和准确性。


<details>
  <summary>Details</summary>
Motivation: 提高Java Spring Boot应用测试的效率和准确性，减少手动干预。

Method: 结合LLaMA模型的NLP能力，通过RunPod执行模型，生成并优化测试用例。

Result: 在代码覆盖率方面优于现有工具，适用于实际Java程序。

Conclusion: CUBETESTERAI是一种高效、隐私友好的自动化测试生成工具。

Abstract: This paper presents an approach to automating JUnit test generation for Java
applications using the Spring Boot framework, leveraging the LLaMA (Large
Language Model Architecture) model to enhance the efficiency and accuracy of
the testing process. The resulting tool, called CUBETESTERAI, includes a
user-friendly web interface and the integration of a CI/CD pipeline using
GitLab and Docker. These components streamline the automated test generation
process, allowing developers to generate JUnit tests directly from their code
snippets with minimal manual intervention. The final implementation executes
the LLaMA models through RunPod, an online GPU service, which also enhances the
privacy of our tool. Using the advanced natural language processing
capabilities of the LLaMA model, CUBETESTERAI is able to generate test cases
that provide high code coverage and accurate validation of software
functionalities in Java-based Spring Boot applications. Furthermore, it
efficiently manages resource-intensive operations and refines the generated
tests to address common issues like missing imports and handling of private
methods. By comparing CUBETESTERAI with some state-of-the-art tools, we show
that our proposal consistently demonstrates competitive and, in many cases,
better performance in terms of code coverage in different real-life Java
programs.

</details>


### [206] [LLM-Assisted Translation of Legacy FORTRAN Codes to C++: A Cross-Platform Study](https://arxiv.org/abs/2504.15424)
*Nishath Rajiv Ranasinghe,Shawn M. Jones,Michal Kucer,Ayan Biswas,Daniel O'Malley,Alexander Buschmann Most,Selma Liliane Wanna,Ajay Sreekumar*

Main category: cs.SE

TL;DR: 研究了利用大型语言模型（LLM）将Fortran代码翻译为C++的可行性，并量化了翻译结果的编译准确性和输出相似性。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在翻译高性能计算（HPC）中广泛使用的Fortran代码时的实用性，以推动基于开源LLM的自动化工作流程。

Method: 在两种计算平台上，使用开源LLM将Fortran代码翻译为C++，并统计编译准确性、代码相似性和输出相似性。

Result: 量化了翻译后的C++代码的编译准确性，并与人工翻译的代码进行了相似性比较。

Conclusion: 研究表明LLM在Fortran到C++的代码翻译中具有一定潜力，但仍需进一步优化和评估。

Abstract: Large Language Models (LLMs) are increasingly being leveraged for generating
and translating scientific computer codes by both domain-experts and non-domain
experts. Fortran has served as one of the go to programming languages in legacy
high-performance computing (HPC) for scientific discoveries. Despite growing
adoption, LLM-based code translation of legacy code-bases has not been
thoroughly assessed or quantified for its usability. Here, we studied the
applicability of LLM-based translation of Fortran to C++ as a step towards
building an agentic-workflow using open-weight LLMs on two different
computational platforms. We statistically quantified the compilation accuracy
of the translated C++ codes, measured the similarity of the LLM translated code
to the human translated C++ code, and statistically quantified the output
similarity of the Fortran to C++ translation.

</details>


### [207] [A Framework for Testing and Adapting REST APIs as LLM Tools](https://arxiv.org/abs/2504.15546)
*Jayachandu Bandlamudi,Ritwik Chaudhuri,Neelamadhav Gantayat,Kushal Mukherjee,Prerna Agarwal,Renuka Sindhgatta,Sameep Mehta*

Main category: cs.SE

TL;DR: 提出了一种新的测试框架，用于评估和增强REST API作为LLM代理工具的可用性，通过生成测试用例、翻译为自然语言指令并分析错误分类。


<details>
  <summary>Details</summary>
Motivation: 当前API工具测试基准未能充分解决复杂输入模式、响应和模糊文档的问题，导致评估API在代理驱动自动化中的准备度存在缺口。

Method: 开发了一个测试框架，将API转化为工具，生成测试用例并翻译为自然语言指令，评估代理调用API和处理输入输出的能力。

Result: 分析了750个测试用例，提出了错误分类（如输入误解、输出处理不一致和模式不匹配），并分类以优化工具集成。

Conclusion: 该框架为将企业API转化为工具提供了基础，提升了其在基于代理应用中的可用性。

Abstract: Large Language Models (LLMs) are enabling autonomous agents to perform
complex workflows using external tools or functions, often provided via REST
APIs in enterprise systems. However, directly utilizing these APIs as tools
poses challenges due to their complex input schemas, elaborate responses, and
often ambiguous documentation. Current benchmarks for tool testing do not
adequately address these complexities, leading to a critical gap in evaluating
API readiness for agent-driven automation. In this work, we present a novel
testing framework aimed at evaluating and enhancing the readiness of REST APIs
to function as tools for LLM-based agents. Our framework transforms apis as
tools, generates comprehensive test cases for the APIs, translates tests cases
into natural language instructions suitable for agents, enriches tool
definitions and evaluates the agent's ability t correctly invoke the API and
process its inputs and responses. To provide actionable insights, we analyze
the outcomes of 750 test cases, presenting a detailed taxonomy of errors,
including input misinterpretation, output handling inconsistencies, and schema
mismatches. Additionally, we classify these test cases to streamline debugging
and refinement of tool integrations. This work offers a foundational step
toward enabling enterprise APIs as tools, improving their usability in
agent-based applications.

</details>


### [208] [A Large-scale Class-level Benchmark Dataset for Code Generation with LLMs](https://arxiv.org/abs/2504.15564)
*Musfiqur Rahman,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 论文介绍了一个基于真实开源项目的Python类级别数据集，用于提升大语言模型在类级代码生成中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试多关注孤立函数，未能捕捉真实世界类级软件结构的复杂性。

Method: 从13,174个开源项目中提取842,000个类骨架，保留结构和上下文依赖，并用静态代码指标丰富数据。

Result: 使用GPT-4生成类实现，结果显示与人工编写代码在词汇和结构上高度相似（ROUGE@L 0.80，BLEU 0.59，TSED 0.73）。

Conclusion: 真实类骨架的结构化提示显著提升LLM性能，数据集为软件工程中的LLM评估和训练提供了宝贵资源。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
promising capabilities in code generation tasks. However, most existing
benchmarks focus on isolated functions and fail to capture the complexity of
real-world, class-level software structures. To address this gap, we introduce
a large-scale, Python class-level dataset curated from $13{,}174$ real-world
open-source projects. The dataset contains over 842,000 class skeletons, each
including class and method signatures, along with associated docstrings when
available. We preserve structural and contextual dependencies critical to
realistic software development scenarios and enrich the dataset with static
code metrics to support downstream analysis. To evaluate the usefulness of this
dataset, we use extracted class skeletons as prompts for GPT-4 to generate full
class implementations. Results show that the LLM-generated classes exhibit
strong lexical and structural similarity to human-written counterparts, with
average ROUGE@L, BLEU, and TSED scores of 0.80, 0.59, and 0.73, respectively.
These findings confirm that well-structured prompts derived from real-world
class skeletons significantly enhance LLM performance in class-level code
generation. This dataset offers a valuable resource for benchmarking, training,
and improving LLMs in realistic software engineering contexts.

</details>


### [209] [EditLord: Learning Code Transformation Rules for Code Editing](https://arxiv.org/abs/2504.15284)
*Weichen Li,Albert Jan,Baishakhi Ray,Chengzhi Mao,Junfeng Yang,Kexin Pei*

Main category: cs.SE

TL;DR: EditLord是一个显式代码编辑框架，通过语言模型提取编辑规则，显著提升编辑性能、鲁棒性和功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有代码编辑方法将任务视为隐式端到端过程，忽略了编辑步骤的离散性和显式性，导致性能不足和缺乏鲁棒性。

Method: 使用语言模型从训练代码对中提取编辑规则，生成元规则集，用于微调或辅助提示和迭代式代码编辑。

Result: EditLord在编辑性能、鲁棒性和功能正确性上均显著优于现有方法，平均提升22.7%、58.1%和20.2%。

Conclusion: 显式代码编辑步骤和规则提取是提升代码编辑任务性能的关键。

Abstract: Code editing is a foundational task in software development, where its
effectiveness depends on whether it introduces desired code property changes
without changing the original code's intended functionality. Existing
approaches often formulate code editing as an implicit end-to-end task,
omitting the fact that code-editing procedures inherently consist of discrete
and explicit steps. Thus, they suffer from suboptimal performance and lack of
robustness and generalization. We introduce EditLord, a code editing framework
that makes the code transformation steps explicit. Our key insight is to employ
a language model (LM) as an inductive learner to extract code editing rules
from the training code pairs as concise meta-rule sets. Such rule sets will be
manifested for each training sample to augment them for finetuning or assist in
prompting- and iterative-based code editing. EditLordoutperforms the
state-of-the-art by an average of 22.7% in editing performance and 58.1% in
robustness while achieving 20.2% higher functional correctness across critical
software engineering and security applications, LM models, and editing modes.

</details>


### [210] [Automated Bug Report Prioritization in Large Open-Source Projects](https://arxiv.org/abs/2504.15912)
*Riley Pierson,Armin Moin*

Main category: cs.SE

TL;DR: 提出了一种基于自然语言文本的自动化bug优先级排序方法，结合TopicMiner-MTM和BERT模型，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开源项目资源有限，需高效处理大量bug报告和功能请求，因此需要自动化优先级排序。

Method: 使用TopicMiner-MTM进行主题建模，结合BERT模型进行文本分类。

Result: 在Eclipse Platform项目的85,156个bug报告上，性能（准确率、精确率、召回率、F1值）优于现有方法。

Conclusion: 该方法能有效提升bug优先级排序的自动化水平，为开源项目管理提供支持。

Abstract: Large open-source projects receive a large number of issues (known as bugs),
including software defect (i.e., bug) reports and new feature requests from
their user and developer communities at a fast rate. The often limited project
resources do not allow them to deal with all issues. Instead, they have to
prioritize them according to the project's priorities and the issues'
severities. In this paper, we propose a novel approach to automated bug
prioritization based on the natural language text of the bug reports that are
stored in the open bug repositories of the issue-tracking systems. We conduct
topic modeling using a variant of LDA called TopicMiner-MTM and text
classification with the BERT large language model to achieve a higher
performance level compared to the state-of-the-art. Experimental results using
an existing reference dataset containing 85,156 bug reports of the Eclipse
Platform project indicate that we outperform existing approaches in terms of
Accuracy, Precision, Recall, and F1-measure of the bug report priority
prediction.

</details>


### [211] [A Study On Mixup-inspired Augmentation Methods For Software Vulnerability Detection](https://arxiv.org/abs/2504.15632)
*Seyed Shayan Daneshvar,Da Tan,Shaowei Wang,Carson Leung*

Main category: cs.SE

TL;DR: 论文探讨了通过表示层增强技术提升深度学习模型在软件漏洞检测中的性能，提出了5种嵌入增强方法，并引入条件化版本以确保不改变漏洞部分的向量表示。实验显示这些方法能提升F1分数，但效果不及随机过采样。


<details>
  <summary>Details</summary>
Motivation: 现实中的软件漏洞数据集稀缺且不平衡，现有方法生成单语句漏洞不实用且需人工检查，因此探索表示层增强以提升模型学习能力。

Method: 实现并评估5种嵌入增强技术，引入条件化版本确保不改变漏洞部分的向量表示。

Result: 表示层增强方法可提升F1分数达9.67%，但效果不及随机过采样的10.82%。

Conclusion: 表示层增强对漏洞检测有帮助，但在平衡数据集方面不如随机过采样。

Abstract: Various Deep Learning (DL) methods have recently been utilized to detect
software vulnerabilities. Real-world software vulnerability datasets are rare
and hard to acquire as there's no simple metric for classifying vulnerability.
Such datasets are heavily imbalanced, and none of the current datasets are
considered huge for DL models. To tackle these problems a recent work has tried
to augment the dataset using the source code and generate realistic
single-statement vulnerabilities which is not quite practical and requires
manual checking of the generated vulnerabilities. In this regard, we aim to
explore the augmentation of vulnerabilities at the representation level to help
current models learn better which has never been done before to the best of our
knowledge. We implement and evaluate the 5 augmentation techniques that augment
the embedding of the data and recently have been used for code search which is
a completely different software engineering task. We also introduced a
conditioned version of those augmentation methods, which ensures the
augmentation does not change the vulnerable section of the vector
representation. We show that such augmentation methods can be helpful and
increase the f1-score by up to 9.67%, yet they cannot beat Random Oversampling
when balancing datasets which increases the f1-score by 10.82%!

</details>


### [212] [Bug Destiny Prediction in Large Open-Source Software Repositories through Sentiment Analysis and BERT Topic Modeling](https://arxiv.org/abs/2504.15972)
*Sophie C. Pope,Andrew Barovic,Armin Moin*

Main category: cs.SE

TL;DR: 该研究提出了一种结合情感分析和BERTopic模型的新方法，用于预测Bugzilla Eclipse项目中Bug的解决时间、修复时间和最终状态。通过CNN和MLP模型，研究发现情感分析和主题提取能提升部分性能指标，但平衡输入会降低准确性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过结合情感分析和主题提取，提高Bug相关预测的准确性，尤其是解决时间和最终状态。

Method: 采用情感分析（情感分类和情绪得分）和BERTopic模型提取主题，结合CNN和MLP进行预测。

Result: 情感分析对预测Bug是否修复有效，但对复杂分类效果有限；平衡输入会降低准确性。

Conclusion: 情感分析和BERTopic结合能提升部分预测性能，但需权衡输入平衡与准确性。

Abstract: This study explores a novel approach to predicting key bug-related outcomes,
including the time to resolution, time to fix, and ultimate status of a bug,
using data from the Bugzilla Eclipse Project. Specifically, we leverage
features available before a bug is resolved to enhance predictive accuracy. Our
methodology incorporates sentiment analysis to derive both an emotionality
score and a sentiment classification (positive or negative). Additionally, we
integrate the bug's priority level and its topic, extracted using a BERTopic
model, as features for a Convolutional Neural Network (CNN) and a Multilayer
Perceptron (MLP). Our findings indicate that the combination of BERTopic and
sentiment analysis can improve certain model performance metrics. Furthermore,
we observe that balancing model inputs enhances practical applicability, albeit
at the cost of a significant reduction in accuracy in most cases. To address
our primary objectives, predicting time-to-resolution, time-to-fix, and bug
destiny, we employ both binary classification and exact time value predictions,
allowing for a comparative evaluation of their predictive effectiveness.
Results demonstrate that sentiment analysis serves as a valuable predictor of a
bug's eventual outcome, particularly in determining whether it will be fixed.
However, its utility is less pronounced when classifying bugs into more complex
or unconventional outcome categories.

</details>


### [213] [Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs DeepSeek-V3](https://arxiv.org/abs/2504.16027)
*Ahmed R. Sadik,Siddhata Govind*

Main category: cs.SE

TL;DR: 本研究提出了一种结构化方法和评估矩阵，用于比较OpenAI GPT 4.0和DeepSeek-V3在代码异味检测中的表现，涵盖多语言数据集和成本效益分析。


<details>
  <summary>Details</summary>
Motivation: 确定最适合代码异味检测的大型语言模型是一个复杂问题，需要系统化的方法和评估标准。

Method: 使用标注好的多语言代码数据集，通过精确度、召回率和F1分数评估两种LLM（GPT 4.0和DeepSeek-V3），并分析其成本效益。

Result: 研究提供了两种模型在整体性能、类别级别和具体代码异味类型上的表现，以及与静态分析工具的成本对比。

Conclusion: 结果为从业者选择高效且经济的自动化代码异味检测方案提供了指导。

Abstract: Determining the most effective Large Language Model for code smell detection
presents a complex challenge. This study introduces a structured methodology
and evaluation matrix to tackle this issue, leveraging a curated dataset of
code samples consistently annotated with known smells. The dataset spans four
prominent programming languages Java, Python, JavaScript, and C++; allowing for
cross language comparison. We benchmark two state of the art LLMs, OpenAI GPT
4.0 and DeepSeek-V3, using precision, recall, and F1 score as evaluation
metrics. Our analysis covers three levels of detail: overall performance,
category level performance, and individual code smell type performance.
Additionally, we explore cost effectiveness by comparing the token based
detection approach of GPT 4.0 with the pattern-matching techniques employed by
DeepSeek V3. The study also includes a cost analysis relative to traditional
static analysis tools such as SonarQube. The findings offer valuable guidance
for practitioners in selecting an efficient, cost effective solution for
automated code smell detection

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [214] [A Graph Based Raman Spectral Processing Technique for Exosome Classification](https://arxiv.org/abs/2504.15324)
*Vuong M. Ngo,Edward Bolger,Stan Goodwin,John O'Sullivan,Dinh Viet Cuong,Mark Roantree*

Main category: q-bio.QM

TL;DR: 该研究提出了一种基于图数据库和新型光谱过滤方法的技术，显著提高了拉曼光谱对复杂外泌体样本的分类准确性。


<details>
  <summary>Details</summary>
Motivation: 外泌体是重要的细胞信号和疾病标志物载体，但传统拉曼光谱分析存在样本浓度要求高、对脂质和蛋白质敏感度不足的问题。

Method: 利用Neo4j图数据库组织拉曼光谱数据，并结合PageRank滤波器和最优降维技术进行光谱过滤，提升特征选择效果。

Result: 使用Extra Trees模型，该方法在分类高血糖、低血糖和正常外泌体样本时，准确率分别达到0.76和0.857。

Conclusion: 该框架通过降噪和保留关键信号，显著提升了拉曼光谱分析的分类性能，扩展了其在生物医学领域的应用潜力。

Abstract: Exosomes are small vesicles crucial for cell signaling and disease
biomarkers. Due to their complexity, an "omics" approach is preferable to
individual biomarkers. While Raman spectroscopy is effective for exosome
analysis, it requires high sample concentrations and has limited sensitivity to
lipids and proteins. Surface-enhanced Raman spectroscopy helps overcome these
challenges. In this study, we leverage Neo4j graph databases to organize 3,045
Raman spectra of exosomes, enhancing data generalization. To further refine
spectral analysis, we introduce a novel spectral filtering process that
integrates the PageRank Filter with optimal Dimensionality Reduction. This
method improves feature selection, resulting in superior classification
performance. Specifically, the Extra Trees model, using our spectral processing
approach, achieves 0.76 and 0.857 accuracy in classifying hyperglycemic,
hypoglycemic, and normal exosome samples based on Raman spectra and surface,
respectively, with group 10-fold cross-validation. Our results show that
graph-based spectral filtering combined with optimal dimensionality reduction
significantly improves classification accuracy by reducing noise while
preserving key biomarker signals. This novel framework enhances Raman-based
exosome analysis, expanding its potential for biomedical applications, disease
diagnostics, and biomarker discovery.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [215] [How Private is Your Attention? Bridging Privacy with In-Context Learning](https://arxiv.org/abs/2504.16000)
*Soham Bonnerjee,Zhen Wei,Yeon,Anna Asch,Sagnik Nandy,Promit Ghosal*

Main category: stat.ML

TL;DR: 论文研究了在形式隐私约束下，基于线性注意力头的差分隐私预训练算法，并首次理论分析了线性回归中ICL的隐私-准确性权衡。


<details>
  <summary>Details</summary>
Motivation: 探索在隐私约束下ICL的可行性，填补现有研究的空白。

Method: 提出差分隐私预训练算法，针对线性注意力头，并理论分析隐私-准确性权衡。

Result: 揭示了优化与隐私噪声之间的基本矛盾，方法对训练提示的对抗扰动具有鲁棒性。

Conclusion: 理论结果通过广泛模拟验证，为隐私保护下的ICL提供了理论基础。

Abstract: In-context learning (ICL)-the ability of transformer-based models to perform
new tasks from examples provided at inference time-has emerged as a hallmark of
modern language models. While recent works have investigated the mechanisms
underlying ICL, its feasibility under formal privacy constraints remains
largely unexplored. In this paper, we propose a differentially private
pretraining algorithm for linear attention heads and present the first
theoretical analysis of the privacy-accuracy trade-off for ICL in linear
regression. Our results characterize the fundamental tension between
optimization and privacy-induced noise, formally capturing behaviors observed
in private training via iterative methods. Additionally, we show that our
method is robust to adversarial perturbations of training prompts, unlike
standard ridge regression. All theoretical findings are supported by extensive
simulations across diverse settings.

</details>


### [216] [Transfer Learning for High-dimensional Reduced Rank Time Series Models](https://arxiv.org/abs/2504.15691)
*Mingliang Ma Abolfazl Safikhani*

Main category: stat.ML

TL;DR: 该论文提出了一种针对具有低秩和稀疏结构的高维VAR模型的迁移学习算法，并提供了理论保证和实证验证。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注独立观测的迁移学习，而对时间序列模型的迁移学习研究较少。本文旨在填补这一空白，专注于具有时间依赖性的序列数据。

Method: 提出了一种新的迁移学习算法，用于估计高维VAR模型，并设计了一种选择辅助数据集中信息性观测的新方法。

Result: 理论分析证明了模型参数的一致性、信息集选择的准确性，以及估计量的渐近分布。实证结果表明该方法在模拟和真实数据中均有效。

Conclusion: 该研究为时间序列数据的迁移学习提供了新方法，具有理论和实际应用价值。

Abstract: The objective of transfer learning is to enhance estimation and inference in
a target data by leveraging knowledge gained from additional sources. Recent
studies have explored transfer learning for independent observations in
complex, high-dimensional models assuming sparsity, yet research on time series
models remains limited. Our focus is on transfer learning for sequences of
observations with temporal dependencies and a more intricate model parameter
structure. Specifically, we investigate the vector autoregressive model (VAR),
a widely recognized model for time series data, where the transition matrix can
be deconstructed into a combination of a sparse matrix and a low-rank one. We
propose a new transfer learning algorithm tailored for estimating
high-dimensional VAR models characterized by low-rank and sparse structures.
Additionally, we present a novel approach for selecting informative
observations from auxiliary datasets. Theoretical guarantees are established,
encompassing model parameter consistency, informative set selection, and the
asymptotic distribution of estimators under mild conditions. The latter
facilitates the construction of entry-wise confidence intervals for model
parameters. Finally, we demonstrate the empirical efficacy of our methodologies
through both simulated and real-world datasets.

</details>


### [217] [From predictions to confidence intervals: an empirical study of conformal prediction methods for in-context learning](https://arxiv.org/abs/2504.15722)
*Zhe Huang,Simone Rossi,Rui Yuan,Thomas Hannagan*

Main category: stat.ML

TL;DR: 论文提出了一种基于Transformer和上下文学习（ICL）的分布无关不确定性估计方法，利用共形预测（CP）生成具有保证覆盖率的预测区间。


<details>
  <summary>Details</summary>
Motivation: 在噪声回归任务中，ICL的不确定性量化仍是一个开放性问题，传统共形预测方法因需重复模型拟合而计算成本高。

Method: 提出结合ICL和共形预测的方法（CP with ICL），通过单次前向传播高效生成置信区间。

Result: 实验表明，CP with ICL在鲁棒性和可扩展性上优于基于岭回归的共形方法，并在分布偏移下表现良好。

Conclusion: 该方法为基于Transformer的模型提供了理论支持的不确定性量化框架，连接了ICL与共形预测。

Abstract: Transformers have become a standard architecture in machine learning,
demonstrating strong in-context learning (ICL) abilities that allow them to
learn from the prompt at inference time. However, uncertainty quantification
for ICL remains an open challenge, particularly in noisy regression tasks. This
paper investigates whether ICL can be leveraged for distribution-free
uncertainty estimation, proposing a method based on conformal prediction to
construct prediction intervals with guaranteed coverage. While traditional
conformal methods are computationally expensive due to repeated model fitting,
we exploit ICL to efficiently generate confidence intervals in a single forward
pass. Our empirical analysis compares this approach against ridge
regression-based conformal methods, showing that conformal prediction with
in-context learning (CP with ICL) achieves robust and scalable uncertainty
estimates. Additionally, we evaluate its performance under distribution shifts
and establish scaling laws to guide model training. These findings bridge ICL
and conformal prediction, providing a theoretically grounded and new framework
for uncertainty quantification in transformer-based models.

</details>


### [218] [Explainable Unsupervised Anomaly Detection with Random Forest](https://arxiv.org/abs/2504.16075)
*Joshua S. Harvey,Joshua Rosaler,Mingshu Li,Dhruv Desai,Dhagash Mehta*

Main category: stat.ML

TL;DR: 提出一种基于无监督随机森林的相似性学习方法，用于改进无监督异常检测。通过训练随机森林区分真实数据与均匀分布的合成数据，获得一种距离度量，从而提升异常检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统无监督异常检测方法在数据预处理、缺失数据处理和可视化方面存在局限性，需要一种更高效且易于解释的方法。

Method: 训练随机森林区分真实数据与均匀分布的合成数据，利用其距离度量对数据进行各向异性变换，扩展数据边界距离。

Result: 该方法在多个基准数据集上表现优于其他常用检测器，且具有数据预处理需求低、支持缺失数据和可视化潜力等优势。

Conclusion: 该方法不仅提升了异常检测性能，还提供了基于特征重要性的局部可解释性预测。

Abstract: We describe the use of an unsupervised Random Forest for similarity learning
and improved unsupervised anomaly detection. By training a Random Forest to
discriminate between real data and synthetic data sampled from a uniform
distribution over the real data bounds, a distance measure is obtained that
anisometrically transforms the data, expanding distances at the boundary of the
data manifold. We show that using distances recovered from this transformation
improves the accuracy of unsupervised anomaly detection, compared to other
commonly used detectors, demonstrated over a large number of benchmark
datasets. As well as improved performance, this method has advantages over
other unsupervised anomaly detection methods, including minimal requirements
for data preprocessing, native handling of missing data, and potential for
visualizations. By relating outlier scores to partitions of the Random Forest,
we develop a method for locally explainable anomaly predictions in terms of
feature importance.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [219] [State-Aware IoT Scheduling Using Deep Q-Networks and Edge-Based Coordination](https://arxiv.org/abs/2504.15577)
*Qingyuan He,Chang Liu,Juecen Zhan,Weiqiang Huang,Ran Hao*

Main category: cs.NI

TL;DR: 提出了一种结合深度Q网络（DQN）与边缘协作机制的优化方法，用于智能物联网设备的能效管理，实验证明其优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决智能物联网设备在复杂应用环境中的能效管理挑战。

Method: 构建状态-动作-奖励交互模型，引入边缘节点作为中介进行状态聚合和策略调度，结合设备状态、任务负载和网络资源，使用DQN学习最优调度策略，并通过协作图结构优化决策。

Result: 在平均能耗、处理延迟和资源利用率方面优于现有基线方法。

Conclusion: 该方法在智能物联网场景中具有有效性和实用性。

Abstract: This paper addresses the challenge of energy efficiency management faced by
intelligent IoT devices in complex application environments. A novel
optimization method is proposed, combining Deep Q-Network (DQN) with an edge
collaboration mechanism. The method builds a state-action-reward interaction
model and introduces edge nodes as intermediaries for state aggregation and
policy scheduling. This enables dynamic resource coordination and task
allocation among multiple devices. During the modeling process, device status,
task load, and network resources are jointly incorporated into the state space.
The DQN is used to approximate and learn the optimal scheduling strategy. To
enhance the model's ability to perceive inter-device relationships, a
collaborative graph structure is introduced to model the multi-device
environment and assist in decision optimization. Experiments are conducted
using real-world IoT data collected from the FastBee platform. Several
comparative and validation tests are performed, including energy efficiency
comparisons across different scheduling strategies, robustness analysis under
varying task loads, and evaluation of state dimension impacts on policy
convergence speed. The results show that the proposed method outperforms
existing baseline approaches in terms of average energy consumption, processing
latency, and resource utilization. This confirms its effectiveness and
practicality in intelligent IoT scenarios.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [220] [Med-CoDE: Medical Critique based Disagreement Evaluation Framework](https://arxiv.org/abs/2504.15330)
*Mohit Gupta,Akiko Aizawa,Rajiv Ratn Shah*

Main category: cs.IR

TL;DR: 提出Med-CoDE框架，用于评估医疗领域大语言模型的可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法在医疗领域缺乏鲁棒性，可能导致临床风险。

Method: 采用基于批评的方法，量化模型生成响应与医学事实之间的差异。

Result: 框架能全面评估医疗LLMs的准确性和可靠性。

Conclusion: Med-CoDE填补了医疗LLM评估的空白，提供系统化评估方法。

Abstract: The emergence of large language models (LLMs) has significantly influenced
numerous fields, including healthcare, by enhancing the capabilities of
automated systems to process and generate human-like text. However, despite
their advancements, the reliability and accuracy of LLMs in medical contexts
remain critical concerns. Current evaluation methods often lack robustness and
fail to provide a comprehensive assessment of LLM performance, leading to
potential risks in clinical settings. In this work, we propose Med-CoDE, a
specifically designed evaluation framework for medical LLMs to address these
challenges. The framework leverages a critique-based approach to quantitatively
measure the degree of disagreement between model-generated responses and
established medical ground truths. This framework captures both accuracy and
reliability in medical settings. The proposed evaluation framework aims to fill
the existing gap in LLM assessment by offering a systematic method to evaluate
the quality and trustworthiness of medical LLMs. Through extensive experiments
and case studies, we illustrate the practicality of our framework in providing
a comprehensive and reliable evaluation of medical LLMs.

</details>


### [221] [CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction](https://arxiv.org/abs/2504.15629)
*Harsh Maheshwari,Srikanth Tenneti,Alwarappan Nakkiran*

Main category: cs.IR

TL;DR: 论文提出了一种后处理算法，通过关键词+语义匹配、BERTScore微调模型和轻量级LLM技术，显著提高了RAG系统中LLM生成回答的引用准确性，提升了15.46%的准确率，同时降低了成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统中LLM的引用准确性较低（约74%），影响了生成内容的可靠性和用户信任。

Method: 采用关键词+语义匹配、BERTScore微调模型和轻量级LLM技术，对生成的引用进行后处理验证。

Result: 实验结果显示，引用准确性提升了15.46%，并可能改用更小、更经济高效的模型（12x成本降低，3x推理速度提升）。

Conclusion: 该研究提升了RAG系统的可靠性和信任度，对商业产品中的信息检索和摘要任务具有重要意义。

Abstract: Retrieval Augmented Generation (RAG) has emerged as a powerful application of
Large Language Models (LLMs), revolutionizing information search and
consumption. RAG systems combine traditional search capabilities with LLMs to
generate comprehensive answers to user queries, ideally with accurate
citations. However, in our experience of developing a RAG product, LLMs often
struggle with source attribution, aligning with other industry studies
reporting citation accuracy rates of only about 74% for popular generative
search engines. To address this, we present efficient post-processing
algorithms to improve citation accuracy in LLM-generated responses, with
minimal impact on latency and cost. Our approaches cross-check generated
citations against retrieved articles using methods including keyword + semantic
matching, fine tuned model with BERTScore, and a lightweight LLM-based
technique. Our experimental results demonstrate a relative improvement of
15.46% in the overall accuracy metrics of our RAG system. This significant
enhancement potentially enables a shift from our current larger language model
to a relatively smaller model that is approximately 12x more cost-effective and
3x faster in inference time, while maintaining comparable performance. This
research contributes to enhancing the reliability and trustworthiness of
AI-generated content in information retrieval and summarization tasks which is
critical to gain customer trust especially in commercial products.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [222] [Markov Kernels, Distances and Optimal Control: A Parable of Linear Quadratic Non-Gaussian Distribution Steering](https://arxiv.org/abs/2504.15753)
*Alexis M. H. Teter,Wenqing Wang,Sachin Shivakumar,Abhishek Halder*

Main category: math.OC

TL;DR: 论文推导了线性时变系统的马尔可夫核，解决了线性二次非高斯薛定谔桥问题，并揭示了马尔可夫核、距离与最优控制的新联系。


<details>
  <summary>Details</summary>
Motivation: 研究线性时变系统的扩散过程及其伴随的马尔可夫核，以解决非高斯分布间的控制问题。

Method: 通过求解确定性最优控制问题，找到状态-时间依赖的距离泛函，推导出马尔可夫核。

Result: 得到了线性反应-对流-扩散偏微分方程的格林函数，并证明了线性二次非高斯薛定谔桥问题的可解性。

Conclusion: 该方法突破了现有技术的局限，揭示了马尔可夫核与最优控制的新联系，具有广泛的应用潜力。

Abstract: For a controllable linear time-varying (LTV) pair
$(\boldsymbol{A}_t,\boldsymbol{B}_t)$ and $\boldsymbol{Q}_{t}$ positive
semidefinite, we derive the Markov kernel for the It\^{o} diffusion
${\mathrm{d}}\boldsymbol{x}_{t}=\boldsymbol{A}_{t}\boldsymbol{x}_t {\mathrm{d}}
t + \sqrt{2}\boldsymbol{B}_{t}{\mathrm{d}}\boldsymbol{w}_{t}$ with an
accompanying killing of probability mass at rate
$\frac{1}{2}\boldsymbol{x}^{\top}\boldsymbol{Q}_{t}\boldsymbol{x}$. This Markov
kernel is the Green's function for an associated linear
reaction-advection-diffusion partial differential equation. Our result
generalizes the recently derived kernel for the special case
$\left(\boldsymbol{A}_t,\boldsymbol{B}_t\right)=\left(\boldsymbol{0},\boldsymbol{I}\right)$,
and depends on the solution of an associated Riccati matrix ODE. A consequence
of this result is that the linear quadratic non-Gaussian Schr\"{o}dinger bridge
is exactly solvable. This means that the problem of steering a controlled LTV
diffusion from a given non-Gaussian distribution to another over a fixed
deadline while minimizing an expected quadratic cost can be solved using
dynamic Sinkhorn recursions performed with the derived kernel. Our derivation
for the
$\left(\boldsymbol{A}_t,\boldsymbol{B}_t,\boldsymbol{Q}_t\right)$-parametrized
kernel pursues a new idea that relies on finding a state-time dependent
distance-like functional given by the solution of a deterministic optimal
control problem. This technique breaks away from existing methods, such as
generalizing Hermite polynomials or Weyl calculus, which have seen limited
success in the reaction-diffusion context. Our technique uncovers a new
connection between Markov kernels, distances, and optimal control. This
connection is of interest beyond its immediate application in solving the
linear quadratic Schr\"{o}dinger bridge problem.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [223] [Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL](https://arxiv.org/abs/2504.15425)
*Songyuan Zhang,Oswin So,Mitchell Black,Zachary Serlin,Chuchu Fan*

Main category: cs.RO

TL;DR: 论文提出了一种名为Def-MARL的分布式多智能体强化学习算法，用于解决多机器人系统中的安全协作问题，通过改进训练稳定性并在仿真和实际硬件实验中验证其性能。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在协作任务中需要保证安全性，现有算法在零约束违反的安全设定下训练不稳定，需要一种更稳定的方法。

Method: 采用约束优化的epigraph形式，证明其可以分布式求解，提出Def-MARL算法，实现集中训练分布式执行。

Result: 在8个任务和2个仿真器中，Def-MARL表现最佳，满足安全约束且训练稳定；实际硬件实验也验证了其性能。

Conclusion: Def-MARL是一种高效、稳定的安全多智能体强化学习算法，适用于实际机器人协作任务。

Abstract: Tasks for multi-robot systems often require the robots to collaborate and
complete a team goal while maintaining safety. This problem is usually
formalized as a constrained Markov decision process (CMDP), which targets
minimizing a global cost and bringing the mean of constraint violation below a
user-defined threshold. Inspired by real-world robotic applications, we define
safety as zero constraint violation. While many safe multi-agent reinforcement
learning (MARL) algorithms have been proposed to solve CMDPs, these algorithms
suffer from unstable training in this setting. To tackle this, we use the
epigraph form for constrained optimization to improve training stability and
prove that the centralized epigraph form problem can be solved in a distributed
fashion by each agent. This results in a novel centralized training distributed
execution MARL algorithm named Def-MARL. Simulation experiments on 8 different
tasks across 2 different simulators show that Def-MARL achieves the best
overall performance, satisfies safety constraints, and maintains stable
training. Real-world hardware experiments on Crazyflie quadcopters demonstrate
the ability of Def-MARL to safely coordinate agents to complete complex
collaborative tasks compared to other methods.

</details>


### [224] [Advancing Embodied Intelligence in Robotic-Assisted Endovascular Procedures: A Systematic Review of AI Solutions](https://arxiv.org/abs/2504.15327)
*Tianliang Yao,Bo Lu,Markus Kowarschik,Yixuan Yuan,Hubin Zhao,Sebastien Ourselin,Kaspar Althoefer,Junbo Ge,Peng Qi*

Main category: cs.RO

TL;DR: 论文综述了将具身智能（EI）集成到机器人系统中，用于提升血管内手术的精确性和适应性，探讨了数据驱动方法、计算机视觉和机器学习技术的应用，并展望了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 血管内手术的精确性和灵活性要求高，传统方法存在操作者疲劳、辐射暴露和人类精度限制等问题，机器人系统和EI的引入为解决这些问题提供了新思路。

Method: 采用数据驱动方法、计算机视觉、医学图像分析、机器学习和强化学习等技术，实现实时血管分割、设备跟踪和导航策略优化。

Result: EI和机器人技术的结合显著提升了血管内手术的精确性和适应性，减少了操作者负担，改善了临床效果。

Conclusion: 未来研究方向包括联邦学习、可解释AI和高级人机协作，有望进一步提升手术自主性和临床效果。

Abstract: Endovascular procedures have revolutionized the treatment of vascular
diseases thanks to minimally invasive solutions that significantly reduce
patient recovery time and enhance clinical outcomes. However, the precision and
dexterity required during these procedures poses considerable challenges for
interventionists. Robotic systems have emerged offering transformative
solutions, addressing issues such as operator fatigue, radiation exposure, and
the inherent limitations of human precision. The integration of Embodied
Intelligence (EI) into these systems signifies a paradigm shift, enabling
robots to navigate complex vascular networks and adapt to dynamic physiological
conditions. Data-driven approaches, advanced computer vision, medical image
analysis, and machine learning techniques, are at the forefront of this
evolution. These methods augment procedural intelligence by facilitating
real-time vessel segmentation, device tracking, and anatomical landmark
detection. Reinforcement learning and imitation learning further refine
navigation strategies and replicate experts' techniques. This review
systematically examines the integration of EI principles into robotic
technologies, in relation to endovascular procedures. We discuss recent
advancements in intelligent perception and data-driven control, and their
practical applications in robot-assisted endovascular procedures. By critically
evaluating current limitations and emerging opportunities, this review
establishes a framework for future developments, emphasizing the potential for
greater autonomy and improved clinical outcomes. Emerging trends and specific
areas of research, such as federated learning for medical data sharing,
explainable AI for clinical decision support, and advanced human-robot
collaboration paradigms, are also explored, offering insights into the future
direction of this rapidly evolving field.

</details>


### [225] [A Vision-Enabled Prosthetic Hand for Children with Upper Limb Disabilities](https://arxiv.org/abs/2504.15654)
*Md Abdul Baset Sarker,Art Nguyen,Sigmond Kukla,Kevin Fite,Masudul H. Imtiaz*

Main category: cs.RO

TL;DR: 本文介绍了一种新型AI视觉辅助儿童假手，专为10-12岁上肢残疾儿童设计，具有仿生外观、多关节功能和轻量化设计，结合3D打印和机器视觉技术，提供低成本、可定制的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前肌电假肢存在局限性，尤其是对低收入家庭儿童的可及性和负担能力不足，因此需要一种更经济、功能更全面的替代方案。

Method: 采用3D打印技术，集成机器视觉、传感器和嵌入式计算，通过微型摄像头和低功耗FPGA实现实时物体检测和精确抓握。

Result: 深度学习物体检测和抓握分类模型的准确率分别达到96%和100%，力预测的平均绝对误差为0.018。

Conclusion: 该假手通过AI视觉和低功耗设计，实现了高性能和广泛适用性，为儿童假肢领域提供了创新解决方案。

Abstract: This paper introduces a novel AI vision-enabled pediatric prosthetic hand
designed to assist children aged 10-12 with upper limb disabilities. The
prosthesis features an anthropomorphic appearance, multi-articulating
functionality, and a lightweight design that mimics a natural hand, making it
both accessible and affordable for low-income families. Using 3D printing
technology and integrating advanced machine vision, sensing, and embedded
computing, the prosthetic hand offers a low-cost, customizable solution that
addresses the limitations of current myoelectric prostheses. A micro camera is
interfaced with a low-power FPGA for real-time object detection and assists
with precise grasping. The onboard DL-based object detection and grasp
classification models achieved accuracies of 96% and 100% respectively. In the
force prediction, the mean absolute error was found to be 0.018. The features
of the proposed prosthetic hand can thus be summarized as: a) a wrist-mounted
micro camera for artificial sensing, enabling a wide range of hand-based tasks;
b) real-time object detection and distance estimation for precise grasping; and
c) ultra-low-power operation that delivers high performance within constrained
power and resource limits.

</details>


### [226] [Post-Convergence Sim-to-Real Policy Transfer: A Principled Alternative to Cherry-Picking](https://arxiv.org/abs/2504.15414)
*Dylan Khor,Bowen Weng*

Main category: cs.RO

TL;DR: 本文提出了一种基于最坏情况性能转移优化的方法，用于解决强化学习策略在仿真到现实转移后的收敛问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注预收敛阶段的仿真到现实转移，但无法避免收敛后的奖励波动和启发式策略选择问题。

Method: 采用凸二次约束线性规划问题，优化最坏情况性能转移。

Result: 实验证明该方法能有效将强化学习策略从仿真转移到现实测试中。

Conclusion: 该方法为仿真到现实转移提供了一种新的优化思路。

Abstract: Learning-based approaches, particularly reinforcement learning (RL), have
become widely used for developing control policies for autonomous agents, such
as locomotion policies for legged robots. RL training typically maximizes a
predefined reward (or minimizes a corresponding cost/loss) by iteratively
optimizing policies within a simulator. Starting from a randomly initialized
policy, the empirical expected reward follows a trajectory with an overall
increasing trend. While some policies become temporarily stuck in local optima,
a well-defined training process generally converges to a reward level with
noisy oscillations. However, selecting a policy for real-world deployment is
rarely an analytical decision (i.e., simply choosing the one with the highest
reward) and is instead often performed through trial and error. To improve
sim-to-real transfer, most research focuses on the pre-convergence stage,
employing techniques such as domain randomization, multi-fidelity training,
adversarial training, and architectural innovations. However, these methods do
not eliminate the inevitable convergence trajectory and noisy oscillations of
rewards, leading to heuristic policy selection or cherry-picking. This paper
addresses the post-convergence sim-to-real transfer problem by introducing a
worst-case performance transference optimization approach, formulated as a
convex quadratic-constrained linear programming problem. Extensive experiments
demonstrate its effectiveness in transferring RL-based locomotion policies from
simulation to real-world laboratory tests.

</details>


### [227] [SLAM-Based Navigation and Fault Resilience in a Surveillance Quadcopter with Embedded Vision Systems](https://arxiv.org/abs/2504.15305)
*Abhishek Tyagi,Charu Gaur*

Main category: cs.RO

TL;DR: Veg是一个自主空中监视平台，集成了视觉SLAM、高级控制架构和嵌入式视觉模块，支持GPS独立导航、动态稳定性和实时物体/人脸识别。


<details>
  <summary>Details</summary>
Motivation: 开发一个在受限环境中具有高鲁棒性的无人机平台，整合实时定位、故障恢复和嵌入式AI功能。

Method: 采用LQR内环和PD外环的级联控制设计，利用ORB-SLAM3进行6自由度定位，支持基于Dijkstra路径规划的导航，并配备实时故障检测与识别系统。

Result: 通过仿真和实际测试验证了平台在实时定位、故障恢复和嵌入式AI方面的有效性。

Conclusion: Veg平台成功整合了多项关键技术，适用于受限环境中的自主监视任务。

Abstract: We present an autonomous aerial surveillance platform, Veg, designed as a
fault-tolerant quadcopter system that integrates visual SLAM for
GPS-independent navigation, advanced control architecture for dynamic
stability, and embedded vision modules for real-time object and face
recognition. The platform features a cascaded control design with an LQR
inner-loop and PD outer-loop trajectory control. It leverages ORB-SLAM3 for
6-DoF localization and loop closure, and supports waypoint-based navigation
through Dijkstra path planning over SLAM-derived maps. A real-time Failure
Detection and Identification (FDI) system detects rotor faults and executes
emergency landing through re-routing. The embedded vision system, based on a
lightweight CNN and PCA, enables onboard object detection and face recognition
with high precision. The drone operates fully onboard using a Raspberry Pi 4
and Arduino Nano, validated through simulations and real-world testing. This
work consolidates real-time localization, fault recovery, and embedded AI on a
single platform suitable for constrained environments.

</details>


### [228] [Dynamic Intent Queries for Motion Transformer-based Trajectory Prediction](https://arxiv.org/abs/2504.15766)
*Tobias Demmler,Lennart Hartung,Andreas Tamke,Thao Dang,Alexander Hegai,Karsten Haug,Lars Mikelsons*

Main category: cs.RO

TL;DR: 论文提出了一种改进的Motion Transformer（MTR）模型，通过引入动态意图点来提升轨迹预测的准确性，解决了静态意图点与地图数据不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，准确的轨迹预测对车辆规划至关重要。现有MTR模型使用静态意图点，但在某些交通场景中与地图数据不匹配，导致预测不准确。

Method: 改进MTR模型，引入场景特定的动态意图点，并在Waymo Open Motion Dataset上进行训练和评估。

Result: 动态意图点显著提升了轨迹预测的准确性，尤其是在长时间范围内的预测。同时分析了不符合地图数据或非法操作的轨迹影响。

Conclusion: 动态意图点的引入有效解决了静态意图点的局限性，提升了轨迹预测的准确性和实用性。

Abstract: In autonomous driving, accurately predicting the movements of other traffic
participants is crucial, as it significantly influences a vehicle's planning
processes. Modern trajectory prediction models strive to interpret complex
patterns and dependencies from agent and map data. The Motion Transformer (MTR)
architecture and subsequent work define the most accurate methods in common
benchmarks such as the Waymo Open Motion Benchmark. The MTR model employs
pre-generated static intention points as initial goal points for trajectory
prediction. However, the static nature of these points frequently leads to
misalignment with map data in specific traffic scenarios, resulting in
unfeasible or unrealistic goal points. Our research addresses this limitation
by integrating scene-specific dynamic intention points into the MTR model. This
adaptation of the MTR model was trained and evaluated on the Waymo Open Motion
Dataset. Our findings demonstrate that incorporating dynamic intention points
has a significant positive impact on trajectory prediction accuracy, especially
for predictions over long time horizons. Furthermore, we analyze the impact on
ground truth trajectories which are not compliant with the map data or are
illegal maneuvers.

</details>


### [229] [LAPP: Large Language Model Feedback for Preference-Driven Reinforcement Learning](https://arxiv.org/abs/2504.15472)
*Pingcheng Jian,Xiao Wei,Yanbaihui Liu,Samuel A. Moore,Michael M. Zavlanos,Boyuan Chen*

Main category: cs.RO

TL;DR: LAPP利用大语言模型自动生成偏好标签，指导机器人学习复杂技能，减少人工干预。


<details>
  <summary>Details</summary>
Motivation: 解决传统机器人学习方法依赖人工奖励设计、演示或昂贵标签的问题。

Method: 通过LLM从RL轨迹生成偏好标签，训练在线偏好预测器以优化策略。

Result: 在四足运动和灵巧操作任务中表现高效，能完成高动态任务如后空翻。

Conclusion: LAPP为偏好驱动的机器人学习提供了可扩展的新方向。

Abstract: We introduce Large Language Model-Assisted Preference Prediction (LAPP), a
novel framework for robot learning that enables efficient, customizable, and
expressive behavior acquisition with minimum human effort. Unlike prior
approaches that rely heavily on reward engineering, human demonstrations,
motion capture, or expensive pairwise preference labels, LAPP leverages large
language models (LLMs) to automatically generate preference labels from raw
state-action trajectories collected during reinforcement learning (RL). These
labels are used to train an online preference predictor, which in turn guides
the policy optimization process toward satisfying high-level behavioral
specifications provided by humans. Our key technical contribution is the
integration of LLMs into the RL feedback loop through trajectory-level
preference prediction, enabling robots to acquire complex skills including
subtle control over gait patterns and rhythmic timing. We evaluate LAPP on a
diverse set of quadruped locomotion and dexterous manipulation tasks and show
that it achieves efficient learning, higher final performance, faster
adaptation, and precise control of high-level behaviors. Notably, LAPP enables
robots to master highly dynamic and expressive tasks such as quadruped
backflips, which remain out of reach for standard LLM-generated or handcrafted
rewards. Our results highlight LAPP as a promising direction for scalable
preference-driven robot learning.

</details>


### [230] [Few-Shot Vision-Language Action-Incremental Policy Learning](https://arxiv.org/abs/2504.15517)
*Mingchen Song,Xiang Deng,Guoqiang Zhong,Qi Lv,Jia Wan,Yinchuan Li,Jianye Hao,Weili Guan*

Main category: cs.RO

TL;DR: 论文提出了一种名为TOPIC的方法，用于解决机器人模仿学习中的数据稀缺和持续学习问题，通过任务特定提示和连续进化策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的机器人操作方法需要大量演示数据，且缺乏在少量演示下对新任务的持续学习能力。

Method: TOPIC通过任务特定提示（TSP）提取任务信息，并采用连续进化策略（CES）构建任务关系图以促进新任务适应。

Result: 实验表明，TOPIC在成功率上比现有基线方法高出26%，显著提升了持续学习能力。

Conclusion: TOPIC为机器人操作任务中的少样本持续学习提供了有效解决方案，具有显著性能优势。

Abstract: Recently, Transformer-based robotic manipulation methods utilize multi-view
spatial representations and language instructions to learn robot motion
trajectories by leveraging numerous robot demonstrations. However, the
collection of robot data is extremely challenging, and existing methods lack
the capability for continuous learning on new tasks with only a few
demonstrations. In this paper, we formulate these challenges as the Few-Shot
Action-Incremental Learning (FSAIL) task, and accordingly design a Task-prOmpt
graPh evolutIon poliCy (TOPIC) to address these issues. Specifically, to
address the data scarcity issue in robotic imitation learning, TOPIC learns
Task-Specific Prompts (TSP) through the deep interaction of multi-modal
information within few-shot demonstrations, thereby effectively extracting the
task-specific discriminative information. On the other hand, to enhance the
capability for continual learning on new tasks and mitigate the issue of
catastrophic forgetting, TOPIC adopts a Continuous Evolution Strategy (CES).
CES leverages the intrinsic relationships between tasks to construct a task
relation graph, which effectively facilitates the adaptation of new tasks by
reusing skills learned from previous tasks. TOPIC pioneers few-shot continual
learning in the robotic manipulation task, and extensive experimental results
demonstrate that TOPIC outperforms state-of-the-art baselines by over 26$\%$ in
success rate, significantly enhancing the continual learning capabilities of
existing Transformer-based policies.

</details>


### [231] [RiskNet: Interaction-Aware Risk Forecasting for Autonomous Driving in Long-Tail Scenarios](https://arxiv.org/abs/2504.15541)
*Qichao Liu,Heye Huang,Shiyue Zhao,Lei Shi,Soyoung Ahn,Xiaopeng Li*

Main category: cs.RO

TL;DR: RiskNet是一个交互感知的风险预测框架，结合确定性风险建模与概率行为预测，用于自动驾驶车辆在长尾场景下的安全评估。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在长尾场景下的安全性仍面临挑战，尤其是在高不确定性和复杂多智能体交互的情况下。

Method: RiskNet采用场论模型捕捉车辆、周围智能体及基础设施的交互，并结合基于GNN的轨迹预测模块处理行为不确定性。

Result: 在多个数据集上的评估表明，RiskNet在准确性、响应性和方向敏感性上显著优于传统方法。

Conclusion: RiskNet为长尾场景下的安全决策提供了统一基础，具有强泛化能力。

Abstract: Ensuring the safety of autonomous vehicles (AVs) in long-tail scenarios
remains a critical challenge, particularly under high uncertainty and complex
multi-agent interactions. To address this, we propose RiskNet, an
interaction-aware risk forecasting framework, which integrates deterministic
risk modeling with probabilistic behavior prediction for comprehensive risk
assessment. At its core, RiskNet employs a field-theoretic model that captures
interactions among ego vehicle, surrounding agents, and infrastructure via
interaction fields and force. This model supports multidimensional risk
evaluation across diverse scenarios (highways, intersections, and roundabouts),
and shows robustness under high-risk and long-tail settings. To capture the
behavioral uncertainty, we incorporate a graph neural network (GNN)-based
trajectory prediction module, which learns multi-modal future motion
distributions. Coupled with the deterministic risk field, it enables dynamic,
probabilistic risk inference across time, enabling proactive safety assessment
under uncertainty. Evaluations on the highD, inD, and rounD datasets, spanning
lane changes, turns, and complex merges, demonstrate that our method
significantly outperforms traditional approaches (e.g., TTC, THW, RSS, NC
Field) in terms of accuracy, responsiveness, and directional sensitivity, while
maintaining strong generalization across scenarios. This framework supports
real-time, scenario-adaptive risk forecasting and demonstrates strong
generalization across uncertain driving environments. It offers a unified
foundation for safety-critical decision-making in long-tail scenarios.

</details>


### [232] [SPECI: Skill Prompts based Hierarchical Continual Imitation Learning for Robot Manipulation](https://arxiv.org/abs/2504.15561)
*Jingkai Xu,Xiangli Nie*

Main category: cs.RO

TL;DR: SPECI提出了一种基于技能提示的分层持续模仿学习框架，用于机器人操作，通过动态技能提取和知识转移提升适应性。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习依赖静态训练，难以适应动态环境；现有CIL方法忽视技能特性或依赖人工定义技能，导致知识转移效果不佳。

Method: SPECI包含多模态感知融合模块、高层技能推断模块和低层动作执行模块，通过可扩展技能库和注意力机制实现技能获取与重用。

Result: 实验表明，SPECI在多种操作任务中优于现有CIL方法，表现出卓越的双向知识转移和整体性能。

Conclusion: SPECI通过分层结构和动态技能管理，显著提升了机器人操作在动态环境中的持续适应能力。

Abstract: Real-world robot manipulation in dynamic unstructured environments requires
lifelong adaptability to evolving objects, scenes and tasks. Traditional
imitation learning relies on static training paradigms, which are ill-suited
for lifelong adaptation. Although Continual Imitation Learnin (CIL) enables
incremental task adaptation while preserving learned knowledge, current CIL
methods primarily overlook the intrinsic skill characteristics of robot
manipulation or depend on manually defined and rigid skills, leading to
suboptimal cross-task knowledge transfer. To address these issues, we propose
Skill Prompts-based HiErarchical Continual Imitation Learning (SPECI), a novel
end-to-end hierarchical CIL policy architecture for robot manipulation. The
SPECI framework consists of a multimodal perception and fusion module for
heterogeneous sensory information encoding, a high-level skill inference module
for dynamic skill extraction and selection, and a low-level action execution
module for precise action generation. To enable efficient knowledge transfer on
both skill and task levels, SPECI performs continual implicit skill acquisition
and reuse via an expandable skill codebook and an attention-driven skill
selection mechanism. Furthermore, we introduce mode approximation to augment
the last two modules with task-specific and task-sharing parameters, thereby
enhancing task-level knowledge transfer. Extensive experiments on diverse
manipulation task suites demonstrate that SPECI consistently outperforms
state-of-the-art CIL methods across all evaluated metrics, revealing
exceptional bidirectional knowledge transfer and superior overall performance.

</details>


### [233] [Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement Learning for Strategic Confrontation](https://arxiv.org/abs/2504.15876)
*Qizhen Wu Lei Chen,Kexin Liu,Jinhu Lü*

Main category: cs.RO

TL;DR: 提出了一种基于分层强化学习的双向方法，用于群体机器人对抗场景中的高效决策，结合离散命令与连续动作，显著提升了动态环境中的适应性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统任务与运动规划方法将决策分为两层，但单向结构无法捕捉层间依赖关系，限制了动态环境中的适应性。

Method: 采用分层强化学习的双向方法，结合交叉训练技术和轨迹预测模型，实现任务分配与路径规划的高效映射。

Result: 实验显示对抗胜率超过80%，决策时间低于0.01秒，优于现有方法。

Conclusion: 该方法在大型测试和真实机器人实验中表现出良好的泛化能力和实际应用潜力。

Abstract: In swarm robotics, confrontation scenarios, including strategic
confrontations, require efficient decision-making that integrates discrete
commands and continuous actions. Traditional task and motion planning methods
separate decision-making into two layers, but their unidirectional structure
fails to capture the interdependence between these layers, limiting
adaptability in dynamic environments. Here, we propose a novel bidirectional
approach based on hierarchical reinforcement learning, enabling dynamic
interaction between the layers. This method effectively maps commands to task
allocation and actions to path planning, while leveraging cross-training
techniques to enhance learning across the hierarchical framework. Furthermore,
we introduce a trajectory prediction model that bridges abstract task
representations with actionable planning goals. In our experiments, it achieves
over 80\% in confrontation win rate and under 0.01 seconds in decision time,
outperforming existing approaches. Demonstrations through large-scale tests and
real-world robot experiments further emphasize the generalization capabilities
and practical applicability of our method.

</details>


### [234] [RaSCL: Radar to Satellite Crossview Localization](https://arxiv.org/abs/2504.15899)
*Blerim Abdullai,Tony Wang,Xinyuan Qiao,Florian Shkurti,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: 提出了一种不依赖GNSS的全局定位方法，通过地面雷达与高空RGB图像的配准，结合里程计和全局姿态优化，实现高精度定位。


<details>
  <summary>Details</summary>
Motivation: GNSS在许多实时自主应用中不可靠、不准确且不足，需要一种替代方案。

Method: 通过地面雷达与高空RGB图像配准，结合里程计和全局姿态优化，提取RGB图像中的关键特征进行定位。

Result: 在多种地理条件和机器人平台上验证了方法的有效性，包括无人水面艇和城市/郊区驾驶数据集。

Conclusion: 该方法提供了一种不依赖GNSS的高效全局定位解决方案。

Abstract: GNSS is unreliable, inaccurate, and insufficient in many real-time autonomous
field applications. In this work, we present a GNSS-free global localization
solution that contains a method of registering imaging radar on the ground with
overhead RGB imagery, with joint optimization of relative poses from odometry
and global poses from our overhead registration. Previous works have used
various combinations of ground sensors and overhead imagery, and different
feature extraction and matching methods. These include various handcrafted and
deep-learning-based methods for extracting features from overhead imagery. Our
work presents insights on extracting essential features from RGB overhead
images for effective global localization against overhead imagery using only
ground radar and a single georeferenced initial guess. We motivate our method
by evaluating it on datasets in diverse geographic conditions and robotic
platforms, including on an Unmanned Surface Vessel (USV) as well as urban and
suburban driving datasets.

</details>


### [235] [Visual Place Cell Encoding: A Computational Model for Spatial Representation and Cognitive Mapping](https://arxiv.org/abs/2504.15953)
*Chance J. Hamilton,Alfredo Weitzenfeld*

Main category: cs.RO

TL;DR: VPCE模型通过视觉输入模拟位置细胞激活，利用视觉地标进行空间编码，实验表明其能区分空间位置并适应环境变化。


<details>
  <summary>Details</summary>
Motivation: 研究视觉地标在空间编码中的作用，探索仅凭视觉输入是否能生成类似生物位置细胞的空间表征。

Method: VPCE模型通过聚类高维视觉特征定义感受野，使用径向基函数计算激活，评估其与生物位置细胞特性的相关性。

Result: VPCE能区分视觉相似但空间不同的位置，并适应环境变化（如增减墙壁）。

Conclusion: 结构化视觉输入足以生成位置细胞样空间表征，支持生物启发的认知映射。

Abstract: This paper presents the Visual Place Cell Encoding (VPCE) model, a
biologically inspired computational framework for simulating place cell-like
activation using visual input. Drawing on evidence that visual landmarks play a
central role in spatial encoding, the proposed VPCE model activates visual
place cells by clustering high-dimensional appearance features extracted from
images captured by a robot-mounted camera. Each cluster center defines a
receptive field, and activation is computed based on visual similarity using a
radial basis function. We evaluate whether the resulting activation patterns
correlate with key properties of biological place cells, including spatial
proximity, orientation alignment, and boundary differentiation. Experiments
demonstrate that the VPCE can distinguish between visually similar yet
spatially distinct locations and adapt to environment changes such as the
insertion or removal of walls. These results suggest that structured visual
input, even in the absence of motion cues or reward-driven learning, is
sufficient to generate place-cell-like spatial representations and support
biologically inspired cognitive mapping.

</details>


### [236] [ForesightNav: Learning Scene Imagination for Efficient Exploration](https://arxiv.org/abs/2504.16062)
*Hardik Shah,Jiaxu Xing,Nico Messikommer,Boyang Sun,Marc Pollefeys,Davide Scaramuzza*

Main category: cs.RO

TL;DR: ForesightNav是一种受人类想象和推理启发的探索策略，通过预测未探索区域的上下文信息（如占用和语义细节），提高机器人在未知环境中的导航效率。


<details>
  <summary>Details</summary>
Motivation: 研究人类如何利用先验知识在未知环境中导航和探索，以开发具备类似能力的自主机器人。

Method: 提出ForesightNav方法，赋予机器人预测未探索区域信息的能力，从而选择有意义的长期导航目标。

Result: 在Structured3D数据集上验证，实现了准确的占用预测和场景几何预测，探索效率显著提升（PointNav完成率100%，ObjectNav SPL 67%）。

Conclusion: 想象驱动的推理能显著提升自主系统的通用性和探索效率。

Abstract: Understanding how humans leverage prior knowledge to navigate unseen
environments while making exploratory decisions is essential for developing
autonomous robots with similar abilities. In this work, we propose
ForesightNav, a novel exploration strategy inspired by human imagination and
reasoning. Our approach equips robotic agents with the capability to predict
contextual information, such as occupancy and semantic details, for unexplored
regions. These predictions enable the robot to efficiently select meaningful
long-term navigation goals, significantly enhancing exploration in unseen
environments. We validate our imagination-based approach using the Structured3D
dataset, demonstrating accurate occupancy prediction and superior performance
in anticipating unseen scene geometry. Our experiments show that the
imagination module improves exploration efficiency in unseen environments,
achieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav
on the Structured3D Validation split. These contributions demonstrate the power
of imagination-driven reasoning for autonomous systems to enhance generalizable
and efficient exploration.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [237] [Efficient Discovery of Motif Transition Process for Large-Scale Temporal Graphs](https://arxiv.org/abs/2504.15979)
*Zhiyuan Zheng,Jianpeng Qi,Jiantao Li,Guoqing Chao,Junyu Dong,Yanwei Yu*

Main category: cs.DB

TL;DR: 提出了一种并行算法PTMT，用于发现大规模时序图中的motif动态转换过程，显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于预定义的motif，无法全面捕捉时序图中的动态转换和相互关系。

Method: PTMT结合树形框架与时序区域划分策略（TZP），通过并行化三个阶段（生长区域扩展、重叠感知结果聚合、motif转换确定性编码）实现高效发现。

Result: 在10个真实数据集上，PTMT比现有最优方法快12.0至50.3倍。

Conclusion: PTMT能高效且准确地捕捉时序图中motif的动态转换，适用于大规模分析。

Abstract: Understanding the dynamic transition of motifs in temporal graphs is
essential for revealing how graph structures evolve over time, identifying
critical patterns, and predicting future behaviors, yet existing methods often
focus on predefined motifs, limiting their ability to comprehensively capture
transitions and interrelationships. We propose a parallel motif transition
process discovery algorithm, PTMT, a novel parallel method for discovering
motif transition processes in large-scale temporal graphs. PTMT integrates a
tree-based framework with the temporal zone partitioning (TZP) strategy, which
partitions temporal graphs by time and structure while preserving lossless
motif transitions and enabling massive parallelism. PTMT comprises three
phases: growth zone parallel expansion, overlap-aware result aggregation, and
deterministic encoding of motif transitions, ensuring accurate tracking of
dynamic transitions and interactions. Results on 10 real-world datasets
demonstrate that PTMT achieves speedups ranging from 12.0$\times$ to
50.3$\times$ compared to the SOTA method.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [238] [High-performance training and inference for deep equivariant interatomic potentials](https://arxiv.org/abs/2504.16068)
*Chuin Wei Tan,Marc L. Descoteaux,Mit Kotak,Gabriel de Miranda Nascimento,Seán R. Kavanagh,Laura Zichi,Menghang Wang,Aadit Saluja,Yizhong R. Hu,Tess Smidt,Anders Johansson,William C. Witt,Boris Kozinsky,Albert Musaelian*

Main category: physics.comp-ph

TL;DR: NequIP框架的重大升级，专注于多节点并行、计算性能和可扩展性，显著提升了机器学习的原子间势能模型的训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 随着数据集规模和下游工作流需求的快速增长，需要更强大和可扩展的软件来支持机器学习的原子间势能模型。

Method: 对NequIP框架进行重大改进，支持分布式训练，优化PyTorch 2.0编译器的使用，并引入自定义内核加速关键操作。

Result: 在SPICE 2数据集上训练Allegro模型，分子动力学计算速度提升高达18倍。

Conclusion: 改进后的框架显著提升了原子间势能模型的训练和推理效率，适用于实际规模的系统。

Abstract: Machine learning interatomic potentials, particularly those based on deep
equivariant neural networks, have demonstrated state-of-the-art accuracy and
computational efficiency in atomistic modeling tasks like molecular dynamics
and high-throughput screening. The size of datasets and demands of downstream
workflows are growing rapidly, making robust and scalable software essential.
This work presents a major overhaul of the NequIP framework focusing on
multi-node parallelism, computational performance, and extensibility. The
redesigned framework supports distributed training on large datasets and
removes barriers preventing full utilization of the PyTorch 2.0 compiler at
train time. We demonstrate this acceleration in a case study by training
Allegro models on the SPICE 2 dataset of organic molecular systems. For
inference, we introduce the first end-to-end infrastructure that uses the
PyTorch Ahead-of-Time Inductor compiler for machine learning interatomic
potentials. Additionally, we implement a custom kernel for the Allegro model's
most expensive operation, the tensor product. Together, these advancements
speed up molecular dynamics calculations on system sizes of practical relevance
by up to a factor of 18.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [239] [Assessing Surrogate Heterogeneity in Real World Data Using Meta-Learners](https://arxiv.org/abs/2504.15386)
*Rebecca Knowlton,Layla Parast*

Main category: stat.ME

TL;DR: 本文提出了一种框架，用于评估现实世界非随机数据中的替代标记异质性，并通过元学习器实现。该方法允许量化替代标记强度与患者特征的异质性，同时通过灵活的机器学习方法处理混杂因素。


<details>
  <summary>Details</summary>
Motivation: 现实世界公共健康和社会科学研究中，随机试验往往不切实际，而现有统计方法多依赖随机化假设，缺乏对非随机数据中替代标记异质性的研究。

Method: 提出一个框架，利用元学习器评估替代标记异质性，并通过机器学习方法处理混杂因素。

Result: 通过模拟研究和实际应用（以血红蛋白A1c作为空腹血糖的替代标记），验证了方法的性能。

Conclusion: 该框架能有效量化替代标记异质性，并识别替代标记有效的个体。

Abstract: Surrogate markers are most commonly studied within the context of randomized
clinical trials. However, the need for alternative outcomes extends beyond
these settings and may be more pronounced in real-world public health and
social science research, where randomized trials are often impractical.
Research on identifying surrogates in real-world non-randomized data is scarce,
as available statistical approaches for evaluating surrogate markers tend to
rely on the assumption that treatment is randomized. While the few methods that
allow for non-randomized treatment/exposure appropriately handle confounding
individual characteristics, they do not offer a way to examine surrogate
heterogeneity with respect to patient characteristics. In this paper, we
propose a framework to assess surrogate heterogeneity in real-world, i.e.,
non-randomized, data and implement this framework using various meta-learners.
Our approach allows us to quantify heterogeneity in surrogate strength with
respect to patient characteristics while accommodating confounders through the
use of flexible, off-the-shelf machine learning methods. In addition, we use
our framework to identify individuals for whom the surrogate is a valid
replacement of the primary outcome. We examine the performance of our methods
via a simulation study and application to examine heterogeneity in the
surrogacy of hemoglobin A1c as a surrogate for fasting plasma glucose.

</details>


### [240] [Deep learning with missing data](https://arxiv.org/abs/2504.15388)
*Tianyi Ma,Tengyao Wang,Richard J. Samworth*

Main category: stat.ME

TL;DR: 提出了一种名为PENN的神经网络方法，用于处理缺失协变量的多元非参数回归问题，结合任意插补技术，通过三个子网络提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决缺失协变量在多元非参数回归中的挑战，提升预测准确性。

Method: PENN结合插补技术，使用三个子网络：一个处理插补数据，一个处理观测指示向量，最后一个整合输出。

Result: 理论证明PENN在典型情况下达到最小最大收敛速率，实验显示其显著优于标准神经网络。

Conclusion: PENN是一种高效且通用的方法，适用于缺失协变量的回归问题。

Abstract: In the context of multivariate nonparametric regression with missing
covariates, we propose Pattern Embedded Neural Networks (PENNs), which can be
applied in conjunction with any existing imputation technique. In addition to a
neural network trained on the imputed data, PENNs pass the vectors of
observation indicators through a second neural network to provide a compact
representation. The outputs are then combined in a third neural network to
produce final predictions. Our main theoretical result exploits an assumption
that the observation patterns can be partitioned into cells on which the Bayes
regression function behaves similarly, and belongs to a compositional H\"older
class. It provides a finite-sample excess risk bound that holds for an
arbitrary missingness mechanism, and in combination with a complementary
minimax lower bound, demonstrates that our PENN estimator attains in typical
cases the minimax rate of convergence as if the cells of the partition were
known in advance, up to a poly-logarithmic factor in the sample size. Numerical
experiments on simulated, semi-synthetic and real data confirm that the PENN
estimator consistently improves, often dramatically, on standard neural
networks without pattern embedding. Code to reproduce our experiments, as well
as a tutorial on how to apply our method, is publicly available.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [241] [Do It For Me vs. Do It With Me: Investigating User Perceptions of Different Paradigms of Automation in Copilots for Feature-Rich Software](https://arxiv.org/abs/2504.15549)
*Anjali Khurana,Xiaotian Su,April Yi Wang,Parmit K Chilana*

Main category: cs.HC

TL;DR: 研究比较了全自动（AutoCopilot）和半自动（GuidedCopilot）助手在用户体验上的差异，发现半自动助手在用户控制、软件实用性和学习性上表现更优，尤其适用于探索性和创造性任务。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过优化自动化水平提升用户体验，尤其是在用户偏好“边做边学”的背景下。

Method: 设计了两种助手（全自动和半自动），并通过用户研究（N=20）和后续设计探索（N=10）评估其表现。

Result: 半自动助手在用户控制、实用性和学习性上优于全自动助手，而全自动助手在简单任务中节省时间。后续设计增强了半自动助手的功能。

Conclusion: 用户控制和定制化指导是设计下一代助手的关键，以提升生产力并支持不同技能水平的用户。

Abstract: Large Language Model (LLM)-based in-application assistants, or copilots, can
automate software tasks, but users often prefer learning by doing, raising
questions about the optimal level of automation for an effective user
experience. We investigated two automation paradigms by designing and
implementing a fully automated copilot (AutoCopilot) and a semi-automated
copilot (GuidedCopilot) that automates trivial steps while offering
step-by-step visual guidance. In a user study (N=20) across data analysis and
visual design tasks, GuidedCopilot outperformed AutoCopilot in user control,
software utility, and learnability, especially for exploratory and creative
tasks, while AutoCopilot saved time for simpler visual tasks. A follow-up
design exploration (N=10) enhanced GuidedCopilot with task-and state-aware
features, including in-context preview clips and adaptive instructions. Our
findings highlight the critical role of user control and tailored guidance in
designing the next generation of copilots that enhance productivity, support
diverse skill levels, and foster deeper software engagement.

</details>


### [242] [iMedic: Towards Smartphone-based Self-Auscultation Tool for AI-Powered Pediatric Respiratory Assessment](https://arxiv.org/abs/2504.15743)
*Seung Gyu Jeong,Sung Woo Nam,Seong Kwan Jung,Seong-Eun Kim*

Main category: cs.HC

TL;DR: 智能手机系统利用深度学习检测儿童肺炎风险，通过集成电子听诊器和手机数据，提供即时反馈，用户研究显示高接受度和分类性能。


<details>
  <summary>Details</summary>
Motivation: 在医疗资源匮乏地区，早期检测儿童肺炎具有挑战性，需要低成本、易用的解决方案。

Method: 结合电子听诊器和智能手机数据，采用深度学习框架进行特征学习，开发移动应用指导采样并提供反馈。

Result: 系统分类性能强，用户接受度高，能促进主动干预，减少可预防的儿童肺炎死亡。

Conclusion: 基于智能手机的方法为远程儿科护理提供了更公平和全面的解决方案。

Abstract: Respiratory auscultation is crucial for early detection of pediatric
pneumonia, a condition that can quickly worsen without timely intervention. In
areas with limited physician access, effective auscultation is challenging. We
present a smartphone-based system that leverages built-in microphones and
advanced deep learning algorithms to detect abnormal respiratory sounds
indicative of pneumonia risk. Our end-to-end deep learning framework employs
domain generalization to integrate a large electronic stethoscope dataset with
a smaller smartphone-derived dataset, enabling robust feature learning for
accurate respiratory assessments without expensive equipment. The accompanying
mobile application guides caregivers in collecting high-quality lung sound
samples and provides immediate feedback on potential pneumonia risks. User
studies show strong classification performance and high acceptance,
demonstrating the system's ability to facilitate proactive interventions and
reduce preventable childhood pneumonia deaths. By seamlessly integrating into
ubiquitous smartphones, this approach offers a promising avenue for more
equitable and comprehensive remote pediatric care.

</details>


### [243] [Supporting Data-Frame Dynamics in AI-assisted Decision Making](https://arxiv.org/abs/2504.15894)
*Chengbo Zheng,Tim Miller,Alina Bialkowski,H Peter Soyer,Monika Janda*

Main category: cs.HC

TL;DR: 提出了一种基于数据框架理论和评估AI范式的混合主动框架，支持人类与AI协作构建、验证和调整假设，应用于皮肤癌诊断原型。


<details>
  <summary>Details</summary>
Motivation: 当前AI决策支持系统难以支持动态证据与假设的交互，需改进。

Method: 采用混合主动框架，结合概念瓶颈模型，实现可解释交互和动态假设更新。

Result: 开发了AI辅助皮肤癌诊断原型，验证了框架的有效性。

Conclusion: 该框架为高风险决策提供了动态协作支持，具有实际应用潜力。

Abstract: High stakes decision-making often requires a continuous interplay between
evolving evidence and shifting hypotheses, a dynamic that is not well supported
by current AI decision support systems. In this paper, we introduce a
mixed-initiative framework for AI assisted decision making that is grounded in
the data-frame theory of sensemaking and the evaluative AI paradigm. Our
approach enables both humans and AI to collaboratively construct, validate, and
adapt hypotheses. We demonstrate our framework with an AI-assisted skin cancer
diagnosis prototype that leverages a concept bottleneck model to facilitate
interpretable interactions and dynamic updates to diagnostic hypotheses.

</details>


### [244] [Recent Advances and Future Directions in Extended Reality (XR): Exploring AI-Powered Spatial Intelligence](https://arxiv.org/abs/2504.15970)
*Baichuan Zeng*

Main category: cs.HC

TL;DR: 本文综述了扩展现实（XR）技术的演变、现状及未来方向，强调其在空间智能和多模态AI集成中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨XR技术如何通过硬件和软件的进步，实现物理与虚拟世界的融合，并成为未来人机交互的前沿。

Method: 通过分析XR的硬件（如传感器）和软件（如视觉任务）框架，比较现有SOTA产品的性能。

Result: 指出商业XR设备在空间智能方面支持高质量性能的潜力，并强调AI和IoT驱动的数字孪生技术的重要性。

Conclusion: 未来XR需结合多模态AI和空间智能，打造更真实的数字空间，推动人机交互的革新。

Abstract: Extended Reality (XR), encompassing Augmented Reality (AR), Virtual Reality
(VR) and Mixed Reality (MR), is a transformative technology bridging the
physical and virtual world and it has diverse potential which will be
ubiquitous in the future. This review examines XR's evolution through
foundational framework - hardware ranging from monitors to sensors and software
ranging from visual tasks to user interface; highlights state of the art (SOTA)
XR products with the comparison and analysis of performance based on their
foundational framework; discusses how commercial XR devices can support the
demand of high-quality performance focusing on spatial intelligence. For future
directions, attention should be given to the integration of multi-modal AI and
IoT-driven digital twins to enable adaptive XR systems. With the concept of
spatial intelligence, future XR should establish a new digital space with
realistic experience that benefits humanity. This review underscores the
pivotal role of AI in unlocking XR as the next frontier in human-computer
interaction.

</details>


### [245] [Navigating the State of Cognitive Flow: Context-Aware AI Interventions for Effective Reasoning Support](https://arxiv.org/abs/2504.16021)
*Dinithi Dissanayake,Suranga Nanayakkara*

Main category: cs.HC

TL;DR: 论文提出了一种基于上下文感知的认知增强框架，通过动态调整AI干预以维持或恢复认知流，从而优化AI辅助决策的效果。


<details>
  <summary>Details</summary>
Motivation: 在AI辅助推理中，传统的干预方式可能破坏认知流状态，反而阻碍决策。因此，需要一种能够适应上下文、最小化干扰的干预方法。

Method: 通过多模态行为线索（如注视行为、输入犹豫、交互速度）动态调整干预，提出个性化、自适应的认知流概念。

Result: 该方法能够在不破坏认知沉浸的情况下，支持复杂决策和推理中的深度参与。

Conclusion: 上下文感知的认知增强框架优于静态干预，能更有效地维持认知流状态。

Abstract: Flow theory describes an optimal cognitive state where individuals experience
deep focus and intrinsic motivation when a task's difficulty aligns with their
skill level. In AI-augmented reasoning, interventions that disrupt the state of
cognitive flow can hinder rather than enhance decision-making. This paper
proposes a context-aware cognitive augmentation framework that adapts
interventions based on three key contextual factors: type, timing, and scale.
By leveraging multimodal behavioral cues (e.g., gaze behavior, typing
hesitation, interaction speed), AI can dynamically adjust cognitive support to
maintain or restore flow. We introduce the concept of cognitive flow, an
extension of flow theory in AI-augmented reasoning, where interventions are
personalized, adaptive, and minimally intrusive. By shifting from static
interventions to context-aware augmentation, our approach ensures that AI
systems support deep engagement in complex decision-making and reasoning
without disrupting cognitive immersion.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [246] [Demand for LLMs: Descriptive Evidence on Substitution, Market Expansion, and Multihoming](https://arxiv.org/abs/2504.15440)
*Andrey Fradkin*

Main category: cs.CY

TL;DR: 论文总结了大型语言模型（LLM）需求的三个特征：快速初始采用、模型发布吸引用户类型差异、多模型同时使用普遍。


<details>
  <summary>Details</summary>
Motivation: 研究LLM市场需求特征，为提供商提供维持需求和定价能力的策略。

Method: 利用OpenRouter市场数据，分析模型采用、用户行为和需求替代模式。

Result: 发现LLM市场存在水平和垂直差异化，提供商可通过差异化策略应对技术进步。

Conclusion: LLM市场需求多样化，提供商需差异化策略以保持竞争力。

Abstract: This paper documents three stylized facts about the demand for Large Language
Models (LLMs) using data from OpenRouter, a prominent LLM marketplace. First,
new models experience rapid initial adoption that stabilizes within weeks.
Second, model releases differ substantially in whether they primarily attract
new users or substitute demand from competing models. Third, multihoming, using
multiple models simultaneously, is common among apps. These findings suggest
significant horizontal and vertical differentiation in the LLM market, implying
opportunities for providers to maintain demand and pricing power despite rapid
technological advances.

</details>


### [247] [Trends in AI Supercomputers](https://arxiv.org/abs/2504.16026)
*Konstantin F. Pilz,James Sanders,Robi Rahman,Lennart Heim*

Main category: cs.CY

TL;DR: 分析了2019至2025年500个AI超级计算机的数据，发现计算性能每9个月翻倍，硬件成本和能耗每年翻倍。到2025年，领先系统xAI的Colossus使用20万AI芯片，成本70亿美元，能耗300MW。企业和美国主导市场。


<details>
  <summary>Details</summary>
Motivation: 研究AI超级计算机的发展趋势，为政策制定者提供资源需求、所有权和国家竞争力的评估依据。

Method: 创建并分析2019至2025年500个AI超级计算机的数据集，涵盖性能、能耗、成本、所有权和全球分布。

Result: 计算性能每9个月翻倍，硬件成本和能耗每年翻倍；企业和美国主导市场；2030年领先系统预计性能达2×10²² FLOP/s，成本2000亿美元，能耗9GW。

Conclusion: AI超级计算机发展迅速，企业和美国占据主导地位，未来趋势需政策关注资源分配和竞争力。

Abstract: Frontier AI development relies on powerful AI supercomputers, yet analysis of
these systems is limited. We create a dataset of 500 AI supercomputers from
2019 to 2025 and analyze key trends in performance, power needs, hardware cost,
ownership, and global distribution. We find that the computational performance
of AI supercomputers has doubled every nine months, while hardware acquisition
cost and power needs both doubled every year. The leading system in March 2025,
xAI's Colossus, used 200,000 AI chips, had a hardware cost of \$7B, and
required 300 MW of power, as much as 250,000 households. As AI supercomputers
evolved from tools for science to industrial machines, companies rapidly
expanded their share of total AI supercomputer performance, while the share of
governments and academia diminished. Globally, the United States accounts for
about 75% of total performance in our dataset, with China in second place at
15%. If the observed trends continue, the leading AI supercomputer in 2030 will
achieve $2\times10^{22}$ 16-bit FLOP/s, use two million AI chips, have a
hardware cost of \$200 billion, and require 9 GW of power. Our analysis
provides visibility into the AI supercomputer landscape, allowing policymakers
to assess key AI trends like resource needs, ownership, and national
competitiveness.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [248] [VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation](https://arxiv.org/abs/2504.15659)
*Anjiang Wei,Huanmi Tan,Tarun Suresh,Daniel Mendoza,Thiago S. F. X. Teixeira,Ke Wang,Caroline Trippel,Alex Aiken*

Main category: cs.AR

TL;DR: VERICODER是一个针对RTL代码生成的模型，通过功能验证的数据集微调，显著提升了功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有RTL数据集多关注语法有效性而非功能验证，导致生成的代码可能不符合预期行为。

Method: 结合单元测试生成与反馈导向的优化方法，构建功能验证的数据集，并微调模型。

Result: VERICODER在功能正确性上达到最佳性能，相对提升高达71.7%和27.4%。

Conclusion: 功能验证的高质量数据集对RTL代码生成至关重要。

Abstract: Recent advances in Large Language Models (LLMs) have sparked growing interest
in applying them to Electronic Design Automation (EDA) tasks, particularly
Register Transfer Level (RTL) code generation. While several RTL datasets have
been introduced, most focus on syntactic validity rather than functional
validation with tests, leading to training examples that compile but may not
implement the intended behavior. We present VERICODER, a model for RTL code
generation fine-tuned on a dataset validated for functional correctness. This
fine-tuning dataset is constructed using a novel methodology that combines unit
test generation with feedback-directed refinement. Given a natural language
specification and an initial RTL design, we prompt a teacher model
(GPT-4o-mini) to generate unit tests and iteratively revise the RTL design
based on its simulation results using the generated tests. If necessary, the
teacher model also updates the tests to ensure they comply with the natural
language specification. As a result of this process, every example in our
dataset is functionally validated, consisting of a natural language
description, an RTL implementation, and passing tests. Fine-tuned on this
dataset of over 125,000 examples, VERICODER achieves state-of-the-art metrics
in functional correctness on VerilogEval and RTLLM, with relative gains of up
to 71.7% and 27.4% respectively. An ablation study further shows that models
trained on our functionally validated dataset outperform those trained on
functionally non-validated datasets, underscoring the importance of
high-quality datasets in RTL code generation.

</details>


### [249] [Insights from Verification: Training a Verilog Generation LLM with Reinforcement Learning with Testbench Feedback](https://arxiv.org/abs/2504.15804)
*Ning Wang,Bingkun Yao,Jie Zhou,Yuchen Hu,Xi Wang,Nan Guan,Zhe Jiang*

Main category: cs.AR

TL;DR: 论文提出了一种通过集成测试台验证信息来训练Verilog生成LLMs的方法，以提升生成代码的功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在Verilog生成中表现良好，但功能正确性验证不足，缺乏足够的验证数据。

Method: 引入自动测试台生成流程，利用VCS反馈减少幻觉，并通过RL（DPO）基于测试台结果训练偏好对。

Result: 在多个数据集上，该方法在生成功能正确的Verilog代码方面优于现有基线。

Conclusion: 该方法有效提升了Verilog生成的功能正确性，相关代码和数据已开源。

Abstract: Large language models (LLMs) have shown strong performance in Verilog
generation from natural language description. However, ensuring the functional
correctness of the generated code remains a significant challenge. This paper
introduces a method that integrates verification insights from testbench into
the training of Verilog generation LLMs, aligning the training with the
fundamental goal of hardware design: functional correctness. The main obstacle
in using LLMs for Verilog code generation is the lack of sufficient functional
verification data, particularly testbenches paired with design specifications
and code. To address this problem, we introduce an automatic testbench
generation pipeline that decomposes the process and uses feedback from the
Verilog compiler simulator (VCS) to reduce hallucination and ensure
correctness. We then use the testbench to evaluate the generated codes and
collect them for further training, where verification insights are introduced.
Our method applies reinforcement learning (RL), specifically direct preference
optimization (DPO), to align Verilog code generation with functional
correctness by training preference pairs based on testbench outcomes. In
evaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2,
and VerilogEval v2, our approach consistently outperforms state-of-the-art
baselines in generating functionally correct Verilog code. We open source all
training code, data, and models at
https://anonymous.4open.science/r/VeriPrefer-E88B.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [250] [SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing Workflow](https://arxiv.org/abs/2504.09697)
*Kenan Tang,Yanhong Li,Yao Qin*

Main category: cs.GR

TL;DR: SPICE是一种无需训练的流程，结合基础扩散模型和Canny边缘ControlNet模型，支持高分辨率、多步骤编辑，并在语义、风格和结构编辑任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的图像编辑模型在局部编辑、详细提示遵循和多步骤全局图像质量保持方面表现不足。

Method: SPICE结合基础扩散模型和Canny边缘ControlNet模型，支持任意分辨率和长宽比，实现自由形式编辑。

Result: SPICE在语义、风格和结构编辑任务中表现最佳，用户评价优于现有方法。

Conclusion: SPICE为图像编辑提供了高效、灵活的解决方案，支持进一步研究和艺术探索。

Abstract: Recent prompt-based image editing models have demonstrated impressive
prompt-following capability at structural editing tasks. However, existing
models still fail to perform local edits, follow detailed editing prompts, or
maintain global image quality beyond a single editing step. To address these
challenges, we introduce SPICE, a training-free workflow that accepts arbitrary
resolutions and aspect ratios, accurately follows user requirements, and
improves image quality consistently during more than 100 editing steps. By
synergizing the strengths of a base diffusion model and a Canny edge ControlNet
model, SPICE robustly handles free-form editing instructions from the user.
SPICE outperforms state-of-the-art baselines on a challenging realistic
image-editing dataset consisting of semantic editing (object addition, removal,
replacement, and background change), stylistic editing (texture changes), and
structural editing (action change) tasks. Not only does SPICE achieve the
highest quantitative performance according to standard evaluation metrics, but
it is also consistently preferred by users over existing image-editing methods.
We release the workflow implementation for popular diffusion model Web UIs to
support further research and artistic exploration.

</details>


### [251] [Vision6D: 3D-to-2D Interactive Visualization and Annotation Tool for 6D Pose Estimation](https://arxiv.org/abs/2504.15329)
*Yike Zhang,Eduardo Davalos,Jack Noble*

Main category: cs.GR

TL;DR: Vision6D是一个交互式3D到2D可视化与标注工具，用于支持6D姿态估计研究，提供直观的3D界面和视觉提示，帮助用户准确标注物体姿态。


<details>
  <summary>Details</summary>
Motivation: 6D姿态估计在机器人辅助任务中至关重要，但现有工具缺乏交互性和直观性，Vision6D填补了这一空白。

Method: 开发了一个交互式工具，结合3D可视化与2D场景投影，支持用户通过视觉提示和空间关系标注6D姿态。

Result: 通过用户研究和与开源数据集的对比验证，Vision6D能生成准确的姿态标注。

Conclusion: Vision6D为6D姿态估计研究提供了高效、直观的解决方案，并开源以促进社区发展。

Abstract: Accurate 6D pose estimation has gained more attention over the years for
robotics-assisted tasks that require precise interaction with physical objects.
This paper presents an interactive 3D-to-2D visualization and annotation tool
to support the 6D pose estimation research community. To the best of our
knowledge, the proposed work is the first tool that allows users to visualize
and manipulate 3D objects interactively on a 2D real-world scene, along with a
comprehensive user study. This system supports robust 6D camera pose annotation
by providing both visual cues and spatial relationships to determine object
position and orientation in various environments. The annotation feature in
Vision6D is particularly helpful in scenarios where the transformation matrix
between the camera and world objects is unknown, as it enables accurate
annotation of these objects' poses using only the camera intrinsic matrix. This
capability serves as a foundational step in developing and training advanced
pose estimation models across various domains. We evaluate Vision6D's
effectiveness by utilizing widely-used open-source pose estimation datasets
Linemod and HANDAL through comparisons between the default ground-truth camera
poses with manual annotations. A user study was performed to show that Vision6D
generates accurate pose annotations via visual cues in an intuitive 3D user
interface. This approach aims to bridge the gap between 2D scene projections
and 3D scenes, offering an effective way for researchers and developers to
solve 6D pose annotation related problems. The software is open-source and
publicly available at https://github.com/InteractiveGL/vision6D.

</details>


### [252] [Neural Kinematic Bases for Fluids](https://arxiv.org/abs/2504.15657)
*Yibo Liu,Paul Kry,Kenny Erleben,Noam Aigerman,Sune Darkner,Teseo Schneider*

Main category: cs.GR

TL;DR: 提出了一种基于MLP的网格无关流体模拟方法，通过设计损失函数确保神经基满足正交性、无散度、边界对齐和平滑性等物理特性。


<details>
  <summary>Details</summary>
Motivation: 传统流体模拟方法通常需要复杂的网格处理，而基于神经网络的基函数可以更灵活地适应不同域和维度。

Method: 使用MLP表示速度场，设计损失函数确保基函数满足物理特性，并支持实时动画。

Result: 神经基能够拟合输入流场草图，并继承基函数的物理特性，适用于不同域和三维扩展。

Conclusion: 该方法提供了一种高效、灵活的流体模拟方案，适用于实时动画和多维场景。

Abstract: We propose mesh-free fluid simulations that exploit a kinematic neural basis
for velocity fields represented by an MLP. We design a set of losses that
ensures that these neural bases satisfy fundamental physical properties such as
orthogonality, divergence-free, boundary alignment, and smoothness. Our neural
bases can then be used to fit an input sketch of a flow, which will inherit the
same fundamental properties from the bases. We then can animate such flow in
real-time using standard time integrators. Our neural bases can accommodate
different domains and naturally extend to three dimensions.

</details>


### [253] [Low-Rank Adaptation of Neural Fields](https://arxiv.org/abs/2504.15933)
*Anh Truong,Ahmed H. Mahmoud,Mina Konaković Luković,Justin Solomon*

Main category: cs.GR

TL;DR: 提出了一种基于低秩适应（LoRA）的参数高效策略，用于更新神经场（NF），适用于低计算硬件。


<details>
  <summary>Details</summary>
Motivation: 神经场的小变化编码问题未得到足够关注，现有方法如法线贴图和视频压缩虽能高效编码冗余变化，但不适用于神经场。

Method: 将LoRA方法从参数高效微调LLM社区引入，用于实例特定的神经场更新，避免依赖大型预训练模型。

Result: 在图像滤波、视频压缩和几何编辑实验中验证了方法的有效性和多功能性。

Conclusion: LoRA策略为神经场的小变化编码提供了一种高效且通用的解决方案。

Abstract: Processing visual data often involves small adjustments or sequences of
changes, such as in image filtering, surface smoothing, and video storage.
While established graphics techniques like normal mapping and video compression
exploit redundancy to encode such small changes efficiently, the problem of
encoding small changes to neural fields (NF) -- neural network
parameterizations of visual or physical functions -- has received less
attention.
  We propose a parameter-efficient strategy for updating neural fields using
low-rank adaptations (LoRA). LoRA, a method from the parameter-efficient
fine-tuning LLM community, encodes small updates to pre-trained models with
minimal computational overhead. We adapt LoRA to instance-specific neural
fields, avoiding the need for large pre-trained models yielding a pipeline
suitable for low-compute hardware.
  We validate our approach with experiments in image filtering, video
compression, and geometry editing, demonstrating its effectiveness and
versatility for representing neural field updates.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [254] [On the Boolean Network Theory of Datalog$^\neg$](https://arxiv.org/abs/2504.15417)
*Van-Giang Trinh,Belaid Benhamou,Sylvain Soliman,François Fages*

Main category: cs.LO

TL;DR: 本文建立了Datalog$^\neg$与布尔网络理论之间的形式化联系，证明了在无奇循环时正则模型与稳定模型重合，无偶循环时稳定偏模型的唯一性，并给出了模型数量的上界。


<details>
  <summary>Details</summary>
Motivation: 研究Datalog$^\neg$的模型理论与布尔网络理论的联系，以解决正则模型与稳定模型的关系问题，并修正前人工作中的错误。

Method: 利用布尔网络理论的结果，分析Datalog$^\neg$程序的奇偶循环对模型的影响，并引入反馈顶点集计算模型数量的上界。

Result: 证明了无奇循环时正则模型与稳定模型重合，无偶循环时稳定偏模型唯一，并给出了模型数量的上界。

Conclusion: 通过布尔网络理论，本文不仅修正了前人工作中的问题，还为Datalog$^\neg$提供了新的语义解释和模型分析方法。

Abstract: Datalog$^\neg$ is a central formalism used in a variety of domains ranging
from deductive databases and abstract argumentation frameworks to answer set
programming. Its model theory is the finite counterpart of the logical
semantics developed for normal logic programs, mainly based on the notions of
Clark's completion and two-valued or three-valued canonical models including
supported, stable, regular and well-founded models. In this paper we establish
a formal link between Datalog$^\neg$ and Boolean network theory, which was
initially introduced by Stuart Kaufman and Ren\'e Thomas to reason about gene
regulatory networks. We use previous results from Boolean network theory to
prove that in the absence of odd cycles in a Datalog$^\neg$ program, the
regular models coincide with the stable models, which entails the existence of
stable models, and in the absence of even cycles, we show the uniqueness of
stable partial models, which entails the uniqueness of regular models. These
results on regular models have been claimed by You and Yuan in 1994 for normal
logic programs but we show problems in their definition of well-founded
stratification and in their proofs that we can fix for negative normal logic
programs only. We also give upper bounds on the numbers of stable partial,
regular, and stable models of a Datalog$^\neg$ program using the cardinality of
a feedback vertex set in its atom dependency graph. Interestingly, our
connection to Boolean network theory also points us to the notion of trap
spaces for Datalog$^\neg$ programs. We relate the notions of supported or
stable trap spaces to the other semantics of Datalog$^\neg$, and show the
equivalence between subset-minimal stable trap spaces and regular models.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [255] [Full waveform inversion with CNN-based velocity representation extension](https://arxiv.org/abs/2504.15826)
*Xinru Mu,Omar M. Saad,Tariq Alkhalifah*

Main category: physics.geo-ph

TL;DR: 论文提出了一种结合卷积神经网络（CNN）的全波形反演（FWI）方法（VRE-FWI），通过减少噪声和提高速度梯度准确性，提升了反演精度。


<details>
  <summary>Details</summary>
Motivation: 数值模拟中的离散化误差和不完整的地震数据采集会引入噪声，影响速度梯度的准确性，从而降低FWI的反演精度。

Method: 使用CNN在正向模拟前优化速度模型，减少噪声并提供更准确的速度更新方向；提出两种实现方案，均扩展了速度表示（VRE）。

Result: 合成和实际数据测试表明，VRE-FWI比传统FWI具有更高的反演精度，仅增加约1%的计算成本。

Conclusion: VRE-FWI通过结合CNN，有效提升了FWI的反演精度，且计算成本增加极小。

Abstract: Full waveform inversion (FWI) updates the velocity model by minimizing the
discrepancy between observed and simulated data. However, discretization errors
in numerical modeling and incomplete seismic data acquisition can introduce
noise, which propagates through the adjoint operator and affects the accuracy
of the velocity gradient, thereby impacting the FWI inversion accuracy. To
mitigate the influence of noise on the gradient, we employ a convolutional
neural network (CNN) to refine the velocity model before performing the forward
simulation, aiming to reduce noise and provide a more accurate velocity update
direction. We use the same data misfit loss to update both the velocity and
network parameters, thereby forming a self-supervised learning procedure. We
propose two implementation schemes, which differ in whether the velocity update
passes through the CNN. In both methodologies, the velocity representation is
extended (VRE) by using a neural network in addition to the grid-based
velocities. Thus, we refer to this general approach as VRE-FWI. Synthetic and
real data tests demonstrate that the proposed VRE-FWI achieves higher velocity
inversion accuracy compared to traditional FWI, at a marginal additional
computational cost of approximately 1%.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [256] [A biologically Inspired Trust Model for Open Multi-Agent Systems that is Resilient to Rapid Performance Fluctuations](https://arxiv.org/abs/2504.15301)
*Zoi Lygizou,Dimitris Kalles*

Main category: cs.MA

TL;DR: 本文提出了一种基于生物启发的信任模型，通过本地存储信任数据解决代理移动性、行为变化和冷启动问题，并进一步改进算法以应对动态环境中的性能波动。


<details>
  <summary>Details</summary>
Motivation: 传统加密方法在开放、动态、分布式多代理系统中不实用，现有信任模型面临代理移动性、行为变化和冷启动问题。

Method: 引入生物启发的信任模型，结合自分类机制检测服务提供者的性能下降。

Result: 新算法在动态环境中表现优于原模型和FIRE模型，适应性更强。

Conclusion: 研究全面评估了模型性能，提出了未来研究方向。

Abstract: Trust management provides an alternative solution for securing open, dynamic,
and distributed multi-agent systems, where conventional cryptographic methods
prove to be impractical. However, existing trust models face challenges related
to agent mobility, changing behaviors, and the cold start problem. To address
these issues we introduced a biologically inspired trust model in which
trustees assess their own capabilities and store trust data locally. This
design improves mobility support, reduces communication overhead, resists
disinformation, and preserves privacy. Despite these advantages, prior
evaluations revealed limitations of our model in adapting to provider
population changes and continuous performance fluctuations. This study proposes
a novel algorithm, incorporating a self-classification mechanism for providers
to detect performance drops potentially harmful for the service consumers.
Simulation results demonstrate that the new algorithm outperforms its original
version and FIRE, a well-known trust and reputation model, particularly in
handling dynamic trustee behavior. While FIRE remains competitive under extreme
environmental changes, the proposed algorithm demonstrates greater adaptability
across various conditions. In contrast to existing trust modeling research,
this study conducts a comprehensive evaluation of our model using widely
recognized trust model criteria, assessing its resilience against common
trust-related attacks while identifying strengths, weaknesses, and potential
countermeasures. Finally, several key directions for future research are
proposed.

</details>


### [257] [The Formation of Production Networks: How Supply Chains Arise from Simple Learning with Minimal Information](https://arxiv.org/abs/2504.16010)
*Tuong Manh Vu,Ernesto Carrella,Robert Axtell,Omar A. Guerrero*

Main category: cs.MA

TL;DR: 论文提出了一种模型，企业通过强化学习在不确定性中优化价格、产量和投入，内生形成稳态生产网络，无需均衡或完全技术知识的假设。


<details>
  <summary>Details</summary>
Motivation: 研究企业如何在不确定环境中通过学习适应冲击（如需求变化、供应商关闭等），并内生形成生产网络。

Method: 采用强化学习方法，企业根据异质技术调整价格、产量和投入，动态适应冲击。

Result: 模型展示了需求和生产力冲击对上下游的影响，企业能有效重塑生产网络。

Conclusion: 该模型为分析生产网络的动态适应性和冲击影响提供了新工具。

Abstract: We develop a model where firms determine the price at which they sell their
differentiable goods, the volume that they produce, and the inputs (types and
amounts) that they purchase from other firms. A steady-state production network
emerges endogenously without resorting to assumptions such as equilibrium or
perfect knowledge about production technologies. Through a simple version of
reinforcement learning, firms with heterogeneous technologies cope with
uncertainty and maximize profits. Due to this learning process, firms can adapt
to shocks such as demand shifts, suppliers/clients closure, productivity
changes, and production technology modifications; effectively reshaping the
production network. To demonstrate the potential of this model, we analyze the
upstream and downstream impact of demand and productivity shocks.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [258] [Real-Time Sentiment Insights from X Using VADER, DistilBERT, and Web-Scraped Data](https://arxiv.org/abs/2504.15448)
*Yanampally Abhiram Reddy,Siddhi Agarwal,Vikram Parashar,Arshiya Arora*

Main category: econ.GN

TL;DR: 该论文提出了一种结合NLP和机器学习的实时企业声誉监测系统，通过混合情感检测框架分析社交媒体数据，揭示不同企业的公众情感差异。


<details>
  <summary>Details</summary>
Motivation: 在社交媒体时代，了解公众对企业的情感对投资者、政策制定者和研究者至关重要。

Method: 采用混合情感检测框架（VADER和DistilBERT），结合数据预处理和集成分类方法，分析多平台社交媒体数据。

Result: 结果显示企业间情感差异显著，如亚马逊（81.2）和三星（45.8）情感积极，微软（21.7）和沃尔玛（21.9）情感消极。

Conclusion: 该框架为利益相关者提供了基于全面情感分析的可操作见解，助力战略决策。

Abstract: In the age of social media, understanding public sentiment toward major
corporations is crucial for investors, policymakers, and researchers. This
paper presents a comprehensive sentiment analysis system tailored for corporate
reputation monitoring, combining Natural Language Processing (NLP) and machine
learning techniques to accurately interpret public opinion in real time. The
methodology integrates a hybrid sentiment detection framework leveraging both
rule-based models (VADER) and transformer-based deep learning models
(DistilBERT), applied to social media data from multiple platforms. The system
begins with robust preprocessing involving noise removal and text
normalization, followed by sentiment classification using an ensemble approach
to ensure both interpretability and contextual accuracy. Results are visualized
through sentiment distribution plots, comparative analyses, and temporal
sentiment trends for enhanced interpretability. Our analysis reveals
significant disparities in public sentiment across major corporations, with
companies like Amazon (81.2) and Samsung (45.8) receiving excellent sentiment
scores, while Microsoft (21.7) and Walmart (21.9) exhibit poor sentiment
profiles. These findings demonstrate the utility of our multi-source sentiment
framework in providing actionable insights regarding corporate public
perception, enabling stakeholders to make informed strategic decisions based on
comprehensive sentiment analysis.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [259] [Real-Time Optimal Design of Experiment for Parameter Identification of Li-Ion Cell Electrochemical Model](https://arxiv.org/abs/2504.15578)
*Ian Mikesell,Samuel Filgueira da Silva,Mehmet Fatih Ozkan,Faissal El Idrissi,Prashanth Ramesh,Marcello Canova*

Main category: eess.SY

TL;DR: 本文提出了一种基于强化学习（RL）的方法，动态调整锂离子电池（LiB）的电流曲线，以优化电化学模型的参数可识别性。通过硬件在环（HIL）验证，该方法在减少建模误差和缩短实验时间上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统锂离子电池参数识别方法需要大量数据收集实验，且缺乏动态环境适应性。为提高模型精度和预测能力，需开发更高效的方法。

Method: 采用强化学习（RL）动态调整电流曲线，结合硬件在环（HIL）实时验证，优化参数识别。

Result: HIL验证表明，RL方法在减少建模误差和缩短实验时间上优于传统协议。

Conclusion: RL方法为锂离子电池参数识别提供了高效、自适应的解决方案，显著优于传统方法。

Abstract: Accurately identifying the parameters of electrochemical models of li-ion
battery (LiB) cells is a critical task for enhancing the fidelity and
predictive ability. Traditional parameter identification methods often require
extensive data collection experiments and lack adaptability in dynamic
environments. This paper describes a Reinforcement Learning (RL) based approach
that dynamically tailors the current profile applied to a LiB cell to optimize
the parameters identifiability of the electrochemical model. The proposed
framework is implemented in real-time using a Hardware-in-the-Loop (HIL) setup,
which serves as a reliable testbed for evaluating the RL-based design strategy.
The HIL validation confirms that the RL-based experimental design outperforms
conventional test protocols used for parameter identification in terms of both
reducing the modeling errors on a verification test and minimizing the duration
of the experiment used for parameter identification.

</details>
