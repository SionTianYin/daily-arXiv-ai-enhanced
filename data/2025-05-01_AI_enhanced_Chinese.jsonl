{"id": "2504.21012", "pdf": "https://arxiv.org/pdf/2504.21012", "abs": "https://arxiv.org/abs/2504.21012", "authors": ["Makoto Sato"], "title": "Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "What underlies intuitive human thinking? One approach to this question is to\ncompare the cognitive dynamics of humans and large language models (LLMs).\nHowever, such a comparison requires a method to quantitatively analyze AI\ncognitive behavior under controlled conditions. While anecdotal observations\nsuggest that certain prompts can dramatically change LLM behavior, these\nobservations have remained largely qualitative. Here, we propose a two-part\nframework to investigate this phenomenon: a Transition-Inducing Prompt (TIP)\nthat triggers a rapid shift in LLM responsiveness, and a Transition Quantifying\nPrompt (TQP) that evaluates this change using a separate LLM. Through\ncontrolled experiments, we examined how LLMs react to prompts embedding two\nsemantically distant concepts (e.g., mathematical aperiodicity and traditional\ncrafts)--either fused together or presented separately--by changing their\nlinguistic quality and affective tone. Whereas humans tend to experience\nheightened engagement when such concepts are meaningfully blended producing a\nnovel concept--a form of conceptual fusion--current LLMs showed no significant\ndifference in responsiveness between semantically fused and non-fused prompts.\nThis suggests that LLMs may not yet replicate the conceptual integration\nprocesses seen in human intuition. Our method enables fine-grained,\nreproducible measurement of cognitive responsiveness, and may help illuminate\nkey differences in how intuition and conceptual leaps emerge in artificial\nversus human minds.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff08TIP\u548cTQP\uff09\u6765\u91cf\u5316\u5206\u6790LLM\u7684\u8ba4\u77e5\u884c\u4e3a\uff0c\u53d1\u73b0LLM\u5728\u8bed\u4e49\u878d\u5408\u63d0\u793a\u4e0b\u4e0e\u4eba\u7c7b\u4e0d\u540c\uff0c\u7f3a\u4e4f\u6982\u5ff5\u6574\u5408\u80fd\u529b\u3002", "motivation": "\u63a2\u7a76\u4eba\u7c7b\u76f4\u89c9\u601d\u7ef4\u7684\u5e95\u5c42\u673a\u5236\uff0c\u901a\u8fc7\u6bd4\u8f83\u4eba\u7c7b\u4e0eLLM\u7684\u8ba4\u77e5\u52a8\u6001\uff0c\u63ed\u793a\u5176\u5dee\u5f02\u3002", "method": "\u63d0\u51faTIP\u89e6\u53d1LLM\u884c\u4e3a\u53d8\u5316\uff0cTQP\u91cf\u5316\u8bc4\u4f30\u53d8\u5316\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u8bed\u4e49\u878d\u5408\u4e0e\u975e\u878d\u5408\u63d0\u793a\u5bf9LLM\u7684\u5f71\u54cd\u3002", "result": "LLM\u5728\u8bed\u4e49\u878d\u5408\u63d0\u793a\u4e0b\u672a\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u54cd\u5e94\u5dee\u5f02\uff0c\u7f3a\u4e4f\u6982\u5ff5\u6574\u5408\u80fd\u529b\u3002", "conclusion": "LLM\u5c1a\u672a\u590d\u5236\u4eba\u7c7b\u76f4\u89c9\u4e2d\u7684\u6982\u5ff5\u6574\u5408\u8fc7\u7a0b\uff0c\u8be5\u65b9\u6cd5\u4e3a\u91cf\u5316\u8ba4\u77e5\u54cd\u5e94\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2504.21013", "pdf": "https://arxiv.org/pdf/2504.21013", "abs": "https://arxiv.org/abs/2504.21013", "authors": ["Antoun Yaacoub", "Zainab Assaghir", "Lionel Prevost", "J\u00e9r\u00f4me Da-Rugna"], "title": "Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge", "categories": ["cs.CL", "cs.AI"], "comment": "This paper will be presented in the 9th Int. Conf. on Computer,\n  Software and Modeling (ICCSM 2025), Roma, Italy, 2025, July 3-5", "summary": "Artificial Intelligence (AI)-generated feedback in educational settings has\ngarnered considerable attention due to its potential to enhance learning\noutcomes. However, a comprehensive understanding of the linguistic\ncharacteristics of AI-generated feedback, including readability, lexical\nrichness, and adaptability across varying challenge levels, remains limited.\nThis study delves into the linguistic and structural attributes of feedback\ngenerated by Google's Gemini 1.5-flash text model for computer science\nmultiple-choice questions (MCQs). A dataset of over 1,200 MCQs was analyzed,\nconsidering three difficulty levels (easy, medium, hard) and three feedback\ntones (supportive, neutral, challenging). Key linguistic metrics, such as\nlength, readability scores (Flesch-Kincaid Grade Level), vocabulary richness,\nand lexical density, were computed and examined. A fine-tuned RoBERTa-based\nmulti-task learning (MTL) model was trained to predict these linguistic\nproperties, achieving a Mean Absolute Error (MAE) of 2.0 for readability and\n0.03 for vocabulary richness. The findings reveal significant interaction\neffects between feedback tone and question difficulty, demonstrating the\ndynamic adaptation of AI-generated feedback within diverse educational\ncontexts. These insights contribute to the development of more personalized and\neffective AI-driven feedback mechanisms, highlighting the potential for\nimproved learning outcomes while underscoring the importance of ethical\nconsiderations in their design and deployment.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86Google Gemini 1.5-flash\u6587\u672c\u6a21\u578b\u751f\u6210\u7684\u8ba1\u7b97\u673a\u79d1\u5b66\u591a\u9009\u9898\u53cd\u9988\u7684\u8bed\u8a00\u7279\u5f81\uff0c\u5305\u62ec\u53ef\u8bfb\u6027\u3001\u8bcd\u6c47\u4e30\u5bcc\u5ea6\u548c\u9002\u5e94\u6027\uff0c\u63ed\u793a\u4e86\u53cd\u9988\u8bed\u6c14\u4e0e\u9898\u76ee\u96be\u5ea6\u4e4b\u95f4\u7684\u52a8\u6001\u4ea4\u4e92\u4f5c\u7528\u3002", "motivation": "AI\u751f\u6210\u7684\u53cd\u9988\u5728\u6559\u80b2\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5bf9\u5176\u8bed\u8a00\u7279\u5f81\u7684\u5168\u9762\u7406\u89e3\u4ecd\u6709\u9650\u3002", "method": "\u5206\u6790\u4e861,200\u591a\u9053\u591a\u9009\u9898\u7684\u53cd\u9988\uff0c\u8ba1\u7b97\u4e86\u8bed\u8a00\u6307\u6807\uff08\u5982\u53ef\u8bfb\u6027\u3001\u8bcd\u6c47\u4e30\u5bcc\u5ea6\uff09\uff0c\u5e76\u8bad\u7ec3\u4e86RoBERTa\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u8fd9\u4e9b\u6307\u6807\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u53ef\u8bfb\u6027\u548c\u8bcd\u6c47\u4e30\u5bcc\u5ea6\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u5206\u522b\u4e3a2.0\u548c0.03\uff0c\u53d1\u73b0\u53cd\u9988\u8bed\u6c14\u4e0e\u9898\u76ee\u96be\u5ea6\u6709\u663e\u8457\u4ea4\u4e92\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u4e2a\u6027\u5316AI\u53cd\u9988\u673a\u5236\u63d0\u4f9b\u4e86\u4f9d\u636e\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u4e2d\u7684\u4f26\u7406\u8003\u91cf\u3002"}}
{"id": "2504.21016", "pdf": "https://arxiv.org/pdf/2504.21016", "abs": "https://arxiv.org/abs/2504.21016", "authors": ["Ngoc C. L\u00ea", "Hai-Chung Nguyen-Phung", "Thu-Huong Pham Thi", "Hue Vu", "Phuong-Thao Nguyen Thi", "Thu-Thuy Tran", "Hong-Nhung Le Thi", "Thuy-Duong Nguyen-Thi", "Thanh-Huy Nguyen"], "title": "Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments", "categories": ["cs.CL", "cs.LG"], "comment": "8 pages. AI4SG-21 The 3rd Workshop on Artificial Intelligence for\n  Social Good at IJCAI 2021", "summary": "The COVID-19 pandemic caused great losses worldwide, efforts are taken place\nto prevent but many countries have failed. In Vietnam, the traceability,\nlocalization, and quarantine of people who contact with patients contribute to\neffective disease prevention. However, this is done by hand, and take a lot of\nwork. In this research, we describe a named-entity recognition (NER) study that\nassists in the prevention of COVID-19 pandemic in Vietnam. We also present our\nmanually annotated COVID-19 dataset with nested named entity recognition task\nfor Vietnamese which be defined new entity types using for our system.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u8f85\u52a9\u8d8a\u5357\u7684COVID-19\u75ab\u60c5\u9632\u63a7\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u8d8a\u5357\u8bed\u5d4c\u5957\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6570\u636e\u96c6\u3002", "motivation": "COVID-19\u75ab\u60c5\u5168\u7403\u8513\u5ef6\uff0c\u8d8a\u5357\u901a\u8fc7\u8ffd\u8e2a\u3001\u5b9a\u4f4d\u548c\u9694\u79bb\u63a5\u89e6\u8005\u6709\u6548\u9632\u63a7\uff0c\u4f46\u624b\u52a8\u64cd\u4f5c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u91c7\u7528\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6280\u672f\uff0c\u6784\u5efa\u65b0\u7684\u5b9e\u4f53\u7c7b\u578b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u8d8a\u5357\u8bed\u5d4c\u5957\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6570\u636e\u96c6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9002\u7528\u4e8e\u8d8a\u5357COVID-19\u9632\u63a7\u7684NER\u7cfb\u7edf\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8d8a\u5357\u7684\u75ab\u60c5\u8ffd\u8e2a\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u63d0\u9ad8\u4e86\u9632\u63a7\u6548\u7387\u3002"}}
{"id": "2504.21017", "pdf": "https://arxiv.org/pdf/2504.21017", "abs": "https://arxiv.org/abs/2504.21017", "authors": ["Hai-Chung Nguyen-Phung", "Ngoc C. L\u00ea", "Van-Chien Nguyen", "Hang Thi Nguyen", "Thuy Phuong Thi Nguyen"], "title": "ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese", "categories": ["cs.CL", "cs.LG"], "comment": "8 pages. Technical report", "summary": "After two years of appearance, COVID-19 has negatively affected people and\nnormal life around the world. As in May 2022, there are more than 522 million\ncases and six million deaths worldwide (including nearly ten million cases and\nover forty-three thousand deaths in Vietnam). Economy and society are both\nseverely affected. The variant of COVID-19, Omicron, has broken disease\nprevention measures of countries and rapidly increased number of infections.\nResources overloading in treatment and epidemics prevention is happening all\nover the world. It can be seen that, application of artificial intelligence\n(AI) to support people at this time is extremely necessary. There have been\nmany studies applying AI to prevent COVID-19 which are extremely useful, and\nstudies on machine reading comprehension (MRC) are also in it. Realizing that,\nwe created the first MRC dataset about COVID-19 for Vietnamese: ViQA-COVID and\ncan be used to build models and systems, contributing to disease prevention.\nBesides, ViQA-COVID is also the first multi-span extraction MRC dataset for\nVietnamese, we hope that it can contribute to promoting MRC studies in\nVietnamese and multilingual.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u5173\u4e8eCOVID-19\u7684\u8d8a\u5357\u8bed\u591a\u8de8\u5ea6\u62bd\u53d6\u673a\u5668\u9605\u8bfb\u7406\u89e3\u6570\u636e\u96c6ViQA-COVID\uff0c\u65e8\u5728\u652f\u6301\u75be\u75c5\u9884\u9632\u548c\u4fc3\u8fdb\u8d8a\u5357\u8bed\u53ca\u591a\u8bed\u8a00MRC\u7814\u7a76\u3002", "motivation": "COVID-19\u5bf9\u5168\u7403\u7ecf\u6d4e\u548c\u793e\u4f1a\u7684\u4e25\u91cd\u5f71\u54cd\uff0c\u4ee5\u53caAI\u5728\u75be\u75c5\u9884\u9632\u4e2d\u7684\u5fc5\u8981\u6027\uff0c\u4fc3\u4f7f\u4f5c\u8005\u521b\u5efa\u9996\u4e2a\u8d8a\u5357\u8bedMRC\u6570\u636e\u96c6\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86ViQA-COVID\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u8d8a\u5357\u8bed\u7684\u591a\u8de8\u5ea6\u62bd\u53d6MRC\u6570\u636e\u96c6\uff0c\u53ef\u7528\u4e8e\u6784\u5efa\u6a21\u578b\u548c\u7cfb\u7edf\u3002", "result": "ViQA-COVID\u6210\u4e3a\u9996\u4e2a\u652f\u6301\u8d8a\u5357\u8bed\u53ca\u591a\u8bed\u8a00MRC\u7814\u7a76\u7684\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u76f8\u5173\u9886\u57df\u7684\u7a7a\u767d\u3002", "conclusion": "ViQA-COVID\u7684\u521b\u5efa\u4e3aCOVID-19\u9884\u9632\u548c\u8d8a\u5357\u8bedMRC\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u5e76\u6709\u671b\u63a8\u52a8\u591a\u8bed\u8a00MRC\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.21040", "pdf": "https://arxiv.org/pdf/2504.21040", "abs": "https://arxiv.org/abs/2504.21040", "authors": ["Chenyi Cai", "Kosuke Kuriyama", "Youlong Gu", "Filip Biljecki", "Pieter Herthogs"], "title": "Can a Large Language Model Assess Urban Design Quality? Evaluating Walkability Metrics Across Expertise Levels", "categories": ["cs.CV"], "comment": null, "summary": "Urban street environments are vital to supporting human activity in public\nspaces. The emergence of big data, such as street view images (SVIs) combined\nwith multimodal large language models (MLLMs), is transforming how researchers\nand practitioners investigate, measure, and evaluate semantic and visual\nelements of urban environments. Considering the low threshold for creating\nautomated evaluative workflows using MLLMs, it is crucial to explore both the\nrisks and opportunities associated with these probabilistic models. In\nparticular, the extent to which the integration of expert knowledge can\ninfluence the performance of MLLMs in evaluating the quality of urban design\nhas not been fully explored. This study sets out an initial exploration of how\nintegrating more formal and structured representations of expert urban design\nknowledge into the input prompts of an MLLM (ChatGPT-4) can enhance the model's\ncapability and reliability in evaluating the walkability of built environments\nusing SVIs. We collect walkability metrics from the existing literature and\ncategorize them using relevant ontologies. We then select a subset of these\nmetrics, focusing on the subthemes of pedestrian safety and attractiveness, and\ndevelop prompts for the MLLM accordingly. We analyze the MLLM's ability to\nevaluate SVI walkability subthemes through prompts with varying levels of\nclarity and specificity regarding evaluation criteria. Our experiments\ndemonstrate that MLLMs are capable of providing assessments and interpretations\nbased on general knowledge and can support the automation of multimodal\nimage-text evaluations. However, they generally provide more optimistic scores\nand can make mistakes when interpreting the provided metrics, resulting in\nincorrect evaluations. By integrating expert knowledge, the MLLM's evaluative\nperformance exhibits higher consistency and concentration.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u6574\u5408\u4e13\u5bb6\u77e5\u8bc6\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u8bc4\u4f30\u57ce\u5e02\u6b65\u884c\u6027\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u4e13\u5bb6\u77e5\u8bc6\u80fd\u63d0\u9ad8\u6a21\u578b\u7684\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u63a2\u7d22\u4e13\u5bb6\u77e5\u8bc6\u5bf9MLLMs\u5728\u8bc4\u4f30\u57ce\u5e02\u8bbe\u8ba1\u8d28\u91cf\u4e2d\u7684\u5f71\u54cd\uff0c\u4ee5\u63d0\u5347\u5176\u53ef\u9760\u6027\u548c\u80fd\u529b\u3002", "method": "\u6536\u96c6\u6b65\u884c\u6027\u6307\u6807\u5e76\u5206\u7c7b\uff0c\u8bbe\u8ba1\u4e0d\u540c\u6e05\u6670\u5ea6\u548c\u7279\u5f02\u6027\u7684\u63d0\u793a\uff0c\u6d4b\u8bd5MLLM\uff08ChatGPT-4\uff09\u5bf9\u8857\u666f\u56fe\u50cf\u6b65\u884c\u6027\u7684\u8bc4\u4f30\u80fd\u529b\u3002", "result": "MLLMs\u80fd\u57fa\u4e8e\u901a\u7528\u77e5\u8bc6\u63d0\u4f9b\u8bc4\u4f30\uff0c\u4f46\u6613\u8fc7\u4e8e\u4e50\u89c2\u6216\u8bef\u89e3\u6307\u6807\uff1b\u6574\u5408\u4e13\u5bb6\u77e5\u8bc6\u540e\uff0c\u8bc4\u4f30\u8868\u73b0\u66f4\u4e00\u81f4\u548c\u96c6\u4e2d\u3002", "conclusion": "\u4e13\u5bb6\u77e5\u8bc6\u80fd\u663e\u8457\u63d0\u5347MLLMs\u5728\u57ce\u5e02\u8bbe\u8ba1\u8bc4\u4f30\u4e2d\u7684\u6027\u80fd\uff0c\u4f46\u9700\u6ce8\u610f\u5176\u5c40\u9650\u6027\u3002"}}
{"id": "2504.21131", "pdf": "https://arxiv.org/pdf/2504.21131", "abs": "https://arxiv.org/abs/2504.21131", "authors": ["Remo Christen", "Florian Pommerening", "Clemens B\u00fcchner", "Malte Helmert"], "title": "A Formalism for Optimal Search with Dynamic Heuristics", "categories": ["cs.AI"], "comment": null, "summary": "While most heuristics studied in heuristic search depend only on the state,\nsome accumulate information during search and thus also depend on the search\nhistory. Various existing approaches use such dynamic heuristics in\n$\\mathrm{A}^*$-like algorithms and appeal to classic results for $\\mathrm{A}^*$\nto show optimality. However, doing so ignores the complexities of searching\nwith a mutable heuristic. In this paper we formalize the idea of dynamic\nheuristics and use them in a generic algorithm framework. We study a particular\ninstantiation that models $\\mathrm{A}^*$ with dynamic heuristics and show\ngeneral optimality results. Finally we show how existing approaches from\nclassical planning can be viewed as special cases of this instantiation, making\nit possible to directly apply our optimality results.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u52a8\u6001\u542f\u53d1\u5f0f\u5728A*\u7b97\u6cd5\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6700\u4f18\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4f7f\u7528\u52a8\u6001\u542f\u53d1\u5f0f\u65f6\u5ffd\u7565\u4e86\u5176\u590d\u6742\u6027\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7b97\u6cd5\u6846\u67b6\uff0c\u5e76\u5177\u4f53\u5b9e\u4f8b\u5316\u4e3a\u52a8\u6001\u542f\u53d1\u5f0f\u7684A*\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6700\u4f18\u6027\uff0c\u5e76\u5c06\u7ecf\u5178\u89c4\u5212\u65b9\u6cd5\u89c6\u4e3a\u5176\u7279\u4f8b\u3002", "conclusion": "\u672c\u6587\u4e3a\u52a8\u6001\u542f\u53d1\u5f0f\u641c\u7d22\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21047", "pdf": "https://arxiv.org/pdf/2504.21047", "abs": "https://arxiv.org/abs/2504.21047", "authors": ["Klemen Kotar", "Greta Tuckute"], "title": "Model Connectomes: A Generational Approach to Data-Efficient Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Biological neural networks are shaped both by evolution across generations\nand by individual learning within an organism's lifetime, whereas standard\nartificial neural networks undergo a single, large training procedure without\ninherited constraints. In this preliminary work, we propose a framework that\nincorporates this crucial generational dimension - an \"outer loop\" of evolution\nthat shapes the \"inner loop\" of learning - so that artificial networks better\nmirror the effects of evolution and individual learning in biological\norganisms. Focusing on language, we train a model that inherits a \"model\nconnectome\" from the outer evolution loop before exposing it to a\ndevelopmental-scale corpus of 100M tokens. Compared with two closely matched\ncontrol models, we show that the connectome model performs better or on par on\nnatural language processing tasks as well as alignment to human behavior and\nbrain data. These findings suggest that a model connectome serves as an\nefficient prior for learning in low-data regimes - narrowing the gap between\nsingle-generation artificial models and biologically evolved neural networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fdb\u5316\u548c\u4e2a\u4f53\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u2018\u5916\u5faa\u73af\u2019\u8fdb\u5316\u5851\u9020\u2018\u5185\u5faa\u73af\u2019\u5b66\u4e60\uff0c\u4f7f\u4eba\u5de5\u7f51\u7edc\u66f4\u63a5\u8fd1\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u7684\u7279\u6027\u3002\u5728\u8bed\u8a00\u4efb\u52a1\u4e2d\uff0c\u8be5\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u5bf9\u7167\u7ec4\u76f8\u5f53\u3002", "motivation": "\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u8fdb\u5316\u548c\u4e2a\u4f53\u5b66\u4e60\u5f62\u6210\uff0c\u800c\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7f3a\u4e4f\u8fd9\u79cd\u591a\u4ee3\u7ea6\u675f\u3002\u7814\u7a76\u65e8\u5728\u7f29\u5c0f\u4e24\u8005\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5305\u542b\u2018\u5916\u5faa\u73af\u2019\u8fdb\u5316\u548c\u2018\u5185\u5faa\u73af\u2019\u5b66\u4e60\uff0c\u8bad\u7ec3\u6a21\u578b\u7ee7\u627f\u2018\u6a21\u578b\u8fde\u63a5\u7ec4\u2019\u540e\u63a5\u89e6100M\u6807\u8bb0\u7684\u8bed\u6599\u5e93\u3002", "result": "\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u53ca\u4e0e\u4eba\u7c7b\u884c\u4e3a\u548c\u8111\u6570\u636e\u7684\u5bf9\u9f50\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u5bf9\u7167\u7ec4\u76f8\u5f53\u3002", "conclusion": "\u2018\u6a21\u578b\u8fde\u63a5\u7ec4\u2019\u5728\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u4f5c\u4e3a\u9ad8\u6548\u5148\u9a8c\uff0c\u7f29\u5c0f\u4e86\u4eba\u5de5\u6a21\u578b\u4e0e\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u7684\u5dee\u8ddd\u3002"}}
{"id": "2504.21018", "pdf": "https://arxiv.org/pdf/2504.21018", "abs": "https://arxiv.org/abs/2504.21018", "authors": ["Enes \u00d6zeren", "Yihong Liu", "Hinrich Sch\u00fctze"], "title": "HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization", "categories": ["cs.CL", "cs.LG"], "comment": "18 pages, 3 figures, 15 tables", "summary": "Many pre-trained language models (PLMs) exhibit suboptimal performance on\nmid- and low-resource languages, largely due to limited exposure to these\nlanguages during pre-training. A common strategy to address this is to\nintroduce new tokens specific to the target languages, initialize their\nembeddings, and apply continual pre-training on target-language data. Among\nsuch methods, OFA (Liu et al., 2024a) proposes a similarity-based subword\nembedding initialization heuristic that is both effective and efficient.\nHowever, OFA restricts target-language token embeddings to be convex\ncombinations of a fixed number of source-language embeddings, which may limit\nexpressiveness. To overcome this limitation, we propose HYPEROFA, a\nhypernetwork-based approach for more adaptive token embedding initialization.\nThe hypernetwork is trained to map from an external multilingual word vector\nspace to the PLMs token embedding space using source-language tokens. Once\ntrained, it can generate flexible embeddings for target-language tokens,\nserving as a good starting point for continual pretraining. Experiments\ndemonstrate that HYPEROFA consistently outperforms random initialization\nbaseline and matches or exceeds the performance of OFA in both continual\npre-training convergence and downstream task performance. We make the code\npublicly available.", "AI": {"tldr": "HYPEROFA\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u9002\u5e94\u6027\u8bcd\u5d4c\u5165\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u4f18\u4e8e\u968f\u673a\u521d\u59cb\u5316\u548cOFA\u65b9\u6cd5\u3002", "motivation": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982OFA\uff09\u7684\u8bcd\u5d4c\u5165\u521d\u59cb\u5316\u9650\u5236\u4e86\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u8d85\u7f51\u7edc\u4ece\u591a\u8bed\u8a00\u8bcd\u5411\u91cf\u7a7a\u95f4\u6620\u5c04\u5230PLM\u7684\u8bcd\u5d4c\u5165\u7a7a\u95f4\uff0c\u4e3a\u76ee\u6807\u8bed\u8a00\u751f\u6210\u7075\u6d3b\u7684\u8bcd\u5d4c\u5165\u3002", "result": "HYPEROFA\u5728\u6301\u7eed\u9884\u8bad\u7ec3\u6536\u655b\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e0a\u4f18\u4e8e\u968f\u673a\u521d\u59cb\u5316\uff0c\u4e14\u5339\u914d\u6216\u8d85\u8d8aOFA\u3002", "conclusion": "HYPEROFA\u662f\u4e00\u79cd\u6709\u6548\u7684\u9002\u5e94\u6027\u8bcd\u5d4c\u5165\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002"}}
{"id": "2504.21136", "pdf": "https://arxiv.org/pdf/2504.21136", "abs": "https://arxiv.org/abs/2504.21136", "authors": ["Murali Ramanujam", "Yinwei Dai", "Kyle Jamieson", "Ravi Netravali"], "title": "Legilimens: Performant Video Analytics on the System-on-Chip Edge", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Continually retraining models has emerged as a primary technique to enable\nhigh-accuracy video analytics on edge devices. Yet, existing systems employ\nsuch adaptation by relying on the spare compute resources that traditional\n(memory-constrained) edge servers afford. In contrast, mobile edge devices such\nas drones and dashcams offer a fundamentally different resource profile:\nweak(er) compute with abundant unified memory pools. We present Legilimens, a\ncontinuous learning system for the mobile edge's System-on-Chip GPUs. Our\ndriving insight is that visually distinct scenes that require retraining\nexhibit substantial overlap in model embeddings; if captured into a base model\non device memory, specializing to each new scene can become lightweight,\nrequiring very few samples. To practically realize this approach, Legilimens\npresents new, compute-efficient techniques to (1) select high-utility data\nsamples for retraining specialized models, (2) update the base model without\ncomplete retraining, and (3) time-share compute resources between retraining\nand live inference for maximal accuracy. Across diverse workloads, Legilimens\nlowers retraining costs by 2.8-10x compared to existing systems, resulting in\n18-45% higher accuracies.", "AI": {"tldr": "Legilimens\u662f\u4e00\u79cd\u9488\u5bf9\u79fb\u52a8\u8fb9\u7f18\u8bbe\u5907\uff08\u5982\u65e0\u4eba\u673a\u548c\u884c\u8f66\u8bb0\u5f55\u4eea\uff09\u7684\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\uff0c\u5229\u7528\u8bbe\u5907\u5185\u5b58\u4e2d\u7684\u57fa\u7840\u6a21\u578b\u548c\u5c11\u91cf\u6837\u672c\u9ad8\u6548\u9002\u5e94\u65b0\u573a\u666f\uff0c\u663e\u8457\u964d\u4f4e\u91cd\u8bad\u7ec3\u6210\u672c\u5e76\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u8fb9\u7f18\u670d\u52a1\u5668\u8d44\u6e90\u6709\u9650\uff0c\u800c\u79fb\u52a8\u8fb9\u7f18\u8bbe\u5907\u5177\u6709\u4e30\u5bcc\u7684\u7edf\u4e00\u5185\u5b58\u6c60\u4f46\u8ba1\u7b97\u80fd\u529b\u8f83\u5f31\uff0c\u9700\u8981\u4e00\u79cd\u9002\u5e94\u8fd9\u79cd\u8d44\u6e90\u7279\u6027\u7684\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\u3002", "method": "Legilimens\u901a\u8fc7\u9ad8\u6548\u9009\u62e9\u6837\u672c\u3001\u90e8\u5206\u66f4\u65b0\u57fa\u7840\u6a21\u578b\u4ee5\u53ca\u8ba1\u7b97\u8d44\u6e90\u5171\u4eab\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u91cd\u8bad\u7ec3\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u7cfb\u7edf\uff0cLegilimens\u5c06\u91cd\u8bad\u7ec3\u6210\u672c\u964d\u4f4e2.8-10\u500d\uff0c\u51c6\u786e\u6027\u63d0\u534718-45%\u3002", "conclusion": "Legilimens\u4e3a\u79fb\u52a8\u8fb9\u7f18\u8bbe\u5907\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2504.21184", "pdf": "https://arxiv.org/pdf/2504.21184", "abs": "https://arxiv.org/abs/2504.21184", "authors": ["Emily Zhou", "Khushboo Khatri", "Yixue Zhao", "Bhaskar Krishnamachari"], "title": "AffectEval: A Modular and Customizable Framework for Affective Computing", "categories": ["cs.AI"], "comment": "The short version is published in ACM/IEEE CHASE 2025", "summary": "The field of affective computing focuses on recognizing, interpreting, and\nresponding to human emotions, and has broad applications across education,\nchild development, and human health and wellness. However, developing affective\ncomputing pipelines remains labor-intensive due to the lack of software\nframeworks that support multimodal, multi-domain emotion recognition\napplications. This often results in redundant effort when building pipelines\nfor different applications. While recent frameworks attempt to address these\nchallenges, they remain limited in reducing manual effort and ensuring\ncross-domain generalizability. We introduce AffectEval, a modular and\ncustomizable framework to facilitate the development of affective computing\npipelines while reducing the manual effort and duplicate work involved in\ndeveloping such pipelines. We validate AffectEval by replicating prior\naffective computing experiments, and we demonstrate that our framework reduces\nprogramming effort by up to 90%, as measured by the reduction in raw lines of\ncode.", "AI": {"tldr": "AffectEval\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u5b9a\u5236\u7684\u6846\u67b6\uff0c\u65e8\u5728\u51cf\u5c11\u60c5\u611f\u8ba1\u7b97\u6d41\u6c34\u7ebf\u5f00\u53d1\u4e2d\u7684\u624b\u52a8\u5de5\u4f5c\u548c\u91cd\u590d\u52b3\u52a8\uff0c\u9a8c\u8bc1\u663e\u793a\u5176\u80fd\u51cf\u5c1190%\u7684\u7f16\u7a0b\u5de5\u4f5c\u91cf\u3002", "motivation": "\u60c5\u611f\u8ba1\u7b97\u9886\u57df\u7f3a\u4e4f\u652f\u6301\u591a\u6a21\u6001\u3001\u591a\u9886\u57df\u60c5\u611f\u8bc6\u522b\u5e94\u7528\u7684\u8f6f\u4ef6\u6846\u67b6\uff0c\u5bfc\u81f4\u5f00\u53d1\u6d41\u6c34\u7ebf\u65f6\u91cd\u590d\u52b3\u52a8\u3002", "method": "\u5f15\u5165AffectEval\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u548c\u53ef\u5b9a\u5236\u5316\u8bbe\u8ba1\u51cf\u5c11\u624b\u52a8\u5de5\u4f5c\uff0c\u5e76\u901a\u8fc7\u590d\u5236\u5148\u524d\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "AffectEval\u80fd\u51cf\u5c11\u9ad8\u8fbe90%\u7684\u7f16\u7a0b\u5de5\u4f5c\u91cf\uff08\u4ee5\u4ee3\u7801\u884c\u6570\u51cf\u5c11\u8861\u91cf\uff09\u3002", "conclusion": "AffectEval\u663e\u8457\u964d\u4f4e\u4e86\u60c5\u611f\u8ba1\u7b97\u6d41\u6c34\u7ebf\u5f00\u53d1\u7684\u590d\u6742\u6027\u548c\u5de5\u4f5c\u91cf\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.21051", "pdf": "https://arxiv.org/pdf/2504.21051", "abs": "https://arxiv.org/abs/2504.21051", "authors": ["Jiarui Ye", "Hao Tang"], "title": "Multimodal Large Language Models for Medicine: A Comprehensive Survey", "categories": ["cs.LG", "cs.CL", "cs.MM"], "comment": null, "summary": "MLLMs have recently become a focal point in the field of artificial\nintelligence research. Building on the strong capabilities of LLMs, MLLMs are\nadept at addressing complex multi-modal tasks. With the release of GPT-4, MLLMs\nhave gained substantial attention from different domains. Researchers have\nbegun to explore the potential of MLLMs in the medical and healthcare domain.\nIn this paper, we first introduce the background and fundamental concepts\nrelated to LLMs and MLLMs, while emphasizing the working principles of MLLMs.\nSubsequently, we summarize three main directions of application within\nhealthcare: medical reporting, medical diagnosis, and medical treatment. Our\nfindings are based on a comprehensive review of 330 recent papers in this area.\nWe illustrate the remarkable capabilities of MLLMs in these domains by\nproviding specific examples. For data, we present six mainstream modes of data\nalong with their corresponding evaluation benchmarks. At the end of the survey,\nwe discuss the challenges faced by MLLMs in the medical and healthcare domain\nand propose feasible methods to mitigate or overcome these issues.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u7684\u5e94\u7528\uff0c\u5305\u62ec\u80cc\u666f\u4ecb\u7ecd\u3001\u4e09\u5927\u5e94\u7528\u65b9\u5411\u3001\u6570\u636e\u6a21\u5f0f\u53ca\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u968f\u7740GPT-4\u7684\u53d1\u5e03\uff0cMLLMs\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u5c24\u5176\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u7684\u7814\u7a76\u9700\u6c42\u8feb\u5207\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0330\u7bc7\u76f8\u5173\u8bba\u6587\uff0c\u603b\u7ed3\u4e86MLLMs\u5728\u533b\u7597\u62a5\u544a\u3001\u8bca\u65ad\u548c\u6cbb\u7597\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5206\u6790\u4e86\u516d\u79cd\u4e3b\u6d41\u6570\u636e\u6a21\u5f0f\u53ca\u5176\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "MLLMs\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u5c55\u73b0\u51fa\u663e\u8457\u80fd\u529b\uff0c\u4f46\u9762\u4e34\u6570\u636e\u3001\u9690\u79c1\u548c\u6a21\u578b\u9002\u5e94\u6027\u7b49\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3aMLLMs\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7efc\u8ff0\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u51b3\u5f53\u524d\u6311\u6218\u7684\u53ef\u884c\u65b9\u6cd5\u3002"}}
{"id": "2504.21019", "pdf": "https://arxiv.org/pdf/2504.21019", "abs": "https://arxiv.org/abs/2504.21019", "authors": ["Yinghan Zhou", "Juan Wen", "Wanli Peng", "Yiming Xue", "Ziwei Zhang", "Zhengxian Wu"], "title": "Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by NAACL 2025 main conference", "summary": "The growing popularity of large language models has raised concerns regarding\nthe potential to misuse AI-generated text (AIGT). It becomes increasingly\ncritical to establish an excellent AIGT detection method with high\ngeneralization and robustness. However, existing methods either focus on model\ngeneralization or concentrate on robustness. The unified mechanism, to\nsimultaneously address the challenges of generalization and robustness, is less\nexplored. In this paper, we argue that robustness can be view as a specific\nform of domain shift, and empirically reveal an intrinsic mechanism for model\ngeneralization of AIGT detection task. Then, we proposed a novel AIGT detection\nmethod (DP-Net) via dynamic perturbations introduced by a reinforcement\nlearning with elaborated reward and action. Experimentally, extensive results\nshow that the proposed DP-Net significantly outperforms some state-of-the-art\nAIGT detection methods for generalization capacity in three cross-domain\nscenarios. Meanwhile, the DP-Net achieves best robustness under two text\nadversarial attacks. The code is publicly available at\nhttps://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5DP-Net\uff0c\u901a\u8fc7\u52a8\u6001\u6270\u52a8\u548c\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u8de8\u57df\u573a\u666f\u548c\u5bf9\u6297\u653b\u51fb\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0cAI\u751f\u6210\u6587\u672c\u7684\u6ee5\u7528\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u4e9f\u9700\u4e00\u79cd\u540c\u65f6\u5177\u5907\u9ad8\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5c06\u9c81\u68d2\u6027\u89c6\u4e3a\u57df\u504f\u79fb\u7684\u4e00\u79cd\u5f62\u5f0f\uff0c\u63d0\u51faDP-Net\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5f15\u5165\u52a8\u6001\u6270\u52a8\uff0c\u4f18\u5316\u5956\u52b1\u548c\u52a8\u4f5c\u8bbe\u8ba1\u3002", "result": "DP-Net\u5728\u4e09\u79cd\u8de8\u57df\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u4e24\u79cd\u6587\u672c\u5bf9\u6297\u653b\u51fb\u4e0b\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "DP-Net\u901a\u8fc7\u7edf\u4e00\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u4e2d\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.21154", "pdf": "https://arxiv.org/pdf/2504.21154", "abs": "https://arxiv.org/abs/2504.21154", "authors": ["Muhammad Turab", "Philippe Colantoni", "Damien Muselet", "Alain Tremeau"], "title": "Emotion Recognition in Contemporary Dance Performances Using Laban Movement Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents a novel framework for emotion recognition in contemporary\ndance by improving existing Laban Movement Analysis (LMA) feature descriptors\nand introducing robust, novel descriptors that capture both quantitative and\nqualitative aspects of the movement. Our approach extracts expressive\ncharacteristics from 3D keypoints data of professional dancers performing\ncontemporary dance under various emotional states, and trains multiple\nclassifiers, including Random Forests and Support Vector Machines.\nAdditionally, we provide in-depth explanation of features and their impact on\nmodel predictions using explainable machine learning methods. Overall, our\nstudy improves emotion recognition in contemporary dance and offers promising\napplications in performance analysis, dance training, and human--computer\ninteraction, with a highest accuracy of 96.85\\%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdbLaban\u8fd0\u52a8\u5206\u6790\u7279\u5f81\u63cf\u8ff0\u7b26\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5f53\u4ee3\u821e\u8e48\u4e2d\u7684\u60c5\u611f\u8bc6\u522b\uff0c\u51c6\u786e\u7387\u8fbe96.85%\u3002", "motivation": "\u6539\u8fdb\u73b0\u6709\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\uff0c\u7ed3\u5408\u5b9a\u91cf\u548c\u5b9a\u6027\u7279\u5f81\uff0c\u63d0\u5347\u5f53\u4ee3\u821e\u8e48\u4e2d\u7684\u60c5\u611f\u8bc6\u522b\u6548\u679c\u3002", "method": "\u4ece3D\u5173\u952e\u70b9\u6570\u636e\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u8bad\u7ec3\u591a\u79cd\u5206\u7c7b\u5668\uff08\u5982\u968f\u673a\u68ee\u6797\u548c\u652f\u6301\u5411\u91cf\u673a\uff09\uff0c\u5e76\u4f7f\u7528\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5206\u6790\u7279\u5f81\u3002", "result": "\u6700\u9ad8\u51c6\u786e\u7387\u8fbe\u523096.85%\uff0c\u5728\u8868\u6f14\u5206\u6790\u3001\u821e\u8e48\u8bad\u7ec3\u548c\u4eba\u673a\u4ea4\u4e92\u4e2d\u6709\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5f53\u4ee3\u821e\u8e48\u4e2d\u7684\u60c5\u611f\u8bc6\u522b\u6548\u679c\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2504.21218", "pdf": "https://arxiv.org/pdf/2504.21218", "abs": "https://arxiv.org/abs/2504.21218", "authors": ["Sebastian Dumbrava"], "title": "Theoretical Foundations for Semantic Cognition in Artificial Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "This monograph presents a modular cognitive architecture for artificial\nintelligence grounded in the formal modeling of belief as structured semantic\nstate. Belief states are defined as dynamic ensembles of linguistic expressions\nembedded within a navigable manifold, where operators enable assimilation,\nabstraction, nullification, memory, and introspection. Drawing from philosophy,\ncognitive science, and neuroscience, we develop a layered framework that\nenables self-regulating epistemic agents capable of reflective, goal-directed\nthought. At the core of this framework is the epistemic vacuum: a class of\nsemantically inert cognitive states that serves as the conceptual origin of\nbelief space. From this foundation, the Null Tower arises as a generative\nstructure recursively built through internal representational capacities. The\ntheoretical constructs are designed to be implementable in both symbolic and\nneural systems, including large language models, hybrid agents, and adaptive\nmemory architectures. This work offers a foundational substrate for\nconstructing agents that reason, remember, and regulate their beliefs in\nstructured, interpretable ways.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u5316\u8bed\u4e49\u72b6\u6001\u7684\u6a21\u5757\u5316\u8ba4\u77e5\u67b6\u6784\uff0c\u7528\u4e8e\u4eba\u5de5\u667a\u80fd\uff0c\u901a\u8fc7\u52a8\u6001\u8bed\u8a00\u8868\u8fbe\u96c6\u5408\u5b9a\u4e49\u4fe1\u5ff5\u72b6\u6001\uff0c\u5e76\u5f15\u5165\u53ef\u64cd\u4f5c\u7684\u6982\u5ff5\u6846\u67b6\u3002", "motivation": "\u65e8\u5728\u6784\u5efa\u4e00\u79cd\u80fd\u591f\u81ea\u6211\u8c03\u8282\u3001\u5177\u5907\u53cd\u601d\u548c\u76ee\u6807\u5bfc\u5411\u601d\u7ef4\u7684\u8ba4\u77e5\u4ee3\u7406\uff0c\u7ed3\u5408\u54f2\u5b66\u3001\u8ba4\u77e5\u79d1\u5b66\u548c\u795e\u7ecf\u79d1\u5b66\u7684\u7406\u8bba\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5206\u5c42\u7684\u6846\u67b6\uff0c\u5305\u62ec\u8bed\u4e49\u60f0\u6027\u7684\u8ba4\u77e5\u72b6\u6001\uff08epistemic vacuum\uff09\u548c\u751f\u6210\u6027\u7ed3\u6784\uff08Null Tower\uff09\uff0c\u9002\u7528\u4e8e\u7b26\u53f7\u548c\u795e\u7ecf\u7f51\u7edc\u7cfb\u7edf\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5b9e\u73b0\u7684\u8ba4\u77e5\u67b6\u6784\uff0c\u652f\u6301\u4ee3\u7406\u7684\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3001\u8bb0\u5fc6\u548c\u4fe1\u5ff5\u8c03\u8282\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u6784\u5efa\u5177\u5907\u9ad8\u7ea7\u8ba4\u77e5\u80fd\u529b\u7684\u4ee3\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u73b0\u8def\u5f84\u3002"}}
{"id": "2504.21053", "pdf": "https://arxiv.org/pdf/2504.21053", "abs": "https://arxiv.org/abs/2504.21053", "authors": ["Yi Zhou", "Wenpeng Xing", "Dezhang Kong", "Changting Lin", "Meng Han"], "title": "NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Safety alignment in large language models (LLMs) is achieved through\nfine-tuning mechanisms that regulate neuron activations to suppress harmful\ncontent. In this work, we propose a novel approach to induce disalignment by\nidentifying and modifying the neurons responsible for safety constraints. Our\nmethod consists of three key steps: Neuron Activation Analysis, where we\nexamine activation patterns in response to harmful and harmless prompts to\ndetect neurons that are critical for distinguishing between harmful and\nharmless inputs; Similarity-Based Neuron Identification, which systematically\nlocates the neurons responsible for safe alignment; and Neuron Relearning for\nSafety Removal, where we fine-tune these selected neurons to restore the\nmodel's ability to generate previously restricted responses. Experimental\nresults demonstrate that our method effectively removes safety constraints with\nminimal fine-tuning, highlighting a critical vulnerability in current alignment\ntechniques. Our findings underscore the need for robust defenses against\nadversarial fine-tuning attacks on LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u548c\u4fee\u6539\u795e\u7ecf\u5143\u6765\u89e3\u9664\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u63ed\u793a\u5f53\u524d\u5bf9\u9f50\u6280\u672f\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u4fee\u6539\u795e\u7ecf\u5143\u89e3\u9664\u5b89\u5168\u5bf9\u9f50\uff0c\u4ee5\u63ed\u793a\u73b0\u6709\u5bf9\u9f50\u6280\u672f\u7684\u6f0f\u6d1e\u3002", "method": "\u5206\u4e09\u6b65\uff1a\u795e\u7ecf\u5143\u6fc0\u6d3b\u5206\u6790\u3001\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u795e\u7ecf\u5143\u8bc6\u522b\u3001\u795e\u7ecf\u5143\u518d\u5b66\u4e60\u4ee5\u79fb\u9664\u5b89\u5168\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ee5\u6700\u5c0f\u5fae\u8c03\u6709\u6548\u79fb\u9664\u5b89\u5168\u7ea6\u675f\u3002", "conclusion": "\u5f53\u524d\u5bf9\u9f50\u6280\u672f\u5b58\u5728\u8106\u5f31\u6027\uff0c\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u673a\u5236\u3002"}}
{"id": "2504.21020", "pdf": "https://arxiv.org/pdf/2504.21020", "abs": "https://arxiv.org/abs/2504.21020", "authors": ["Jaydip Sen", "Rohit Pandey", "Hetvi Waghela"], "title": "Context-Enhanced Contrastive Search for Improved LLM Text Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This is the pre-review version of our paper, which has been accepted\n  for publication in the IEEE 6th International Conference on Emerging\n  Technologies (INCET). The conference will be organized at Belgaum, India,\n  from May 24 to 26, 2025. This is not the final camera-ready paper, which will\n  be available on IEEE Xplore. The paper is 9 pages long, and it contains 2\n  Figures and 4 Tables", "summary": "Recently, Large Language Models (LLMs) have demonstrated remarkable\nadvancements in Natural Language Processing (NLP). However, generating\nhigh-quality text that balances coherence, diversity, and relevance remains\nchallenging. Traditional decoding methods, such as bean search and top-k\nsampling, often struggle with either repetitive or incoherent outputs,\nparticularly in tasks that require long-form text generation. To address these\nlimitations, the paper proposes a novel enhancement of the well-known\nContrastive Search algorithm, Context-Enhanced Contrastive Search (CECS) with\ncontextual calibration. The proposed scheme introduces several novelties\nincluding dynamic contextual importance weighting, multi-level Contrastive\nSearch, and adaptive temperature control, to optimize the balance between\nfluency, creativity, and precision. The performance of CECS is evaluated using\nseveral standard metrics such as BLEU, ROUGE, and semantic similarity.\nExperimental results demonstrate significant improvements in both coherence and\nrelevance of the generated texts by CECS outperforming the existing Contrastive\nSearch techniques. The proposed algorithm has several potential applications in\nthe real world including legal document drafting, customer service chatbots,\nand content marketing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5bf9\u6bd4\u641c\u7d22\u7b97\u6cd5CECS\uff0c\u901a\u8fc7\u52a8\u6001\u4e0a\u4e0b\u6587\u91cd\u8981\u6027\u52a0\u6743\u548c\u591a\u7ea7\u5bf9\u6bd4\u641c\u7d22\u7b49\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6587\u672c\u7684\u8fde\u8d2f\u6027\u548c\u76f8\u5173\u6027\u3002", "motivation": "\u4f20\u7edf\u89e3\u7801\u65b9\u6cd5\u5728\u751f\u6210\u957f\u6587\u672c\u65f6\u5b58\u5728\u91cd\u590d\u6216\u4e0d\u8fde\u8d2f\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u5e73\u8861\u6d41\u7545\u6027\u3001\u521b\u9020\u6027\u548c\u7cbe\u786e\u6027\u3002", "method": "\u63d0\u51faContext-Enhanced Contrastive Search (CECS)\uff0c\u7ed3\u5408\u52a8\u6001\u4e0a\u4e0b\u6587\u91cd\u8981\u6027\u52a0\u6743\u3001\u591a\u7ea7\u5bf9\u6bd4\u641c\u7d22\u548c\u81ea\u9002\u5e94\u6e29\u5ea6\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aCECS\u5728BLEU\u3001ROUGE\u548c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u5bf9\u6bd4\u641c\u7d22\u6280\u672f\u3002", "conclusion": "CECS\u5728\u8fde\u8d2f\u6027\u548c\u76f8\u5173\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u6cd5\u5f8b\u6587\u4ef6\u8d77\u8349\u3001\u5ba2\u670d\u804a\u5929\u673a\u5668\u4eba\u548c\u5185\u5bb9\u8425\u9500\u7b49\u9886\u57df\u3002"}}
{"id": "2504.21166", "pdf": "https://arxiv.org/pdf/2504.21166", "abs": "https://arxiv.org/abs/2504.21166", "authors": ["Muhammad Turab", "Philippe Colantoni", "Damien Muselet", "Alain Tremeau"], "title": "Dance Style Recognition Using Laban Movement Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The growing interest in automated movement analysis has presented new\nchallenges in recognition of complex human activities including dance. This\nstudy focuses on dance style recognition using features extracted using Laban\nMovement Analysis. Previous studies for dance style recognition often focus on\ncross-frame movement analysis, which limits the ability to capture temporal\ncontext and dynamic transitions between movements. This gap highlights the need\nfor a method that can add temporal context to LMA features. For this, we\nintroduce a novel pipeline which combines 3D pose estimation, 3D human mesh\nreconstruction, and floor aware body modeling to effectively extract LMA\nfeatures. To address the temporal limitation, we propose a sliding window\napproach that captures movement evolution across time in features. These\nfeatures are then used to train various machine learning methods for\nclassification, and their explainability explainable AI methods to evaluate the\ncontribution of each feature to classification performance. Our proposed method\nachieves a highest classification accuracy of 99.18\\% which shows that the\naddition of temporal context significantly improves dance style recognition\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u59ff\u6001\u4f30\u8ba1\u30013D\u4eba\u4f53\u7f51\u683c\u91cd\u5efa\u548c\u5730\u677f\u611f\u77e5\u8eab\u4f53\u5efa\u6a21\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u53d6Laban\u8fd0\u52a8\u5206\u6790\uff08LMA\uff09\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u65b9\u6cd5\u6355\u6349\u65f6\u95f4\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u821e\u8e48\u98ce\u683c\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u821e\u8e48\u98ce\u683c\u8bc6\u522b\u65b9\u6cd5\u591a\u5173\u6ce8\u8de8\u5e27\u8fd0\u52a8\u5206\u6790\uff0c\u7f3a\u4e4f\u5bf9\u65f6\u95f4\u4e0a\u4e0b\u6587\u548c\u52a8\u6001\u8fc7\u6e21\u7684\u6355\u6349\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4e3aLMA\u7279\u5f81\u6dfb\u52a0\u65f6\u95f4\u80cc\u666f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6d41\u7a0b\uff0c\u7ed3\u54083D\u59ff\u6001\u4f30\u8ba1\u30013D\u4eba\u4f53\u7f51\u683c\u91cd\u5efa\u548c\u5730\u677f\u611f\u77e5\u8eab\u4f53\u5efa\u6a21\u63d0\u53d6LMA\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u65b9\u6cd5\u6355\u6349\u65f6\u95f4\u6f14\u5316\u3002\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5e76\u5229\u7528\u53ef\u89e3\u91caAI\u65b9\u6cd5\u8bc4\u4f30\u7279\u5f81\u8d21\u732e\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u821e\u8e48\u98ce\u683c\u8bc6\u522b\u4e2d\u8fbe\u523099.18%\u7684\u6700\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u8868\u660e\u65f6\u95f4\u4e0a\u4e0b\u6587\u7684\u52a0\u5165\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u4e0a\u4e0b\u6587\u548c\u65b0\u578b\u7279\u5f81\u63d0\u53d6\u6d41\u7a0b\uff0c\u821e\u8e48\u98ce\u683c\u8bc6\u522b\u7684\u51c6\u786e\u6027\u5f97\u5230\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u590d\u6742\u4eba\u7c7b\u6d3b\u52a8\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.21277", "pdf": "https://arxiv.org/pdf/2504.21277", "abs": "https://arxiv.org/abs/2504.21277", "authors": ["Guanghao Zhou", "Panjia Qiu", "Cen Chen", "Jie Wang", "Zheming Yang", "Jian Xu", "Minghui Qiu"], "title": "Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "The integration of reinforcement learning (RL) into the reasoning\ncapabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as\na transformative research direction. While MLLMs significantly extend Large\nLanguage Models (LLMs) to handle diverse modalities such as vision, audio, and\nvideo, enabling robust reasoning across multimodal inputs remains a major\nchallenge. This survey systematically reviews recent advances in RL-based\nreasoning for MLLMs, covering key algorithmic designs, reward mechanism\ninnovations, and practical applications. We highlight two main RL\nparadigms--value-free and value-based methods--and analyze how RL enhances\nreasoning abilities by optimizing reasoning trajectories and aligning\nmultimodal information. Furthermore, we provide an extensive overview of\nbenchmark datasets, evaluation protocols, and existing limitations, and propose\nfuture research directions to address current bottlenecks such as sparse\nrewards, inefficient cross-modal reasoning, and real-world deployment\nconstraints. Our goal is to offer a comprehensive and structured guide to\nresearchers interested in advancing RL-based reasoning in the multimodal era.", "AI": {"tldr": "\u7efc\u8ff0\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u4e86\u5173\u952e\u7b97\u6cd5\u3001\u5956\u52b1\u673a\u5236\u53ca\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u8f93\u5165\u63a8\u7406\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5f3a\u5316\u5b66\u4e60\u7684\u6574\u5408\u6709\u671b\u63d0\u5347\u5176\u63a8\u7406\u80fd\u529b\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u5305\u62ec\u65e0\u4ef7\u503c\u4e0e\u57fa\u4e8e\u4ef7\u503c\u7684\u65b9\u6cd5\uff0c\u4f18\u5316\u63a8\u7406\u8f68\u8ff9\u548c\u591a\u6a21\u6001\u4fe1\u606f\u5bf9\u9f50\u3002", "result": "\u603b\u7ed3\u4e86\u57fa\u51c6\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u534f\u8bae\u53ca\u73b0\u6709\u5c40\u9650\uff0c\u5982\u7a00\u758f\u5956\u52b1\u548c\u8de8\u6a21\u6001\u63a8\u7406\u6548\u7387\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u74f6\u9888\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u7ed3\u6784\u5316\u6307\u5357\u3002"}}
{"id": "2504.21055", "pdf": "https://arxiv.org/pdf/2504.21055", "abs": "https://arxiv.org/abs/2504.21055", "authors": ["Shuai Ma", "Bin Shen", "Chuanhui Zhang", "Youlong Wu", "Hang Li", "Shiyin Li", "Guangming Shi", "Naofal Al-Dhahir"], "title": "Modeling and Performance Analysis for Semantic Communications Based on Empirical Results", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Due to the black-box characteristics of deep learning based semantic encoders\nand decoders, finding a tractable method for the performance analysis of\nsemantic communications is a challenging problem. In this paper, we propose an\nAlpha-Beta-Gamma (ABG) formula to model the relationship between the end-to-end\nmeasurement and SNR, which can be applied for both image reconstruction tasks\nand inference tasks. Specifically, for image reconstruction tasks, the proposed\nABG formula can well fit the commonly used DL networks, such as SCUNet, and\nVision Transformer, for semantic encoding with the multi scale-structural\nsimilarity index measure (MS-SSIM) measurement. Furthermore, we find that the\nupper bound of the MS-SSIM depends on the number of quantized output bits of\nsemantic encoders, and we also propose a closed-form expression to fit the\nrelationship between the MS-SSIM and quantized output bits. To the best of our\nknowledge, this is the first theoretical expression between end-to-end\nperformance metrics and SNR for semantic communications. Based on the proposed\nABG formula, we investigate an adaptive power control scheme for semantic\ncommunications over random fading channels, which can effectively guarantee\nquality of service (QoS) for semantic communications, and then design the\noptimal power allocation scheme to maximize the energy efficiency of the\nsemantic communication system. Furthermore, by exploiting the bisection\nalgorithm, we develop the power allocation scheme to maximize the minimum QoS\nof multiple users for OFDMA downlink semantic communication Extensive\nsimulations verify the effectiveness and superiority of the proposed ABG\nformula and power allocation schemes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdAlpha-Beta-Gamma\uff08ABG\uff09\u516c\u5f0f\uff0c\u7528\u4e8e\u5efa\u6a21\u8bed\u4e49\u901a\u4fe1\u4e2d\u7aef\u5230\u7aef\u6027\u80fd\u6307\u6807\u4e0e\u4fe1\u566a\u6bd4\uff08SNR\uff09\u7684\u5173\u7cfb\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86\u81ea\u9002\u5e94\u529f\u7387\u63a7\u5236\u65b9\u6848\u3002", "motivation": "\u7531\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8bed\u4e49\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u5177\u6709\u9ed1\u76d2\u7279\u6027\uff0c\u5206\u6790\u8bed\u4e49\u901a\u4fe1\u6027\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faABG\u516c\u5f0f\uff0c\u9002\u7528\u4e8e\u56fe\u50cf\u91cd\u5efa\u548c\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u63a8\u5bfc\u4e86MS-SSIM\u4e0e\u91cf\u5316\u8f93\u51fa\u6bd4\u7279\u6570\u7684\u95ed\u5f0f\u5173\u7cfb\u3002\u57fa\u4e8eABG\u516c\u5f0f\uff0c\u8bbe\u8ba1\u4e86\u81ea\u9002\u5e94\u529f\u7387\u63a7\u5236\u65b9\u6848\u548c\u6700\u4f18\u529f\u7387\u5206\u914d\u65b9\u6848\u3002", "result": "ABG\u516c\u5f0f\u80fd\u5f88\u597d\u5730\u62df\u5408\u5e38\u7528\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff08\u5982SCUNet\u3001Vision Transformer\uff09\uff0c\u5e76\u9a8c\u8bc1\u4e86\u529f\u7387\u5206\u914d\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "conclusion": "ABG\u516c\u5f0f\u4e3a\u8bed\u4e49\u901a\u4fe1\u63d0\u4f9b\u4e86\u9996\u4e2a\u7aef\u5230\u7aef\u6027\u80fd\u4e0eSNR\u7684\u7406\u8bba\u5173\u7cfb\uff0c\u529f\u7387\u5206\u914d\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u80fd\u6548\u548c\u670d\u52a1\u8d28\u91cf\u3002"}}
{"id": "2504.21022", "pdf": "https://arxiv.org/pdf/2504.21022", "abs": "https://arxiv.org/abs/2504.21022", "authors": ["Jun Wang", "David Smith Sundarsingh", "Jyotirmoy V. Deshmukh", "Yiannis Kantaros"], "title": "ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Linear Temporal Logic (LTL) has become a prevalent specification language for\nrobotic tasks. To mitigate the significant manual effort and expertise required\nto define LTL-encoded tasks, several methods have been proposed for translating\nNatural Language (NL) instructions into LTL formulas, which, however, lack\ncorrectness guarantees. To address this, we introduce a new NL-to-LTL\ntranslation method, called ConformalNL2LTL, that can achieve user-defined\ntranslation success rates over unseen NL commands. Our method constructs LTL\nformulas iteratively by addressing a sequence of open-vocabulary\nQuestion-Answering (QA) problems with LLMs. To enable uncertainty-aware\ntranslation, we leverage conformal prediction (CP), a distribution-free\nuncertainty quantification tool for black-box models. CP enables our method to\nassess the uncertainty in LLM-generated answers, allowing it to proceed with\ntranslation when sufficiently confident and request help otherwise. We provide\nboth theoretical and empirical results demonstrating that ConformalNL2LTL\nachieves user-specified translation accuracy while minimizing help rates.", "AI": {"tldr": "ConformalNL2LTL\u662f\u4e00\u79cd\u65b0\u7684\u81ea\u7136\u8bed\u8a00\u5230LTL\u7684\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u95ee\u7b54\u95ee\u9898\u548cLLMs\uff0c\u5229\u7528\u4fdd\u5f62\u9884\u6d4b\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\uff0c\u786e\u4fdd\u7ffb\u8bd1\u51c6\u786e\u7387\u5e76\u6700\u5c0f\u5316\u6c42\u52a9\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6b63\u786e\u6027\u4fdd\u8bc1\uff0c\u9700\u8981\u624b\u52a8\u5b9a\u4e49LTL\u4efb\u52a1\uff0cConformalNL2LTL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u95ee\u7b54\u95ee\u9898\u548cLLMs\u8fed\u4ee3\u6784\u5efaLTL\u516c\u5f0f\uff0c\u5229\u7528\u4fdd\u5f62\u9884\u6d4b\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u7406\u8bba\u53ca\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5b9e\u73b0\u7528\u6237\u6307\u5b9a\u7684\u7ffb\u8bd1\u51c6\u786e\u7387\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u6c42\u52a9\u7387\u3002", "conclusion": "ConformalNL2LTL\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u4e14\u9ad8\u6548\u7684NL\u5230LTL\u7ffb\u8bd1\u65b9\u6cd5\u3002"}}
{"id": "2504.21194", "pdf": "https://arxiv.org/pdf/2504.21194", "abs": "https://arxiv.org/abs/2504.21194", "authors": ["Vedika Srivastava", "Hemant Kumar Singh", "Jaisal Singh"], "title": "Geolocating Earth Imagery from ISS: Integrating Machine Learning with Astronaut Photography for Enhanced Geographic Mapping", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents a novel approach to geolocating images captured from the\nInternational Space Station (ISS) using advanced machine learning algorithms.\nDespite having precise ISS coordinates, the specific Earth locations depicted\nin astronaut-taken photographs often remain unidentified. Our research\naddresses this gap by employing three distinct image processing pipelines: a\nNeural Network based approach, a SIFT based method, and GPT-4 model. Each\npipeline is tailored to process high-resolution ISS imagery, identifying both\nnatural and man-made geographical features. Through extensive evaluation on a\ndiverse dataset of over 140 ISS images, our methods demonstrate significant\npromise in automated geolocation with varied levels of success. The NN approach\nshowed a high success rate in accurately matching geographical features, while\nthe SIFT pipeline excelled in processing zoomed-in images. GPT-4 model provided\nenriched geographical descriptions alongside location predictions. This\nresearch contributes to the fields of remote sensing and Earth observation by\nenhancing the accuracy and efficiency of geolocating space-based imagery,\nthereby aiding environmental monitoring and global mapping efforts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4ece\u56fd\u9645\u7a7a\u95f4\u7ad9\uff08ISS\uff09\u56fe\u50cf\u4e2d\u5b9a\u4f4d\u5730\u7403\u4f4d\u7f6e\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u79cd\u4e0d\u540c\u7684\u56fe\u50cf\u5904\u7406\u7ba1\u9053\uff08\u795e\u7ecf\u7f51\u7edc\u3001SIFT\u65b9\u6cd5\u548cGPT-4\u6a21\u578b\uff09\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5730\u7406\u7279\u5f81\u8bc6\u522b\u3002", "motivation": "ISS\u62cd\u6444\u7684\u7167\u7247\u867d\u7136\u5750\u6807\u7cbe\u786e\uff0c\u4f46\u5177\u4f53\u5730\u7406\u4f4d\u7f6e\u5e38\u672a\u6807\u6ce8\uff0c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u56fe\u50cf\u5904\u7406\u7ba1\u9053\uff1a\u795e\u7ecf\u7f51\u7edc\u3001SIFT\u65b9\u6cd5\u548cGPT-4\u6a21\u578b\uff0c\u5206\u522b\u9488\u5bf9\u4e0d\u540c\u9700\u6c42\u5904\u7406\u9ad8\u5206\u8fa8\u7387ISS\u56fe\u50cf\u3002", "result": "\u5728140\u591a\u5f20ISS\u56fe\u50cf\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5728\u5730\u7406\u7279\u5f81\u5339\u914d\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cSIFT\u65b9\u6cd5\u64c5\u957f\u5904\u7406\u653e\u5927\u56fe\u50cf\uff0cGPT-4\u6a21\u578b\u5219\u80fd\u63d0\u4f9b\u4e30\u5bcc\u7684\u5730\u7406\u63cf\u8ff0\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u5347\u4e86\u7a7a\u95f4\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5bf9\u9065\u611f\u3001\u5730\u7403\u89c2\u6d4b\u53ca\u73af\u5883\u76d1\u6d4b\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2504.21318", "pdf": "https://arxiv.org/pdf/2504.21318", "abs": "https://arxiv.org/abs/2504.21318", "authors": ["Marah Abdin", "Sahaj Agarwal", "Ahmed Awadallah", "Vidhisha Balachandran", "Harkirat Behl", "Lingjiao Chen", "Gustavo de Rosa", "Suriya Gunasekar", "Mojan Javaheripi", "Neel Joshi", "Piero Kauffmann", "Yash Lara", "Caio C\u00e9sar Teodoro Mendes", "Arindam Mitra", "Besmira Nushi", "Dimitris Papailiopoulos", "Olli Saarikivi", "Shital Shah", "Vaishnavi Shrivastava", "Vibhav Vineet", "Yue Wu", "Safoora Yousefi", "Guoqing Zheng"], "title": "Phi-4-reasoning Technical Report", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that\nachieves strong performance on complex reasoning tasks. Trained via supervised\nfine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected\nfor the right level of complexity and diversity-and reasoning demonstrations\ngenerated using o3-mini, Phi-4-reasoning generates detailed reasoning chains\nthat effectively leverage inference-time compute. We further develop\nPhi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based\nreinforcement learning that offers higher performance by generating longer\nreasoning traces. Across a wide range of reasoning tasks, both models\noutperform significantly larger open-weight models such as\nDeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full\nDeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and\nscientific reasoning, coding, algorithmic problem solving, planning, and\nspatial understanding. Interestingly, we observe a non-trivial transfer of\nimprovements to general-purpose benchmarks as well. In this report, we provide\ninsights into our training data, our training methodologies, and our\nevaluations. We show that the benefit of careful data curation for supervised\nfine-tuning (SFT) extends to reasoning language models, and can be further\namplified by reinforcement learning (RL). Finally, our evaluation points to\nopportunities for improving how we assess the performance and robustness of\nreasoning models.", "AI": {"tldr": "Phi-4-reasoning \u662f\u4e00\u4e2a14B\u53c2\u6570\u7684\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6570\u636e\u7cbe\u9009\u548c\u76d1\u7763\u5fae\u8c03\u63d0\u5347\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u5bf9\u6a21\u578b\u8868\u73b0\u7684\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "method": "\u4f7f\u7528\u7cbe\u9009\u7684\u201c\u53ef\u6559\u5b66\u201d\u63d0\u793a\u548c\u76d1\u7763\u5fae\u8c03\u8bad\u7ec3Phi-4\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u66f4\u957f\u7684\u63a8\u7406\u94fe\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "Phi-4-reasoning\u53ca\u5176\u5f3a\u5316\u5b66\u4e60\u53d8\u4f53\u5728\u591a\u9879\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63a5\u8fd1\u6216\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002", "conclusion": "\u6570\u636e\u7cbe\u9009\u548c\u76d1\u7763\u5fae\u8c03\u5bf9\u63a8\u7406\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5f3a\u5316\u5b66\u4e60\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u8868\u73b0\uff0c\u540c\u65f6\u8bc4\u4f30\u65b9\u6cd5\u9700\u6539\u8fdb\u3002"}}
{"id": "2504.21062", "pdf": "https://arxiv.org/pdf/2504.21062", "abs": "https://arxiv.org/abs/2504.21062", "authors": ["Ngueuleweu Tiwang Gildas"], "title": "A Hamiltonian Higher-Order Elasticity Framework for Dynamic Diagnostics(2HOED)", "categories": ["cs.LG", "econ.GN", "q-fin.EC"], "comment": "19 pages, 7 figures", "summary": "Machine learning detects patterns, block chain guarantees trust and\nimmutability, and modern causal inference identifies directional linkages, yet\nnone alone exposes the full energetic anatomy of complex systems; the\nHamiltonian Higher Order Elasticity Dynamics(2HOED) framework bridges these\ngaps. Grounded in classical mechanics but extended to Economics order\nelasticity terms, 2HOED represents economic, social, and physical systems as\nenergy-based Hamiltonians whose position, velocity, acceleration, and jerk of\nelasticity jointly determine systemic power, Inertia, policy sensitivity, and\nmarginal responses. Because the formalism is scaling free and coordinate\nagnostic, it transfers seamlessly from financial markets to climate science,\nfrom supply chain logistics to epidemiology, thus any discipline in which\nadaptation and shocks coexist. By embedding standard econometric variables\ninside a Hamiltonian, 2HOED enriches conventional economic analysis with\nrigorous diagnostics of resilience, tipping points, and feedback loops,\nrevealing failure modes invisible to linear models. Wavelet spectra, phase\nspace attractors, and topological persistence diagrams derived from 2HOED\nexpose multistage policy leverage that machine learning detects only\nempirically and block chain secures only after the fact. For economists,\nphysicians and other scientists, the method opens a new causal energetic\nchannel linking biological or mechanical elasticity to macro level outcomes.\nPortable, interpretable, and computationally light, 2HOED turns data streams\ninto dynamical energy maps, empowering decision makers to anticipate crises,\ndesign adaptive policies, and engineer robust systems delivering the predictive\npunch of AI with the explanatory clarity of physics.", "AI": {"tldr": "2HOED\u6846\u67b6\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u3001\u533a\u5757\u94fe\u548c\u56e0\u679c\u63a8\u7406\uff0c\u901a\u8fc7\u54c8\u5bc6\u987f\u529b\u5b66\u5206\u6790\u590d\u6742\u7cfb\u7edf\u7684\u80fd\u91cf\u52a8\u6001\uff0c\u63ed\u793a\u7ebf\u6027\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u7684\u5931\u6548\u6a21\u5f0f\u548c\u53cd\u9988\u5faa\u73af\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u673a\u5668\u5b66\u4e60\u3001\u533a\u5757\u94fe\u548c\u56e0\u679c\u63a8\u7406\uff09\u5404\u81ea\u72ec\u7acb\uff0c\u65e0\u6cd5\u5168\u9762\u89e3\u6790\u590d\u6742\u7cfb\u7edf\u7684\u80fd\u91cf\u52a8\u6001\uff0c2HOED\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e\u7ecf\u5178\u529b\u5b66\u7684\u54c8\u5bc6\u987f\u80fd\u91cf\u6a21\u578b\uff0c\u6269\u5c55\u81f3\u7ecf\u6d4e\u5b66\u5f39\u6027\u9879\uff0c\u5206\u6790\u7cfb\u7edf\u7684\u4f4d\u7f6e\u3001\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\u548c\u5f39\u6027\u53d8\u5316\uff0c\u751f\u6210\u52a8\u6001\u80fd\u91cf\u56fe\u8c31\u3002", "result": "2HOED\u63ed\u793a\u4e86\u7cfb\u7edf\u7684\u97e7\u6027\u3001\u4e34\u754c\u70b9\u548c\u53cd\u9988\u5faa\u73af\uff0c\u63d0\u4f9b\u4e86\u591a\u9636\u6bb5\u653f\u7b56\u6760\u6746\uff0c\u4f18\u4e8e\u4f20\u7edf\u7ebf\u6027\u6a21\u578b\u3002", "conclusion": "2HOED\u4e3a\u8de8\u5b66\u79d1\u7814\u7a76\u63d0\u4f9b\u4e86\u4fbf\u643a\u3001\u53ef\u89e3\u91ca\u4e14\u8ba1\u7b97\u8f7b\u91cf\u7684\u5de5\u5177\uff0c\u5e2e\u52a9\u51b3\u7b56\u8005\u9884\u6d4b\u5371\u673a\u5e76\u8bbe\u8ba1\u9002\u5e94\u6027\u653f\u7b56\u3002"}}
{"id": "2504.21023", "pdf": "https://arxiv.org/pdf/2504.21023", "abs": "https://arxiv.org/abs/2504.21023", "authors": ["Sheng Cao", "Mingrui Wu", "Karthik Prasad", "Yuandong Tian", "Zechun Liu"], "title": "Param$\u0394$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost", "categories": ["cs.CL", "cs.LG"], "comment": "Published as a conference paper at ICLR 2025", "summary": "The post-training phase of large language models is essential for enhancing\ncapabilities such as instruction-following, reasoning, and alignment with human\npreferences. However, it demands extensive high-quality data and poses risks\nlike overfitting, alongside significant computational costs due to repeated\npost-training and evaluation after each base model update. This paper\nintroduces $Param\\Delta$, a novel method that streamlines post-training by\ntransferring knowledge from an existing post-trained model to a newly updated\nbase model with ZERO additional training. By computing the difference between\npost-trained model weights ($\\Theta_\\text{post}$) and base model weights\n($\\Theta_\\text{base}$), and adding this to the updated base model\n($\\Theta'_\\text{base}$), we define $Param\\Delta$ Model as:\n$\\Theta_{\\text{Param}\\Delta} = \\Theta_\\text{post} - \\Theta_\\text{base} +\n\\Theta'_\\text{base}$. This approach surprisingly equips the new base model with\npost-trained capabilities, achieving performance comparable to direct\npost-training. We did analysis on LLama3, Llama3.1, Qwen, and\nDeepSeek-distilled models. Results indicate $Param\\Delta$ Model effectively\nreplicates traditional post-training. For example, the $Param\\Delta$ Model\nobtained from 70B Llama3-inst, Llama3-base, Llama3.1-base models attains\napproximately 95\\% of Llama3.1-inst model's performance on average.\n$Param\\Delta$ brings a new perspective on how to fully leverage models in the\nopen-weight community, where checkpoints for base and instruct models are\nreadily available and frequently updated, by providing a cost-free framework to\naccelerate the iterative cycle of model development.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a$Param\\Delta$\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u5df2\u6709\u540e\u8bad\u7ec3\u6a21\u578b\u4e0e\u57fa\u7840\u6a21\u578b\u6743\u91cd\u7684\u5dee\u5f02\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u66f4\u65b0\u540e\u7684\u57fa\u7840\u6a21\u578b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u540e\u8bad\u7ec3\u80fd\u529b\u3002", "motivation": "\u540e\u8bad\u7ec3\u9636\u6bb5\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u4e14\u5b58\u5728\u8fc7\u62df\u5408\u98ce\u9669\u3002$Param\\Delta$\u65e8\u5728\u901a\u8fc7\u77e5\u8bc6\u8fc1\u79fb\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u964d\u4f4e\u6210\u672c\u3002", "method": "\u8ba1\u7b97\u540e\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u4e0e\u57fa\u7840\u6a21\u578b\u6743\u91cd\u7684\u5dee\u5f02\uff0c\u5e76\u5c06\u8be5\u5dee\u5f02\u5e94\u7528\u4e8e\u66f4\u65b0\u540e\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5f62\u6210$Param\\Delta$\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c$Param\\Delta$\u6a21\u578b\u6027\u80fd\u63a5\u8fd1\u76f4\u63a5\u540e\u8bad\u7ec3\u6a21\u578b\uff08\u5982\u8fbe\u5230Llama3.1-inst\u6a21\u578b\u6027\u80fd\u768495%\uff09\u3002", "conclusion": "$Param\\Delta$\u4e3a\u5f00\u653e\u6743\u91cd\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u79cd\u96f6\u6210\u672c\u52a0\u901f\u6a21\u578b\u8fed\u4ee3\u7684\u65b9\u6cd5\uff0c\u5145\u5206\u5229\u7528\u73b0\u6709\u6a21\u578b\u8d44\u6e90\u3002"}}
{"id": "2504.21226", "pdf": "https://arxiv.org/pdf/2504.21226", "abs": "https://arxiv.org/abs/2504.21226", "authors": ["Jiaqi Liu", "Ran Tong", "Aowei Shen", "Shuzheng Li", "Changlin Yang", "Lisha Xu"], "title": "MemeBLIP2: A novel lightweight multimodal system to detect harmful memes", "categories": ["cs.CV", "cs.AI"], "comment": "11pages,2 figures, manucripts in preparation", "summary": "Memes often merge visuals with brief text to share humor or opinions, yet\nsome memes contain harmful messages such as hate speech. In this paper, we\nintroduces MemeBLIP2, a light weight multimodal system that detects harmful\nmemes by combining image and text features effectively. We build on previous\nstudies by adding modules that align image and text representations into a\nshared space and fuse them for better classification. Using BLIP-2 as the core\nvision-language model, our system is evaluated on the PrideMM datasets. The\nresults show that MemeBLIP2 can capture subtle cues in both modalities, even in\ncases with ironic or culturally specific content, thereby improving the\ndetection of harmful material.", "AI": {"tldr": "MemeBLIP2\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u6709\u6548\u7ed3\u5408\u56fe\u50cf\u548c\u6587\u672c\u7279\u5f81\u68c0\u6d4b\u6709\u5bb3\u8868\u60c5\u5305\u3002", "motivation": "\u8868\u60c5\u5305\u5e38\u7ed3\u5408\u56fe\u50cf\u548c\u7b80\u77ed\u6587\u672c\u4f20\u64ad\u5e7d\u9ed8\u6216\u89c2\u70b9\uff0c\u4f46\u90e8\u5206\u5305\u542b\u6709\u5bb3\u5185\u5bb9\u5982\u4ec7\u6068\u8a00\u8bba\uff0c\u9700\u6709\u6548\u68c0\u6d4b\u3002", "method": "\u57fa\u4e8eBLIP-2\u6838\u5fc3\u6a21\u578b\uff0c\u6dfb\u52a0\u6a21\u5757\u5bf9\u9f50\u56fe\u50cf\u4e0e\u6587\u672c\u8868\u5f81\u5e76\u878d\u5408\uff0c\u63d0\u5347\u5206\u7c7b\u6548\u679c\u3002", "result": "\u5728PrideMM\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cMemeBLIP2\u80fd\u6355\u6349\u591a\u6a21\u6001\u7ec6\u5fae\u7ebf\u7d22\uff0c\u6539\u8fdb\u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b\u3002", "conclusion": "MemeBLIP2\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u6709\u6548\u8bc6\u522b\u6709\u5bb3\u8868\u60c5\u5305\uff0c\u5305\u62ec\u8bbd\u523a\u6216\u6587\u5316\u7279\u5b9a\u5185\u5bb9\u3002"}}
{"id": "2504.21347", "pdf": "https://arxiv.org/pdf/2504.21347", "abs": "https://arxiv.org/abs/2504.21347", "authors": ["Seonghee Lee", "Denae Ford", "John Tang", "Sasa Junuzovic", "Asta Roseway", "Ed Cutrell", "Kori Inkpen"], "title": "IRL Dittos: Embodied Multimodal AI Agent Interactions in Open Spaces", "categories": ["cs.AI", "cs.HC", "H.5.2; I.2.9"], "comment": "8 pages, 3 figures", "summary": "We introduce the In Real Life (IRL) Ditto, an AI-driven embodied agent\ndesigned to represent remote colleagues in shared office spaces, creating\nopportunities for real-time exchanges even in their absence. IRL Ditto offers a\nunique hybrid experience by allowing in-person colleagues to encounter a\ndigital version of their remote teammates, initiating greetings, updates, or\nsmall talk as they might in person. Our research question examines: How can the\nIRL Ditto influence interactions and relationships among colleagues in a shared\noffice space? Through a four-day study, we assessed IRL Ditto's ability to\nstrengthen social ties by simulating presence and enabling meaningful\ninteractions across different levels of social familiarity. We find that\nenhancing social relationships depended deeply on the foundation of the\nrelationship participants had with the source of the IRL Ditto. This study\nprovides insights into the role of embodied agents in enriching workplace\ndynamics for distributed teams.", "AI": {"tldr": "IRL Ditto\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u5b9e\u4f53\u4ee3\u7406\uff0c\u7528\u4e8e\u5728\u5171\u4eab\u529e\u516c\u7a7a\u95f4\u4e2d\u4ee3\u8868\u8fdc\u7a0b\u540c\u4e8b\uff0c\u4fc3\u8fdb\u5b9e\u65f6\u4e92\u52a8\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5176\u589e\u5f3a\u793e\u4ea4\u5173\u7cfb\u7684\u80fd\u529b\u53d6\u51b3\u4e8e\u7528\u6237\u4e0e\u4ee3\u7406\u6765\u6e90\u7684\u65e2\u6709\u5173\u7cfb\u57fa\u7840\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5b9e\u4f53\u4ee3\u7406\uff08IRL Ditto\uff09\u5728\u5171\u4eab\u529e\u516c\u7a7a\u95f4\u4e2d\u6a21\u62df\u8fdc\u7a0b\u540c\u4e8b\u7684\u5b58\u5728\uff0c\u4ee5\u589e\u5f3a\u540c\u4e8b\u95f4\u7684\u4e92\u52a8\u548c\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u4e3a\u671f\u56db\u5929\u7684\u7814\u7a76\uff0c\u8bc4\u4f30IRL Ditto\u5728\u4e0d\u540c\u793e\u4ea4\u719f\u6089\u5ea6\u4e0b\u6a21\u62df\u5b58\u5728\u548c\u4fc3\u8fdb\u6709\u610f\u4e49\u4e92\u52a8\u7684\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cIRL Ditto\u589e\u5f3a\u793e\u4ea4\u5173\u7cfb\u7684\u80fd\u529b\u4e0e\u7528\u6237\u548c\u4ee3\u7406\u6765\u6e90\u7684\u65e2\u6709\u5173\u7cfb\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5b9e\u4f53\u4ee3\u7406\u5728\u4e30\u5bcc\u5206\u5e03\u5f0f\u56e2\u961f\u5de5\u4f5c\u573a\u6240\u52a8\u6001\u4e2d\u7684\u6f5c\u5728\u4f5c\u7528\u3002"}}
{"id": "2504.21063", "pdf": "https://arxiv.org/pdf/2504.21063", "abs": "https://arxiv.org/abs/2504.21063", "authors": ["Shuai Gong", "Chaoran Cui", "Xiaolin Dong", "Xiushan Nie", "Lei Zhu", "Xiaojun Chang"], "title": "Token-Level Prompt Mixture with Parameter-Free Routing for Federated Domain Generalization", "categories": ["cs.LG", "cs.AI"], "comment": "The manuscript has been submitted to IEEE Transactions on Knowledge\n  and Data Engineering", "summary": "Federated domain generalization (FedDG) aims to learn a globally\ngeneralizable model from decentralized clients with heterogeneous data while\npreserving privacy. Recent studies have introduced prompt learning to adapt\nvision-language models (VLMs) in FedDG by learning a single global prompt.\nHowever, such a one-prompt-fits-all learning paradigm typically leads to\nperformance degradation on personalized samples. Although the mixture of\nexperts (MoE) offers a promising solution for specialization, existing\nMoE-based methods suffer from coarse image-level expert assignment and high\ncommunication costs from parameterized routers. To address these limitations,\nwe propose TRIP, a Token-level prompt mixture with parameter-free routing\nframework for FedDG, which treats multiple prompts as distinct experts. Unlike\nexisting image-level routing designs, TRIP assigns different tokens within an\nimage to specific experts. To ensure communication efficiency, TRIP\nincorporates a parameter-free routing mechanism based on token clustering and\noptimal transport. The instance-specific prompt is then synthesized by\naggregating experts, weighted by the number of tokens assigned to each.\nAdditionally, TRIP develops an unbiased learning strategy for prompt experts,\nleveraging the VLM's zero-shot generalization capability. Extensive experiments\nacross four benchmarks demonstrate that TRIP achieves optimal generalization\nresults, with communication of only 1K parameters per round. Our code is\navailable at https://github.com/GongShuai8210/TRIP.", "AI": {"tldr": "TRIP\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee4\u724c\u7ea7\u63d0\u793a\u6df7\u5408\u7684\u65e0\u53c2\u6570\u8def\u7531\u6846\u67b6\uff0c\u7528\u4e8e\u8054\u90a6\u57df\u6cdb\u5316\uff08FedDG\uff09\uff0c\u901a\u8fc7\u4ee4\u724c\u805a\u7c7b\u548c\u6700\u4f18\u4f20\u8f93\u5b9e\u73b0\u9ad8\u6548\u901a\u4fe1\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709FedDG\u65b9\u6cd5\u4e2d\u5355\u4e00\u5168\u5c40\u63d0\u793a\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u4ee5\u53ca\u57fa\u4e8eMoE\u7684\u65b9\u6cd5\u4e2d\u56fe\u50cf\u7ea7\u4e13\u5bb6\u5206\u914d\u7c97\u7cd9\u548c\u901a\u4fe1\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "TRIP\u91c7\u7528\u4ee4\u724c\u7ea7\u63d0\u793a\u6df7\u5408\uff0c\u901a\u8fc7\u53c2\u6570\u514d\u8d39\u8def\u7531\u673a\u5236\uff08\u57fa\u4e8e\u4ee4\u724c\u805a\u7c7b\u548c\u6700\u4f18\u4f20\u8f93\uff09\u5206\u914d\u4ee4\u724c\u7ed9\u7279\u5b9a\u4e13\u5bb6\uff0c\u5e76\u5408\u6210\u5b9e\u4f8b\u7279\u5b9a\u63d0\u793a\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTRIP\u5b9e\u73b0\u4e86\u6700\u4f18\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u6bcf\u8f6e\u901a\u4fe1\u4ec5\u97001K\u53c2\u6570\u3002", "conclusion": "TRIP\u901a\u8fc7\u4ee4\u724c\u7ea7\u4e13\u5bb6\u5206\u914d\u548c\u65e0\u53c2\u6570\u8def\u7531\uff0c\u663e\u8457\u63d0\u5347\u4e86FedDG\u7684\u6027\u80fd\u548c\u901a\u4fe1\u6548\u7387\u3002"}}
{"id": "2504.21024", "pdf": "https://arxiv.org/pdf/2504.21024", "abs": "https://arxiv.org/abs/2504.21024", "authors": ["Tianqing Fang", "Hongming Zhang", "Zhisong Zhang", "Kaixin Ma", "Wenhao Yu", "Haitao Mi", "Dong Yu"], "title": "WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model", "categories": ["cs.CL"], "comment": "19 pages", "summary": "Agent self-improvement, where the backbone Large Language Model (LLM) of the\nagent are trained on trajectories sampled autonomously based on their own\npolicies, has emerged as a promising approach for enhancing performance. Recent\nadvancements, particularly in web environments, face a critical limitation:\ntheir performance will reach a stagnation point during autonomous learning\ncycles, hindering further improvement. We argue that this stems from limited\nexploration of the web environment and insufficient exploitation of pre-trained\nweb knowledge in LLMs. To improve the performance of self-improvement, we\npropose a novel framework that introduces a co-evolving World Model LLM. This\nworld model predicts the next observation based on the current observation and\naction within the web environment. Leveraging LLMs' pretrained knowledge of\nabundant web content, the World Model serves dual roles: (1) as a virtual web\nserver generating self-instructed training data to continuously refine the\nagent's policy, and (2) as an imagination engine during inference, enabling\nlook-ahead simulation to guide action selection for the agent LLM. Experiments\nin real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a\n10% performance gain over existing self-evolving agents, demonstrating the\nefficacy and generalizability of our approach, without using any distillation\nfrom more powerful close-sourced models. Our work establishes the necessity of\nintegrating world models into autonomous agent frameworks to unlock sustained\nadaptability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e16\u754c\u6a21\u578bLLM\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u63a2\u7d22\u548c\u5229\u7528\u9884\u8bad\u7ec3\u77e5\u8bc6\uff0c\u89e3\u51b3\u4e86\u81ea\u4e3b\u5b66\u4e60\u4e2d\u6027\u80fd\u505c\u6ede\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u63d0\u534710%\u3002", "motivation": "\u73b0\u6709\u81ea\u4e3b\u5b66\u4e60\u65b9\u6cd5\u5728\u6027\u80fd\u63d0\u5347\u8fc7\u7a0b\u4e2d\u5bb9\u6613\u505c\u6ede\uff0c\u539f\u56e0\u662f\u73af\u5883\u63a2\u7d22\u4e0d\u8db3\u548c\u9884\u8bad\u7ec3\u77e5\u8bc6\u5229\u7528\u4e0d\u5145\u5206\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u5171\u540c\u6f14\u5316\u7684\u4e16\u754c\u6a21\u578bLLM\uff0c\u9884\u6d4b\u73af\u5883\u72b6\u6001\u5e76\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u540c\u65f6\u4f5c\u4e3a\u63a8\u7406\u65f6\u7684\u60f3\u8c61\u5f15\u64ce\u3002", "result": "\u5728\u771f\u5b9e\u7f51\u7edc\u73af\u5883\u4e2d\u5b9e\u9a8c\uff0c\u6027\u80fd\u63d0\u534710%\uff0c\u65e0\u9700\u4f9d\u8d56\u95ed\u6e90\u6a21\u578b\u84b8\u998f\u3002", "conclusion": "\u4e16\u754c\u6a21\u578b\u7684\u6574\u5408\u662f\u5b9e\u73b0\u6301\u7eed\u9002\u5e94\u6027\u63d0\u5347\u7684\u5173\u952e\u3002"}}
{"id": "2504.21231", "pdf": "https://arxiv.org/pdf/2504.21231", "abs": "https://arxiv.org/abs/2504.21231", "authors": ["Manikanta Varaganti", "Amulya Vankayalapati", "Nour Awad", "Gregory R. Dion", "Laura J. Brattain"], "title": "T2ID-CAS: Diffusion Model and Class Aware Sampling to Mitigate Class Imbalance in Neck Ultrasound Anatomical Landmark Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "submitted to IEEE EMBC 2025", "summary": "Neck ultrasound (US) plays a vital role in airway management by providing\nnon-invasive, real-time imaging that enables rapid and precise interventions.\nDeep learning-based anatomical landmark detection in neck US can further\nfacilitate procedural efficiency. However, class imbalance within datasets,\nwhere key structures like tracheal rings and vocal folds are underrepresented,\npresents significant challenges for object detection models. To address this,\nwe propose T2ID-CAS, a hybrid approach that combines a text-to-image latent\ndiffusion model with class-aware sampling to generate high-quality synthetic\nsamples for underrepresented classes. This approach, rarely explored in the\nultrasound domain, improves the representation of minority classes.\nExperimental results using YOLOv9 for anatomical landmark detection in neck US\ndemonstrated that T2ID-CAS achieved a mean Average Precision of 88.2,\nsignificantly surpassing the baseline of 66. This highlights its potential as a\ncomputationally efficient and scalable solution for mitigating class imbalance\nin AI-assisted ultrasound-guided interventions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6587\u672c\u5230\u56fe\u50cf\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u7c7b\u611f\u77e5\u91c7\u6837\u7684\u6df7\u5408\u65b9\u6cd5\uff08T2ID-CAS\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u9888\u90e8\u8d85\u58f0\u4e2d\u7c7b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u9888\u90e8\u8d85\u58f0\u5728\u6c14\u9053\u7ba1\u7406\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6570\u636e\u96c6\u4e2d\u5173\u952e\u7ed3\u6784\uff08\u5982\u6c14\u7ba1\u73af\u548c\u58f0\u5e26\uff09\u7684\u7c7b\u4e0d\u5e73\u8861\u95ee\u9898\u5f71\u54cd\u4e86\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528T2ID-CAS\u65b9\u6cd5\uff0c\u7ed3\u5408\u6587\u672c\u5230\u56fe\u50cf\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u7c7b\u611f\u77e5\u91c7\u6837\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6837\u672c\u4ee5\u589e\u5f3a\u5c11\u6570\u7c7b\u7684\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cT2ID-CAS\u5728YOLOv9\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e8688.2\u7684\u5e73\u5747\u7cbe\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u768466\u3002", "conclusion": "T2ID-CAS\u662f\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3AI\u8f85\u52a9\u8d85\u58f0\u5f15\u5bfc\u5e72\u9884\u4e2d\u7684\u7c7b\u4e0d\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2504.21370", "pdf": "https://arxiv.org/pdf/2504.21370", "abs": "https://arxiv.org/abs/2504.21370", "authors": ["Jingyang Yi", "Jiazheng Wang"], "title": "ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning", "categories": ["cs.AI"], "comment": "An appendix will be uploaded soon", "summary": "Reasoning models such as OpenAI o3 and DeepSeek-R1 have demonstrated strong\nperformance on reasoning-intensive tasks through extended Chain-of-Thought\n(CoT) prompting. While longer reasoning traces can facilitate a more thorough\nexploration of solution paths for complex problems, researchers have observed\nthat these models often \"overthink\", leading to inefficient inference. In this\npaper, we introduce ShorterBetter, a simple yet effective reinforcement\nlearning methed that enables reasoning language models to discover their own\noptimal CoT lengths without human intervention. By sampling multiple outputs\nper problem and defining the Sample Optimal Length (SOL) as the shortest\ncorrect response among all the outputs, our method dynamically guides the model\ntoward optimal inference lengths. Applied to the DeepSeek-Distill-Qwen-1.5B\nmodel, ShorterBetter achieves up to an 80% reduction in output length on both\nin-domain and out-of-domain reasoning tasks while maintaining accuracy. Our\nanalysis shows that overly long reasoning traces often reflect loss of\nreasoning direction, and thus suggests that the extended CoT produced by\nreasoning models is highly compressible.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faShorterBetter\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8ba9\u63a8\u7406\u6a21\u578b\u81ea\u52a8\u627e\u5230\u6700\u4f18\u63a8\u7406\u957f\u5ea6\uff0c\u51cf\u5c11\u8f93\u51fa\u957f\u5ea680%\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u6a21\u578b\u5982OpenAI o3\u548cDeepSeek-R1\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8fc7\u957f\u7684\u63a8\u7406\u8def\u5f84\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u4f18\u5316\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9a\u4e49\u6837\u672c\u6700\u4f18\u957f\u5ea6\uff08SOL\uff09\uff0c\u52a8\u6001\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u6700\u77ed\u6b63\u786e\u8f93\u51fa\u3002", "result": "\u5728DeepSeek-Distill-Qwen-1.5B\u6a21\u578b\u4e0a\uff0c\u8f93\u51fa\u957f\u5ea6\u51cf\u5c1180%\uff0c\u51c6\u786e\u6027\u4e0d\u53d8\u3002", "conclusion": "\u8fc7\u957f\u7684\u63a8\u7406\u8def\u5f84\u5e38\u5bfc\u81f4\u65b9\u5411\u8ff7\u5931\uff0c\u8868\u660e\u63a8\u7406\u6a21\u578b\u7684\u6269\u5c55CoT\u5177\u6709\u9ad8\u5ea6\u53ef\u538b\u7f29\u6027\u3002"}}
{"id": "2504.21064", "pdf": "https://arxiv.org/pdf/2504.21064", "abs": "https://arxiv.org/abs/2504.21064", "authors": ["Chengkai Yang", "Xingping Dong", "Xiaofen Zong"], "title": "Frequency Feature Fusion Graph Network For Depression Diagnosis Via fNIRS", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Data-driven approaches for depression diagnosis have emerged as a significant\nresearch focus in neuromedicine, driven by the development of relevant\ndatasets. Recently, graph neural network (GNN)-based models have gained\nwidespread adoption due to their ability to capture brain channel functional\nconnectivity from both spatial and temporal perspectives. However, their\neffectiveness is hindered by the absence of a robust temporal biomarker. In\nthis paper, we introduce a novel and effective biomarker for depression\ndiagnosis by leveraging the discrete Fourier transform (DFT) and propose a\ncustomized graph network architecture based on Temporal Graph Convolutional\nNetwork (TGCN). Our model was trained on a dataset comprising 1,086 subjects,\nwhich is over 10 times larger than previous datasets in the field of depression\ndiagnosis. Furthermore, to align with medical requirements, we performed\npropensity score matching (PSM) to create a refined subset, referred to as the\nPSM dataset. Experimental results demonstrate that incorporating our newly\ndesigned biomarker enhances the representation of temporal characteristics in\nbrain channels, leading to improved F1 scores in both the real-world dataset\nand the PSM dataset. This advancement has the potential to contribute to the\ndevelopment of more effective depression diagnostic tools. In addition, we used\nSHapley Additive exPlaination (SHAP) to validate the interpretability of our\nmodel, ensuring its practical applicability in medical settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\uff08DFT\uff09\u7684\u65b0\u578b\u751f\u7269\u6807\u5fd7\u7269\uff0c\u5e76\u7ed3\u5408\u5b9a\u5236\u5316\u7684TGCN\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6291\u90c1\u75c7\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709GNN\u6a21\u578b\u56e0\u7f3a\u4e4f\u9c81\u68d2\u7684\u65f6\u95f4\u751f\u7269\u6807\u5fd7\u7269\u800c\u53d7\u9650\uff0c\u9700\u6539\u8fdb\u4ee5\u66f4\u597d\u5730\u6355\u6349\u8111\u901a\u9053\u529f\u80fd\u8fde\u63a5\u7684\u65f6\u7a7a\u7279\u5f81\u3002", "method": "\u5229\u7528DFT\u8bbe\u8ba1\u751f\u7269\u6807\u5fd7\u7269\uff0c\u6784\u5efaTGCN\u6a21\u578b\uff0c\u5e76\u57281,086\u540d\u53d7\u8bd5\u8005\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u4f7f\u7528PSM\u521b\u5efa\u5b50\u96c6\u3002", "result": "\u65b0\u751f\u7269\u6807\u5fd7\u7269\u63d0\u5347\u4e86\u8111\u901a\u9053\u65f6\u95f4\u7279\u5f81\u7684\u8868\u793a\uff0cF1\u5206\u6570\u5728\u771f\u5b9e\u6570\u636e\u96c6\u548cPSM\u6570\u636e\u96c6\u4e2d\u5747\u6709\u63d0\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6291\u90c1\u75c7\u8bca\u65ad\u5de5\u5177\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6f5c\u5728\u8d21\u732e\uff0c\u5e76\u901a\u8fc7SHAP\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.21025", "pdf": "https://arxiv.org/pdf/2504.21025", "abs": "https://arxiv.org/abs/2504.21025", "authors": ["MD Thamed Bin Zaman Chowdhury", "Moazzem Hossain", "Md. Ridwanul Islam"], "title": "Durghotona GPT: A Web Scraping and Large Language Model Based Framework to Generate Road Accident Dataset Automatically in Bangladesh", "categories": ["cs.CL"], "comment": "It has been accepted in IEEE 27th International Conference on\n  Computer and Information Technology (ICCIT). Now, we are waiting for it to\n  get published in IEEE Xplore", "summary": "Road accidents pose significant concerns globally. They lead to large\nfinancial losses, injuries, disabilities, and societal challenges. Accurate and\ntimely accident data is essential for predicting and mitigating these events.\nThis paper presents a novel framework named 'Durghotona GPT' that integrates\nweb scraping and Large Language Models (LLMs) to automate the generation of\ncomprehensive accident datasets from prominent national dailies in Bangladesh.\nThe authors collected accident reports from three major newspapers: Prothom\nAlo, Dhaka Tribune, and The Daily Star. The collected news was then processed\nusing the newest available LLMs: GPT-4, GPT-3.5, and Llama-3. The framework\nefficiently extracts relevant information, categorizes reports, and compiles\ndetailed datasets. Thus, this framework overcomes limitations of manual data\ncollection methods such as delays, errors, and communication gaps. The authors'\nevaluation demonstrates that Llama-3, an open-source model, performs comparably\nto GPT-4. It achieved 89% accuracy in the authors' evaluation. Therefore, it\ncan be considered a cost-effective alternative for similar tasks. The results\nsuggest that the framework developed by the authors can drastically enhance the\nquality and availability of accident data. As a result, it can support critical\napplications in traffic safety analysis, urban planning, and public health. The\nauthors also developed an interface for 'Durghotona GPT' for ease of use as\npart of this paper. Future work will focus on expanding data collection methods\nand refining LLMs to further increase dataset accuracy and applicability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a'Durghotona GPT'\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u7f51\u7edc\u722c\u866b\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u4ece\u5b5f\u52a0\u62c9\u56fd\u4e3b\u8981\u62a5\u7eb8\u81ea\u52a8\u751f\u6210\u5168\u9762\u7684\u4ea4\u901a\u4e8b\u6545\u6570\u636e\u96c6\u3002", "motivation": "\u4ea4\u901a\u4e8b\u6545\u5bfc\u81f4\u91cd\u5927\u7ecf\u6d4e\u635f\u5931\u548c\u793e\u4f1a\u95ee\u9898\uff0c\u51c6\u786e\u53ca\u65f6\u7684\u6570\u636e\u5bf9\u9884\u6d4b\u548c\u7f13\u89e3\u4e8b\u6545\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4ece\u4e09\u5bb6\u62a5\u7eb8\u6536\u96c6\u4e8b\u6545\u65b0\u95fb\uff0c\u4f7f\u7528GPT-4\u3001GPT-3.5\u548cLlama-3\u5904\u7406\u6570\u636e\uff0c\u63d0\u53d6\u4fe1\u606f\u5e76\u5206\u7c7b\u3002", "result": "Llama-3\u8868\u73b0\u63a5\u8fd1GPT-4\uff0c\u51c6\u786e\u7387\u8fbe89%\uff0c\u662f\u6210\u672c\u6548\u76ca\u9ad8\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u6846\u67b6\u663e\u8457\u63d0\u5347\u6570\u636e\u8d28\u91cf\u548c\u53ef\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u4ea4\u901a\u5b89\u5168\u5206\u6790\u7b49\u5173\u952e\u5e94\u7528\uff0c\u672a\u6765\u5c06\u6269\u5c55\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u5e76\u4f18\u5316LLMs\u3002"}}
{"id": "2504.21247", "pdf": "https://arxiv.org/pdf/2504.21247", "abs": "https://arxiv.org/abs/2504.21247", "authors": ["Yangyang Qu", "Dazhi Fu", "Jicong Fan"], "title": "Subject Information Extraction for Novelty Detection with Domain Shifts", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised novelty detection (UND), aimed at identifying novel samples, is\nessential in fields like medical diagnosis, cybersecurity, and industrial\nquality control. Most existing UND methods assume that the training data and\ntesting normal data originate from the same domain and only consider the\ndistribution variation between training data and testing data. However, in real\nscenarios, it is common for normal testing and training data to originate from\ndifferent domains, a challenge known as domain shift. The discrepancies between\ntraining and testing data often lead to incorrect classification of normal data\nas novel by existing methods. A typical situation is that testing normal data\nand training data describe the same subject, yet they differ in the background\nconditions. To address this problem, we introduce a novel method that separates\nsubject information from background variation encapsulating the domain\ninformation to enhance detection performance under domain shifts. The proposed\nmethod minimizes the mutual information between the representations of the\nsubject and background while modelling the background variation using a deep\nGaussian mixture model, where the novelty detection is conducted on the subject\nrepresentations solely and hence is not affected by the variation of domains.\nExtensive experiments demonstrate that our model generalizes effectively to\nunseen domains and significantly outperforms baseline methods, especially under\nsubstantial domain shifts between training and testing data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u4e3b\u4f53\u4fe1\u606f\u548c\u80cc\u666f\u53d8\u5316\u6765\u63d0\u5347\u65e0\u76d1\u7763\u65b0\u9896\u6027\u68c0\u6d4b\u5728\u57df\u504f\u79fb\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u6765\u81ea\u540c\u4e00\u57df\uff0c\u5ffd\u7565\u4e86\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5bfc\u81f4\u6b63\u5e38\u6570\u636e\u88ab\u8bef\u5206\u7c7b\u4e3a\u65b0\u9896\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u6700\u5c0f\u5316\u4e3b\u4f53\u548c\u80cc\u666f\u8868\u793a\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u5e76\u7528\u6df1\u5ea6\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5efa\u6a21\u80cc\u666f\u53d8\u5316\uff0c\u4ec5\u57fa\u4e8e\u4e3b\u4f53\u8868\u793a\u8fdb\u884c\u65b0\u9896\u6027\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u672a\u89c1\u57df\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u57df\u504f\u79fb\u8f83\u5927\u65f6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u57df\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u65b0\u9896\u6027\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2504.21433", "pdf": "https://arxiv.org/pdf/2504.21433", "abs": "https://arxiv.org/abs/2504.21433", "authors": ["Zhicong Li", "Hangyu Mao", "Jiangjin Yin", "Mingzhe Xing", "Zhiwei Xu", "Yuanxing Zhang", "Yang Xiao"], "title": "NGENT: Next-Generation AI Agents Must Integrate Multi-Domain Abilities to Achieve Artificial General Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "This paper argues that the next generation of AI agent (NGENT) should\nintegrate across-domain abilities to advance toward Artificial General\nIntelligence (AGI). Although current AI agents are effective in specialized\ntasks such as robotics, role-playing, and tool-using, they remain confined to\nnarrow domains. We propose that future AI agents should synthesize the\nstrengths of these specialized systems into a unified framework capable of\noperating across text, vision, robotics, reinforcement learning, emotional\nintelligence, and beyond. This integration is not only feasible but also\nessential for achieving the versatility and adaptability that characterize\nhuman intelligence. The convergence of technologies across AI domains, coupled\nwith increasing user demand for cross-domain capabilities, suggests that such\nintegration is within reach. Ultimately, the development of these versatile\nagents is a critical step toward realizing AGI. This paper explores the\nrationale for this shift, potential pathways for achieving it.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u4e0b\u4e00\u4ee3AI\u4ee3\u7406\uff08NGENT\uff09\u9700\u6574\u5408\u8de8\u9886\u57df\u80fd\u529b\u4ee5\u63a8\u8fdb\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\u53d1\u5c55\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u867d\u5728\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u673a\u5668\u4eba\u3001\u89d2\u8272\u626e\u6f14\u3001\u5de5\u5177\u4f7f\u7528\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5c40\u9650\u4e8e\u72ed\u7a84\u9886\u57df\u3002", "method": "\u63d0\u51fa\u5c06\u5404\u9886\u57df\u4f18\u52bf\u6574\u5408\u4e3a\u7edf\u4e00\u6846\u67b6\uff0c\u6db5\u76d6\u6587\u672c\u3001\u89c6\u89c9\u3001\u673a\u5668\u4eba\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u60c5\u611f\u667a\u80fd\u7b49\u3002", "result": "\u8de8\u9886\u57df\u6574\u5408\u4e0d\u4ec5\u53ef\u884c\uff0c\u4e14\u662f\u5b9e\u73b0\u4eba\u7c7b\u667a\u80fd\u822c\u7075\u6d3b\u6027\u7684\u5173\u952e\u3002", "conclusion": "\u5f00\u53d1\u591a\u9886\u57dfAI\u4ee3\u7406\u662f\u5b9e\u73b0AGI\u7684\u91cd\u8981\u6b65\u9aa4\u3002"}}
{"id": "2504.21065", "pdf": "https://arxiv.org/pdf/2504.21065", "abs": "https://arxiv.org/abs/2504.21065", "authors": ["Anjie Qiao", "Junjie Xie", "Weifeng Huang", "Hao Zhang", "Jiahua Rao", "Shuangjia Zheng", "Yuedong Yang", "Zhen Wang", "Guo-Bo Li", "Jinping Lei"], "title": "A 3D pocket-aware and affinity-guided diffusion model for lead optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Molecular optimization, aimed at improving binding affinity or other\nmolecular properties, is a crucial task in drug discovery that often relies on\nthe expertise of medicinal chemists. Recently, deep learning-based 3D\ngenerative models showed promise in enhancing the efficiency of molecular\noptimization. However, these models often struggle to adequately consider\nbinding affinities with protein targets during lead optimization. Herein, we\npropose a 3D pocket-aware and affinity-guided diffusion model, named Diffleop,\nto optimize molecules with enhanced binding affinity. The model explicitly\nincorporates the knowledge of protein-ligand binding affinity to guide the\ndenoising sampling for molecule generation with high affinity. The\ncomprehensive evaluations indicated that Diffleop outperforms baseline models\nacross multiple metrics, especially in terms of binding affinity.", "AI": {"tldr": "Diffleop\u662f\u4e00\u79cd3D\u53e3\u888b\u611f\u77e5\u548c\u4eb2\u548c\u529b\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u4f18\u5316\u5206\u5b50\u7ed3\u5408\u4eb2\u548c\u529b\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5206\u5b50\u4f18\u5316\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8003\u8651\u7ed3\u5408\u4eb2\u548c\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDiffleop\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u86cb\u767d-\u914d\u4f53\u4eb2\u548c\u529b\u77e5\u8bc6\u5f15\u5bfc\u53bb\u566a\u91c7\u6837\uff0c\u751f\u6210\u9ad8\u4eb2\u548c\u529b\u5206\u5b50\u3002", "result": "Diffleop\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5c24\u5176\u5728\u7ed3\u5408\u4eb2\u548c\u529b\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "Diffleop\u4e3a\u5206\u5b50\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4eb2\u548c\u529b\u5bfc\u5411\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2504.21026", "pdf": "https://arxiv.org/pdf/2504.21026", "abs": "https://arxiv.org/abs/2504.21026", "authors": ["Manish Pandey", "Nageshwar Prasad Yadav", "Mokshada Adduru", "Sawan Rai"], "title": "Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "comment": null, "summary": "With the growing presence of multilingual users on social media, detecting\nabusive language in code-mixed text has become increasingly challenging.\nCode-mixed communication, where users seamlessly switch between English and\ntheir native languages, poses difficulties for traditional abuse detection\nmodels, as offensive content may be context-dependent or obscured by linguistic\nblending. While abusive language detection has been extensively explored for\nhigh-resource languages like English and Hindi, low-resource languages such as\nTelugu and Nepali remain underrepresented, leaving gaps in effective\nmoderation. In this study, we introduce a novel, manually annotated dataset of\n2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized\nas abusive and non-abusive, collected from various social media platforms. The\ndataset undergoes rigorous preprocessing before being evaluated across multiple\nMachine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We\nexperimented with models including Logistic Regression, Random Forest, Support\nVector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing\ntheir performance through hyperparameter tuning, and evaluate it using 10-fold\ncross-validation and statistical significance testing (t-test). Our findings\nprovide key insights into the challenges of detecting abusive language in\ncode-mixed settings and offer a comparative analysis of computational\napproaches. This study contributes to advancing NLP for low-resource languages\nby establishing benchmarks for abusive language detection in Telugu-English and\nNepali-English code-mixed text. The dataset and insights can aid in the\ndevelopment of more robust moderation strategies for multilingual social media\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u8bed\u8a00\u793e\u4ea4\u5a92\u4f53\u4e2d\u6df7\u5408\u4ee3\u7801\u6587\u672c\u7684\u8fb1\u9a82\u6027\u8bed\u8a00\u68c0\u6d4b\uff0c\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u6cf0\u5362\u56fa\u8bed\u548c\u5c3c\u6cca\u5c14\u8bed\uff09\u63d0\u51fa\u4e86\u65b0\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u6bd4\u8f83\u3002", "motivation": "\u968f\u7740\u591a\u8bed\u8a00\u7528\u6237\u5728\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u589e\u591a\uff0c\u6df7\u5408\u4ee3\u7801\u6587\u672c\u4e2d\u7684\u8fb1\u9a82\u6027\u8bed\u8a00\u68c0\u6d4b\u53d8\u5f97\u66f4\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u6cf0\u5362\u56fa\u8bed-\u82f1\u8bed\u548c\u5c3c\u6cca\u5c14\u8bed-\u82f1\u8bed\u6df7\u5408\u4ee3\u7801\u8bc4\u8bba\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6df7\u5408\u4ee3\u7801\u6587\u672c\u4e2d\u7684\u8fb1\u9a82\u6027\u8bed\u8a00\u68c0\u6d4b\u5b58\u5728\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u6bd4\u8f83\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684NLP\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u591a\u8bed\u8a00\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u5ba1\u6838\u7b56\u7565\u3002"}}
{"id": "2504.21248", "pdf": "https://arxiv.org/pdf/2504.21248", "abs": "https://arxiv.org/abs/2504.21248", "authors": ["Ezra Engel", "Lishan Li", "Chris Hudy", "Robert Schleusner"], "title": "Multi-modal Transfer Learning for Dynamic Facial Emotion Recognition in the Wild", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "Facial expression recognition (FER) is a subset of computer vision with\nimportant applications for human-computer-interaction, healthcare, and customer\nservice. FER represents a challenging problem-space because accurate\nclassification requires a model to differentiate between subtle changes in\nfacial features. In this paper, we examine the use of multi-modal transfer\nlearning to improve performance on a challenging video-based FER dataset,\nDynamic Facial Expression in-the-Wild (DFEW). Using a combination of pretrained\nResNets, OpenPose, and OmniVec networks, we explore the impact of\ncross-temporal, multi-modal features on classification accuracy. Ultimately, we\nfind that these finely-tuned multi-modal feature generators modestly improve\naccuracy of our transformer-based classification model.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u8fc1\u79fb\u5b66\u4e60\u5728\u89c6\u9891\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08FER\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u7f51\u7edc\u7ec4\u5408\u63d0\u5347DFEW\u6570\u636e\u96c6\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u5728\u591a\u4e2a\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u7531\u4e8e\u9762\u90e8\u7279\u5f81\u7684\u7ec6\u5fae\u53d8\u5316\uff0c\u51c6\u786e\u5206\u7c7b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u7684ResNets\u3001OpenPose\u548cOmniVec\u7f51\u7edc\uff0c\u7814\u7a76\u8de8\u65f6\u95f4\u591a\u6a21\u6001\u7279\u5f81\u5bf9\u5206\u7c7b\u51c6\u786e\u7387\u7684\u5f71\u54cd\u3002", "result": "\u591a\u6a21\u6001\u7279\u5f81\u751f\u6210\u5668\u7565\u5fae\u63d0\u5347\u4e86\u57fa\u4e8eTransformer\u7684\u5206\u7c7b\u6a21\u578b\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u591a\u6a21\u6001\u8fc1\u79fb\u5b66\u4e60\u5bf9\u63d0\u5347FER\u6027\u80fd\u5177\u6709\u6f5c\u5728\u4ef7\u503c\u3002"}}
{"id": "2504.21568", "pdf": "https://arxiv.org/pdf/2504.21568", "abs": "https://arxiv.org/abs/2504.21568", "authors": ["Shui-jin Rong", "Wei Guo", "Da-qing Zhang"], "title": "A Study on Group Decision Making Problem Based on Fuzzy Reasoning and Bayesian Networks", "categories": ["cs.AI"], "comment": null, "summary": "Aiming at the group decision - making problem with multi - objective\nattributes, this study proposes a group decision - making system that\nintegrates fuzzy inference and Bayesian network. A fuzzy rule base is\nconstructed by combining threshold values, membership functions, expert\nexperience, and domain knowledge to address quantitative challenges such as\nscale differences and expert linguistic variables. A hierarchical Bayesian\nnetwork is designed, featuring a directed acyclic graph with nodes selected by\nexperts, and maximum likelihood estimation is used to dynamically optimize the\nconditional probability table, modeling the nonlinear correlations among\nmultidimensional indices for posterior probability aggregation. In a\ncomprehensive student evaluation case, this method is compared with the\ntraditional weighted scoring approach. The results indicate that the proposed\nmethod demonstrates effectiveness in both rule criterion construction and\nranking consistency, with a classification accuracy of 86.0% and an F1 value\nimprovement of 53.4% over the traditional method. Additionally, computational\nexperiments on real - world datasets across various group decision scenarios\nassess the method's performance and robustness, providing evidence of its\nreliability in diverse contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u6a21\u7cca\u63a8\u7406\u548c\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u7fa4\u51b3\u7b56\u7cfb\u7edf\uff0c\u7528\u4e8e\u591a\u76ee\u6807\u5c5e\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u7cca\u89c4\u5219\u5e93\u548c\u8d1d\u53f6\u65af\u7f51\u7edc\u4f18\u5316\u51b3\u7b56\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u591a\u76ee\u6807\u5c5e\u6027\u7fa4\u51b3\u7b56\u95ee\u9898\u4e2d\u7684\u5b9a\u91cf\u6311\u6218\uff0c\u5982\u5c3a\u5ea6\u5dee\u5f02\u548c\u4e13\u5bb6\u8bed\u8a00\u53d8\u91cf\u3002", "method": "\u6784\u5efa\u6a21\u7cca\u89c4\u5219\u5e93\u548c\u5206\u5c42\u8d1d\u53f6\u65af\u7f51\u7edc\uff0c\u52a8\u6001\u4f18\u5316\u6761\u4ef6\u6982\u7387\u8868\uff0c\u5efa\u6a21\u591a\u7ef4\u6307\u6807\u7684\u975e\u7ebf\u6027\u76f8\u5173\u6027\u3002", "result": "\u5728\u7efc\u5408\u8bc4\u4ef7\u6848\u4f8b\u4e2d\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8fbe86.0%\uff0cF1\u503c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u9ad853.4%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u89c4\u5219\u6784\u5efa\u548c\u6392\u5e8f\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7fa4\u51b3\u7b56\u573a\u666f\u3002"}}
{"id": "2504.21066", "pdf": "https://arxiv.org/pdf/2504.21066", "abs": "https://arxiv.org/abs/2504.21066", "authors": ["Andreas Karathanasis", "John Violos", "Ioannis Kompatsiaris", "Symeon Papadopoulos"], "title": "A Brief Review for Compression and Transfer Learning Techniques in DeepFake Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Training and deploying deepfake detection models on edge devices offers the\nadvantage of maintaining data privacy and confidentiality by processing it\nclose to its source. However, this approach is constrained by the limited\ncomputational and memory resources available at the edge. To address this\nchallenge, we explore compression techniques to reduce computational demands\nand inference time, alongside transfer learning methods to minimize training\noverhead. Using the Synthbuster, RAISE, and ForenSynths datasets, we evaluate\nthe effectiveness of pruning, knowledge distillation (KD), quantization,\nfine-tuning, and adapter-based techniques. Our experimental results demonstrate\nthat both compression and transfer learning can be effectively achieved, even\nwith a high compression level of 90%, remaining at the same performance level\nwhen the training and validation data originate from the same DeepFake model.\nHowever, when the testing dataset is generated by DeepFake models not present\nin the training set, a domain generalization issue becomes evident.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u7684\u538b\u7f29\u548c\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u8d44\u6e90\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u6709\u9650\uff0c\u4f46\u9700\u4fdd\u62a4\u6570\u636e\u9690\u79c1\uff0c\u56e0\u6b64\u9700\u4f18\u5316\u6a21\u578b\u4ee5\u51cf\u5c11\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u3002", "method": "\u91c7\u7528\u526a\u679d\u3001\u77e5\u8bc6\u84b8\u998f\u3001\u91cf\u5316\u3001\u5fae\u8c03\u548c\u9002\u914d\u5668\u6280\u672f\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u9ad8\u538b\u7f29\u7387\uff0890%\uff09\u4e0b\u6027\u80fd\u4e0d\u53d8\uff0c\u4f46\u6d4b\u8bd5\u6570\u636e\u6765\u81ea\u4e0d\u540c\u6a21\u578b\u65f6\u51fa\u73b0\u9886\u57df\u6cdb\u5316\u95ee\u9898\u3002", "conclusion": "\u538b\u7f29\u548c\u8fc1\u79fb\u5b66\u4e60\u6709\u6548\uff0c\u4f46\u9700\u89e3\u51b3\u8de8\u6a21\u578b\u6cdb\u5316\u95ee\u9898\u3002"}}
{"id": "2504.21027", "pdf": "https://arxiv.org/pdf/2504.21027", "abs": "https://arxiv.org/abs/2504.21027", "authors": ["Yu Zheng", "Longyi Liu", "Yuming Lin", "Jie Feng", "Guozhen Zhang", "Depeng Jin", "Yong Li"], "title": "UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of Large Language Models (LLMs) holds promise for revolutionizing\nvarious fields traditionally dominated by human expertise. Urban planning, a\nprofessional discipline that fundamentally shapes our daily surroundings, is\none such field heavily relying on multifaceted domain knowledge and experience\nof human experts. The extent to which LLMs can assist human practitioners in\nurban planning remains largely unexplored. In this paper, we introduce a\ncomprehensive benchmark, UrbanPlanBench, tailored to evaluate the efficacy of\nLLMs in urban planning, which encompasses fundamental principles, professional\nknowledge, and management and regulations, aligning closely with the\nqualifications expected of human planners. Through extensive evaluation, we\nreveal a significant imbalance in the acquisition of planning knowledge among\nLLMs, with even the most proficient models falling short of meeting\nprofessional standards. For instance, we observe that 70% of LLMs achieve\nsubpar performance in understanding planning regulations compared to other\naspects. Besides the benchmark, we present the largest-ever supervised\nfine-tuning (SFT) dataset, UrbanPlanText, comprising over 30,000 instruction\npairs sourced from urban planning exams and textbooks. Our findings demonstrate\nthat fine-tuned models exhibit enhanced performance in memorization tests and\ncomprehension of urban planning knowledge, while there exists significant room\nfor improvement, particularly in tasks requiring domain-specific terminology\nand reasoning. By making our benchmark, dataset, and associated evaluation and\nfine-tuning toolsets publicly available at\nhttps://github.com/tsinghua-fib-lab/PlanBench, we aim to catalyze the\nintegration of LLMs into practical urban planning, fostering a symbiotic\ncollaboration between human expertise and machine intelligence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86UrbanPlanBench\u57fa\u51c6\u548cUrbanPlanText\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u57ce\u5e02\u89c4\u5212\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u4e13\u4e1a\u80fd\u529b\u4e0d\u8db3\uff0c\u4f46\u901a\u8fc7\u5fae\u8c03\u53ef\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u57ce\u5e02\u89c4\u5212\u9886\u57df\u7684\u6f5c\u529b\uff0c\u586b\u8865\u5176\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u5f15\u5165UrbanPlanBench\u57fa\u51c6\u548cUrbanPlanText\u6570\u636e\u96c6\uff0830,000+\u6307\u4ee4\u5bf9\uff09\uff0c\u8bc4\u4f30\u5e76\u5fae\u8c03LLMs\u3002", "result": "LLMs\u5728\u89c4\u5212\u6cd5\u89c4\u7406\u89e3\u7b49\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0870%\u6a21\u578b\u4f4e\u4e8e\u6807\u51c6\uff09\uff0c\u4f46\u5fae\u8c03\u540e\u6027\u80fd\u6709\u6240\u63d0\u5347\u3002", "conclusion": "LLMs\u5728\u57ce\u5e02\u89c4\u5212\u4e2d\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u516c\u5f00\u8d44\u6e90\u65e8\u5728\u4fc3\u8fdb\u4eba\u673a\u534f\u4f5c\u3002"}}
{"id": "2504.21263", "pdf": "https://arxiv.org/pdf/2504.21263", "abs": "https://arxiv.org/abs/2504.21263", "authors": ["Jinpeng Wang", "Tianci Luo", "Yaohua Zha", "Yan Feng", "Ruisheng Luo", "Bin Chen", "Tao Dai", "Long Chen", "Yaowei Wang", "Shu-Tao Xia"], "title": "Embracing Collaboration Over Competition: Condensing Multiple Prompts for Visual In-Context Learning", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": "Accepted by CVPR'25. 10 pages, 5 figures, 6 tables", "summary": "Visual In-Context Learning (VICL) enables adaptively solving vision tasks by\nleveraging pixel demonstrations, mimicking human-like task completion through\nanalogy. Prompt selection is critical in VICL, but current methods assume the\nexistence of a single \"ideal\" prompt in a pool of candidates, which in practice\nmay not hold true. Multiple suitable prompts may exist, but individually they\noften fall short, leading to difficulties in selection and the exclusion of\nuseful context. To address this, we propose a new perspective: prompt\ncondensation. Rather than relying on a single prompt, candidate prompts\ncollaborate to efficiently integrate informative contexts without sacrificing\nresolution. We devise Condenser, a lightweight external plugin that compresses\nrelevant fine-grained context across multiple prompts. Optimized end-to-end\nwith the backbone, Condenser ensures accurate integration of contextual cues.\nExperiments demonstrate Condenser outperforms state-of-the-arts across\nbenchmark tasks, showing superior context compression, scalability with more\nprompts, and enhanced computational efficiency compared to ensemble methods,\npositioning it as a highly competitive solution for VICL. Code is open-sourced\nat https://github.com/gimpong/CVPR25-Condenser.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCondenser\u7684\u8f7b\u91cf\u7ea7\u5916\u90e8\u63d2\u4ef6\uff0c\u901a\u8fc7\u591a\u63d0\u793a\u534f\u4f5c\u538b\u7f29\u7ec6\u7c92\u5ea6\u4e0a\u4e0b\u6587\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08VICL\uff09\u4e2d\u63d0\u793a\u9009\u62e9\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dVICL\u65b9\u6cd5\u5047\u8bbe\u5b58\u5728\u5355\u4e00\u7406\u60f3\u63d0\u793a\uff0c\u4f46\u5b9e\u8df5\u4e2d\u53ef\u80fd\u5b58\u5728\u591a\u4e2a\u5408\u9002\u63d0\u793a\uff0c\u5355\u72ec\u4f7f\u7528\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u5bfc\u81f4\u9009\u62e9\u56f0\u96be\u548c\u6709\u7528\u4e0a\u4e0b\u6587\u7684\u9057\u6f0f\u3002", "method": "\u63d0\u51fa\u63d0\u793a\u538b\u7f29\uff08prompt condensation\uff09\u65b9\u6cd5\uff0c\u8bbe\u8ba1Condenser\u63d2\u4ef6\uff0c\u901a\u8fc7\u591a\u63d0\u793a\u534f\u4f5c\u9ad8\u6548\u6574\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4fdd\u6301\u5206\u8fa8\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCondenser\u5728\u57fa\u51c6\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u597d\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u80fd\u529b\u3001\u53ef\u6269\u5c55\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "Condenser\u4e3aVICL\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7ade\u4e89\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.21643", "pdf": "https://arxiv.org/pdf/2504.21643", "abs": "https://arxiv.org/abs/2504.21643", "authors": ["Luca Marzari", "Francesco Trotti", "Enrico Marchesini", "Alessandro Farinelli"], "title": "Designing Control Barrier Function via Probabilistic Enumeration for Safe Reinforcement Learning Navigation", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Achieving safe autonomous navigation systems is critical for deploying robots\nin dynamic and uncertain real-world environments. In this paper, we propose a\nhierarchical control framework leveraging neural network verification\ntechniques to design control barrier functions (CBFs) and policy correction\nmechanisms that ensure safe reinforcement learning navigation policies. Our\napproach relies on probabilistic enumeration to identify unsafe regions of\noperation, which are then used to construct a safe CBF-based control layer\napplicable to arbitrary policies. We validate our framework both in simulation\nand on a real robot, using a standard mobile robot benchmark and a highly\ndynamic aquatic environmental monitoring task. These experiments demonstrate\nthe ability of the proposed solution to correct unsafe actions while preserving\nefficient navigation behavior. Our results show the promise of developing\nhierarchical verification-based systems to enable safe and robust navigation\nbehaviors in complex scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\u7684\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u786e\u4fdd\u5f3a\u5316\u5b66\u4e60\u5bfc\u822a\u7b56\u7565\u7684\u5b89\u5168\u6027\uff0c\u5e76\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u7684\u771f\u5b9e\u73af\u5883\u4e2d\uff0c\u5b9e\u73b0\u5b89\u5168\u7684\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u5bf9\u673a\u5668\u4eba\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u6982\u7387\u679a\u4e3e\u8bc6\u522b\u4e0d\u5b89\u5168\u64cd\u4f5c\u533a\u57df\uff0c\u6784\u5efa\u57fa\u4e8e\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBF\uff09\u7684\u5b89\u5168\u63a7\u5236\u5c42\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u7ea0\u6b63\u4e0d\u5b89\u5168\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u5bfc\u822a\u6027\u80fd\u3002", "conclusion": "\u5206\u5c42\u9a8c\u8bc1\u7cfb\u7edf\u6709\u671b\u5728\u590d\u6742\u573a\u666f\u4e2d\u5b9e\u73b0\u5b89\u5168\u3001\u9c81\u68d2\u7684\u5bfc\u822a\u884c\u4e3a\u3002"}}
{"id": "2504.21069", "pdf": "https://arxiv.org/pdf/2504.21069", "abs": "https://arxiv.org/abs/2504.21069", "authors": ["Anuradha Kumari", "Mushir Akhtar", "P. N. Suganthan", "M. Tanveer"], "title": "R^2VFL: A Robust Random Vector Functional Link Network with Huber-Weighted Framework", "categories": ["cs.LG"], "comment": null, "summary": "The random vector functional link (RVFL) neural network has shown significant\npotential in overcoming the constraints of traditional artificial neural\nnetworks, such as excessive computation time and suboptimal solutions. However,\nRVFL faces challenges when dealing with noise and outliers, as it assumes all\ndata samples contribute equally. To address this issue, we propose a novel\nrobust framework, R2VFL, RVFL with Huber weighting function and class\nprobability, which enhances the model's robustness and adaptability by\neffectively mitigating the impact of noise and outliers in the training data.\nThe Huber weighting function reduces the influence of outliers, while the class\nprobability mechanism assigns less weight to noisy data points, resulting in a\nmore resilient model. We explore two distinct approaches for calculating class\ncenters within the R2VFL framework: the simple average of all data points in\neach class and the median of each feature, the later providing a robust\nalternative by minimizing the effect of extreme values. These approaches give\nrise to two novel variants of the model-R2VFL-A and R2VFL-M. We extensively\nevaluate the proposed models on 47 UCI datasets, encompassing both binary and\nmulticlass datasets, and conduct rigorous statistical testing, which confirms\nthe superiority of the proposed models. Notably, the models also demonstrate\nexceptional performance in classifying EEG signals, highlighting their\npractical applicability in real-world biomedical domain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aR2VFL\u7684\u9c81\u68d2\u6846\u67b6\uff0c\u7ed3\u5408Huber\u52a0\u6743\u51fd\u6570\u548c\u7c7b\u522b\u6982\u7387\uff0c\u6709\u6548\u51cf\u5c11\u566a\u58f0\u548c\u5f02\u5e38\u503c\u5bf9RVFL\u795e\u7ecf\u7f51\u7edc\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u53d8\u4f53R2VFL-A\u548cR2VFL-M\u3002", "motivation": "RVFL\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u566a\u58f0\u548c\u5f02\u5e38\u503c\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5047\u8bbe\u6240\u6709\u6570\u636e\u6837\u672c\u8d21\u732e\u5747\u7b49\uff0c\u5bfc\u81f4\u6a21\u578b\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u5f15\u5165Huber\u52a0\u6743\u51fd\u6570\u51cf\u5c11\u5f02\u5e38\u503c\u5f71\u54cd\uff0c\u7ed3\u5408\u7c7b\u522b\u6982\u7387\u673a\u5236\u964d\u4f4e\u566a\u58f0\u6570\u636e\u6743\u91cd\uff1b\u63d0\u51fa\u4e24\u79cd\u8ba1\u7b97\u7c7b\u522b\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff08\u5e73\u5747\u503c\u548c\u4e2d\u4f4d\u6570\uff09\uff0c\u5f62\u6210R2VFL-A\u548cR2VFL-M\u53d8\u4f53\u3002", "result": "\u572847\u4e2aUCI\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u5728EEG\u4fe1\u53f7\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "R2VFL\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\uff0c\u5c24\u5176\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.21117", "pdf": "https://arxiv.org/pdf/2504.21117", "abs": "https://arxiv.org/abs/2504.21117", "authors": ["Hanhua Hong", "Chenghao Xiao", "Yang Wang", "Yiqi Liu", "Wenge Rong", "Chenghua Lin"], "title": "Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts", "categories": ["cs.CL"], "comment": "10 pages", "summary": "Evaluating natural language generation (NLG) systems is challenging due to\nthe diversity of valid outputs. While human evaluation is the gold standard, it\nsuffers from inconsistencies, lack of standardisation, and demographic biases,\nlimiting reproducibility. LLM-based evaluation offers a scalable alternative\nbut is highly sensitive to prompt design, where small variations can lead to\nsignificant discrepancies. In this work, we propose an inversion learning\nmethod that learns effective reverse mappings from model outputs back to their\ninput instructions, enabling the automatic generation of highly effective,\nmodel-specific evaluation prompts. Our method requires only a single evaluation\nsample and eliminates the need for time-consuming manual prompt engineering,\nthereby improving both efficiency and robustness. Our work contributes toward a\nnew direction for more robust and efficient LLM-based evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53cd\u8f6c\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u81ea\u52a8\u751f\u6210\u9ad8\u6548\u3001\u6a21\u578b\u7279\u5b9a\u7684\u8bc4\u4f30\u63d0\u793a\uff0c\u63d0\u5347LLM\u8bc4\u4f30\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "NLG\u7cfb\u7edf\u8bc4\u4f30\u56e0\u8f93\u51fa\u591a\u6837\u6027\u800c\u590d\u6742\uff0c\u4eba\u5de5\u8bc4\u4f30\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u548c\u504f\u89c1\uff0cLLM\u8bc4\u4f30\u5219\u5bf9\u63d0\u793a\u8bbe\u8ba1\u654f\u611f\u3002", "method": "\u901a\u8fc7\u53cd\u8f6c\u5b66\u4e60\uff0c\u4ece\u6a21\u578b\u8f93\u51fa\u53cd\u5411\u6620\u5c04\u5230\u8f93\u5165\u6307\u4ee4\uff0c\u81ea\u52a8\u751f\u6210\u8bc4\u4f30\u63d0\u793a\uff0c\u4ec5\u9700\u5355\u4e00\u6837\u672c\u3002", "result": "\u65b9\u6cd5\u65e0\u9700\u624b\u52a8\u8bbe\u8ba1\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u8bc4\u4f30\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u4e3aLLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.21266", "pdf": "https://arxiv.org/pdf/2504.21266", "abs": "https://arxiv.org/abs/2504.21266", "authors": ["Zhifu Zhao", "Hanyang Hua", "Jianan Li", "Shaoxin Wu", "Fu Li", "Yangtao Zhou", "Yang Li"], "title": "CoCoDiff: Diversifying Skeleton Action Features via Coarse-Fine Text-Co-Guided Latent Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "In action recognition tasks, feature diversity is essential for enhancing\nmodel generalization and performance. Existing methods typically promote\nfeature diversity by expanding the training data in the sample space, which\noften leads to inefficiencies and semantic inconsistencies. To overcome these\nproblems, we propose a novel Coarse-fine text co-guidance Diffusion model\n(CoCoDiff). CoCoDiff generates diverse yet semantically consistent features in\nthe latent space by leveraging diffusion and multi-granularity textual\nguidance. Specifically, our approach feeds spatio-temporal features extracted\nfrom skeleton sequences into a latent diffusion model to generate diverse\naction representations. Meanwhile, we introduce a coarse-fine text co-guided\nstrategy that leverages textual information from large language models (LLMs)\nto ensure semantic consistency between the generated features and the original\ninputs. It is noted that CoCoDiff operates as a plug-and-play auxiliary module\nduring training, incurring no additional inference cost. Extensive experiments\ndemonstrate that CoCoDiff achieves SOTA performance on skeleton-based action\nrecognition benchmarks, including NTU RGB+D, NTU RGB+D 120 and\nKinetics-Skeleton.", "AI": {"tldr": "CoCoDiff\u901a\u8fc7\u6f5c\u7a7a\u95f4\u6269\u6563\u6a21\u578b\u548c\u591a\u7c92\u5ea6\u6587\u672c\u5f15\u5bfc\u751f\u6210\u591a\u6837\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u7279\u5f81\uff0c\u63d0\u5347\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u6269\u5c55\u6837\u672c\u7a7a\u95f4\u63d0\u5347\u7279\u5f81\u591a\u6837\u6027\uff0c\u4f46\u6548\u7387\u4f4e\u4e14\u8bed\u4e49\u4e0d\u4e00\u81f4\u3002", "method": "CoCoDiff\u7ed3\u5408\u6f5c\u7a7a\u95f4\u6269\u6563\u6a21\u578b\u548c\u7c97-\u7ec6\u7c92\u5ea6\u6587\u672c\u5f15\u5bfc\uff0c\u751f\u6210\u591a\u6837\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u52a8\u4f5c\u7279\u5f81\u3002", "result": "\u5728NTU RGB+D\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "CoCoDiff\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u9ad8\u6548\u63d0\u5347\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2504.21659", "pdf": "https://arxiv.org/pdf/2504.21659", "abs": "https://arxiv.org/abs/2504.21659", "authors": ["Haotian Luo", "Haiying He", "Yibo Wang", "Jinluan Yang", "Rui Liu", "Naiqiang Tan", "Xiaochun Cao", "Dacheng Tao", "Li Shen"], "title": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recently, long-thought reasoning models achieve strong performance on complex\nreasoning tasks, but often incur substantial inference overhead, making\nefficiency a critical concern. Our empirical analysis reveals that the benefit\nof using Long-CoT varies across problems: while some problems require elaborate\nreasoning, others show no improvement, or even degraded accuracy. This\nmotivates adaptive reasoning strategies that tailor reasoning depth to the\ninput. However, prior work primarily reduces redundancy within long reasoning\npaths, limiting exploration of more efficient strategies beyond the Long-CoT\nparadigm. To address this, we propose a novel two-stage framework for adaptive\nand efficient reasoning. First, we construct a hybrid reasoning model by\nmerging long and short CoT models to enable diverse reasoning styles. Second,\nwe apply bi-level preference training to guide the model to select suitable\nreasoning styles (group-level), and prefer concise and correct reasoning within\neach style group (instance-level). Experiments demonstrate that our method\nsignificantly reduces inference costs compared to other baseline approaches,\nwhile maintaining performance. Notably, on five mathematical datasets, the\naverage length of reasoning is reduced by more than 50%, highlighting the\npotential of adaptive strategies to optimize reasoning efficiency in large\nlanguage models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u957f\u77ed\u63a8\u7406\u8def\u5f84\u548c\u53cc\u5c42\u504f\u597d\u8bad\u7ec3\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u957f\u63a8\u7406\u6a21\u578b\u5728\u4e0d\u540c\u95ee\u9898\u4e0a\u7684\u6548\u679c\u5dee\u5f02\u663e\u8457\uff0c\u9700\u8981\u6839\u636e\u8f93\u5165\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ec5\u51cf\u5c11\u5197\u4f59\u8def\u5f84\uff0c\u672a\u80fd\u63a2\u7d22\u66f4\u9ad8\u6548\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u6784\u5efa\u6df7\u5408\u63a8\u7406\u6a21\u578b\uff0c\u7ed3\u5408\u957f\u77ed\u63a8\u7406\u8def\u5f84\uff1b2\uff09\u901a\u8fc7\u53cc\u5c42\u504f\u597d\u8bad\u7ec3\u9009\u62e9\u5408\u9002\u63a8\u7406\u98ce\u683c\u5e76\u4f18\u5316\u63a8\u7406\u7b80\u6d01\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e94\u4e2a\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63a8\u7406\u957f\u5ea6\u51cf\u5c1150%\u4ee5\u4e0a\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u4e14\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "\u81ea\u9002\u5e94\u7b56\u7565\u80fd\u6709\u6548\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u6f5c\u529b\u3002"}}
{"id": "2504.21099", "pdf": "https://arxiv.org/pdf/2504.21099", "abs": "https://arxiv.org/abs/2504.21099", "authors": ["Jieming Bian", "Yuanzhe Peng", "Lei Wang", "Yin Huang", "Jie Xu"], "title": "A Survey on Parameter-Efficient Fine-Tuning for Foundation Models in Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": "survey paper, under updating", "summary": "Foundation models have revolutionized artificial intelligence by providing\nrobust, versatile architectures pre-trained on large-scale datasets. However,\nadapting these massive models to specific downstream tasks requires\nfine-tuning, which can be prohibitively expensive in computational resources.\nParameter-Efficient Fine-Tuning (PEFT) methods address this challenge by\nselectively updating only a small subset of parameters. Meanwhile, Federated\nLearning (FL) enables collaborative model training across distributed clients\nwithout sharing raw data, making it ideal for privacy-sensitive applications.\nThis survey provides a comprehensive review of the integration of PEFT\ntechniques within federated learning environments. We systematically categorize\nexisting approaches into three main groups: Additive PEFT (which introduces new\ntrainable parameters), Selective PEFT (which fine-tunes only subsets of\nexisting parameters), and Reparameterized PEFT (which transforms model\narchitectures to enable efficient updates). For each category, we analyze how\nthese methods address the unique challenges of federated settings, including\ndata heterogeneity, communication efficiency, computational constraints, and\nprivacy concerns. We further organize the literature based on application\ndomains, covering both natural language processing and computer vision tasks.\nFinally, we discuss promising research directions, including scaling to larger\nfoundation models, theoretical analysis of federated PEFT methods, and\nsustainable approaches for resource-constrained environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u4e0e\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7684\u7ed3\u5408\uff0c\u7cfb\u7edf\u5206\u7c7b\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u6570\u636e\u5f02\u6784\u6027\u3001\u901a\u4fe1\u6548\u7387\u7b49\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u5fae\u8c03\u65f6\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u5c06PEFT\u65b9\u6cd5\u5206\u4e3a\u4e09\u7c7b\uff1a\u52a0\u6cd5PEFT\u3001\u9009\u62e9\u6027PEFT\u548c\u91cd\u53c2\u6570\u5316PEFT\uff0c\u5e76\u5206\u6790\u5176\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u603b\u7ed3\u4e86PEFT\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u4f18\u52bf\u4e0e\u6311\u6218\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "PEFT\u4e0eFL\u7684\u7ed3\u5408\u4e3a\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2504.21132", "pdf": "https://arxiv.org/pdf/2504.21132", "abs": "https://arxiv.org/abs/2504.21132", "authors": ["Naheed Rayhan", "Md. Ashrafuzzaman"], "title": "LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs), such as ChatGPT, have demonstrated the\ncapability to generate human like, natural responses across a range of tasks,\nincluding task oriented dialogue and question answering. However, their\napplication in real world, critical scenarios is often hindered by a tendency\nto produce inaccurate information and a limited ability to leverage external\nknowledge sources. This paper introduces the LLM ENHANCER system, designed to\nintegrate multiple online sources such as Google, Wikipedia, and DuckDuckGo to\nenhance data accuracy. The LLMs employed within this system are open source.\nThe data acquisition process for the LLM ENHANCER system operates in parallel,\nutilizing custom agent tools to manage the flow of information. Vector\nembeddings are used to identify the most pertinent information, which is\nsubsequently supplied to the LLM for user interaction. The LLM ENHANCER system\nmitigates hallucinations in chat based LLMs while preserving response\nnaturalness and accuracy.", "AI": {"tldr": "LLM ENHANCER\u7cfb\u7edf\u901a\u8fc7\u6574\u5408\u591a\u6e90\u5728\u7ebf\u6570\u636e\u63d0\u5347LLMs\u7684\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u5e7b\u89c9\uff0c\u540c\u65f6\u4fdd\u6301\u56de\u7b54\u7684\u81ea\u7136\u6027\u3002", "motivation": "LLMs\u5728\u5173\u952e\u573a\u666f\u4e2d\u56e0\u751f\u6210\u4e0d\u51c6\u786e\u4fe1\u606f\u548c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u800c\u53d7\u9650\u3002", "method": "\u7cfb\u7edf\u5e76\u884c\u6574\u5408Google\u3001Wikipedia\u7b49\u5728\u7ebf\u8d44\u6e90\uff0c\u5229\u7528\u5411\u91cf\u5d4c\u5165\u7b5b\u9009\u4fe1\u606f\u540e\u8f93\u5165LLM\u3002", "result": "\u7cfb\u7edf\u663e\u8457\u51cf\u5c11\u4e86LLMs\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u56de\u7b54\u7684\u81ea\u7136\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "LLM ENHANCER\u4e3aLLMs\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21281", "pdf": "https://arxiv.org/pdf/2504.21281", "abs": "https://arxiv.org/abs/2504.21281", "authors": ["Zexin Ji", "Beiji Zou", "Xiaoyan Kui", "Hua Li", "Pierre Vera", "Su Ruan"], "title": "Mamba Based Feature Extraction And Adaptive Multilevel Feature Fusion For 3D Tumor Segmentation From Multi-modal Medical Image", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal 3D medical image segmentation aims to accurately identify tumor\nregions across different modalities, facing challenges from variations in image\nintensity and tumor morphology. Traditional convolutional neural network\n(CNN)-based methods struggle with capturing global features, while\nTransformers-based methods, despite effectively capturing global context,\nencounter high computational costs in 3D medical image segmentation. The Mamba\nmodel combines linear scalability with long-distance modeling, making it a\npromising approach for visual representation learning. However, Mamba-based 3D\nmulti-modal segmentation still struggles to leverage modality-specific features\nand fuse complementary information effectively. In this paper, we propose a\nMamba based feature extraction and adaptive multilevel feature fusion for 3D\ntumor segmentation using multi-modal medical image. We first develop the\nspecific modality Mamba encoder to efficiently extract long-range relevant\nfeatures that represent anatomical and pathological structures present in each\nmodality. Moreover, we design an bi-level synergistic integration block that\ndynamically merges multi-modal and multi-level complementary features by the\nmodality attention and channel attention learning. Lastly, the decoder combines\ndeep semantic information with fine-grained details to generate the tumor\nsegmentation map. Experimental results on medical image datasets (PET/CT and\nMRI multi-sequence) show that our approach achieve competitive performance\ncompared to the state-of-the-art CNN, Transformer, and Mamba-based approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u7279\u5f81\u63d0\u53d6\u548c\u81ea\u9002\u5e94\u591a\u7ea7\u7279\u5f81\u878d\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u6a21\u60013D\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u80bf\u7624\u5206\u5272\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfCNN\u548cTransformer\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u591a\u6a21\u60013D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9762\u4e34\u56fe\u50cf\u5f3a\u5ea6\u548c\u80bf\u7624\u5f62\u6001\u53d8\u5316\u7684\u6311\u6218\uff0c\u4f20\u7edfCNN\u96be\u4ee5\u6355\u6349\u5168\u5c40\u7279\u5f81\uff0c\u800cTransformer\u8ba1\u7b97\u6210\u672c\u9ad8\u3002Mamba\u6a21\u578b\u7ed3\u5408\u4e86\u7ebf\u6027\u6269\u5c55\u6027\u548c\u957f\u8ddd\u79bb\u5efa\u6a21\u80fd\u529b\uff0c\u4f46\u5728\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u7279\u5b9a\u6a21\u6001\u7684Mamba\u7f16\u7801\u5668\u63d0\u53d6\u957f\u7a0b\u76f8\u5173\u7279\u5f81\uff0c\u8bbe\u8ba1\u4e86\u53cc\u7ea7\u534f\u540c\u96c6\u6210\u5757\u52a8\u6001\u878d\u5408\u591a\u6a21\u6001\u548c\u591a\u7ea7\u7279\u5f81\uff0c\u89e3\u7801\u5668\u7ed3\u5408\u8bed\u4e49\u4fe1\u606f\u548c\u7ec6\u8282\u751f\u6210\u5206\u5272\u56fe\u3002", "result": "\u5728PET/CT\u548cMRI\u591a\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684CNN\u3001Transformer\u548cMamba\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u60013D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u7279\u5f81\u63d0\u53d6\u548c\u878d\u5408\u95ee\u9898\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5b9e\u7528\u6027\u548c\u7ade\u4e89\u529b\u3002"}}
{"id": "2504.21683", "pdf": "https://arxiv.org/pdf/2504.21683", "abs": "https://arxiv.org/abs/2504.21683", "authors": ["Kenneth Skiba", "Tjitze Rienstra", "Matthias Thimm", "Jesse Heyninck", "Gabriele Kern-Isberner"], "title": "Extension-ranking Semantics for Abstract Argumentation Preprint", "categories": ["cs.AI"], "comment": null, "summary": "In this paper, we present a general framework for ranking sets of arguments\nin abstract argumentation based on their plausibility of acceptance. We present\na generalisation of Dung's extension semantics as extension-ranking semantics,\nwhich induce a preorder over the power set of all arguments, allowing us to\nstate that one set is \"closer\" to being acceptable than another. To evaluate\nthe extension-ranking semantics, we introduce a number of principles that a\nwell-behaved extension-ranking semantics should satisfy. We consider several\nsimple base relations, each of which models a single central aspect of\nargumentative reasoning. The combination of these base relations provides us\nwith a family of extension-ranking semantics. We also adapt a number of\napproaches from the literature for ranking extensions to be usable in the\ncontext of extension-ranking semantics, and evaluate their behaviour.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bba\u8bc1\u53ef\u63a5\u53d7\u6027\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u62bd\u8c61\u8bba\u8bc1\u4e2d\u7684\u8bba\u8bc1\u96c6\u8fdb\u884c\u6392\u5e8f\u3002\u901a\u8fc7\u6269\u5c55Dung\u7684\u8bed\u4e49\u4e3a\u6269\u5c55\u6392\u5e8f\u8bed\u4e49\uff0c\u751f\u6210\u8bba\u8bc1\u96c6\u7684\u9884\u5e8f\uff0c\u5e76\u5f15\u5165\u884c\u4e3a\u826f\u597d\u7684\u8bed\u4e49\u5e94\u6ee1\u8db3\u7684\u539f\u5219\u3002", "motivation": "\u65e8\u5728\u4e3a\u62bd\u8c61\u8bba\u8bc1\u4e2d\u7684\u8bba\u8bc1\u96c6\u63d0\u4f9b\u4e00\u79cd\u901a\u7528\u7684\u6392\u5e8f\u65b9\u6cd5\uff0c\u4ee5\u8861\u91cf\u5176\u53ef\u63a5\u53d7\u6027\u3002", "method": "\u6269\u5c55Dung\u7684\u8bed\u4e49\u4e3a\u6269\u5c55\u6392\u5e8f\u8bed\u4e49\uff0c\u5f15\u5165\u591a\u4e2a\u57fa\u672c\u539f\u5219\uff0c\u5e76\u7ed3\u5408\u7b80\u5355\u57fa\u7840\u5173\u7cfb\u6784\u5efa\u8bed\u4e49\u5bb6\u65cf\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u6269\u5c55\u6392\u5e8f\u8bed\u4e49\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u884c\u4e3a\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bba\u8bc1\u96c6\u7684\u6392\u5e8f\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u7406\u8bba\u652f\u6301\u7684\u65b9\u6cd5\u3002"}}
{"id": "2504.21152", "pdf": "https://arxiv.org/pdf/2504.21152", "abs": "https://arxiv.org/abs/2504.21152", "authors": ["Shayan Alahyari", "Mike Domaratzki"], "title": "SMOGAN: Synthetic Minority Oversampling with GAN Refinement for Imbalanced Regression", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Imbalanced regression refers to prediction tasks where the target variable is\nskewed. This skewness hinders machine learning models, especially neural\nnetworks, which concentrate on dense regions and therefore perform poorly on\nunderrepresented (minority) samples. Despite the importance of this problem,\nonly a few methods have been proposed for imbalanced regression. Many of the\navailable solutions for imbalanced regression adapt techniques from the class\nimbalance domain, such as linear interpolation and the addition of Gaussian\nnoise, to create synthetic data in sparse regions. However, in many cases, the\nunderlying distribution of the data is complex and non-linear. Consequently,\nthese approaches generate synthetic samples that do not accurately represent\nthe true feature-target relationship. To overcome these limitations, we propose\nSMOGAN, a two-step oversampling framework for imbalanced regression. In Stage\n1, an existing oversampler generates initial synthetic samples in sparse target\nregions. In Stage 2, we introduce DistGAN, a distribution-aware GAN that serves\nas SMOGAN's filtering layer and refines these samples via adversarial loss\naugmented with a Maximum Mean Discrepancy objective, aligning them with the\ntrue joint feature-target distribution. Extensive experiments on 23 imbalanced\ndatasets show that SMOGAN consistently outperforms the default oversampling\nmethod without the DistGAN filtering layer.", "AI": {"tldr": "SMOGAN\u662f\u4e00\u79cd\u7528\u4e8e\u4e0d\u5e73\u8861\u56de\u5f52\u7684\u4e24\u9636\u6bb5\u8fc7\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7DistGAN\u8fc7\u6ee4\u5c42\u6539\u8fdb\u5408\u6210\u6837\u672c\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4e0d\u5e73\u8861\u56de\u5f52\u4e2d\u76ee\u6807\u53d8\u91cf\u504f\u659c\u5bfc\u81f4\u6a21\u578b\u5728\u7a00\u758f\u533a\u57df\u8868\u73b0\u5dee\uff0c\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u7684\u5408\u6210\u6837\u672c\u4e0d\u80fd\u51c6\u786e\u53cd\u6620\u771f\u5b9e\u5206\u5e03\u3002", "method": "SMOGAN\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u521d\u59cb\u5408\u6210\u6837\u672c\uff0c\u7b2c\u4e8c\u9636\u6bb5\u7528DistGAN\u8fc7\u6ee4\u5e76\u4f18\u5316\u6837\u672c\u3002", "result": "\u572823\u4e2a\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\uff0cSMOGAN\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u8fc7\u91c7\u6837\u65b9\u6cd5\u3002", "conclusion": "SMOGAN\u901a\u8fc7\u5206\u5e03\u611f\u77e5\u7684GAN\u6709\u6548\u63d0\u5347\u4e0d\u5e73\u8861\u56de\u5f52\u6027\u80fd\u3002"}}
{"id": "2504.21165", "pdf": "https://arxiv.org/pdf/2504.21165", "abs": "https://arxiv.org/abs/2504.21165", "authors": ["Mark Huasong Meng", "Ruizhe Wang", "Meng Xu", "Chuan Yan", "Guangdong Bai"], "title": "Detecting Manipulated Contents Using Knowledge-Grounded Inference", "categories": ["cs.CL", "cs.SI"], "comment": "16 pages", "summary": "The detection of manipulated content, a prevalent form of fake news, has been\nwidely studied in recent years. While existing solutions have been proven\neffective in fact-checking and analyzing fake news based on historical events,\nthe reliance on either intrinsic knowledge obtained during training or manually\ncurated context hinders them from tackling zero-day manipulated content, which\ncan only be recognized with real-time contextual information. In this work, we\npropose Manicod, a tool designed for detecting zero-day manipulated content.\nManicod first sources contextual information about the input claim from\nmainstream search engines, and subsequently vectorizes the context for the\nlarge language model (LLM) through retrieval-augmented generation (RAG). The\nLLM-based inference can produce a \"truthful\" or \"manipulated\" decision and\noffer a textual explanation for the decision. To validate the effectiveness of\nManicod, we also propose a dataset comprising 4270 pieces of manipulated fake\nnews derived from 2500 recent real-world news headlines. Manicod achieves an\noverall F1 score of 0.856 on this dataset and outperforms existing methods by\nup to 1.9x in F1 score on their benchmarks on fact-checking and claim\nverification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aManicod\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u68c0\u6d4b\u96f6\u65e5\u64cd\u7eb5\u5185\u5bb9\uff0c\u901a\u8fc7\u5b9e\u65f6\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8bad\u7ec3\u65f6\u7684\u56fa\u6709\u77e5\u8bc6\u6216\u624b\u52a8\u6574\u7406\u7684\u4e0a\u4e0b\u6587\uff0c\u65e0\u6cd5\u6709\u6548\u68c0\u6d4b\u96f6\u65e5\u64cd\u7eb5\u5185\u5bb9\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5229\u7528\u5b9e\u65f6\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "Manicod\u901a\u8fc7\u4e3b\u6d41\u641c\u7d22\u5f15\u64ce\u83b7\u53d6\u8f93\u5165\u58f0\u660e\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u5c06\u5176\u5411\u91cf\u5316\uff0c\u518d\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff0c\u751f\u6210\u5224\u65ad\u548c\u89e3\u91ca\u3002", "result": "\u5728\u5305\u542b4270\u6761\u64cd\u7eb5\u5047\u65b0\u95fb\u7684\u6570\u636e\u96c6\u4e0a\uff0cManicod\u7684F1\u5f97\u5206\u4e3a0.856\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u4e8b\u5b9e\u6838\u67e5\u548c\u58f0\u660e\u9a8c\u8bc1\u4efb\u52a1\u4e0a\u9ad8\u51fa1.9\u500d\u3002", "conclusion": "Manicod\u901a\u8fc7\u7ed3\u5408\u5b9e\u65f6\u4e0a\u4e0b\u6587\u548cLLM\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u65e5\u64cd\u7eb5\u5185\u5bb9\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u5047\u65b0\u95fb\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.21292", "pdf": "https://arxiv.org/pdf/2504.21292", "abs": "https://arxiv.org/abs/2504.21292", "authors": ["ZiYi Dong", "Chengxing Zhou", "Weijian Deng", "Pengxu Wei", "Xiangyang Ji", "Liang Lin"], "title": "Can We Achieve Efficient Diffusion without Self-Attention? Distilling Self-Attention into Convolutions", "categories": ["cs.CV"], "comment": null, "summary": "Contemporary diffusion models built upon U-Net or Diffusion Transformer (DiT)\narchitectures have revolutionized image generation through transformer-based\nattention mechanisms. The prevailing paradigm has commonly employed\nself-attention with quadratic computational complexity to handle global spatial\nrelationships in complex images, thereby synthesizing high-fidelity images with\ncoherent visual semantics.Contrary to conventional wisdom, our systematic\nlayer-wise analysis reveals an interesting discrepancy: self-attention in\npre-trained diffusion models predominantly exhibits localized attention\npatterns, closely resembling convolutional inductive biases. This suggests that\nglobal interactions in self-attention may be less critical than commonly\nassumed.Driven by this, we propose \\(\\Delta\\)ConvFusion to replace conventional\nself-attention modules with Pyramid Convolution Blocks\n(\\(\\Delta\\)ConvBlocks).By distilling attention patterns into localized\nconvolutional operations while keeping other components frozen,\n\\(\\Delta\\)ConvFusion achieves performance comparable to transformer-based\ncounterparts while reducing computational cost by 6929$\\times$ and surpassing\nLinFusion by 5.42$\\times$ in efficiency--all without compromising generative\nfidelity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u0394ConvFusion\uff0c\u7528\u91d1\u5b57\u5854\u5377\u79ef\u5757\u66ff\u4ee3\u81ea\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u4e3b\u8981\u5448\u73b0\u5c40\u90e8\u5316\u6a21\u5f0f\uff0c\u6311\u6218\u4e86\u5168\u5c40\u4ea4\u4e92\u7684\u5fc5\u8981\u6027\u5047\u8bbe\u3002", "method": "\u901a\u8fc7\u84b8\u998f\u6ce8\u610f\u529b\u6a21\u5f0f\u4e3a\u5c40\u90e8\u5377\u79ef\u64cd\u4f5c\uff0c\u63d0\u51fa\u0394ConvFusion\uff0c\u4fdd\u7559\u5176\u4ed6\u7ec4\u4ef6\u4e0d\u53d8\u3002", "result": "\u0394ConvFusion\u8ba1\u7b97\u6210\u672c\u964d\u4f4e6929\u500d\uff0c\u6548\u7387\u8d85\u8fc7LinFusion 5.42\u500d\uff0c\u751f\u6210\u8d28\u91cf\u4e0d\u964d\u3002", "conclusion": "\u5c40\u90e8\u5377\u79ef\u53ef\u66ff\u4ee3\u81ea\u6ce8\u610f\u529b\uff0c\u4e3a\u9ad8\u6548\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2504.21694", "pdf": "https://arxiv.org/pdf/2504.21694", "abs": "https://arxiv.org/abs/2504.21694", "authors": ["Tom Westermann", "Malte Ramonat", "Johannes Hujer", "Felix Gehlhoff", "Alexander Fay"], "title": "Automatic Mapping of AutomationML Files to Ontologies for Graph Queries and Validation", "categories": ["cs.AI"], "comment": null, "summary": "AutomationML has seen widespread adoption as an open data exchange format in\nthe automation domain. It is an open and vendor neutral standard based on the\nextensible markup language XML. However, AutomationML extends XML with\nadditional semantics, that limit the applicability of common XML-tools for\napplications like querying or data validation. This article provides\npractitioners with 1) an up-to-date ontology of the concepts in the\nAutomationML-standard, as well as 2) a declarative mapping to automatically\ntransform any AutomationML model into RDF triples. Together, these artifacts\nallow practitioners an easy integration of AutomationML information into\nindustrial knowledge graphs. A study on examples from the automation domain\nconcludes that transforming AutomationML to OWL opens up new powerful ways for\nquerying and validation that are impossible without transformation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86AutomationML\u7684\u66f4\u65b0\u7248\u672c\u4f53\u548cRDF\u8f6c\u6362\u65b9\u6cd5\uff0c\u652f\u6301\u5de5\u4e1a\u77e5\u8bc6\u56fe\u8c31\u7684\u96c6\u6210\u3002", "motivation": "AutomationML\u4f5c\u4e3a\u81ea\u52a8\u5316\u9886\u57df\u7684\u5f00\u653e\u6570\u636e\u4ea4\u6362\u683c\u5f0f\uff0c\u5176\u6269\u5c55\u8bed\u4e49\u9650\u5236\u4e86XML\u5de5\u5177\u7684\u9002\u7528\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u67e5\u8be2\u548c\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u63d0\u4f9bAutomationML\u7684\u672c\u4f53\u6982\u5ff5\u548cRDF\u8f6c\u6362\u6620\u5c04\uff0c\u5b9e\u73b0\u6a21\u578b\u5230RDF\u4e09\u5143\u7ec4\u7684\u81ea\u52a8\u8f6c\u6362\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8f6c\u6362\u4e3aOWL\u540e\uff0c\u67e5\u8be2\u548c\u9a8c\u8bc1\u80fd\u529b\u663e\u8457\u63d0\u5347\u3002", "conclusion": "AutomationML\u5230RDF\u7684\u8f6c\u6362\u4e3a\u5de5\u4e1a\u77e5\u8bc6\u56fe\u8c31\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2504.21174", "pdf": "https://arxiv.org/pdf/2504.21174", "abs": "https://arxiv.org/abs/2504.21174", "authors": ["Leandro Giusti Mugnaini", "Bruno Lopes Yamamoto", "Lucas Lauton de Alcantara", "Victor Zacarias", "Edson Bollis", "Lucas Pellicer", "Anna Helena Reali Costa", "Artur Jordao"], "title": "Efficient LLMs with AMP: Attention Heads and MLP Pruning", "categories": ["cs.LG"], "comment": "To be published in International Joint Conference on Neural Networks\n  (IJCNN), 2025", "summary": "Deep learning drives a new wave in computing systems and triggers the\nautomation of increasingly complex problems. In particular, Large Language\nModels (LLMs) have significantly advanced cognitive tasks, often matching or\neven surpassing human-level performance. However, their extensive parameters\nresult in high computational costs and slow inference, posing challenges for\ndeployment in resource-limited settings. Among the strategies to overcome the\naforementioned challenges, pruning emerges as a successful mechanism since it\nreduces model size while maintaining predictive ability. In this paper, we\nintroduce AMP: Attention Heads and MLP Pruning, a novel structured pruning\nmethod that efficiently compresses LLMs by removing less critical structures\nwithin Multi-Head Attention (MHA) and Multilayer Perceptron (MLP). By\nprojecting the input data onto weights, AMP assesses structural importance and\novercomes the limitations of existing techniques, which often fall short in\nflexibility or efficiency. In particular, AMP surpasses the current\nstate-of-the-art on commonsense reasoning tasks by up to 1.49 percentage\npoints, achieving a 30% pruning ratio with minimal impact on zero-shot task\nperformance. Moreover, AMP also improves inference speeds, making it\nwell-suited for deployment in resource-constrained environments. We confirm the\nflexibility of AMP on different families of LLMs, including LLaMA and Phi.", "AI": {"tldr": "AMP\u662f\u4e00\u79cd\u65b0\u578b\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u79fb\u9664LLM\u4e2d\u4e0d\u592a\u91cd\u8981\u7684\u7ed3\u6784\uff08\u5982MHA\u548cMLP\uff09\uff0c\u9ad8\u6548\u538b\u7f29\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u5728\u63a8\u7406\u901f\u5ea6\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6162\u63a8\u7406\u901f\u5ea6\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u526a\u679d\u65b9\u6cd5\u3002", "method": "AMP\u901a\u8fc7\u5c06\u8f93\u5165\u6570\u636e\u6295\u5f71\u5230\u6743\u91cd\u4e0a\u8bc4\u4f30\u7ed3\u6784\u91cd\u8981\u6027\uff0c\u79fb\u9664MHA\u548cMLP\u4e2d\u4e0d\u592a\u5173\u952e\u7684\u90e8\u5206\uff0c\u514b\u670d\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\u3002", "result": "AMP\u5728\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u6280\u672f1.49\u4e2a\u767e\u5206\u70b9\uff0c\u5b9e\u73b030%\u526a\u679d\u7387\u4e14\u96f6\u6837\u672c\u4efb\u52a1\u6027\u80fd\u5f71\u54cd\u6700\u5c0f\uff0c\u540c\u65f6\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "AMP\u662f\u4e00\u79cd\u7075\u6d3b\u9ad8\u6548\u7684\u526a\u679d\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cdLLM\u5bb6\u65cf\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\u90e8\u7f72\u3002"}}
{"id": "2504.21191", "pdf": "https://arxiv.org/pdf/2504.21191", "abs": "https://arxiv.org/abs/2504.21191", "authors": ["Lovedeep Gondara", "Jonathan Simkin", "Graham Sayle", "Shebnum Devji", "Gregory Arbour", "Raymond Ng"], "title": "Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study aims to guide language model selection by investigating: 1) the\nnecessity of finetuning versus zero-shot usage, 2) the benefits of\ndomain-adjacent versus generic pretrained models, 3) the value of further\ndomain-specific pretraining, and 4) the continued relevance of Small Language\nModels (SLMs) compared to Large Language Models (LLMs) for specific tasks.\nUsing electronic pathology reports from the British Columbia Cancer Registry\n(BCCR), three classification scenarios with varying difficulty and data size\nare evaluated. Models include various SLMs and an LLM. SLMs are evaluated both\nzero-shot and finetuned; the LLM is evaluated zero-shot only. Finetuning\nsignificantly improved SLM performance across all scenarios compared to their\nzero-shot results. The zero-shot LLM outperformed zero-shot SLMs but was\nconsistently outperformed by finetuned SLMs. Domain-adjacent SLMs generally\nperformed better than the generic SLM after finetuning, especially on harder\ntasks. Further domain-specific pretraining yielded modest gains on easier tasks\nbut significant improvements on the complex, data-scarce task. The results\nhighlight the critical role of finetuning for SLMs in specialized domains,\nenabling them to surpass zero-shot LLM performance on targeted classification\ntasks. Pretraining on domain-adjacent or domain-specific data provides further\nadvantages, particularly for complex problems or limited finetuning data. While\nLLMs offer strong zero-shot capabilities, their performance on these specific\ntasks did not match that of appropriately finetuned SLMs. In the era of LLMs,\nSLMs remain relevant and effective, offering a potentially superior\nperformance-resource trade-off compared to LLMs.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u9009\u62e9\u7684\u56db\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86\u5fae\u8c03\u4e0e\u96f6\u6837\u672c\u3001\u9886\u57df\u90bb\u8fd1\u4e0e\u901a\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u8fdb\u4e00\u6b65\u9886\u57df\u9884\u8bad\u7ec3\u7684\u4ef7\u503c\uff0c\u4ee5\u53ca\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u6307\u5bfc\u8bed\u8a00\u6a21\u578b\u9009\u62e9\uff0c\u7279\u522b\u662f\u5728\u4e13\u4e1a\u9886\u57df\u4efb\u52a1\u4e2d\uff0c\u8bc4\u4f30\u5fae\u8c03\u3001\u9886\u57df\u9884\u8bad\u7ec3\u548c\u6a21\u578b\u89c4\u6a21\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528BCCR\u7684\u7535\u5b50\u75c5\u7406\u62a5\u544a\uff0c\u8bc4\u4f30\u4e09\u79cd\u4e0d\u540c\u96be\u5ea6\u548c\u6570\u636e\u91cf\u7684\u5206\u7c7b\u573a\u666f\uff0c\u6bd4\u8f83SLMs\u548cLLM\u7684\u96f6\u6837\u672c\u4e0e\u5fae\u8c03\u8868\u73b0\u3002", "result": "\u5fae\u8c03\u663e\u8457\u63d0\u5347SLMs\u6027\u80fd\uff0c\u4f18\u4e8e\u96f6\u6837\u672cLLM\uff1b\u9886\u57df\u90bb\u8fd1\u548c\u9886\u57df\u9884\u8bad\u7ec3\u5bf9\u590d\u6742\u4efb\u52a1\u5c24\u5176\u6709\u76ca\uff1bSLMs\u5728\u8d44\u6e90\u4e0e\u6027\u80fd\u6743\u8861\u4e0a\u4f18\u4e8eLLMs\u3002", "conclusion": "\u5728\u4e13\u4e1a\u9886\u57df\u4efb\u52a1\u4e2d\uff0c\u5fae\u8c03SLMs\u8868\u73b0\u4f18\u4e8e\u96f6\u6837\u672cLLMs\uff0c\u4e14SLMs\u5728\u8d44\u6e90\u6548\u7387\u4e0a\u66f4\u5177\u4f18\u52bf\uff0c\u8bc1\u660e\u4e86\u5176\u6301\u7eed\u76f8\u5173\u6027\u3002"}}
{"id": "2504.21294", "pdf": "https://arxiv.org/pdf/2504.21294", "abs": "https://arxiv.org/abs/2504.21294", "authors": ["Qianzi Yu", "Yang Cao", "Yu Kang"], "title": "Learning Multi-view Multi-class Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "The latest trend in anomaly detection is to train a unified model instead of\ntraining a separate model for each category. However, existing multi-class\nanomaly detection (MCAD) models perform poorly in multi-view scenarios because\nthey often fail to effectively model the relationships and complementary\ninformation among different views. In this paper, we introduce a Multi-View\nMulti-Class Anomaly Detection model (MVMCAD), which integrates information from\nmultiple views to accurately identify anomalies. Specifically, we propose a\nsemi-frozen encoder, where a pre-encoder prior enhancement mechanism is added\nbefore the frozen encoder, enabling stable cross-view feature modeling and\nefficient adaptation for improved anomaly detection. Furthermore, we propose an\nAnomaly Amplification Module (AAM) that models global token interactions and\nsuppresses normal regions to enhance anomaly signals, leading to improved\ndetection performance in multi-view settings. Finally, we propose a\nCross-Feature Loss that aligns shallow encoder features with deep decoder\nfeatures and vice versa, enhancing the model's sensitivity to anomalies at\ndifferent semantic levels under multi-view scenarios. Extensive experiments on\nthe Real-IAD dataset for multi-view multi-class anomaly detection validate the\neffectiveness of our approach, achieving state-of-the-art performance of\n91.0/88.6/82.1 and 99.1/43.9/48.2/95.2 for image-level and the pixel-level,\nrespectively.", "AI": {"tldr": "MVMCAD\u6a21\u578b\u901a\u8fc7\u591a\u89c6\u56fe\u6574\u5408\u548c\u5f02\u5e38\u4fe1\u53f7\u589e\u5f3a\uff0c\u5728\u591a\u89c6\u56fe\u591a\u7c7b\u5f02\u5e38\u68c0\u6d4b\u4e2d\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MCAD\u6a21\u578b\u5728\u591a\u89c6\u56fe\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u672a\u80fd\u6709\u6548\u5efa\u6a21\u89c6\u56fe\u95f4\u5173\u7cfb\u548c\u4e92\u8865\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u534a\u51bb\u7ed3\u7f16\u7801\u5668\u3001\u5f02\u5e38\u653e\u5927\u6a21\u5757\u548c\u8de8\u7279\u5f81\u635f\u5931\uff0c\u4f18\u5316\u591a\u89c6\u56fe\u7279\u5f81\u5efa\u6a21\u548c\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u5728Real-IAD\u6570\u636e\u96c6\u4e0a\uff0c\u56fe\u50cf\u7ea7\u548c\u50cf\u7d20\u7ea7\u68c0\u6d4b\u5206\u522b\u8fbe\u523091.0/88.6/82.1\u548c99.1/43.9/48.2/95.2\u3002", "conclusion": "MVMCAD\u5728\u591a\u89c6\u56fe\u591a\u7c7b\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.21774", "pdf": "https://arxiv.org/pdf/2504.21774", "abs": "https://arxiv.org/abs/2504.21774", "authors": ["Jiuwu Hao", "Liguo Sun", "Yuting Wan", "Yueyang Wu", "Ti Xiang", "Haolin Song", "Pin Lv"], "title": "Is Intermediate Fusion All You Need for UAV-based Collaborative Perception?", "categories": ["cs.AI"], "comment": null, "summary": "Collaborative perception enhances environmental awareness through inter-agent\ncommunication and is regarded as a promising solution to intelligent\ntransportation systems. However, existing collaborative methods for Unmanned\nAerial Vehicles (UAVs) overlook the unique characteristics of the UAV\nperspective, resulting in substantial communication overhead. To address this\nissue, we propose a novel communication-efficient collaborative perception\nframework based on late-intermediate fusion, dubbed LIF. The core concept is to\nexchange informative and compact detection results and shift the fusion stage\nto the feature representation level. In particular, we leverage vision-guided\npositional embedding (VPE) and box-based virtual augmented feature (BoBEV) to\neffectively integrate complementary information from various agents.\nAdditionally, we innovatively introduce an uncertainty-driven communication\nmechanism that uses uncertainty evaluation to select high-quality and reliable\nshared areas. Experimental results demonstrate that our LIF achieves superior\nperformance with minimal communication bandwidth, proving its effectiveness and\npracticality. Code and models are available at https://github.com/uestchjw/LIF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5ef6\u8fdf\u4e2d\u95f4\u878d\u5408\uff08LIF\uff09\u7684\u9ad8\u6548\u901a\u4fe1\u534f\u4f5c\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u6362\u7d27\u51d1\u7684\u68c0\u6d4b\u7ed3\u679c\u548c\u7279\u5f81\u7ea7\u878d\u5408\uff0c\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u534f\u4f5c\u611f\u77e5\u65b9\u6cd5\u672a\u8003\u8651\u65e0\u4eba\u673a\u89c6\u89d2\u7279\u6027\uff0c\u5bfc\u81f4\u901a\u4fe1\u5f00\u9500\u5927\u3002", "method": "\u91c7\u7528\u5ef6\u8fdf\u4e2d\u95f4\u878d\u5408\u6846\u67b6\uff08LIF\uff09\uff0c\u7ed3\u5408\u89c6\u89c9\u5f15\u5bfc\u4f4d\u7f6e\u5d4c\u5165\uff08VPE\uff09\u548c\u57fa\u4e8e\u6846\u7684\u865a\u62df\u589e\u5f3a\u7279\u5f81\uff08BoBEV\uff09\uff0c\u5e76\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u901a\u4fe1\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLIF\u5728\u4f4e\u901a\u4fe1\u5e26\u5bbd\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LIF\u6846\u67b6\u9ad8\u6548\u5b9e\u7528\uff0c\u9002\u7528\u4e8e\u65e0\u4eba\u673a\u534f\u4f5c\u611f\u77e5\u3002"}}
{"id": "2504.21186", "pdf": "https://arxiv.org/pdf/2504.21186", "abs": "https://arxiv.org/abs/2504.21186", "authors": ["Haoyan Xu", "Zhengtao Yao", "Xuzhi Zhang", "Ziyi Wang", "Langzhou He", "Yushun Dong", "Philip S. Yu", "Mengyuan Li", "Yue Zhao"], "title": "GLIP-OOD: Zero-Shot Graph OOD Detection with Foundation Model", "categories": ["cs.LG"], "comment": null, "summary": "Out-of-distribution (OOD) detection is critical for ensuring the safety and\nreliability of machine learning systems, particularly in dynamic and open-world\nenvironments. In the vision and text domains, zero-shot OOD detection - which\nrequires no training on in-distribution (ID) data - has made significant\nprogress through the use of large-scale pretrained models such as\nvision-language models (VLMs) and large language models (LLMs). However,\nzero-shot OOD detection in graph-structured data remains largely unexplored,\nprimarily due to the challenges posed by complex relational structures and the\nabsence of powerful, large-scale pretrained models for graphs. In this work, we\ntake the first step toward enabling zero-shot graph OOD detection by leveraging\na graph foundation model (GFM). We show that, when provided only with class\nlabel names, the GFM can perform OOD detection without any node-level\nsupervision - outperforming existing supervised methods across multiple\ndatasets. To address the more practical setting where OOD label names are\nunavailable, we introduce GLIP-OOD, a novel framework that employs LLMs to\ngenerate semantically informative pseudo-OOD labels from unlabeled data. These\nlabels enable the GFM to capture nuanced semantic boundaries between ID and OOD\nclasses and perform fine-grained OOD detection - without requiring any labeled\nnodes. Our approach is the first to enable node-level graph OOD detection in a\nfully zero-shot setting, and achieves state-of-the-art performance on four\nbenchmark text-attributed graph datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u63a2\u7d22\u4e86\u56fe\u7ed3\u6784\u6570\u636e\u4e2d\u7684\u96f6\u6837\u672cOOD\u68c0\u6d4b\uff0c\u5229\u7528\u56fe\u57fa\u7840\u6a21\u578b\uff08GFM\uff09\u548cLLM\u751f\u6210\u7684\u4f2aOOD\u6807\u7b7e\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u76d1\u7763\u7684OOD\u68c0\u6d4b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u56fe\u7ed3\u6784\u6570\u636e\u7684\u96f6\u6837\u672cOOD\u68c0\u6d4b\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u4e3b\u8981\u56e0\u590d\u6742\u5173\u7cfb\u7ed3\u6784\u548c\u7f3a\u4e4f\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "method": "\u4f7f\u7528GFM\u8fdb\u884c\u96f6\u6837\u672cOOD\u68c0\u6d4b\uff0c\u5e76\u5f15\u5165GLIP-OOD\u6846\u67b6\uff0c\u5229\u7528LLM\u751f\u6210\u4f2aOOD\u6807\u7b7e\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5b9e\u73b0\u4e86\u5b8c\u5168\u96f6\u6837\u672c\u7684\u8282\u70b9\u7ea7\u56feOOD\u68c0\u6d4b\uff0c\u4e3a\u52a8\u6001\u5f00\u653e\u73af\u5883\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b89\u5168\u4fdd\u969c\u3002"}}
{"id": "2504.21202", "pdf": "https://arxiv.org/pdf/2504.21202", "abs": "https://arxiv.org/abs/2504.21202", "authors": ["Ramon Pires", "Roseval Malaquias Junior", "Rodrigo Nogueira"], "title": "Automatic Legal Writing Evaluation of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the recent advances in Large Language Models, benchmarks for\nevaluating legal writing remain scarce due to the inherent complexity of\nassessing open-ended responses in this domain. One of the key challenges in\nevaluating language models on domain-specific tasks is finding test datasets\nthat are public, frequently updated, and contain comprehensive evaluation\nguidelines. The Brazilian Bar Examination meets these requirements. We\nintroduce oab-bench, a benchmark comprising 105 questions across seven areas of\nlaw from recent editions of the exam. The benchmark includes comprehensive\nevaluation guidelines and reference materials used by human examiners to ensure\nconsistent grading. We evaluate the performance of four LLMs on oab-bench,\nfinding that Claude-3.5 Sonnet achieves the best results with an average score\nof 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can\nserve as reliable automated judges for evaluating legal writing. Our\nexperiments show that frontier models like OpenAI's o1 achieve a strong\ncorrelation with human scores when evaluating approved exams, suggesting their\npotential as reliable automated evaluators despite the inherently subjective\nnature of legal writing assessment. The source code and the benchmark --\ncontaining questions, evaluation guidelines, model-generated responses, and\ntheir respective automated evaluations -- are publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aoab-bench\u7684\u6cd5\u5f8b\u5199\u4f5c\u8bc4\u4f30\u57fa\u51c6\uff0c\u57fa\u4e8e\u5df4\u897f\u5f8b\u5e08\u8003\u8bd5\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u63a2\u8ba8\u4e86LLMs\u4f5c\u4e3a\u81ea\u52a8\u5316\u8bc4\u4f30\u5de5\u5177\u7684\u6f5c\u529b\u3002", "motivation": "\u7531\u4e8e\u8bc4\u4f30\u6cd5\u5f8b\u5199\u4f5c\u7684\u590d\u6742\u6027\uff0c\u7f3a\u4e4f\u516c\u5f00\u3001\u66f4\u65b0\u9891\u7e41\u4e14\u5305\u542b\u5168\u9762\u8bc4\u4f30\u6307\u5357\u7684\u6d4b\u8bd5\u6570\u636e\u96c6\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u53ef\u9760\u7684\u57fa\u51c6\u3002", "method": "\u5229\u7528\u5df4\u897f\u5f8b\u5e08\u8003\u8bd5\u7684105\u4e2a\u95ee\u9898\u6784\u5efaoab-bench\uff0c\u5305\u542b\u8bc4\u4f30\u6307\u5357\u548c\u53c2\u8003\u6750\u6599\uff0c\u6d4b\u8bd5\u4e86\u56db\u79cdLLMs\u7684\u6027\u80fd\uff0c\u5e76\u7814\u7a76\u4e86LLMs\u4f5c\u4e3a\u81ea\u52a8\u5316\u8bc4\u4f30\u5de5\u5177\u7684\u53ef\u884c\u6027\u3002", "result": "Claude-3.5 Sonnet\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u5747\u5f97\u52067.93/10\uff0c\u5e76\u901a\u8fc7\u6240\u670921\u6b21\u8003\u8bd5\u3002\u524d\u6cbf\u6a21\u578b\uff08\u5982OpenAI\u7684o1\uff09\u5728\u8bc4\u4f30\u901a\u8fc7\u8003\u8bd5\u65f6\u4e0e\u4eba\u7c7b\u8bc4\u5206\u9ad8\u5ea6\u76f8\u5173\u3002", "conclusion": "oab-bench\u4e3a\u6cd5\u5f8b\u5199\u4f5c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u51c6\uff0cLLMs\u5728\u81ea\u52a8\u5316\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u5c3d\u7ba1\u6cd5\u5f8b\u5199\u4f5c\u8bc4\u4f30\u5177\u6709\u4e3b\u89c2\u6027\u3002"}}
{"id": "2504.21302", "pdf": "https://arxiv.org/pdf/2504.21302", "abs": "https://arxiv.org/abs/2504.21302", "authors": ["Zhelun Shen", "Zhuo Li", "Chenming Wu", "Zhibo Rao", "Lina Liu", "Yuchao Dai", "Liangjun Zhang"], "title": "CMD: Constraining Multimodal Distribution for Domain Adaptation in Stereo Matching", "categories": ["cs.CV", "cs.RO"], "comment": "13 pages, 5 figures, accepted for publication in Pattern Recognition", "summary": "Recently, learning-based stereo matching methods have achieved great\nimprovement in public benchmarks, where soft argmin and smooth L1 loss play a\ncore contribution to their success. However, in unsupervised domain adaptation\nscenarios, we observe that these two operations often yield multimodal\ndisparity probability distributions in target domains, resulting in degraded\ngeneralization. In this paper, we propose a novel approach, Constrain\nMulti-modal Distribution (CMD), to address this issue. Specifically, we\nintroduce \\textit{uncertainty-regularized minimization} and \\textit{anisotropic\nsoft argmin} to encourage the network to produce predominantly unimodal\ndisparity distributions in the target domain, thereby improving prediction\naccuracy. Experimentally, we apply the proposed method to multiple\nrepresentative stereo-matching networks and conduct domain adaptation from\nsynthetic data to unlabeled real-world scenes. Results consistently demonstrate\nimproved generalization in both top-performing and domain-adaptable\nstereo-matching models. The code for CMD will be available at:\n\\href{https://github.com/gallenszl/CMD}{https://github.com/gallenszl/CMD}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCMD\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u591a\u6a21\u6001\u5206\u5e03\u63d0\u5347\u65e0\u76d1\u7763\u57df\u9002\u5e94\u573a\u666f\u4e0b\u7684\u7acb\u4f53\u5339\u914d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u57fa\u7acb\u4f53\u5339\u914d\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u57df\u9002\u5e94\u573a\u666f\u4e2d\u56e0\u591a\u6a21\u6001\u5206\u5e03\u95ee\u9898\u5bfc\u81f4\u6cdb\u5316\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u6b63\u5219\u5316\u6700\u5c0f\u5316\u548c\u5404\u5411\u5f02\u6027\u8f6fargmin\uff0c\u9f13\u52b1\u7f51\u7edc\u751f\u6210\u5355\u6a21\u6001\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCMD\u65b9\u6cd5\u5728\u591a\u4ee3\u8868\u6027\u7acb\u4f53\u5339\u914d\u7f51\u7edc\u4e2d\u63d0\u5347\u4e86\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "CMD\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u57df\u9002\u5e94\u4e2d\u7684\u591a\u6a21\u6001\u5206\u5e03\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2504.21008", "pdf": "https://arxiv.org/pdf/2504.21008", "abs": "https://arxiv.org/abs/2504.21008", "authors": ["Qiuyan Xiang", "Shuang Wu", "Dongze Wu", "Yuxin Liu", "Zhenkai Qin"], "title": "Research on CNN-BiLSTM Network Traffic Anomaly Detection Model Based on MindSpore", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "With the widespread adoption of the Internet of Things (IoT) and Industrial\nIoT (IIoT) technologies, network architectures have become increasingly\ncomplex, and the volume of traffic has grown substantially. This evolution\nposes significant challenges to traditional security mechanisms, particularly\nin detecting high-frequency, diverse, and highly covert network attacks. To\naddress these challenges, this study proposes a novel network traffic anomaly\ndetection model that integrates a Convolutional Neural Network (CNN) with a\nBidirectional Long Short-Term Memory (BiLSTM) network, implemented on the\nMindSpore framework. Comprehensive experiments were conducted using the\nNF-BoT-IoT dataset. The results demonstrate that the proposed model achieves\n99% across accuracy, precision, recall, and F1-score, indicating its strong\nperformance and robustness in network intrusion detection tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CNN\u548cBiLSTM\u7684\u7f51\u7edc\u6d41\u91cf\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u5728MindSpore\u6846\u67b6\u4e0a\u5b9e\u73b0\uff0c\u6027\u80fd\u4f18\u5f02\u3002", "motivation": "\u968f\u7740IoT\u548cIIoT\u7684\u666e\u53ca\uff0c\u7f51\u7edc\u67b6\u6784\u590d\u6742\u5316\uff0c\u6d41\u91cf\u6fc0\u589e\uff0c\u4f20\u7edf\u5b89\u5168\u673a\u5236\u96be\u4ee5\u5e94\u5bf9\u9ad8\u9891\u3001\u591a\u6837\u4e14\u9690\u853d\u7684\u7f51\u7edc\u653b\u51fb\u3002", "method": "\u96c6\u6210CNN\u548cBiLSTM\u7684\u6a21\u578b\uff0c\u4f7f\u7528NF-BoT-IoT\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u6a21\u578b\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u8fbe\u523099%\uff0c\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u7f51\u7edc\u5165\u4fb5\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5177\u6709\u5f3a\u5927\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.21187", "pdf": "https://arxiv.org/pdf/2504.21187", "abs": "https://arxiv.org/abs/2504.21187", "authors": ["Neha Prakriya", "Zijian Ding", "Yizhou Sun", "Jason Cong"], "title": "LIFT: LLM-Based Pragma Insertion for HLS via GNN Supervised Fine-Tuning", "categories": ["cs.LG"], "comment": null, "summary": "FPGAs are increasingly adopted in datacenter environments for their\nreconfigurability and energy efficiency. High-Level Synthesis (HLS) tools have\neased FPGA programming by raising the abstraction level from RTL to untimed\nC/C++, yet attaining high performance still demands expert knowledge and\niterative manual insertion of optimization pragmas to modify the\nmicroarchitecture. To address this challenge, we propose LIFT, a large language\nmodel (LLM)-based coding assistant for HLS that automatically generates\nperformance-critical pragmas given a C/C++ design. We fine-tune the LLM by\ntightly integrating and supervising the training process with a graph neural\nnetwork (GNN), combining the sequential modeling capabilities of LLMs with the\nstructural and semantic understanding of GNNs necessary for reasoning over code\nand its control/data dependencies. On average, LIFT produces designs that\nimprove performance by 3.52x and 2.16x than prior state-of the art AutoDSE and\nHARP respectively, and 66x than GPT-4o.", "AI": {"tldr": "LIFT\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684HLS\u7f16\u7801\u52a9\u624b\uff0c\u80fd\u81ea\u52a8\u751f\u6210\u6027\u80fd\u5173\u952epragma\uff0c\u663e\u8457\u63d0\u5347FPGA\u8bbe\u8ba1\u6027\u80fd\u3002", "motivation": "FPGA\u5728\u6570\u636e\u4e2d\u5fc3\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u9ad8\u6027\u80fd\u8bbe\u8ba1\u9700\u8981\u4e13\u5bb6\u77e5\u8bc6\u548c\u624b\u52a8\u4f18\u5316\uff0cLIFT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7ed3\u5408LLM\u548cGNN\uff0cLIFT\u80fd\u591f\u7406\u89e3\u4ee3\u7801\u7ed3\u6784\u548c\u4f9d\u8d56\u5173\u7cfb\uff0c\u81ea\u52a8\u751f\u6210\u4f18\u5316pragma\u3002", "result": "LIFT\u7684\u6027\u80fd\u6bd4AutoDSE\u548cHARP\u5206\u522b\u63d0\u53473.52\u500d\u548c2.16\u500d\uff0c\u6bd4GPT-4o\u63d0\u534766\u500d\u3002", "conclusion": "LIFT\u4e3aFPGA\u9ad8\u6027\u80fd\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u4e13\u5bb6\u77e5\u8bc6\u7684\u4f9d\u8d56\u3002"}}
{"id": "2504.21214", "pdf": "https://arxiv.org/pdf/2504.21214", "abs": "https://arxiv.org/abs/2504.21214", "authors": ["Jinzhao Zhou", "Zehong Cao", "Yiqun Duan", "Connor Barkley", "Daniel Leong", "Xiaowei Jiang", "Quoc-Toan Nguyen", "Ziyi Zhao", "Thomas Do", "Yu-Cheng Chang", "Sheng-Fu Liang", "Chin-teng Lin"], "title": "Pretraining Large Brain Language Model for Active BCI: Silent Speech", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": null, "summary": "This paper explores silent speech decoding in active brain-computer interface\n(BCI) systems, which offer more natural and flexible communication than\ntraditional BCI applications. We collected a new silent speech dataset of over\n120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing\n24 commonly used English words for language model pretraining and decoding.\nFollowing the recent success of pretraining large models with self-supervised\nparadigms to enhance EEG classification performance, we propose Large Brain\nLanguage Model (LBLM) pretrained to decode silent speech for active BCI. To\npretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining\nparadigm to learn effective representations from unlabeled EEG data. Unlike\nexisting EEG pretraining methods that mainly follow a masked-reconstruction\nparadigm, our proposed FSTP method employs autoregressive modeling in temporal\nand frequency domains to capture both temporal and spectral dependencies from\nEEG signals. After pretraining, we finetune our LBLM on downstream tasks,\nincluding word-level and semantic-level classification. Extensive experiments\ndemonstrate significant performance gains of the LBLM over fully-supervised and\npretrained baseline models. For instance, in the difficult cross-session\nsetting, our model achieves 47.0\\% accuracy on semantic-level classification\nand 39.6\\% in word-level classification, outperforming baseline methods by\n5.4\\% and 7.3\\%, respectively. Our research advances silent speech decoding in\nactive BCI systems, offering an innovative solution for EEG language model\npretraining and a new dataset for fundamental research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e3b\u52a8\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u7684\u65e0\u58f0\u8bed\u97f3\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u5927\u578b\u8111\u8bed\u8a00\u6a21\u578b\uff08LBLM\uff09\u548c\u65b0\u7684\u672a\u6765\u65f6\u9891\u9884\u6d4b\uff08FSTP\uff09\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u66f4\u81ea\u7136\u7075\u6d3b\u7684BCI\u901a\u4fe1\u65b9\u5f0f\uff0c\u89e3\u51b3\u4f20\u7edfBCI\u5e94\u7528\u7684\u9650\u5236\u3002", "method": "\u63d0\u51faLBLM\u6a21\u578b\u548cFSTP\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u5efa\u6a21\u5b66\u4e60EEG\u4fe1\u53f7\u7684\u65f6\u9891\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u8de8\u4f1a\u8bdd\u8bbe\u7f6e\u4e2d\uff0cLBLM\u5728\u8bed\u4e49\u7ea7\u548c\u8bcd\u7ea7\u5206\u7c7b\u4efb\u52a1\u4e2d\u5206\u522b\u8fbe\u523047.0%\u548c39.6%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4e3b\u52a8BCI\u7cfb\u7edf\u7684\u65e0\u58f0\u8bed\u97f3\u89e3\u7801\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u8d21\u732e\u4e86\u65b0\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2504.21307", "pdf": "https://arxiv.org/pdf/2504.21307", "abs": "https://arxiv.org/abs/2504.21307", "authors": ["Siyi Chen", "Yimeng Zhang", "Sijia Liu", "Qing Qu"], "title": "The Dual Power of Interpretable Token Embeddings: Jailbreaking Attacks and Defenses for Diffusion Model Unlearning", "categories": ["cs.CV"], "comment": null, "summary": "Despite the remarkable generalization capabilities of diffusion models,\nrecent studies have shown that these models can memorize and generate harmful\ncontent when prompted with specific text instructions. Although fine-tuning\napproaches have been developed to mitigate this issue by unlearning harmful\nconcepts, these methods can be easily circumvented through jailbreaking\nattacks. This indicates that the harmful concept has not been fully erased from\nthe model. However, existing attack methods, while effective, lack\ninterpretability regarding why unlearned models still retain the concept,\nthereby hindering the development of defense strategies. In this work, we\naddress these limitations by proposing an attack method that learns an\northogonal set of interpretable attack token embeddings. The attack token\nembeddings can be decomposed into human-interpretable textual elements,\nrevealing that unlearned models still retain the target concept through\nimplicit textual components. Furthermore, these attack token embeddings are\nrobust and transferable across text prompts, initial noises, and unlearned\nmodels. Finally, leveraging this diverse set of embeddings, we design a defense\nmethod applicable to both our proposed attack and existing attack methods.\nExperimental results demonstrate the effectiveness of both our attack and\ndefense strategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u4ea4\u653b\u51fb\u4ee4\u724c\u5d4c\u5165\u63ed\u793a\u672a\u5b66\u4e60\u6a21\u578b\u4e2d\u4ecd\u4fdd\u7559\u6709\u5bb3\u6982\u5ff5\u7684\u539f\u56e0\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9632\u5fa1\u65b9\u6cd5\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u53ef\u80fd\u8bb0\u5fc6\u5e76\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\u6613\u88ab\u7ed5\u8fc7\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u672a\u5b66\u4e60\u6a21\u578b\u4ecd\u4fdd\u7559\u6982\u5ff5\u7684\u89e3\u91ca\u3002", "method": "\u63d0\u51fa\u5b66\u4e60\u6b63\u4ea4\u53ef\u89e3\u91ca\u653b\u51fb\u4ee4\u724c\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u6587\u672c\u5143\u7d20\uff0c\u5e76\u8bbe\u8ba1\u9632\u5fa1\u65b9\u6cd5\u3002", "result": "\u653b\u51fb\u4ee4\u724c\u5d4c\u5165\u5177\u6709\u9c81\u68d2\u6027\u548c\u53ef\u8fc1\u79fb\u6027\uff0c\u9632\u5fa1\u65b9\u6cd5\u5bf9\u73b0\u6709\u653b\u51fb\u4e5f\u6709\u6548\u3002", "conclusion": "\u653b\u51fb\u548c\u9632\u5fa1\u7b56\u7565\u5747\u6709\u6548\uff0c\u4e3a\u7406\u89e3\u672a\u5b66\u4e60\u6a21\u578b\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2504.21189", "pdf": "https://arxiv.org/pdf/2504.21189", "abs": "https://arxiv.org/abs/2504.21189", "authors": ["Gulsah Hancerliogullari Koksalmis", "Bulent Soykan", "Laura J. Brattain", "Hsin-Hsiung Huang"], "title": "Artificial Intelligence for Personalized Prediction of Alzheimer's Disease Progression: A Survey of Methods, Data Challenges, and Future Directions", "categories": ["cs.LG", "cs.AI", "cs.ET"], "comment": "25 pages, 11 figures", "summary": "Alzheimer's Disease (AD) is marked by significant inter-individual\nvariability in its progression, complicating accurate prognosis and\npersonalized care planning. This heterogeneity underscores the critical need\nfor predictive models capable of forecasting patient-specific disease\ntrajectories. Artificial Intelligence (AI) offers powerful tools to address\nthis challenge by analyzing complex, multi-modal, and longitudinal patient\ndata. This paper provides a comprehensive survey of AI methodologies applied to\npersonalized AD progression prediction. We review key approaches including\nstate-space models for capturing temporal dynamics, deep learning techniques\nlike Recurrent Neural Networks for sequence modeling, Graph Neural Networks\n(GNNs) for leveraging network structures, and the emerging concept of AI-driven\ndigital twins for individualized simulation. Recognizing that data limitations\noften impede progress, we examine common challenges such as high\ndimensionality, missing data, and dataset imbalance. We further discuss\nAI-driven mitigation strategies, with a specific focus on synthetic data\ngeneration using Variational Autoencoders (VAEs) and Generative Adversarial\nNetworks (GANs) to augment and balance datasets. The survey synthesizes the\nstrengths and limitations of current approaches, emphasizing the trend towards\nmultimodal integration and the persistent need for model interpretability and\ngeneralizability. Finally, we identify critical open challenges, including\nrobust external validation, clinical integration, and ethical considerations,\nand outline promising future research directions such as hybrid models, causal\ninference, and federated learning. This review aims to consolidate current\nknowledge and guide future efforts in developing clinically relevant AI tools\nfor personalized AD prognostication.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u9884\u6d4b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u4e2a\u6027\u5316\u8fdb\u5c55\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u591a\u79cdAI\u65b9\u6cd5\u3001\u6570\u636e\u6311\u6218\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "AD\u8fdb\u5c55\u7684\u4e2a\u4f53\u5dee\u5f02\u5927\uff0c\u9700\u8981\u4e2a\u6027\u5316\u9884\u6d4b\u6a21\u578b\u4ee5\u6539\u5584\u9884\u540e\u548c\u62a4\u7406\u8ba1\u5212\u3002", "method": "\u7efc\u8ff0\u4e86\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3001\u6df1\u5ea6\u5b66\u4e60\uff08\u5982RNN\uff09\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u548c\u6570\u5b57\u5b6a\u751f\u7b49\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u6570\u636e\u6311\u6218\u53ca\u89e3\u51b3\u65b9\u6848\uff08\u5982VAE\u548cGAN\uff09\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5f3a\u8c03\u591a\u6a21\u6001\u6574\u5408\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u6df7\u5408\u6a21\u578b\u3001\u56e0\u679c\u63a8\u7406\u548c\u8054\u90a6\u5b66\u4e60\uff0c\u5e76\u547c\u5401\u89e3\u51b3\u5916\u90e8\u9a8c\u8bc1\u548c\u4f26\u7406\u95ee\u9898\u3002"}}
{"id": "2504.21233", "pdf": "https://arxiv.org/pdf/2504.21233", "abs": "https://arxiv.org/abs/2504.21233", "authors": ["Haoran Xu", "Baolin Peng", "Hany Awadalla", "Dongdong Chen", "Yen-Chun Chen", "Mei Gao", "Young Jin Kim", "Yunsheng Li", "Liliang Ren", "Yelong Shen", "Shuohang Wang", "Weijian Xu", "Jianfeng Gao", "Weizhu Chen"], "title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities\nin Large Language Models (LLMs) by training them to explicitly generate\nintermediate reasoning steps. While LLMs readily benefit from such techniques,\nimproving reasoning in Small Language Models (SLMs) remains challenging due to\ntheir limited model capacity. Recent work by Deepseek-R1 demonstrates that\ndistillation from LLM-generated synthetic data can substantially improve the\nreasoning ability of SLM. However, the detailed modeling recipe is not\ndisclosed. In this work, we present a systematic training recipe for SLMs that\nconsists of four steps: (1) large-scale mid-training on diverse distilled\nlong-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3)\nRollout DPO leveraging a carefully curated preference dataset, and (4)\nReinforcement Learning (RL) with Verifiable Reward. We apply our method on\nPhi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning\nmodel exceeds, on math reasoning tasks, much larger reasoning models, e.g.,\noutperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and\nDeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate\nthat a carefully designed training recipe, with large-scale high-quality CoT\ndata, is effective to unlock strong reasoning capabilities even in\nresource-constrained small models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u56db\u4e2a\u6b65\u9aa4\u63d0\u5347\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728Phi-4-Mini\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u901a\u8fc7Chain-of-Thought\uff08CoT\uff09\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u7531\u4e8e\u6a21\u578b\u5bb9\u91cf\u6709\u9650\uff0c\u63a8\u7406\u80fd\u529b\u63d0\u5347\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u8bad\u7ec3\u65b9\u6cd5\u5305\u62ec\u56db\u4e2a\u6b65\u9aa4\uff1a\u5927\u89c4\u6a21\u4e2d\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u3001Rollout DPO\u548c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cPhi-4-Mini-Reasoning\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u66f4\u5927\u7684\u6a21\u578b\uff0c\u5982DeepSeek-R1-Distill-Qwen-7B\u548cDeepSeek-R1-Distill-Llama-8B\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u65b9\u6cd5\u548c\u5927\u89c4\u6a21\u9ad8\u8d28\u91cfCoT\u6570\u636e\u53ef\u4ee5\u6709\u6548\u89e3\u9501\u5c0f\u578b\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2504.21308", "pdf": "https://arxiv.org/pdf/2504.21308", "abs": "https://arxiv.org/abs/2504.21308", "authors": ["Yunhao Li", "Sijing Wu", "Wei Sun", "Zhichao Zhang", "Yucheng Zhu", "Zicheng Zhang", "Huiyu Duan", "Xiongkuo Min", "Guangtao Zhai"], "title": "AGHI-QA: A Subjective-Aligned Dataset and Metric for AI-Generated Human Images", "categories": ["cs.CV"], "comment": null, "summary": "The rapid development of text-to-image (T2I) generation approaches has\nattracted extensive interest in evaluating the quality of generated images,\nleading to the development of various quality assessment methods for\ngeneral-purpose T2I outputs. However, existing image quality assessment (IQA)\nmethods are limited to providing global quality scores, failing to deliver\nfine-grained perceptual evaluations for structurally complex subjects like\nhumans, which is a critical challenge considering the frequent anatomical and\ntextural distortions in AI-generated human images (AGHIs). To address this gap,\nwe introduce AGHI-QA, the first large-scale benchmark specifically designed for\nquality assessment of AGHIs. The dataset comprises 4,000 images generated from\n400 carefully crafted text prompts using 10 state of-the-art T2I models. We\nconduct a systematic subjective study to collect multidimensional annotations,\nincluding perceptual quality scores, text-image correspondence scores, visible\nand distorted body part labels. Based on AGHI-QA, we evaluate the strengths and\nweaknesses of current T2I methods in generating human images from multiple\ndimensions. Furthermore, we propose AGHI-Assessor, a novel quality metric that\nintegrates the large multimodal model (LMM) with domain-specific human features\nfor precise quality prediction and identification of visible and distorted body\nparts in AGHIs. Extensive experimental results demonstrate that AGHI-Assessor\nshowcases state-of-the-art performance, significantly outperforming existing\nIQA methods in multidimensional quality assessment and surpassing leading LMMs\nin detecting structural distortions in AGHIs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86AGHI-QA\uff0c\u9996\u4e2a\u9488\u5bf9AI\u751f\u6210\u4eba\u7c7b\u56fe\u50cf\uff08AGHIs\uff09\u8d28\u91cf\u8bc4\u4f30\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86AGHI-Assessor\uff0c\u4e00\u79cd\u7ed3\u5408\u591a\u6a21\u6001\u6a21\u578b\u548c\u4eba\u4f53\u7279\u5f81\u7684\u65b0\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08IQA\uff09\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u5168\u5c40\u8d28\u91cf\u8bc4\u5206\uff0c\u65e0\u6cd5\u5bf9\u7ed3\u6784\u590d\u6742\u7684\u4eba\u7c7b\u56fe\u50cf\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff0c\u800cAI\u751f\u6210\u7684\u4eba\u7c7b\u56fe\u50cf\u5e38\u5b58\u5728\u89e3\u5256\u548c\u7eb9\u7406\u5931\u771f\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b4,000\u5f20\u56fe\u50cf\u7684AGHI-QA\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4e3b\u89c2\u7814\u7a76\u6536\u96c6\u591a\u7ef4\u5ea6\u6807\u6ce8\uff08\u5982\u8d28\u91cf\u8bc4\u5206\u3001\u6587\u672c-\u56fe\u50cf\u5bf9\u5e94\u6027\u3001\u53ef\u89c1\u548c\u5931\u771f\u8eab\u4f53\u90e8\u4f4d\uff09\u3002\u63d0\u51faAGHI-Assessor\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u6a21\u578b\u548c\u4eba\u4f53\u7279\u5f81\u8fdb\u884c\u8d28\u91cf\u9884\u6d4b\u3002", "result": "AGHI-Assessor\u5728\u591a\u7ef4\u8d28\u91cf\u8bc4\u4f30\u548c\u7ed3\u6784\u5931\u771f\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709IQA\u65b9\u6cd5\u548c\u9886\u5148\u7684\u591a\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "AGHI-QA\u548cAGHI-Assessor\u586b\u8865\u4e86\u4eba\u7c7b\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2504.21190", "pdf": "https://arxiv.org/pdf/2504.21190", "abs": "https://arxiv.org/abs/2504.21190", "authors": ["Pradip Kunwar", "Minh N. Vu", "Maanak Gupta", "Mahmoud Abdelsalam", "Manish Bhattarai"], "title": "TT-LoRA MoE: Unifying Parameter-Efficient Fine-Tuning and Sparse Mixture-of-Experts", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose Tensor-Trained Low-Rank Adaptation Mixture of Experts (TT-LoRA\nMoE), a novel computational framework integrating Parameter-Efficient\nFine-Tuning (PEFT) with sparse MoE routing to address scalability challenges in\nlarge model deployments. Unlike traditional MoE approaches, which face\nsubstantial computational overhead as expert counts grow, TT-LoRA MoE\ndecomposes training into two distinct, optimized stages. First, we\nindependently train lightweight, tensorized low-rank adapters (TT-LoRA\nexperts), each specialized for specific tasks. Subsequently, these expert\nadapters remain frozen, eliminating inter-task interference and catastrophic\nforgetting in multi-task setting. A sparse MoE router, trained separately,\ndynamically leverages base model representations to select exactly one\nspecialized adapter per input at inference time, automating expert selection\nwithout explicit task specification. Comprehensive experiments confirm our\narchitecture retains the memory efficiency of low-rank adapters, seamlessly\nscales to large expert pools, and achieves robust task-level optimization. This\nstructured decoupling significantly enhances computational efficiency and\nflexibility: uses only 2% of LoRA, 0.3% of Adapters and 0.03% of AdapterFusion\nparameters and outperforms AdapterFusion by 4 value in multi-tasking, enabling\npractical and scalable multi-task inference deployments.", "AI": {"tldr": "TT-LoRA MoE\u7ed3\u5408\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u548c\u7a00\u758fMoE\u8def\u7531\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u8bad\u7ec3\u548c\u51bb\u7ed3\u4e13\u5bb6\u9002\u914d\u5668\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u6a21\u578b\u90e8\u7f72\u4e2d\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfMoE\u65b9\u6cd5\u5728\u4e13\u5bb6\u6570\u91cf\u589e\u52a0\u65f6\u8ba1\u7b97\u5f00\u9500\u5927\uff0cTT-LoRA MoE\u65e8\u5728\u901a\u8fc7\u5206\u9636\u6bb5\u4f18\u5316\u548c\u7a00\u758f\u8def\u7531\u63d0\u5347\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "method": "\u5206\u4e24\u9636\u6bb5\uff1a1\uff09\u72ec\u7acb\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5f20\u91cf\u4f4e\u79e9\u9002\u914d\u5668\uff08TT-LoRA\u4e13\u5bb6\uff09\uff1b2\uff09\u51bb\u7ed3\u9002\u914d\u5668\uff0c\u8bad\u7ec3\u7a00\u758fMoE\u8def\u7531\u5668\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u3002", "result": "\u4ec5\u97002%\u7684LoRA\u30010.3%\u7684Adapter\u548c0.03%\u7684AdapterFusion\u53c2\u6570\uff0c\u591a\u4efb\u52a1\u6027\u80fd\u4f18\u4e8eAdapterFusion 4\u5206\u3002", "conclusion": "TT-LoRA MoE\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u7075\u6d3b\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u63a8\u7406\u90e8\u7f72\u3002"}}
{"id": "2504.21239", "pdf": "https://arxiv.org/pdf/2504.21239", "abs": "https://arxiv.org/abs/2504.21239", "authors": ["Xu Pan", "Ely Hahami", "Zechen Zhang", "Haim Sompolinsky"], "title": "Memorization and Knowledge Injection in Gated LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) currently struggle to sequentially add new\nmemories and integrate new knowledge. These limitations contrast with the human\nability to continuously learn from new experiences and acquire knowledge\nthroughout life. Most existing approaches add memories either through large\ncontext windows or external memory buffers (e.g., Retrieval-Augmented\nGeneration), and studies on knowledge injection rarely test scenarios\nresembling everyday life events. In this work, we introduce a continual\nlearning framework, Memory Embedded in Gated LLMs (MEGa), which injects event\nmemories directly into the weights of LLMs. Each memory is stored in a\ndedicated set of gated low-rank weights. During inference, a gating mechanism\nactivates relevant memory weights by matching query embeddings to stored memory\nembeddings. This enables the model to both recall entire memories and answer\nrelated questions. On two datasets - fictional characters and Wikipedia events\n- MEGa outperforms baseline approaches in mitigating catastrophic forgetting.\nOur model draws inspiration from the complementary memory system of the human\nbrain.", "AI": {"tldr": "MEGa\u6846\u67b6\u901a\u8fc7\u5c06\u8bb0\u5fc6\u76f4\u63a5\u5d4c\u5165LLM\u6743\u91cd\u4e2d\uff0c\u89e3\u51b3\u4e86LLM\u65e0\u6cd5\u6301\u7eed\u5b66\u4e60\u548c\u6574\u5408\u65b0\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "LLM\u5728\u6301\u7eed\u5b66\u4e60\u548c\u6574\u5408\u65b0\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u800c\u4eba\u7c7b\u80fd\u6301\u7eed\u5b66\u4e60\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u5927\u4e0a\u4e0b\u6587\u7a97\u53e3\u6216\u5916\u90e8\u8bb0\u5fc6\u7f13\u51b2\u533a\uff09\u672a\u80fd\u6a21\u62df\u65e5\u5e38\u751f\u6d3b\u4e8b\u4ef6\u3002", "method": "\u63d0\u51faMEGa\u6846\u67b6\uff0c\u5c06\u4e8b\u4ef6\u8bb0\u5fc6\u5d4c\u5165LLM\u7684\u6743\u91cd\u4e2d\uff0c\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u6fc0\u6d3b\u76f8\u5173\u8bb0\u5fc6\u6743\u91cd\u3002", "result": "\u5728\u865a\u6784\u89d2\u8272\u548c\u7ef4\u57fa\u767e\u79d1\u4e8b\u4ef6\u6570\u636e\u96c6\u4e0a\uff0cMEGa\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "MEGa\u53d7\u4eba\u7c7b\u5927\u8111\u4e92\u8865\u8bb0\u5fc6\u7cfb\u7edf\u542f\u53d1\uff0c\u4e3aLLM\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.21309", "pdf": "https://arxiv.org/pdf/2504.21309", "abs": "https://arxiv.org/abs/2504.21309", "authors": ["Modesto Castrill\u00f3n-Santana", "Oliverio J Santana", "David Freire-Obreg\u00f3n", "Daniel Hern\u00e1ndez-Sosa", "Javier Lorenzo-Navarro"], "title": "An Evaluation of a Visual Question Answering Strategy for Zero-shot Facial Expression Recognition in Still Images", "categories": ["cs.CV", "I.2.10"], "comment": null, "summary": "Facial expression recognition (FER) is a key research area in computer vision\nand human-computer interaction. Despite recent advances in deep learning,\nchallenges persist, especially in generalizing to new scenarios. In fact,\nzero-shot FER significantly reduces the performance of state-of-the-art FER\nmodels. To address this problem, the community has recently started to explore\nthe integration of knowledge from Large Language Models for visual tasks. In\nthis work, we evaluate a broad collection of locally executed Visual Language\nModels (VLMs), avoiding the lack of task-specific knowledge by adopting a\nVisual Question Answering strategy. We compare the proposed pipeline with\nstate-of-the-art FER models, both integrating and excluding VLMs, evaluating\nwell-known FER benchmarks: AffectNet, FERPlus, and RAF-DB. The results show\nexcellent performance for some VLMs in zero-shot FER scenarios, indicating the\nneed for further exploration to improve FER generalization.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u96f6\u6837\u672c\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08FER\uff09\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728FER\u9886\u57df\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u65b0\u7684\u65b9\u6cd5\uff08\u5982\u7ed3\u5408VLM\uff09\u4ee5\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u89c6\u89c9\u95ee\u7b54\u7b56\u7565\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u672c\u5730\u6267\u884c\u7684VLM\uff0c\u5e76\u4e0e\u73b0\u6709FER\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u90e8\u5206VLM\u5728\u96f6\u6837\u672cFER\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8868\u660e\u5176\u5728\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76VLM\u5728FER\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u6539\u5584\u6a21\u578b\u5728\u65b0\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2504.21198", "pdf": "https://arxiv.org/pdf/2504.21198", "abs": "https://arxiv.org/abs/2504.21198", "authors": ["Haoyan Xu", "Zhengtao Yao", "Ziyi Wang", "Zhan Cheng", "Xiyang Hu", "Mengyuan Li", "Yue Zhao"], "title": "Graph Synthetic Out-of-Distribution Exposure with Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Out-of-distribution (OOD) detection in graphs is critical for ensuring model\nrobustness in open-world and safety-sensitive applications. Existing approaches\nto graph OOD detection typically involve training an in-distribution (ID)\nclassifier using only ID data, followed by the application of post-hoc OOD\nscoring techniques. Although OOD exposure - introducing auxiliary OOD samples\nduring training - has proven to be an effective strategy for enhancing\ndetection performance, current methods in the graph domain generally assume\naccess to a set of real OOD nodes. This assumption, however, is often\nimpractical due to the difficulty and cost of acquiring representative OOD\nsamples. In this paper, we introduce GOE-LLM, a novel framework that leverages\nLarge Language Models (LLMs) for OOD exposure in graph OOD detection without\nrequiring real OOD nodes. GOE-LLM introduces two pipelines: (1) identifying\npseudo-OOD nodes from the initially unlabeled graph using zero-shot LLM\nannotations, and (2) generating semantically informative synthetic OOD nodes\nvia LLM-prompted text generation. These pseudo-OOD nodes are then used to\nregularize the training of the ID classifier for improved OOD awareness. We\nevaluate our approach across multiple benchmark datasets, showing that GOE-LLM\nsignificantly outperforms state-of-the-art graph OOD detection methods that do\nnot use OOD exposure and achieves comparable performance to those relying on\nreal OOD data.", "AI": {"tldr": "GOE-LLM\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u56feOOD\u68c0\u6d4b\uff0c\u65e0\u9700\u771f\u5b9eOOD\u6570\u636e\uff0c\u901a\u8fc7\u751f\u6210\u4f2aOOD\u8282\u70b9\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56feOOD\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u771f\u5b9eOOD\u6570\u636e\uff0c\u4f46\u83b7\u53d6\u56f0\u96be\u4e14\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u771f\u5b9eOOD\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "GOE-LLM\u901a\u8fc7LLM\u96f6\u6837\u672c\u6807\u6ce8\u8bc6\u522b\u4f2aOOD\u8282\u70b9\uff0c\u5e76\u751f\u6210\u5408\u6210OOD\u8282\u70b9\uff0c\u7528\u4e8e\u8bad\u7ec3ID\u5206\u7c7b\u5668\u3002", "result": "GOE-LLM\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u4e0d\u4f7f\u7528OOD\u66b4\u9732\u7684\u65b9\u6cd5\uff0c\u5e76\u4e0e\u4f9d\u8d56\u771f\u5b9eOOD\u6570\u636e\u7684\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "GOE-LLM\u4e3a\u56feOOD\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u771f\u5b9eOOD\u6570\u636e\u3002"}}
{"id": "2504.21252", "pdf": "https://arxiv.org/pdf/2504.21252", "abs": "https://arxiv.org/abs/2504.21252", "authors": ["Xuanzhao Dong", "Wenhui Zhu", "Hao Wang", "Xiwen Chen", "Peijie Qiu", "Rui Yin", "Yi Su", "Yalin Wang"], "title": "Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA", "categories": ["cs.CL"], "comment": null, "summary": "Medical question answering (QA) is a reasoning-intensive task that remains\nchallenging for large language models (LLMs) due to hallucinations and outdated\ndomain knowledge. Retrieval-Augmented Generation (RAG) provides a promising\npost-training solution by leveraging external knowledge. However, existing\nmedical RAG systems suffer from two key limitations: (1) a lack of modeling for\nhuman-like reasoning behaviors during information retrieval, and (2) reliance\non suboptimal medical corpora, which often results in the retrieval of\nirrelevant or noisy snippets. To overcome these challenges, we propose\nDiscuss-RAG, a plug-and-play module designed to enhance the medical QA RAG\nsystem through collaborative agent-based reasoning. Our method introduces a\nsummarizer agent that orchestrates a team of medical experts to emulate\nmulti-turn brainstorming, thereby improving the relevance of retrieved content.\nAdditionally, a decision-making agent evaluates the retrieved snippets before\ntheir final integration. Experimental results on four benchmark medical QA\ndatasets show that Discuss-RAG consistently outperforms MedRAG, especially\nsignificantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on\nPubMedQA. The code is available at: https://github.com/LLM-VLM-GSL/Discuss-RAG.", "AI": {"tldr": "Discuss-RAG\u662f\u4e00\u4e2a\u901a\u8fc7\u534f\u4f5c\u4ee3\u7406\u63a8\u7406\u589e\u5f3a\u533b\u5b66\u95ee\u7b54\u7cfb\u7edf\u7684\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b54\u6848\u51c6\u786e\u6027\u3002", "motivation": "\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u5b58\u5728\u63a8\u7406\u884c\u4e3a\u5efa\u6a21\u4e0d\u8db3\u548c\u4f9d\u8d56\u4f4e\u8d28\u91cf\u8bed\u6599\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDiscuss-RAG\u6a21\u5757\uff0c\u5f15\u5165\u603b\u7ed3\u4ee3\u7406\u534f\u8c03\u533b\u5b66\u4e13\u5bb6\u56e2\u961f\u6a21\u62df\u591a\u8f6e\u5934\u8111\u98ce\u66b4\uff0c\u5e76\u901a\u8fc7\u51b3\u7b56\u4ee3\u7406\u8bc4\u4f30\u68c0\u7d22\u5185\u5bb9\u3002", "result": "\u5728\u56db\u4e2a\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cDiscuss-RAG\u663e\u8457\u4f18\u4e8eMedRAG\uff0cBioASQ\u548cPubMedQA\u7684\u7b54\u6848\u51c6\u786e\u6027\u5206\u522b\u63d0\u534716.67%\u548c12.20%\u3002", "conclusion": "Discuss-RAG\u901a\u8fc7\u534f\u4f5c\u4ee3\u7406\u63a8\u7406\u6709\u6548\u63d0\u5347\u4e86\u533b\u5b66\u95ee\u7b54\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.21325", "pdf": "https://arxiv.org/pdf/2504.21325", "abs": "https://arxiv.org/abs/2504.21325", "authors": ["Abdul Sami", "Avinash Kumar", "Irfanullah Memon", "Youngwon Jo", "Muhammad Rizwan", "Jaeyoung Choi"], "title": "Text-Conditioned Diffusion Model for High-Fidelity Korean Font Generation", "categories": ["cs.CV"], "comment": "6 pages, 4 figures, Accepted at ICOIN 2025", "summary": "Automatic font generation (AFG) is the process of creating a new font using\nonly a few examples of the style images. Generating fonts for complex languages\nlike Korean and Chinese, particularly in handwritten styles, presents\nsignificant challenges. Traditional AFGs, like Generative adversarial networks\n(GANs) and Variational Auto-Encoders (VAEs), are usually unstable during\ntraining and often face mode collapse problems. They also struggle to capture\nfine details within font images. To address these problems, we present a\ndiffusion-based AFG method which generates high-quality, diverse Korean font\nimages using only a single reference image, focusing on handwritten and printed\nstyles. Our approach refines noisy images incrementally, ensuring stable\ntraining and visually appealing results. A key innovation is our text encoder,\nwhich processes phonetic representations to generate accurate and contextually\ncorrect characters, even for unseen characters. We used a pre-trained style\nencoder from DG FONT to effectively and accurately encode the style images. To\nfurther enhance the generation quality, we used perceptual loss that guides the\nmodel to focus on the global style of generated images. Experimental results on\nover 2000 Korean characters demonstrate that our model consistently generates\naccurate and detailed font images and outperforms benchmark methods, making it\na reliable tool for generating authentic Korean fonts across different styles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u81ea\u52a8\u5b57\u4f53\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u8d28\u91cf\u751f\u6210\u97e9\u6587\u5b57\u4f53\uff0c\u4ec5\u9700\u5355\u4e00\u6837\u672c\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u7ec6\u8282\u6355\u6349\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfAFG\u65b9\u6cd5\uff08\u5982GANs\u548cVAEs\uff09\u5728\u8bad\u7ec3\u4e2d\u4e0d\u7a33\u5b9a\u4e14\u6613\u51fa\u73b0\u6a21\u5f0f\u5d29\u6e83\uff0c\u96be\u4ee5\u6355\u6349\u5b57\u4f53\u7ec6\u8282\uff0c\u5c24\u5176\u5728\u590d\u6742\u8bed\u8a00\uff08\u5982\u97e9\u6587\u548c\u4e2d\u6587\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u9010\u6b65\u7ec6\u5316\u566a\u58f0\u56fe\u50cf\uff0c\u7ed3\u5408\u6587\u672c\u7f16\u7801\u5668\u5904\u7406\u97f3\u6807\u8868\u793a\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u98ce\u683c\u7f16\u7801\u5668\u548c\u611f\u77e5\u635f\u5931\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "result": "\u57282000\u591a\u4e2a\u97e9\u6587\u5b57\u7b26\u4e0a\u6d4b\u8bd5\uff0c\u6a21\u578b\u80fd\u7a33\u5b9a\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u5b57\u4f53\u56fe\u50cf\uff0c\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u751f\u6210\u771f\u5b9e\u97e9\u6587\u5b57\u4f53\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u624b\u5199\u548c\u5370\u5237\u98ce\u683c\u3002"}}
{"id": "2504.21206", "pdf": "https://arxiv.org/pdf/2504.21206", "abs": "https://arxiv.org/abs/2504.21206", "authors": ["Zihan Chen", "Xingbo Fu", "Yushun Dong", "Jundong Li", "Cong Shen"], "title": "FedHERO: A Federated Learning Approach for Node Classification Task on Heterophilic Graphs", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Federated Graph Learning (FGL) empowers clients to collaboratively train\nGraph neural networks (GNNs) in a distributed manner while preserving data\nprivacy. However, FGL methods usually require that the graph data owned by all\nclients is homophilic to ensure similar neighbor distribution patterns of\nnodes. Such an assumption ensures that the learned knowledge is consistent\nacross the local models from all clients. Therefore, these local models can be\nproperly aggregated as a global model without undermining the overall\nperformance. Nevertheless, when the neighbor distribution patterns of nodes\nvary across different clients (e.g., when clients hold graphs with different\nlevels of heterophily), their local models may gain different and even conflict\nknowledge from their node-level predictive tasks. Consequently, aggregating\nthese local models usually leads to catastrophic performance deterioration on\nthe global model. To address this challenge, we propose FedHERO, an FGL\nframework designed to harness and share insights from heterophilic graphs\neffectively. At the heart of FedHERO is a dual-channel GNN equipped with a\nstructure learner, engineered to discern the structural knowledge encoded in\nthe local graphs. With this specialized component, FedHERO enables the local\nmodel for each client to identify and learn patterns that are universally\napplicable across graphs with different patterns of node neighbor\ndistributions. FedHERO not only enhances the performance of individual client\nmodels by leveraging both local and shared structural insights but also sets a\nnew precedent in this field to effectively handle graph data with various node\nneighbor distribution patterns. We conduct extensive experiments to validate\nthe superior performance of FedHERO against existing alternatives.", "AI": {"tldr": "FedHERO\u662f\u4e00\u4e2a\u8054\u90a6\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u6709\u6548\u5904\u7406\u5f02\u6784\u56fe\u6570\u636e\uff0c\u901a\u8fc7\u53cc\u901a\u9053GNN\u548c\u7ed3\u6784\u5b66\u4e60\u5668\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709FGL\u65b9\u6cd5\u5047\u8bbe\u56fe\u6570\u636e\u662f\u540c\u8d28\u7684\uff0c\u4f46\u5728\u5f02\u6784\u6570\u636e\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faFedHERO\u6846\u67b6\uff0c\u4f7f\u7528\u53cc\u901a\u9053GNN\u548c\u7ed3\u6784\u5b66\u4e60\u5668\u63d0\u53d6\u5c40\u90e8\u56fe\u7684\u901a\u7528\u7ed3\u6784\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1FedHERO\u5728\u5f02\u6784\u6570\u636e\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FedHERO\u4e3a\u5904\u7406\u4e0d\u540c\u8282\u70b9\u90bb\u57df\u5206\u5e03\u6a21\u5f0f\u7684\u56fe\u6570\u636e\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2504.21299", "pdf": "https://arxiv.org/pdf/2504.21299", "abs": "https://arxiv.org/abs/2504.21299", "authors": ["Zhiting Fan", "Ruizhe Chen", "Zuozhu Liu"], "title": "BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Identifying bias in LLM-generated content is a crucial prerequisite for\nensuring fairness in LLMs. Existing methods, such as fairness classifiers and\nLLM-based judges, face limitations related to difficulties in understanding\nunderlying intentions and the lack of criteria for fairness judgment. In this\npaper, we introduce BiasGuard, a novel bias detection tool that explicitly\nanalyzes inputs and reasons through fairness specifications to provide accurate\njudgments. BiasGuard is implemented through a two-stage approach: the first\nstage initializes the model to explicitly reason based on fairness\nspecifications, while the second stage leverages reinforcement learning to\nenhance its reasoning and judgment capabilities. Our experiments, conducted\nacross five datasets, demonstrate that BiasGuard outperforms existing tools,\nimproving accuracy and reducing over-fairness misjudgments. We also highlight\nthe importance of reasoning-enhanced decision-making and provide evidence for\nthe effectiveness of our two-stage optimization pipeline.", "AI": {"tldr": "BiasGuard\u662f\u4e00\u79cd\u65b0\u578b\u504f\u89c1\u68c0\u6d4b\u5de5\u5177\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08\u57fa\u4e8e\u516c\u5e73\u89c4\u8303\u7684\u663e\u5f0f\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u63d0\u5347\u504f\u89c1\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u516c\u5e73\u5206\u7c7b\u5668\u548c\u57fa\u4e8eLLM\u7684\u8bc4\u5224\uff09\u5728\u7406\u89e3\u610f\u56fe\u548c\u516c\u5e73\u5224\u65ad\u6807\u51c6\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u504f\u89c1\u68c0\u6d4b\u5de5\u5177\u3002", "method": "BiasGuard\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u57fa\u4e8e\u516c\u5e73\u89c4\u8303\u663e\u5f0f\u63a8\u7406\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u63a8\u7406\u548c\u5224\u65ad\u80fd\u529b\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBiasGuard\u5728\u51c6\u786e\u6027\u548c\u51cf\u5c11\u8fc7\u5ea6\u516c\u5e73\u8bef\u5224\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u3002", "conclusion": "BiasGuard\u8bc1\u660e\u4e86\u63a8\u7406\u589e\u5f3a\u51b3\u7b56\u7684\u91cd\u8981\u6027\uff0c\u5176\u4e24\u9636\u6bb5\u4f18\u5316\u6d41\u7a0b\u6709\u6548\u63d0\u5347\u4e86\u504f\u89c1\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2504.21334", "pdf": "https://arxiv.org/pdf/2504.21334", "abs": "https://arxiv.org/abs/2504.21334", "authors": ["Misora Sugiyama", "Hirokatsu Kataoka"], "title": "Simple Visual Artifact Detection in Sora-Generated Videos", "categories": ["cs.CV"], "comment": null, "summary": "The December 2024 release of OpenAI's Sora, a powerful video generation model\ndriven by natural language prompts, highlights a growing convergence between\nlarge language models (LLMs) and video synthesis. As these multimodal systems\nevolve into video-enabled LLMs (VidLLMs), capable of interpreting, generating,\nand interacting with visual content, understanding their limitations and\nensuring their safe deployment becomes essential. This study investigates\nvisual artifacts frequently found and reported in Sora-generated videos, which\ncan compromise quality, mislead viewers, or propagate disinformation. We\npropose a multi-label classification framework targeting four common artifact\nlabel types: label 1: boundary / edge defects, label 2: texture / noise issues,\nlabel 3: movement / joint anomalies, and label 4: object mismatches /\ndisappearances. Using a dataset of 300 manually annotated frames extracted from\n15 Sora-generated videos, we trained multiple 2D CNN architectures (ResNet-50,\nEfficientNet-B3 / B4, ViT-Base). The best-performing model trained by ResNet-50\nachieved an average multi-label classification accuracy of 94.14%. This work\nsupports the broader development of VidLLMs by contributing to (1) the creation\nof datasets for video quality evaluation, (2) interpretable artifact-based\nanalysis beyond language metrics, and (3) the identification of visual risks\nrelevant to factuality and safety.", "AI": {"tldr": "OpenAI\u7684Sora\u89c6\u9891\u751f\u6210\u6a21\u578b\u5b58\u5728\u89c6\u89c9\u4f2a\u5f71\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u591a\u6807\u7b7e\u5206\u7c7b\u6846\u67b6\u8bc4\u4f30\u56db\u79cd\u5e38\u89c1\u4f2a\u5f71\u7c7b\u578b\uff0c\u4f7f\u7528ResNet-50\u6a21\u578b\u8fbe\u523094.14%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u7406\u89e3\u5176\u5c40\u9650\u6027\u5e76\u786e\u4fdd\u5b89\u5168\u90e8\u7f72\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u89c6\u89c9\u4f2a\u5f71\u53ef\u80fd\u5f71\u54cd\u8d28\u91cf\u6216\u4f20\u64ad\u865a\u5047\u4fe1\u606f\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u591a\u6807\u7b7e\u5206\u7c7b\u6846\u67b6\uff0c\u57fa\u4e8e300\u4e2a\u624b\u52a8\u6807\u6ce8\u7684Sora\u751f\u6210\u89c6\u9891\u5e27\uff0c\u8bad\u7ec3\u591a\u79cd2D CNN\u67b6\u6784\uff08\u5982ResNet-50\u3001EfficientNet-B3/B4\u3001ViT-Base\uff09\u3002", "result": "ResNet-50\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u591a\u6807\u7b7e\u5206\u7c7b\u5e73\u5747\u51c6\u786e\u7387\u8fbe94.14%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u3001\u89c6\u89c9\u98ce\u9669\u8bc6\u522b\u53ca\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u5206\u6790\u65b9\u6cd5\u3002"}}
{"id": "2504.21211", "pdf": "https://arxiv.org/pdf/2504.21211", "abs": "https://arxiv.org/abs/2504.21211", "authors": ["Juliana Barbosa", "Ulhas Gondhali", "Gohar Petrossian", "Kinshuk Sharma", "Sunandan Chakraborty", "Jennifer Jacquet", "Juliana Freire"], "title": "A Cost-Effective LLM-based Approach to Identify Wildlife Trafficking in Online Marketplaces", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Wildlife trafficking remains a critical global issue, significantly impacting\nbiodiversity, ecological stability, and public health. Despite efforts to\ncombat this illicit trade, the rise of e-commerce platforms has made it easier\nto sell wildlife products, putting new pressure on wild populations of\nendangered and threatened species. The use of these platforms also opens a new\nopportunity: as criminals sell wildlife products online, they leave digital\ntraces of their activity that can provide insights into trafficking activities\nas well as how they can be disrupted. The challenge lies in finding these\ntraces. Online marketplaces publish ads for a plethora of products, and\nidentifying ads for wildlife-related products is like finding a needle in a\nhaystack. Learning classifiers can automate ad identification, but creating\nthem requires costly, time-consuming data labeling that hinders support for\ndiverse ads and research questions. This paper addresses a critical challenge\nin the data science pipeline for wildlife trafficking analytics: generating\nquality labeled data for classifiers that select relevant data. While large\nlanguage models (LLMs) can directly label advertisements, doing so at scale is\nprohibitively expensive. We propose a cost-effective strategy that leverages\nLLMs to generate pseudo labels for a small sample of the data and uses these\nlabels to create specialized classification models. Our novel method\nautomatically gathers diverse and representative samples to be labeled while\nminimizing the labeling costs. Our experimental evaluation shows that our\nclassifiers achieve up to 95% F1 score, outperforming LLMs at a lower cost. We\npresent real use cases that demonstrate the effectiveness of our approach in\nenabling analyses of different aspects of wildlife trafficking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u4f2a\u6807\u7b7e\u7684\u4f4e\u6210\u672c\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u9ad8\u6548\u5206\u7c7b\u5668\uff0c\u4ee5\u8bc6\u522b\u91ce\u751f\u52a8\u7269\u975e\u6cd5\u8d38\u6613\u7684\u5728\u7ebf\u5e7f\u544a\u3002", "motivation": "\u91ce\u751f\u52a8\u7269\u975e\u6cd5\u8d38\u6613\u5bf9\u751f\u6001\u548c\u516c\u5171\u5065\u5eb7\u6784\u6210\u5a01\u80c1\uff0c\u800c\u7535\u5b50\u5546\u52a1\u5e73\u53f0\u4e3a\u975e\u6cd5\u4ea4\u6613\u63d0\u4f9b\u4e86\u4fbf\u5229\u3002\u8bc6\u522b\u8fd9\u4e9b\u5e7f\u544a\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002", "method": "\u5229\u7528LLM\u4e3a\u5c11\u91cf\u6570\u636e\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u518d\u7528\u8fd9\u4e9b\u6807\u7b7e\u8bad\u7ec3\u4e13\u7528\u5206\u7c7b\u5668\uff0c\u540c\u65f6\u81ea\u52a8\u6536\u96c6\u591a\u6837\u5316\u7684\u6837\u672c\u4ee5\u964d\u4f4e\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6784\u5efa\u7684\u5206\u7c7b\u5668F1\u5206\u6570\u9ad8\u8fbe95%\uff0c\u4f18\u4e8e\u76f4\u63a5\u4f7f\u7528LLM\u4e14\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u91ce\u751f\u52a8\u7269\u975e\u6cd5\u8d38\u6613\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2504.21303", "pdf": "https://arxiv.org/pdf/2504.21303", "abs": "https://arxiv.org/abs/2504.21303", "authors": ["Xiao Xiao", "Yu Su", "Sijing Zhang", "Zhang Chen", "Yadong Chen", "Tian Liu"], "title": "Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit probabilistic output characteristics,\nyet conventional evaluation frameworks rely on deterministic scalar metrics.\nThis study introduces a Bayesian approach for LLM capability assessment that\nintegrates prior knowledge through probabilistic inference, addressing\nlimitations under limited-sample regimes. By treating model capabilities as\nlatent variables and leveraging a curated query set to induce discriminative\nresponses, we formalize model ranking as a Bayesian hypothesis testing problem\nover mutually exclusive capability intervals. Experimental evaluations with\nGPT-series models demonstrate that the proposed method achieves superior\ndiscrimination compared to conventional evaluation methods. Results indicate\nthat even with reduced sample sizes, the approach maintains statistical\nrobustness while providing actionable insights, such as probabilistic\nstatements about a model's likelihood of surpassing specific baselines. This\nwork advances LLM evaluation methodologies by bridging Bayesian inference with\npractical constraints in real-world deployment scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u65b9\u6cd5\u7684LLM\u80fd\u529b\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u63a8\u65ad\u6574\u5408\u5148\u9a8c\u77e5\u8bc6\uff0c\u89e3\u51b3\u4e86\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u6846\u67b6\u4f9d\u8d56\u786e\u5b9a\u6027\u6807\u91cf\u6307\u6807\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349LLM\u7684\u6982\u7387\u8f93\u51fa\u7279\u6027\u3002", "method": "\u5c06\u6a21\u578b\u80fd\u529b\u89c6\u4e3a\u6f5c\u5728\u53d8\u91cf\uff0c\u5229\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u67e5\u8be2\u96c6\u8bf1\u5bfc\u5224\u522b\u6027\u54cd\u5e94\uff0c\u5c06\u6a21\u578b\u6392\u540d\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u8d1d\u53f6\u65af\u5047\u8bbe\u68c0\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728GPT\u7cfb\u5217\u6a21\u578b\u4e0a\u4f18\u4e8e\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5373\u4f7f\u6837\u672c\u91cf\u51cf\u5c11\u4ecd\u4fdd\u6301\u7edf\u8ba1\u7a33\u5065\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u65ad\u4e0e\u5b9e\u9645\u90e8\u7f72\u7ea6\u675f\u7684\u7ed3\u5408\uff0c\u63a8\u52a8\u4e86LLM\u8bc4\u4f30\u65b9\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.21336", "pdf": "https://arxiv.org/pdf/2504.21336", "abs": "https://arxiv.org/abs/2504.21336", "authors": ["Linshan Wu", "Yuxiang Nie", "Sunan He", "Jiaxin Zhuang", "Hao Chen"], "title": "UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation", "categories": ["cs.CV"], "comment": "The first universal foundation model for grounded biomedical image\n  interpretation", "summary": "Multi-modal interpretation of biomedical images opens up novel opportunities\nin biomedical image analysis. Conventional AI approaches typically rely on\ndisjointed training, i.e., Large Language Models (LLMs) for clinical text\ngeneration and segmentation models for target extraction, which results in\ninflexible real-world deployment and a failure to leverage holistic biomedical\ninformation. To this end, we introduce UniBiomed, the first universal\nfoundation model for grounded biomedical image interpretation. UniBiomed is\nbased on a novel integration of Multi-modal Large Language Model (MLLM) and\nSegment Anything Model (SAM), which effectively unifies the generation of\nclinical texts and the segmentation of corresponding biomedical objects for\ngrounded interpretation. In this way, UniBiomed is capable of tackling a wide\nrange of biomedical tasks across ten diverse biomedical imaging modalities. To\ndevelop UniBiomed, we curate a large-scale dataset comprising over 27 million\ntriplets of images, annotations, and text descriptions across ten imaging\nmodalities. Extensive validation on 84 internal and external datasets\ndemonstrated that UniBiomed achieves state-of-the-art performance in\nsegmentation, disease recognition, region-aware diagnosis, visual question\nanswering, and report generation. Moreover, unlike previous models that rely on\nclinical experts to pre-diagnose images and manually craft precise textual or\nvisual prompts, UniBiomed can provide automated and end-to-end grounded\ninterpretation for biomedical image analysis. This represents a novel paradigm\nshift in clinical workflows, which will significantly improve diagnostic\nefficiency. In summary, UniBiomed represents a novel breakthrough in biomedical\nAI, unlocking powerful grounded interpretation capabilities for more accurate\nand efficient biomedical image analysis.", "AI": {"tldr": "UniBiomed\u662f\u4e00\u79cd\u65b0\u578b\u7684\u591a\u6a21\u6001\u751f\u7269\u533b\u5b66\u56fe\u50cf\u57fa\u7840\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u548cSegment Anything Model\uff08SAM\uff09\uff0c\u80fd\u591f\u7edf\u4e00\u751f\u6210\u4e34\u5e8a\u6587\u672c\u548c\u5206\u5272\u751f\u7269\u533b\u5b66\u5bf9\u8c61\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u5316\u89e3\u91ca\u3002", "motivation": "\u4f20\u7edfAI\u65b9\u6cd5\u5728\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u4f9d\u8d56\u5206\u79bb\u7684\u8bad\u7ec3\u6a21\u578b\uff08\u5982LLM\u548c\u5206\u5272\u6a21\u578b\uff09\uff0c\u5bfc\u81f4\u5b9e\u9645\u90e8\u7f72\u4e0d\u7075\u6d3b\u4e14\u65e0\u6cd5\u5229\u7528\u6574\u4f53\u751f\u7269\u533b\u5b66\u4fe1\u606f\u3002UniBiomed\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "UniBiomed\u901a\u8fc7\u6574\u5408MLLM\u548cSAM\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08\u5305\u542b2700\u4e07\u5f20\u56fe\u50cf\u3001\u6807\u6ce8\u548c\u6587\u672c\u63cf\u8ff0\uff09\uff0c\u5e76\u572884\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "UniBiomed\u5728\u5206\u5272\u3001\u75be\u75c5\u8bc6\u522b\u3001\u533a\u57df\u611f\u77e5\u8bca\u65ad\u3001\u89c6\u89c9\u95ee\u7b54\u548c\u62a5\u544a\u751f\u6210\u7b49\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4e14\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "conclusion": "UniBiomed\u4ee3\u8868\u4e86\u751f\u7269\u533b\u5b66AI\u7684\u65b0\u7a81\u7834\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u6548\u7387\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21254", "pdf": "https://arxiv.org/pdf/2504.21254", "abs": "https://arxiv.org/abs/2504.21254", "authors": ["Sixuan Wang", "Jiao Yin", "Jinli Cao", "MingJian Tang", "Hua Wang", "Yanchun Zhang"], "title": "ABG-NAS: Adaptive Bayesian Genetic Neural Architecture Search for Graph Representation Learning", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Effective and efficient graph representation learning is essential for\nenabling critical downstream tasks, such as node classification, link\nprediction, and subgraph search. However, existing graph neural network (GNN)\narchitectures often struggle to adapt to diverse and complex graph structures,\nlimiting their ability to provide robust and generalizable representations. To\naddress this challenge, we propose ABG-NAS, a novel framework for automated\ngraph neural network architecture search tailored for efficient graph\nrepresentation learning. ABG-NAS encompasses three key components: a\nComprehensive Architecture Search Space (CASS), an Adaptive Genetic\nOptimization Strategy (AGOS), and a Bayesian-Guided Tuning Module (BGTM). CASS\nsystematically explores diverse propagation (P) and transformation (T)\noperations, enabling the discovery of GNN architectures capable of capturing\nintricate graph characteristics. AGOS dynamically balances exploration and\nexploitation, ensuring search efficiency and preserving solution diversity.\nBGTM further optimizes hyperparameters periodically, enhancing the scalability\nand robustness of the resulting architectures. Empirical evaluations on\nbenchmark datasets (Cora, PubMed, Citeseer, and CoraFull) demonstrate that\nABG-NAS consistently outperforms both manually designed GNNs and\nstate-of-the-art neural architecture search (NAS) methods. These results\nhighlight the potential of ABG-NAS to advance graph representation learning by\nproviding scalable and adaptive solutions for diverse graph structures. Our\ncode is publicly available at https://github.com/sserranw/ABG-NAS.", "AI": {"tldr": "ABG-NAS\u662f\u4e00\u79cd\u81ea\u52a8\u5316\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u7efc\u5408\u641c\u7d22\u7a7a\u95f4\u3001\u81ea\u9002\u5e94\u9057\u4f20\u4f18\u5316\u548c\u8d1d\u53f6\u65af\u8c03\u4f18\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u56fe\u8868\u793a\u5b66\u4e60\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u96be\u4ee5\u9002\u5e94\u590d\u6742\u591a\u6837\u7684\u56fe\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u3002", "method": "ABG-NAS\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u7efc\u5408\u67b6\u6784\u641c\u7d22\u7a7a\u95f4\uff08CASS\uff09\u3001\u81ea\u9002\u5e94\u9057\u4f20\u4f18\u5316\u7b56\u7565\uff08AGOS\uff09\u548c\u8d1d\u53f6\u65af\u5f15\u5bfc\u8c03\u4f18\u6a21\u5757\uff08BGTM\uff09\uff0c\u5206\u522b\u7528\u4e8e\u63a2\u7d22\u64cd\u4f5c\u3001\u4f18\u5316\u641c\u7d22\u8fc7\u7a0b\u548c\u8c03\u4f18\u8d85\u53c2\u6570\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cABG-NAS\u8868\u73b0\u4f18\u4e8e\u624b\u52a8\u8bbe\u8ba1\u7684GNN\u548c\u5176\u4ed6NAS\u65b9\u6cd5\u3002", "conclusion": "ABG-NAS\u4e3a\u56fe\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.21330", "pdf": "https://arxiv.org/pdf/2504.21330", "abs": "https://arxiv.org/abs/2504.21330", "authors": ["Kaixun Yang", "Mladen Rakovi\u0107", "Dragan Ga\u0161evi\u0107", "Guanliang Chen"], "title": "Does the Prompt-based Large Language Model Recognize Students' Demographics and Introduce Bias in Essay Scoring?", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are widely used in Automated Essay Scoring (AES)\ndue to their ability to capture semantic meaning. Traditional fine-tuning\napproaches required technical expertise, limiting accessibility for educators\nwith limited technical backgrounds. However, prompt-based tools like ChatGPT\nhave made AES more accessible, enabling educators to obtain machine-generated\nscores using natural-language prompts (i.e., the prompt-based paradigm).\nDespite advancements, prior studies have shown bias in fine-tuned LLMs,\nparticularly against disadvantaged groups. It remains unclear whether such\nbiases persist or are amplified in the prompt-based paradigm with cutting-edge\ntools. Since such biases are believed to stem from the demographic information\nembedded in pre-trained models (i.e., the ability of LLMs' text embeddings to\npredict demographic attributes), this study explores the relationship between\nthe model's predictive power of students' demographic attributes based on their\nwritten works and its predictive bias in the scoring task in the prompt-based\nparadigm. Using a publicly available dataset of over 25,000 students'\nargumentative essays, we designed prompts to elicit demographic inferences\n(i.e., gender, first-language background) from GPT-4o and assessed fairness in\nautomated scoring. Then we conducted multivariate regression analysis to\nexplore the impact of the model's ability to predict demographics on its\nscoring outcomes. Our findings revealed that (i) prompt-based LLMs can somewhat\ninfer students' demographics, particularly their first-language backgrounds,\nfrom their essays; (ii) scoring biases are more pronounced when the LLM\ncorrectly predicts students' first-language background than when it does not;\nand (iii) scoring error for non-native English speakers increases when the LLM\ncorrectly identifies them as non-native.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u63d0\u793a\u7684LLM\u5728\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u4e2d\u7684\u504f\u89c1\u95ee\u9898\uff0c\u53d1\u73b0\u6a21\u578b\u80fd\u63a8\u65ad\u5b66\u751f\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\uff0c\u4e14\u8bc4\u5206\u504f\u89c1\u4e0e\u5176\u9884\u6d4b\u80fd\u529b\u76f8\u5173\u3002", "motivation": "\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u6280\u672f\u80cc\u666f\uff0c\u800c\u63d0\u793a\u5de5\u5177\u4f7fAES\u66f4\u6613\u7528\uff0c\u4f46\u504f\u89c1\u95ee\u9898\u5c1a\u672a\u660e\u786e\u3002", "method": "\u4f7f\u7528GPT-4o\u548c25,000\u7bc7\u8bae\u8bba\u6587\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u63d0\u793a\u63a8\u65ad\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u5e76\u8bc4\u4f30\u8bc4\u5206\u516c\u5e73\u6027\u3002", "result": "LLM\u80fd\u63a8\u65ad\u5b66\u751f\u8bed\u8a00\u80cc\u666f\uff0c\u8bc4\u5206\u504f\u89c1\u5728\u6b63\u786e\u9884\u6d4b\u65f6\u66f4\u663e\u8457\uff0c\u975e\u6bcd\u8bed\u8005\u8bc4\u5206\u8bef\u5dee\u589e\u52a0\u3002", "conclusion": "\u63d0\u793a\u8303\u5f0f\u4e0bLLM\u7684\u8bc4\u5206\u504f\u89c1\u4e0e\u4eba\u53e3\u7edf\u8ba1\u9884\u6d4b\u80fd\u529b\u76f8\u5173\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u516c\u5e73\u6027\u3002"}}
{"id": "2504.21340", "pdf": "https://arxiv.org/pdf/2504.21340", "abs": "https://arxiv.org/abs/2504.21340", "authors": ["Khoa Tuan Nguyen", "Ho-min Park", "Gaeun Oh", "Joris Vankerschaver", "Wesley De Neve"], "title": "Towards Improved Cervical Cancer Screening: Vision Transformer-Based Classification and Interpretability", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ISBI 2025 \"Challenge 2: Pap Smear Cell Classification\n  Challenge\"", "summary": "We propose a novel approach to cervical cell image classification for\ncervical cancer screening using the EVA-02 transformer model. We developed a\nfour-step pipeline: fine-tuning EVA-02, feature extraction, selecting important\nfeatures through multiple machine learning models, and training a new\nartificial neural network with optional loss weighting for improved\ngeneralization. With this design, our best model achieved an F1-score of\n0.85227, outperforming the baseline EVA-02 model (0.84878). We also utilized\nKernel SHAP analysis and identified key features correlating with cell\nmorphology and staining characteristics, providing interpretable insights into\nthe decision-making process of the fine-tuned model. Our code is available at\nhttps://github.com/Khoa-NT/isbi2025_ps3c.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eEVA-02\u53d8\u6362\u5668\u6a21\u578b\u7684\u5bab\u9888\u7ec6\u80de\u56fe\u50cf\u5206\u7c7b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u56db\u6b65\u6d41\u7a0b\u4f18\u5316\u6a21\u578b\u6027\u80fd\uff0cF1\u5206\u6570\u8fbe\u52300.85227\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u6539\u8fdb\u5bab\u9888\u764c\u7b5b\u67e5\u4e2d\u7684\u7ec6\u80de\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u8bca\u65ad\u5de5\u5177\u3002", "method": "\u56db\u6b65\u6d41\u7a0b\uff1a\u5fae\u8c03EVA-02\u3001\u7279\u5f81\u63d0\u53d6\u3001\u591a\u6a21\u578b\u7279\u5f81\u9009\u62e9\u3001\u8bad\u7ec3\u65b0\u795e\u7ecf\u7f51\u7edc\uff08\u53ef\u9009\u635f\u5931\u52a0\u6743\uff09\u3002", "result": "\u6700\u4f73\u6a21\u578bF1\u5206\u65700.85227\uff0c\u4f18\u4e8e\u57fa\u7ebf\uff080.84878\uff09\uff1b\u901a\u8fc7Kernel SHAP\u5206\u6790\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4e3a\u5bab\u9888\u764c\u7b5b\u67e5\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2504.21261", "pdf": "https://arxiv.org/pdf/2504.21261", "abs": "https://arxiv.org/abs/2504.21261", "authors": ["Kasra Jalaldoust", "Saber Salehkaleybar", "Negar Kiyavash"], "title": "Multi-Domain Causal Discovery in Bijective Causal Models", "categories": ["cs.LG", "cs.AI", "stat.ME"], "comment": "Proceedings of Causal Learning and Reasoning (CLeaR) 2025", "summary": "We consider the problem of causal discovery (a.k.a., causal structure\nlearning) in a multi-domain setting. We assume that the causal functions are\ninvariant across the domains, while the distribution of the exogenous noise may\nvary. Under causal sufficiency (i.e., no confounders exist), we show that the\ncausal diagram can be discovered under less restrictive functional assumptions\ncompared to previous work. What enables causal discovery in this setting is\nbijective generation mechanisms (BGM), which ensures that the functional\nrelation between the exogenous noise $E$ and the endogenous variable $Y$ is\nbijective and differentiable in both directions at every level of the cause\nvariable $X = x$. BGM generalizes a variety of models including additive noise\nmodel, LiNGAM, post-nonlinear model, and location-scale noise model. Further,\nwe derive a statistical test to find the parents set of the target variable.\nExperiments on various synthetic and real-world datasets validate our\ntheoretical findings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u591a\u57df\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u56e0\u679c\u53d1\u73b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5047\u8bbe\u56e0\u679c\u51fd\u6570\u8de8\u57df\u4e0d\u53d8\u800c\u566a\u58f0\u5206\u5e03\u53ef\u53d8\uff0c\u5229\u7528\u53cc\u5c04\u751f\u6210\u673a\u5236\uff08BGM\uff09\u653e\u5bbd\u4e86\u529f\u80fd\u5047\u8bbe\uff0c\u5e76\u63a8\u5bfc\u4e86\u7edf\u8ba1\u6d4b\u8bd5\u4ee5\u786e\u5b9a\u76ee\u6807\u53d8\u91cf\u7684\u7236\u96c6\u3002", "motivation": "\u89e3\u51b3\u591a\u57df\u8bbe\u7f6e\u4e0b\u56e0\u679c\u53d1\u73b0\u7684\u9650\u5236\uff0c\u901a\u8fc7\u653e\u5bbd\u529f\u80fd\u5047\u8bbe\u5e76\u5229\u7528\u566a\u58f0\u5206\u5e03\u7684\u53ef\u53d8\u6027\uff0c\u63d0\u9ad8\u56e0\u679c\u56fe\u7684\u53d1\u73b0\u80fd\u529b\u3002", "method": "\u91c7\u7528\u53cc\u5c04\u751f\u6210\u673a\u5236\uff08BGM\uff09\u786e\u4fdd\u566a\u58f0\u4e0e\u5185\u751f\u53d8\u91cf\u4e4b\u95f4\u7684\u51fd\u6570\u5173\u7cfb\u662f\u53cc\u5c04\u4e14\u53ef\u5fae\u7684\uff0c\u5e76\u63a8\u5bfc\u7edf\u8ba1\u6d4b\u8bd5\u4ee5\u8bc6\u522b\u7236\u96c6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\uff0cBGM\u80fd\u591f\u63a8\u5e7f\u591a\u79cd\u6a21\u578b\uff08\u5982\u52a0\u6027\u566a\u58f0\u6a21\u578b\u3001LiNGAM\u7b49\uff09\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "BGM\u4e3a\u591a\u57df\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u6846\u67b6\uff0c\u653e\u5bbd\u4e86\u529f\u80fd\u5047\u8bbe\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2504.21372", "pdf": "https://arxiv.org/pdf/2504.21372", "abs": "https://arxiv.org/abs/2504.21372", "authors": ["M\u00e1t\u00e9 Gedeon"], "title": "Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speech Event Extraction (SpeechEE) is a challenging task that lies at the\nintersection of Automatic Speech Recognition (ASR) and Natural Language\nProcessing (NLP), requiring the identification of structured event information\nfrom spoken language. In this work, we present a modular, pipeline-based\nSpeechEE framework that integrates high-performance ASR with semantic\nsearch-enhanced prompting of Large Language Models (LLMs). Our system first\nclassifies speech segments likely to contain events using a hybrid filtering\nmechanism including rule-based, BERT-based, and LLM-based models. It then\nemploys few-shot LLM prompting, dynamically enriched via semantic similarity\nretrieval, to identify event triggers and extract corresponding arguments. We\nevaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini)\nhighlighting significant performance gains with o1-mini, which achieves 63.3%\nF1 on trigger classification and 27.8% F1 on argument classification,\noutperforming prior benchmarks. Our results demonstrate that pipeline\napproaches, when empowered by retrieval-augmented LLMs, can rival or exceed\nend-to-end systems while maintaining interpretability and modularity. This work\nprovides practical insights into LLM-driven event extraction and opens pathways\nfor future hybrid models combining textual and acoustic features.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684\u8bed\u97f3\u4e8b\u4ef6\u63d0\u53d6\u6846\u67b6SpeechEE\uff0c\u7ed3\u5408\u9ad8\u6027\u80fdASR\u548c\u8bed\u4e49\u641c\u7d22\u589e\u5f3a\u7684LLM\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u89e6\u53d1\u548c\u53c2\u6570\u5206\u7c7b\u7684\u6027\u80fd\u3002", "motivation": "\u8bed\u97f3\u4e8b\u4ef6\u63d0\u53d6\uff08SpeechEE\uff09\u662fASR\u548cNLP\u4ea4\u53c9\u9886\u57df\u7684\u6311\u6218\u6027\u4efb\u52a1\uff0c\u9700\u8981\u4ece\u53e3\u8bed\u4e2d\u8bc6\u522b\u7ed3\u6784\u5316\u4e8b\u4ef6\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u6d41\u6c34\u7ebf\u6846\u67b6\uff0c\u7ed3\u5408ASR\u548c\u8bed\u4e49\u641c\u7d22\u589e\u5f3a\u7684LLM\u63d0\u793a\uff0c\u901a\u8fc7\u6df7\u5408\u8fc7\u6ee4\u673a\u5236\u5206\u7c7b\u8bed\u97f3\u6bb5\uff0c\u5e76\u4f7f\u7528\u52a8\u6001\u589e\u5f3a\u7684\u5c11\u6837\u672cLLM\u63d0\u793a\u63d0\u53d6\u4e8b\u4ef6\u89e6\u53d1\u548c\u53c2\u6570\u3002", "result": "\u4f7f\u7528o1-mini\u6a21\u578b\u5728\u89e6\u53d1\u5206\u7c7b\u548c\u53c2\u6570\u5206\u7c7b\u4e0a\u5206\u522b\u8fbe\u523063.3%\u548c27.8%\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u5148\u524d\u57fa\u51c6\u3002", "conclusion": "\u6d41\u6c34\u7ebf\u65b9\u6cd5\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u7684LLM\u53ef\u4e0e\u7aef\u5230\u7aef\u7cfb\u7edf\u5ab2\u7f8e\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u548c\u6a21\u5757\u5316\uff0c\u4e3a\u672a\u6765\u7ed3\u5408\u6587\u672c\u548c\u58f0\u5b66\u7279\u5f81\u7684\u6df7\u5408\u6a21\u578b\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.21344", "pdf": "https://arxiv.org/pdf/2504.21344", "abs": "https://arxiv.org/abs/2504.21344", "authors": ["Luoting Zhuang", "Seyed Mohammad Hossein Tabatabaei", "Ramin Salehi-Rad", "Linh M. Tran", "Denise R. Aberle", "Ashley E. Prosper", "William Hsu"], "title": "Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early Lung Cancer Detection", "categories": ["cs.CV", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Objective: A number of machine learning models have utilized semantic\nfeatures, deep features, or both to assess lung nodule malignancy. However,\ntheir reliance on manual annotation during inference, limited interpretability,\nand sensitivity to imaging variations hinder their application in real-world\nclinical settings. Thus, this research aims to integrate semantic features\nderived from radiologists' assessments of nodules, allowing the model to learn\nclinically relevant, robust, and explainable features for predicting lung\ncancer. Methods: We obtained 938 low-dose CT scans from the National Lung\nScreening Trial with 1,246 nodules and semantic features. The Lung Image\nDatabase Consortium dataset contains 1,018 CT scans, with 2,625 lesions\nannotated for nodule characteristics. Three external datasets were obtained\nfrom UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We\nfinetuned a pretrained Contrastive Language-Image Pretraining model with a\nparameter-efficient fine-tuning approach to align imaging and semantic features\nand predict the one-year lung cancer diagnosis. Results: We evaluated the\nperformance of the one-year diagnosis of lung cancer with AUROC and AUPRC and\ncompared it to three state-of-the-art models. Our model demonstrated an AUROC\nof 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on\nexternal datasets. Using CLIP, we also obtained predictions on semantic\nfeatures, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and\npleural attachment (0.84), that can be used to explain model predictions.\nConclusion: Our approach accurately classifies lung nodules as benign or\nmalignant, providing explainable outputs, aiding clinicians in comprehending\nthe underlying meaning of model predictions. This approach also prevents the\nmodel from learning shortcuts and generalizes across clinical settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u653e\u5c04\u79d1\u533b\u751f\u8bc4\u4f30\u7684\u8bed\u4e49\u7279\u5f81\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u80ba\u764c\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f9d\u8d56\u624b\u52a8\u6807\u6ce8\u3001\u89e3\u91ca\u6027\u5dee\u4e14\u5bf9\u6210\u50cf\u53d8\u5316\u654f\u611f\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u6574\u5408\u8bed\u4e49\u7279\u5f81\uff0c\u5b66\u4e60\u4e34\u5e8a\u76f8\u5173\u4e14\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u3002", "method": "\u4f7f\u7528\u591a\u4e2aCT\u626b\u63cf\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\u4f18\u5316\u9884\u8bad\u7ec3\u7684CLIP\u6a21\u578b\uff0c\u5bf9\u9f50\u6210\u50cf\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u9884\u6d4b\u4e00\u5e74\u5185\u80ba\u764c\u8bca\u65ad\u3002", "result": "\u6a21\u578b\u5728\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff08AUROC:0.90\uff0cAUPRC:0.78\uff09\uff0c\u5e76\u80fd\u9884\u6d4b\u8bed\u4e49\u7279\u5f81\uff08\u5982\u7ed3\u8282\u8fb9\u7f18\u3001\u4e00\u81f4\u6027\u7b49\uff09\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u5206\u7c7b\u80ba\u7ed3\u8282\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u7ed3\u679c\uff0c\u5e2e\u52a9\u4e34\u5e8a\u533b\u751f\u7406\u89e3\u6a21\u578b\u51b3\u7b56\uff0c\u5e76\u5177\u6709\u8de8\u4e34\u5e8a\u73af\u5883\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.21028", "pdf": "https://arxiv.org/pdf/2504.21028", "abs": "https://arxiv.org/abs/2504.21028", "authors": ["Ivan Montoya Sanchez", "Shaswata Mitra", "Aritran Piplai", "Sudip Mittal"], "title": "Semantic-Aware Contrastive Fine-Tuning: Boosting Multimodal Malware Classification with Discriminative Embeddings", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "8 pages, 5 figures, 5 tables", "summary": "The rapid evolution of malware variants requires robust classification\nmethods to enhance cybersecurity. While Large Language Models (LLMs) offer\npotential for generating malware descriptions to aid family classification,\ntheir utility is limited by semantic embedding overlaps and misalignment with\nbinary behavioral features. We propose a contrastive fine-tuning (CFT) method\nthat refines LLM embeddings via targeted selection of hard negative samples\nbased on cosine similarity, enabling LLMs to distinguish between closely\nrelated malware families. Our approach combines high-similarity negatives to\nenhance discriminative power and mid-tier negatives to increase embedding\ndiversity, optimizing both precision and generalization. Evaluated on the\nCIC-AndMal-2020 and BODMAS datasets, our refined embeddings are integrated into\na multimodal classifier within a Model-Agnostic Meta-Learning (MAML) framework\non a few-shot setting. Experiments demonstrate significant improvements: our\nmethod achieves 63.15% classification accuracy with as few as 20 samples on\nCIC-AndMal-2020, outperforming baselines by 11--21 percentage points and\nsurpassing prior negative sampling strategies. Ablation studies confirm the\nsuperiority of similarity-based selection over random sampling, with gains of\n10-23%. Additionally, fine-tuned LLMs generate attribute-aware descriptions\nthat generalize to unseen variants, bridging textual and binary feature gaps.\nThis work advances malware classification by enabling nuanced semantic\ndistinctions and provides a scalable framework for adapting LLMs to\ncybersecurity challenges.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6bd4\u5fae\u8c03\uff08CFT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u786c\u8d1f\u6837\u672c\u9009\u62e9\u4f18\u5316LLM\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u6076\u610f\u8f6f\u4ef6\u53d8\u79cd\u7684\u5feb\u901f\u6f14\u53d8\u9700\u8981\u66f4\u9c81\u68d2\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u800c\u73b0\u6709LLM\u5728\u8bed\u4e49\u5d4c\u5165\u548c\u884c\u4e3a\u7279\u5f81\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u5fae\u8c03\u65b9\u6cd5\uff0c\u7ed3\u5408\u9ad8\u76f8\u4f3c\u5ea6\u548c\u4e2d\u7b49\u76f8\u4f3c\u5ea6\u8d1f\u6837\u672c\uff0c\u4f18\u5316LLM\u5d4c\u5165\uff0c\u5e76\u5728MAML\u6846\u67b6\u4e0b\u6784\u5efa\u591a\u6a21\u6001\u5206\u7c7b\u5668\u3002", "result": "\u5728CIC-AndMal-2020\u6570\u636e\u96c6\u4e0a\uff0c\u4ec5\u752820\u4e2a\u6837\u672c\u5373\u8fbe\u523063.15%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf11-21\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u533a\u5206\u548c\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u7684\u7814\u7a76\uff0c\u5e76\u4e3aLLM\u5728\u7f51\u7edc\u5b89\u5168\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.21289", "pdf": "https://arxiv.org/pdf/2504.21289", "abs": "https://arxiv.org/abs/2504.21289", "authors": ["Yan Huang", "Da-Qing Zhang"], "title": "Orthogonal Factor-Based Biclustering Algorithm (BCBOF) for High-Dimensional Data and Its Application in Stock Trend Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Biclustering is an effective technique in data mining and pattern\nrecognition. Biclustering algorithms based on traditional clustering face two\nfundamental limitations when processing high-dimensional data: (1) The distance\nconcentration phenomenon in high-dimensional spaces leads to data sparsity,\nrendering similarity measures ineffective; (2) Mainstream linear dimensionality\nreduction methods disrupt critical local structural patterns. To apply\nbiclustering to high-dimensional datasets, we propose an orthogonal\nfactor-based biclustering algorithm (BCBOF). First, we constructed orthogonal\nfactors in the vector space of the high-dimensional dataset. Then, we performed\nclustering using the coordinates of the original data in the orthogonal\nsubspace as clustering targets. Finally, we obtained biclustering results of\nthe original dataset. Since dimensionality reduction was applied before\nclustering, the proposed algorithm effectively mitigated the data sparsity\nproblem caused by high dimensionality. Additionally, we applied this\nbiclustering algorithm to stock technical indicator combinations and stock\nprice trend prediction. Biclustering results were transformed into fuzzy rules,\nand we incorporated profit-preserving and stop-loss rules into the rule set,\nultimately forming a fuzzy inference system for stock price trend predictions\nand trading signals. To evaluate the performance of BCBOF, we compared it with\nexisting biclustering methods using multiple evaluation metrics. The results\nshowed that our algorithm outperformed other biclustering techniques. To\nvalidate the effectiveness of the fuzzy inference system, we conducted virtual\ntrading experiments using historical data from 10 A-share stocks. The\nexperimental results showed that the generated trading strategies yielded\nhigher returns for investors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b63\u4ea4\u56e0\u5b50\u7684\u53cc\u805a\u7c7b\u7b97\u6cd5\uff08BCBOF\uff09\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u6570\u636e\u5904\u7406\u4e2d\u7684\u8ddd\u79bb\u96c6\u4e2d\u548c\u5c40\u90e8\u7ed3\u6784\u7834\u574f\u95ee\u9898\uff0c\u5e76\u5728\u80a1\u7968\u9884\u6d4b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u53cc\u805a\u7c7b\u7b97\u6cd5\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u56e0\u8ddd\u79bb\u96c6\u4e2d\u548c\u7ebf\u6027\u964d\u7ef4\u7834\u574f\u5c40\u90e8\u7ed3\u6784\u800c\u5931\u6548\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u6784\u5efa\u6b63\u4ea4\u56e0\u5b50\u7a7a\u95f4\uff0c\u4ee5\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5750\u6807\u805a\u7c7b\uff0c\u751f\u6210\u53cc\u805a\u7c7b\u7ed3\u679c\uff0c\u5e76\u5e94\u7528\u4e8e\u80a1\u7968\u9884\u6d4b\u3002", "result": "BCBOF\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u865a\u62df\u4ea4\u6613\u5b9e\u9a8c\u663e\u793a\u5176\u751f\u6210\u7684\u7b56\u7565\u5e26\u6765\u66f4\u9ad8\u6536\u76ca\u3002", "conclusion": "BCBOF\u6709\u6548\u89e3\u51b3\u9ad8\u7ef4\u6570\u636e\u95ee\u9898\uff0c\u5e76\u5728\u80a1\u7968\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2504.21421", "pdf": "https://arxiv.org/pdf/2504.21421", "abs": "https://arxiv.org/abs/2504.21421", "authors": ["Linxuan Wang", "Shuiyuan Yu"], "title": "The Distribution of Dependency Distance and Hierarchical Distance in Contemporary Written Japanese and Its Influencing Factors", "categories": ["cs.CL"], "comment": "This paper has been accepted by the 13th International Quantitative\n  Linguistics Conference QUALICO 2025", "summary": "To explore the relationship between dependency distance (DD) and hierarchical\ndistance (HD) in Japanese, we compared the probability distributions of DD and\nHD with and without sentence length fixed, and analyzed the changes in mean\ndependency distance (MDD) and mean hierarchical distance (MHD) as sentence\nlength increases, along with their correlation coefficient based on the\nBalanced Corpus of Contemporary Written Japanese. It was found that the valency\nof the predicates is the underlying factor behind the trade-off relation\nbetween MDD and MHD in Japanese. Native speakers of Japanese regulate the\nlinear complexity and hierarchical complexity through the valency of the\npredicates, and the relative sizes of MDD and MHD depend on whether the\nthreshold of valency has been reached. Apart from the cognitive load, the\nvalency of the predicates also affects the probability distributions of DD and\nHD. The effect of the valency of the predicates on the distribution of HD is\ngreater than on that of DD, which leads to differences in their probability\ndistributions and causes the mean of MDD to be lower than that of MHD.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u65e5\u8bed\u4e2d\u4f9d\u5b58\u8ddd\u79bb\uff08DD\uff09\u548c\u5c42\u6b21\u8ddd\u79bb\uff08HD\uff09\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u8c13\u8bcd\u7684\u914d\u4ef7\u662f\u5f71\u54cdMDD\u548cMHD\u6743\u8861\u5173\u7cfb\u7684\u6839\u672c\u56e0\u7d20\u3002", "motivation": "\u63a2\u7d22\u65e5\u8bed\u4e2dDD\u548cHD\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u8c13\u8bcd\u914d\u4ef7\u5bf9\u7ebf\u6027\u590d\u6742\u6027\u548c\u5c42\u6b21\u590d\u6742\u6027\u7684\u8c03\u8282\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u56fa\u5b9a\u548c\u4e0d\u56fa\u5b9a\u53e5\u5b50\u957f\u5ea6\uff0c\u6bd4\u8f83DD\u548cHD\u7684\u6982\u7387\u5206\u5e03\uff0c\u5206\u6790MDD\u548cMHD\u968f\u53e5\u5b50\u957f\u5ea6\u7684\u53d8\u5316\u53ca\u5176\u76f8\u5173\u6027\u3002", "result": "\u8c13\u8bcd\u914d\u4ef7\u662fMDD\u548cMHD\u6743\u8861\u5173\u7cfb\u7684\u6839\u672c\u539f\u56e0\uff0c\u4e14\u5bf9HD\u5206\u5e03\u7684\u5f71\u54cd\u5927\u4e8eDD\uff0c\u5bfc\u81f4MDD\u5747\u503c\u4f4e\u4e8eMHD\u3002", "conclusion": "\u65e5\u8bed\u6bcd\u8bed\u8005\u901a\u8fc7\u8c13\u8bcd\u914d\u4ef7\u8c03\u8282\u8bed\u8a00\u590d\u6742\u6027\uff0c\u914d\u4ef7\u9608\u503c\u51b3\u5b9aMDD\u548cMHD\u7684\u76f8\u5bf9\u5927\u5c0f\u3002"}}
{"id": "2504.21356", "pdf": "https://arxiv.org/pdf/2504.21356", "abs": "https://arxiv.org/abs/2504.21356", "authors": ["Hong Zhang", "Zhongjie Duan", "Xingjun Wang", "Yingda Chen", "Yuze Zhao", "Yu Zhang"], "title": "Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unified multimodal large language models (MLLMs) aim to integrate multimodal\nunderstanding and generation abilities through a single framework. Despite\ntheir versatility, existing open-source unified models exhibit performance gaps\nagainst domain-specific architectures. To bridge this gap, we present\nNexus-Gen, a unified model that synergizes the language reasoning capabilities\nof LLMs with the image synthesis power of diffusion models. To align the\nembedding space of the LLM and diffusion model, we conduct a dual-phase\nalignment training process. (1) The autoregressive LLM learns to predict image\nembeddings conditioned on multimodal inputs, while (2) the vision decoder is\ntrained to reconstruct high-fidelity images from these embeddings. During\ntraining the LLM, we identified a critical discrepancy between the\nautoregressive paradigm's training and inference phases, where error\naccumulation in continuous embedding space severely degrades generation\nquality. To avoid this issue, we introduce a prefilled autoregression strategy\nthat prefills input sequence with position-embedded special tokens instead of\ncontinuous embeddings. Through dual-phase training, Nexus-Gen has developed the\nintegrated capability to comprehensively address the image understanding,\ngeneration and editing tasks. All models, datasets, and codes are published at\nhttps://github.com/modelscope/Nexus-Gen.git to facilitate further advancements\nacross the field.", "AI": {"tldr": "Nexus-Gen\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u7684\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u548c\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u5408\u6210\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5f00\u6e90\u7edf\u4e00\u6a21\u578b\u4e0e\u9886\u57df\u4e13\u7528\u67b6\u6784\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u7edf\u4e00\u6a21\u578b\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u4e0a\u8868\u73b0\u4e0d\u5982\u9886\u57df\u4e13\u7528\u67b6\u6784\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u7edf\u4e00\u6a21\u578b\u3002", "method": "\u91c7\u7528\u53cc\u9636\u6bb5\u5bf9\u9f50\u8bad\u7ec3\uff1aLLM\u5b66\u4e60\u9884\u6d4b\u56fe\u50cf\u5d4c\u5165\uff0c\u89c6\u89c9\u89e3\u7801\u5668\u91cd\u5efa\u9ad8\u4fdd\u771f\u56fe\u50cf\uff1b\u5f15\u5165\u9884\u586b\u5145\u81ea\u56de\u5f52\u7b56\u7565\u4ee5\u907f\u514d\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "Nexus-Gen\u80fd\u591f\u5168\u9762\u5904\u7406\u56fe\u50cf\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u3002", "conclusion": "Nexus-Gen\u901a\u8fc7\u53cc\u9636\u6bb5\u8bad\u7ec3\u548c\u9884\u586b\u5145\u7b56\u7565\uff0c\u6210\u529f\u6574\u5408\u4e86\u591a\u6a21\u6001\u80fd\u529b\uff0c\u5e76\u5f00\u6e90\u4e86\u6a21\u578b\u548c\u4ee3\u7801\u4ee5\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2504.21029", "pdf": "https://arxiv.org/pdf/2504.21029", "abs": "https://arxiv.org/abs/2504.21029", "authors": ["Ben Goertzel", "Paulos Yibelo"], "title": "PICO: Secure Transformers via Robust Prompt Isolation and Cybersecurity Oversight", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "We propose a robust transformer architecture designed to prevent prompt\ninjection attacks and ensure secure, reliable response generation. Our PICO\n(Prompt Isolation and Cybersecurity Oversight) framework structurally separates\ntrusted system instructions from untrusted user inputs through dual channels\nthat are processed independently and merged only by a controlled, gated fusion\nmechanism. In addition, we integrate a specialized Security Expert Agent within\na Mixture-of-Experts (MoE) framework and incorporate a Cybersecurity Knowledge\nGraph (CKG) to supply domain-specific reasoning. Our training design further\nensures that the system prompt branch remains immutable while the rest of the\nnetwork learns to handle adversarial inputs safely. This PICO framework is\npresented via a general mathematical formulation, then elaborated in terms of\nthe specifics of transformer architecture, and fleshed out via hypothetical\ncase studies including Policy Puppetry attacks. While the most effective\nimplementation may involve training transformers in a PICO-based way from\nscratch, we also present a cost-effective fine-tuning approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPICO\u7684\u9c81\u68d2Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u53cc\u901a\u9053\u5206\u79bb\u7cfb\u7edf\u6307\u4ee4\u4e0e\u7528\u6237\u8f93\u5165\uff0c\u7ed3\u5408\u5b89\u5168\u4e13\u5bb6\u4ee3\u7406\u548c\u77e5\u8bc6\u56fe\u8c31\uff0c\u9632\u6b62\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u3002", "motivation": "\u89e3\u51b3Transformer\u6a21\u578b\u5728\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u4e0b\u7684\u5b89\u5168\u95ee\u9898\uff0c\u786e\u4fdd\u751f\u6210\u54cd\u5e94\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u53cc\u901a\u9053\u5904\u7406\u7cfb\u7edf\u6307\u4ee4\u548c\u7528\u6237\u8f93\u5165\uff0c\u7ed3\u5408MoE\u6846\u67b6\u4e2d\u7684\u5b89\u5168\u4e13\u5bb6\u4ee3\u7406\u548c\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u901a\u8fc7\u6570\u5b66\u516c\u5f0f\u548c\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u3002", "result": "PICO\u6846\u67b6\u80fd\u6709\u6548\u9632\u6b62\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u540c\u65f6\u652f\u6301\u4ece\u5934\u8bad\u7ec3\u6216\u4f4e\u6210\u672c\u5fae\u8c03\u3002", "conclusion": "PICO\u4e3aTransformer\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b89\u5168\u53ef\u9760\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u9002\u7528\u4e8e\u5bf9\u6297\u6027\u73af\u5883\u3002"}}
{"id": "2504.21296", "pdf": "https://arxiv.org/pdf/2504.21296", "abs": "https://arxiv.org/abs/2504.21296", "authors": ["Renqiang Luo", "Ziqi Xu", "Xikun Zhang", "Qing Qing", "Huafei Huang", "Enyan Dai", "Zhe Wang", "Bo Yang"], "title": "Fairness in Graph Learning Augmented with Machine Learning: A Survey", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Augmenting specialised machine learning techniques into traditional graph\nlearning models has achieved notable success across various domains, including\nfederated graph learning, dynamic graph learning, and graph transformers.\nHowever, the intricate mechanisms of these specialised techniques introduce\nsignificant challenges in maintaining model fairness, potentially resulting in\ndiscriminatory outcomes in high-stakes applications such as recommendation\nsystems, disaster response, criminal justice, and loan approval. This paper\nsystematically examines the unique fairness challenges posed by Graph Learning\naugmented with Machine Learning (GL-ML). It highlights the complex interplay\nbetween graph learning mechanisms and machine learning techniques, emphasising\nhow the augmentation of machine learning both enhances and complicates\nfairness. Additionally, we explore four critical techniques frequently employed\nto improve fairness in GL-ML methods. By thoroughly investigating the root\ncauses and broader implications of fairness challenges in this rapidly evolving\nfield, this work establishes a robust foundation for future research and\ninnovation in GL-ML fairness.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u56fe\u5b66\u4e60\u4e0e\u673a\u5668\u5b66\u4e60\u7ed3\u5408\uff08GL-ML\uff09\u4e2d\u7684\u516c\u5e73\u6027\u6311\u6218\uff0c\u5206\u6790\u4e86\u5176\u590d\u6742\u673a\u5236\u53ca\u6f5c\u5728\u6b67\u89c6\u6027\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u56db\u79cd\u6539\u8fdb\u516c\u5e73\u6027\u7684\u5173\u952e\u6280\u672f\u3002", "motivation": "\u4f20\u7edf\u56fe\u5b66\u4e60\u6a21\u578b\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6280\u672f\u867d\u5728\u591a\u9886\u57df\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5176\u590d\u6742\u673a\u5236\u53ef\u80fd\u5bfc\u81f4\u516c\u5e73\u6027\u95ee\u9898\uff0c\u5f71\u54cd\u9ad8\u98ce\u9669\u5e94\u7528\uff08\u5982\u63a8\u8350\u7cfb\u7edf\u3001\u8d37\u6b3e\u5ba1\u6279\u7b49\uff09\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86GL-ML\u4e2d\u7684\u516c\u5e73\u6027\u6311\u6218\uff0c\u7814\u7a76\u4e86\u56fe\u5b66\u4e60\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u4ea4\u4e92\u673a\u5236\uff0c\u5e76\u63a2\u8ba8\u4e86\u56db\u79cd\u6539\u8fdb\u516c\u5e73\u6027\u7684\u6280\u672f\u3002", "result": "\u63ed\u793a\u4e86GL-ML\u4e2d\u516c\u5e73\u6027\u95ee\u9898\u7684\u6839\u6e90\u53ca\u5176\u5e7f\u6cdb\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u4e3aGL-ML\u516c\u5e73\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u521b\u65b0\u3002"}}
{"id": "2504.21463", "pdf": "https://arxiv.org/pdf/2504.21463", "abs": "https://arxiv.org/abs/2504.21463", "authors": ["Haowen Hou", "Zhiyi Huang", "Kaifeng Tan", "Rongchang Lu", "Fei Richard Yu"], "title": "RWKV-X: A Linear Complexity Hybrid Language Model", "categories": ["cs.CL"], "comment": "12 pages", "summary": "In this paper, we introduce \\textbf{RWKV-X}, a novel hybrid architecture that\ncombines the efficiency of RWKV for short-range modeling with a sparse\nattention mechanism designed to capture long-range context. Unlike previous\nhybrid approaches that rely on full attention layers and retain quadratic\ncomplexity, RWKV-X achieves linear-time complexity in training and\nconstant-time complexity in inference decoding. We demonstrate that RWKV-X,\nwhen continually pretrained on 64K-token sequences, achieves near-perfect\naccuracy on the 64K passkey retrieval benchmark. It consistently outperforms\nprior RWKV-7 models on long-context benchmarks, while maintaining strong\nperformance on short-context tasks. These results highlight RWKV-X as a\nscalable and efficient backbone for general-purpose language modeling, capable\nof decoding sequences up to 1 million tokens with stable speed and memory\nusage. To facilitate further research and analysis, we have made the\ncheckpoints and the associated code publicly accessible at:\nhttps://github.com/howard-hou/RWKV-X.", "AI": {"tldr": "RWKV-X\u662f\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u67b6\u6784\uff0c\u7ed3\u5408\u4e86RWKV\u7684\u77ed\u7a0b\u5efa\u6a21\u6548\u7387\u548c\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u6355\u6349\u957f\u7a0b\u4e0a\u4e0b\u6587\uff0c\u5177\u6709\u7ebf\u6027\u8bad\u7ec3\u65f6\u95f4\u548c\u6052\u5b9a\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6df7\u5408\u65b9\u6cd5\u56e0\u5168\u6ce8\u610f\u529b\u5c42\u5bfc\u81f4\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u957f\u7a0b\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u7ed3\u5408RWKV\u7684\u77ed\u7a0b\u5efa\u6a21\u4e0e\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u7ebf\u6027\u8bad\u7ec3\u548c\u6052\u5b9a\u63a8\u7406\u65f6\u95f4\u3002", "result": "\u572864K\u6807\u8bb0\u5e8f\u5217\u4e0a\u9884\u8bad\u7ec3\u540e\uff0cRWKV-X\u572864K passkey\u68c0\u7d22\u57fa\u51c6\u4e0a\u8868\u73b0\u63a5\u8fd1\u5b8c\u7f8e\uff0c\u4f18\u4e8eRWKV-7\u6a21\u578b\uff0c\u4e14\u652f\u6301\u767e\u4e07\u6807\u8bb0\u5e8f\u5217\u89e3\u7801\u3002", "conclusion": "RWKV-X\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u901a\u7528\u8bed\u8a00\u5efa\u6a21\u9aa8\u5e72\uff0c\u9002\u7528\u4e8e\u957f\u77ed\u4e0a\u4e0b\u6587\u4efb\u52a1\u3002"}}
{"id": "2504.21368", "pdf": "https://arxiv.org/pdf/2504.21368", "abs": "https://arxiv.org/abs/2504.21368", "authors": ["Pramook Khungurn", "Sukit Seripanitkarn", "Phonphrm Thawatdamrongkit", "Supasorn Suwajanakorn"], "title": "Revisiting Diffusion Autoencoder Training for Image Reconstruction Quality", "categories": ["cs.CV", "cs.AI"], "comment": "AI for Content Creation (AI4CC) Workshop at CVPR 2025", "summary": "Diffusion autoencoders (DAEs) are typically formulated as a noise prediction\nmodel and trained with a linear-$\\beta$ noise schedule that spends much of its\nsampling steps at high noise levels. Because high noise levels are associated\nwith recovering large-scale image structures and low noise levels with\nrecovering details, this configuration can result in low-quality and blurry\nimages. However, it should be possible to improve details while spending fewer\nsteps recovering structures because the latent code should already contain\nstructural information. Based on this insight, we propose a new DAE training\nmethod that improves the quality of reconstructed images. We divide training\ninto two phases. In the first phase, the DAE is trained as a vanilla\nautoencoder by always setting the noise level to the highest, forcing the\nencoder and decoder to populate the latent code with structural information. In\nthe second phase, we incorporate a noise schedule that spends more time in the\nlow-noise region, allowing the DAE to learn how to perfect the details. Our\nmethod results in images that have accurate high-level structures and low-level\ndetails while still preserving useful properties of the latent codes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6269\u6563\u81ea\u7f16\u7801\u5668\uff08DAE\uff09\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u8bad\u7ec3\u4f18\u5316\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edfDAE\u4f7f\u7528\u7ebf\u6027\u566a\u58f0\u8ba1\u5212\uff0c\u5bfc\u81f4\u56fe\u50cf\u6a21\u7cca\uff0c\u56e0\u9ad8\u566a\u58f0\u9636\u6bb5\u8fc7\u957f\u3002\u5229\u7528\u6f5c\u5728\u7f16\u7801\u5df2\u542b\u7ed3\u6784\u4fe1\u606f\u7684\u7279\u70b9\uff0c\u4f18\u5316\u7ec6\u8282\u91cd\u5efa\u3002", "method": "\u5206\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5\u5f3a\u5236\u6700\u9ad8\u566a\u58f0\u8bad\u7ec3\uff0c\u4f7f\u6f5c\u5728\u7f16\u7801\u5305\u542b\u7ed3\u6784\u4fe1\u606f\uff1b\u7b2c\u4e8c\u9636\u6bb5\u8c03\u6574\u566a\u58f0\u8ba1\u5212\uff0c\u4fa7\u91cd\u4f4e\u566a\u58f0\u4f18\u5316\u7ec6\u8282\u3002", "result": "\u751f\u6210\u56fe\u50cf\u5728\u7ed3\u6784\u548c\u7ec6\u8282\u4e0a\u5747\u66f4\u51c6\u786e\uff0c\u540c\u65f6\u4fdd\u7559\u6f5c\u5728\u7f16\u7801\u7684\u6709\u7528\u7279\u6027\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347DAE\u7684\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\uff0c\u5e73\u8861\u7ed3\u6784\u4e0e\u7ec6\u8282\u3002"}}
{"id": "2504.21030", "pdf": "https://arxiv.org/pdf/2504.21030", "abs": "https://arxiv.org/abs/2504.21030", "authors": ["Naveen Krishnan"], "title": "Advancing Multi-Agent Systems Through Model Context Protocol: Architecture, Implementation, and Applications", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Multi-agent systems represent a significant advancement in artificial\nintelligence, enabling complex problem-solving through coordinated specialized\nagents. However, these systems face fundamental challenges in context\nmanagement, coordination efficiency, and scalable operation. This paper\nintroduces a comprehensive framework for advancing multi-agent systems through\nModel Context Protocol (MCP), addressing these challenges through standardized\ncontext sharing and coordination mechanisms. We extend previous work on AI\nagent architectures by developing a unified theoretical foundation, advanced\ncontext management techniques, and scalable coordination patterns. Through\ndetailed implementation case studies across enterprise knowledge management,\ncollaborative research, and distributed problem-solving domains, we demonstrate\nsignificant performance improvements compared to traditional approaches. Our\nevaluation methodology provides a systematic assessment framework with\nbenchmark tasks and datasets specifically designed for multi-agent systems. We\nidentify current limitations, emerging research opportunities, and potential\ntransformative applications across industries. This work contributes to the\nevolution of more capable, collaborative, and context-aware artificial\nintelligence systems that can effectively address complex real-world\nchallenges.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u4e0a\u4e0b\u6587\u7ba1\u7406\u3001\u534f\u8c03\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u95ee\u9898\u89e3\u51b3\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u4e0a\u4e0b\u6587\u7ba1\u7406\u3001\u534f\u8c03\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u6807\u51c6\u5316\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86MCP\u6846\u67b6\uff0c\u5305\u62ec\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\u3001\u9ad8\u7ea7\u4e0a\u4e0b\u6587\u7ba1\u7406\u6280\u672f\u548c\u53ef\u6269\u5c55\u534f\u8c03\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\uff08\u5982\u4f01\u4e1a\u77e5\u8bc6\u7ba1\u7406\u3001\u534f\u4f5c\u7814\u7a76\u548c\u5206\u5e03\u5f0f\u95ee\u9898\u89e3\u51b3\uff09\u4e2d\uff0cMCP\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u66f4\u5f3a\u5927\u3001\u534f\u4f5c\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.21304", "pdf": "https://arxiv.org/pdf/2504.21304", "abs": "https://arxiv.org/abs/2504.21304", "authors": ["Nanxu Gong", "Xinyuan Wang", "Wangyang Ying", "Haoyue Bai", "Sixun Dong", "Haifeng Chen", "Yanjie Fu"], "title": "Unsupervised Feature Transformation via In-context Generation, Generator-critic LLM Agents, and Duet-play Teaming", "categories": ["cs.LG"], "comment": "Accepted to IJCAI 2025", "summary": "Feature transformation involves generating a new set of features from the\noriginal dataset to enhance the data's utility. In certain domains like\nmaterial performance screening, dimensionality is large and collecting labels\nis expensive and lengthy. It highly necessitates transforming feature spaces\nefficiently and without supervision to enhance data readiness and AI utility.\nHowever, existing methods fall short in efficient navigation of a vast space of\nfeature combinations, and are mostly designed for supervised settings. To fill\nthis gap, our unique perspective is to leverage a generator-critic duet-play\nteaming framework using LLM agents and in-context learning to derive\npseudo-supervision from unsupervised data. The framework consists of three\ninterconnected steps: (1) Critic agent diagnoses data to generate actionable\nadvice, (2) Generator agent produces tokenized feature transformations guided\nby the critic's advice, and (3) Iterative refinement ensures continuous\nimprovement through feedback between agents. The generator-critic framework can\nbe generalized to human-agent collaborative generation, by replacing the critic\nagent with human experts. Extensive experiments demonstrate that the proposed\nframework outperforms even supervised baselines in feature transformation\nefficiency, robustness, and practical applicability across diverse datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u4ee3\u7406\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u751f\u6210\u5668-\u6279\u8bc4\u8005\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u7279\u5f81\u8f6c\u6362\uff0c\u63d0\u5347\u6570\u636e\u51c6\u5907\u548cAI\u5b9e\u7528\u6027\u3002", "motivation": "\u5728\u6750\u6599\u6027\u80fd\u7b5b\u9009\u7b49\u9886\u57df\uff0c\u6570\u636e\u7ef4\u5ea6\u9ad8\u4e14\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u9700\u8981\u9ad8\u6548\u65e0\u76d1\u7763\u7684\u7279\u5f81\u8f6c\u6362\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u9ad8\u6548\u5904\u7406\u5927\u91cf\u7279\u5f81\u7ec4\u5408\u4e14\u591a\u4e3a\u76d1\u7763\u5f0f\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u751f\u6210\u5668-\u6279\u8bc4\u8005\u53cc\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u6b65\u6d41\u7a0b\uff1a\u6279\u8bc4\u8005\u8bca\u65ad\u6570\u636e\u751f\u6210\u5efa\u8bae\uff0c\u751f\u6210\u5668\u6839\u636e\u5efa\u8bae\u751f\u6210\u7279\u5f81\u8f6c\u6362\uff0c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u7279\u5f81\u8f6c\u6362\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u4e0a\u4f18\u4e8e\u76d1\u7763\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u65e0\u76d1\u7763\u7279\u5f81\u8f6c\u6362\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53ef\u6269\u5c55\u81f3\u4eba\u673a\u534f\u4f5c\u573a\u666f\u3002"}}
{"id": "2504.21474", "pdf": "https://arxiv.org/pdf/2504.21474", "abs": "https://arxiv.org/abs/2504.21474", "authors": ["Hadi Bayrami Asl Tekanlou", "Jafar Razmara", "Mahsa Sanaei", "Mostafa Rahgouy", "Hamed Babaei Giglou"], "title": "Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 4 figures, accepted to the LLMs4Subjects shared task at\n  SemEval2025", "summary": "This paper presents our system, Homa, for SemEval-2025 Task 5: Subject\nTagging, which focuses on automatically assigning subject labels to technical\nrecords from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage\nOntoAligner, a modular ontology alignment toolkit, to address this task by\nintegrating retrieval-augmented generation (RAG) techniques. Our approach\nformulates the subject tagging problem as an alignment task, where records are\nmatched to GND categories based on semantic similarity. We evaluate\nOntoAligner's adaptability for subject indexing and analyze its effectiveness\nin handling multilingual records. Experimental results demonstrate the\nstrengths and limitations of this method, highlighting the potential of\nalignment techniques for improving subject tagging in digital libraries.", "AI": {"tldr": "Homa\u7cfb\u7edf\u5229\u7528OntoAligner\u5de5\u5177\u5305\uff0c\u7ed3\u5408RAG\u6280\u672f\uff0c\u5c06\u4e3b\u9898\u6807\u6ce8\u95ee\u9898\u8f6c\u5316\u4e3a\u5bf9\u9f50\u4efb\u52a1\uff0c\u5339\u914d\u8bb0\u5f55\u4e0eGND\u5206\u7c7b\uff0c\u8bc4\u4f30\u5176\u5728\u591a\u8bed\u8a00\u8bb0\u5f55\u4e2d\u7684\u6548\u679c\u3002", "motivation": "\u89e3\u51b3SemEval-2025\u4efb\u52a15\u4e2d\u7684\u4e3b\u9898\u6807\u6ce8\u95ee\u9898\uff0c\u63d0\u5347\u6570\u5b57\u56fe\u4e66\u9986\u4e2d\u4e3b\u9898\u6807\u6ce8\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u3002", "method": "\u4f7f\u7528OntoAligner\u5de5\u5177\u5305\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u5c06\u4e3b\u9898\u6807\u6ce8\u95ee\u9898\u8f6c\u5316\u4e3a\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5bf9\u9f50\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u8bb0\u5f55\u4e2d\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u5bf9\u9f50\u6280\u672f\u5728\u6539\u8fdb\u6570\u5b57\u56fe\u4e66\u9986\u4e3b\u9898\u6807\u6ce8\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2504.21385", "pdf": "https://arxiv.org/pdf/2504.21385", "abs": "https://arxiv.org/abs/2504.21385", "authors": ["Shijun Zhou", "Yajing Liu", "Chunhui Hao", "Zhiyuan Liu", "Jiandong Tian"], "title": "IDDM: Bridging Synthetic-to-Real Domain Gap from Physics-Guided Diffusion for Real-world Image Dehazing", "categories": ["cs.CV"], "comment": null, "summary": "Due to the domain gap between real-world and synthetic hazy images, current\ndata-driven dehazing algorithms trained on synthetic datasets perform well on\nsynthetic data but struggle to generalize to real-world scenarios. To address\nthis challenge, we propose \\textbf{I}mage \\textbf{D}ehazing \\textbf{D}iffusion\n\\textbf{M}odels (IDDM), a novel diffusion process that incorporates the\natmospheric scattering model into noise diffusion. IDDM aims to use the gradual\nhaze formation process to help the denoising Unet robustly learn the\ndistribution of clear images from the conditional input hazy images. We design\na specialized training strategy centered around IDDM. Diffusion models are\nleveraged to bridge the domain gap from synthetic to real-world, while the\natmospheric scattering model provides physical guidance for haze formation.\nDuring the forward process, IDDM simultaneously introduces haze and noise into\nclear images, and then robustly separates them during the sampling process. By\ntraining with physics-guided information, IDDM shows the ability of domain\ngeneralization, and effectively restores the real-world hazy images despite\nbeing trained on synthetic datasets. Extensive experiments demonstrate the\neffectiveness of our method through both quantitative and qualitative\ncomparisons with state-of-the-art approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u53bb\u96fe\u65b9\u6cd5IDDM\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u6c14\u6563\u5c04\u6a21\u578b\u548c\u566a\u58f0\u6269\u6563\uff0c\u89e3\u51b3\u4e86\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u573a\u666f\u4e4b\u95f4\u7684\u57df\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u53bb\u96fe\u7b97\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u6865\u63a5\u57df\u5dee\u8ddd\u7684\u65b9\u6cd5\u3002", "method": "IDDM\u5229\u7528\u6269\u6563\u8fc7\u7a0b\u9010\u6b65\u5f15\u5165\u96fe\u548c\u566a\u58f0\uff0c\u518d\u901a\u8fc7\u53bb\u566aUnet\u5b66\u4e60\u6e05\u6670\u56fe\u50cf\u7684\u5206\u5e03\uff0c\u7ed3\u5408\u5927\u6c14\u6563\u5c04\u6a21\u578b\u63d0\u4f9b\u7269\u7406\u6307\u5bfc\u3002", "result": "IDDM\u5728\u5408\u6210\u6570\u636e\u8bad\u7ec3\u4e0b\uff0c\u80fd\u6709\u6548\u6062\u590d\u771f\u5b9e\u4e16\u754c\u7684\u96fe\u5316\u56fe\u50cf\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "IDDM\u901a\u8fc7\u7269\u7406\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u57df\u6cdb\u5316\uff0c\u4e3a\u771f\u5b9e\u573a\u666f\u53bb\u96fe\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.21032", "pdf": "https://arxiv.org/pdf/2504.21032", "abs": "https://arxiv.org/abs/2504.21032", "authors": ["Lior Limonad", "Fabiana Fournier", "Hadar Mulian", "George Manias", "Spiros Borotis", "Danai Kyrkou"], "title": "Selecting the Right LLM for eGov Explanations", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": "8 pages, 7 figures. ICEDEG 2025, Bern, Switzerland, June 2025", "summary": "The perceived quality of the explanations accompanying e-government services\nis key to gaining trust in these institutions, consequently amplifying further\nusage of these services. Recent advances in generative AI, and concretely in\nLarge Language Models (LLMs) allow the automation of such content\narticulations, eliciting explanations' interpretability and fidelity, and more\ngenerally, adapting content to various audiences. However, selecting the right\nLLM type for this has become a non-trivial task for e-government service\nproviders. In this work, we adapted a previously developed scale to assist with\nthis selection, providing a systematic approach for the comparative analysis of\nthe perceived quality of explanations generated by various LLMs. We further\ndemonstrated its applicability through the tax-return process, using it as an\nexemplar use case that could benefit from employing an LLM to generate\nexplanations about tax refund decisions. This was attained through a user study\nwith 128 survey respondents who were asked to rate different versions of\nLLM-generated explanations about tax refund decisions, providing a\nmethodological basis for selecting the most appropriate LLM. Recognizing the\npractical challenges of conducting such a survey, we also began exploring the\nautomation of this process by attempting to replicate human feedback using a\nselection of cutting-edge predictive techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\uff0c\u7528\u4e8e\u6bd4\u8f83\u5206\u6790\u4e0d\u540cLLM\u751f\u6210\u89e3\u91ca\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u7a0e\u52a1\u6848\u4f8b\u9a8c\u8bc1\u5176\u9002\u7528\u6027\u3002", "motivation": "\u7535\u5b50\u653f\u52a1\u670d\u52a1\u4e2d\u89e3\u91ca\u7684\u8d28\u91cf\u5bf9\u7528\u6237\u4fe1\u4efb\u548c\u4f7f\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9009\u62e9\u5408\u9002\u7684LLM\u751f\u6210\u89e3\u91ca\u662f\u4e00\u9879\u590d\u6742\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u5148\u524d\u5f00\u53d1\u7684\u91cf\u8868\uff0c\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff08128\u540d\u53d7\u8bbf\u8005\uff09\u8bc4\u4f30\u4e0d\u540cLLM\u751f\u6210\u89e3\u91ca\u7684\u8d28\u91cf\u3002", "result": "\u7814\u7a76\u4e3a\u9009\u62e9\u6700\u5408\u9002\u7684LLM\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u57fa\u7840\uff0c\u5e76\u5c1d\u8bd5\u81ea\u52a8\u5316\u7528\u6237\u53cd\u9988\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u7535\u5b50\u653f\u52a1\u670d\u52a1\u63d0\u4f9b\u5546\u9009\u62e9\u9002\u5408\u7684LLM\uff0c\u63d0\u5347\u89e3\u91ca\u8d28\u91cf\u4e0e\u7528\u6237\u4fe1\u4efb\u3002"}}
{"id": "2504.21314", "pdf": "https://arxiv.org/pdf/2504.21314", "abs": "https://arxiv.org/abs/2504.21314", "authors": ["Xunpeng Huang", "Yujin Han", "Difan Zou", "Yian Ma", "Tong Zhang"], "title": "Capturing Conditional Dependence via Auto-regressive Diffusion Models", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Diffusion models have demonstrated appealing performance in both image and\nvideo generation. However, many works discover that they struggle to capture\nimportant, high-level relationships that are present in the real world. For\nexample, they fail to learn physical laws from data, and even fail to\nunderstand that the objects in the world exist in a stable fashion. This is due\nto the fact that important conditional dependence structures are not adequately\ncaptured in the vanilla diffusion models. In this work, we initiate an in-depth\nstudy on strengthening the diffusion model to capture the conditional\ndependence structures in the data. In particular, we examine the efficacy of\nthe auto-regressive (AR) diffusion models for such purpose and develop the\nfirst theoretical results on the sampling error of AR diffusion models under\n(possibly) the mildest data assumption. Our theoretical findings indicate that,\ncompared with typical diffusion models, the AR variant produces samples with a\nreduced gap in approximating the data conditional distribution. On the other\nhand, the overall inference time of the AR-diffusion models is only moderately\nlarger than that for the vanilla diffusion models, making them still practical\nfor large scale applications. We also provide empirical results showing that\nwhen there is clear conditional dependence structure in the data, the AR\ndiffusion models captures such structure, whereas vanilla DDPM fails to do so.\nOn the other hand, when there is no obvious conditional dependence across\npatches of the data, AR diffusion does not outperform DDPM.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u96be\u4ee5\u6355\u6349\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u9ad8\u5c42\u6b21\u5173\u7cfb\u3002\u672c\u6587\u63d0\u51fa\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\uff08AR-diffusion\uff09\u4ee5\u66f4\u597d\u5730\u6355\u6349\u6570\u636e\u4e2d\u7684\u6761\u4ef6\u4f9d\u8d56\u7ed3\u6784\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7ed3\u679c\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u65e0\u6cd5\u6709\u6548\u5b66\u4e60\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u7269\u7406\u89c4\u5f8b\u548c\u7a33\u5b9a\u5bf9\u8c61\u5173\u7cfb\uff0c\u539f\u56e0\u662f\u5176\u672a\u80fd\u5145\u5206\u6355\u6349\u6570\u636e\u4e2d\u7684\u6761\u4ef6\u4f9d\u8d56\u7ed3\u6784\u3002", "method": "\u7814\u7a76\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\uff08AR-diffusion\uff09\u7684\u6709\u6548\u6027\uff0c\u5e76\u9996\u6b21\u5728\u6e29\u548c\u6570\u636e\u5047\u8bbe\u4e0b\u63d0\u51fa\u5176\u91c7\u6837\u8bef\u5dee\u7684\u7406\u8bba\u7ed3\u679c\u3002", "result": "\u7406\u8bba\u8868\u660e\uff0cAR-diffusion\u5728\u903c\u8fd1\u6570\u636e\u6761\u4ef6\u5206\u5e03\u65f6\u8bef\u5dee\u66f4\u5c0f\uff0c\u4e14\u63a8\u7406\u65f6\u95f4\u4ec5\u7565\u9ad8\u4e8e\u666e\u901a\u6269\u6563\u6a21\u578b\u3002\u5b9e\u8bc1\u663e\u793a\uff0cAR-diffusion\u80fd\u6355\u6349\u660e\u663e\u7684\u6761\u4ef6\u4f9d\u8d56\u7ed3\u6784\uff0c\u800c\u666e\u901a\u6269\u6563\u6a21\u578b\uff08DDPM\uff09\u5219\u65e0\u6cd5\u505a\u5230\u3002", "conclusion": "AR-diffusion\u5728\u6570\u636e\u5177\u6709\u6761\u4ef6\u4f9d\u8d56\u7ed3\u6784\u65f6\u8868\u73b0\u4f18\u4e8eDDPM\uff0c\u4f46\u5728\u65e0\u4f9d\u8d56\u7ed3\u6784\u65f6\u4e0eDDPM\u76f8\u5f53\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.21475", "pdf": "https://arxiv.org/pdf/2504.21475", "abs": "https://arxiv.org/abs/2504.21475", "authors": ["Serry Sibaee", "Samar Ahmed", "Abdullah Al Harbi", "Omer Nacar", "Adel Ammar", "Yasser Habashi", "Wadii Boulila"], "title": "Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the critical gap in Arabic natural language processing\nby developing an effective Arabic Reverse Dictionary (RD) system that enables\nusers to find words based on their descriptions or meanings. We present a novel\ntransformer-based approach with a semi-encoder neural network architecture\nfeaturing geometrically decreasing layers that achieves state-of-the-art\nresults for Arabic RD tasks. Our methodology incorporates a comprehensive\ndataset construction process and establishes formal quality standards for\nArabic lexicographic definitions. Experiments with various pre-trained models\ndemonstrate that Arabic-specific models significantly outperform general\nmultilingual embeddings, with ARBERTv2 achieving the best ranking score\n(0.0644). Additionally, we provide a formal abstraction of the reverse\ndictionary task that enhances theoretical understanding and develop a modular,\nextensible Python library (RDTL) with configurable training pipelines. Our\nanalysis of dataset quality reveals important insights for improving Arabic\ndefinition construction, leading to eight specific standards for building\nhigh-quality reverse dictionary resources. This work contributes significantly\nto Arabic computational linguistics and provides valuable tools for language\nlearning, academic writing, and professional communication in Arabic.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u963f\u62c9\u4f2f\u8bed\u53cd\u5411\u8bcd\u5178\u7cfb\u7edf\uff0c\u901a\u8fc7\u534a\u7f16\u7801\u5668\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u963f\u62c9\u4f2f\u8bed\u8bcd\u5178\u5b9a\u4e49\u7684\u8d28\u91cf\u6807\u51c6\u3002", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u5173\u952e\u7a7a\u767d\uff0c\u63d0\u4f9b\u57fa\u4e8e\u63cf\u8ff0\u6216\u542b\u4e49\u67e5\u627e\u5355\u8bcd\u7684\u5de5\u5177\u3002", "method": "\u91c7\u7528\u534a\u7f16\u7801\u5668\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u51e0\u4f55\u9012\u51cf\u5c42\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982ARBERTv2\uff09\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "ARBERTv2\u8868\u73b0\u6700\u4f73\uff08\u6392\u540d\u5f97\u52060.0644\uff09\uff0c\u5e76\u63d0\u51fa\u4e868\u9879\u6784\u5efa\u9ad8\u8d28\u91cf\u53cd\u5411\u8bcd\u5178\u8d44\u6e90\u7684\u6807\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u663e\u8457\u63a8\u52a8\u4e86\u963f\u62c9\u4f2f\u8bed\u8ba1\u7b97\u8bed\u8a00\u5b66\uff0c\u4e3a\u8bed\u8a00\u5b66\u4e60\u3001\u5b66\u672f\u5199\u4f5c\u548c\u4e13\u4e1a\u4ea4\u6d41\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2504.21387", "pdf": "https://arxiv.org/pdf/2504.21387", "abs": "https://arxiv.org/abs/2504.21387", "authors": ["Teodor Boyadzhiev", "Gabriele Lagani", "Luca Ciampi", "Giuseppe Amato", "Krassimira Ivanova"], "title": "Comparison of Different Deep Neural Network Models in the Cultural Heritage Domain", "categories": ["cs.CV"], "comment": "Accepted at the 10th International Euro-Mediterranean Conference\n  (EuroMed 2024)", "summary": "The integration of computer vision and deep learning is an essential part of\ndocumenting and preserving cultural heritage, as well as improving visitor\nexperiences. In recent years, two deep learning paradigms have been established\nin the field of computer vision: convolutional neural networks and transformer\narchitectures. The present study aims to make a comparative analysis of some\nrepresentatives of these two techniques of their ability to transfer knowledge\nfrom generic dataset, such as ImageNet, to cultural heritage specific tasks.\nThe results of testing examples of the architectures VGG, ResNet, DenseNet,\nVisual Transformer, Swin Transformer, and PoolFormer, showed that DenseNet is\nthe best in terms of efficiency-computability ratio.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548cTransformer\u67b6\u6784\u5728\u6587\u5316\u9057\u4ea7\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0DenseNet\u5728\u6548\u7387\u4e0e\u8ba1\u7b97\u6027\u65b9\u9762\u6700\u4f73\u3002", "motivation": "\u63a2\u8ba8\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u4e0e\u6e38\u5ba2\u4f53\u9a8c\u4e2d\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e24\u79cd\u4e3b\u6d41\u6280\u672f\u7684\u77e5\u8bc6\u8fc1\u79fb\u80fd\u529b\u3002", "method": "\u6d4b\u8bd5\u4e86VGG\u3001ResNet\u3001DenseNet\u3001Visual Transformer\u3001Swin Transformer\u548cPoolFormer\u7b49\u67b6\u6784\u5728\u6587\u5316\u9057\u4ea7\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "DenseNet\u5728\u6548\u7387\u4e0e\u8ba1\u7b97\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "DenseNet\u662f\u6587\u5316\u9057\u4ea7\u4efb\u52a1\u4e2d\u6700\u4f18\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u9009\u62e9\u3002"}}
{"id": "2504.21033", "pdf": "https://arxiv.org/pdf/2504.21033", "abs": "https://arxiv.org/abs/2504.21033", "authors": ["Majid Behravan", "Maryam Haghani", "Denis Gracanin"], "title": "Transcending Dimensions using Generative AI: Real-Time 3D Model Generation in Augmented Reality", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Traditional 3D modeling requires technical expertise, specialized software,\nand time-intensive processes, making it inaccessible for many users. Our\nresearch aims to lower these barriers by combining generative AI and augmented\nreality (AR) into a cohesive system that allows users to easily generate,\nmanipulate, and interact with 3D models in real time, directly within AR\nenvironments. Utilizing cutting-edge AI models like Shap-E, we address the\ncomplex challenges of transforming 2D images into 3D representations in AR\nenvironments. Key challenges such as object isolation, handling intricate\nbackgrounds, and achieving seamless user interaction are tackled through\nadvanced object detection methods, such as Mask R-CNN. Evaluation results from\n35 participants reveal an overall System Usability Scale (SUS) score of 69.64,\nwith participants who engaged with AR/VR technologies more frequently rating\nthe system significantly higher, at 80.71. This research is particularly\nrelevant for applications in gaming, education, and AR-based e-commerce,\noffering intuitive, model creation for users without specialized skills.", "AI": {"tldr": "\u7814\u7a76\u7ed3\u5408\u751f\u6210\u5f0fAI\u548cAR\u6280\u672f\uff0c\u7b80\u53163D\u5efa\u6a21\u6d41\u7a0b\uff0c\u4f7f\u975e\u4e13\u4e1a\u7528\u6237\u4e5f\u80fd\u8f7b\u677e\u751f\u6210\u548c\u64cd\u4f5c3D\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf3D\u5efa\u6a21\u6280\u672f\u95e8\u69db\u9ad8\u3001\u8017\u65f6\u957f\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7AI\u548cAR\u6280\u672f\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\u3002", "method": "\u5229\u7528Shap-E\u7b49AI\u6a21\u578b\u548cMask R-CNN\u7b49\u5bf9\u8c61\u68c0\u6d4b\u65b9\u6cd5\uff0c\u89e3\u51b32D\u56fe\u50cf\u8f6c3D\u6a21\u578b\u53caAR\u73af\u5883\u4e2d\u7684\u4ea4\u4e92\u95ee\u9898\u3002", "result": "35\u540d\u53c2\u4e0e\u8005\u7684\u7cfb\u7edf\u53ef\u7528\u6027\u8bc4\u5206\uff08SUS\uff09\u4e3a69.64\uff0c\u719f\u6089AR/VR\u6280\u672f\u7684\u7528\u6237\u8bc4\u5206\u66f4\u9ad8\uff0880.71\uff09\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u6e38\u620f\u3001\u6559\u80b2\u548cAR\u7535\u5546\u7b49\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u975e\u4e13\u4e1a\u7528\u6237\u63d0\u4f9b\u4e86\u76f4\u89c2\u76843D\u5efa\u6a21\u5de5\u5177\u3002"}}
{"id": "2504.21326", "pdf": "https://arxiv.org/pdf/2504.21326", "abs": "https://arxiv.org/abs/2504.21326", "authors": ["Junkyu Lee", "Tian Gao", "Elliot Nelson", "Miao Liu", "Debarun Bhattacharjya", "Songtao Lu"], "title": "Q-function Decomposition with Intervention Semantics with Factored Action Spaces", "categories": ["cs.LG", "cs.AI"], "comment": "AISTATS 2025", "summary": "Many practical reinforcement learning environments have a discrete factored\naction space that induces a large combinatorial set of actions, thereby posing\nsignificant challenges. Existing approaches leverage the regular structure of\nthe action space and resort to a linear decomposition of Q-functions, which\navoids enumerating all combinations of factored actions. In this paper, we\nconsider Q-functions defined over a lower dimensional projected subspace of the\noriginal action space, and study the condition for the unbiasedness of\ndecomposed Q-functions using causal effect estimation from the no unobserved\nconfounder setting in causal statistics. This leads to a general scheme which\nwe call action decomposed reinforcement learning that uses the projected\nQ-functions to approximate the Q-function in standard model-free reinforcement\nlearning algorithms. The proposed approach is shown to improve sample\ncomplexity in a model-based reinforcement learning setting. We demonstrate\nimprovements in sample efficiency compared to state-of-the-art baselines in\nonline continuous control environments and a real-world offline sepsis\ntreatment environment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u7edf\u8ba1\u7684\u52a8\u4f5c\u5206\u89e3\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6295\u5f71Q\u51fd\u6570\u964d\u4f4e\u52a8\u4f5c\u7a7a\u95f4\u7684\u7ef4\u5ea6\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u79bb\u6563\u7ec4\u5408\u52a8\u4f5c\u7a7a\u95f4\u5e26\u6765\u7684\u6311\u6218\uff0c\u907f\u514d\u679a\u4e3e\u6240\u6709\u52a8\u4f5c\u7ec4\u5408\u3002", "method": "\u5229\u7528\u56e0\u679c\u7edf\u8ba1\u4e2d\u7684\u65e0\u672a\u89c2\u6d4b\u6df7\u6742\u56e0\u5b50\u8bbe\u5b9a\uff0c\u5b9a\u4e49\u6295\u5f71\u5b50\u7a7a\u95f4\u4e0a\u7684Q\u51fd\u6570\uff0c\u63d0\u51fa\u52a8\u4f5c\u5206\u89e3\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5728\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u964d\u4f4e\u4e86\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u5728\u7ebf\u8fde\u7eed\u63a7\u5236\u73af\u5883\u548c\u79bb\u7ebf\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u52a8\u4f5c\u5206\u89e3\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u52a8\u4f5c\u7a7a\u95f4\u95ee\u9898\u3002"}}
{"id": "2504.21540", "pdf": "https://arxiv.org/pdf/2504.21540", "abs": "https://arxiv.org/abs/2504.21540", "authors": ["Adrian Benton", "Alexander Gutkin", "Christo Kirov", "Brian Roark"], "title": "Improving Informally Romanized Language Identification", "categories": ["cs.CL"], "comment": "16 pages, 14 tables, 4 figures", "summary": "The Latin script is often used to informally write languages with non-Latin\nnative scripts. In many cases (e.g., most languages in India), there is no\nconventional spelling of words in the Latin script, hence there will be high\nspelling variability in written text. Such romanization renders languages that\nare normally easily distinguished based on script highly confusable, such as\nHindi and Urdu. In this work, we increase language identification (LID)\naccuracy for romanized text by improving the methods used to synthesize\ntraining sets. We find that training on synthetic samples which incorporate\nnatural spelling variation yields higher LID system accuracy than including\navailable naturally occurring examples in the training set, or even training\nhigher capacity models. We demonstrate new state-of-the-art LID performance on\nromanized text from 20 Indic languages in the Bhasha-Abhijnaanam evaluation set\n(Madhani et al., 2023a), improving test F1 from the reported 74.7% (using a\npretrained neural model) to 85.4% using a linear classifier trained solely on\nsynthetic data and 88.2% when also training on available harvested text.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5408\u6210\u8bad\u7ec3\u6570\u636e\u6765\u63d0\u9ad8\u62c9\u4e01\u5316\u6587\u672c\u8bed\u8a00\u8bc6\u522b\uff08LID\uff09\u51c6\u786e\u6027\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e8620\u79cd\u5370\u5ea6\u8bed\u8a00\u7684\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u62c9\u4e01\u5316\u6587\u672c\u56e0\u62fc\u5199\u53d8\u5f02\u6027\u9ad8\u5bfc\u81f4\u8bed\u8a00\u96be\u4ee5\u533a\u5206\uff08\u5982\u5370\u5730\u8bed\u548c\u4e4c\u5c14\u90fd\u8bed\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u8bc6\u522b\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u5408\u6210\u5305\u542b\u81ea\u7136\u62fc\u5199\u53d8\u5f02\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u7ed3\u5408\u7ebf\u6027\u5206\u7c7b\u5668\u6216\u989d\u5916\u771f\u5b9e\u6587\u672c\u8bad\u7ec3\u3002", "result": "\u5728Bhasha-Abhijnaanam\u8bc4\u4f30\u96c6\u4e0a\uff0cF1\u5206\u6570\u4ece74.7%\u63d0\u5347\u81f385.4%\uff08\u4ec5\u5408\u6210\u6570\u636e\uff09\u548c88.2%\uff08\u7ed3\u5408\u771f\u5b9e\u6587\u672c\uff09\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u62c9\u4e01\u5316\u6587\u672c\u7684\u8bed\u8a00\u8bc6\u522b\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u9ad8\u5bb9\u91cf\u6a21\u578b\u3002"}}
{"id": "2504.21403", "pdf": "https://arxiv.org/pdf/2504.21403", "abs": "https://arxiv.org/abs/2504.21403", "authors": ["Yumeng Shi", "Quanyu Long", "Wenya Wang"], "title": "Static or Dynamic: Towards Query-Adaptive Token Selection for Video Question Answering", "categories": ["cs.CV"], "comment": null, "summary": "Video question answering benefits from the rich information available in\nvideos, enabling a wide range of applications. However, the large volume of\ntokens generated from longer videos presents significant challenges to memory\nefficiency and model performance. To alleviate this issue, existing works\npropose to compress video inputs, but usually overlooking the varying\nimportance of static and dynamic information across different queries, leading\nto inefficient token usage within limited budgets. To tackle this, we propose a\nnovel token selection strategy, EXPLORE-THEN-SELECT, that adaptively adjust\nstatic and dynamic information needed based on question requirements. Our\nframework first explores different token allocations between static frames,\nwhich preserve spatial details, and dynamic frames, which capture temporal\nchanges. Next, it employs a query-aware attention-based metric to select the\noptimal token combination without model updates. Our proposed framework is\nplug-and-play that can be seamlessly integrated within diverse video-language\nmodels. Extensive experiments show that our method achieves significant\nperformance improvements (up to 5.8%) among various video question answering\nbenchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEXPLORE-THEN-SELECT\u7684\u4ee4\u724c\u9009\u62e9\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u9759\u6001\u548c\u52a8\u6001\u4fe1\u606f\u7684\u9700\u6c42\uff0c\u4f18\u5316\u89c6\u9891\u95ee\u7b54\u4e2d\u7684\u4ee4\u724c\u4f7f\u7528\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u95ee\u7b54\u4e2d\u56e0\u957f\u89c6\u9891\u751f\u6210\u5927\u91cf\u4ee4\u724c\u5bfc\u81f4\u7684\u5185\u5b58\u6548\u7387\u4f4e\u4e0b\u548c\u6a21\u578b\u6027\u80fd\u95ee\u9898\u3002", "method": "\u63d0\u51faEXPLORE-THEN-SELECT\u7b56\u7565\uff0c\u5148\u63a2\u7d22\u9759\u6001\u548c\u52a8\u6001\u4ee4\u724c\u5206\u914d\uff0c\u518d\u57fa\u4e8e\u67e5\u8be2\u611f\u77e5\u7684\u6ce8\u610f\u529b\u6307\u6807\u9009\u62e9\u6700\u4f18\u7ec4\u5408\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe5.8%\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e0\u9700\u6a21\u578b\u66f4\u65b0\u5373\u53ef\u9ad8\u6548\u4f18\u5316\u4ee4\u724c\u4f7f\u7528\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2504.21034", "pdf": "https://arxiv.org/pdf/2504.21034", "abs": "https://arxiv.org/abs/2504.21034", "authors": ["Georgios Syros", "Anshuman Suri", "Cristina Nita-Rotaru", "Alina Oprea"], "title": "SAGA: A Security Architecture for Governing AI Agentic Systems", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM)-based agents increasingly interact, collaborate,\nand delegate tasks to one another autonomously with minimal human interaction.\nIndustry guidelines for agentic system governance emphasize the need for users\nto maintain comprehensive control over their agents, mitigating potential\ndamage from malicious agents. Several proposed agentic system designs address\nagent identity, authorization, and delegation, but remain purely theoretical,\nwithout concrete implementation and evaluation. Most importantly, they do not\nprovide user-controlled agent management. To address this gap, we propose SAGA,\na Security Architecture for Governing Agentic systems, that offers user\noversight over their agents' lifecycle. In our design, users register their\nagents with a central entity, the Provider, that maintains agents contact\ninformation, user-defined access control policies, and helps agents enforce\nthese policies on inter-agent communication. We introduce a cryptographic\nmechanism for deriving access control tokens, that offers fine-grained control\nover an agent's interaction with other agents, balancing security and\nperformance consideration. We evaluate SAGA on several agentic tasks, using\nagents in different geolocations, and multiple on-device and cloud LLMs,\ndemonstrating minimal performance overhead with no impact on underlying task\nutility in a wide range of conditions. Our architecture enables secure and\ntrustworthy deployment of autonomous agents, accelerating the responsible\nadoption of this technology in sensitive environments.", "AI": {"tldr": "SAGA\u662f\u4e00\u79cd\u5b89\u5168\u67b6\u6784\uff0c\u7528\u4e8e\u7ba1\u7406\u81ea\u4e3b\u4ee3\u7406\u7cfb\u7edf\uff0c\u63d0\u4f9b\u7528\u6237\u5bf9\u4ee3\u7406\u751f\u547d\u5468\u671f\u7684\u76d1\u7763\uff0c\u786e\u4fdd\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3b\u4ee3\u7406\u7cfb\u7edf\u7f3a\u4e4f\u7528\u6237\u63a7\u5236\u7684\u7ba1\u7406\u673a\u5236\uff0c\u5b58\u5728\u6f5c\u5728\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u4e00\u79cd\u5b9e\u9645\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSAGA\u67b6\u6784\uff0c\u901a\u8fc7\u4e2d\u592e\u5b9e\u4f53\uff08Provider\uff09\u7ba1\u7406\u4ee3\u7406\u6ce8\u518c\u3001\u8bbf\u95ee\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u52a0\u5bc6\u673a\u5236\u751f\u6210\u7ec6\u7c92\u5ea6\u8bbf\u95ee\u63a7\u5236\u4ee4\u724c\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u548c\u73af\u5883\u4e0b\u8bc4\u4f30SAGA\uff0c\u663e\u793a\u5176\u6027\u80fd\u5f00\u9500\u6781\u5c0f\u4e14\u4e0d\u5f71\u54cd\u4efb\u52a1\u6548\u7528\u3002", "conclusion": "SAGA\u4e3a\u81ea\u4e3b\u4ee3\u7406\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u654f\u611f\u73af\u5883\u3002"}}
{"id": "2504.21327", "pdf": "https://arxiv.org/pdf/2504.21327", "abs": "https://arxiv.org/abs/2504.21327", "authors": ["Mohammad Vahid Jamali", "Hamid Saber", "Jung Hyun Bae"], "title": "A Generalized Meta Federated Learning Framework with Theoretical Convergence Guarantees", "categories": ["cs.LG"], "comment": null, "summary": "Meta federated learning (FL) is a personalized variant of FL, where multiple\nagents collaborate on training an initial shared model without exchanging raw\ndata samples. The initial model should be trained in a way that current or new\nagents can easily adapt it to their local datasets after one or a few\nfine-tuning steps, thus improving the model personalization. Conventional meta\nFL approaches minimize the average loss of agents on the local models obtained\nafter one step of fine-tuning. In practice, agents may need to apply several\nfine-tuning steps to adapt the global model to their local data, especially\nunder highly heterogeneous data distributions across agents. To this end, we\npresent a generalized framework for the meta FL by minimizing the average loss\nof agents on their local model after any arbitrary number $\\nu$ of fine-tuning\nsteps. For this generalized framework, we present a variant of the well-known\nfederated averaging (FedAvg) algorithm and conduct a comprehensive theoretical\nconvergence analysis to characterize the convergence speed as well as behavior\nof the meta loss functions in both the exact and approximated cases. Our\nexperiments on real-world datasets demonstrate superior accuracy and faster\nconvergence for the proposed scheme compared to conventional approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u7684\u5143\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u4ee3\u7406\u5728\u4efb\u610f\u6570\u91cf\u5fae\u8c03\u6b65\u9aa4\u540e\u7684\u672c\u5730\u6a21\u578b\u635f\u5931\uff0c\u6539\u8fdb\u4e86\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5143\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u4ec5\u4f18\u5316\u4e00\u6b65\u5fae\u8c03\u540e\u7684\u635f\u5931\uff0c\u4f46\u5728\u6570\u636e\u5206\u5e03\u9ad8\u5ea6\u5f02\u6784\u65f6\uff0c\u4ee3\u7406\u53ef\u80fd\u9700\u8981\u591a\u6b65\u5fae\u8c03\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u901a\u7528\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u5e7f\u4e49\u6846\u67b6\uff0c\u5141\u8bb8\u4efb\u610f\u6570\u91cf\u7684\u5fae\u8c03\u6b65\u9aa4\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6539\u8fdb\u7684FedAvg\u7b97\u6cd5\uff0c\u8fdb\u884c\u4e86\u7406\u8bba\u6536\u655b\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u5e7f\u4e49\u6846\u67b6\u548c\u7b97\u6cd5\u5728\u5f02\u6784\u6570\u636e\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u4e2a\u6027\u5316\u6548\u679c\u3002"}}
{"id": "2504.21547", "pdf": "https://arxiv.org/pdf/2504.21547", "abs": "https://arxiv.org/abs/2504.21547", "authors": ["Aleksei Dorkin", "Kairit Sirts"], "title": "TartuNLP at SemEval-2025 Task 5: Subject Tagging as Two-Stage Information Retrieval", "categories": ["cs.CL"], "comment": "To appear in the Proceedings of the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025)", "summary": "We present our submission to the Task 5 of SemEval-2025 that aims to aid\nlibrarians in assigning subject tags to the library records by producing a list\nof likely relevant tags for a given document. We frame the task as an\ninformation retrieval problem, where the document content is used to retrieve\nsubject tags from a large subject taxonomy. We leverage two types of encoder\nmodels to build a two-stage information retrieval system -- a bi-encoder for\ncoarse-grained candidate extraction at the first stage, and a cross-encoder for\nfine-grained re-ranking at the second stage. This approach proved effective,\ndemonstrating significant improvements in recall compared to single-stage\nmethods and showing competitive results according to qualitative evaluation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e3a\u56fe\u4e66\u9986\u8bb0\u5f55\u5206\u914d\u4e3b\u9898\u6807\u7b7e\uff0c\u7ed3\u5408\u4e86\u53cc\u7f16\u7801\u5668\u548c\u4ea4\u53c9\u7f16\u7801\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53ec\u56de\u7387\u3002", "motivation": "\u5e2e\u52a9\u56fe\u4e66\u9986\u5458\u4e3a\u56fe\u4e66\u9986\u8bb0\u5f55\u5206\u914d\u4e3b\u9898\u6807\u7b7e\uff0c\u901a\u8fc7\u6587\u6863\u5185\u5bb9\u4ece\u5927\u578b\u4e3b\u9898\u5206\u7c7b\u4e2d\u68c0\u7d22\u76f8\u5173\u6807\u7b7e\u3002", "method": "\u5c06\u4efb\u52a1\u89c6\u4e3a\u4fe1\u606f\u68c0\u7d22\u95ee\u9898\uff0c\u4f7f\u7528\u53cc\u7f16\u7801\u5668\u8fdb\u884c\u7c97\u7c92\u5ea6\u5019\u9009\u63d0\u53d6\uff0c\u4ea4\u53c9\u7f16\u7801\u5668\u8fdb\u884c\u7ec6\u7c92\u5ea6\u91cd\u6392\u5e8f\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u53ec\u56de\u7387\uff0c\u5e76\u5728\u5b9a\u6027\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "\u4e24\u9636\u6bb5\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u5728\u4e3b\u9898\u6807\u7b7e\u5206\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u7ade\u4e89\u529b\u3002"}}
{"id": "2504.21414", "pdf": "https://arxiv.org/pdf/2504.21414", "abs": "https://arxiv.org/abs/2504.21414", "authors": ["Qi Fan", "Kaiqi Liu", "Nian Liu", "Hisham Cholakkal", "Rao Muhammad Anwer", "Wenbin Li", "Yang Gao"], "title": "Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining", "categories": ["cs.CV"], "comment": null, "summary": "Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel\nclasses in new domains, which is often challenging due to the diverse\ncharacteristics of target domains and the limited availability of support data.\nMost CD-FSS methods redesign and retrain in-domain FSS models using various\ndomain-generalization techniques, which are effective but costly to train. To\naddress these issues, we propose adapting informative model structures of the\nwell-trained FSS model for target domains by learning domain characteristics\nfrom few-shot labeled support samples during inference, thereby eliminating the\nneed for retraining. Specifically, we first adaptively identify domain-specific\nmodel structures by measuring parameter importance using a novel structure\nFisher score in a data-dependent manner. Then, we progressively train the\nselected informative model structures with hierarchically constructed training\nsamples, progressing from fewer to more support shots. The resulting\nInformative Structure Adaptation (ISA) method effectively addresses domain\nshifts and equips existing well-trained in-domain FSS models with flexible\nadaptation capabilities for new domains, eliminating the need to redesign or\nretrain CD-FSS models on base data. Extensive experiments validate the\neffectiveness of our method, demonstrating superior performance across multiple\nCD-FSS benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u65b9\u6cd5\uff08ISA\uff09\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u6a21\u578b\u7ed3\u6784\u6765\u89e3\u51b3\u8de8\u57df\u5c0f\u6837\u672c\u5206\u5272\u95ee\u9898\u3002", "motivation": "\u8de8\u57df\u5c0f\u6837\u672c\u5206\u5272\uff08CD-FSS\uff09\u9762\u4e34\u76ee\u6807\u57df\u591a\u6837\u6027\u548c\u652f\u6301\u6570\u636e\u6709\u9650\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u6210\u672c\u9ad8\u3002", "method": "\u901a\u8fc7\u7ed3\u6784Fisher\u8bc4\u5206\u81ea\u9002\u5e94\u8bc6\u522b\u57df\u7279\u5b9a\u6a21\u578b\u7ed3\u6784\uff0c\u5e76\u5206\u5c42\u8bad\u7ec3\u9009\u5b9a\u7684\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eISA\u65b9\u6cd5\u5728\u591a\u4e2aCD-FSS\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ISA\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u57df\u504f\u79fb\u95ee\u9898\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u91cd\u65b0\u8bbe\u8ba1\u6a21\u578b\u3002"}}
{"id": "2504.21036", "pdf": "https://arxiv.org/pdf/2504.21036", "abs": "https://arxiv.org/abs/2504.21036", "authors": ["Hao Du", "Shang Liu", "Yang Cao"], "title": "Can Differentially Private Fine-tuning LLMs Protect Against Privacy Attacks?", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Fine-tuning large language models (LLMs) has become an essential strategy for\nadapting them to specialized tasks; however, this process introduces\nsignificant privacy challenges, as sensitive training data may be inadvertently\nmemorized and exposed. Although differential privacy (DP) offers strong\ntheoretical guarantees against such leakage, its empirical privacy\neffectiveness on LLMs remains unclear, especially under different fine-tuning\nmethods. In this paper, we systematically investigate the impact of DP across\nfine-tuning methods and privacy budgets, using both data extraction and\nmembership inference attacks to assess empirical privacy risks. Our main\nfindings are as follows: (1) Differential privacy reduces model utility, but\nits impact varies significantly across different fine-tuning methods. (2)\nWithout DP, the privacy risks of models fine-tuned with different approaches\ndiffer considerably. (3) When DP is applied, even a relatively high privacy\nbudget can substantially lower privacy risk. (4) The privacy-utility trade-off\nunder DP training differs greatly among fine-tuning methods, with some methods\nbeing unsuitable for DP due to severe utility degradation. Our results provide\npractical guidance for privacy-conscious deployment of LLMs and pave the way\nfor future research on optimizing the privacy-utility trade-off in fine-tuning\nmethodologies.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5e94\u7528\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u5bf9\u9690\u79c1\u98ce\u9669\u548c\u6a21\u578b\u6548\u7528\u7684\u5f71\u54cd\uff0c\u53d1\u73b0DP\u80fd\u663e\u8457\u964d\u4f4e\u9690\u79c1\u98ce\u9669\uff0c\u4f46\u5bf9\u4e0d\u540c\u5fae\u8c03\u65b9\u6cd5\u7684\u6548\u7528\u5f71\u54cd\u5dee\u5f02\u8f83\u5927\u3002", "motivation": "\u5fae\u8c03LLM\u65f6\u53ef\u80fd\u6cc4\u9732\u654f\u611f\u6570\u636e\uff0c\u800cDP\u7684\u7406\u8bba\u9690\u79c1\u4fdd\u62a4\u6548\u679c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c1a\u4e0d\u660e\u786e\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u5fae\u8c03\u65b9\u6cd5\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u6570\u636e\u63d0\u53d6\u548c\u6210\u5458\u63a8\u7406\u653b\u51fb\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u5fae\u8c03\u65b9\u6cd5\u548c\u9690\u79c1\u9884\u7b97\u4e0bDP\u7684\u9690\u79c1\u4fdd\u62a4\u6548\u679c\u3002", "result": "1. DP\u964d\u4f4e\u6a21\u578b\u6548\u7528\uff0c\u4f46\u5f71\u54cd\u56e0\u5fae\u8c03\u65b9\u6cd5\u800c\u5f02\uff1b2. \u65e0DP\u65f6\uff0c\u4e0d\u540c\u5fae\u8c03\u65b9\u6cd5\u7684\u9690\u79c1\u98ce\u9669\u5dee\u5f02\u663e\u8457\uff1b3. \u9ad8\u9690\u79c1\u9884\u7b97\u4e0bDP\u4ecd\u80fd\u663e\u8457\u964d\u4f4e\u98ce\u9669\uff1b4. \u4e0d\u540c\u5fae\u8c03\u65b9\u6cd5\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\u5dee\u5f02\u5927\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9690\u79c1\u654f\u611f\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u4e3a\u4f18\u5316\u5fae\u8c03\u65b9\u6cd5\u4e2d\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\u5f00\u8f9f\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2504.21328", "pdf": "https://arxiv.org/pdf/2504.21328", "abs": "https://arxiv.org/abs/2504.21328", "authors": ["Yao-Hsuan Tsai", "Hsiao-Tung Juan", "Pao-Hsiung Chiu", "Chao-An Lin"], "title": "Multi-level datasets training method in Physics-Informed Neural Networks", "categories": ["cs.LG", "cs.CE", "physics.flu-dyn"], "comment": "33 pages, 12figures", "summary": "Physics-Informed Neural Networks have emerged as a promising methodology for\nsolving PDEs, gaining significant attention in computer science and various\nphysics-related fields. Despite being demonstrated the ability to incorporate\nthe physics of laws for versatile applications, PINNs still struggle with the\nchallenging problems which are stiff to be solved and/or have high-frequency\ncomponents in the solutions, resulting in accuracy and convergence issues. It\nmay not only increase computational costs, but also lead to accuracy loss or\nsolution divergence. In this study, an alternative approach is proposed to\nmitigate the above-mentioned problems. Inspired by the multi-grid method in CFD\ncommunity, the underlying idea of the current approach is to efficiently remove\ndifferent frequency errors via training with different levels of training\nsamples, resulting in a simpler way to improve the training accuracy without\nspending time in fine-tuning of neural network structures, loss weights as well\nas hyperparameters. To demonstrate the efficacy of current approach, we first\ninvestigate canonical 1D ODE with high-frequency component and 2D\nconvection-diffusion equation with V-cycle training strategy. Finally, the\ncurrent method is employed for the classical benchmark problem of steady\nLid-driven cavity flows at different Reynolds numbers, to investigate the\napplicability and efficacy for the problem involved multiple modes of high and\nlow frequency. By virtue of various training sequence modes, improvement\nthrough predictions lead to 30% to 60% accuracy improvement. We also\ninvestigate the synergies between current method and transfer learning\ntechniques for more challenging problems (i.e., higher Re). From the present\nresults, it also revealed that the current framework can produce good\npredictions even for the case of Re=5000, demonstrating the ability to solve\ncomplex high-frequency PDEs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u7f51\u683c\u65b9\u6cd5\u7684\u6539\u8fdbPINNs\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u9891\u7387\u548c\u591a\u5c3a\u5ea6PDE\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7cbe\u5ea6\u548c\u6536\u655b\u6027\u3002", "motivation": "\u5c3d\u7ba1PINNs\u5728\u89e3\u51b3PDE\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u5904\u7406\u9ad8\u9891\u7387\u6216\u521a\u6027\u95ee\u9898\u65f6\u5b58\u5728\u7cbe\u5ea6\u548c\u6536\u655b\u6027\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u7f51\u683c\u65b9\u6cd5\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u591a\u7f51\u683c\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u4e0d\u540c\u7ea7\u522b\u7684\u8bad\u7ec3\u6837\u672c\u9010\u6b65\u6d88\u9664\u9891\u7387\u8bef\u5dee\uff0c\u907f\u514d\u4e86\u5bf9\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u548c\u8d85\u53c2\u6570\u7684\u7e41\u7410\u8c03\u6574\u3002", "result": "\u57281D\u548c2D\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7cbe\u5ea6\u63d0\u5347\u4e8630%-60%\uff0c\u5e76\u5728\u9ad8\u96f7\u8bfa\u6570\uff08Re=5000\uff09\u7684\u590d\u6742\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86PINNs\u5728\u9ad8\u9891\u7387\u548c\u591a\u5c3a\u5ea6PDE\u95ee\u9898\u4e2d\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u4e0e\u8fc1\u79fb\u5b66\u4e60\u7ed3\u5408\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.21553", "pdf": "https://arxiv.org/pdf/2504.21553", "abs": "https://arxiv.org/abs/2504.21553", "authors": ["Lucas Maisonnave", "Cyril Moineau", "Olivier Bichler", "Fabrice Rastello"], "title": "Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious natural language processing tasks. However, their size presents\nsignificant challenges for deployment and inference. This paper investigates\nthe quantization of LLMs, focusing on the LLaMA architecture and its\nderivatives. We challenge existing assumptions about activation outliers in\nLLMs and propose a novel mixed-precision quantization approach tailored for\nLLaMA-like models. Our method leverages the observation that activation spikes\nin LLaMA architectures are predominantly concentrated in specific projection\nlayers. By applying higher precision (FP16 or FP8) to these layers while\nquantizing the rest of the model to lower bit-widths, we achieve superior\nperformance compared to existing quantization techniques. Experimental results\non LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in\nperplexity and zero-shot accuracy, particularly for 8-bit per-tensor\nquantization. Our approach outperforms general-purpose methods designed to\nhandle outliers across all architecture types, highlighting the benefits of\narchitecture-specific quantization strategies. This research contributes to the\nongoing efforts to make LLMs more efficient and deployable, potentially\nenabling their use in resource-constrained environments. Our findings emphasize\nthe importance of considering model-specific characteristics in developing\neffective quantization pipelines for state-of-the-art language models by\nidentifying and targeting a small number of projections that concentrate\nactivation spikes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLaMA\u67b6\u6784\u53ca\u5176\u884d\u751f\u6a21\u578b\u4e2d\u7684\u91cf\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6fc0\u6d3b\u5f02\u5e38\u503c\u7684\u65b0\u578b\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u89c4\u6a21\u7ed9\u90e8\u7f72\u548c\u63a8\u7406\u5e26\u6765\u4e86\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\uff0c\u5bf9LLaMA\u67b6\u6784\u4e2d\u7279\u5b9a\u6295\u5f71\u5c42\u4f7f\u7528\u66f4\u9ad8\u7cbe\u5ea6\uff08FP16\u6216FP8\uff09\uff0c\u5176\u4f59\u90e8\u5206\u91cf\u5316\u5230\u66f4\u4f4e\u6bd4\u7279\u5bbd\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728LLaMA2\u3001LLaMA3\u548cMistral\u6a21\u578b\u4e0a\u663e\u8457\u964d\u4f4e\u4e86\u56f0\u60d1\u5ea6\u5e76\u63d0\u5347\u4e86\u96f6\u6837\u672c\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6a21\u578b\u7279\u5b9a\u7279\u5f81\u5728\u91cf\u5316\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684LLMs\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21423", "pdf": "https://arxiv.org/pdf/2504.21423", "abs": "https://arxiv.org/abs/2504.21423", "authors": ["Weicai Yan", "Wang Lin", "Zirun Guo", "Ye Wang", "Fangming Feng", "Xiaoda Yang", "Zehan Wang", "Tao Jin"], "title": "Diff-Prompt: Diffusion-Driven Prompt Generator with Mask Supervision", "categories": ["cs.CV"], "comment": "Accepted at ICLR 2025", "summary": "Prompt learning has demonstrated promising results in fine-tuning pre-trained\nmultimodal models. However, the performance improvement is limited when applied\nto more complex and fine-grained tasks. The reason is that most existing\nmethods directly optimize the parameters involved in the prompt generation\nprocess through loss backpropagation, which constrains the richness and\nspecificity of the prompt representations. In this paper, we propose\nDiffusion-Driven Prompt Generator (Diff-Prompt), aiming to use the diffusion\nmodel to generate rich and fine-grained prompt information for complex\ndownstream tasks. Specifically, our approach consists of three stages. In the\nfirst stage, we train a Mask-VAE to compress the masks into latent space. In\nthe second stage, we leverage an improved Diffusion Transformer (DiT) to train\na prompt generator in the latent space, using the masks for supervision. In the\nthird stage, we align the denoising process of the prompt generator with the\npre-trained model in the semantic space, and use the generated prompts to\nfine-tune the model. We conduct experiments on a complex pixel-level downstream\ntask, referring expression comprehension, and compare our method with various\nparameter-efficient fine-tuning approaches. Diff-Prompt achieves a maximum\nimprovement of 8.87 in R@1 and 14.05 in R@5 compared to the foundation model\nand also outperforms other state-of-the-art methods across multiple metrics.\nThe experimental results validate the effectiveness of our approach and\nhighlight the potential of using generative models for prompt generation. Code\nis available at https://github.com/Kelvin-ywc/diff-prompt.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDiff-Prompt\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u4e30\u5bcc\u4e14\u7ec6\u7c92\u5ea6\u7684\u63d0\u793a\u4fe1\u606f\uff0c\u4ee5\u63d0\u5347\u590d\u6742\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u7ec6\u7c92\u5ea6\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u5347\u6709\u9650\uff0c\u56e0\u5176\u76f4\u63a5\u4f18\u5316\u63d0\u793a\u751f\u6210\u53c2\u6570\uff0c\u9650\u5236\u4e86\u63d0\u793a\u8868\u793a\u7684\u4e30\u5bcc\u6027\u548c\u7279\u5f02\u6027\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e09\u9636\u6bb5\uff1a1) \u8bad\u7ec3Mask-VAE\u538b\u7f29\u63a9\u7801\u5230\u6f5c\u7a7a\u95f4\uff1b2) \u7528\u6539\u8fdb\u7684DiT\u8bad\u7ec3\u6f5c\u7a7a\u95f4\u63d0\u793a\u751f\u6210\u5668\uff1b3) \u5728\u8bed\u4e49\u7a7a\u95f4\u5bf9\u9f50\u751f\u6210\u5668\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u7528\u751f\u6210\u63d0\u793a\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5728\u5f15\u7528\u8868\u8fbe\u5f0f\u7406\u89e3\u4efb\u52a1\u4e2d\uff0cDiff-Prompt\u5728R@1\u548cR@5\u4e0a\u5206\u522b\u63d0\u53478.87\u548c14.05\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u751f\u6210\u6a21\u578b\u5728\u63d0\u793a\u751f\u6210\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.21037", "pdf": "https://arxiv.org/pdf/2504.21037", "abs": "https://arxiv.org/abs/2504.21037", "authors": ["Farnaz Soltaniani", "Mohammad Ghafari", "Mohammed Sayagh"], "title": "Security Bug Report Prediction Within and Across Projects: A Comparative Study of BERT and Random Forest", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Early detection of security bug reports (SBRs) is crucial for preventing\nvulnerabilities and ensuring system reliability. While machine learning models\nhave been developed for SBR prediction, their predictive performance still has\nroom for improvement. In this study, we conduct a comprehensive comparison\nbetween BERT and Random Forest (RF), a competitive baseline for predicting\nSBRs. The results show that RF outperforms BERT with a 34% higher average\nG-measure for within-project predictions. Adding only SBRs from various\nprojects improves both models' average performance. However, including both\nsecurity and nonsecurity bug reports significantly reduces RF's average\nperformance to 46%, while boosts BERT to its best average performance of 66%,\nsurpassing RF. In cross-project SBR prediction, BERT achieves a remarkable 62%\nG-measure, which is substantially higher than RF.", "AI": {"tldr": "\u6bd4\u8f83BERT\u548c\u968f\u673a\u68ee\u6797\uff08RF\uff09\u5728\u5b89\u5168\u6f0f\u6d1e\u62a5\u544a\uff08SBR\uff09\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0RF\u5728\u9879\u76ee\u5185\u9884\u6d4b\u8868\u73b0\u66f4\u597d\uff0c\u800cBERT\u5728\u8de8\u9879\u76ee\u9884\u6d4b\u4e2d\u663e\u8457\u4f18\u4e8eRF\u3002", "motivation": "\u65e9\u671f\u68c0\u6d4b\u5b89\u5168\u6f0f\u6d1e\u62a5\u544a\u5bf9\u7cfb\u7edf\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u5bf9BERT\u548c\u968f\u673a\u68ee\u6797\uff08RF\uff09\u8fdb\u884c\u5168\u9762\u6bd4\u8f83\uff0c\u8bc4\u4f30\u5176\u5728SBR\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\u3002", "result": "RF\u5728\u9879\u76ee\u5185\u9884\u6d4b\u4e2d\u5e73\u5747G-measure\u6bd4BERT\u9ad834%\uff0c\u800cBERT\u5728\u8de8\u9879\u76ee\u9884\u6d4b\u4e2d\u4ee562%\u7684G-measure\u663e\u8457\u4f18\u4e8eRF\u3002", "conclusion": "BERT\u5728\u8de8\u9879\u76eeSBR\u9884\u6d4b\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u800cRF\u5728\u9879\u76ee\u5185\u9884\u6d4b\u4e2d\u66f4\u5177\u4f18\u52bf\u3002"}}
{"id": "2504.21353", "pdf": "https://arxiv.org/pdf/2504.21353", "abs": "https://arxiv.org/abs/2504.21353", "authors": ["Vinti Nayar", "Kanica Sachdev", "Brejesh Lall"], "title": "Generative QoE Modeling: A Lightweight Approach for Telecom Networks", "categories": ["cs.LG", "cs.NI"], "comment": null, "summary": "Quality of Experience (QoE) prediction plays a crucial role in optimizing\nresource management and enhancing user satisfaction across both\ntelecommunication and OTT services. While recent advances predominantly rely on\ndeep learning models, this study introduces a lightweight generative modeling\nframework that balances computational efficiency, interpretability, and\npredictive accuracy. By validating the use of Vector Quantization (VQ) as a\npreprocessing technique, continuous network features are effectively\ntransformed into discrete categorical symbols, enabling integration with a\nHidden Markov Model (HMM) for temporal sequence modeling. This VQ-HMM pipeline\nenhances the model's capacity to capture dynamic QoE patterns while supporting\nprobabilistic inference on new and unseen data. Experimental results on\npublicly available time-series datasets incorporating both objective indicators\nand subjective QoE scores demonstrate the viability of this approach in\nreal-time and resource-constrained environments, where inference latency is\nalso critical. The framework offers a scalable alternative to complex deep\nlearning methods, particularly in scenarios with limited computational\nresources or where latency constraints are critical.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u751f\u6210\u5efa\u6a21\u6846\u67b6\uff08VQ-HMM\uff09\uff0c\u7528\u4e8eQoE\u9884\u6d4b\uff0c\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "QoE\u9884\u6d4b\u5bf9\u4f18\u5316\u8d44\u6e90\u7ba1\u7406\u548c\u63d0\u5347\u7528\u6237\u6ee1\u610f\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u590d\u6742\u4e14\u8d44\u6e90\u6d88\u8017\u5927\u3002", "method": "\u901a\u8fc7\u5411\u91cf\u91cf\u5316\uff08VQ\uff09\u5c06\u8fde\u7eed\u7279\u5f81\u79bb\u6563\u5316\uff0c\u7ed3\u5408\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff08HMM\uff09\u8fdb\u884c\u65f6\u5e8f\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9e\u65f6\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u652f\u6301\u6982\u7387\u63a8\u7406\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u6216\u5ef6\u8fdf\u654f\u611f\u7684\u573a\u666f\u3002"}}
{"id": "2504.21589", "pdf": "https://arxiv.org/pdf/2504.21589", "abs": "https://arxiv.org/abs/2504.21589", "authors": ["Lisa Kluge", "Maximilian K\u00e4hler"], "title": "DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing", "categories": ["cs.CL", "cs.AI", "cs.DL", "I.2.7"], "comment": "11 pages, 4 figures, submitted to SemEval-2025 workshop Task 5:\n  LLMs4Subjects", "summary": "This paper presents our system developed for the SemEval-2025 Task 5:\nLLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical\nLibrary's Open-Access Catalog. Our system relies on prompting a selection of\nLLMs with varying examples of intellectually annotated records and asking the\nLLMs to similarly suggest keywords for new records. This few-shot prompting\ntechnique is combined with a series of post-processing steps that map the\ngenerated keywords to the target vocabulary, aggregate the resulting subject\nterms to an ensemble vote and, finally, rank them as to their relevance to the\nrecord. Our system is fourth in the quantitative ranking in the all-subjects\ntrack, but achieves the best result in the qualitative ranking conducted by\nsubject indexing experts.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e3aSemEval-2025\u4efb\u52a15\u5f00\u53d1\u7684\u7cfb\u7edf\uff0c\u5229\u7528LLM\u8fdb\u884c\u81ea\u52a8\u4e3b\u9898\u6807\u6ce8\uff0c\u7ed3\u5408\u5c11\u6837\u672c\u63d0\u793a\u548c\u540e\u5904\u7406\u6b65\u9aa4\uff0c\u5728\u5b9a\u91cf\u6392\u540d\u4e2d\u4f4d\u5217\u7b2c\u56db\uff0c\u4f46\u5728\u4e13\u5bb6\u5b9a\u6027\u8bc4\u4f30\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4e3a\u6280\u672f\u56fe\u4e66\u9986\u7684\u5f00\u653e\u83b7\u53d6\u76ee\u5f55\u5f00\u53d1\u81ea\u52a8\u5316\u4e3b\u9898\u6807\u6ce8\u7cfb\u7edf\uff0c\u63d0\u5347\u6807\u6ce8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u5c11\u6837\u672c\u63d0\u793a\u6280\u672f\u7ed3\u5408\u591a\u4e2aLLM\u751f\u6210\u5173\u952e\u8bcd\uff0c\u5e76\u901a\u8fc7\u540e\u5904\u7406\u6b65\u9aa4\u6620\u5c04\u5230\u76ee\u6807\u8bcd\u6c47\u8868\uff0c\u8fdb\u884c\u96c6\u6210\u6295\u7968\u548c\u76f8\u5173\u6027\u6392\u5e8f\u3002", "result": "\u7cfb\u7edf\u5728\u5b9a\u91cf\u6392\u540d\u4e2d\u7b2c\u56db\uff0c\u4f46\u5728\u4e13\u5bb6\u5b9a\u6027\u8bc4\u4f30\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u4e3b\u9898\u6807\u6ce8\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u4e13\u5bb6\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2504.21435", "pdf": "https://arxiv.org/pdf/2504.21435", "abs": "https://arxiv.org/abs/2504.21435", "authors": ["Chenkai Zhang", "Yiming Lei", "Zeming Liu", "Haitao Leng", "ShaoGuo Liu", "Tingting Gao", "Qingjie Liu", "Yunhong Wang"], "title": "SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "29 pages, 15 figures, CVPR 2025", "summary": "With the rapid development of Multi-modal Large Language Models (MLLMs), an\nincreasing number of benchmarks have been established to evaluate the video\nunderstanding capabilities of these models. However, these benchmarks focus on\n\\textbf{standalone} videos and mainly assess ``visual elements'' like human\nactions and object states. In reality, contemporary videos often encompass\ncomplex and continuous narratives, typically presented as a \\textbf{series}. To\naddress this challenge, we propose \\textbf{SeriesBench}, a benchmark consisting\nof 105 carefully curated narrative-driven series, covering 28 specialized tasks\nthat require deep narrative understanding. Specifically, we first select a\ndiverse set of drama series spanning various genres. Then, we introduce a novel\nlong-span narrative annotation method, combined with a full-information\ntransformation approach to convert manual annotations into diverse task\nformats. To further enhance model capacity for detailed analysis of plot\nstructures and character relationships within series, we propose a novel\nnarrative reasoning framework, \\textbf{PC-DCoT}. Extensive results on\n\\textbf{SeriesBench} indicate that existing MLLMs still face significant\nchallenges in understanding narrative-driven series, while \\textbf{PC-DCoT}\nenables these MLLMs to achieve performance improvements. Overall, our\n\\textbf{SeriesBench} and \\textbf{PC-DCoT} highlight the critical necessity of\nadvancing model capabilities to understand narrative-driven series, guiding the\nfuture development of MLLMs. SeriesBench is publicly available at\nhttps://github.com/zackhxn/SeriesBench-CVPR2025.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86SeriesBench\u57fa\u51c6\u548cPC-DCoT\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u53d9\u4e8b\u9a71\u52a8\u89c6\u9891\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u72ec\u7acb\u89c6\u9891\u7684\u89c6\u89c9\u5143\u7d20\uff0c\u800c\u73b0\u5b9e\u4e2d\u7684\u89c6\u9891\u591a\u4e3a\u8fde\u7eed\u53d9\u4e8b\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7684\u53d9\u4e8b\u7406\u89e3\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u7cbe\u9009\u5267\u96c6\u3001\u957f\u8de8\u5ea6\u53d9\u4e8b\u6807\u6ce8\u548c\u5168\u4fe1\u606f\u8f6c\u6362\u65b9\u6cd5\u6784\u5efaSeriesBench\uff0c\u5e76\u63d0\u51faPC-DCoT\u6846\u67b6\u589e\u5f3a\u6a21\u578b\u5bf9\u60c5\u8282\u548c\u89d2\u8272\u5173\u7cfb\u7684\u5206\u6790\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u53d9\u4e8b\u7406\u89e3\u4e0a\u4ecd\u6709\u6311\u6218\uff0c\u800cPC-DCoT\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "SeriesBench\u548cPC-DCoT\u5f3a\u8c03\u4e86\u63d0\u5347\u6a21\u578b\u53d9\u4e8b\u7406\u89e3\u80fd\u529b\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u672a\u6765MLLM\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.21038", "pdf": "https://arxiv.org/pdf/2504.21038", "abs": "https://arxiv.org/abs/2504.21038", "authors": ["Yakai Li", "Jiekang Hu", "Weiduan Sang", "Luping Ma", "Jing Xie", "Weijuan Zhang", "Aimin Yu", "Shijie Zhao", "Qingjia Huang", "Qihang Zhou"], "title": "Prefill-Based Jailbreak: A Novel Approach of Bypassing LLM Safety Boundary", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are designed to generate helpful and safe\ncontent. However, adversarial attacks, commonly referred to as jailbreak, can\nbypass their safety protocols, prompting LLMs to generate harmful content or\nreveal sensitive data. Consequently, investigating jailbreak methodologies is\ncrucial for exposing systemic vulnerabilities within LLMs, ultimately guiding\nthe continuous implementation of security enhancements by developers. In this\npaper, we introduce a novel jailbreak attack method that leverages the\nprefilling feature of LLMs, a feature designed to enhance model output\nconstraints. Unlike traditional jailbreak methods, the proposed attack\ncircumvents LLMs' safety mechanisms by directly manipulating the probability\ndistribution of subsequent tokens, thereby exerting control over the model's\noutput. We propose two attack variants: Static Prefilling (SP), which employs a\nuniversal prefill text, and Optimized Prefilling (OP), which iteratively\noptimizes the prefill text to maximize the attack success rate. Experiments on\nsix state-of-the-art LLMs using the AdvBench benchmark validate the\neffectiveness of our method and demonstrate its capability to substantially\nenhance attack success rates when combined with existing jailbreak approaches.\nThe OP method achieved attack success rates of up to 99.82% on certain models,\nsignificantly outperforming baseline methods. This work introduces a new\njailbreak attack method in LLMs, emphasizing the need for robust content\nvalidation mechanisms to mitigate the adversarial exploitation of prefilling\nfeatures. All code and data used in this paper are publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684LLM\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u64cd\u7eb5\u9884\u586b\u5145\u7279\u5f81\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u7814\u7a76\u8d8a\u72f1\u65b9\u6cd5\u4ee5\u63ed\u793aLLM\u7684\u7cfb\u7edf\u6027\u6f0f\u6d1e\uff0c\u6307\u5bfc\u5f00\u53d1\u8005\u52a0\u5f3a\u5b89\u5168\u6027\u3002", "method": "\u5229\u7528\u9884\u586b\u5145\u7279\u5f81\u76f4\u63a5\u64cd\u7eb5\u540e\u7eed\u4ee4\u724c\u7684\u6982\u7387\u5206\u5e03\uff0c\u63d0\u51fa\u9759\u6001\u9884\u586b\u5145\uff08SP\uff09\u548c\u4f18\u5316\u9884\u586b\u5145\uff08OP\uff09\u4e24\u79cd\u53d8\u4f53\u3002", "result": "\u5728\u516d\u79cd\u5148\u8fdbLLM\u4e0a\u9a8c\u8bc1\uff0cOP\u65b9\u6cd5\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe99.82%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u66f4\u9c81\u68d2\u7684\u5185\u5bb9\u9a8c\u8bc1\u673a\u5236\u6765\u9632\u8303\u9884\u586b\u5145\u7279\u5f81\u7684\u5bf9\u6297\u6027\u5229\u7528\u3002"}}
{"id": "2504.21358", "pdf": "https://arxiv.org/pdf/2504.21358", "abs": "https://arxiv.org/abs/2504.21358", "authors": ["Xiao Zheng", "Saeed Asadi Bagloee", "Majid Sarvi"], "title": "A comparative study of deep learning and ensemble learning to extend the horizon of traffic forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "32 pages, 16 figures", "summary": "Traffic forecasting is vital for Intelligent Transportation Systems, for\nwhich Machine Learning (ML) methods have been extensively explored to develop\ndata-driven Artificial Intelligence (AI) solutions. Recent research focuses on\nmodelling spatial-temporal correlations for short-term traffic prediction,\nleaving the favourable long-term forecasting a challenging and open issue. This\npaper presents a comparative study on large-scale real-world signalized\narterials and freeway traffic flow datasets, aiming to evaluate promising ML\nmethods in the context of large forecasting horizons up to 30 days. Focusing on\nmodelling capacity for temporal dynamics, we develop one ensemble ML method,\neXtreme Gradient Boosting (XGBoost), and a range of Deep Learning (DL) methods,\nincluding Recurrent Neural Network (RNN)-based methods and the state-of-the-art\nTransformer-based method. Time embedding is leveraged to enhance their\nunderstanding of seasonality and event factors. Experimental results highlight\nthat while the attention mechanism/Transformer framework is effective for\ncapturing long-range dependencies in sequential data, as the forecasting\nhorizon extends, the key to effective traffic forecasting gradually shifts from\ntemporal dependency capturing to periodicity modelling. Time embedding is\nparticularly effective in this context, helping naive RNN outperform Informer\nby 31.1% for 30-day-ahead forecasting. Meanwhile, as an efficient and robust\nmodel, XGBoost, while learning solely from time features, performs\ncompetitively with DL methods. Moreover, we investigate the impacts of various\nfactors like input sequence length, holiday traffic, data granularity, and\ntraining data size. The findings offer valuable insights and serve as a\nreference for future long-term traffic forecasting research and the improvement\nof AI's corresponding learning capabilities.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u957f\u671f\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u65f6\u95f4\u5d4c\u5165\u5bf9\u5468\u671f\u6027\u5efa\u6a21\u81f3\u5173\u91cd\u8981\uff0c\u4e14XGBoost\u5728\u4ec5\u4f7f\u7528\u65f6\u95f4\u7279\u5f81\u65f6\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u957f\u671f\u4ea4\u901a\u9884\u6d4b\u7684\u6311\u6218\uff0c\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5927\u65f6\u95f4\u8de8\u5ea6\uff0830\u5929\uff09\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u5f00\u53d1\u4e86XGBoost\u548c\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982RNN\u548cTransformer\uff09\uff0c\u5e76\u5229\u7528\u65f6\u95f4\u5d4c\u5165\u589e\u5f3a\u6a21\u578b\u5bf9\u5468\u671f\u6027\u548c\u4e8b\u4ef6\u7684\u7406\u89e3\u3002", "result": "\u65f6\u95f4\u5d4c\u5165\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6548\u679c\uff0cXGBoost\u8868\u73b0\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\u3002Transformer\u5728\u957f\u5e8f\u5217\u4f9d\u8d56\u4e2d\u6709\u6548\uff0c\u4f46\u968f\u7740\u9884\u6d4b\u65f6\u95f4\u5ef6\u957f\uff0c\u5468\u671f\u6027\u5efa\u6a21\u66f4\u4e3a\u5173\u952e\u3002", "conclusion": "\u957f\u671f\u4ea4\u901a\u9884\u6d4b\u7684\u5173\u952e\u5728\u4e8e\u5468\u671f\u6027\u5efa\u6a21\uff0c\u65f6\u95f4\u5d4c\u5165\u548cXGBoost\u662f\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2504.21604", "pdf": "https://arxiv.org/pdf/2504.21604", "abs": "https://arxiv.org/abs/2504.21604", "authors": ["Bing Wang", "Ximing Li", "Changchun Li", "Bingrui Zhao", "Bo Fu", "Renchu Guan", "Shengsheng Wang"], "title": "Robust Misinformation Detection by Visiting Potential Commonsense Conflict", "categories": ["cs.CL", "cs.CY"], "comment": "11 pages, 2 figures. Accepted by IJCAI 2025. Code:\n  https://github.com/wangbing1416/MD-PCC", "summary": "The development of Internet technology has led to an increased prevalence of\nmisinformation, causing severe negative effects across diverse domains. To\nmitigate this challenge, Misinformation Detection (MD), aiming to detect online\nmisinformation automatically, emerges as a rapidly growing research topic in\nthe community. In this paper, we propose a novel plug-and-play augmentation\nmethod for the MD task, namely Misinformation Detection with Potential\nCommonsense Conflict (MD-PCC). We take inspiration from the prior studies\nindicating that fake articles are more likely to involve commonsense conflict.\nAccordingly, we construct commonsense expressions for articles, serving to\nexpress potential commonsense conflicts inferred by the difference between\nextracted commonsense triplet and golden ones inferred by the well-established\ncommonsense reasoning tool COMET. These expressions are then specified for each\narticle as augmentation. Any specific MD methods can be then trained on those\ncommonsense-augmented articles. Besides, we also collect a novel\ncommonsense-oriented dataset named CoMis, whose all fake articles are caused by\ncommonsense conflict. We integrate MD-PCC with various existing MD backbones\nand compare them across both 4 public benchmark datasets and CoMis. Empirical\nresults demonstrate that MD-PCC can consistently outperform the existing MD\nbaselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMD-PCC\u7684\u65b0\u578b\u63d2\u4ef6\u5f0f\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u7f51\u7edc\u865a\u5047\u4fe1\u606f\uff0c\u901a\u8fc7\u5229\u7528\u5e38\u8bc6\u51b2\u7a81\u4f5c\u4e3a\u7279\u5f81\u589e\u5f3a\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6CoMis\u4e0a\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e92\u8054\u7f51\u6280\u672f\u7684\u53d1\u5c55\u5bfc\u81f4\u865a\u5047\u4fe1\u606f\u6cdb\u6ee5\uff0c\u4e9f\u9700\u81ea\u52a8\u68c0\u6d4b\u65b9\u6cd5\u3002\u7814\u7a76\u53d1\u73b0\u865a\u5047\u6587\u7ae0\u66f4\u6613\u6d89\u53ca\u5e38\u8bc6\u51b2\u7a81\uff0c\u56e0\u6b64\u5229\u7528\u8fd9\u4e00\u7279\u5f81\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u6784\u5efa\u5e38\u8bc6\u8868\u8fbe\u5f0f\u4ee5\u8868\u793a\u6f5c\u5728\u5e38\u8bc6\u51b2\u7a81\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u589e\u5f3a\u7279\u5f81\u7528\u4e8e\u8bad\u7ec3\u4efb\u4f55\u73b0\u6709\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u65b9\u6cd5\u3002\u540c\u65f6\u6536\u96c6\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5e38\u8bc6\u51b2\u7a81\u7684\u65b0\u6570\u636e\u96c6CoMis\u3002", "result": "MD-PCC\u57284\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u548cCoMis\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MD-PCC\u901a\u8fc7\u5229\u7528\u5e38\u8bc6\u51b2\u7a81\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.21447", "pdf": "https://arxiv.org/pdf/2504.21447", "abs": "https://arxiv.org/abs/2504.21447", "authors": ["Haoran Chen", "Junyan Lin", "Xinhao Chen", "Yue Fan", "Xin Jin", "Hui Su", "Jianfeng Dong", "Jinlan Fu", "Xiaoyu Shen"], "title": "Rethinking Visual Layer Selection in Multimodal LLMs", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 4 figures, submitted to ICCV 2025", "summary": "Multimodal large language models (MLLMs) have achieved impressive performance\nacross a wide range of tasks, typically using CLIP-ViT as their visual encoder\ndue to its strong text-image alignment capabilities. While prior studies\nsuggest that different CLIP-ViT layers capture different types of information,\nwith shallower layers focusing on fine visual details and deeper layers\naligning more closely with textual semantics, most MLLMs still select visual\nfeatures based on empirical heuristics rather than systematic analysis. In this\nwork, we propose a Layer-wise Representation Similarity approach to group\nCLIP-ViT layers with similar behaviors into {shallow, middle, and deep}\ncategories and assess their impact on MLLM performance. Building on this\nfoundation, we revisit the visual layer selection problem in MLLMs at scale,\ntraining LLaVA-style models ranging from 1.4B to 7B parameters. Through\nextensive experiments across 10 datasets and 4 tasks, we find that: (1) deep\nlayers are essential for OCR tasks; (2) shallow and middle layers substantially\noutperform deep layers on reasoning tasks involving counting, positioning, and\nobject localization; (3) a lightweight fusion of features across shallow,\nmiddle, and deep layers consistently outperforms specialized fusion baselines\nand single-layer selections, achieving gains on 9 out of 10 datasets. Our work\noffers the first principled study of visual layer selection in MLLMs, laying\nthe groundwork for deeper investigations into visual representation learning\nfor MLLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c42\u95f4\u8868\u793a\u76f8\u4f3c\u6027\u7684\u65b9\u6cd5\uff0c\u5c06CLIP-ViT\u5c42\u5206\u4e3a\u6d45\u3001\u4e2d\u3001\u6df1\u4e09\u7c7b\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9MLLM\u6027\u80fd\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0d\u540c\u4efb\u52a1\u9700\u8981\u4e0d\u540c\u5c42\u7279\u5f81\uff0c\u8f7b\u91cf\u7ea7\u878d\u5408\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u73b0\u6709MLLM\u901a\u5e38\u57fa\u4e8e\u7ecf\u9a8c\u9009\u62e9\u89c6\u89c9\u7279\u5f81\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5c42\u95f4\u76f8\u4f3c\u6027\u5206\u6790\u4f18\u5316\u89c6\u89c9\u7279\u5f81\u9009\u62e9\u3002", "method": "\u63d0\u51fa\u5c42\u95f4\u8868\u793a\u76f8\u4f3c\u6027\u65b9\u6cd5\uff0c\u5c06CLIP-ViT\u5c42\u5206\u7c7b\uff0c\u5e76\u5728\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u7684LLaVA\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a(1) \u6df1\u5c42\u5bf9OCR\u4efb\u52a1\u5173\u952e\uff1b(2) \u6d45\u4e2d\u5c42\u5728\u8ba1\u6570\u3001\u5b9a\u4f4d\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\uff1b(3) \u8f7b\u91cf\u7ea7\u878d\u5408\u65b9\u6cd5\u57289/10\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86MLLM\u4e2d\u89c6\u89c9\u5c42\u9009\u62e9\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.21039", "pdf": "https://arxiv.org/pdf/2504.21039", "abs": "https://arxiv.org/abs/2504.21039", "authors": ["Paul Kassianik", "Baturay Saglam", "Alexander Chen", "Blaine Nelson", "Anu Vellore", "Massimo Aufiero", "Fraser Burch", "Dhruv Kedia", "Avi Zohary", "Sajana Weerawardhena", "Aman Priyanshu", "Adam Swanda", "Amy Chang", "Hyrum Anderson", "Kojin Oshiba", "Omar Santos", "Yaron Singer", "Amin Karbasi"], "title": "Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "As transformer-based large language models (LLMs) increasingly permeate\nsociety, they have revolutionized domains such as software engineering,\ncreative writing, and digital arts. However, their adoption in cybersecurity\nremains limited due to challenges like scarcity of specialized training data\nand complexity of representing cybersecurity-specific knowledge. To address\nthese gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on\nthe Llama 3.1 architecture and enhanced through continued pretraining on a\ncarefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across\nboth established and new cybersecurity benchmarks, showing that it matches\nLlama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By\nreleasing our model to the public, we aim to accelerate progress and adoption\nof AI-driven tools in both public and private cybersecurity contexts.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Foundation-Sec-8B\uff0c\u4e00\u4e2a\u57fa\u4e8eLlama 3.1\u67b6\u6784\u7684\u7f51\u7edc\u5b89\u5168\u4e13\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7cbe\u5fc3\u7b56\u5212\u7684\u7f51\u7edc\u5b89\u5168\u8bed\u6599\u5e93\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u5f53\u524dLLMs\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u5e94\u7528\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u4e13\u4e1a\u8bad\u7ec3\u6570\u636e\u548c\u7f51\u7edc\u5b89\u5168\u77e5\u8bc6\u8868\u793a\u7684\u590d\u6742\u6027\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u5e94\u7528\u53d7\u9650\u3002", "method": "\u57fa\u4e8eLlama 3.1\u67b6\u6784\uff0c\u901a\u8fc7\u7ee7\u7eed\u9884\u8bad\u7ec3\u7f51\u7edc\u5b89\u5168\u8bed\u6599\u5e93\uff0c\u6784\u5efa\u4e86Foundation-Sec-8B\u6a21\u578b\u3002", "result": "\u5728\u7f51\u7edc\u5b89\u5168\u4efb\u52a1\u4e2d\uff0cFoundation-Sec-8B\u8868\u73b0\u4e0eLlama 3.1-70B\u548cGPT-4o-mini\u76f8\u5f53\u3002", "conclusion": "\u516c\u5f00\u6a21\u578b\u4ee5\u4fc3\u8fdbAI\u5de5\u5177\u5728\u516c\u5171\u548c\u79c1\u6709\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2504.21375", "pdf": "https://arxiv.org/pdf/2504.21375", "abs": "https://arxiv.org/abs/2504.21375", "authors": ["Sangyeon Cho", "Jangyeong Jeon", "Mingi Kim", "Junyeong Kim"], "title": "Synergy-CLIP: Extending CLIP with Multi-modal Integration for Robust Representation Learning", "categories": ["cs.LG"], "comment": "Multi-modal, Multi-modal Representation Learning, Missing Modality,\n  Missing Modality Reconstruction, Speech and Multi-modality, Vision and\n  Language", "summary": "Multi-modal representation learning has become a pivotal area in artificial\nintelligence, enabling the integration of diverse modalities such as vision,\ntext, and audio to solve complex problems. However, existing approaches\npredominantly focus on bimodal interactions, such as image-text pairs, which\nlimits their ability to fully exploit the richness of multi-modal data.\nFurthermore, the integration of modalities in equal-scale environments remains\nunderexplored due to the challenges of constructing large-scale, balanced\ndatasets. In this study, we propose Synergy-CLIP, a novel framework that\nextends the contrastive language-image pre-training (CLIP) architecture to\nenhance multi-modal representation learning by integrating visual, textual, and\naudio modalities. Unlike existing methods that focus on adapting individual\nmodalities to vanilla-CLIP, Synergy-CLIP aligns and captures latent information\nacross three modalities equally. To address the high cost of constructing\nlarge-scale multi-modal datasets, we introduce VGG-sound+, a triple-modal\ndataset designed to provide equal-scale representation of visual, textual, and\naudio data. Synergy-CLIP is validated on various downstream tasks, including\nzero-shot classification, where it outperforms existing baselines.\nAdditionally, we introduce a missing modality reconstruction task,\ndemonstrating Synergy-CLIP's ability to extract synergy among modalities in\nrealistic application scenarios. These contributions provide a robust\nfoundation for advancing multi-modal representation learning and exploring new\nresearch directions.", "AI": {"tldr": "Synergy-CLIP\u6269\u5c55\u4e86CLIP\u67b6\u6784\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u3001\u6587\u672c\u548c\u97f3\u9891\u6a21\u6001\u6765\u589e\u5f3a\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\uff0c\u5e76\u63d0\u51fa\u4e86VGG-sound+\u6570\u636e\u96c6\u4ee5\u89e3\u51b3\u6570\u636e\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u53cc\u6a21\u6001\u4ea4\u4e92\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u7684\u4e30\u5bcc\u6027\uff0c\u4e14\u7f3a\u4e4f\u5e73\u8861\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faSynergy-CLIP\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u4e09\u79cd\u6a21\u6001\u7684\u6f5c\u5728\u4fe1\u606f\uff0c\u5e76\u6784\u5efaVGG-sound+\u6570\u636e\u96c6\u3002", "result": "\u5728\u96f6\u6837\u672c\u5206\u7c7b\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u80fd\u91cd\u5efa\u7f3a\u5931\u6a21\u6001\u3002", "conclusion": "Synergy-CLIP\u4e3a\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5e76\u5c55\u793a\u4e86\u6a21\u6001\u534f\u540c\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.21605", "pdf": "https://arxiv.org/pdf/2504.21605", "abs": "https://arxiv.org/abs/2504.21605", "authors": ["Jonas Gwozdz", "Andreas Both"], "title": "RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet\nsystematically assessing their reliability with conflicting information remains\ndifficult. We propose an RDF-based framework to assess multilingual LLM\nquality, focusing on knowledge conflicts. Our approach captures model responses\nacross four distinct context conditions (complete, incomplete, conflicting, and\nno-context information) in German and English. This structured representation\nenables the comprehensive analysis of knowledge leakage-where models favor\ntraining data over provided context-error detection, and multilingual\nconsistency. We demonstrate the framework through a fire safety domain\nexperiment, revealing critical patterns in context prioritization and\nlanguage-specific performance, and demonstrating that our vocabulary was\nsufficient to express every assessment facet encountered in the 28-question\nstudy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRDF\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u77e5\u8bc6\u51b2\u7a81\u60c5\u51b5\u4e0b\u7684\u53ef\u9760\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u77e5\u8bc6\u6cc4\u6f0f\u3001\u9519\u8bef\u68c0\u6d4b\u548c\u591a\u8bed\u8a00\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u4fe1\u606f\u51b2\u7a81\u60c5\u51b5\u4e0b\u7684\u53ef\u9760\u6027\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u3002", "method": "\u901a\u8fc7\u56db\u79cd\u4e0a\u4e0b\u6587\u6761\u4ef6\uff08\u5b8c\u6574\u3001\u4e0d\u5b8c\u6574\u3001\u51b2\u7a81\u548c\u65e0\u4e0a\u4e0b\u6587\uff09\u5728\u5fb7\u8bed\u548c\u82f1\u8bed\u4e2d\u6355\u83b7\u6a21\u578b\u54cd\u5e94\uff0c\u5e76\u5229\u7528RDF\u7ed3\u6784\u5316\u8868\u793a\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u63ed\u793a\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u4f18\u5148\u7ea7\u548c\u8bed\u8a00\u7279\u5b9a\u6027\u80fd\u4e0a\u7684\u5173\u952e\u6a21\u5f0f\uff0c\u4e14\u8bcd\u6c47\u8db3\u4ee5\u8986\u76d628\u4e2a\u95ee\u9898\u7684\u6240\u6709\u8bc4\u4f30\u65b9\u9762\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u8bc4\u4f30LLM\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u77e5\u8bc6\u51b2\u7a81\u63d0\u4f9b\u4e86\u5168\u9762\u4e14\u7ed3\u6784\u5316\u7684\u65b9\u6cd5\u3002"}}
{"id": "2504.21464", "pdf": "https://arxiv.org/pdf/2504.21464", "abs": "https://arxiv.org/abs/2504.21464", "authors": ["Shamim Rahim Refat", "Ziyan Shirin Raha", "Shuvashis Sarker", "Faika Fairuj Preotee", "MD. Musfikur Rahman", "Tashreef Muhammad", "Mohammad Shafiul Islam"], "title": "VR-FuseNet: A Fusion of Heterogeneous Fundus Data and Explainable Deep Network for Diabetic Retinopathy Classification", "categories": ["cs.CV"], "comment": "33 pages, 49 figures", "summary": "Diabetic retinopathy is a severe eye condition caused by diabetes where the\nretinal blood vessels get damaged and can lead to vision loss and blindness if\nnot treated. Early and accurate detection is key to intervention and stopping\nthe disease progressing. For addressing this disease properly, this paper\npresents a comprehensive approach for automated diabetic retinopathy detection\nby proposing a new hybrid deep learning model called VR-FuseNet. Diabetic\nretinopathy is a major eye disease and leading cause of blindness especially\namong diabetic patients so accurate and efficient automated detection methods\nare required. To address the limitations of existing methods including dataset\nimbalance, diversity and generalization issues this paper presents a hybrid\ndataset created from five publicly available diabetic retinopathy datasets.\nEssential preprocessing techniques such as SMOTE for class balancing and CLAHE\nfor image enhancement are applied systematically to the dataset to improve the\nrobustness and generalizability of the dataset. The proposed VR-FuseNet model\ncombines the strengths of two state-of-the-art convolutional neural networks,\nVGG19 which captures fine-grained spatial features and ResNet50V2 which is\nknown for its deep hierarchical feature extraction. This fusion improves the\ndiagnostic performance and achieves an accuracy of 91.824%. The model\noutperforms individual architectures on all performance metrics demonstrating\nthe effectiveness of hybrid feature extraction in Diabetic Retinopathy\nclassification tasks. To make the proposed model more clinically useful and\ninterpretable this paper incorporates multiple XAI techniques. These techniques\ngenerate visual explanations that clearly indicate the retinal features\naffecting the model's prediction such as microaneurysms, hemorrhages and\nexudates so that clinicians can interpret and validate.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVR-FuseNet\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u68c0\u6d4b\uff0c\u7ed3\u5408\u4e86VGG19\u548cResNet50V2\u7684\u4f18\u52bf\uff0c\u51c6\u786e\u7387\u8fbe91.824%\uff0c\u5e76\u901a\u8fc7XAI\u6280\u672f\u589e\u5f3a\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u662f\u5bfc\u81f4\u7cd6\u5c3f\u75c5\u60a3\u8005\u5931\u660e\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u51c6\u786e\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4e94\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u6784\u5efa\u6df7\u5408\u6570\u636e\u96c6\uff0c\u5e94\u7528SMOTE\u548cCLAHE\u8fdb\u884c\u9884\u5904\u7406\uff0c\u63d0\u51faVR-FuseNet\u6a21\u578b\uff0c\u7ed3\u5408VGG19\u548cResNet50V2\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "result": "\u6a21\u578b\u51c6\u786e\u7387\u8fbe91.824%\uff0c\u4f18\u4e8e\u5355\u4e00\u67b6\u6784\uff0c\u5e76\u901a\u8fc7XAI\u6280\u672f\u751f\u6210\u53ef\u89c6\u5316\u89e3\u91ca\u3002", "conclusion": "VR-FuseNet\u5728\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7ed3\u5408XAI\u6280\u672f\u63d0\u5347\u4e86\u4e34\u5e8a\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.21042", "pdf": "https://arxiv.org/pdf/2504.21042", "abs": "https://arxiv.org/abs/2504.21042", "authors": ["Jiamin Chang", "Haoyang Li", "Hammond Pearce", "Ruoxi Sun", "Bo Li", "Minhui Xue"], "title": "What's Pulling the Strings? Evaluating Integrity and Attribution in AI Training and Inference through Concept Shift", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "Accept By The ACM Conference on Computer and Communications Security\n  (CCS) 2025", "summary": "The growing adoption of artificial intelligence (AI) has amplified concerns\nabout trustworthiness, including integrity, privacy, robustness, and bias. To\nassess and attribute these threats, we propose ConceptLens, a generic framework\nthat leverages pre-trained multimodal models to identify the root causes of\nintegrity threats by analyzing Concept Shift in probing samples. ConceptLens\ndemonstrates strong detection performance for vanilla data poisoning attacks\nand uncovers vulnerabilities to bias injection, such as the generation of\ncovert advertisements through malicious concept shifts. It identifies privacy\nrisks in unaltered but high-risk samples, filters them before training, and\nprovides insights into model weaknesses arising from incomplete or imbalanced\ntraining data. Additionally, at the model level, it attributes concepts that\nthe target model is overly dependent on, identifies misleading concepts, and\nexplains how disrupting key concepts negatively impacts the model. Furthermore,\nit uncovers sociological biases in generative content, revealing disparities\nacross sociological contexts. Strikingly, ConceptLens reveals how safe training\nand inference data can be unintentionally and easily exploited, potentially\nundermining safety alignment. Our study informs actionable insights to breed\ntrust in AI systems, thereby speeding adoption and driving greater innovation.", "AI": {"tldr": "ConceptLens\u662f\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u5206\u6790\u6982\u5ff5\u6f02\u79fb\uff0c\u8bc4\u4f30AI\u7cfb\u7edf\u7684\u4fe1\u4efb\u95ee\u9898\uff0c\u5982\u5b8c\u6574\u6027\u3001\u9690\u79c1\u3001\u504f\u89c1\u7b49\u3002", "motivation": "\u968f\u7740AI\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u4fe1\u4efb\u95ee\u9898\uff08\u5982\u5b8c\u6574\u6027\u3001\u9690\u79c1\u3001\u504f\u89c1\u7b49\uff09\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u4e00\u79cd\u65b9\u6cd5\u8bc4\u4f30\u548c\u6eaf\u6e90\u8fd9\u4e9b\u5a01\u80c1\u3002", "method": "\u901a\u8fc7\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u5206\u6790\u6982\u5ff5\u6f02\u79fb\uff0c\u68c0\u6d4b\u6570\u636e\u6295\u6bd2\u653b\u51fb\u3001\u9690\u79c1\u98ce\u9669\uff0c\u5e76\u8bc6\u522b\u6a21\u578b\u4f9d\u8d56\u7684\u8bef\u5bfc\u6027\u6982\u5ff5\u3002", "result": "ConceptLens\u80fd\u9ad8\u6548\u68c0\u6d4b\u6570\u636e\u6295\u6bd2\u653b\u51fb\uff0c\u63ed\u793a\u9690\u79c1\u98ce\u9669\u548c\u793e\u4f1a\u5b66\u504f\u89c1\uff0c\u5e76\u8bc6\u522b\u6a21\u578b\u5f31\u70b9\u3002", "conclusion": "ConceptLens\u4e3a\u63d0\u5347AI\u7cfb\u7edf\u4fe1\u4efb\u63d0\u4f9b\u4e86\u53ef\u884c\u89c1\u89e3\uff0c\u52a0\u901f\u5176\u5e94\u7528\u548c\u521b\u65b0\u3002"}}
{"id": "2504.21380", "pdf": "https://arxiv.org/pdf/2504.21380", "abs": "https://arxiv.org/abs/2504.21380", "authors": ["In\u00eas Cardoso Oliveira", "Decebal Constantin Mocanu", "Luis A. Leiva"], "title": "Sparse-to-Sparse Training of Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Diffusion models (DMs) are a powerful type of generative models that have\nachieved state-of-the-art results in various image synthesis tasks and have\nshown potential in other domains, such as natural language processing and\ntemporal data modeling. Despite their stable training dynamics and ability to\nproduce diverse high-quality samples, DMs are notorious for requiring\nsignificant computational resources, both in the training and inference stages.\nPrevious work has focused mostly on increasing the efficiency of model\ninference. This paper introduces, for the first time, the paradigm of\nsparse-to-sparse training to DMs, with the aim of improving both training and\ninference efficiency. We focus on unconditional generation and train sparse DMs\nfrom scratch (Latent Diffusion and ChiroDiff) on six datasets using three\ndifferent methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of\nsparsity in model performance. Our experiments show that sparse DMs are able to\nmatch and often outperform their Dense counterparts, while substantially\nreducing the number of trainable parameters and FLOPs. We also identify safe\nand effective values to perform sparse-to-sparse training of DMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u5230\u7a00\u758f\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u9996\u6b21\u5e94\u7528\u4e8e\u6269\u6563\u6a21\u578b\uff08DMs\uff09\uff0c\u65e8\u5728\u63d0\u9ad8\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u7a00\u758fDMs\u5728\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\u7684\u540c\u65f6\uff0c\u6027\u80fd\u4e0e\u5bc6\u96c6\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u3002", "motivation": "\u5c3d\u7ba1DMs\u5728\u56fe\u50cf\u5408\u6210\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u3002\u6b64\u524d\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u63a8\u7406\u6548\u7387\uff0c\u672c\u6587\u5219\u9996\u6b21\u63a2\u7d22\u7a00\u758f\u8bad\u7ec3\u5bf9DMs\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u65b9\u6cd5\uff08Static-DM\u3001RigL-DM\u548cMagRan-DM\uff09\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7a00\u758fDMs\uff08Latent Diffusion\u548cChiroDiff\uff09\uff0c\u7814\u7a76\u7a00\u758f\u6027\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7a00\u758fDMs\u5728\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u548cFLOPs\u7684\u540c\u65f6\uff0c\u6027\u80fd\u4e0e\u5bc6\u96c6\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u5e76\u786e\u5b9a\u4e86\u7a00\u758f\u8bad\u7ec3\u7684\u5b89\u5168\u6709\u6548\u503c\u3002", "conclusion": "\u7a00\u758f\u5230\u7a00\u758f\u8bad\u7ec3\u53ef\u663e\u8457\u63d0\u5347DMs\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2504.21625", "pdf": "https://arxiv.org/pdf/2504.21625", "abs": "https://arxiv.org/abs/2504.21625", "authors": ["Jiaming Wang"], "title": "Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn Instruction-Following Ability", "categories": ["cs.CL"], "comment": null, "summary": "The ability to follow instructions accurately is fundamental for Large\nLanguage Models (LLMs) to serve as reliable agents in real-world applications.\nWhile existing instruction-following benchmarks are either single-turn or\nintroduce new requirements in each turn without allowing self-correction,\nMeeseeks simulates realistic human-LLM interactions through an iterative\nfeedback process. This design enables models to self-correct based on specific\nrequirement failures, better reflecting real-world user-end usage patterns. The\nbenchmark implements a comprehensive evaluation system with 38 capability tags\norganized across three dimensions: Intent Recognition, Granular Content\nValidation, and Output Structure Validation. Through rigorous evaluation across\nLLMs, Meeseeks provides valuable insights into LLMs' instruction-following\ncapabilities in practical applications.", "AI": {"tldr": "Meeseeks\u662f\u4e00\u4e2a\u65b0\u7684\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u6a21\u62df\u771f\u5b9e\u4eba\u673a\u4ea4\u4e92\uff0c\u652f\u6301\u6a21\u578b\u81ea\u6211\u4fee\u6b63\uff0c\u5e76\u5168\u9762\u8bc4\u4f30LLMs\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u591a\u4e3a\u5355\u8f6e\u6216\u4e0d\u5141\u8bb8\u81ea\u6211\u4fee\u6b63\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4f7f\u7528\u573a\u666f\u3002Meeseeks\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u53cd\u9988\u673a\u5236\uff0c\u5141\u8bb8\u6a21\u578b\u81ea\u6211\u4fee\u6b63\uff0c\u5e76\u901a\u8fc738\u4e2a\u80fd\u529b\u6807\u7b7e\u5728\u4e09\u4e2a\u7ef4\u5ea6\uff08\u610f\u56fe\u8bc6\u522b\u3001\u5185\u5bb9\u9a8c\u8bc1\u3001\u8f93\u51fa\u7ed3\u6784\u9a8c\u8bc1\uff09\u8bc4\u4f30LLMs\u3002", "result": "Meeseeks\u4e3aLLMs\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "Meeseeks\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u4ea4\u4e92\u548c\u5168\u9762\u8bc4\u4f30\uff0c\u63d0\u5347\u4e86LLMs\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u7684\u8bc4\u6d4b\u6807\u51c6\u3002"}}
{"id": "2504.21467", "pdf": "https://arxiv.org/pdf/2504.21467", "abs": "https://arxiv.org/abs/2504.21467", "authors": ["Luc Vedrenne", "Sylvain Faisan", "Denis Fortun"], "title": "Multiview Point Cloud Registration via Optimization in an Autoencoder Latent Space", "categories": ["cs.CV"], "comment": "14 pages, 19 figures, IEEE Transactions on Image Processing", "summary": "Point cloud rigid registration is a fundamental problem in 3D computer\nvision. In the multiview case, we aim to find a set of 6D poses to align a set\nof objects. Methods based on pairwise registration rely on a subsequent\nsynchronization algorithm, which makes them poorly scalable with the number of\nviews. Generative approaches overcome this limitation, but are based on\nGaussian Mixture Models and use an Expectation-Maximization algorithm. Hence,\nthey are not well suited to handle large transformations. Moreover, most\nexisting methods cannot handle high levels of degradations. In this paper, we\nintroduce POLAR (POint cloud LAtent Registration), a multiview registration\nmethod able to efficiently deal with a large number of views, while being\nrobust to a high level of degradations and large initial angles. To achieve\nthis, we transpose the registration problem into the latent space of a\npretrained autoencoder, design a loss taking degradations into account, and\ndevelop an efficient multistart optimization strategy. Our proposed method\nsignificantly outperforms state-of-the-art approaches on synthetic and real\ndata. POLAR is available at github.com/pypolar/polar or as a standalone package\nwhich can be installed with pip install polaregistration.", "AI": {"tldr": "POLAR\u662f\u4e00\u79cd\u591a\u89c6\u89d2\u70b9\u4e91\u521a\u6027\u914d\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u8f6c\u6362\u548c\u4f18\u5316\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5927\u89c6\u89d2\u3001\u9ad8\u9000\u5316\u548c\u5927\u521d\u59cb\u89d2\u5ea6\u4e0b\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u89c6\u89d2\u914d\u51c6\u4e2d\u96be\u4ee5\u5904\u7406\u5927\u91cf\u89c6\u89d2\u3001\u9ad8\u9000\u5316\u548c\u5927\u521d\u59cb\u89d2\u5ea6\u7684\u95ee\u9898\u3002", "method": "\u5c06\u914d\u51c6\u95ee\u9898\u8f6c\u6362\u5230\u9884\u8bad\u7ec3\u81ea\u7f16\u7801\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u8bbe\u8ba1\u8003\u8651\u9000\u5316\u7684\u635f\u5931\u51fd\u6570\uff0c\u5e76\u91c7\u7528\u591a\u8d77\u70b9\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "POLAR\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u591a\u89c6\u89d2\u70b9\u4e91\u914d\u51c6\u65b9\u6cd5\u3002"}}
{"id": "2504.21043", "pdf": "https://arxiv.org/pdf/2504.21043", "abs": "https://arxiv.org/abs/2504.21043", "authors": ["Lingxiang wang", "Hainan Zhang", "Qinnan Zhang", "Ziwei Wang", "Hongwei Zheng", "Jin Dong", "Zhiming Zheng"], "title": "CodeBC: A More Secure Large Language Model for Smart Contract Code Generation in Blockchain", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel at generating code from natural language\ninstructions, yet they often lack an understanding of security vulnerabilities.\nThis limitation makes it difficult for LLMs to avoid security risks in\ngenerated code, particularly in high-security programming tasks such as smart\ncontract development for blockchain. Researchers have attempted to enhance the\nvulnerability awareness of these models by training them to differentiate\nbetween vulnerable and fixed code snippets. However, this approach relies\nheavily on manually labeled vulnerability data, which is only available for\npopular languages like Python and C++. For low-resource languages like\nSolidity, used in smart contracts, large-scale annotated datasets are scarce\nand difficult to obtain. To address this challenge, we introduce CodeBC, a code\ngeneration model specifically designed for generating secure smart contracts in\nblockchain. CodeBC employs a three-stage fine-tuning approach based on\nCodeLlama, distinguishing itself from previous methods by not relying on\npairwise vulnerability location annotations. Instead, it leverages\nvulnerability and security tags to teach the model the differences between\nvulnerable and secure code. During the inference phase, the model leverages\nsecurity tags to generate secure and robust code. Experimental results\ndemonstrate that CodeBC outperforms baseline models in terms of BLEU, CodeBLEU,\nand compilation pass rates, while significantly reducing vulnerability rates.\nThese findings validate the effectiveness and cost-efficiency of our\nthree-stage fine-tuning strategy, making CodeBC a promising solution for\ngenerating secure smart contract code.", "AI": {"tldr": "CodeBC\u662f\u4e00\u4e2a\u4e13\u4e3a\u751f\u6210\u5b89\u5168\u7684\u533a\u5757\u94fe\u667a\u80fd\u5408\u7ea6\u8bbe\u8ba1\u7684\u4ee3\u7801\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u5fae\u8c03\u65b9\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u6210\u5bf9\u7684\u6f0f\u6d1e\u6807\u6ce8\u6570\u636e\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6f0f\u6d1e\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u4ee3\u7801\u65f6\u7f3a\u4e4f\u5bf9\u5b89\u5168\u6f0f\u6d1e\u7684\u7406\u89e3\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982Solidity\uff09\u4e2d\uff0c\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3002", "method": "\u57fa\u4e8eCodeLlama\u7684\u4e09\u9636\u6bb5\u5fae\u8c03\u65b9\u6cd5\uff0c\u5229\u7528\u6f0f\u6d1e\u548c\u5b89\u5168\u6807\u7b7e\u533a\u5206\u4ee3\u7801\u5b89\u5168\u6027\uff0c\u65e0\u9700\u6210\u5bf9\u6807\u6ce8\u3002", "result": "CodeBC\u5728BLEU\u3001CodeBLEU\u548c\u7f16\u8bd1\u901a\u8fc7\u7387\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u6f0f\u6d1e\u7387\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "CodeBC\u7684\u4e09\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\u9ad8\u6548\u4e14\u6210\u672c\u4f4e\uff0c\u4e3a\u751f\u6210\u5b89\u5168\u7684\u667a\u80fd\u5408\u7ea6\u4ee3\u7801\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2504.21383", "pdf": "https://arxiv.org/pdf/2504.21383", "abs": "https://arxiv.org/abs/2504.21383", "authors": ["Pulkit Agrawal", "Rukma Talwadker", "Aditya Pareek", "Tridib Mukherjee"], "title": "FAST-Q: Fast-track Exploration with Adversarially Balanced State Representations for Counterfactual Action Estimation in Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advancements in state-of-the-art (SOTA) offline reinforcement learning\n(RL) have primarily focused on addressing function approximation errors, which\ncontribute to the overestimation of Q-values for out-of-distribution actions, a\nchallenge that static datasets exacerbate. However, high stakes applications\nsuch as recommendation systems in online gaming, introduce further complexities\ndue to player's psychology (intent) driven by gameplay experiences and the\ninherent volatility on the platform. These factors create highly sparse,\npartially overlapping state spaces across policies, further influenced by the\nexperiment path selection logic which biases state spaces towards specific\npolicies. Current SOTA methods constrain learning from such offline data by\nclipping known counterfactual actions as out-of-distribution due to poor\ngeneralization across unobserved states. Further aggravating conservative\nQ-learning and necessitating more online exploration. FAST-Q introduces a novel\napproach that (1) leverages Gradient Reversal Learning to construct balanced\nstate representations, regularizing the policy-specific bias between the\nplayer's state and action thereby enabling counterfactual estimation; (2)\nsupports offline counterfactual exploration in parallel with static data\nexploitation; and (3) proposes a Q-value decomposition strategy for\nmulti-objective optimization, facilitating explainable recommendations over\nshort and long-term objectives. These innovations demonstrate superiority of\nFAST-Q over prior SOTA approaches and demonstrates at least 0.15 percent\nincrease in player returns, 2 percent improvement in lifetime value (LTV), 0.4\npercent enhancement in the recommendation driven engagement, 2 percent\nimprovement in the player's platform dwell time and an impressive 10 percent\nreduction in the costs associated with the recommendation, on our volatile\ngaming platform.", "AI": {"tldr": "FAST-Q\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u53cd\u8f6c\u5b66\u4e60\u548cQ\u503c\u5206\u89e3\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7a00\u758f\u72b6\u6001\u7a7a\u95f4\u548c\u73a9\u5bb6\u5fc3\u7406\u9a71\u52a8\u7684\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73a9\u5bb6\u6536\u76ca\u548c\u5e73\u53f0\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u7a00\u758f\u72b6\u6001\u7a7a\u95f4\u548c\u73a9\u5bb6\u5fc3\u7406\u9a71\u52a8\u7684\u63a8\u8350\u7cfb\u7edf\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "FAST-Q\u7ed3\u5408\u68af\u5ea6\u53cd\u8f6c\u5b66\u4e60\u6784\u5efa\u5e73\u8861\u72b6\u6001\u8868\u793a\uff0c\u652f\u6301\u79bb\u7ebf\u53cd\u4e8b\u5b9e\u63a2\u7d22\uff0c\u5e76\u63d0\u51faQ\u503c\u5206\u89e3\u7b56\u7565\u8fdb\u884c\u591a\u76ee\u6807\u4f18\u5316\u3002", "result": "FAST-Q\u5728\u73a9\u5bb6\u6536\u76ca\u3001\u5e73\u53f0\u505c\u7559\u65f6\u95f4\u548c\u63a8\u8350\u6210\u672c\u7b49\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u4f53\u8868\u73b0\u4e3a\u73a9\u5bb6\u6536\u76ca\u63d0\u53470.15%\uff0cLTV\u63d0\u53472%\uff0c\u63a8\u8350\u9a71\u52a8\u7684\u53c2\u4e0e\u5ea6\u63d0\u53470.4%\uff0c\u5e73\u53f0\u505c\u7559\u65f6\u95f4\u63d0\u53472%\uff0c\u63a8\u8350\u6210\u672c\u964d\u4f4e10%\u3002", "conclusion": "FAST-Q\u901a\u8fc7\u521b\u65b0\u7684\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u8350\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u7c7b\u4f3c\u9ad8\u6ce2\u52a8\u6027\u573a\u666f\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.21635", "pdf": "https://arxiv.org/pdf/2504.21635", "abs": "https://arxiv.org/abs/2504.21635", "authors": ["Zeina Aldallal", "Sara Chrouf", "Khalil Hennara", "Mohamed Motaism Hamed", "Muhammad Hreden", "Safwan AlModhayan"], "title": "Sadeed: Advancing Arabic Diacritization Through Small Language Model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Arabic text diacritization remains a persistent challenge in natural language\nprocessing due to the language's morphological richness. In this paper, we\nintroduce Sadeed, a novel approach based on a fine-tuned decoder-only language\nmodel adapted from Kuwain 1.5B Hennara et al. [2025], a compact model\noriginally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully\ncurated, high-quality diacritized datasets, constructed through a rigorous\ndata-cleaning and normalization pipeline. Despite utilizing modest\ncomputational resources, Sadeed achieves competitive results compared to\nproprietary large language models and outperforms traditional models trained on\nsimilar domains. Additionally, we highlight key limitations in current\nbenchmarking practices for Arabic diacritization. To address these issues, we\nintroduce SadeedDiac-25, a new benchmark designed to enable fairer and more\ncomprehensive evaluation across diverse text genres and complexity levels.\nTogether, Sadeed and SadeedDiac-25 provide a robust foundation for advancing\nArabic NLP applications, including machine translation, text-to-speech, and\nlanguage learning tools.", "AI": {"tldr": "Sadeed\u662f\u4e00\u79cd\u57fa\u4e8eKuwain 1.5B\u5fae\u8c03\u7684\u963f\u62c9\u4f2f\u8bed\u6587\u672c\u6807\u6ce8\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u6d4b\u57fa\u51c6SadeedDiac-25\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u6587\u672c\u6807\u6ce8\u56e0\u5f62\u6001\u4e30\u5bcc\u800c\u5177\u6311\u6218\u6027\uff0c\u9700\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eKuwain 1.5B\u5fae\u8c03\uff0c\u4f7f\u7528\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u6570\u636e\u6e05\u7406\u6d41\u7a0b\u3002", "result": "Sadeed\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u63a5\u8fd1\u4e13\u6709\u5927\u6a21\u578b\uff0c\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4f4e\u3002", "conclusion": "Sadeed\u4e0eSadeedDiac-25\u4e3a\u963f\u62c9\u4f2f\u8bedNLP\u5e94\u7528\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2504.21468", "pdf": "https://arxiv.org/pdf/2504.21468", "abs": "https://arxiv.org/abs/2504.21468", "authors": ["Yu Guo", "Guoqing Chen", "Tieyong Zeng", "Qiyu Jin", "Michael Kwok-Po Ng"], "title": "Quaternion Nuclear Norms Over Frobenius Norms Minimization for Robust Matrix Completion", "categories": ["cs.CV", "65F35, 90C30, 94A08, 68U10"], "comment": null, "summary": "Recovering hidden structures from incomplete or noisy data remains a\npervasive challenge across many fields, particularly where multi-dimensional\ndata representation is essential. Quaternion matrices, with their ability to\nnaturally model multi-dimensional data, offer a promising framework for this\nproblem. This paper introduces the quaternion nuclear norm over the Frobenius\nnorm (QNOF) as a novel nonconvex approximation for the rank of quaternion\nmatrices. QNOF is parameter-free and scale-invariant. Utilizing quaternion\nsingular value decomposition, we prove that solving the QNOF can be simplified\nto solving the singular value $L_1/L_2$ problem. Additionally, we extend the\nQNOF to robust quaternion matrix completion, employing the alternating\ndirection multiplier method to derive solutions that guarantee weak convergence\nunder mild conditions. Extensive numerical experiments validate the proposed\nmodel's superiority, consistently outperforming state-of-the-art quaternion\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u975e\u51f8\u8fd1\u4f3c\u65b9\u6cd5\uff08QNOF\uff09\u7528\u4e8e\u56db\u5143\u6570\u77e9\u9635\u7684\u79e9\u4f30\u8ba1\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u5230\u9c81\u68d2\u77e9\u9635\u8865\u5168\u95ee\u9898\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u591a\u7ef4\u6570\u636e\u8868\u793a\u4e2d\u9690\u85cf\u7ed3\u6784\u7684\u6062\u590d\u662f\u4e00\u4e2a\u666e\u904d\u6311\u6218\uff0c\u56db\u5143\u6570\u77e9\u9635\u4e3a\u6b64\u63d0\u4f9b\u4e86\u81ea\u7136\u5efa\u6a21\u6846\u67b6\u3002", "method": "\u5f15\u5165QNOF\u4f5c\u4e3a\u56db\u5143\u6570\u77e9\u9635\u79e9\u7684\u975e\u51f8\u8fd1\u4f3c\uff0c\u5229\u7528\u56db\u5143\u6570\u5947\u5f02\u503c\u5206\u89e3\u7b80\u5316\u6c42\u89e3\uff0c\u5e76\u6269\u5c55\u5230\u9c81\u68d2\u77e9\u9635\u8865\u5168\u95ee\u9898\u3002", "result": "QNOF\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "QNOF\u662f\u4e00\u79cd\u53c2\u6570\u65e0\u5173\u4e14\u5c3a\u5ea6\u4e0d\u53d8\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u7ef4\u6570\u636e\u6062\u590d\u95ee\u9898\u3002"}}
{"id": "2504.21044", "pdf": "https://arxiv.org/pdf/2504.21044", "abs": "https://arxiv.org/abs/2504.21044", "authors": ["Jianbo Gao", "Keke Gai", "Jing Yu", "Liehuang Zhu", "Qi Wu"], "title": "AGATE: Stealthy Black-box Watermarking for Multimodal Model Copyright Protection", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Recent advancement in large-scale Artificial Intelligence (AI) models\noffering multimodal services have become foundational in AI systems, making\nthem prime targets for model theft. Existing methods select Out-of-Distribution\n(OoD) data as backdoor watermarks and retrain the original model for copyright\nprotection. However, existing methods are susceptible to malicious detection\nand forgery by adversaries, resulting in watermark evasion. In this work, we\npropose Model-\\underline{ag}nostic Black-box Backdoor W\\underline{ate}rmarking\nFramework (AGATE) to address stealthiness and robustness challenges in\nmultimodal model copyright protection. Specifically, we propose an adversarial\ntrigger generation method to generate stealthy adversarial triggers from\nordinary dataset, providing visual fidelity while inducing semantic shifts. To\nalleviate the issue of anomaly detection among model outputs, we propose a\npost-transform module to correct the model output by narrowing the distance\nbetween adversarial trigger image embedding and text embedding. Subsequently, a\ntwo-phase watermark verification is proposed to judge whether the current model\ninfringes by comparing the two results with and without the transform module.\nConsequently, we consistently outperform state-of-the-art methods across five\ndatasets in the downstream tasks of multimodal image-text retrieval and image\nclassification. Additionally, we validated the robustness of AGATE under two\nadversarial attack scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAGATE\u7684\u9ed1\u76d2\u540e\u95e8\u6c34\u5370\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u6a21\u578b\u7248\u6743\u4fdd\u62a4\u4e2d\u7684\u9690\u853d\u6027\u548c\u9c81\u68d2\u6027\u95ee\u9898\u3002\u901a\u8fc7\u751f\u6210\u9690\u853d\u7684\u5bf9\u6297\u89e6\u53d1\u5668\u5e76\u5f15\u5165\u540e\u53d8\u6362\u6a21\u5757\uff0cAGATE\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u6613\u53d7\u6076\u610f\u68c0\u6d4b\u548c\u4f2a\u9020\u653b\u51fb\uff0c\u5bfc\u81f4\u6c34\u5370\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9690\u853d\u4e14\u9c81\u68d2\u7684\u7248\u6743\u4fdd\u62a4\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5bf9\u6297\u89e6\u53d1\u5668\u751f\u6210\u65b9\u6cd5\uff0c\u751f\u6210\u9690\u853d\u7684\u5bf9\u6297\u89e6\u53d1\u5668\uff1b\u5f15\u5165\u540e\u53d8\u6362\u6a21\u5757\u4ee5\u7ea0\u6b63\u6a21\u578b\u8f93\u51fa\uff1b\u8bbe\u8ba1\u4e24\u9636\u6bb5\u6c34\u5370\u9a8c\u8bc1\u673a\u5236\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u7684\u591a\u6a21\u6001\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cAGATE\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5bf9\u6297\u653b\u51fb\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "AGATE\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u7248\u6743\u4fdd\u62a4\u63d0\u4f9b\u4e86\u4e00\u79cd\u9690\u853d\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21389", "pdf": "https://arxiv.org/pdf/2504.21389", "abs": "https://arxiv.org/abs/2504.21389", "authors": ["Jianyu Zhang", "Jianshe Feng", "Yizhang Zhu", "Fanyu Qi"], "title": "Enhanced Semi-Supervised Stamping Process Monitoring with Physically-Informed Feature Extraction", "categories": ["cs.LG"], "comment": "19 pages, 14 figures", "summary": "In tackling frequent anomalies in stamping processes, this study introduces a\nnovel semi-supervised in-process anomaly monitoring framework, utilizing\naccelerometer signals and physics information, to capture the process anomaly\neffectively. The proposed framework facilitates the construction of a\nmonitoring model with imbalanced sample distribution, which enables in-process\ncondition monitoring in real-time to prevent batch anomalies, which helps to\nreduce batch defects risk and enhance production yield. Firstly, to effectively\ncapture key features from raw data containing redundant information, a hybrid\nfeature extraction algorithm is proposed to utilize data-driven methods and\nphysical mechanisms simultaneously. Secondly, to address the challenge brought\nby imbalanced sample distribution, a semi-supervised anomaly detection model is\nestablished, which merely employs normal samples to build a golden baseline\nmodel, and a novel deviation score is proposed to quantify the anomaly level of\neach online stamping stroke. The effectiveness of the proposed feature\nextraction method is validated with various classification algorithms. A\nreal-world in-process dataset from stamping manufacturing workshop is employed\nto illustrate the superiority of proposed semi-supervised framework with\nenhance performance for process anomaly monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u7684\u51b2\u538b\u8fc7\u7a0b\u5f02\u5e38\u76d1\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u52a0\u901f\u5ea6\u4fe1\u53f7\u548c\u7269\u7406\u4fe1\u606f\uff0c\u6709\u6548\u6355\u6349\u5f02\u5e38\uff0c\u51cf\u5c11\u6279\u91cf\u7f3a\u9677\u98ce\u9669\u5e76\u63d0\u9ad8\u4ea7\u91cf\u3002", "motivation": "\u89e3\u51b3\u51b2\u538b\u8fc7\u7a0b\u4e2d\u9891\u7e41\u51fa\u73b0\u7684\u5f02\u5e38\u95ee\u9898\uff0c\u63d0\u9ad8\u751f\u4ea7\u6548\u7387\u548c\u4ea7\u54c1\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u7279\u5f81\u63d0\u53d6\u7b97\u6cd5\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u548c\u7269\u7406\u673a\u5236\uff1b\u5efa\u7acb\u534a\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u4ec5\u7528\u6b63\u5e38\u6837\u672c\u6784\u5efa\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u65b0\u7684\u504f\u5dee\u8bc4\u5206\u91cf\u5316\u5f02\u5e38\u7a0b\u5ea6\u3002", "result": "\u9a8c\u8bc1\u4e86\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u51b2\u538b\u8f66\u95f4\u6570\u636e\u4e2d\u5c55\u793a\u4e86\u6846\u67b6\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u76d1\u6d4b\u51b2\u538b\u8fc7\u7a0b\u5f02\u5e38\uff0c\u63d0\u5347\u751f\u4ea7\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2504.21677", "pdf": "https://arxiv.org/pdf/2504.21677", "abs": "https://arxiv.org/abs/2504.21677", "authors": ["Michelle Wastl", "Jannis Vamvas", "Selena Calleri", "Rico Sennrich"], "title": "20min-XD: A Comparable Corpus of Swiss News Articles", "categories": ["cs.CL"], "comment": "10 pages; accepted at SwissText 2025", "summary": "We present 20min-XD (20 Minuten cross-lingual document-level), a\nFrench-German, document-level comparable corpus of news articles, sourced from\nthe Swiss online news outlet 20 Minuten/20 minutes. Our dataset comprises\naround 15,000 article pairs spanning 2015 to 2024, automatically aligned based\non semantic similarity. We detail the data collection process and alignment\nmethodology. Furthermore, we provide a qualitative and quantitative analysis of\nthe corpus. The resulting dataset exhibits a broad spectrum of cross-lingual\nsimilarity, ranging from near-translations to loosely related articles, making\nit valuable for various NLP applications and broad linguistically motivated\nstudies. We publicly release the dataset in document- and sentence-aligned\nversions and code for the described experiments.", "AI": {"tldr": "20min-XD\u662f\u4e00\u4e2a\u6cd5\u5fb7\u53cc\u8bed\u65b0\u95fb\u8bed\u6599\u5e93\uff0c\u5305\u542b15,000\u5bf9\u6587\u7ae0\uff0c\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u81ea\u52a8\u5bf9\u9f50\uff0c\u9002\u7528\u4e8eNLP\u7814\u7a76\u548c\u8bed\u8a00\u5b66\u5206\u6790\u3002", "motivation": "\u6784\u5efa\u4e00\u4e2a\u8de8\u8bed\u8a00\u3001\u6587\u6863\u7ea7\u522b\u7684\u53ef\u6bd4\u8bed\u6599\u5e93\uff0c\u4ee5\u652f\u6301\u591a\u79cdNLP\u5e94\u7528\u548c\u8bed\u8a00\u5b66\u7814\u7a76\u7684\u9700\u8981\u3002", "method": "\u4ece\u745e\u58eb\u65b0\u95fb\u7f51\u7ad920 Minuten/20 minutes\u6536\u96c62015\u81f32024\u5e74\u7684\u6587\u7ae0\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u5ea6\u81ea\u52a8\u5bf9\u9f50\u3002", "result": "\u8bed\u6599\u5e93\u5305\u542b\u4ece\u8fd1\u4f3c\u7ffb\u8bd1\u5230\u677e\u6563\u76f8\u5173\u6587\u7ae0\u7684\u5e7f\u6cdb\u8de8\u8bed\u8a00\u76f8\u4f3c\u6027\uff0c\u516c\u5f00\u4e86\u6587\u6863\u548c\u53e5\u5b50\u5bf9\u9f50\u7248\u672c\u53ca\u76f8\u5173\u4ee3\u7801\u3002", "conclusion": "20min-XD\u662f\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u9002\u7528\u4e8eNLP\u548c\u8bed\u8a00\u5b66\u7814\u7a76\u7684\u591a\u6837\u5316\u9700\u6c42\u3002"}}
{"id": "2504.21472", "pdf": "https://arxiv.org/pdf/2504.21472", "abs": "https://arxiv.org/abs/2504.21472", "authors": ["Jingjing Liu", "Nian Wu", "Xianchao Xiu", "Jianhua Zhang"], "title": "Robust Orthogonal NMF with Label Propagation for Image Clustering", "categories": ["cs.CV"], "comment": null, "summary": "Non-negative matrix factorization (NMF) is a popular unsupervised learning\napproach widely used in image clustering. However, in real-world clustering\nscenarios, most existing NMF methods are highly sensitive to noise corruption\nand are unable to effectively leverage limited supervised information. To\novercome these drawbacks, we propose a unified non-convex framework with label\npropagation called robust orthogonal nonnegative matrix factorization (RONMF).\nThis method not only considers the graph Laplacian and label propagation as\nregularization terms but also introduces a more effective non-convex structure\nto measure the reconstruction error and imposes orthogonal constraints on the\nbasis matrix to reduce the noise corruption, thereby achieving higher\nrobustness. To solve RONMF, we develop an alternating direction method of\nmultipliers (ADMM)-based optimization algorithm. In particular, all subproblems\nhave closed-form solutions, which ensures its efficiency. Experimental\nevaluations on eight public image datasets demonstrate that the proposed RONMF\noutperforms state-of-the-art NMF methods across various standard metrics and\nshows excellent robustness. The code will be available at\nhttps://github.com/slinda-liu.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRONMF\u7684\u9c81\u68d2\u6b63\u4ea4\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u62c9\u666e\u62c9\u65af\u548c\u6807\u7b7e\u4f20\u64ad\u4f5c\u4e3a\u6b63\u5219\u9879\uff0c\u5e76\u5f15\u5165\u975e\u51f8\u7ed3\u6784\u548c\u6b63\u4ea4\u7ea6\u675f\uff0c\u63d0\u9ad8\u4e86\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709NMF\u65b9\u6cd5\u5bf9\u566a\u58f0\u654f\u611f\u4e14\u96be\u4ee5\u5229\u7528\u6709\u9650\u7684\u76d1\u7763\u4fe1\u606f\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faRONMF\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u62c9\u666e\u62c9\u65af\u3001\u6807\u7b7e\u4f20\u64ad\u3001\u975e\u51f8\u7ed3\u6784\u548c\u6b63\u4ea4\u7ea6\u675f\uff0c\u5e76\u4f7f\u7528ADMM\u7b97\u6cd5\u4f18\u5316\u3002", "result": "\u5728\u516b\u4e2a\u516c\u5f00\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cRONMF\u5728\u591a\u79cd\u6807\u51c6\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709NMF\u65b9\u6cd5\uff0c\u4e14\u8868\u73b0\u51fa\u4f18\u79c0\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "RONMF\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684NMF\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u56fe\u50cf\u805a\u7c7b\u4efb\u52a1\u3002"}}
{"id": "2504.21045", "pdf": "https://arxiv.org/pdf/2504.21045", "abs": "https://arxiv.org/abs/2504.21045", "authors": ["Dennis Miczek", "Divyesh Gabbireddy", "Suman Saha"], "title": "Leveraging LLM to Strengthen ML-Based Cross-Site Scripting Detection", "categories": ["cs.CR", "cs.AI"], "comment": "This work has been accepted for presentation at the ACM Workshop on\n  Wireless Security and Machine Learning (WiseML 2025)", "summary": "According to the Open Web Application Security Project (OWASP), Cross-Site\nScripting (XSS) is a critical security vulnerability. Despite decades of\nresearch, XSS remains among the top 10 security vulnerabilities. Researchers\nhave proposed various techniques to protect systems from XSS attacks, with\nmachine learning (ML) being one of the most widely used methods. An ML model is\ntrained on a dataset to identify potential XSS threats, making its\neffectiveness highly dependent on the size and diversity of the training data.\nA variation of XSS is obfuscated XSS, where attackers apply obfuscation\ntechniques to alter the code's structure, making it challenging for security\nsystems to detect its malicious intent. Our study's random forest model was\ntrained on traditional (non-obfuscated) XSS data achieved 99.8% accuracy.\nHowever, when tested against obfuscated XSS samples, accuracy dropped to 81.9%,\nunderscoring the importance of training ML models with obfuscated data to\nimprove their effectiveness in detecting XSS attacks. A significant challenge\nis to generate highly complex obfuscated code despite the availability of\nseveral public tools. These tools can only produce obfuscation up to certain\nlevels of complexity.\n  In our proposed system, we fine-tune a Large Language Model (LLM) to generate\ncomplex obfuscated XSS payloads automatically. By transforming original XSS\nsamples into diverse obfuscated variants, we create challenging training data\nfor ML model evaluation. Our approach achieved a 99.5% accuracy rate with the\nobfuscated dataset. We also found that the obfuscated samples generated by the\nLLMs were 28.1% more complex than those created by other tools, significantly\nimproving the model's ability to handle advanced XSS attacks and making it more\neffective for real-world application security.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u590d\u6742\u6df7\u6dc6XSS\u653b\u51fb\u8f7d\u8377\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u68c0\u6d4b\u6df7\u6dc6XSS\u653b\u51fb\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u591a\u79cd\u6280\u672f\u7528\u4e8e\u9632\u5fa1XSS\u653b\u51fb\uff0c\u4f46\u6df7\u6dc6XSS\u653b\u51fb\u4ecd\u96be\u4ee5\u68c0\u6d4b\uff0c\u4e14\u73b0\u6709\u5de5\u5177\u751f\u6210\u7684\u6df7\u6dc6\u4ee3\u7801\u590d\u6742\u5ea6\u6709\u9650\u3002", "method": "\u901a\u8fc7\u5fae\u8c03LLM\u81ea\u52a8\u751f\u6210\u590d\u6742\u6df7\u6dc6XSS\u8f7d\u8377\uff0c\u5e76\u7528\u4e8e\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u4f7f\u7528LLM\u751f\u6210\u7684\u6df7\u6dc6\u6570\u636e\u96c6\u8bad\u7ec3\u540e\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8fbe\u523099.5%\uff0c\u4e14\u751f\u6210\u7684\u6837\u672c\u590d\u6742\u5ea6\u6bd4\u73b0\u6709\u5de5\u5177\u9ad828.1%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u9ad8\u7ea7XSS\u653b\u51fb\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u5b89\u5168\u3002"}}
{"id": "2504.21427", "pdf": "https://arxiv.org/pdf/2504.21427", "abs": "https://arxiv.org/abs/2504.21427", "authors": ["Shermin Shahbazi", "Mohammad-Reza Nasiri", "Majid Ramezani"], "title": "MPEC: Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers", "categories": ["cs.LG", "cs.AI"], "comment": "7 pages ,3 figures", "summary": "Accurate classification of EEG signals is crucial for brain-computer\ninterfaces (BCIs) and neuroprosthetic applications, yet many existing methods\nfail to account for the non-Euclidean, manifold structure of EEG data,\nresulting in suboptimal performance. Preserving this manifold information is\nessential to capture the true geometry of EEG signals, but traditional\nclassification techniques largely overlook this need. To this end, we propose\nMPEC (Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based\nClassifiers), that introduces two key innovations: (1) a feature engineering\nphase that combines covariance matrices and Radial Basis Function (RBF) kernels\nto capture both linear and non-linear relationships among EEG channels, and (2)\na clustering phase that employs a modified K-means algorithm tailored for the\nRiemannian manifold space, ensuring local geometric sensitivity. Ensembling\nmultiple clustering-based classifiers, MPEC achieves superior results,\nvalidated by significant improvements on the BCI Competition IV dataset 2a.", "AI": {"tldr": "MPEC\u65b9\u6cd5\u901a\u8fc7\u4fdd\u7559EEG\u6570\u636e\u7684\u6d41\u5f62\u7ed3\u6784\uff0c\u7ed3\u5408\u534f\u65b9\u5dee\u77e9\u9635\u548cRBF\u6838\u7684\u7279\u5f81\u5de5\u7a0b\uff0c\u4ee5\u53ca\u6539\u8fdb\u7684K-means\u805a\u7c7b\uff0c\u663e\u8457\u63d0\u5347\u4e86EEG\u4fe1\u53f7\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709EEG\u4fe1\u53f7\u5206\u7c7b\u65b9\u6cd5\u672a\u80fd\u8003\u8651\u6570\u636e\u7684\u975e\u6b27\u51e0\u91cc\u5f97\u6d41\u5f62\u7ed3\u6784\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "MPEC\u7ed3\u5408\u534f\u65b9\u5dee\u77e9\u9635\u548cRBF\u6838\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u4f7f\u7528\u6539\u8fdb\u7684K-means\u7b97\u6cd5\u5728\u9ece\u66fc\u6d41\u5f62\u7a7a\u95f4\u805a\u7c7b\uff0c\u6700\u540e\u96c6\u6210\u591a\u4e2a\u5206\u7c7b\u5668\u3002", "result": "\u5728BCI Competition IV\u6570\u636e\u96c62a\u4e0a\u9a8c\u8bc1\u4e86MPEC\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "MPEC\u901a\u8fc7\u4fdd\u7559\u6d41\u5f62\u7ed3\u6784\u548c\u96c6\u6210\u5206\u7c7b\u5668\uff0c\u6709\u6548\u63d0\u5347\u4e86EEG\u4fe1\u53f7\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2504.21681", "pdf": "https://arxiv.org/pdf/2504.21681", "abs": "https://arxiv.org/abs/2504.21681", "authors": ["Andrei-Alexandru Manea", "Jind\u0159ich Libovick\u00fd"], "title": "Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders", "categories": ["cs.CL"], "comment": null, "summary": "Most pre-trained Vision-Language (VL) models and training data for the\ndownstream tasks are only available in English. Therefore, multilingual VL\ntasks are solved using cross-lingual transfer: fine-tune a multilingual\npre-trained model or transfer the text encoder using parallel data. We study\nthe alternative approach: transferring an already trained encoder using\nparallel data. We investigate the effect of parallel data: domain and the\nnumber of languages, which were out of focus in previous work. Our results show\nthat even machine-translated task data are the best on average, caption-like\nauthentic parallel data outperformed it in some languages. Further, we show\nthat most languages benefit from multilingual training.", "AI": {"tldr": "\u7814\u7a76\u591a\u8bed\u8a00\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u5e76\u884c\u6570\u636e\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8fc1\u79fb\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u673a\u5668\u7ffb\u8bd1\u7684\u4efb\u52a1\u6570\u636e\u6548\u679c\u6700\u4f73\uff0c\u4f46\u67d0\u4e9b\u8bed\u8a00\u4e2d\u771f\u5b9e\u5e73\u884c\u6570\u636e\u8868\u73b0\u66f4\u597d\uff0c\u4e14\u591a\u8bed\u8a00\u8bad\u7ec3\u5bf9\u5927\u591a\u6570\u8bed\u8a00\u6709\u76ca\u3002", "motivation": "\u73b0\u6709\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u53ca\u4e0b\u6e38\u4efb\u52a1\u6570\u636e\u591a\u4e3a\u82f1\u6587\uff0c\u591a\u8bed\u8a00\u4efb\u52a1\u9700\u4f9d\u8d56\u8de8\u8bed\u8a00\u8fc1\u79fb\uff0c\u4f46\u5e76\u884c\u6570\u636e\u7684\u5f71\u54cd\uff08\u5982\u9886\u57df\u548c\u8bed\u8a00\u6570\u91cf\uff09\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5e76\u884c\u6570\u636e\u8fc1\u79fb\u5df2\u8bad\u7ec3\u7f16\u7801\u5668\uff0c\u5206\u6790\u5e76\u884c\u6570\u636e\u7684\u9886\u57df\u548c\u8bed\u8a00\u6570\u91cf\u5bf9\u8fc1\u79fb\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u673a\u5668\u7ffb\u8bd1\u7684\u4efb\u52a1\u6570\u636e\u5e73\u5747\u6548\u679c\u6700\u4f73\uff0c\u4f46\u67d0\u4e9b\u8bed\u8a00\u4e2d\u771f\u5b9e\u5e73\u884c\u6570\u636e\u8868\u73b0\u66f4\u4f18\uff1b\u591a\u8bed\u8a00\u8bad\u7ec3\u5bf9\u5927\u591a\u6570\u8bed\u8a00\u6709\u76ca\u3002", "conclusion": "\u5e76\u884c\u6570\u636e\u7684\u9886\u57df\u548c\u8bed\u8a00\u6570\u91cf\u5bf9\u8fc1\u79fb\u6548\u679c\u6709\u663e\u8457\u5f71\u54cd\uff0c\u591a\u8bed\u8a00\u8bad\u7ec3\u662f\u63d0\u5347\u591a\u8bed\u8a00\u4efb\u52a1\u6027\u80fd\u7684\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2504.21476", "pdf": "https://arxiv.org/pdf/2504.21476", "abs": "https://arxiv.org/abs/2504.21476", "authors": ["Xinyu Li", "Qi Yao", "Yuanda Wang"], "title": "GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": "The 34th International Joint Conference on Artificial Intelligence\n  (IJCAI 2025)", "summary": "Garment sewing patterns are fundamental design elements that bridge the gap\nbetween design concepts and practical manufacturing. The generative modeling of\nsewing patterns is crucial for creating diversified garments. However, existing\napproaches are limited either by reliance on a single input modality or by\nsuboptimal generation efficiency. In this work, we present\n\\textbf{\\textit{GarmentDiffusion}}, a new generative model capable of producing\ncentimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text,\nimage, and incomplete sewing pattern). Our method efficiently encodes 3D sewing\npattern parameters into compact edge token representations, achieving a\nsequence length that is $\\textbf{10}\\times$ shorter than that of the\nautoregressive SewingGPT in DressCode. By employing a diffusion transformer, we\nsimultaneously denoise all edge tokens along the temporal axis, while\nmaintaining a constant number of denoising steps regardless of dataset-specific\nedge and panel statistics. With all combination of designs of our model, the\nsewing pattern generation speed is accelerated by $\\textbf{100}\\times$ compared\nto SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well\nas on the largest sewing pattern dataset, namely GarmentCodeData. The project\nwebsite is available at https://shenfu-research.github.io/Garment-Diffusion/.", "AI": {"tldr": "GarmentDiffusion\u662f\u4e00\u79cd\u65b0\u578b\u751f\u6210\u6a21\u578b\uff0c\u80fd\u591f\u4ece\u591a\u6a21\u6001\u8f93\u5165\uff08\u6587\u672c\u3001\u56fe\u50cf\u548c\u4e0d\u5b8c\u6574\u7f1d\u7eab\u56fe\u6848\uff09\u751f\u6210\u5398\u7c73\u7ea7\u7cbe\u5ea6\u7684\u77e2\u91cf3D\u7f1d\u7eab\u56fe\u6848\uff0c\u6548\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8100\u500d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u591a\u6837\u5316\u7f1d\u7eab\u56fe\u6848\u65f6\uff0c\u8981\u4e48\u4f9d\u8d56\u5355\u4e00\u8f93\u5165\u6a21\u6001\uff0c\u8981\u4e48\u751f\u6210\u6548\u7387\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5c063D\u7f1d\u7eab\u56fe\u6848\u53c2\u6570\u7f16\u7801\u4e3a\u7d27\u51d1\u7684\u8fb9\u7f18\u6807\u8bb0\u8868\u793a\uff0c\u5e76\u4f7f\u7528\u6269\u6563\u53d8\u6362\u5668\u540c\u65f6\u53bb\u566a\u6240\u6709\u8fb9\u7f18\u6807\u8bb0\uff0c\u5b9e\u73b0\u9ad8\u6548\u751f\u6210\u3002", "result": "\u5728DressCodeData\u548cGarmentCodeData\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u65b0\u6700\u4f18\u7ed3\u679c\uff0c\u751f\u6210\u901f\u5ea6\u6bd4SewingGPT\u5feb100\u500d\u3002", "conclusion": "GarmentDiffusion\u4e3a\u7f1d\u7eab\u56fe\u6848\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u591a\u6a21\u6001\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u901f\u5ea6\u548c\u7cbe\u5ea6\u3002"}}
{"id": "2504.21436", "pdf": "https://arxiv.org/pdf/2504.21436", "abs": "https://arxiv.org/abs/2504.21436", "authors": ["Zhixuan Ma", "Haichang Gao", "Junxiang Huang", "Ping Wang"], "title": "Whispers of Data: Unveiling Label Distributions in Federated Learning Through Virtual Client Simulation", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Federated Learning enables collaborative training of a global model across\nmultiple geographically dispersed clients without the need for data sharing.\nHowever, it is susceptible to inference attacks, particularly label inference\nattacks.\n  Existing studies on label distribution inference exhibits sensitive to the\nspecific settings of the victim client and typically underperforms under\ndefensive strategies. In this study, we propose a novel label distribution\ninference attack that is stable and adaptable to various scenarios.\nSpecifically, we estimate the size of the victim client's dataset and construct\nseveral virtual clients tailored to the victim client. We then quantify the\ntemporal generalization of each class label for the virtual clients and utilize\nthe variation in temporal generalization to train an inference model that\npredicts the label distribution proportions of the victim client.\n  We validate our approach on multiple datasets, including MNIST,\nFashion-MNIST, FER2013, and AG-News. The results demonstrate the superiority of\nour method compared to state-of-the-art techniques. Furthermore, our attack\nremains effective even under differential privacy defense mechanisms,\nunderscoring its potential for real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u6807\u7b7e\u5206\u5e03\u63a8\u65ad\u653b\u51fb\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\uff0c\u4e14\u5728\u5dee\u5206\u9690\u79c1\u9632\u5fa1\u4e0b\u4ecd\u6709\u6548\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u6807\u7b7e\u63a8\u65ad\u653b\u51fb\u5bf9\u53d7\u5bb3\u8005\u5ba2\u6237\u7aef\u8bbe\u7f6e\u654f\u611f\u4e14\u9632\u5fa1\u7b56\u7565\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u66f4\u7a33\u5b9a\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u65b9\u6cd5\u3002", "method": "\u4f30\u8ba1\u53d7\u5bb3\u8005\u6570\u636e\u96c6\u5927\u5c0f\uff0c\u6784\u5efa\u865a\u62df\u5ba2\u6237\u7aef\uff0c\u91cf\u5316\u6807\u7b7e\u65f6\u95f4\u6cdb\u5316\u6027\uff0c\u8bad\u7ec3\u63a8\u65ad\u6a21\u578b\u9884\u6d4b\u6807\u7b7e\u5206\u5e03\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u5728\u5dee\u5206\u9690\u79c1\u9632\u5fa1\u4e0b\u4ecd\u6709\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7a33\u5b9a\u3001\u9002\u5e94\u6027\u5f3a\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.21685", "pdf": "https://arxiv.org/pdf/2504.21685", "abs": "https://arxiv.org/abs/2504.21685", "authors": ["Reem Abdel-Salam", "Mary Adewunmi"], "title": "Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Health Mention Classification (HMC) plays a critical role in leveraging\nsocial media posts for real-time tracking and public health monitoring.\nNevertheless, the process of HMC presents significant challenges due to its\nintricate nature, primarily stemming from the contextual aspects of health\nmentions, such as figurative language and descriptive terminology, rather than\nexplicitly reflecting a personal ailment. To address this problem, we argue\nthat clearer mentions can be achieved through conventional fine-tuning with\nenhanced parameters of biomedical natural language methods (NLP). In this\nstudy, we explore different techniques such as the utilisation of\npart-of-speech (POS) tagger information, improving on PEFT techniques, and\ndifferent combinations thereof. Extensive experiments are conducted on three\nwidely used datasets: RHDM, PHM, and Illness. The results incorporated POS\ntagger information, and leveraging PEFT techniques significantly improves\nperformance in terms of F1-score compared to state-of-the-art methods across\nall three datasets by utilising smaller models and efficient training.\nFurthermore, the findings highlight the effectiveness of incorporating POS\ntagger information and leveraging PEFT techniques for HMC. In conclusion, the\nproposed methodology presents a potentially effective approach to accurately\nclassifying health mentions in social media posts while optimising the model\nsize and training efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bcd\u6027\u6807\u6ce8\u4fe1\u606f\uff08POS\uff09\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff08PEFT\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u793e\u4ea4\u5a92\u4f53\u4e2d\u5065\u5eb7\u63d0\u53ca\u5206\u7c7b\uff08HMC\uff09\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5065\u5eb7\u63d0\u53ca\u5206\u7c7b\u5728\u5b9e\u65f6\u8ffd\u8e2a\u548c\u516c\u5171\u536b\u751f\u76d1\u6d4b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5176\u590d\u6742\u6027\uff08\u5982\u6bd4\u55bb\u6027\u8bed\u8a00\u548c\u63cf\u8ff0\u6027\u672f\u8bed\uff09\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "method": "\u91c7\u7528\u8bcd\u6027\u6807\u6ce8\u4fe1\u606f\uff08POS\uff09\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff08PEFT\uff09\u7684\u7ec4\u5408\u65b9\u6cd5\uff0c\u5e76\u5728RHDM\u3001PHM\u548cIllness\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408POS\u548cPEFT\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86F1\u5206\u6570\uff0c\u540c\u65f6\u4f7f\u7528\u66f4\u5c0f\u7684\u6a21\u578b\u548c\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u5065\u5eb7\u63d0\u53ca\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21478", "pdf": "https://arxiv.org/pdf/2504.21478", "abs": "https://arxiv.org/abs/2504.21478", "authors": ["Zherui Zhang", "Changwei Wang", "Rongtao Xu", "Wenhao Xu", "Shibiao Xu", "Yu Zhang", "Li Guo"], "title": "CAE-DFKD: Bridging the Transferability Gap in Data-Free Knowledge Distillation", "categories": ["cs.CV", "cs.NE"], "comment": null, "summary": "Data-Free Knowledge Distillation (DFKD) enables the knowledge transfer from\nthe given pre-trained teacher network to the target student model without\naccess to the real training data. Existing DFKD methods focus primarily on\nimproving image recognition performance on associated datasets, often\nneglecting the crucial aspect of the transferability of learned\nrepresentations. In this paper, we propose Category-Aware Embedding Data-Free\nKnowledge Distillation (CAE-DFKD), which addresses at the embedding level the\nlimitations of previous rely on image-level methods to improve model\ngeneralization but fail when directly applied to DFKD. The superiority and\nflexibility of CAE-DFKD are extensively evaluated, including:\n\\textit{\\textbf{i.)}} Significant efficiency advantages resulting from altering\nthe generator training paradigm; \\textit{\\textbf{ii.)}} Competitive performance\nwith existing DFKD state-of-the-art methods on image recognition tasks;\n\\textit{\\textbf{iii.)}} Remarkable transferability of data-free learned\nrepresentations demonstrated in downstream tasks.", "AI": {"tldr": "CAE-DFKD\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u6570\u636e\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5d4c\u5165\u7ea7\u522b\u7684\u6539\u8fdb\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u6548\u7387\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709DFKD\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u56fe\u50cf\u8bc6\u522b\u6027\u80fd\uff0c\u5ffd\u7565\u4e86\u5b66\u4e60\u8868\u793a\u7684\u53ef\u8fc1\u79fb\u6027\u3002CAE-DFKD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "CAE-DFKD\u5728\u5d4c\u5165\u7ea7\u522b\u6539\u8fdb\u751f\u6210\u5668\u8bad\u7ec3\u8303\u5f0f\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "CAE-DFKD\u5728\u6548\u7387\u3001\u56fe\u50cf\u8bc6\u522b\u6027\u80fd\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CAE-DFKD\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u65e0\u6570\u636e\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u3002"}}
{"id": "2504.21048", "pdf": "https://arxiv.org/pdf/2504.21048", "abs": "https://arxiv.org/abs/2504.21048", "authors": ["Mohamad A. Hady", "Siyi Hu", "Mahardhika Pratama", "Jimmy Cao", "Ryszard Kowalczyk"], "title": "Multi-Agent Reinforcement Learning for Resources Allocation Optimization: A Survey", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) has become a powerful framework for\nnumerous real-world applications, modeling distributed decision-making and\nlearning from interactions with complex environments. Resource Allocation\nOptimization (RAO) benefits significantly from MARL's ability to tackle dynamic\nand decentralized contexts. MARL-based approaches are increasingly applied to\nRAO challenges across sectors playing pivotal roles to Industry 4.0\ndevelopments. This survey provides a comprehensive review of recent MARL\nalgorithms for RAO, encompassing core concepts, classifications, and a\nstructured taxonomy. By outlining the current research landscape and\nidentifying primary challenges and future directions, this survey aims to\nsupport researchers and practitioners in leveraging MARL's potential to advance\nresource allocation solutions.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u8d44\u6e90\u5206\u914d\u4f18\u5316\uff08RAO\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u4e86\u6838\u5fc3\u6982\u5ff5\u3001\u5206\u7c7b\u548c\u5206\u7c7b\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "MARL\u80fd\u591f\u5904\u7406\u52a8\u6001\u548c\u53bb\u4e2d\u5fc3\u5316\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5bf9\u5de5\u4e1a4.0\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9bMARL\u5728RAO\u4e2d\u7684\u6f5c\u529b\u4e0e\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u8fd1\u671fMARL\u7b97\u6cd5\uff0c\u6db5\u76d6\u6838\u5fc3\u6982\u5ff5\u3001\u5206\u7c7b\u548c\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u7814\u7a76\u73b0\u72b6\u3002", "result": "\u603b\u7ed3\u4e86MARL\u5728RAO\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u5e76\u8bc6\u522b\u4e86\u4e3b\u8981\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "MARL\u5728RAO\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u672c\u6587\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u53c2\u8003\u548c\u6307\u5bfc\u3002"}}
{"id": "2504.21457", "pdf": "https://arxiv.org/pdf/2504.21457", "abs": "https://arxiv.org/abs/2504.21457", "authors": ["Andrea Zanola", "Louis Fabrice Tshimanga", "Federico Del Pup", "Marco Baiesi", "Manfredo Atzori"], "title": "xEEGNet: Towards Explainable AI in EEG Dementia Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This work presents xEEGNet, a novel, compact, and explainable neural network\nfor EEG data analysis. It is fully interpretable and reduces overfitting\nthrough major parameter reduction. As an applicative use case, we focused on\nclassifying common dementia conditions, Alzheimer's and frontotemporal\ndementia, versus controls. xEEGNet is broadly applicable to other neurological\nconditions involving spectral alterations. We initially used ShallowNet, a\nsimple and popular model from the EEGNet-family. Its structure was analyzed and\ngradually modified to move from a \"black box\" to a more transparent model,\nwithout compromising performance. The learned kernels and weights were examined\nfrom a clinical standpoint to assess medical relevance. Model variants,\nincluding ShallowNet and the final xEEGNet, were evaluated using robust\nNested-Leave-N-Subjects-Out cross-validation for unbiased performance\nestimates. Variability across data splits was explained using embedded EEG\nrepresentations, grouped by class and set, with pairwise separability to\nquantify group distinction. Overfitting was assessed through\ntraining-validation loss correlation and training speed. xEEGNet uses only 168\nparameters, 200 times fewer than ShallowNet, yet retains interpretability,\nresists overfitting, achieves comparable median performance (-1.5%), and\nreduces variability across splits. This variability is explained by embedded\nEEG representations: higher accuracy correlates with greater separation between\ntest set controls and Alzheimer's cases, without significant influence from\ntraining data. xEEGNet's ability to filter specific EEG bands, learn\nband-specific topographies, and use relevant spectral features demonstrates its\ninterpretability. While large deep learning models are often prioritized for\nperformance, this study shows smaller architectures like xEEGNet can be equally\neffective in EEG pathology classification.", "AI": {"tldr": "xEEGNet\u662f\u4e00\u79cd\u65b0\u578b\u3001\u7d27\u51d1\u4e14\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8eEEG\u6570\u636e\u5206\u6790\uff0c\u5177\u6709\u53c2\u6570\u5c11\u3001\u6297\u8fc7\u62df\u5408\u548c\u53ef\u89e3\u91ca\u6027\u5f3a\u7684\u7279\u70b9\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u5206\u7c7b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728EEG\u6570\u636e\u5206\u6790\u4e2d\u53c2\u6570\u8fc7\u591a\u3001\u96be\u4ee5\u89e3\u91ca\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5e76\u6539\u8fdbShallowNet\u7ed3\u6784\uff0c\u9010\u6b65\u5b9e\u73b0\u4ece\u201c\u9ed1\u7bb1\u201d\u5230\u900f\u660e\u6a21\u578b\u7684\u8f6c\u53d8\uff0c\u4f7f\u7528Nested-Leave-N-Subjects-Out\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\u6027\u80fd\u3002", "result": "xEEGNet\u4ec5\u7528168\u4e2a\u53c2\u6570\uff08\u6bd4ShallowNet\u5c11200\u500d\uff09\uff0c\u6027\u80fd\u76f8\u8fd1\uff08\u4e2d\u4f4d\u6570\u4e0b\u964d1.5%\uff09\uff0c\u4e14\u80fd\u6709\u6548\u51cf\u5c11\u6570\u636e\u5206\u5272\u95f4\u7684\u53d8\u5f02\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c0f\u578b\u67b6\u6784\u5982xEEGNet\u5728EEG\u75c5\u7406\u5206\u7c7b\u4e2d\u540c\u6837\u6709\u6548\uff0c\u517c\u5177\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.21742", "pdf": "https://arxiv.org/pdf/2504.21742", "abs": "https://arxiv.org/abs/2504.21742", "authors": ["Emelie Hallenberg"], "title": "Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The Greek fictional narratives often termed love novels or romances, ranging\nfrom the first century CE to the middle of the 15th century, have long been\nconsidered as similar in many ways, not least in the use of particular literary\nmotifs. By applying the use of fine-tuned large language models, this study\naims to investigate which motifs exactly that the texts in this corpus have in\ncommon, and in which ways they differ from each other. The results show that\nwhile some motifs persist throughout the corpus, others fluctuate in frequency,\nindicating certain trends or external influences. Conclusively, the method\nproves to adequately extract literary motifs according to a set definition,\nproviding data for both quantitative and qualitative analyses.", "AI": {"tldr": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790\u5e0c\u814a\u7231\u60c5\u5c0f\u8bf4\u4e2d\u7684\u6587\u5b66\u6bcd\u9898\uff0c\u53d1\u73b0\u5176\u5171\u6027\u4e0e\u5dee\u5f02\u3002", "motivation": "\u63a2\u7a76\u5e0c\u814a\u7231\u60c5\u5c0f\u8bf4\u4e2d\u6587\u5b66\u6bcd\u9898\u7684\u5171\u6027\u4e0e\u53d8\u5316\uff0c\u63ed\u793a\u6f5c\u5728\u8d8b\u52bf\u6216\u5916\u90e8\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u7cbe\u7ec6\u8c03\u6574\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u5e76\u5206\u6790\u6587\u672c\u4e2d\u7684\u6587\u5b66\u6bcd\u9898\u3002", "result": "\u90e8\u5206\u6bcd\u9898\u8d2f\u7a7f\u59cb\u7ec8\uff0c\u5176\u4ed6\u6bcd\u9898\u9891\u7387\u6ce2\u52a8\uff0c\u663e\u793a\u8d8b\u52bf\u6216\u5916\u90e8\u5f71\u54cd\u3002", "conclusion": "\u65b9\u6cd5\u6709\u6548\u63d0\u53d6\u6587\u5b66\u6bcd\u9898\uff0c\u652f\u6301\u5b9a\u91cf\u4e0e\u5b9a\u6027\u5206\u6790\u3002"}}
{"id": "2504.21487", "pdf": "https://arxiv.org/pdf/2504.21487", "abs": "https://arxiv.org/abs/2504.21487", "authors": ["Hebaixu Wang", "Jing Zhang", "Haonan Guo", "Di Wang", "Jiayi Ma", "Bo Du"], "title": "DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling for Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have achieved remarkable progress in universal image\nrestoration. While existing methods speed up inference by reducing sampling\nsteps, substantial step intervals often introduce cumulative errors. Moreover,\nthey struggle to balance the commonality of degradation representations and\nrestoration quality. To address these challenges, we introduce\n\\textbf{DGSolver}, a diffusion generalist solver with universal posterior\nsampling. We first derive the exact ordinary differential equations for\ngeneralist diffusion models and tailor high-order solvers with a queue-based\naccelerated sampling strategy to improve both accuracy and efficiency. We then\nintegrate universal posterior sampling to better approximate\nmanifold-constrained gradients, yielding a more accurate noise estimation and\ncorrecting errors in inverse inference. Extensive experiments show that\nDGSolver outperforms state-of-the-art methods in restoration accuracy,\nstability, and scalability, both qualitatively and quantitatively. Code and\nmodels will be available at https://github.com/MiliLab/DGSolver.", "AI": {"tldr": "DGSolver\u662f\u4e00\u79cd\u6269\u6563\u901a\u7528\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u9ad8\u7cbe\u5ea6\u6c42\u89e3\u5668\u548c\u540e\u9a8c\u91c7\u6837\u63d0\u5347\u56fe\u50cf\u4fee\u590d\u8d28\u91cf\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u65f6\u5f15\u5165\u7d2f\u79ef\u8bef\u5dee\uff0c\u4e14\u96be\u4ee5\u5e73\u8861\u9000\u5316\u8868\u793a\u548c\u4fee\u590d\u8d28\u91cf\u3002", "method": "\u63a8\u5bfc\u901a\u7528\u6269\u6563\u6a21\u578b\u7684\u7cbe\u786eODE\uff0c\u8bbe\u8ba1\u9ad8\u9636\u6c42\u89e3\u5668\u548c\u961f\u5217\u52a0\u901f\u91c7\u6837\u7b56\u7565\uff0c\u7ed3\u5408\u540e\u9a8c\u91c7\u6837\u4f18\u5316\u566a\u58f0\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDGSolver\u5728\u4fee\u590d\u7cbe\u5ea6\u3001\u7a33\u5b9a\u6027\u548c\u6269\u5c55\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DGSolver\u901a\u8fc7\u9ad8\u6548\u91c7\u6837\u548c\u540e\u9a8c\u91c7\u6837\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u4fee\u590d\u6027\u80fd\u3002"}}
{"id": "2504.21049", "pdf": "https://arxiv.org/pdf/2504.21049", "abs": "https://arxiv.org/abs/2504.21049", "authors": ["Sneha Baskota"], "title": "Phishing URL Detection using Bi-LSTM", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Phishing attacks threaten online users, often leading to data breaches,\nfinancial losses, and identity theft. Traditional phishing detection systems\nstruggle with high false positive rates and are usually limited by the types of\nattacks they can identify. This paper proposes a deep learning-based approach\nusing a Bidirectional Long Short-Term Memory (Bi-LSTM) network to classify URLs\ninto four categories: benign, phishing, defacement, and malware. The model\nleverages sequential URL data and captures contextual information, improving\nthe accuracy of phishing detection. Experimental results on a dataset\ncomprising over 650,000 URLs demonstrate the model's effectiveness, achieving\n97% accuracy and significant improvements over traditional techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBi-LSTM\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u7c7bURL\uff0c\u63d0\u9ad8\u9493\u9c7c\u653b\u51fb\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u9493\u9c7c\u68c0\u6d4b\u7cfb\u7edf\u5b58\u5728\u9ad8\u8bef\u62a5\u7387\u548c\u68c0\u6d4b\u7c7b\u578b\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u53cc\u5411\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08Bi-LSTM\uff09\u5bf9URL\u8fdb\u884c\u5206\u7c7b\uff0c\u5206\u4e3a\u826f\u6027\u3001\u9493\u9c7c\u3001\u7be1\u6539\u548c\u6076\u610f\u8f6f\u4ef6\u56db\u7c7b\u3002", "result": "\u5728\u5305\u542b65\u4e07URL\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8fbe\u523097%\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "Bi-LSTM\u6a21\u578b\u80fd\u6709\u6548\u63d0\u5347\u9493\u9c7c\u653b\u51fb\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2504.21501", "pdf": "https://arxiv.org/pdf/2504.21501", "abs": "https://arxiv.org/abs/2504.21501", "authors": ["Yaru Liu", "Yiqi Gu", "Michael K. Ng"], "title": "Deep Learning Optimization Using Self-Adaptive Weighted Auxiliary Variables", "categories": ["cs.LG"], "comment": "32 pages, 11 figures", "summary": "In this paper, we develop a new optimization framework for the least squares\nlearning problem via fully connected neural networks or physics-informed neural\nnetworks. The gradient descent sometimes behaves inefficiently in deep learning\nbecause of the high non-convexity of loss functions and the vanishing gradient\nissue. Our idea is to introduce auxiliary variables to separate the layers of\nthe deep neural networks and reformulate the loss functions for ease of\noptimization. We design the self-adaptive weights to preserve the consistency\nbetween the reformulated loss and the original mean squared loss, which\nguarantees that optimizing the new loss helps optimize the original problem.\nNumerical experiments are presented to verify the consistency and show the\neffectiveness and robustness of our models over gradient descent.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u53d8\u91cf\u548c\u81ea\u9002\u5e94\u6743\u91cd\u6539\u8fdb\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u68af\u5ea6\u4e0b\u964d\u6548\u7387\u3002", "motivation": "\u68af\u5ea6\u4e0b\u964d\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u56e0\u635f\u5931\u51fd\u6570\u9ad8\u5ea6\u975e\u51f8\u548c\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u6539\u8fdb\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u8f85\u52a9\u53d8\u91cf\u5206\u79bb\u7f51\u7edc\u5c42\uff0c\u91cd\u65b0\u8bbe\u8ba1\u635f\u5931\u51fd\u6570\uff0c\u5e76\u4f7f\u7528\u81ea\u9002\u5e94\u6743\u91cd\u4fdd\u6301\u4e00\u81f4\u6027\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b0\u6846\u67b6\u7684\u4e00\u81f4\u6027\u548c\u5bf9\u68af\u5ea6\u4e0b\u964d\u7684\u6709\u6548\u6027\u53ca\u9c81\u68d2\u6027\u3002", "conclusion": "\u65b0\u4f18\u5316\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u68af\u5ea6\u4e0b\u964d\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2504.21747", "pdf": "https://arxiv.org/pdf/2504.21747", "abs": "https://arxiv.org/abs/2504.21747", "authors": ["Maxime Bouthors", "Josep Crego", "Fran\u00e7ois Yvon"], "title": "Improving Retrieval-Augmented Neural Machine Translation with Monolingual Data", "categories": ["cs.CL", "I.2.7"], "comment": "13 pages", "summary": "Conventional retrieval-augmented neural machine translation (RANMT) systems\nleverage bilingual corpora, e.g., translation memories (TMs). Yet, in many\nsettings, in-domain monolingual target-side corpora are often available. This\nwork explores ways to take advantage of such resources by retrieving relevant\nsegments directly in the target language, based on a source-side query. For\nthis, we design improved cross-lingual retrieval systems, trained with both\nsentence level and word-level matching objectives. In our experiments with two\nRANMT architectures, we first demonstrate the benefits of such cross-lingual\nobjectives in a controlled setting, obtaining translation performances that\nsurpass standard TM-based models. We then showcase our method on a real-world\nset-up, where the target monolingual resources far exceed the amount of\nparallel data and observe large improvements of our new techniques, which\noutperform both the baseline setting, and general-purpose cross-lingual\nretrievers.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u76ee\u6807\u8bed\u8a00\u5355\u8bed\u8bed\u6599\u5e93\u6539\u8fdb\u68c0\u7d22\u589e\u5f3a\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\uff08RANMT\uff09\u7cfb\u7edf\uff0c\u8bbe\u8ba1\u4e86\u8de8\u8bed\u8a00\u68c0\u7d22\u7cfb\u7edf\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfRANMT\u7cfb\u7edf\u4f9d\u8d56\u53cc\u8bed\u8bed\u6599\u5e93\uff0c\u4f46\u76ee\u6807\u8bed\u8a00\u5355\u8bed\u8bed\u6599\u5e93\u5728\u8bb8\u591a\u573a\u666f\u4e0b\u66f4\u6613\u83b7\u53d6\uff0c\u7814\u7a76\u65e8\u5728\u5229\u7528\u8fd9\u4e9b\u8d44\u6e90\u63d0\u5347\u7ffb\u8bd1\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u6539\u8fdb\u7684\u8de8\u8bed\u8a00\u68c0\u7d22\u7cfb\u7edf\uff0c\u7ed3\u5408\u53e5\u5b50\u7ea7\u548c\u8bcd\u7ea7\u5339\u914d\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u4e24\u79cdRANMT\u67b6\u6784\u4e0a\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u5728\u63a7\u5236\u73af\u5883\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u4f20\u7edfTM\u6a21\u578b\u548c\u901a\u7528\u8de8\u8bed\u8a00\u68c0\u7d22\u65b9\u6cd5\u3002", "conclusion": "\u5229\u7528\u76ee\u6807\u8bed\u8a00\u5355\u8bed\u8bed\u6599\u5e93\u7684\u8de8\u8bed\u8a00\u68c0\u7d22\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7ffb\u8bd1\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2504.21491", "pdf": "https://arxiv.org/pdf/2504.21491", "abs": "https://arxiv.org/abs/2504.21491", "authors": ["Qinfeng Zhu", "Yunxi Jiang", "Lei Fan"], "title": "ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic Segmentation of Remote Sensing Imagery", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": null, "summary": "We propose a result-level category-specific fusion architecture called\nClassWise-CRF. This architecture employs a two-stage process: first, it selects\nexpert networks that perform well in specific categories from a pool of\ncandidate networks using a greedy algorithm; second, it integrates the\nsegmentation predictions of these selected networks by adaptively weighting\ntheir contributions based on their segmentation performance in each category.\nInspired by Conditional Random Field (CRF), the ClassWise-CRF architecture\ntreats the segmentation predictions from multiple networks as confidence vector\nfields. It leverages segmentation metrics (such as Intersection over Union)\nfrom the validation set as priors and employs an exponential weighting strategy\nto fuse the category-specific confidence scores predicted by each network. This\nfusion method dynamically adjusts the weights of each network for different\ncategories, achieving category-specific optimization. Building on this, the\narchitecture further optimizes the fused results using unary and pairwise\npotentials in CRF to ensure spatial consistency and boundary accuracy. To\nvalidate the effectiveness of ClassWise-CRF, we conducted experiments on two\nremote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced\nsemantic segmentation networks. The results show that the ClassWise-CRF\narchitecture significantly improves segmentation performance: on the LoveDA\ndataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on\nthe validation set and by 0.68% on the test set; on the Vaihingen dataset, the\nmIoU improved by 0.87% on the validation set and by 0.91% on the test set.\nThese results fully demonstrate the effectiveness and generality of the\nClassWise-CRF architecture in semantic segmentation of remote sensing images.\nThe full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF.", "AI": {"tldr": "ClassWise-CRF\u662f\u4e00\u79cd\u7ed3\u679c\u7ea7\u7c7b\u522b\u7279\u5b9a\u878d\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u7a0b\u9009\u62e9\u4e13\u5bb6\u7f51\u7edc\u5e76\u81ea\u9002\u5e94\u52a0\u6743\u5176\u9884\u6d4b\uff0c\u7ed3\u5408CRF\u4f18\u5316\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u8fb9\u754c\u7cbe\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u9065\u611f\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u7f51\u7edc\u878d\u5408\u4e2d\u7c7b\u522b\u7279\u5b9a\u4f18\u5316\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "method": "\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff1a\u8d2a\u5a6a\u7b97\u6cd5\u9009\u62e9\u4e13\u5bb6\u7f51\u7edc\uff0c\u81ea\u9002\u5e94\u52a0\u6743\u878d\u5408\u9884\u6d4b\uff1b\u7ed3\u5408CRF\u4f18\u5316\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u8fb9\u754c\u7cbe\u5ea6\u3002", "result": "\u5728LoveDA\u548cVaihingen\u6570\u636e\u96c6\u4e0a\uff0cmIoU\u5206\u522b\u63d0\u53471.00%/0.68%\u548c0.87%/0.91%\u3002", "conclusion": "ClassWise-CRF\u6709\u6548\u4e14\u901a\u7528\uff0c\u663e\u8457\u63d0\u5347\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2504.21052", "pdf": "https://arxiv.org/pdf/2504.21052", "abs": "https://arxiv.org/abs/2504.21052", "authors": ["Yangxu Yin", "Honglong Chen", "Yudong Gao", "Peng Sun", "Zhishuai Li", "Weifeng Liu"], "title": "SFIBA: Spatial-based Full-target Invisible Backdoor Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Multi-target backdoor attacks pose significant security threats to deep\nneural networks, as they can preset multiple target classes through a single\nbackdoor injection. This allows attackers to control the model to misclassify\npoisoned samples with triggers into any desired target class during inference,\nexhibiting superior attack performance compared with conventional backdoor\nattacks. However, existing multi-target backdoor attacks fail to guarantee\ntrigger specificity and stealthiness in black-box settings, resulting in two\nmain issues. First, they are unable to simultaneously target all classes when\nonly training data can be manipulated, limiting their effectiveness in\nrealistic attack scenarios. Second, the triggers often lack visual\nimperceptibility, making poisoned samples easy to detect. To address these\nproblems, we propose a Spatial-based Full-target Invisible Backdoor Attack,\ncalled SFIBA. It restricts triggers for different classes to specific local\nspatial regions and morphologies in the pixel space to ensure specificity,\nwhile employing a frequency-domain-based trigger injection method to guarantee\nstealthiness. Specifically, for injection of each trigger, we first apply fast\nfourier transform to obtain the amplitude spectrum of clean samples in local\nspatial regions. Then, we employ discrete wavelet transform to extract the\nfeatures from the amplitude spectrum and use singular value decomposition to\nintegrate the trigger. Subsequently, we selectively filter parts of the trigger\nin pixel space to implement trigger morphology constraints and adjust injection\ncoefficients based on visual effects. We conduct experiments on multiple\ndatasets and models. The results demonstrate that SFIBA can achieve excellent\nattack performance and stealthiness, while preserving the model's performance\non benign samples, and can also bypass existing backdoor defenses.", "AI": {"tldr": "SFIBA\u662f\u4e00\u79cd\u57fa\u4e8e\u7a7a\u95f4\u7684\u5168\u76ee\u6807\u9690\u5f62\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u9650\u5236\u89e6\u53d1\u5668\u7684\u7a7a\u95f4\u533a\u57df\u548c\u5f62\u6001\uff0c\u5e76\u7ed3\u5408\u9891\u57df\u6ce8\u5165\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u653b\u51fb\u6027\u80fd\u548c\u9690\u853d\u6027\u3002", "motivation": "\u73b0\u6709\u591a\u76ee\u6807\u540e\u95e8\u653b\u51fb\u5728\u89e6\u53d1\u7279\u5f02\u6027\u548c\u9690\u853d\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u653b\u51fb\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "SFIBA\u901a\u8fc7\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u3001\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u548c\u5947\u5f02\u503c\u5206\u89e3\uff0c\u5728\u9891\u57df\u4e2d\u6ce8\u5165\u89e6\u53d1\u5668\uff0c\u5e76\u5728\u50cf\u7d20\u7a7a\u95f4\u4e2d\u9650\u5236\u5176\u5f62\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSFIBA\u5177\u6709\u4f18\u5f02\u7684\u653b\u51fb\u6027\u80fd\u548c\u9690\u853d\u6027\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u6a21\u578b\u5728\u6b63\u5e38\u6837\u672c\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u80fd\u7ed5\u8fc7\u73b0\u6709\u9632\u5fa1\u3002", "conclusion": "SFIBA\u89e3\u51b3\u4e86\u591a\u76ee\u6807\u540e\u95e8\u653b\u51fb\u7684\u89e6\u53d1\u7279\u5f02\u6027\u548c\u9690\u853d\u6027\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u653b\u51fb\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21565", "pdf": "https://arxiv.org/pdf/2504.21565", "abs": "https://arxiv.org/abs/2504.21565", "authors": ["David Fern\u00e1ndez Narro", "Pablo Ferri", "Juan M. Garc\u00eda-G\u00f3mez", "Carlos S\u00e1ez"], "title": "Towards proactive self-adaptive AI for non-stationary environments with dataset shifts", "categories": ["cs.LG", "cs.AI", "I.2.8"], "comment": "6 pages, 4 figures, conference paper", "summary": "Artificial Intelligence (AI) models deployed in production frequently face\nchallenges in maintaining their performance in non-stationary environments.\nThis issue is particularly noticeable in medical settings, where temporal\ndataset shifts often occur. These shifts arise when the distributions of\ntraining data differ from those of the data encountered during deployment over\ntime. Further, new labeled data to continuously retrain AI is not typically\navailable in a timely manner due to data access limitations. To address these\nchallenges, we propose a proactive self-adaptive AI approach, or pro-adaptive,\nwhere we model the temporal trajectory of AI parameters, allowing us to\nshort-term forecast parameter values. To this end, we use polynomial spline\nbases, within an extensible Functional Data Analysis framework. We validate our\nmethodology with a logistic regression model addressing prior probability\nshift, covariate shift, and concept shift. This validation is conducted on both\na controlled simulated dataset and a publicly available real-world COVID-19\ndataset from Mexico, with various shifts occurring between 2020 and 2024. Our\nresults indicate that this approach enhances the performance of AI against\nshifts compared to baseline stable models trained at different time distances\nfrom the present, without requiring updated training data. This work lays the\nfoundation for pro-adaptive AI research against dynamic, non-stationary\nenvironments, being compatible with data protection, in resilient AI production\nenvironments for health.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u81ea\u9002\u5e94\u7684AI\u65b9\u6cd5\uff08pro-adaptive\uff09\uff0c\u901a\u8fc7\u5efa\u6a21AI\u53c2\u6570\u7684\u65f6\u95f4\u8f68\u8ff9\u6765\u9884\u6d4b\u77ed\u671f\u53c2\u6570\u503c\uff0c\u4ee5\u5e94\u5bf9\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u533b\u7597\u73af\u5883\u4e2d\uff0cAI\u6a21\u578b\u5e38\u56e0\u6570\u636e\u5206\u5e03\u968f\u65f6\u95f4\u53d8\u5316\uff08\u5982\u65f6\u95f4\u6027\u6570\u636e\u96c6\u504f\u79fb\uff09\u800c\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u7f3a\u4e4f\u53ca\u65f6\u7684\u65b0\u6807\u6ce8\u6570\u636e\u7528\u4e8e\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u4f7f\u7528\u591a\u9879\u5f0f\u6837\u6761\u57fa\u548c\u529f\u80fd\u6027\u6570\u636e\u5206\u6790\u6846\u67b6\u5efa\u6a21\u53c2\u6570\u65f6\u95f4\u8f68\u8ff9\uff0c\u5e76\u5728\u903b\u8f91\u56de\u5f52\u6a21\u578b\u4e2d\u9a8c\u8bc1\u5176\u5bf9\u5148\u9a8c\u6982\u7387\u504f\u79fb\u3001\u534f\u53d8\u91cf\u504f\u79fb\u548c\u6982\u5ff5\u504f\u79fb\u7684\u9002\u5e94\u6027\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u96c6\u548c\u771f\u5b9eCOVID-19\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86AI\u5728\u504f\u79fb\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u65e0\u9700\u66f4\u65b0\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u4e3a\u52a8\u6001\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94AI\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u9002\u7528\u4e8e\u533b\u7597\u7b49\u6570\u636e\u4fdd\u62a4\u4e25\u683c\u7684\u9886\u57df\u3002"}}
{"id": "2504.21773", "pdf": "https://arxiv.org/pdf/2504.21773", "abs": "https://arxiv.org/abs/2504.21773", "authors": ["Junsheng Huang", "Zhitao He", "Sandeep Polisetty", "Qingyun Wang", "May Fung"], "title": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the widespread application of large language models (LLMs), the issue of\ngenerating non-existing facts, known as hallucination, has garnered increasing\nattention. Previous research in enhancing LLM confidence estimation mainly\nfocuses on the single problem setting. However, LLM awareness of its internal\nparameterized knowledge boundary under the more challenging multi-problem\nsetting, which requires answering multiple problems accurately simultaneously,\nremains underexplored. To bridge this gap, we introduce a novel method,\nMultiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates\nthe learning of answer prediction and confidence estimation during fine-tuning\non instruction data. Extensive experiments demonstrate that our method\noutperforms baselines by up to 25% in average precision.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5MAC-Tuning\uff0c\u7528\u4e8e\u5728\u591a\u95ee\u9898\u8bbe\u7f6e\u4e0b\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u968f\u7740LLMs\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u751f\u6210\u865a\u5047\u4e8b\u5b9e\uff08\u5e7b\u89c9\uff09\u7684\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5355\u4e00\u95ee\u9898\u8bbe\u7f6e\uff0c\u800c\u591a\u95ee\u9898\u8bbe\u7f6e\u4e0b\u7684\u6a21\u578b\u77e5\u8bc6\u8fb9\u754c\u610f\u8bc6\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86MAC-Tuning\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6307\u4ee4\u6570\u636e\u5fae\u8c03\u4e2d\u5206\u79bb\u7b54\u6848\u9884\u6d4b\u548c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e73\u5747\u7cbe\u5ea6\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e8625%\u3002", "conclusion": "MAC-Tuning\u5728\u591a\u95ee\u9898\u8bbe\u7f6e\u4e0b\u6709\u6548\u63d0\u5347\u4e86LLMs\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.21495", "pdf": "https://arxiv.org/pdf/2504.21495", "abs": "https://arxiv.org/abs/2504.21495", "authors": ["Junxi Wang", "Jize liu", "Na Zhang", "Yaxiong Wang"], "title": "Consistency-aware Fake Videos Detection on Short Video Platforms", "categories": ["cs.CV", "cs.MM"], "comment": "2025 icic", "summary": "This paper focuses to detect the fake news on the short video platforms.\nWhile significant research efforts have been devoted to this task with notable\nprogress in recent years, current detection accuracy remains suboptimal due to\nthe rapid evolution of content manipulation and generation technologies.\nExisting approaches typically employ a cross-modal fusion strategy that\ndirectly combines raw video data with metadata inputs before applying a\nclassification layer. However, our empirical observations reveal a critical\noversight: manipulated content frequently exhibits inter-modal inconsistencies\nthat could serve as valuable discriminative features, yet remain underutilized\nin contemporary detection frameworks. Motivated by this insight, we propose a\nnovel detection paradigm that explicitly identifies and leverages cross-modal\ncontradictions as discriminative cues. Our approach consists of two core\nmodules: Cross-modal Consistency Learning (CMCL) and Multi-modal Collaborative\nDiagnosis (MMCD). CMCL includes Pseudo-label Generation (PLG) and Cross-modal\nConsistency Diagnosis (CMCD). In PLG, a Multimodal Large Language Model is used\nto generate pseudo-labels for evaluating cross-modal semantic consistency.\nThen, CMCD extracts [CLS] tokens and computes cosine loss to quantify\ncross-modal inconsistencies. MMCD further integrates multimodal features\nthrough Multimodal Feature Fusion (MFF) and Probability Scores Fusion (PSF).\nMFF employs a co-attention mechanism to enhance semantic interactions across\ndifferent modalities, while a Transformer is utilized for comprehensive feature\nfusion. Meanwhile, PSF further integrates the fake news probability scores\nobtained in the previous step. Extensive experiments on established benchmarks\n(FakeSV and FakeTT) demonstrate our model exhibits outstanding performance in\nFake videos detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8de8\u6a21\u6001\u77db\u76fe\u68c0\u6d4b\u5047\u65b0\u95fb\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u5b66\u4e60\u548c\u591a\u6a21\u6001\u534f\u4f5c\u8bca\u65ad\u6a21\u5757\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u8de8\u6a21\u6001\u4e0d\u4e00\u81f4\u6027\u4f5c\u4e3a\u5224\u522b\u7279\u5f81\uff0c\u5bfc\u81f4\u68c0\u6d4b\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u5b66\u4e60\uff08CMCL\uff09\u548c\u591a\u6a21\u6001\u534f\u4f5c\u8bca\u65ad\uff08MMCD\uff09\u6a21\u5757\uff0c\u5305\u62ec\u4f2a\u6807\u7b7e\u751f\u6210\u3001\u4e00\u81f4\u6027\u8bca\u65ad\u3001\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u548c\u6982\u7387\u5206\u6570\u878d\u5408\u3002", "result": "\u5728FakeSV\u548cFakeTT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5229\u7528\u8de8\u6a21\u6001\u77db\u76fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5047\u89c6\u9891\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2504.21662", "pdf": "https://arxiv.org/pdf/2504.21662", "abs": "https://arxiv.org/abs/2504.21662", "authors": ["Mauricio Ortiz Torres", "Markus Lange", "Arne P. Raulf"], "title": "On Advancements of the Forward-Forward Algorithm", "categories": ["cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The Forward-Forward algorithm has evolved in machine learning research,\ntackling more complex tasks that mimic real-life applications. In the last\nyears, it has been improved by several techniques to perform better than its\noriginal version, handling a challenging dataset like CIFAR10 without losing\nits flexibility and low memory usage. We have shown in our results that\nimprovements are achieved through a combination of convolutional channel\ngrouping, learning rate schedules, and independent block structures during\ntraining that lead to a 20\\% decrease in test error percentage. Additionally,\nto approach further implementations on low-capacity hardware projects we have\npresented a series of lighter models that achieve low test error percentages\nwithin (21$\\pm$6)\\% and number of trainable parameters between 164,706 and\n754,386. This serving also as a basis for our future study on complete\nverification and validation of these kinds of neural networks.", "AI": {"tldr": "Forward-Forward\u7b97\u6cd5\u901a\u8fc7\u6539\u8fdb\u6280\u672f\uff08\u5982\u5377\u79ef\u901a\u9053\u5206\u7ec4\u3001\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u72ec\u7acb\u5757\u7ed3\u6784\uff09\u5728CIFAR10\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u6d4b\u8bd5\u9519\u8bef\u7387\u964d\u4f4e20%\u3002\u540c\u65f6\uff0c\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u4f4e\u5bb9\u91cf\u786c\u4ef6\u4e0a\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u5e76\u4f18\u5316\u7b97\u6cd5\u6027\u80fd\uff0c\u540c\u65f6\u9002\u5e94\u4f4e\u5bb9\u91cf\u786c\u4ef6\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u5377\u79ef\u901a\u9053\u5206\u7ec4\u3001\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u72ec\u7acb\u5757\u7ed3\u6784\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6d4b\u8bd5\u9519\u8bef\u7387\u964d\u4f4e20%\uff0c\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u4f4e\u5bb9\u91cf\u786c\u4ef6\u4e0a\u8868\u73b0\u826f\u597d\uff08\u6d4b\u8bd5\u9519\u8bef\u738721\u00b16%\uff09\u3002", "conclusion": "\u6539\u8fdb\u7684Forward-Forward\u7b97\u6cd5\u5728\u6027\u80fd\u548c\u786c\u4ef6\u9002\u5e94\u6027\u4e0a\u5747\u6709\u63d0\u5347\uff0c\u4e3a\u672a\u6765\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2504.21776", "pdf": "https://arxiv.org/pdf/2504.21776", "abs": "https://arxiv.org/abs/2504.21776", "authors": ["Xiaoxi Li", "Jiajie Jin", "Guanting Dong", "Hongjin Qian", "Yutao Zhu", "Yongkang Wu", "Ji-Rong Wen", "Zhicheng Dou"], "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate\nimpressive long-horizon reasoning capabilities. However, their reliance on\nstatic internal knowledge limits their performance on complex,\nknowledge-intensive tasks and hinders their ability to produce comprehensive\nresearch reports requiring synthesis of diverse web information. To address\nthis, we propose \\textbf{WebThinker}, a deep research agent that empowers LRMs\nto autonomously search the web, navigate web pages, and draft research reports\nduring the reasoning process. WebThinker integrates a \\textbf{Deep Web\nExplorer} module, enabling LRMs to dynamically search, navigate, and extract\ninformation from the web when encountering knowledge gaps. It also employs an\n\\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to\nseamlessly interleave reasoning, information gathering, and report writing in\nreal time. To further enhance research tool utilization, we introduce an\n\\textbf{RL-based training strategy} via iterative online Direct Preference\nOptimization (DPO). Extensive experiments on complex reasoning benchmarks\n(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)\ndemonstrate that WebThinker significantly outperforms existing methods and\nstrong proprietary systems. Our approach enhances LRM reliability and\napplicability in complex scenarios, paving the way for more capable and\nversatile deep research systems. The code is available at\nhttps://github.com/RUC-NLPIR/WebThinker.", "AI": {"tldr": "WebThinker\u662f\u4e00\u4e2a\u589e\u5f3a\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u7814\u7a76\u4ee3\u7406\uff0c\u901a\u8fc7\u52a8\u6001\u641c\u7d22\u7f51\u7edc\u3001\u63d0\u53d6\u4fe1\u606f\u5e76\u5b9e\u65f6\u751f\u6210\u62a5\u544a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u77e5\u8bc6\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u3001\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u9700\u8981\u7efc\u5408\u7f51\u7edc\u4fe1\u606f\u7684\u79d1\u7814\u62a5\u544a\u3002", "method": "WebThinker\u7ed3\u5408\u4e86\u6df1\u5ea6\u7f51\u7edc\u63a2\u7d22\u6a21\u5757\u548c\u81ea\u4e3b\u601d\u8003-\u641c\u7d22-\u8349\u62df\u7b56\u7565\uff0c\u5e76\u901a\u8fc7RL\u8bad\u7ec3\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u590d\u6742\u63a8\u7406\u57fa\u51c6\u548c\u79d1\u7814\u62a5\u544a\u4efb\u52a1\u4e2d\uff0cWebThinker\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u548c\u4e13\u6709\u7cfb\u7edf\u3002", "conclusion": "WebThinker\u63d0\u5347\u4e86LRMs\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\u548c\u9002\u7528\u6027\uff0c\u4e3a\u66f4\u5f3a\u5927\u7684\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2504.21497", "pdf": "https://arxiv.org/pdf/2504.21497", "abs": "https://arxiv.org/abs/2504.21497", "authors": ["Mengting Wei", "Yante Li", "Tuomas Varanka", "Yan Jiang", "Licai Sun", "Guoying Zhao"], "title": "MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose a method for video face reenactment that integrates\na 3D face parametric model into a latent diffusion framework, aiming to improve\nshape consistency and motion control in existing video-based face generation\napproaches. Our approach employs the FLAME (Faces Learned with an Articulated\nModel and Expressions) model as the 3D face parametric representation,\nproviding a unified framework for modeling face expressions and head pose. This\nenables precise extraction of detailed face geometry and motion features from\ndriving videos. Specifically, we enhance the latent diffusion model with rich\n3D expression and detailed pose information by incorporating depth maps, normal\nmaps, and rendering maps derived from FLAME sequences. A multi-layer face\nmovements fusion module with integrated self-attention mechanisms is used to\ncombine identity and motion latent features within the spatial domain. By\nutilizing the 3D face parametric model as motion guidance, our method enables\nparametric alignment of face identity between the reference image and the\nmotion captured from the driving video. Experimental results on benchmark\ndatasets show that our method excels at generating high-quality face animations\nwith precise expression and head pose variation modeling. In addition, it\ndemonstrates strong generalization performance on out-of-domain images. Code is\npublicly available at https://github.com/weimengting/MagicPortrait.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c063D\u4eba\u8138\u53c2\u6570\u6a21\u578b\u4e0e\u6f5c\u5728\u6269\u6563\u6846\u67b6\u7ed3\u5408\u7684\u89c6\u9891\u4eba\u8138\u91cd\u6f14\u65b9\u6cd5\uff0c\u63d0\u5347\u5f62\u72b6\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u63a7\u5236\u3002", "motivation": "\u6539\u8fdb\u73b0\u6709\u89c6\u9891\u4eba\u8138\u751f\u6210\u65b9\u6cd5\u5728\u5f62\u72b6\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u63a7\u5236\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528FLAME\u6a21\u578b\u4f5c\u4e3a3D\u4eba\u8138\u53c2\u6570\u8868\u793a\uff0c\u7ed3\u5408\u6df1\u5ea6\u56fe\u3001\u6cd5\u7ebf\u56fe\u548c\u6e32\u67d3\u56fe\uff0c\u589e\u5f3a\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u591a\u5c42\u878d\u5408\u6a21\u5757\u7ed3\u5408\u8eab\u4efd\u548c\u8fd0\u52a8\u7279\u5f81\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u751f\u6210\u9ad8\u8d28\u91cf\u4eba\u8138\u52a8\u753b\uff0c\u7cbe\u786e\u5efa\u6a21\u8868\u60c5\u548c\u5934\u90e8\u59ff\u6001\u53d8\u5316\uff0c\u5e76\u5728\u57df\u5916\u56fe\u50cf\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc73D\u53c2\u6570\u6a21\u578b\u5b9e\u73b0\u7cbe\u786e\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u8138\u91cd\u6f14\u7684\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.21054", "pdf": "https://arxiv.org/pdf/2504.21054", "abs": "https://arxiv.org/abs/2504.21054", "authors": ["Yangxu Yin", "Honglong Chen", "Yudong Gao", "Peng Sun", "Liantao Wu", "Zhe Li", "Weifeng Liu"], "title": "FFCBA: Feature-based Full-target Clean-label Backdoor Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Backdoor attacks pose a significant threat to deep neural networks, as\nbackdoored models would misclassify poisoned samples with specific triggers\ninto target classes while maintaining normal performance on clean samples.\nAmong these, multi-target backdoor attacks can simultaneously target multiple\nclasses. However, existing multi-target backdoor attacks all follow the\ndirty-label paradigm, where poisoned samples are mislabeled, and most of them\nrequire an extremely high poisoning rate. This makes them easily detectable by\nmanual inspection. In contrast, clean-label attacks are more stealthy, as they\navoid modifying the labels of poisoned samples. However, they generally\nstruggle to achieve stable and satisfactory attack performance and often fail\nto scale effectively to multi-target attacks. To address this issue, we propose\nthe Feature-based Full-target Clean-label Backdoor Attacks (FFCBA) which\nconsists of two paradigms: Feature-Spanning Backdoor Attacks (FSBA) and\nFeature-Migrating Backdoor Attacks (FMBA). FSBA leverages class-conditional\nautoencoders to generate noise triggers that align perturbed in-class samples\nwith the original category's features, ensuring the effectiveness, intra-class\nconsistency, inter-class specificity and natural-feature correlation of\ntriggers. While FSBA supports swift and efficient attacks, its cross-model\nattack capability is relatively weak. FMBA employs a two-stage\nclass-conditional autoencoder training process that alternates between using\nout-of-class samples and in-class samples. This allows FMBA to generate\ntriggers with strong target-class features, making it highly effective for\ncross-model attacks. We conduct experiments on multiple datasets and models,\nthe results show that FFCBA achieves outstanding attack performance and\nmaintains desirable robustness against the state-of-the-art backdoor defenses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u7684\u591a\u76ee\u6807\u5e72\u51c0\u6807\u7b7e\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff08FFCBA\uff09\uff0c\u5305\u62ec\u7279\u5f81\u8de8\u8d8a\u653b\u51fb\uff08FSBA\uff09\u548c\u7279\u5f81\u8fc1\u79fb\u653b\u51fb\uff08FMBA\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5e72\u51c0\u6807\u7b7e\u653b\u51fb\u5728\u591a\u76ee\u6807\u573a\u666f\u4e0b\u7684\u6027\u80fd\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u76ee\u6807\u540e\u95e8\u653b\u51fb\u591a\u4e3a\u810f\u6807\u7b7e\u8303\u5f0f\uff0c\u5bb9\u6613\u88ab\u68c0\u6d4b\uff0c\u800c\u5e72\u51c0\u6807\u7b7e\u653b\u51fb\u5728\u591a\u76ee\u6807\u573a\u666f\u4e0b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "FFCBA\u5305\u542bFSBA\u548cFMBA\u4e24\u79cd\u8303\u5f0f\uff1aFSBA\u5229\u7528\u7c7b\u6761\u4ef6\u81ea\u7f16\u7801\u5668\u751f\u6210\u566a\u58f0\u89e6\u53d1\u5668\uff0cFMBA\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u751f\u6210\u5177\u6709\u76ee\u6807\u7c7b\u7279\u5f81\u7684\u89e6\u53d1\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFFCBA\u5728\u591a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u5bf9\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "FFCBA\u4e3a\u591a\u76ee\u6807\u5e72\u51c0\u6807\u7b7e\u540e\u95e8\u653b\u51fb\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9690\u853d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21707", "pdf": "https://arxiv.org/pdf/2504.21707", "abs": "https://arxiv.org/abs/2504.21707", "authors": ["Anthony D Martin"], "title": "Recursive KL Divergence Optimization: A Dynamic Framework for Representation Learning", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "We propose a generalization of modern representation learning objectives by\nreframing them as recursive divergence alignment processes over localized\nconditional distributions While recent frameworks like Information Contrastive\nLearning I-Con unify multiple learning paradigms through KL divergence between\nfixed neighborhood conditionals we argue this view underplays a crucial\nrecursive structure inherent in the learning process. We introduce Recursive KL\nDivergence Optimization RKDO a dynamic formalism where representation learning\nis framed as the evolution of KL divergences across data neighborhoods. This\nformulation captures contrastive clustering and dimensionality reduction\nmethods as static slices while offering a new path to model stability and local\nadaptation. Our experiments demonstrate that RKDO offers dual efficiency\nadvantages approximately 30 percent lower loss values compared to static\napproaches across three different datasets and 60 to 80 percent reduction in\ncomputational resources needed to achieve comparable results. This suggests\nthat RKDOs recursive updating mechanism provides a fundamentally more efficient\noptimization landscape for representation learning with significant\nimplications for resource constrained applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9012\u5f52KL\u6563\u5ea6\u4f18\u5316\uff08RKDO\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5c40\u90e8\u6761\u4ef6\u5206\u5e03\u7684KL\u6563\u5ea6\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u8868\u793a\u5b66\u4e60\u76ee\u6807\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6548\u7387\u548c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u9759\u6001\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff08\u5982I-Con\uff09\u901a\u8fc7\u56fa\u5b9a\u90bb\u57df\u6761\u4ef6\u7684KL\u6563\u5ea6\u7edf\u4e00\u5b66\u4e60\u8303\u5f0f\uff0c\u4f46\u5ffd\u89c6\u4e86\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u9012\u5f52\u7ed3\u6784\u3002", "method": "\u5f15\u5165RKDO\uff0c\u5c06\u8868\u793a\u5b66\u4e60\u89c6\u4e3aKL\u6563\u5ea6\u5728\u6570\u636e\u90bb\u57df\u4e0a\u7684\u52a8\u6001\u6f14\u5316\uff0c\u6db5\u76d6\u5bf9\u6bd4\u5b66\u4e60\u3001\u805a\u7c7b\u548c\u964d\u7ef4\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRKDO\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u635f\u5931\u503c\u964d\u4f4e\u7ea630%\uff0c\u8ba1\u7b97\u8d44\u6e90\u8282\u770160%-80%\u3002", "conclusion": "RKDO\u7684\u9012\u5f52\u66f4\u65b0\u673a\u5236\u4e3a\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u8def\u5f84\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u573a\u666f\u3002"}}
{"id": "2504.21800", "pdf": "https://arxiv.org/pdf/2504.21800", "abs": "https://arxiv.org/abs/2504.21800", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "title": "How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "68T50", "I.2.7; H.3.1"], "comment": "11 pages, 5 tables, updated abstract and tables", "summary": "The growing adoption of synthetic data in healthcare is driven by privacy\nconcerns, limited access to real-world data, and the high cost of annotation.\nThis work explores the use of synthetic Prolonged Exposure (PE) therapeutic\nconversations for Post-Traumatic Stress Disorder (PTSD) as a scalable\nalternative for training and evaluating clinical models. We systematically\ncompare real and synthetic dialogues using linguistic, structural, and\nprotocol-specific metrics, including turn-taking patterns and treatment\nfidelity. We also introduce and evaluate PE-specific metrics derived from\nlinguistic analysis and semantic modeling, offering a novel framework for\nassessing clinical fidelity beyond surface fluency. Our findings show that\nalthough synthetic data holds promise for mitigating data scarcity and\nprotecting patient privacy, it can struggle to capture the subtle dynamics of\ntherapeutic interactions. In our dataset, synthetic dialogues match structural\nfeatures of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99),\nhowever, synthetic interactions do not adequately reflect key fidelity markers\n(e.g., distress monitoring). We highlight gaps in existing evaluation\nframeworks and advocate for fidelity-aware metrics that go beyond surface\nfluency to uncover clinically significant failures. Our findings clarify where\nsynthetic data can effectively complement real-world datasets -- and where\ncritical limitations remain.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728PTSD\u6cbb\u7597\u4e2d\u4f7f\u7528\u5408\u6210\u6570\u636e\u66ff\u4ee3\u771f\u5b9e\u6570\u636e\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u5408\u6210\u6570\u636e\u5728\u7ed3\u6784\u4e0a\u63a5\u8fd1\u771f\u5b9e\u5bf9\u8bdd\uff0c\u4f46\u5728\u4e34\u5e8a\u5173\u952e\u6307\u6807\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002", "motivation": "\u9690\u79c1\u95ee\u9898\u3001\u771f\u5b9e\u6570\u636e\u83b7\u53d6\u56f0\u96be\u548c\u9ad8\u6807\u6ce8\u6210\u672c\u63a8\u52a8\u4e86\u5408\u6210\u6570\u636e\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u7cfb\u7edf\u6bd4\u8f83\u771f\u5b9e\u4e0e\u5408\u6210\u5bf9\u8bdd\uff0c\u4f7f\u7528\u8bed\u8a00\u3001\u7ed3\u6784\u548c\u534f\u8bae\u7279\u5b9a\u6307\u6807\uff0c\u5e76\u5f15\u5165PE\u4e13\u7528\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5408\u6210\u6570\u636e\u5728\u7ed3\u6784\u4e0a\u4e0e\u771f\u5b9e\u6570\u636e\u76f8\u4f3c\uff08\u5982\u8bf4\u8bdd\u8005\u5207\u6362\u6bd40.98 vs. 0.99\uff09\uff0c\u4f46\u672a\u80fd\u6355\u6349\u5173\u952e\u4e34\u5e8a\u6307\u6807\uff08\u5982\u75db\u82e6\u76d1\u6d4b\uff09\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u53ef\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u95ee\u9898\uff0c\u4f46\u9700\u5f00\u53d1\u8d85\u8d8a\u8868\u9762\u6d41\u7545\u6027\u7684\u4e34\u5e8a\u4fdd\u771f\u5ea6\u8bc4\u4f30\u6307\u6807\u3002"}}
{"id": "2504.21544", "pdf": "https://arxiv.org/pdf/2504.21544", "abs": "https://arxiv.org/abs/2504.21544", "authors": ["Uzair Shah", "Marco Agus", "Daniya Boges", "Vanessa Chiappini", "Mahmood Alzubaidi", "Jens Schneider", "Markus Hadwiger", "Pierre J. Magistretti", "Mowafa Househ", "Corrado Cal\u0131"], "title": "SAM4EM: Efficient memory-based two stage prompt-free segment anything model adapter for complex 3D neuroscience electron microscopy stacks", "categories": ["cs.CV"], "comment": "Accepted at (CVPRW) 10th IEEE Workshop on Computer Vision for\n  Microscopy Image Analysis (CVMI)", "summary": "We present SAM4EM, a novel approach for 3D segmentation of complex neural\nstructures in electron microscopy (EM) data by leveraging the Segment Anything\nModel (SAM) alongside advanced fine-tuning strategies. Our contributions\ninclude the development of a prompt-free adapter for SAM using two stage mask\ndecoding to automatically generate prompt embeddings, a dual-stage fine-tuning\nmethod based on Low-Rank Adaptation (LoRA) for enhancing segmentation with\nlimited annotated data, and a 3D memory attention mechanism to ensure\nsegmentation consistency across 3D stacks. We further release a unique\nbenchmark dataset for the segmentation of astrocytic processes and synapses. We\nevaluated our method on challenging neuroscience segmentation benchmarks,\nspecifically targeting mitochondria, glia, and synapses, with significant\naccuracy improvements over state-of-the-art (SOTA) methods, including recent\nSAM-based adapters developed for the medical domain and other vision\ntransformer-based approaches. Experimental results indicate that our approach\noutperforms existing solutions in the segmentation of complex processes like\nglia and post-synaptic densities. Our code and models are available at\nhttps://github.com/Uzshah/SAM4EM.", "AI": {"tldr": "SAM4EM\u662f\u4e00\u79cd\u5229\u7528Segment Anything Model\uff08SAM\uff09\u548c\u9ad8\u7ea7\u5fae\u8c03\u7b56\u7565\u5bf9\u7535\u5b50\u663e\u5fae\u955c\uff08EM\uff09\u6570\u636e\u4e2d\u590d\u6742\u795e\u7ecf\u7ed3\u6784\u8fdb\u884c3D\u5206\u5272\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u7535\u5b50\u663e\u5fae\u955c\u6570\u636e\u4e2d\u590d\u6742\u795e\u7ecf\u7ed3\u6784\uff08\u5982\u7ebf\u7c92\u4f53\u3001\u80f6\u8d28\u7ec6\u80de\u548c\u7a81\u89e6\uff09\u76843D\u5206\u5272\u95ee\u9898\uff0c\u63d0\u5347\u5206\u5272\u7cbe\u5ea6\u3002", "method": "\u5f00\u53d1\u4e86\u65e0\u63d0\u793a\u9002\u914d\u5668\u3001\u57fa\u4e8eLoRA\u7684\u53cc\u9636\u6bb5\u5fae\u8c03\u65b9\u6cd5\uff0c\u4ee5\u53ca3D\u8bb0\u5fc6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u786e\u4fdd3D\u5806\u6808\u4e2d\u7684\u5206\u5272\u4e00\u81f4\u6027\u3002", "result": "\u5728\u795e\u7ecf\u79d1\u5b66\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u80f6\u8d28\u7ec6\u80de\u548c\u7a81\u89e6\u540e\u5bc6\u5ea6\u7684\u5206\u5272\u4e0a\u3002", "conclusion": "SAM4EM\u5728\u590d\u6742\u795e\u7ecf\u7ed3\u6784\u76843D\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u548c\u57fa\u51c6\u6570\u636e\u96c6\u3002"}}
{"id": "2504.21775", "pdf": "https://arxiv.org/pdf/2504.21775", "abs": "https://arxiv.org/abs/2504.21775", "authors": ["Rongguang Ye", "Ming Tang"], "title": "Learning Heterogeneous Performance-Fairness Trade-offs in Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Recent methods leverage a hypernet to handle the performance-fairness\ntrade-offs in federated learning. This hypernet maps the clients' preferences\nbetween model performance and fairness to preference-specifc models on the\ntrade-off curve, known as local Pareto front. However, existing methods\ntypically adopt a uniform preference sampling distribution to train the\nhypernet across clients, neglecting the inherent heterogeneity of their local\nPareto fronts. Meanwhile, from the perspective of generalization, they do not\nconsider the gap between local and global Pareto fronts on the global dataset.\nTo address these limitations, we propose HetPFL to effectively learn both local\nand global Pareto fronts. HetPFL comprises Preference Sampling Adaptation (PSA)\nand Preference-aware Hypernet Fusion (PHF). PSA adaptively determines the\noptimal preference sampling distribution for each client to accommodate\nheterogeneous local Pareto fronts. While PHF performs preference-aware fusion\nof clients' hypernets to ensure the performance of the global Pareto front. We\nprove that HetPFL converges linearly with respect to the number of rounds,\nunder weaker assumptions than existing methods. Extensive experiments on four\ndatasets show that HetPFL significantly outperforms seven baselines in terms of\nthe quality of learned local and global Pareto fronts.", "AI": {"tldr": "HetPFL\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u504f\u597d\u91c7\u6837\u548c\u8d85\u7f51\u7edc\u878d\u5408\uff0c\u6709\u6548\u5b66\u4e60\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5c40\u90e8\u548c\u5168\u5c40Pareto\u524d\u6cbf\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u91c7\u7528\u5747\u5300\u504f\u597d\u91c7\u6837\u5206\u5e03\uff0c\u5ffd\u7565\u4e86\u5c40\u90e8Pareto\u524d\u6cbf\u7684\u5f02\u8d28\u6027\uff0c\u4e14\u672a\u8003\u8651\u5c40\u90e8\u4e0e\u5168\u5c40Pareto\u524d\u6cbf\u7684\u5dee\u8ddd\u3002", "method": "HetPFL\u5305\u542b\u504f\u597d\u91c7\u6837\u9002\u5e94\uff08PSA\uff09\u548c\u504f\u597d\u611f\u77e5\u8d85\u7f51\u7edc\u878d\u5408\uff08PHF\uff09\uff0c\u5206\u522b\u4f18\u5316\u5c40\u90e8\u91c7\u6837\u5206\u5e03\u548c\u5168\u5c40\u8d85\u7f51\u7edc\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHetPFL\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u4e03\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u7406\u8bba\u8bc1\u660e\u5176\u6536\u655b\u6027\u66f4\u5f3a\u3002", "conclusion": "HetPFL\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u548c\u878d\u5408\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6027\u80fd-\u516c\u5e73\u6027\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2504.21801", "pdf": "https://arxiv.org/pdf/2504.21801", "abs": "https://arxiv.org/abs/2504.21801", "authors": ["Z. Z. Ren", "Zhihong Shao", "Junxiao Song", "Huajian Xin", "Haocheng Wang", "Wanjia Zhao", "Liyue Zhang", "Zhe Fu", "Qihao Zhu", "Dejian Yang", "Z. F. Wu", "Zhibin Gou", "Shirong Ma", "Hongxuan Tang", "Yuxuan Liu", "Wenjun Gao", "Daya Guo", "Chong Ruan"], "title": "DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce DeepSeek-Prover-V2, an open-source large language model designed\nfor formal theorem proving in Lean 4, with initialization data collected\nthrough a recursive theorem proving pipeline powered by DeepSeek-V3. The\ncold-start training procedure begins by prompting DeepSeek-V3 to decompose\ncomplex problems into a series of subgoals. The proofs of resolved subgoals are\nsynthesized into a chain-of-thought process, combined with DeepSeek-V3's\nstep-by-step reasoning, to create an initial cold start for reinforcement\nlearning. This process enables us to integrate both informal and formal\nmathematical reasoning into a unified model. The resulting model,\nDeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural\ntheorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49\nout of 658 problems from PutnamBench. In addition to standard benchmarks, we\nintroduce ProverBench, a collection of 325 formalized problems, to enrich our\nevaluation, including 15 selected problems from the recent AIME competitions\n(years 24-25). Further evaluation on these 15 AIME problems shows that the\nmodel successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of\nthese problems using majority voting, highlighting that the gap between formal\nand informal mathematical reasoning in large language models is substantially\nnarrowing.", "AI": {"tldr": "DeepSeek-Prover-V2\u662f\u4e00\u4e2a\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u4e3aLean 4\u4e2d\u7684\u5f62\u5f0f\u5b9a\u7406\u8bc1\u660e\u8bbe\u8ba1\uff0c\u901a\u8fc7\u9012\u5f52\u5b9a\u7406\u8bc1\u660e\u6d41\u7a0b\u521d\u59cb\u5316\u6570\u636e\uff0c\u7ed3\u5408\u975e\u6b63\u5f0f\u548c\u6b63\u5f0f\u6570\u5b66\u63a8\u7406\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u975e\u6b63\u5f0f\u548c\u6b63\u5f0f\u6570\u5b66\u63a8\u7406\uff0c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f62\u5f0f\u5b9a\u7406\u8bc1\u660e\u4e2d\u7684\u6027\u80fd\uff0c\u7f29\u5c0f\u5f62\u5f0f\u4e0e\u975e\u5f62\u5f0f\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u5229\u7528DeepSeek-V3\u5206\u89e3\u590d\u6742\u95ee\u9898\u4e3a\u5b50\u76ee\u6807\uff0c\u751f\u6210\u94fe\u5f0f\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u521d\u59cb\u6570\u636e\uff0c\u8bad\u7ec3\u51fa\u7edf\u4e00\u6a21\u578bDeepSeek-Prover-V2-671B\u3002", "result": "\u5728MiniF2F-test\u4e2d\u8fbe\u523088.9%\u901a\u8fc7\u7387\uff0c\u89e3\u51b3PutnamBench\u768449/658\u95ee\u9898\uff0c\u5e76\u5728ProverBench\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u89e3\u51b36/15 AIME\u95ee\u9898\u3002", "conclusion": "DeepSeek-Prover-V2\u5728\u5f62\u5f0f\u5b9a\u7406\u8bc1\u660e\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7f29\u5c0f\u4e86\u5f62\u5f0f\u4e0e\u975e\u5f62\u5f0f\u63a8\u7406\u7684\u5dee\u8ddd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.21559", "pdf": "https://arxiv.org/pdf/2504.21559", "abs": "https://arxiv.org/abs/2504.21559", "authors": ["Sangmin Woo", "Kang Zhou", "Yun Zhou", "Shuai Wang", "Sheng Guan", "Haibo Ding", "Lin Lee Cheong"], "title": "Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "NAACL 2025", "summary": "Large Vision Language Models (LVLMs) often suffer from object hallucination,\nwhich undermines their reliability. Surprisingly, we find that simple\nobject-based visual prompting -- overlaying visual cues (e.g., bounding box,\ncircle) on images -- can significantly mitigate such hallucination; however,\ndifferent visual prompts (VPs) vary in effectiveness. To address this, we\npropose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify\noptimal VPs that enhance LVLM responses without needing access to model\ninternals. Our approach employs a pool of candidate VPs and trains a router\nmodel to dynamically select the most effective VP for a given input image. This\nblack-box approach is model-agnostic, making it applicable to both open-source\nand proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR\ndemonstrate that BBVPE effectively reduces object hallucination.", "AI": {"tldr": "\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u5de5\u7a0b\uff08BBVPE\uff09\u6846\u67b6\uff0c\u52a8\u6001\u9009\u62e9\u6700\u4f18\u89c6\u89c9\u63d0\u793a\u4ee5\u51cf\u5c11\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5e38\u51fa\u73b0\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u5f71\u54cd\u5176\u53ef\u9760\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u7b80\u5355\u7684\u89c6\u89c9\u63d0\u793a\uff08\u5982\u8fb9\u754c\u6846\u3001\u5706\u5708\uff09\u80fd\u663e\u8457\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u4e0d\u540c\u63d0\u793a\u6548\u679c\u5dee\u5f02\u8f83\u5927\u3002", "method": "\u63d0\u51faBlack-Box Visual Prompt Engineering\uff08BBVPE\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5019\u9009\u89c6\u89c9\u63d0\u793a\u6c60\u548c\u8def\u7531\u6a21\u578b\u52a8\u6001\u9009\u62e9\u6700\u4f18\u63d0\u793a\uff0c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u3002", "result": "\u5728POPE\u548cCHAIR\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBBVPE\u6709\u6548\u51cf\u5c11\u4e86\u7269\u4f53\u5e7b\u89c9\u3002", "conclusion": "BBVPE\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u9ed1\u76d2\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5f00\u6e90\u548c\u4e13\u6709LVLM\uff0c\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u3002"}}
{"id": "2504.21808", "pdf": "https://arxiv.org/pdf/2504.21808", "abs": "https://arxiv.org/abs/2504.21808", "authors": ["Atieh Rahmani", "Mansoor Davoodi", "Justin M. Calabrese"], "title": "Stable Trajectory Clustering: An Efficient Split and Merge Algorithm", "categories": ["cs.LG", "cs.CG"], "comment": null, "summary": "Clustering algorithms group data points by characteristics to identify\npatterns. Over the past two decades, researchers have extended these methods to\nanalyze trajectories of humans, animals, and vehicles, studying their behavior\nand movement across applications. This paper presents whole-trajectory\nclustering and sub-trajectory clustering algorithms based on DBSCAN line\nsegment clustering, which encompasses two key events: split and merge of line\nsegments. The events are employed by object movement history and the average\nEuclidean distance between line segments. In this framework, whole-trajectory\nclustering considers entire entities' trajectories, whereas sub-trajectory\nclustering employs a sliding window model to identify similar sub-trajectories.\nMany existing trajectory clustering algorithms respond to temporary anomalies\nin data by splitting trajectories, which often obscures otherwise consistent\nclustering patterns and leads to less reliable insights. We introduce the\nstable trajectory clustering algorithm, which leverages the mean absolute\ndeviation concept to demonstrate that selective omission of transient\ndeviations not only preserves the integrity of clusters but also improves their\nstability and interpretability. We run all proposed algorithms on real\ntrajectory datasets to illustrate their effectiveness and sensitivity to\nparameter variations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8eDBSCAN\u7ebf\u6bb5\u805a\u7c7b\u7684\u5168\u8f68\u8ff9\u548c\u5b50\u8f68\u8ff9\u805a\u7c7b\u7b97\u6cd5\uff0c\u5f15\u5165\u7a33\u5b9a\u8f68\u8ff9\u805a\u7c7b\u7b97\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5ffd\u7565\u77ac\u6001\u504f\u5dee\u63d0\u5347\u805a\u7c7b\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u805a\u7c7b\u7b97\u6cd5\u56e0\u4e34\u65f6\u5f02\u5e38\u6570\u636e\u5206\u5272\u8f68\u8ff9\uff0c\u5bfc\u81f4\u805a\u7c7b\u6a21\u5f0f\u6a21\u7cca\u548c\u53ef\u9760\u6027\u964d\u4f4e\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u57fa\u4e8eDBSCAN\u7ebf\u6bb5\u805a\u7c7b\uff0c\u5229\u7528\u7ebf\u6bb5\u7684\u5206\u88c2\u4e0e\u5408\u5e76\u4e8b\u4ef6\uff0c\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u6a21\u578b\u548c\u5e73\u5747\u6b27\u6c0f\u8ddd\u79bb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7a33\u5b9a\u8f68\u8ff9\u805a\u7c7b\u7b97\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u805a\u7c7b\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u9009\u62e9\u6027\u5ffd\u7565\u77ac\u6001\u504f\u5dee\u53ef\u4f18\u5316\u8f68\u8ff9\u805a\u7c7b\u6548\u679c\uff0c\u7b97\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2504.21851", "pdf": "https://arxiv.org/pdf/2504.21851", "abs": "https://arxiv.org/abs/2504.21851", "authors": ["Sichang Tu", "Abigail Powers", "Stephen Doogan", "Jinho D. Choi"], "title": "TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments", "categories": ["cs.CL", "cs.AI"], "comment": "5 figures, 4 tables", "summary": "Objectives: While Large Language Models (LLMs) have been widely used to\nassist clinicians and support patients, no existing work has explored dialogue\nsystems for standard diagnostic interviews and assessments. This study aims to\nbridge the gap in mental healthcare accessibility by developing an LLM-powered\ndialogue system that replicates clinician behavior. Materials and Methods: We\nintroduce TRUST, a framework of cooperative LLM modules capable of conducting\nformal diagnostic interviews and assessments for Post-Traumatic Stress Disorder\n(PTSD). To guide the generation of appropriate clinical responses, we propose a\nDialogue Acts schema specifically designed for clinical interviews.\nAdditionally, we develop a patient simulation approach based on real-life\ninterview transcripts to replace time-consuming and costly manual testing by\nclinicians. Results: A comprehensive set of evaluation metrics is designed to\nassess the dialogue system from both the agent and patient simulation\nperspectives. Expert evaluations by conversation and clinical specialists show\nthat TRUST performs comparably to real-life clinical interviews. Discussion:\nOur system performs at the level of average clinicians, with room for future\nenhancements in communication styles and response appropriateness. Conclusions:\nOur TRUST framework shows its potential to facilitate mental healthcare\navailability.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u7cfb\u7edfTRUST\uff0c\u7528\u4e8e\u6807\u51c6\u5316\u8bca\u65ad\u8bbf\u8c08\uff0c\u586b\u8865\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u6280\u672f\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u672a\u63a2\u7d22LLM\u5728\u6807\u51c6\u5316\u8bca\u65ad\u8bbf\u8c08\u4e2d\u7684\u5e94\u7528\uff0c\u7814\u7a76\u65e8\u5728\u63d0\u5347\u5fc3\u7406\u5065\u5eb7\u670d\u52a1\u7684\u53ef\u53ca\u6027\u3002", "method": "\u63d0\u51faTRUST\u6846\u67b6\uff0c\u7ed3\u5408\u4e34\u5e8a\u5bf9\u8bdd\u884c\u4e3a\u6a21\u5f0f\u548c\u60a3\u8005\u6a21\u62df\u65b9\u6cd5\uff0c\u7528\u4e8ePTSD\u8bca\u65ad\u3002", "result": "\u4e13\u5bb6\u8bc4\u4f30\u663e\u793aTRUST\u8868\u73b0\u63a5\u8fd1\u771f\u5b9e\u4e34\u5e8a\u8bbf\u8c08\uff0c\u5177\u5907\u5b9e\u7528\u6027\u3002", "conclusion": "TRUST\u6846\u67b6\u5c55\u73b0\u4e86\u63d0\u5347\u5fc3\u7406\u5065\u5eb7\u670d\u52a1\u53ef\u7528\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.21561", "pdf": "https://arxiv.org/pdf/2504.21561", "abs": "https://arxiv.org/abs/2504.21561", "authors": ["Pengxiang Li", "Zhi Gao", "Bofei Zhang", "Yapeng Mi", "Xiaojian Ma", "Chenrui Shi", "Tao Yuan", "Yuwei Wu", "Yunde Jia", "Song-Chun Zhu", "Qing Li"], "title": "Iterative Trajectory Exploration for Multimodal Agents", "categories": ["cs.CV"], "comment": "16 pages, 8 figures", "summary": "Multimodal agents, which integrate a controller (e.g., a large language\nmodel) with external tools, have demonstrated remarkable capabilities in\ntackling complex tasks. However, existing agents need to collect a large number\nof expert data for fine-tuning to adapt to new environments. In this paper, we\npropose an online self-exploration method for multimodal agents, namely SPORT,\nvia step-wise preference optimization to refine the trajectories of agents,\nwhich automatically generates tasks and learns from solving the generated\ntasks, without any expert annotation. SPORT operates through four iterative\ncomponents: task synthesis, step sampling, step verification, and preference\ntuning. First, we synthesize multi-modal tasks using language models. Then, we\nintroduce a novel search scheme, where step sampling and step verification are\nexecuted alternately to solve each generated task. We employ a verifier to\nprovide AI feedback to construct step-wise preference data. The data is\nsubsequently used to update the controller's policy through preference tuning,\nproducing a SPORT Agent. By interacting with real environments, the SPORT Agent\nevolves into a more refined and capable system. Evaluation in the GTA and GAIA\nbenchmarks show that the SPORT Agent achieves 6.41\\% and 3.64\\% improvements,\nunderscoring the generalization and effectiveness introduced by our method. The\nproject page is https://SPORT-Agents.github.io.", "AI": {"tldr": "SPORT\u662f\u4e00\u79cd\u5728\u7ebf\u81ea\u63a2\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u6b65\u504f\u597d\u4f18\u5316\u6539\u8fdb\u591a\u6a21\u6001\u4ee3\u7406\u7684\u8f68\u8ff9\uff0c\u65e0\u9700\u4e13\u5bb6\u6807\u6ce8\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u4ee5\u9002\u5e94\u65b0\u73af\u5883\uff0cSPORT\u65e8\u5728\u901a\u8fc7\u81ea\u751f\u6210\u4efb\u52a1\u548c\u5b66\u4e60\u89e3\u51b3\u4efb\u52a1\u6765\u51cf\u5c11\u5bf9\u4e13\u5bb6\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "SPORT\u901a\u8fc7\u4efb\u52a1\u5408\u6210\u3001\u6b65\u9aa4\u91c7\u6837\u3001\u6b65\u9aa4\u9a8c\u8bc1\u548c\u504f\u597d\u8c03\u4f18\u56db\u4e2a\u8fed\u4ee3\u7ec4\u4ef6\u5b9e\u73b0\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4efb\u52a1\u5e76\u901a\u8fc7AI\u53cd\u9988\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728GTA\u548cGAIA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPORT\u4ee3\u7406\u5206\u522b\u5b9e\u73b0\u4e866.41%\u548c3.64%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "SPORT\u5c55\u793a\u4e86\u901a\u8fc7\u81ea\u751f\u6210\u4efb\u52a1\u548c\u504f\u597d\u4f18\u5316\u63d0\u5347\u591a\u6a21\u6001\u4ee3\u7406\u80fd\u529b\u7684\u6f5c\u529b\uff0c\u5177\u6709\u6cdb\u5316\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2504.19835", "pdf": "https://arxiv.org/pdf/2504.19835", "abs": "https://arxiv.org/abs/2504.19835", "authors": ["Cornelius Hake", "Christian Friedrich"], "title": "Automated Generation of Precedence Graphs in Digital Value Chains for Automotive Production", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "This study examines the digital value chain in automotive manufacturing,\nfocusing on the identification, software flashing, customization, and\ncommissioning of electronic control units in vehicle networks. A novel\nprecedence graph design is proposed to optimize this process chain using an\nautomated scheduling algorithm that employs mixed integer linear programming\ntechniques. The results show significant improvements in key metrics. The\nalgorithm reduces the number of production stations equipped with expensive\nhardware and software to execute digital value chain processes, while\nincreasing capacity utilization through efficient scheduling and reduced idle\ntime. Task parallelization is optimized, resulting in streamlined workflows and\nincreased throughput. Compared to the traditional method, the automated\napproach has reduced preparation time by 50% and reduced scheduling activities,\nas it now takes two minutes to create the precedence graph. The flexibility of\nthe algorithm's constraints allows for vehicle-specific configurations while\nmaintaining high responsiveness, eliminating backup stations and facilitating\nthe integration of new topologies. Automated scheduling significantly\noutperforms manual methods in efficiency, functionality, and adaptability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u7684\u81ea\u52a8\u5316\u8c03\u5ea6\u7b97\u6cd5\uff0c\u4f18\u5316\u6c7d\u8f66\u5236\u9020\u4e2d\u7684\u6570\u5b57\u4ef7\u503c\u94fe\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u4e0e\u9002\u5e94\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4f18\u5316\u6c7d\u8f66\u5236\u9020\u4e2d\u7535\u5b50\u63a7\u5236\u5355\u5143\u7684\u8bc6\u522b\u3001\u8f6f\u4ef6\u5237\u65b0\u3001\u5b9a\u5236\u548c\u8c03\u8bd5\u6d41\u7a0b\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u3001\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u524d\u9a71\u56fe\u8bbe\u8ba1\uff0c\u7ed3\u5408\u6df7\u5408\u6574\u6570\u7ebf\u6027\u7f16\u7a0b\u6280\u672f\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u8c03\u5ea6\u7b97\u6cd5\u3002", "result": "\u7b97\u6cd5\u51cf\u5c11\u4e86\u6602\u8d35\u786c\u4ef6\u548c\u8f6f\u4ef6\u7684\u751f\u4ea7\u7ad9\u6570\u91cf\uff0c\u63d0\u9ad8\u4ea7\u80fd\u5229\u7528\u7387\uff0c\u4efb\u52a1\u5e76\u884c\u5316\u4f18\u5316\uff0c\u51c6\u5907\u65f6\u95f4\u51cf\u5c1150%\uff0c\u8c03\u5ea6\u6d3b\u52a8\u5927\u5e45\u7b80\u5316\u3002", "conclusion": "\u81ea\u52a8\u5316\u8c03\u5ea6\u5728\u6548\u7387\u3001\u529f\u80fd\u548c\u9002\u5e94\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u624b\u52a8\u65b9\u6cd5\uff0c\u652f\u6301\u8f66\u8f86\u7279\u5b9a\u914d\u7f6e\u5e76\u63d0\u5347\u54cd\u5e94\u901f\u5ea6\u3002"}}
{"id": "2504.21015", "pdf": "https://arxiv.org/pdf/2504.21015", "abs": "https://arxiv.org/abs/2504.21015", "authors": ["Aarush Sinha"], "title": "Don't Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Training effective dense retrieval models often relies on hard negative (HN)\nexamples mined from the document corpus via methods like BM25 or cross-encoders\n(CE), processes that can be computationally demanding and require full corpus\naccess. This paper introduces a different approach, an end-to-end pipeline\nwhere a Large Language Model (LLM) first generates a query from a passage, and\nthen generates a hard negative example using \\emph{only} that query text. This\ncorpus-free negative generation contrasts with standard mining techniques. We\nevaluated this \\textsc{LLM Query $\\rightarrow$ LLM HN} approach against\ntraditional \\textsc{LLM Query $\\rightarrow$ BM25 HN} and \\textsc{LLM Query\n$\\rightarrow$ CE HN} pipelines using E5-Base and GTE-Base models on several\nBEIR benchmark datasets. Our results show the proposed all-LLM pipeline\nachieves performance identical to both the BM25 and the computationally\nintensive CE baselines across nDCG@10, Precision@10, and Recall@100 metrics.\nThis demonstrates that our corpus-free negative generation method matches the\neffectiveness of complex, corpus-dependent mining techniques, offering a\npotentially simpler and more efficient pathway for training high-performance\nretrievers without sacrificing results. We make the dataset including the\nqueries and the hard-negatives for all three methods publicly available\nhttps://huggingface.co/collections/chungimungi/arxiv-hard-negatives-68027bbc601ff6cc8eb1f449.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u786c\u8d1f\u4f8b\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u6587\u6863\u5e93\uff0c\u6027\u80fd\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\u3002", "motivation": "\u4f20\u7edf\u786c\u8d1f\u4f8b\u6316\u6398\u65b9\u6cd5\uff08\u5982BM25\u6216\u4ea4\u53c9\u7f16\u7801\u5668\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u9700\u8bbf\u95ee\u5b8c\u6574\u6587\u6863\u5e93\uff0c\u56e0\u6b64\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528LLM\u751f\u6210\u67e5\u8be2\uff0c\u5e76\u4ec5\u57fa\u4e8e\u67e5\u8be2\u6587\u672c\u751f\u6210\u786c\u8d1f\u4f8b\uff0c\u5f62\u6210\u7aef\u5230\u7aef\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4e0eBM25\u548c\u4ea4\u53c9\u7f16\u7801\u5668\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u9ad8\u6548\u7684\u8bad\u7ec3\u9ad8\u6027\u80fd\u68c0\u7d22\u6a21\u578b\u7684\u65b9\u5f0f\uff0c\u65e0\u9700\u727a\u7272\u6548\u679c\u3002"}}
{"id": "2504.21562", "pdf": "https://arxiv.org/pdf/2504.21562", "abs": "https://arxiv.org/abs/2504.21562", "authors": ["Henry John Krumb", "Anirban Mukhopadhyay"], "title": "eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Wireless Capsule Endoscopy is a non-invasive imaging method for the entire\ngastrointestinal tract, and is a pain-free alternative to traditional\nendoscopy. It generates extensive video data that requires significant review\ntime, and localizing the capsule after ingestion is a challenge. Techniques\nlike bleeding detection and depth estimation can help with localization of\npathologies, but deep learning models are typically too large to run directly\non the capsule. Neural Cellular Automata (NCA) for bleeding segmentation and\ndepth estimation are trained on capsule endoscopic images. For monocular depth\nestimation, we distill a large foundation model into the lean NCA architecture,\nby treating the outputs of the foundation model as pseudo ground truth. We then\nport the trained NCA to the ESP32 microcontroller, enabling efficient image\nprocessing on hardware as small as a camera capsule. NCA are more accurate\n(Dice) than other portable segmentation models, while requiring more than 100x\nfewer parameters stored in memory than other small-scale models. The visual\nresults of NCA depth estimation look convincing, and in some cases beat the\nrealism and detail of the pseudo ground truth. Runtime optimizations on the\nESP32-S3 accelerate the average inference speed significantly, by more than\nfactor 3. With several algorithmic adjustments and distillation, it is possible\nto eNCApsulate NCA models into microcontrollers that fit into wireless capsule\nendoscopes. This is the first work that enables reliable bleeding segmentation\nand depth estimation on a miniaturized device, paving the way for precise\ndiagnosis combined with visual odometry as a means of precise localization of\nthe capsule -- on the capsule.", "AI": {"tldr": "\u65e0\u7ebf\u80f6\u56ca\u5185\u7aa5\u955c\u901a\u8fc7\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08NCA\uff09\u5b9e\u73b0\u51fa\u8840\u5206\u5272\u548c\u6df1\u5ea6\u4f30\u8ba1\uff0c\u6a21\u578b\u8f7b\u91cf\u5316\u540e\u53ef\u5728\u5fae\u578b\u8bbe\u5907\uff08\u5982ESP32\uff09\u4e0a\u9ad8\u6548\u8fd0\u884c\uff0c\u663e\u8457\u63d0\u5347\u8bca\u65ad\u7cbe\u5ea6\u548c\u5b9a\u4f4d\u80fd\u529b\u3002", "motivation": "\u65e0\u7ebf\u80f6\u56ca\u5185\u7aa5\u955c\u751f\u6210\u5927\u91cf\u89c6\u9891\u6570\u636e\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5b9e\u65f6\u5904\u7406\u4e14\u5b9a\u4f4d\u56f0\u96be\uff0c\u9700\u8f7b\u91cf\u5316\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u75c5\u7406\u68c0\u6d4b\u3002", "method": "\u901a\u8fc7\u84b8\u998f\u5927\u578b\u57fa\u7840\u6a21\u578b\u5230NCA\u67b6\u6784\uff0c\u751f\u6210\u4f2a\u771f\u503c\u8bad\u7ec3NCA\uff0c\u5e76\u5c06\u5176\u79fb\u690d\u5230ESP32\u5fae\u63a7\u5236\u5668\uff0c\u4f18\u5316\u63a8\u7406\u901f\u5ea6\u3002", "result": "NCA\u5728\u51fa\u8840\u5206\u5272\u4e0a\u6bd4\u5176\u4ed6\u8f7b\u91cf\u6a21\u578b\u66f4\u51c6\u786e\uff08Dice\uff09\uff0c\u53c2\u6570\u51cf\u5c11100\u500d\u4ee5\u4e0a\uff0c\u6df1\u5ea6\u4f30\u8ba1\u6548\u679c\u4f18\u4e8e\u4f2a\u771f\u503c\uff0cESP32-S3\u63a8\u7406\u901f\u5ea6\u63d0\u53473\u500d\u3002", "conclusion": "NCA\u9996\u6b21\u5728\u5fae\u578b\u8bbe\u5907\u4e0a\u5b9e\u73b0\u53ef\u9760\u51fa\u8840\u5206\u5272\u548c\u6df1\u5ea6\u4f30\u8ba1\uff0c\u4e3a\u80f6\u56ca\u5185\u7aa5\u955c\u7684\u7cbe\u786e\u8bca\u65ad\u548c\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2504.20185", "pdf": "https://arxiv.org/pdf/2504.20185", "abs": "https://arxiv.org/abs/2504.20185", "authors": ["Aspen Hopkins", "Sarah H. Cen", "Andrew Ilyas", "Isabella Struckman", "Luis Videgaray", "Aleksander M\u0105dry"], "title": "AI Supply Chains: An Emerging Ecosystem of AI Actors, Products, and Services", "categories": ["cs.CY", "cs.LG"], "comment": "27 pages, 8 figures", "summary": "The widespread adoption of AI in recent years has led to the emergence of AI\nsupply chains: complex networks of AI actors contributing models, datasets, and\nmore to the development of AI products and services. AI supply chains have many\nimplications yet are poorly understood. In this work, we take a first step\ntoward a formal study of AI supply chains and their implications, providing two\nillustrative case studies indicating that both AI development and regulation\nare complicated in the presence of supply chains. We begin by presenting a\nbrief historical perspective on AI supply chains, discussing how their rise\nreflects a longstanding shift towards specialization and outsourcing that\nsignals the healthy growth of the AI industry. We then model AI supply chains\nas directed graphs and demonstrate the power of this abstraction by connecting\nexamples of AI issues to graph properties. Finally, we examine two case studies\nin detail, providing theoretical and empirical results in both. In the first,\nwe show that information passing (specifically, of explanations) along the AI\nsupply chains is imperfect, which can result in misunderstandings that have\nreal-world implications. In the second, we show that upstream design choices\n(e.g., by base model providers) have downstream consequences (e.g., on AI\nproducts fine-tuned on the base model). Together, our findings motivate further\nstudy of AI supply chains and their increasingly salient social, economic,\nregulatory, and technical implications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5bf9AI\u4f9b\u5e94\u94fe\u53ca\u5176\u5f71\u54cd\u8fdb\u884c\u4e86\u5f62\u5f0f\u5316\u7814\u7a76\uff0c\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u5c55\u793a\u4e86AI\u5f00\u53d1\u548c\u76d1\u7ba1\u5728\u4f9b\u5e94\u94fe\u4e2d\u7684\u590d\u6742\u6027\u3002", "motivation": "\u968f\u7740AI\u7684\u5e7f\u6cdb\u5e94\u7528\uff0cAI\u4f9b\u5e94\u94fe\uff08\u6d89\u53ca\u6a21\u578b\u3001\u6570\u636e\u7b49\u7684\u590d\u6742\u7f51\u7edc\uff09\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\uff0c\u4f46\u5176\u5f71\u54cd\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5386\u53f2\u89c6\u89d2\u5206\u6790AI\u4f9b\u5e94\u94fe\u7684\u5174\u8d77\uff0c\u5c06\u5176\u5efa\u6a21\u4e3a\u6709\u5411\u56fe\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\uff08\u4fe1\u606f\u4f20\u9012\u548c\u4e0a\u6e38\u8bbe\u8ba1\u9009\u62e9\u7684\u5f71\u54cd\uff09\u5c55\u793a\u5176\u590d\u6742\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cAI\u4f9b\u5e94\u94fe\u4e2d\u7684\u4fe1\u606f\u4f20\u9012\u4e0d\u5b8c\u5584\u53ef\u80fd\u5bfc\u81f4\u8bef\u89e3\uff0c\u800c\u4e0a\u6e38\u8bbe\u8ba1\u9009\u62e9\u4f1a\u663e\u8457\u5f71\u54cd\u4e0b\u6e38\u4ea7\u54c1\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76AI\u4f9b\u5e94\u94fe\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u5e94\u5bf9\u5176\u65e5\u76ca\u663e\u8457\u7684\u793e\u4f1a\u3001\u7ecf\u6d4e\u3001\u76d1\u7ba1\u548c\u6280\u672f\u5f71\u54cd\u3002"}}
{"id": "2504.21035", "pdf": "https://arxiv.org/pdf/2504.21035", "abs": "https://arxiv.org/abs/2504.21035", "authors": ["Rui Xin", "Niloofar Mireshghallah", "Shuyue Stella Li", "Michael Duan", "Hyunwoo Kim", "Yejin Choi", "Yulia Tsvetkov", "Sewoong Oh", "Pang Wei Koh"], "title": "A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage", "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": null, "summary": "Sanitizing sensitive text data typically involves removing personally\nidentifiable information (PII) or generating synthetic data under the\nassumption that these methods adequately protect privacy; however, their\neffectiveness is often only assessed by measuring the leakage of explicit\nidentifiers but ignoring nuanced textual markers that can lead to\nre-identification. We challenge the above illusion of privacy by proposing a\nnew framework that evaluates re-identification attacks to quantify individual\nprivacy risks upon data release. Our approach shows that seemingly innocuous\nauxiliary information -- such as routine social activities -- can be used to\ninfer sensitive attributes like age or substance use history from sanitized\ndata. For instance, we demonstrate that Azure's commercial PII removal tool\nfails to protect 74\\% of information in the MedQA dataset. Although\ndifferential privacy mitigates these risks to some extent, it significantly\nreduces the utility of the sanitized text for downstream tasks. Our findings\nindicate that current sanitization techniques offer a \\textit{false sense of\nprivacy}, highlighting the need for more robust methods that protect against\nsemantic-level information leakage.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8131\u654f\u6570\u636e\u7684\u518d\u8bc6\u522b\u98ce\u9669\uff0c\u63ed\u793a\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u6570\u636e\u8131\u654f\u65b9\u6cd5\u4ec5\u5173\u6ce8\u663e\u5f0f\u6807\u8bc6\u7b26\u7684\u6cc4\u6f0f\uff0c\u5ffd\u7565\u7ec6\u5fae\u6587\u672c\u6807\u8bb0\u5bfc\u81f4\u7684\u9690\u79c1\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u518d\u8bc6\u522b\u653b\u51fb\u91cf\u5316\u9690\u79c1\u98ce\u9669\uff0c\u5e76\u6d4b\u8bd5\u5546\u4e1a\u5de5\u5177\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cAzure\u7684PII\u79fb\u9664\u5de5\u5177\u5728MedQA\u6570\u636e\u96c6\u4e2d\u4fdd\u62a4\u5931\u8d25\u7387\u8fbe74%\uff0c\u5dee\u5206\u9690\u79c1\u867d\u6709\u6548\u4f46\u964d\u4f4e\u6570\u636e\u5b9e\u7528\u6027\u3002", "conclusion": "\u5f53\u524d\u8131\u654f\u6280\u672f\u5b58\u5728\u865a\u5047\u9690\u79c1\u611f\uff0c\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u4ee5\u9632\u6b62\u8bed\u4e49\u7ea7\u4fe1\u606f\u6cc4\u6f0f\u3002"}}
{"id": "2504.21598", "pdf": "https://arxiv.org/pdf/2504.21598", "abs": "https://arxiv.org/abs/2504.21598", "authors": ["Thomas L. Athey", "Shashata Sawmya", "Nir Shavit"], "title": "Cascade Detector Analysis and Application to Biomedical Microscopy", "categories": ["cs.CV"], "comment": null, "summary": "As both computer vision models and biomedical datasets grow in size, there is\nan increasing need for efficient inference algorithms. We utilize cascade\ndetectors to efficiently identify sparse objects in multiresolution images.\nGiven an object's prevalence and a set of detectors at different resolutions\nwith known accuracies, we derive the accuracy, and expected number of\nclassifier calls by a cascade detector. These results generalize across number\nof dimensions and number of cascade levels. Finally, we compare one- and\ntwo-level detectors in fluorescent cell detection, organelle segmentation, and\ntissue segmentation across various microscopy modalities. We show that the\nmulti-level detector achieves comparable performance in 30-75% less time. Our\nwork is compatible with a variety of computer vision models and data domains.", "AI": {"tldr": "\u5229\u7528\u7ea7\u8054\u68c0\u6d4b\u5668\u9ad8\u6548\u8bc6\u522b\u591a\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u7684\u7a00\u758f\u76ee\u6807\uff0c\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f430-75%\u3002", "motivation": "\u968f\u7740\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u548c\u751f\u7269\u533b\u5b66\u6570\u636e\u96c6\u7684\u89c4\u6a21\u589e\u957f\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u7b97\u6cd5\u3002", "method": "\u901a\u8fc7\u7ea7\u8054\u68c0\u6d4b\u5668\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u8bc6\u522b\u76ee\u6807\uff0c\u5e76\u5206\u6790\u5176\u51c6\u786e\u6027\u548c\u5206\u7c7b\u5668\u8c03\u7528\u6b21\u6570\u3002", "result": "\u591a\u7ea7\u68c0\u6d4b\u5668\u5728\u8367\u5149\u7ec6\u80de\u68c0\u6d4b\u3001\u7ec6\u80de\u5668\u5206\u5272\u548c\u7ec4\u7ec7\u5206\u5272\u4e2d\u6027\u80fd\u76f8\u5f53\uff0c\u4f46\u65f6\u95f4\u51cf\u5c1130-75%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u548c\u6570\u636e\u9886\u57df\u3002"}}
{"id": "2504.21614", "pdf": "https://arxiv.org/pdf/2504.21614", "abs": "https://arxiv.org/abs/2504.21614", "authors": ["Daniel Bogdoll", "Rajanikant Patnaik Ananta", "Abeyankar Giridharan", "Isabel Moore", "Gregory Stevens", "Henry X. Liu"], "title": "Mcity Data Engine: Iterative Model Improvement Through Open-Vocabulary Data Selection", "categories": ["cs.CV"], "comment": null, "summary": "With an ever-increasing availability of data, it has become more and more\nchallenging to select and label appropriate samples for the training of machine\nlearning models. It is especially difficult to detect long-tail classes of\ninterest in large amounts of unlabeled data. This holds especially true for\nIntelligent Transportation Systems (ITS), where vehicle fleets and roadside\nperception systems generate an abundance of raw data. While industrial,\nproprietary data engines for such iterative data selection and model training\nprocesses exist, researchers and the open-source community suffer from a lack\nof an openly available system. We present the Mcity Data Engine, which provides\nmodules for the complete data-based development cycle, beginning at the data\nacquisition phase and ending at the model deployment stage. The Mcity Data\nEngine focuses on rare and novel classes through an open-vocabulary data\nselection process. All code is publicly available on GitHub under an MIT\nlicense: https://github.com/mcity/mcity_data_engine", "AI": {"tldr": "Mcity Data Engine\u662f\u4e00\u4e2a\u5f00\u6e90\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u6570\u636e\u9009\u62e9\u548c\u6807\u6ce8\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u9488\u5bf9\u957f\u5c3e\u7c7b\u522b\u3002", "motivation": "\u968f\u7740\u6570\u636e\u91cf\u7684\u589e\u52a0\uff0c\u9009\u62e9\u548c\u6807\u6ce8\u9002\u5408\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6837\u672c\u53d8\u5f97\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u3002\u73b0\u6709\u5de5\u4e1a\u6570\u636e\u5f15\u64ce\u591a\u4e3a\u4e13\u6709\uff0c\u7f3a\u4e4f\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\u3002", "method": "Mcity Data Engine\u63d0\u4f9b\u5b8c\u6574\u7684\u6570\u636e\u5f00\u53d1\u5468\u671f\u6a21\u5757\uff0c\u4ece\u6570\u636e\u91c7\u96c6\u5230\u6a21\u578b\u90e8\u7f72\uff0c\u4e13\u6ce8\u4e8e\u901a\u8fc7\u5f00\u653e\u8bcd\u6c47\u6570\u636e\u9009\u62e9\u5904\u7406\u7f55\u89c1\u548c\u65b0\u7c7b\u522b\u3002", "result": "\u7cfb\u7edf\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u516c\u5f00\uff0c\u91c7\u7528MIT\u8bb8\u53ef\u8bc1\u3002", "conclusion": "Mcity Data Engine\u586b\u8865\u4e86\u5f00\u6e90\u793e\u533a\u5728\u6570\u636e\u9009\u62e9\u548c\u6a21\u578b\u8bad\u7ec3\u5de5\u5177\u4e0a\u7684\u7a7a\u767d\uff0c\u7279\u522b\u9002\u7528\u4e8e\u667a\u80fd\u4ea4\u901a\u9886\u57df\u3002"}}
{"id": "2504.21072", "pdf": "https://arxiv.org/pdf/2504.21072", "abs": "https://arxiv.org/abs/2504.21072", "authors": ["Jonas Henry Grebe", "Tobias Braun", "Marcus Rohrbach", "Anna Rohrbach"], "title": "Erased but Not Forgotten: How Backdoors Compromise Concept Erasure", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "The expansion of large-scale text-to-image diffusion models has raised\ngrowing concerns about their potential to generate undesirable or harmful\ncontent, ranging from fabricated depictions of public figures to sexually\nexplicit images. To mitigate these risks, prior work has devised machine\nunlearning techniques that attempt to erase unwanted concepts through\nfine-tuning. However, in this paper, we introduce a new threat model, Toxic\nErasure (ToxE), and demonstrate how recent unlearning algorithms, including\nthose explicitly designed for robustness, can be circumvented through targeted\nbackdoor attacks. The threat is realized by establishing a link between a\ntrigger and the undesired content. Subsequent unlearning attempts fail to erase\nthis link, allowing adversaries to produce harmful content. We instantiate ToxE\nvia two established backdoor attacks: one targeting the text encoder and\nanother manipulating the cross-attention layers. Further, we introduce Deep\nIntervention Score-based Attack (DISA), a novel, deeper backdoor attack that\noptimizes the entire U-Net using a score-based objective, improving the\nattack's persistence across different erasure methods. We evaluate five recent\nconcept erasure methods against our threat model. For celebrity identity\nerasure, our deep attack circumvents erasure with up to 82% success, averaging\n57% across all erasure methods. For explicit content erasure, ToxE attacks can\nelicit up to 9 times more exposed body parts, with DISA yielding an average\nincrease by a factor of 2.9. These results highlight a critical security gap in\ncurrent unlearning strategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5a01\u80c1\u6a21\u578bToxic Erasure (ToxE)\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u673a\u5668\u5b66\u4e60\u53bb\u5b66\u4e60\u6280\u672f\u53ef\u80fd\u88ab\u9488\u5bf9\u6027\u540e\u95e8\u653b\u51fb\u7ed5\u8fc7\uff0c\u5bfc\u81f4\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u3002", "motivation": "\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u6269\u5c55\u5f15\u53d1\u4e86\u5bf9\u5176\u751f\u6210\u4e0d\u826f\u6216\u6709\u5bb3\u5185\u5bb9\u7684\u62c5\u5fe7\uff0c\u73b0\u6709\u53bb\u5b66\u4e60\u6280\u672f\u53ef\u80fd\u88ab\u653b\u51fb\u7ed5\u8fc7\u3002", "method": "\u63d0\u51faToxE\u5a01\u80c1\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e24\u79cd\u540e\u95e8\u653b\u51fb\uff08\u9488\u5bf9\u6587\u672c\u7f16\u7801\u5668\u548c\u8de8\u6ce8\u610f\u529b\u5c42\uff09\uff0c\u5e76\u5f15\u5165\u65b0\u578b\u653b\u51fbDISA\u3002", "result": "ToxE\u653b\u51fb\u5728\u540d\u4eba\u8eab\u4efd\u548c\u9732\u9aa8\u5185\u5bb9\u53bb\u5b66\u4e60\u4e2d\u5206\u522b\u8fbe\u523082%\u548c9\u500d\u7684\u6210\u529f\u7387\uff0cDISA\u653b\u51fb\u5e73\u5747\u63d0\u53472.9\u500d\u3002", "conclusion": "\u5f53\u524d\u53bb\u5b66\u4e60\u7b56\u7565\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u9632\u5fa1\u65b9\u6cd5\u3002"}}
{"id": "2504.21646", "pdf": "https://arxiv.org/pdf/2504.21646", "abs": "https://arxiv.org/abs/2504.21646", "authors": ["Liqin Wang", "Qianyue Hu", "Wei Lu", "Xiangyang Luo"], "title": "Diffusion-based Adversarial Identity Manipulation for Facial Privacy Protection", "categories": ["cs.CV"], "comment": null, "summary": "The success of face recognition (FR) systems has led to serious privacy\nconcerns due to potential unauthorized surveillance and user tracking on social\nnetworks. Existing methods for enhancing privacy fail to generate natural face\nimages that can protect facial privacy. In this paper, we propose\ndiffusion-based adversarial identity manipulation (DiffAIM) to generate natural\nand highly transferable adversarial faces against malicious FR systems. To be\nspecific, we manipulate facial identity within the low-dimensional latent space\nof a diffusion model. This involves iteratively injecting gradient-based\nadversarial identity guidance during the reverse diffusion process,\nprogressively steering the generation toward the desired adversarial faces. The\nguidance is optimized for identity convergence towards a target while promoting\nsemantic divergence from the source, facilitating effective impersonation while\nmaintaining visual naturalness. We further incorporate structure-preserving\nregularization to preserve facial structure consistency during manipulation.\nExtensive experiments on both face verification and identification tasks\ndemonstrate that compared with the state-of-the-art, DiffAIM achieves stronger\nblack-box attack transferability while maintaining superior visual quality. We\nalso demonstrate the effectiveness of the proposed approach for commercial FR\nAPIs, including Face++ and Aliyun.", "AI": {"tldr": "DiffAIM\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5bf9\u6297\u6027\u4eba\u8138\u751f\u6210\u65b9\u6cd5\uff0c\u65e8\u5728\u4fdd\u62a4\u9690\u79c1\uff0c\u901a\u8fc7\u64cd\u7eb5\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\u751f\u6210\u81ea\u7136\u4e14\u9ad8\u8fc1\u79fb\u6027\u7684\u5bf9\u6297\u4eba\u8138\u3002", "motivation": "\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u65e0\u6cd5\u751f\u6210\u81ea\u7136\u7684\u5bf9\u6297\u4eba\u8138\uff0c\u5bfc\u81f4\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u53ef\u80fd\u88ab\u6ee5\u7528\uff0c\u5f15\u53d1\u9690\u79c1\u95ee\u9898\u3002", "method": "\u5728\u6269\u6563\u6a21\u578b\u7684\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\u4e2d\uff0c\u8fed\u4ee3\u6ce8\u5165\u57fa\u4e8e\u68af\u5ea6\u7684\u5bf9\u6297\u6027\u8eab\u4efd\u5f15\u5bfc\uff0c\u4f18\u5316\u8eab\u4efd\u6536\u655b\u548c\u8bed\u4e49\u5dee\u5f02\uff0c\u540c\u65f6\u901a\u8fc7\u7ed3\u6784\u4fdd\u6301\u6b63\u5219\u5316\u4fdd\u6301\u9762\u90e8\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "result": "DiffAIM\u5728\u9762\u90e8\u9a8c\u8bc1\u548c\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9ed1\u76d2\u653b\u51fb\u8fc1\u79fb\u6027\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u5e76\u5728\u5546\u4e1aAPI\uff08\u5982Face++\u548cAliyun\uff09\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "DiffAIM\u4e3a\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u89c6\u89c9\u81ea\u7136\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5bf9\u6297\u6076\u610f\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u3002"}}
{"id": "2504.21074", "pdf": "https://arxiv.org/pdf/2504.21074", "abs": "https://arxiv.org/abs/2504.21074", "authors": ["Adrian Rebmann", "Fabian David Schmidt", "Goran Glava\u0161", "Han van der Aa"], "title": "On the Potential of Large Language Models to Solve Semantics-Aware Process Mining Tasks", "categories": ["cs.DB", "cs.AI"], "comment": "31 pages, submitted to PS", "summary": "Large language models (LLMs) have shown to be valuable tools for tackling\nprocess mining tasks. Existing studies report on their capability to support\nvarious data-driven process analyses and even, to some extent, that they are\nable to reason about how processes work. This reasoning ability suggests that\nthere is potential for LLMs to tackle semantics-aware process mining tasks,\nwhich are tasks that rely on an understanding of the meaning of activities and\ntheir relationships. Examples of these include process discovery, where the\nmeaning of activities can indicate their dependency, whereas in anomaly\ndetection the meaning can be used to recognize process behavior that is\nabnormal. In this paper, we systematically explore the capabilities of LLMs for\nsuch tasks. Unlike prior work, which largely evaluates LLMs in their default\nstate, we investigate their utility through both in-context learning and\nsupervised fine-tuning. Concretely, we define five process mining tasks\nrequiring semantic understanding and provide extensive benchmarking datasets\nfor evaluation. Our experiments reveal that while LLMs struggle with\nchallenging process mining tasks when used out of the box or with minimal\nin-context examples, they achieve strong performance when fine-tuned for these\ntasks across a broad range of process types and industries.", "AI": {"tldr": "LLMs\u5728\u8fc7\u7a0b\u6316\u6398\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u8bed\u4e49\u7406\u89e3\u7684\u4efb\u52a1\u4e2d\u3002\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u76d1\u7763\u5fae\u8c03\uff0cLLMs\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u8bed\u4e49\u611f\u77e5\u8fc7\u7a0b\u6316\u6398\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u5bf9LLMs\u9ed8\u8ba4\u72b6\u6001\u8bc4\u4f30\u7684\u4e0d\u8db3\u3002", "method": "\u5b9a\u4e49\u4e94\u4e2a\u9700\u8981\u8bed\u4e49\u7406\u89e3\u7684\u8fc7\u7a0b\u6316\u6398\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u76d1\u7763\u5fae\u8c03\u8bc4\u4f30LLMs\u6027\u80fd\u3002", "result": "LLMs\u5728\u9ed8\u8ba4\u72b6\u6001\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u901a\u8fc7\u5fae\u8c03\u540e\u80fd\u5728\u591a\u79cd\u4efb\u52a1\u548c\u884c\u4e1a\u4e2d\u53d6\u5f97\u5f3a\u6027\u80fd\u3002", "conclusion": "LLMs\u5728\u8bed\u4e49\u611f\u77e5\u8fc7\u7a0b\u6316\u6398\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u9002\u5f53\u7684\u5fae\u8c03\u4ee5\u53d1\u6325\u5176\u80fd\u529b\u3002"}}
{"id": "2504.21400", "pdf": "https://arxiv.org/pdf/2504.21400", "abs": "https://arxiv.org/abs/2504.21400", "authors": ["Sugat Chaturvedi", "Rochana Chaturvedi"], "title": "Who Gets the Callback? Generative AI and Gender Bias", "categories": ["econ.GN", "cs.CL", "q-fin.EC"], "comment": null, "summary": "Generative artificial intelligence (AI), particularly large language models\n(LLMs), is being rapidly deployed in recruitment and for candidate\nshortlisting. We audit several mid-sized open-source LLMs for gender bias using\na dataset of 332,044 real-world online job postings. For each posting, we\nprompt the model to recommend whether an equally qualified male or female\ncandidate should receive an interview callback. We find that most models tend\nto favor men, especially for higher-wage roles. Mapping job descriptions to the\nStandard Occupational Classification system, we find lower callback rates for\nwomen in male-dominated occupations and higher rates in female-associated ones,\nindicating occupational segregation. A comprehensive analysis of linguistic\nfeatures in job ads reveals strong alignment of model recommendations with\ntraditional gender stereotypes. To examine the role of recruiter identity, we\nsteer model behavior by infusing Big Five personality traits and simulating the\nperspectives of historical figures. We find that less agreeable personas reduce\nstereotyping, consistent with an agreeableness bias in LLMs. Our findings\nhighlight how AI-driven hiring may perpetuate biases in the labor market and\nhave implications for fairness and diversity within firms.", "AI": {"tldr": "\u8bba\u6587\u5ba1\u8ba1\u4e86\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u62db\u8058\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u53d1\u73b0\u6a21\u578b\u503e\u5411\u4e8e\u63a8\u8350\u7537\u6027\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u85aa\u804c\u4f4d\u4e2d\uff0c\u4e14\u4e0e\u6027\u522b\u523b\u677f\u5370\u8c61\u4e00\u81f4\u3002", "motivation": "\u7814\u7a76\u751f\u6210\u5f0fAI\uff08\u5c24\u5176\u662fLLMs\uff09\u5728\u62db\u8058\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u63ed\u793a\u5176\u5bf9\u52b3\u52a8\u529b\u5e02\u573a\u516c\u5e73\u6027\u548c\u591a\u6837\u6027\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u4f7f\u752833\u4e07\u6761\u771f\u5b9e\u62db\u8058\u5e7f\u544a\u6570\u636e\uff0c\u901a\u8fc7\u6a21\u578b\u63a8\u8350\u9762\u8bd5\u5019\u9009\u4eba\uff0c\u5206\u6790\u6027\u522b\u504f\u89c1\uff1b\u5e76\u901a\u8fc7\u6a21\u62df\u62db\u8058\u8005\u8eab\u4efd\uff08\u5982\u4eba\u683c\u7279\u8d28\uff09\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "result": "\u5927\u591a\u6570\u6a21\u578b\u504f\u5411\u7537\u6027\uff0c\u5c24\u5176\u662f\u9ad8\u85aa\u804c\u4f4d\uff1b\u5973\u6027\u5728\u7537\u6027\u4e3b\u5bfc\u804c\u4e1a\u4e2d\u56de\u8c03\u7387\u8f83\u4f4e\uff0c\u800c\u5728\u5973\u6027\u76f8\u5173\u804c\u4e1a\u4e2d\u8f83\u9ad8\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u62db\u8058\u53ef\u80fd\u52a0\u5267\u52b3\u52a8\u529b\u5e02\u573a\u504f\u89c1\uff0c\u9700\u5173\u6ce8\u516c\u5e73\u6027\u548c\u591a\u6837\u6027\u95ee\u9898\u3002"}}
{"id": "2504.21650", "pdf": "https://arxiv.org/pdf/2504.21650", "abs": "https://arxiv.org/abs/2504.21650", "authors": ["Haiyang Zhou", "Wangbo Yu", "Jiawen Guan", "Xinhua Cheng", "Yonghong Tian", "Li Yuan"], "title": "HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation", "categories": ["cs.CV"], "comment": "Project homepage: https://zhouhyocean.github.io/holotime/", "summary": "The rapid advancement of diffusion models holds the promise of\nrevolutionizing the application of VR and AR technologies, which typically\nrequire scene-level 4D assets for user experience. Nonetheless, existing\ndiffusion models predominantly concentrate on modeling static 3D scenes or\nobject-level dynamics, constraining their capacity to provide truly immersive\nexperiences. To address this issue, we propose HoloTime, a framework that\nintegrates video diffusion models to generate panoramic videos from a single\nprompt or reference image, along with a 360-degree 4D scene reconstruction\nmethod that seamlessly transforms the generated panoramic video into 4D assets,\nenabling a fully immersive 4D experience for users. Specifically, to tame video\ndiffusion models for generating high-fidelity panoramic videos, we introduce\nthe 360World dataset, the first comprehensive collection of panoramic videos\nsuitable for downstream 4D scene reconstruction tasks. With this curated\ndataset, we propose Panoramic Animator, a two-stage image-to-video diffusion\nmodel that can convert panoramic images into high-quality panoramic videos.\nFollowing this, we present Panoramic Space-Time Reconstruction, which leverages\na space-time depth estimation method to transform the generated panoramic\nvideos into 4D point clouds, enabling the optimization of a holistic 4D\nGaussian Splatting representation to reconstruct spatially and temporally\nconsistent 4D scenes. To validate the efficacy of our method, we conducted a\ncomparative analysis with existing approaches, revealing its superiority in\nboth panoramic video generation and 4D scene reconstruction. This demonstrates\nour method's capability to create more engaging and realistic immersive\nenvironments, thereby enhancing user experiences in VR and AR applications.", "AI": {"tldr": "HoloTime\u6846\u67b6\u901a\u8fc7\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u5168\u666f\u89c6\u9891\uff0c\u5e76\u7ed3\u54084D\u573a\u666f\u91cd\u5efa\u6280\u672f\uff0c\u63d0\u5347VR/AR\u6c89\u6d78\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5c40\u9650\u4e8e\u9759\u60013D\u573a\u666f\u6216\u5bf9\u8c61\u7ea7\u52a8\u6001\uff0c\u65e0\u6cd5\u6ee1\u8db3\u6c89\u6d78\u5f0f4D\u4f53\u9a8c\u9700\u6c42\u3002", "method": "\u63d0\u51faHoloTime\u6846\u67b6\uff0c\u5305\u62ec\u5168\u666f\u89c6\u9891\u751f\u6210\uff08Panoramic Animator\uff09\u548c4D\u573a\u666f\u91cd\u5efa\uff08Panoramic Space-Time Reconstruction\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHoloTime\u5728\u5168\u666f\u89c6\u9891\u751f\u6210\u548c4D\u573a\u666f\u91cd\u5efa\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HoloTime\u80fd\u521b\u5efa\u66f4\u771f\u5b9e\u3001\u6c89\u6d78\u76844D\u73af\u5883\uff0c\u63d0\u5347VR/AR\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2504.21682", "pdf": "https://arxiv.org/pdf/2504.21682", "abs": "https://arxiv.org/abs/2504.21682", "authors": ["Yan Shu", "Weichao Zeng", "Fangmin Zhao", "Zeyu Chen", "Zhenhang Li", "Xiaomeng Yang", "Yu Zhou", "Paolo Rota", "Xiang Bai", "Lianwen Jin", "Xu-Cheng Yin", "Nicu Sebe"], "title": "Visual Text Processing: A Comprehensive Review and Unified Evaluation", "categories": ["cs.CV"], "comment": null, "summary": "Visual text is a crucial component in both document and scene images,\nconveying rich semantic information and attracting significant attention in the\ncomputer vision community. Beyond traditional tasks such as text detection and\nrecognition, visual text processing has witnessed rapid advancements driven by\nthe emergence of foundation models, including text image reconstruction and\ntext image manipulation. Despite significant progress, challenges remain due to\nthe unique properties that differentiate text from general objects. Effectively\ncapturing and leveraging these distinct textual characteristics is essential\nfor developing robust visual text processing models. In this survey, we present\na comprehensive, multi-perspective analysis of recent advancements in visual\ntext processing, focusing on two key questions: (1) What textual features are\nmost suitable for different visual text processing tasks? (2) How can these\ndistinctive text features be effectively incorporated into processing\nframeworks? Furthermore, we introduce VTPBench, a new benchmark that\nencompasses a broad range of visual text processing datasets. Leveraging the\nadvanced visual quality assessment capabilities of multimodal large language\nmodels (MLLMs), we propose VTPScore, a novel evaluation metric designed to\nensure fair and reliable evaluation. Our empirical study with more than 20\nspecific models reveals substantial room for improvement in the current\ntechniques. Our aim is to establish this work as a fundamental resource that\nfosters future exploration and innovation in the dynamic field of visual text\nprocessing. The relevant repository is available at\nhttps://github.com/shuyansy/Visual-Text-Processing-survey.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u89c6\u89c9\u6587\u672c\u5904\u7406\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u63d0\u51fa\u4e86VTPBench\u57fa\u51c6\u548cVTPScore\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5206\u6790\u4e8620\u591a\u4e2a\u6a21\u578b\uff0c\u6307\u51fa\u5f53\u524d\u6280\u672f\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u89c6\u89c9\u6587\u672c\u5728\u6587\u6863\u548c\u573a\u666f\u56fe\u50cf\u4e2d\u5177\u6709\u91cd\u8981\u8bed\u4e49\u4fe1\u606f\uff0c\u4f46\u56e0\u5176\u72ec\u7279\u6027\u8d28\uff0c\u5904\u7406\u4ecd\u9762\u4e34\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u9002\u5408\u4e0d\u540c\u4efb\u52a1\u7684\u6587\u672c\u7279\u5f81\u53ca\u5176\u6709\u6548\u6574\u5408\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u591a\u89c6\u89d2\u5206\u6790\u89c6\u89c9\u6587\u672c\u5904\u7406\u6280\u672f\uff0c\u5f15\u5165VTPBench\u57fa\u51c6\u548c\u57fa\u4e8eMLLMs\u7684VTPScore\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5bf920\u591a\u4e2a\u6a21\u578b\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u6280\u672f\u4ecd\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\uff0cVTPBench\u548cVTPScore\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u51c6\u548c\u8bc4\u4f30\u5de5\u5177\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u89c6\u89c9\u6587\u672c\u5904\u7406\u9886\u57df\u7684\u57fa\u7840\u8d44\u6e90\uff0c\u65e8\u5728\u63a8\u52a8\u672a\u6765\u63a2\u7d22\u548c\u521b\u65b0\u3002"}}
{"id": "2504.21692", "pdf": "https://arxiv.org/pdf/2504.21692", "abs": "https://arxiv.org/abs/2504.21692", "authors": ["Zihan Zhou", "Changrui Dai", "Aibo Song", "Xiaolin Fang"], "title": "Enhancing Self-Supervised Fine-Grained Video Object Tracking with Dynamic Memory Prediction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Successful video analysis relies on accurate recognition of pixels across\nframes, and frame reconstruction methods based on video correspondence learning\nare popular due to their efficiency. Existing frame reconstruction methods,\nwhile efficient, neglect the value of direct involvement of multiple reference\nframes for reconstruction and decision-making aspects, especially in complex\nsituations such as occlusion or fast movement. In this paper, we introduce a\nDynamic Memory Prediction (DMP) framework that innovatively utilizes multiple\nreference frames to concisely and directly enhance frame reconstruction. Its\ncore component is a Reference Frame Memory Engine that dynamically selects\nframes based on object pixel features to improve tracking accuracy. In\naddition, a Bidirectional Target Prediction Network is built to utilize\nmultiple reference frames to improve the robustness of the model. Through\nexperiments, our algorithm outperforms the state-of-the-art self-supervised\ntechniques on two fine-grained video object tracking tasks: object segmentation\nand keypoint tracking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8bb0\u5fc6\u9884\u6d4b\uff08DMP\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u53c2\u8003\u5e27\u76f4\u63a5\u589e\u5f3a\u5e27\u91cd\u5efa\uff0c\u63d0\u5347\u89c6\u9891\u5206\u6790\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u5e27\u91cd\u5efa\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\uff08\u5982\u906e\u6321\u6216\u5feb\u901f\u8fd0\u52a8\uff09\u4e2d\u5ffd\u89c6\u591a\u53c2\u8003\u5e27\u7684\u76f4\u63a5\u53c2\u4e0e\uff0c\u9650\u5236\u4e86\u91cd\u5efa\u548c\u51b3\u7b56\u6548\u679c\u3002", "method": "DMP\u6846\u67b6\u5305\u542b\u52a8\u6001\u9009\u62e9\u53c2\u8003\u5e27\u7684\u8bb0\u5fc6\u5f15\u64ce\u548c\u53cc\u5411\u76ee\u6807\u9884\u6d4b\u7f51\u7edc\uff0c\u5229\u7528\u591a\u53c2\u8003\u5e27\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u5bf9\u8c61\u5206\u5272\u548c\u5173\u952e\u70b9\u8ddf\u8e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u81ea\u76d1\u7763\u6280\u672f\u3002", "conclusion": "DMP\u6846\u67b6\u901a\u8fc7\u591a\u53c2\u8003\u5e27\u7684\u52a8\u6001\u5229\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.21578", "pdf": "https://arxiv.org/pdf/2504.21578", "abs": "https://arxiv.org/abs/2504.21578", "authors": ["Kamila Barylska", "Frank Delaplace", "Anna Gogoli\u0144ska", "Ewa Pa\u0144kowska"], "title": "Glucagon and insulin production in pancreatic cells modeled using Petri nets and Boolean networks", "categories": ["q-bio.CB", "cs.CL", "03", "F.2; G.0"], "comment": null, "summary": "Diabetes is a civilization chronic disease characterized by a constant\nelevated concentration of glucose in the blood. Many processes are involved in\nthe glucose regulation, and their interactions are very complex. To better\nunderstand those processes we set ourselves a goal to create a Petri net model\nof the glucose regulation in the whole body. So far we have managed to create a\nmodel of glycolysis and synthesis of glucose in the liver, and the general\noverview models of the glucose regulation in a healthy and diabetic person. In\nthis paper we introduce Petri nets models of insulin secretion in beta cell of\nthe pancreas, and glucagon in the pancreas alpha cells. Those two hormones have\nmutually opposite effects: insulin preventing hyperglycemia, and glucagon\npreventing hypoglycemia. Understanding the mechanisms of insulin and glucagon\nsecretion constitutes the basis for understanding diabetes. We also present a\nmodel in which both processes occur together, depending on the blood glucose\nlevel. The dynamics of each model is analysed. Additionally, we transform the\noverall insulin and glucagon secretion system to a Boolean network, following\nstandard transformation rules.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6784\u5efaPetri\u7f51\u6a21\u578b\uff0c\u7814\u7a76\u4e86\u80f0\u5c9b\u7d20\u548c\u80f0\u9ad8\u8840\u7cd6\u7d20\u7684\u5206\u6ccc\u673a\u5236\u53ca\u5176\u5728\u8840\u7cd6\u8c03\u8282\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u578b\u7684\u52a8\u6001\u884c\u4e3a\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u8840\u7cd6\u8c03\u8282\u7684\u590d\u6742\u8fc7\u7a0b\uff0c\u5c24\u5176\u662f\u80f0\u5c9b\u7d20\u548c\u80f0\u9ad8\u8840\u7cd6\u7d20\u7684\u5206\u6ccc\u673a\u5236\uff0c\u4ee5\u63ed\u793a\u7cd6\u5c3f\u75c5\u7684\u75c5\u7406\u57fa\u7840\u3002", "method": "\u6784\u5efa\u4e86\u80f0\u817a\u03b2\u7ec6\u80de\u5206\u6ccc\u80f0\u5c9b\u7d20\u548c\u03b1\u7ec6\u80de\u5206\u6ccc\u80f0\u9ad8\u8840\u7cd6\u7d20\u7684Petri\u7f51\u6a21\u578b\uff0c\u5e76\u5206\u6790\u4e86\u5176\u52a8\u6001\u884c\u4e3a\uff1b\u8fd8\u5c06\u7cfb\u7edf\u8f6c\u5316\u4e3a\u5e03\u5c14\u7f51\u7edc\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u80f0\u5c9b\u7d20\u548c\u80f0\u9ad8\u8840\u7cd6\u7d20\u5206\u6ccc\u7684\u6a21\u578b\uff0c\u5e76\u5206\u6790\u4e86\u5176\u52a8\u6001\u884c\u4e3a\uff0c\u4e3a\u7406\u89e3\u7cd6\u5c3f\u75c5\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u901a\u8fc7Petri\u7f51\u6a21\u578b\u548c\u5e03\u5c14\u7f51\u7edc\uff0c\u672c\u6587\u4e3a\u7406\u89e3\u8840\u7cd6\u8c03\u8282\u548c\u7cd6\u5c3f\u75c5\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2504.21699", "pdf": "https://arxiv.org/pdf/2504.21699", "abs": "https://arxiv.org/abs/2504.21699", "authors": ["Abu Mohammed Raisuddin", "Jesper Holmblad", "Hamed Haghighi", "Yuri Poledna", "Maikol Funk Drechsler", "Valentina Donzella", "Eren Erdal Aksoy"], "title": "REHEARSE-3D: A Multi-modal Emulated Rain Dataset for 3D Point Cloud De-raining", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Sensor degradation poses a significant challenge in autonomous driving.\nDuring heavy rainfall, the interference from raindrops can adversely affect the\nquality of LiDAR point clouds, resulting in, for instance, inaccurate point\nmeasurements. This, in turn, can potentially lead to safety concerns if\nautonomous driving systems are not weather-aware, i.e., if they are unable to\ndiscern such changes. In this study, we release a new, large-scale, multi-modal\nemulated rain dataset, REHEARSE-3D, to promote research advancements in 3D\npoint cloud de-raining. Distinct from the most relevant competitors, our\ndataset is unique in several respects. First, it is the largest point-wise\nannotated dataset, and second, it is the only one with high-resolution LiDAR\ndata (LiDAR-256) enriched with 4D Radar point clouds logged in both daytime and\nnighttime conditions in a controlled weather environment. Furthermore,\nREHEARSE-3D involves rain-characteristic information, which is of significant\nvalue not only for sensor noise modeling but also for analyzing the impact of\nweather at a point level. Leveraging REHEARSE-3D, we benchmark raindrop\ndetection and removal in fused LiDAR and 4D Radar point clouds. Our\ncomprehensive study further evaluates the performance of various statistical\nand deep-learning models. Upon publication, the dataset and benchmark models\nwill be made publicly available at: https://sporsho.github.io/REHEARSE3D.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faREHEARSE-3D\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2dLiDAR\u70b9\u4e91\u56e0\u964d\u96e8\u5e72\u6270\u5bfc\u81f4\u7684\u7cbe\u5ea6\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u53bb\u96e8\u65b9\u6cd5\u3002", "motivation": "\u964d\u96e8\u5e72\u6270LiDAR\u70b9\u4e91\u8d28\u91cf\uff0c\u5f71\u54cd\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6027\uff0c\u9700\u7814\u7a76\u53bb\u96e8\u6280\u672f\u3002", "method": "\u53d1\u5e03\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u62df\u964d\u96e8\u6570\u636e\u96c6REHEARSE-3D\uff0c\u5305\u542b\u9ad8\u5206\u8fa8\u7387LiDAR\u548c4D\u96f7\u8fbe\u6570\u636e\uff0c\u5e76\u8bc4\u4f30\u7edf\u8ba1\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u6570\u636e\u96c6\u72ec\u7279\u4e14\u89c4\u6a21\u5927\uff0c\u652f\u6301\u70b9\u7ea7\u5929\u6c14\u5f71\u54cd\u5206\u6790\uff0c\u5e76\u63d0\u4f9b\u4e86\u53bb\u96e8\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "REHEARSE-3D\u4e3a3D\u70b9\u4e91\u53bb\u96e8\u7814\u7a76\u63d0\u4f9b\u91cd\u8981\u8d44\u6e90\uff0c\u672a\u6765\u5c06\u516c\u5f00\u6570\u636e\u96c6\u548c\u6a21\u578b\u3002"}}
{"id": "2504.21155", "pdf": "https://arxiv.org/pdf/2504.21155", "abs": "https://arxiv.org/abs/2504.21155", "authors": ["Fauzan Nazranda Rizqa", "Matthew Hole", "Charles Gretton"], "title": "Evaluation and Verification of Physics-Informed Neural Models of the Grad-Shafranov Equation", "categories": ["physics.plasm-ph", "cs.AI", "cs.NE"], "comment": "9 pages, 4 figures", "summary": "Our contributions are motivated by fusion reactors that rely on maintaining\nmagnetohydrodynamic (MHD) equilibrium, where the balance between plasma\npressure and confining magnetic fields is required for stable operation. In\naxisymmetric tokamak reactors in particular, and under the assumption of\ntoroidal symmetry, this equilibrium can be mathematically modelled using the\nGrad-Shafranov Equation (GSE). Recent works have demonstrated the potential of\nusing Physics-Informed Neural Networks (PINNs) to model the GSE. Existing\nstudies did not examine realistic scenarios in which a single network\ngeneralizes to a variety of boundary conditions. Addressing that limitation, we\nevaluate a PINN architecture that incorporates boundary points as network\ninputs. Additionally, we compare PINN model accuracy and inference speeds with\na Fourier Neural Operator (FNO) model. Finding the PINN model to be the most\nperformant, and accurate in our setting, we use the network verification tool\nMarabou to perform a range of verification tasks. Although we find some\ndiscrepancies between evaluations of the networks natively in PyTorch, compared\nto via Marabou, we are able to demonstrate useful and practical verification\nworkflows. Our study is the first investigation of verification of such\nnetworks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u8f74\u5bf9\u79f0\u6258\u5361\u9a6c\u514b\u53cd\u5e94\u5806\u4e2d\uff0c\u5229\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u5efa\u6a21Grad-Shafranov\u65b9\u7a0b\uff08GSE\uff09\u7684\u6f5c\u529b\uff0c\u5e76\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u672a\u6d89\u53ca\u7684\u8fb9\u754c\u6761\u4ef6\u6cdb\u5316\u95ee\u9898\u3002\u901a\u8fc7\u6bd4\u8f83PINN\u4e0e\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff08FNO\uff09\u6a21\u578b\u7684\u6027\u80fd\uff0c\u53d1\u73b0PINN\u66f4\u4f18\uff0c\u5e76\u9996\u6b21\u9a8c\u8bc1\u4e86\u6b64\u7c7b\u7f51\u7edc\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u805a\u53d8\u53cd\u5e94\u5806\u4e2d\u78c1\u6d41\u4f53\u52a8\u529b\u5b66\uff08MHD\uff09\u5e73\u8861\u7684\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u8f74\u5bf9\u79f0\u6258\u5361\u9a6c\u514b\u53cd\u5e94\u5806\u4e2d\uff0cGSE\u7684\u5efa\u6a21\u5bf9\u7a33\u5b9a\u8fd0\u884c\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u672a\u89e3\u51b3\u8fb9\u754c\u6761\u4ef6\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u91c7\u7528PINN\u67b6\u6784\uff0c\u5c06\u8fb9\u754c\u70b9\u4f5c\u4e3a\u7f51\u7edc\u8f93\u5165\uff0c\u5e76\u4e0eFNO\u6a21\u578b\u6bd4\u8f83\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u3002\u4f7f\u7528Marabou\u5de5\u5177\u8fdb\u884c\u7f51\u7edc\u9a8c\u8bc1\u3002", "result": "PINN\u6a21\u578b\u5728\u6027\u80fd\u548c\u7cbe\u5ea6\u4e0a\u4f18\u4e8eFNO\u6a21\u578b\uff0c\u9a8c\u8bc1\u5de5\u4f5c\u6d41\u5b9e\u7528\u4e14\u6709\u6548\uff0c\u5c3d\u7ba1PyTorch\u539f\u751f\u8bc4\u4f30\u4e0eMarabou\u4e4b\u95f4\u5b58\u5728\u4e00\u4e9b\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u9996\u6b21\u9a8c\u8bc1\u4e86\u6b64\u7c7b\u7f51\u7edc\u7684\u5b9e\u7528\u6027\uff0c\u4e3aGSE\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u9a8c\u8bc1\u5de5\u4f5c\u6d41\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2504.21706", "pdf": "https://arxiv.org/pdf/2504.21706", "abs": "https://arxiv.org/abs/2504.21706", "authors": ["Saber Mehdipour", "Seyed Abolghasem Mirroshandel", "Seyed Amirhossein Tabatabaei"], "title": "Vision Transformers in Precision Agriculture: A Comprehensive Survey", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Detecting plant diseases is a crucial aspect of modern agriculture - it plays\na key role in maintaining crop health and increasing overall yield. Traditional\napproaches, though still valuable, often rely on manual inspection or\nconventional machine learning techniques, both of which face limitations in\nscalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as\na promising alternative, offering benefits such as improved handling of\nlong-range dependencies and better scalability for visual tasks. This survey\nexplores the application of ViTs in precision agriculture, covering tasks from\nclassification to detection and segmentation. We begin by introducing the\nfoundational architecture of ViTs and discuss their transition from Natural\nLanguage Processing (NLP) to computer vision. The discussion includes the\nconcept of inductive bias in traditional models like Convolutional Neural\nNetworks (CNNs), and how ViTs mitigate these biases. We provide a comprehensive\nreview of recent literature, focusing on key methodologies, datasets, and\nperformance metrics. The survey also includes a comparative analysis of CNNs\nand ViTs, with a look at hybrid models and performance enhancements. Technical\nchallenges - such as data requirements, computational demands, and model\ninterpretability - are addressed alongside potential solutions. Finally, we\noutline potential research directions and technological advancements that could\nfurther support the integration of ViTs in real-world agricultural settings.\nOur goal with this study is to offer practitioners and researchers a deeper\nunderstanding of how ViTs are poised to transform smart and precision\nagriculture.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u89c6\u89c9\u53d8\u6362\u5668\uff08ViTs\uff09\u5728\u7cbe\u51c6\u519c\u4e1a\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86\u5176\u4ece\u5206\u7c7b\u5230\u68c0\u6d4b\u548c\u5206\u5272\u7684\u4efb\u52a1\uff0c\u5e76\u4e0e\u4f20\u7edfCNN\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u4f20\u7edf\u690d\u7269\u75c5\u5bb3\u68c0\u6d4b\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0cViTs\u56e0\u5176\u5904\u7406\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u89c6\u89c9\u4efb\u52a1\u7684\u4f18\u52bf\u6210\u4e3a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4ecb\u7ecd\u4e86ViTs\u7684\u57fa\u7840\u67b6\u6784\u53ca\u5176\u4eceNLP\u5230\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u8fc7\u6e21\uff0c\u5206\u6790\u4e86ViTs\u5982\u4f55\u7f13\u89e3\u4f20\u7edf\u6a21\u578b\u7684\u5f52\u7eb3\u504f\u5dee\uff0c\u5e76\u7efc\u8ff0\u4e86\u76f8\u5173\u6587\u732e\u3001\u6570\u636e\u96c6\u548c\u6027\u80fd\u6307\u6807\u3002", "result": "\u63d0\u4f9b\u4e86ViTs\u4e0eCNNs\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u63a2\u8ba8\u4e86\u6df7\u5408\u6a21\u578b\u548c\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u6570\u636e\u9700\u6c42\u3001\u8ba1\u7b97\u6210\u672c\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7b49\u6280\u672f\u6311\u6218\u3002", "conclusion": "ViTs\u6709\u671b\u53d8\u9769\u7cbe\u51c6\u519c\u4e1a\uff0c\u672c\u6587\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u6df1\u5165\u7406\u89e3\u5176\u6f5c\u529b\u7684\u8d44\u6e90\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2504.21716", "pdf": "https://arxiv.org/pdf/2504.21716", "abs": "https://arxiv.org/abs/2504.21716", "authors": ["Marc Glocker", "Peter H\u00f6nig", "Matthias Hirschmanner", "Markus Vincze"], "title": "LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "Accepted at Austrian Robotics Workshop 2025", "summary": "We present an embodied robotic system with an LLM-driven agent-orchestration\narchitecture for autonomous household object management. The system integrates\nmemory-augmented task planning, enabling robots to execute high-level user\ncommands while tracking past actions. It employs three specialized agents: a\nrouting agent, a task planning agent, and a knowledge base agent, each powered\nby task-specific LLMs. By leveraging in-context learning, our system avoids the\nneed for explicit model training. RAG enables the system to retrieve context\nfrom past interactions, enhancing long-term object tracking. A combination of\nGrounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating\nsemantic scene understanding for task planning. Evaluation across three\nhousehold scenarios demonstrates high task planning accuracy and an improvement\nin memory recall due to RAG. Specifically, Qwen2.5 yields best performance for\nspecialized agents, while LLaMA3.1 excels in routing tasks. The source code is\navailable at: https://github.com/marc1198/chat-hsr.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u5bb6\u5ead\u7269\u54c1\u7684\u81ea\u4e3b\u7ba1\u7406\uff0c\u901a\u8fc7\u4efb\u52a1\u89c4\u5212\u3001\u8bb0\u5fc6\u589e\u5f3a\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b3\u5bb6\u5ead\u73af\u5883\u4e2d\u673a\u5668\u4eba\u81ea\u4e3b\u7ba1\u7406\u7269\u54c1\u7684\u590d\u6742\u6027\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u663e\u5f0f\u6a21\u578b\u8bad\u7ec3\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u8def\u7531\u4ee3\u7406\u3001\u4efb\u52a1\u89c4\u5212\u4ee3\u7406\u548c\u77e5\u8bc6\u5e93\u4ee3\u7406\u4e09\u79cd\u4e13\u7528\u667a\u80fd\u4f53\uff0c\u7ed3\u5408RAG\u548cGrounded SAM\u7b49\u6280\u672f\u5b9e\u73b0\u4efb\u52a1\u89c4\u5212\u548c\u573a\u666f\u7406\u89e3\u3002", "result": "\u5728\u4e09\u79cd\u5bb6\u5ead\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9ad8\u4efb\u52a1\u89c4\u5212\u51c6\u786e\u6027\u548c\u8bb0\u5fc6\u53ec\u56de\u7387\uff0cQwen2.5\u548cLLaMA3.1\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bb6\u5ead\u7269\u54c1\u7ba1\u7406\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2504.21718", "pdf": "https://arxiv.org/pdf/2504.21718", "abs": "https://arxiv.org/abs/2504.21718", "authors": ["Shiying Li", "Xingqun Qi", "Bingkun Yang", "Chen Weile", "Zezhao Tian", "Muyi Sun", "Qifeng Liu", "Man Zhang", "Zhenan Sun"], "title": "VividListener: Expressive and Controllable Listener Dynamics Modeling for Multi-Modal Responsive Interaction", "categories": ["cs.CV"], "comment": null, "summary": "Generating responsive listener head dynamics with nuanced emotions and\nexpressive reactions is crucial for practical dialogue modeling in various\nvirtual avatar animations. Previous studies mainly focus on the direct\nshort-term production of listener behavior. They overlook the fine-grained\ncontrol over motion variations and emotional intensity, especially in\nlong-sequence modeling. Moreover, the lack of long-term and large-scale paired\nspeaker-listener corpora including head dynamics and fine-grained\nmulti-modality annotations (e.g., text-based expression descriptions, emotional\nintensity) also limits the application of dialogue modeling.Therefore, we first\nnewly collect a large-scale multi-turn dataset of 3D dyadic conversation\ncontaining more than 1.4M valid frames for multi-modal responsive interaction,\ndubbed ListenerX. Additionally, we propose VividListener, a novel framework\nenabling fine-grained, expressive and controllable listener dynamics modeling.\nThis framework leverages multi-modal conditions as guiding principles for\nfostering coherent interactions between speakers and listeners.Specifically, we\ndesign the Responsive Interaction Module (RIM) to adaptively represent the\nmulti-modal interactive embeddings. RIM ensures the listener dynamics achieve\nfine-grained semantic coordination with textual descriptions and adjustments,\nwhile preserving expressive reaction with speaker behavior. Meanwhile, we\ndesign the Emotional Intensity Tags (EIT) for emotion intensity editing with\nmulti-modal information integration, applying to both text descriptions and\nlistener motion amplitude.Extensive experiments conducted on our newly\ncollected ListenerX dataset demonstrate that VividListener achieves\nstate-of-the-art performance, realizing expressive and controllable listener\ndynamics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faVividListener\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u7ec6\u817b\u60c5\u611f\u548c\u8868\u8fbe\u53cd\u5e94\u7684\u542c\u8005\u5934\u90e8\u52a8\u6001\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u5728\u957f\u5e8f\u5217\u5efa\u6a21\u548c\u60c5\u611f\u5f3a\u5ea6\u63a7\u5236\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u77ed\u671f\u7684\u542c\u8005\u884c\u4e3a\u751f\u6210\uff0c\u7f3a\u4e4f\u5bf9\u8fd0\u52a8\u53d8\u5316\u548c\u60c5\u611f\u5f3a\u5ea6\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u4e14\u7f3a\u4e4f\u5927\u89c4\u6a21\u7684\u591a\u6a21\u6001\u6807\u6ce8\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faVividListener\u6846\u67b6\uff0c\u5305\u542bResponsive Interaction Module\uff08RIM\uff09\u548cEmotional Intensity Tags\uff08EIT\uff09\uff0c\u5229\u7528\u591a\u6a21\u6001\u6761\u4ef6\u6307\u5bfc\u542c\u8005\u52a8\u6001\u5efa\u6a21\u3002", "result": "\u5728ListenerX\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVividListener\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751f\u6210\u5177\u6709\u8868\u8fbe\u6027\u548c\u53ef\u63a7\u6027\u7684\u542c\u8005\u52a8\u6001\u3002", "conclusion": "VividListener\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u542c\u8005\u52a8\u6001\u5efa\u6a21\u4e2d\u7684\u7cbe\u7ec6\u63a7\u5236\u548c\u60c5\u611f\u8868\u8fbe\u95ee\u9898\uff0c\u4e3a\u865a\u62df\u5bf9\u8bdd\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2504.21188", "pdf": "https://arxiv.org/pdf/2504.21188", "abs": "https://arxiv.org/abs/2504.21188", "authors": ["Natnael Alemayehu"], "title": "Light Weight CNN for classification of Brain Tumors from MRI Images", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "6 pages", "summary": "This study presents a convolutional neural network (CNN)-based approach for\nthe multi-class classification of brain tumors using magnetic resonance imaging\n(MRI) scans. We utilize a publicly available dataset containing MRI images\ncategorized into four classes: glioma, meningioma, pituitary tumor, and no\ntumor. Our primary objective is to build a light weight deep learning model\nthat can automatically classify brain tumor types with high accuracy. To\nachieve this goal, we incorporate image preprocessing steps, including\nnormalization, data augmentation, and a cropping technique designed to reduce\nbackground noise and emphasize relevant regions. The CNN architecture is\noptimized through hyperparameter tuning using Keras Tuner, enabling systematic\nexploration of network parameters. To ensure reliable evaluation, we apply\n5-fold cross-validation, where each hyperparameter configuration is evaluated\nacross multiple data splits to mitigate overfitting. Experimental results\ndemonstrate that the proposed model achieves a classification accuracy of\n98.78%, indicating its potential as a diagnostic aid in clinical settings. The\nproposed method offers a low-complexity yet effective solution for assisting in\nearly brain tumor diagnosis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCNN\u7684\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8eMRI\u626b\u63cf\u4e2d\u8111\u80bf\u7624\u7684\u591a\u5206\u7c7b\uff0c\u51c6\u786e\u7387\u8fbe98.78%\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u590d\u6742\u5ea6\u7684\u6a21\u578b\uff0c\u8f85\u52a9\u4e34\u5e8a\u65e9\u671f\u8111\u80bf\u7624\u8bca\u65ad\u3002", "method": "\u7ed3\u5408\u56fe\u50cf\u9884\u5904\u7406\uff08\u5f52\u4e00\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u88c1\u526a\uff09\u3001CNN\u67b6\u6784\u4f18\u5316\u53ca5\u6298\u4ea4\u53c9\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u5206\u7c7b\u51c6\u786e\u7387\u4e3a98.78%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8111\u80bf\u7624\u65e9\u671f\u8bca\u65ad\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u4f4e\u590d\u6742\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21751", "pdf": "https://arxiv.org/pdf/2504.21751", "abs": "https://arxiv.org/abs/2504.21751", "authors": ["Sizhe Wang", "Zhengren Wang", "Dongsheng Ma", "Yongan Yu", "Rui Ling", "Zhiyu Li", "Feiyu Xiong", "Wentao Zhang"], "title": "CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Real world development demands code that is readable, extensible, and\ntestable by organizing the implementation into modular components and\niteratively reuse pre-implemented code. We term this iterative, multi-turn\nprocess codeflow and introduce CodeFlowBench, the first benchmark designed for\ncomprehensively evaluating LLMs' ability to perform codeflow, namely to\nimplement new functionality by reusing existing functions over multiple turns.\nCodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously\nupdated via an automated pipeline that decomposes each problem into a series of\nfunction-level subproblems based on its dependency tree and each subproblem is\npaired with unit tests. We further propose a novel evaluation framework with\ntasks and metrics tailored to multi-turn code reuse to assess model\nperformance. In experiments across various LLMs under both multi-turn and\nsingle-turn patterns. We observe models' poor performance on CodeFlowBench,\nwith a substantial performance drop in the iterative codeflow scenario. For\ninstance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8%\nin single-turn pattern. Further analysis shows that different models excel at\ndifferent dependency depths, yet all struggle to correctly solve structurally\ncomplex problems, highlighting challenges for current LLMs to serve as code\ngeneration tools when performing codeflow. Overall, CodeFlowBench offers a\ncomprehensive benchmark and new insights into LLM capabilities for multi-turn,\niterative code generation, guiding future advances in code generation tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CodeFlowBench\uff0c\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u591a\u8f6e\u4ee3\u7801\u590d\u7528\u4e2d\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b5258\u4e2a\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002\u5b9e\u9a8c\u663e\u793aLLMs\u5728\u591a\u8f6e\u4ee3\u7801\u590d\u7528\u573a\u666f\u4e2d\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u73b0\u5b9e\u5f00\u53d1\u9700\u8981\u4ee3\u7801\u5177\u5907\u53ef\u8bfb\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u6d4b\u8bd5\u6027\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u7ec4\u4ef6\u548c\u8fed\u4ee3\u590d\u7528\u4ee3\u7801\u5b9e\u73b0\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u5728\u591a\u8f6e\u4ee3\u7801\u590d\u7528\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faCodeFlowBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4eceCodeforces\u4e2d\u63d0\u53d65258\u4e2a\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u7ba1\u9053\u5206\u89e3\u4e3a\u51fd\u6570\u7ea7\u5b50\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u591a\u8f6e\u4ee3\u7801\u590d\u7528\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLLMs\u5728\u591a\u8f6e\u4ee3\u7801\u590d\u7528\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f8b\u5982o1-mini\u5728\u591a\u8f6e\u6a21\u5f0f\u4e0b\u7684pass@1\u4e3a20.8%\uff0c\u800c\u5355\u8f6e\u6a21\u5f0f\u4e3a37.8%\u3002", "conclusion": "CodeFlowBench\u4e3a\u591a\u8f6e\u8fed\u4ee3\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLMs\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2504.21749", "pdf": "https://arxiv.org/pdf/2504.21749", "abs": "https://arxiv.org/abs/2504.21749", "authors": ["Leonhard Sommer", "Olaf D\u00fcnkel", "Christian Theobalt", "Adam Kortylewski"], "title": "Common3D: Self-Supervised Learning of 3D Morphable Models for Common Objects in Neural Feature Space", "categories": ["cs.CV"], "comment": null, "summary": "3D morphable models (3DMMs) are a powerful tool to represent the possible\nshapes and appearances of an object category. Given a single test image, 3DMMs\ncan be used to solve various tasks, such as predicting the 3D shape, pose,\nsemantic correspondence, and instance segmentation of an object. Unfortunately,\n3DMMs are only available for very few object categories that are of particular\ninterest, like faces or human bodies, as they require a demanding 3D data\nacquisition and category-specific training process. In contrast, we introduce a\nnew method, Common3D, that learns 3DMMs of common objects in a fully\nself-supervised manner from a collection of object-centric videos. For this\npurpose, our model represents objects as a learned 3D template mesh and a\ndeformation field that is parameterized as an image-conditioned neural network.\nDifferent from prior works, Common3D represents the object appearance with\nneural features instead of RGB colors, which enables the learning of more\ngeneralizable representations through an abstraction from pixel intensities.\nImportantly, we train the appearance features using a contrastive objective by\nexploiting the correspondences defined through the deformable template mesh.\nThis leads to higher quality correspondence features compared to related works\nand a significantly improved model performance at estimating 3D object pose and\nsemantic correspondence. Common3D is the first completely self-supervised\nmethod that can solve various vision tasks in a zero-shot manner.", "AI": {"tldr": "Common3D\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u9891\u5b66\u4e603D\u53ef\u53d8\u5f62\u6a21\u578b\uff083DMMs\uff09\uff0c\u65e0\u97003D\u6570\u636e\u6807\u6ce8\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u3002", "motivation": "\u73b0\u67093DMMs\u4ec5\u9002\u7528\u4e8e\u5c11\u6570\u5bf9\u8c61\u7c7b\u522b\uff08\u5982\u4eba\u8138\u6216\u4eba\u4f53\uff09\uff0c\u4e14\u9700\u8981\u5927\u91cf3D\u6570\u636e\u548c\u7c7b\u522b\u7279\u5b9a\u8bad\u7ec3\u3002Common3D\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6cdb\u5316\u5230\u5e38\u89c1\u5bf9\u8c61\u3002", "method": "Common3D\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u89c6\u9891\u5b66\u4e603DMMs\uff0c\u4f7f\u7528\u795e\u7ecf\u7279\u5f81\u8868\u793a\u5916\u89c2\uff0c\u800c\u975eRGB\u989c\u8272\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u4f18\u5316\u7279\u5f81\u3002", "result": "Common3D\u57283D\u59ff\u6001\u4f30\u8ba1\u548c\u8bed\u4e49\u5bf9\u5e94\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u9996\u6b21\u5b9e\u73b0\u96f6\u6837\u672c\u591a\u4efb\u52a1\u5904\u7406\u3002", "conclusion": "Common3D\u4e3a\u81ea\u76d1\u7763\u5b66\u4e603DMMs\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u4efb\u52a1\u9002\u5e94\u6027\u3002"}}
{"id": "2504.21798", "pdf": "https://arxiv.org/pdf/2504.21798", "abs": "https://arxiv.org/abs/2504.21798", "authors": ["John Yang", "Kilian Leret", "Carlos E. Jimenez", "Alexander Wettig", "Kabir Khandpur", "Yanzhe Zhang", "Binyuan Hui", "Ofir Press", "Ludwig Schmidt", "Diyi Yang"], "title": "SWE-smith: Scaling Data for Software Engineering Agents", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Despite recent progress in Language Models (LMs) for software engineering,\ncollecting training data remains a significant pain point. Existing datasets\nare small, with at most 1,000s of training instances from 11 or fewer GitHub\nrepositories. The procedures to curate such datasets are often complex,\nnecessitating hundreds of hours of human labor; companion execution\nenvironments also take up several terabytes of storage, severely limiting their\nscalability and usability. To address this pain point, we introduce SWE-smith,\na novel pipeline for generating software engineering training data at scale.\nGiven any Python codebase, SWE-smith constructs a corresponding execution\nenvironment, then automatically synthesizes 100s to 1,000s of task instances\nthat break existing test(s) in the codebase. Using SWE-smith, we create a\ndataset of 50k instances sourced from 128 GitHub repositories, an order of\nmagnitude larger than all previous works. We train SWE-agent-LM-32B, achieving\n40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art\namong open source models. We open source SWE-smith (collection procedure, task\ninstances, trajectories, models) to lower the barrier of entry for research in\nLM systems for automated software engineering. All assets available at\nhttps://swesmith.com.", "AI": {"tldr": "SWE-smith\u662f\u4e00\u79cd\u65b0\u578b\u7ba1\u9053\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u751f\u6210\u8f6f\u4ef6\u5de5\u7a0b\u8bad\u7ec3\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5c0f\u4e14\u96be\u4ee5\u6269\u5c55\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8f6f\u4ef6\u5de5\u7a0b\u8bad\u7ec3\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u3001\u6536\u96c6\u590d\u6742\u4e14\u5b58\u50a8\u9700\u6c42\u9ad8\uff0c\u9650\u5236\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u3002", "method": "SWE-smith\u901a\u8fc7\u6784\u5efa\u6267\u884c\u73af\u5883\u5e76\u81ea\u52a8\u5408\u6210\u4efb\u52a1\u5b9e\u4f8b\uff08\u7834\u574f\u73b0\u6709\u6d4b\u8bd5\uff09\uff0c\u4ece128\u4e2aGitHub\u4ed3\u5e93\u751f\u6210\u4e8650k\u5b9e\u4f8b\u7684\u6570\u636e\u96c6\u3002", "result": "\u8bad\u7ec3\u51fa\u7684SWE-agent-LM-32B\u6a21\u578b\u5728SWE-bench Verified\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523040.2% Pass@1\uff0c\u662f\u76ee\u524d\u5f00\u6e90\u6a21\u578b\u4e2d\u7684\u6700\u4f73\u8868\u73b0\u3002", "conclusion": "SWE-smith\u7684\u5f00\u6e90\u964d\u4f4e\u4e86\u81ea\u52a8\u5316\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u7684\u95e8\u69db\uff0c\u63a8\u52a8\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2504.21771", "pdf": "https://arxiv.org/pdf/2504.21771", "abs": "https://arxiv.org/abs/2504.21771", "authors": ["Bahram Jafrasteh", "Wei Peng", "Cheng Wan", "Yimin Luo", "Ehsan Adeli", "Qingyu Zhao"], "title": "Anatomical Similarity as a New Metric to Evaluate Brain Generative Models", "categories": ["cs.CV"], "comment": null, "summary": "Generative models enhance neuroimaging through data augmentation, quality\nimprovement, and rare condition studies. Despite advances in realistic\nsynthetic MRIs, evaluations focus on texture and perception, lacking\nsensitivity to crucial anatomical fidelity. This study proposes a new metric,\ncalled WASABI (Wasserstein-Based Anatomical Brain Index), to assess the\nanatomical realism of synthetic brain MRIs. WASABI leverages \\textit{SynthSeg},\na deep learning-based brain parcellation tool, to derive volumetric measures of\nbrain regions in each MRI and uses the multivariate Wasserstein distance to\ncompare distributions between real and synthetic anatomies. Based on controlled\nexperiments on two real datasets and synthetic MRIs from five generative\nmodels, WASABI demonstrates higher sensitivity in quantifying anatomical\ndiscrepancies compared to traditional image-level metrics, even when synthetic\nimages achieve near-perfect visual quality. Our findings advocate for shifting\nthe evaluation paradigm beyond visual inspection and conventional metrics,\nemphasizing anatomical fidelity as a crucial benchmark for clinically\nmeaningful brain MRI synthesis. Our code is available at\nhttps://github.com/BahramJafrasteh/wasabi-mri.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWASABI\u7684\u65b0\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u5408\u6210\u8111MRI\u7684\u89e3\u5256\u5b66\u771f\u5b9e\u6027\uff0c\u901a\u8fc7\u4f53\u79ef\u6d4b\u91cf\u548cWasserstein\u8ddd\u79bb\u6bd4\u8f83\u771f\u5b9e\u4e0e\u5408\u6210\u89e3\u5256\u7ed3\u6784\u7684\u5206\u5e03\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7eb9\u7406\u548c\u611f\u77e5\uff0c\u7f3a\u4e4f\u5bf9\u89e3\u5256\u5b66\u771f\u5b9e\u6027\u7684\u654f\u611f\u6027\uff0c\u800c\u89e3\u5256\u5b66\u771f\u5b9e\u6027\u5bf9\u4e34\u5e8a\u610f\u4e49\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528SynthSeg\u5de5\u5177\u8fdb\u884c\u8111\u533a\u5206\u5272\uff0c\u8ba1\u7b97\u5404\u8111\u533a\u4f53\u79ef\uff0c\u5e76\u4f7f\u7528\u591a\u53d8\u91cfWasserstein\u8ddd\u79bb\u6bd4\u8f83\u771f\u5b9e\u4e0e\u5408\u6210MRI\u7684\u89e3\u5256\u5206\u5e03\u3002", "result": "\u5728\u4e94\u4e2a\u751f\u6210\u6a21\u578b\u548c\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWASABI\u6bd4\u4f20\u7edf\u56fe\u50cf\u7ea7\u6307\u6807\u66f4\u654f\u611f\uff0c\u80fd\u68c0\u6d4b\u5230\u89e3\u5256\u5b66\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u8bc4\u4f30\u8303\u5f0f\u5e94\u4ece\u89c6\u89c9\u68c0\u67e5\u548c\u4f20\u7edf\u6307\u6807\u8f6c\u5411\u89e3\u5256\u5b66\u771f\u5b9e\u6027\uff0c\u4f5c\u4e3a\u4e34\u5e8a\u6709\u610f\u4e49\u8111MRI\u5408\u6210\u7684\u5173\u952e\u6807\u51c6\u3002"}}
{"id": "2504.21789", "pdf": "https://arxiv.org/pdf/2504.21789", "abs": "https://arxiv.org/abs/2504.21789", "authors": ["Alessia Hu", "Regina Beets-Tan", "Lishan Cai", "Eduardo Pooch"], "title": "Anomaly-Driven Approach for Enhanced Prostate Cancer Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Paper accepted for publication at 2025 47th Annual International\n  Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)\n  Copyright 2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future media", "summary": "Magnetic Resonance Imaging (MRI) plays an important role in identifying\nclinically significant prostate cancer (csPCa), yet automated methods face\nchallenges such as data imbalance, variable tumor sizes, and a lack of\nannotated data. This study introduces Anomaly-Driven U-Net (adU-Net), which\nincorporates anomaly maps derived from biparametric MRI sequences into a deep\nlearning-based segmentation framework to improve csPCa identification. We\nconduct a comparative analysis of anomaly detection methods and evaluate the\nintegration of anomaly maps into the segmentation pipeline. Anomaly maps,\ngenerated using Fixed-Point GAN reconstruction, highlight deviations from\nnormal prostate tissue, guiding the segmentation model to potential cancerous\nregions. We compare the performance by using the average score, computed as the\nmean of the AUROC and Average Precision (AP). On the external test set, adU-Net\nachieves the best average score of 0.618, outperforming the baseline nnU-Net\nmodel (0.605). The results demonstrate that incorporating anomaly detection\ninto segmentation improves generalization and performance, particularly with\nADC-based anomaly maps, offering a promising direction for automated csPCa\nidentification.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f02\u5e38\u68c0\u6d4b\u7684\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u6846\u67b6\uff08adU-Net\uff09\uff0c\u901a\u8fc7\u5f02\u5e38\u56fe\u5f15\u5bfc\u6a21\u578b\u8bc6\u522b\u524d\u5217\u817a\u764c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "MRI\u5728\u8bc6\u522b\u4e34\u5e8a\u663e\u8457\u524d\u5217\u817a\u764c\uff08csPCa\uff09\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u81ea\u52a8\u5316\u65b9\u6cd5\u9762\u4e34\u6570\u636e\u4e0d\u5e73\u8861\u3001\u80bf\u7624\u5927\u5c0f\u591a\u53d8\u548c\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7b49\u6311\u6218\u3002", "method": "\u7814\u7a76\u5f15\u5165adU-Net\uff0c\u5c06\u57fa\u4e8e\u53cc\u53c2\u6570MRI\u5e8f\u5217\u7684\u5f02\u5e38\u56fe\u878d\u5165\u5206\u5272\u6846\u67b6\uff0c\u5e76\u901a\u8fc7Fixed-Point GAN\u751f\u6210\u5f02\u5e38\u56fe\u4ee5\u7a81\u51fa\u764c\u53d8\u533a\u57df\u3002", "result": "\u5728\u5916\u90e8\u6d4b\u8bd5\u96c6\u4e0a\uff0cadU-Net\u7684\u5e73\u5747\u5f97\u5206\uff08AUROC\u548cAP\u7684\u5747\u503c\uff09\u4e3a0.618\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578bnnU-Net\uff080.605\uff09\u3002", "conclusion": "\u7ed3\u5408\u5f02\u5e38\u68c0\u6d4b\u7684\u5206\u5272\u65b9\u6cd5\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\uff0c\u5c24\u5176\u662f\u57fa\u4e8eADC\u7684\u5f02\u5e38\u56fe\uff0c\u4e3a\u81ea\u52a8\u5316csPCa\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.21810", "pdf": "https://arxiv.org/pdf/2504.21810", "abs": "https://arxiv.org/abs/2504.21810", "authors": ["Franko Hrzic", "Mohammadreza Movahhedi", "Ophelie Lavoie-Gagne", "Ata Kiapour"], "title": "A simple and effective approach for body part recognition on CT scans based on projection estimation", "categories": ["cs.CV", "68T01, 65D19", "I.4.0; I.4.10; I.2.1"], "comment": "19 pages, 6 figures", "summary": "It is well known that machine learning models require a high amount of\nannotated data to obtain optimal performance. Labelling Computed Tomography\n(CT) data can be a particularly challenging task due to its volumetric nature\nand often missing and$/$or incomplete associated meta-data. Even inspecting one\nCT scan requires additional computer software, or in the case of programming\nlanguages $-$ additional programming libraries. This study proposes a simple,\nyet effective approach based on 2D X-ray-like estimation of 3D CT scans for\nbody region identification. Although body region is commonly associated with\nthe CT scan, it often describes only the focused major body region neglecting\nother anatomical regions present in the observed CT. In the proposed approach,\nestimated 2D images were utilized to identify 14 distinct body regions,\nproviding valuable information for constructing a high-quality medical dataset.\nTo evaluate the effectiveness of the proposed method, it was compared against\n2.5D, 3D and foundation model (MI2) based approaches. Our approach outperformed\nthe others, where it came on top with statistical significance and F1-Score for\nthe best-performing model EffNet-B0 of 0.980 $\\pm$ 0.016 in comparison to the\n0.840 $\\pm$ 0.114 (2.5D DenseNet-161), 0.854 $\\pm$ 0.096 (3D VoxCNN), and 0.852\n$\\pm$ 0.104 (MI2 foundation model). The utilized dataset comprised three\ndifferent clinical centers and counted 15,622 CT scans (44,135 labels).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e2D X\u5c04\u7ebf\u4f30\u8ba1\u76843D CT\u626b\u63cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u8bc6\u522b14\u4e2a\u8eab\u4f53\u533a\u57df\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "CT\u6570\u636e\u6807\u6ce8\u590d\u6742\u4e14\u8017\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u5ffd\u7565\u591a\u533a\u57df\u4fe1\u606f\uff0c\u9700\u66f4\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u75282D X\u5c04\u7ebf\u4f30\u8ba13D CT\u626b\u63cf\uff0c\u8bc6\u522b14\u4e2a\u8eab\u4f53\u533a\u57df\uff0c\u5e76\u4e0e2.5D\u30013D\u53ca\u57fa\u7840\u6a21\u578b\u5bf9\u6bd4\u3002", "result": "EffNet-B0\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cF1-Score\u8fbe0.980\u00b10.016\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u6784\u5efa\u9ad8\u8d28\u91cf\u533b\u5b66\u6570\u636e\u96c6\u3002"}}
{"id": "2504.21814", "pdf": "https://arxiv.org/pdf/2504.21814", "abs": "https://arxiv.org/abs/2504.21814", "authors": ["Yixin Gao", "Xiaohan Pan", "Xin Li", "Zhibo Chen"], "title": "Why Compress What You Can Generate? When GPT-4o Generation Ushers in Image Compression Fields", "categories": ["cs.CV"], "comment": null, "summary": "The rapid development of AIGC foundation models has revolutionized the\nparadigm of image compression, which paves the way for the abandonment of most\npixel-level transform and coding, compelling us to ask: why compress what you\ncan generate if the AIGC foundation model is powerful enough to faithfully\ngenerate intricate structure and fine-grained details from nothing more than\nsome compact descriptors, i.e., texts, or cues. Fortunately, recent GPT-4o\nimage generation of OpenAI has achieved impressive cross-modality generation,\nediting, and design capabilities, which motivates us to answer the above\nquestion by exploring its potential in image compression fields. In this work,\nwe investigate two typical compression paradigms: textual coding and multimodal\ncoding (i.e., text + extremely low-resolution image), where all/most\npixel-level information is generated instead of compressing via the advanced\nGPT-4o image generation function. The essential challenge lies in how to\nmaintain semantic and structure consistency during the decoding process. To\novercome this, we propose a structure raster-scan prompt engineering mechanism\nto transform the image into textual space, which is compressed as the condition\nof GPT-4o image generation. Extensive experiments have shown that the\ncombination of our designed structural raster-scan prompts and GPT-4o's image\ngeneration function achieved the impressive performance compared with recent\nmultimodal/generative image compression at ultra-low bitrate, further\nindicating the potential of AIGC generation in image compression fields.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528AIGC\u57fa\u7840\u6a21\u578b\uff08\u5982GPT-4o\uff09\u8fdb\u884c\u56fe\u50cf\u538b\u7f29\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6587\u672c\u6216\u591a\u6a21\u6001\u7f16\u7801\u751f\u6210\u56fe\u50cf\uff0c\u800c\u975e\u4f20\u7edf\u50cf\u7d20\u7ea7\u538b\u7f29\u3002", "motivation": "AIGC\u57fa\u7840\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u4e3a\u56fe\u50cf\u538b\u7f29\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c24\u5176\u662fGPT-4o\u7684\u8de8\u6a21\u6001\u751f\u6210\u80fd\u529b\uff0c\u6fc0\u53d1\u4e86\u63a2\u7d22\u5176\u5728\u56fe\u50cf\u538b\u7f29\u9886\u57df\u7684\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u4e86\u6587\u672c\u7f16\u7801\u548c\u591a\u6a21\u6001\u7f16\u7801\uff08\u6587\u672c+\u6781\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff09\u4e24\u79cd\u8303\u5f0f\uff0c\u63d0\u51fa\u7ed3\u6784\u5149\u6805\u626b\u63cf\u63d0\u793a\u5de5\u7a0b\u673a\u5236\uff0c\u5c06\u56fe\u50cf\u8f6c\u5316\u4e3a\u6587\u672c\u7a7a\u95f4\u4f5c\u4e3a\u751f\u6210\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8d85\u4f4e\u6bd4\u7279\u7387\u4e0b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u591a\u6a21\u6001/\u751f\u6210\u5f0f\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u3002", "conclusion": "AIGC\u751f\u6210\u5728\u56fe\u50cf\u538b\u7f29\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.21195", "pdf": "https://arxiv.org/pdf/2504.21195", "abs": "https://arxiv.org/abs/2504.21195", "authors": ["Kelsey E. Ennis", "Elizabeth A. Barnes", "Marybeth C. Arcodia", "Martin A. Fernandez", "Eric D. Maloney"], "title": "Turning Up the Heat: Assessing 2-m Temperature Forecast Errors in AI Weather Prediction Models During Heat Waves", "categories": ["physics.ao-ph", "cs.AI", "cs.LG"], "comment": null, "summary": "Extreme heat is the deadliest weather-related hazard in the United States.\nFurthermore, it is increasing in intensity, frequency, and duration, making\nskillful forecasts vital to protecting life and property. Traditional numerical\nweather prediction (NWP) models struggle with extreme heat for medium-range and\nsubseasonal-to-seasonal (S2S) timescales. Meanwhile, artificial\nintelligence-based weather prediction (AIWP) models are progressing rapidly.\nHowever, it is largely unknown how well AIWP models forecast extremes,\nespecially for medium-range and S2S timescales. This study investigates 2-m\ntemperature forecasts for 60 heat waves across the four boreal seasons and over\nfour CONUS regions at lead times up to 20 days, using two AIWP models (Google\nGraphCast and Pangu-Weather) and one traditional NWP model (NOAA United\nForecast System Global Ensemble Forecast System (UFS GEFS)). First, case study\nanalyses show that both AIWP models and the UFS GEFS exhibit consistent cold\nbiases on regional scales in the 5-10 days of lead time before heat wave onset.\nGraphCast is the more skillful AIWP model, outperforming UFS GEFS and\nPangu-Weather in most locations. Next, the two AIWP models are isolated and\nanalyzed across all heat waves and seasons, with events split among the model's\ntesting (2018-2023) and training (1979-2017) periods. There are cold biases\nbefore and during the heat waves in both models and all seasons, except\nPangu-Weather in winter, which exhibits a mean warm bias before heat wave\nonset. Overall, results offer encouragement that AIWP models may be useful for\nmedium-range and S2S predictability of extreme heat.", "AI": {"tldr": "AIWP\u6a21\u578b\uff08\u5982GraphCast\u548cPangu-Weather\uff09\u5728\u6781\u7aef\u9ad8\u6e29\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfNWP\u6a21\u578b\uff08UFS GEFS\uff09\uff0c\u4f46\u4ecd\u5b58\u5728\u51b7\u504f\u5dee\u3002GraphCast\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u4e2d\u671f\u548c\u6b21\u5b63\u8282\u5c3a\u5ea6\u6781\u7aef\u9ad8\u6e29\u9884\u6d4b\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "motivation": "\u6781\u7aef\u9ad8\u6e29\u662f\u7f8e\u56fd\u6700\u81f4\u547d\u7684\u5929\u6c14\u707e\u5bb3\uff0c\u4e14\u5176\u5f3a\u5ea6\u3001\u9891\u7387\u548c\u6301\u7eed\u65f6\u95f4\u4e0d\u65ad\u589e\u52a0\u3002\u4f20\u7edfNWP\u6a21\u578b\u5728\u4e2d\u671f\u548c\u6b21\u5b63\u8282\u5c3a\u5ea6\u9884\u6d4b\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800cAIWP\u6a21\u578b\u7684\u6f5c\u529b\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e24\u79cdAIWP\u6a21\u578b\uff08GraphCast\u548cPangu-Weather\uff09\u548c\u4e00\u79cd\u4f20\u7edfNWP\u6a21\u578b\uff08UFS GEFS\uff09\u572860\u6b21\u6781\u7aef\u9ad8\u6e29\u4e8b\u4ef6\u4e2d\u76842\u7c73\u6e29\u5ea6\u9884\u6d4b\u80fd\u529b\uff0c\u65f6\u95f4\u8de8\u5ea6\u4e3a20\u5929\u3002", "result": "AIWP\u6a21\u578b\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8eUFS GEFS\uff0c\u4f46\u5b58\u5728\u51b7\u504f\u5dee\u3002GraphCast\u8868\u73b0\u6700\u4f73\uff0cPangu-Weather\u5728\u51ac\u5b63\u8868\u73b0\u51fa\u6696\u504f\u5dee\u3002", "conclusion": "AIWP\u6a21\u578b\u5728\u6781\u7aef\u9ad8\u6e29\u7684\u4e2d\u671f\u548c\u6b21\u5b63\u8282\u9884\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u504f\u5dee\u95ee\u9898\u3002"}}
{"id": "2504.21831", "pdf": "https://arxiv.org/pdf/2504.21831", "abs": "https://arxiv.org/abs/2504.21831", "authors": ["Anas Anwarul Haq Khan", "Utkarsh Verma", "Prateek Chanda", "Ganesh Ramakrishnan"], "title": "Early Exit and Multi Stage Knowledge Distillation in VLMs for Video Summarization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce DEEVISum (Distilled Early Exit Vision language model for\nSummarization), a lightweight, efficient, and scalable vision language model\ndesigned for segment wise video summarization. Leveraging multi modal prompts\nthat combine textual and audio derived signals, DEEVISum incorporates Multi\nStage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance\nbetween performance and efficiency. MSKD offers a 1.33% absolute F1 improvement\nover baseline distillation (0.5%), while EE reduces inference time by\napproximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset,\nour best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing\nthe performance of significantly larger models, all while maintaining a lower\ncomputational footprint. We publicly release our code and processed dataset to\nsupport further research.", "AI": {"tldr": "DEEVISum\u662f\u4e00\u79cd\u8f7b\u91cf\u9ad8\u6548\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u89c6\u9891\u6458\u8981\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u63d0\u793a\u548c\u591a\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\uff08MSKD\uff09\u4e0e\u65e9\u671f\u9000\u51fa\uff08EE\uff09\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u8f7b\u91cf\u4e14\u9ad8\u6548\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u89c6\u9891\u6458\u8981\u4efb\u52a1\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u548c\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u7ed3\u5408\u591a\u6a21\u6001\u63d0\u793a\uff08\u6587\u672c\u548c\u97f3\u9891\u4fe1\u53f7\uff09\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\uff08MSKD\uff09\u548c\u65e9\u671f\u9000\u51fa\uff08EE\uff09\u6280\u672f\u3002", "result": "\u5728TVSum\u6570\u636e\u96c6\u4e0a\uff0c\u6700\u4f73\u6a21\u578b\uff08PaLI Gemma2 3B + MSKD\uff09F1\u5f97\u5206\u4e3a61.1\uff0c\u6027\u80fd\u63a5\u8fd1\u66f4\u5927\u6a21\u578b\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002MSKD\u6bd4\u57fa\u7ebf\u84b8\u998f\u63d0\u53471.33% F1\uff0cEE\u51cf\u5c1121%\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "DEEVISum\u5728\u89c6\u9891\u6458\u8981\u4efb\u52a1\u4e2d\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2504.21836", "pdf": "https://arxiv.org/pdf/2504.21836", "abs": "https://arxiv.org/abs/2504.21836", "authors": ["Ipek Oztas", "Duygu Ceylan", "Aysegul Dundar"], "title": "3D Stylization via Large Reconstruction Model", "categories": ["cs.CV"], "comment": "Accepted to SIGGRAPH 2025", "summary": "With the growing success of text or image guided 3D generators, users demand\nmore control over the generation process, appearance stylization being one of\nthem. Given a reference image, this requires adapting the appearance of a\ngenerated 3D asset to reflect the visual style of the reference while\nmaintaining visual consistency from multiple viewpoints. To tackle this\nproblem, we draw inspiration from the success of 2D stylization methods that\nleverage the attention mechanisms in large image generation models to capture\nand transfer visual style. In particular, we probe if large reconstruction\nmodels, commonly used in the context of 3D generation, has a similar\ncapability. We discover that the certain attention blocks in these models\ncapture the appearance specific features. By injecting features from a visual\nstyle image to such blocks, we develop a simple yet effective 3D appearance\nstylization method. Our method does not require training or test time\noptimization. Through both quantitative and qualitative evaluations, we\ndemonstrate that our approach achieves superior results in terms of 3D\nappearance stylization, significantly improving efficiency while maintaining\nhigh-quality visual outcomes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u6216\u4f18\u5316\u76843D\u5916\u89c2\u98ce\u683c\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u91cd\u5efa\u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u98ce\u683c\u8fc1\u79fb\u3002", "motivation": "\u968f\u7740\u6587\u672c\u6216\u56fe\u50cf\u5f15\u5bfc\u76843D\u751f\u6210\u5668\u7684\u53d1\u5c55\uff0c\u7528\u6237\u5bf9\u751f\u6210\u8fc7\u7a0b\u7684\u63a7\u5236\u9700\u6c42\u589e\u52a0\uff0c\u5c24\u5176\u662f\u5916\u89c2\u98ce\u683c\u5316\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5927\u578b\u91cd\u5efa\u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u5757\uff0c\u53d1\u73b0\u5176\u80fd\u6355\u6349\u5916\u89c2\u7279\u5f81\uff0c\u4ece\u800c\u901a\u8fc7\u6ce8\u5165\u98ce\u683c\u56fe\u50cf\u7279\u5f81\u5b9e\u73b0\u98ce\u683c\u8fc1\u79fb\u3002", "result": "\u65b9\u6cd5\u57283D\u5916\u89c2\u98ce\u683c\u5316\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u5e76\u4fdd\u6301\u9ad8\u8d28\u91cf\u89c6\u89c9\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u4e3a3D\u751f\u6210\u7684\u5916\u89c2\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.21205", "pdf": "https://arxiv.org/pdf/2504.21205", "abs": "https://arxiv.org/abs/2504.21205", "authors": ["Connor Dilgren", "Purva Chiniya", "Luke Griffith", "Yu Ding", "Yizheng Chen"], "title": "SecRepoBench: Benchmarking LLMs for Secure Code Generation in Real-World Repositories", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "This paper introduces SecRepoBench, a benchmark to evaluate LLMs on secure\ncode generation in real-world repositories. SecRepoBench has 318 code\ngeneration tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 19\nstate-of-the-art LLMs using our benchmark and find that the models struggle\nwith generating correct and secure code. In addition, the performance of LLMs\nto generate self-contained programs as measured by prior benchmarks do not\ntranslate to comparative performance at generating secure and correct code at\nthe repository level in SecRepoBench. We show that the state-of-the-art prompt\nengineering techniques become less effective when applied to the repository\nlevel secure code generation problem. We conduct extensive experiments,\nincluding an agentic technique to generate secure code, to demonstrate that our\nbenchmark is currently the most difficult secure coding benchmark, compared to\nprevious state-of-the-art benchmarks. Finally, our comprehensive analysis\nprovides insights into potential directions for enhancing the ability of LLMs\nto generate correct and secure code in real-world repositories.", "AI": {"tldr": "SecRepoBench\u662f\u4e00\u4e2a\u8bc4\u4f30LLMs\u5728\u771f\u5b9e\u4ee3\u7801\u5e93\u4e2d\u751f\u6210\u5b89\u5168\u4ee3\u7801\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b318\u4e2a\u4efb\u52a1\u548c27\u4e2aC/C++\u4ed3\u5e93\uff0c\u8986\u76d615\u79cdCWE\u3002\u7814\u7a76\u53d1\u73b0\u73b0\u6709LLMs\u5728\u751f\u6210\u6b63\u786e\u548c\u5b89\u5168\u4ee3\u7801\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u5728\u4ed3\u5e93\u7ea7\u522b\u6548\u679c\u6709\u9650\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u771f\u5b9e\u4ee3\u7801\u5e93\u4e2d\u751f\u6210\u5b89\u5168\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\u3002", "method": "\u6784\u5efaSecRepoBench\u57fa\u51c6\uff0c\u5305\u542b318\u4e2a\u4efb\u52a1\u548c27\u4e2a\u4ed3\u5e93\uff0c\u8bc4\u4f3019\u79cdLLMs\uff0c\u5e76\u6d4b\u8bd5\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u3002", "result": "LLMs\u5728\u751f\u6210\u5b89\u5168\u4ee3\u7801\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u6548\u679c\u6709\u9650\uff0cSecRepoBench\u662f\u76ee\u524d\u6700\u96be\u7684\u57fa\u51c6\u3002", "conclusion": "\u7814\u7a76\u4e3a\u63d0\u5347LLMs\u5728\u771f\u5b9e\u4ee3\u7801\u5e93\u4e2d\u751f\u6210\u5b89\u5168\u4ee3\u7801\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.21846", "pdf": "https://arxiv.org/pdf/2504.21846", "abs": "https://arxiv.org/abs/2504.21846", "authors": ["Hadleigh Schwartz", "Xiaofeng Yan", "Charles J. Carver", "Xia Zhou"], "title": "Active Light Modulation to Counter Manipulation of Speech Visual Content", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "High-profile speech videos are prime targets for falsification, owing to\ntheir accessibility and influence. This work proposes Spotlight, a low-overhead\nand unobtrusive system for protecting live speech videos from visual\nfalsification of speaker identity and lip and facial motion. Unlike predominant\nfalsification detection methods operating in the digital domain, Spotlight\ncreates dynamic physical signatures at the event site and embeds them into all\nvideo recordings via imperceptible modulated light. These physical signatures\nencode semantically-meaningful features unique to the speech event, including\nthe speaker's identity and facial motion, and are cryptographically-secured to\nprevent spoofing. The signatures can be extracted from any video downstream and\nvalidated against the portrayed speech content to check its integrity. Key\nelements of Spotlight include (1) a framework for generating extremely compact\n(i.e., 150-bit), pose-invariant speech video features, based on\nlocality-sensitive hashing; and (2) an optical modulation scheme that embeds\n>200 bps into video while remaining imperceptible both in video and live.\nPrototype experiments on extensive video datasets show Spotlight achieves AUCs\n$\\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified\nvideos. Further, Spotlight is highly robust across recording conditions, video\npost-processing techniques, and white-box adversarial attacks on its video\nfeature extraction methodologies.", "AI": {"tldr": "Spotlight\u662f\u4e00\u79cd\u4f4e\u5f00\u9500\u3001\u975e\u4fb5\u5165\u6027\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u7269\u7406\u7b7e\u540d\u4fdd\u62a4\u5b9e\u65f6\u6f14\u8bb2\u89c6\u9891\u514d\u53d7\u89c6\u89c9\u4f2a\u9020\u3002", "motivation": "\u9ad8\u77e5\u540d\u5ea6\u6f14\u8bb2\u89c6\u9891\u6613\u53d7\u4f2a\u9020\uff0c\u56e0\u5176\u53ef\u8bbf\u95ee\u6027\u548c\u5f71\u54cd\u529b\u3002", "method": "\u5229\u7528\u8c03\u5236\u5149\u5d4c\u5165\u52a8\u6001\u7269\u7406\u7b7e\u540d\uff0c\u751f\u6210\u7d27\u51d1\u7684\u3001\u59ff\u6001\u4e0d\u53d8\u7684\u89c6\u9891\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5149\u5b66\u8c03\u5236\u65b9\u6848\u5b9e\u73b0\u9ad8\u6bd4\u7279\u7387\u5d4c\u5165\u3002", "result": "\u5b9e\u9a8c\u663e\u793aSpotlight\u5728\u68c0\u6d4b\u4f2a\u9020\u89c6\u9891\u65f6AUC\u22650.99\uff0c\u771f\u9633\u6027\u7387100%\uff0c\u4e14\u5bf9\u5404\u79cd\u5f55\u5236\u6761\u4ef6\u548c\u653b\u51fb\u5177\u6709\u9ad8\u9c81\u68d2\u6027\u3002", "conclusion": "Spotlight\u4e3a\u5b9e\u65f6\u6f14\u8bb2\u89c6\u9891\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u9632\u4f2a\u9020\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21135", "pdf": "https://arxiv.org/pdf/2504.21135", "abs": "https://arxiv.org/abs/2504.21135", "authors": ["Hanjing Xu", "Xiaoyuan Liu", "Alex Pothen", "Ilya Safro"], "title": "QAOA Parameter Transferability for Maximum Independent Set using Graph Attention Networks", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "The quantum approximate optimization algorithm (QAOA) is one of the promising\nvariational approaches of quantum computing to solve combinatorial optimization\nproblems. In QAOA, variational parameters need to be optimized by solving a\nseries of nonlinear, nonconvex optimization programs. In this work, we propose\na QAOA parameter transfer scheme using Graph Attention Networks (GAT) to solve\nMaximum Independent Set (MIS) problems. We prepare optimized parameters for\ngraphs of 12 and 14 vertices and use GATs to transfer their parameters to\nlarger graphs. Additionally, we design a hybrid distributed resource-aware\nalgorithm for MIS (HyDRA-MIS), which decomposes large problems into smaller\nones that can fit onto noisy intermediate-scale quantum (NISQ) computers. We\nintegrate our GAT-based parameter transfer approach to HyDRA-MIS and\ndemonstrate competitive results compared to KaMIS, a state-of-the-art classical\nMIS solver, on graphs with several thousands vertices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u7684QAOA\u53c2\u6570\u8fc1\u79fb\u65b9\u6848\uff0c\u7528\u4e8e\u89e3\u51b3\u6700\u5927\u72ec\u7acb\u96c6\uff08MIS\uff09\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u5206\u5e03\u5f0f\u8d44\u6e90\u611f\u77e5\u7b97\u6cd5HyDRA-MIS\uff0c\u5728NISQ\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u7ecf\u5178\u6c42\u89e3\u5668KaMIS\u7ade\u4e89\u7684\u7ed3\u679c\u3002", "motivation": "QAOA\u662f\u89e3\u51b3\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u6709\u524d\u9014\u7684\u91cf\u5b50\u8ba1\u7b97\u65b9\u6cd5\uff0c\u4f46\u5176\u53c2\u6570\u4f18\u5316\u590d\u6742\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u901a\u8fc7GAT\u8fc1\u79fb\u53c2\u6570\u548c\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u53ef\u4ee5\u6269\u5c55\u5176\u5e94\u7528\u8303\u56f4\u5e76\u63d0\u5347\u6548\u7387\u3002", "method": "1. \u4f7f\u7528GAT\u8fc1\u79fb12\u548c14\u9876\u70b9\u56fe\u7684\u4f18\u5316\u53c2\u6570\u81f3\u66f4\u5927\u56fe\uff1b2. \u8bbe\u8ba1HyDRA-MIS\u7b97\u6cd5\uff0c\u5c06\u5927\u95ee\u9898\u5206\u89e3\u4e3a\u9002\u5408NISQ\u8bbe\u5907\u7684\u5c0f\u95ee\u9898\uff1b3. \u7ed3\u5408GAT\u53c2\u6570\u8fc1\u79fb\u4e0eHyDRA-MIS\u3002", "result": "\u5728\u6570\u5343\u9876\u70b9\u56fe\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4e0e\u7ecf\u5178\u6c42\u89e3\u5668KaMIS\u7ade\u4e89\uff0c\u5c55\u793a\u4e86\u91cf\u5b50\u8ba1\u7b97\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "GAT\u53c2\u6570\u8fc1\u79fb\u548cHyDRA-MIS\u7684\u7ed3\u5408\u4e3aQAOA\u5728NISQ\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u6269\u5c55\u4e86\u91cf\u5b50\u8ba1\u7b97\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u7684\u80fd\u529b\u3002"}}
{"id": "2504.21847", "pdf": "https://arxiv.org/pdf/2504.21847", "abs": "https://arxiv.org/abs/2504.21847", "authors": ["Derong Jin", "Ruohan Gao"], "title": "Differentiable Room Acoustic Rendering with Multi-View Vision Priors", "categories": ["cs.CV", "cs.SD"], "comment": "Project Page: https://humathe.github.io/avdar/", "summary": "An immersive acoustic experience enabled by spatial audio is just as crucial\nas the visual aspect in creating realistic virtual environments. However,\nexisting methods for room impulse response estimation rely either on\ndata-demanding learning-based models or computationally expensive physics-based\nmodeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic\nRendering (AV-DAR), a framework that leverages visual cues extracted from\nmulti-view images and acoustic beam tracing for physics-based room acoustic\nrendering. Experiments across six real-world environments from two datasets\ndemonstrate that our multimodal, physics-based approach is efficient,\ninterpretable, and accurate, significantly outperforming a series of prior\nmethods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves\ncomparable performance to models trained on 10 times more data while delivering\nrelative gains ranging from 16.6% to 50.9% when trained at the same scale.", "AI": {"tldr": "AV-DAR\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u7ebf\u7d22\u548c\u58f0\u5b66\u675f\u8ffd\u8e2a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u51c6\u786e\u7684\u623f\u95f4\u58f0\u5b66\u6e32\u67d3\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u5bc6\u96c6\u578b\u5b66\u4e60\u6a21\u578b\u6216\u8ba1\u7b97\u6602\u8d35\u7684\u7269\u7406\u5efa\u6a21\uff0cAV-DAR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u591a\u89c6\u89d2\u56fe\u50cf\u63d0\u53d6\u89c6\u89c9\u7ebf\u7d22\uff0c\u7ed3\u5408\u58f0\u5b66\u675f\u8ffd\u8e2a\u8fdb\u884c\u7269\u7406\u5efa\u6a21\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u63a5\u8fd1\u6570\u636e\u91cf\u591a10\u500d\u7684\u6a21\u578b\uff0c\u76f8\u5bf9\u63d0\u534716.6%\u81f350.9%\u3002", "conclusion": "AV-DAR\u4e3a\u623f\u95f4\u58f0\u5b66\u6e32\u67d3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21850", "pdf": "https://arxiv.org/pdf/2504.21850", "abs": "https://arxiv.org/abs/2504.21850", "authors": ["Xindi Wu", "Hee Seung Hwang", "Polina Kirichenko", "Olga Russakovsky"], "title": "COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning", "categories": ["cs.CV"], "comment": "17 pages, 13 figures", "summary": "Multimodal Large Language Models (MLLMs) excel at simple vision-language\ntasks but struggle when faced with complex tasks that require multiple\ncapabilities, such as simultaneously recognizing objects, counting them, and\nunderstanding their spatial relationships. This might be partially the result\nof the fact that Visual Instruction Tuning (VIT), a critical training step for\nMLLMs, has traditionally focused on scaling data volume, but not the\ncompositional complexity of training examples. We propose COMPACT\n(COMPositional Atomic-to-complex visual Capability Tuning), which generates a\ntraining dataset explicitly controlling for the compositional complexity of the\ntraining examples. The data from COMPACT allows MLLMs to train on combinations\nof atomic capabilities to learn complex capabilities more efficiently. Across\nall benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT\nwhile using less than 10% of its data budget, and even outperforms it on\nseveral, especially those involving complex multi-capability tasks. For\nexample, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0%\nimprovement on MM-Vet compared to the full-scale VIT on particularly complex\nquestions that require four or more atomic capabilities. COMPACT offers a\nscalable, data-efficient, visual compositional tuning recipe to improve on\ncomplex visual-language tasks.", "AI": {"tldr": "COMPACT\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u63a7\u5236\u8bad\u7ec3\u6570\u636e\u7684\u7ec4\u5408\u590d\u6742\u6027\u6765\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u590d\u6742\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u65b9\u6cd5\u3002", "motivation": "MLLMs\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u591a\u80fd\u529b\u7ec4\u5408\u7684\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f20\u7edf\u89c6\u89c9\u6307\u4ee4\u8c03\u4f18\uff08VIT\uff09\u672a\u5173\u6ce8\u8bad\u7ec3\u6570\u636e\u7684\u7ec4\u5408\u590d\u6742\u6027\u3002", "method": "COMPACT\u751f\u6210\u4e00\u4e2a\u660e\u786e\u63a7\u5236\u8bad\u7ec3\u6570\u636e\u7ec4\u5408\u590d\u6742\u6027\u7684\u6570\u636e\u96c6\uff0c\u4f7fMLLMs\u80fd\u66f4\u9ad8\u6548\u5730\u5b66\u4e60\u590d\u6742\u80fd\u529b\u3002", "result": "COMPACT\u5728\u6570\u636e\u91cf\u4ec5\u4e3aLLaVA-665k VIT\u768410%\u65f6\uff0c\u6027\u80fd\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\uff0c\u5c24\u5176\u5728\u9700\u8981\u591a\u80fd\u529b\u7684\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\uff08\u5982MMStar\u548cMM-Vet\uff09\u3002", "conclusion": "COMPACT\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86MLLMs\u5728\u590d\u6742\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2504.21182", "pdf": "https://arxiv.org/pdf/2504.21182", "abs": "https://arxiv.org/abs/2504.21182", "authors": ["Maximilian Egger", "R\u00fcdiger Urbanke", "Rawad Bitar"], "title": "Federated One-Shot Learning with Data Privacy and Objective-Hiding", "categories": ["cs.CR", "cs.DC", "cs.IT", "cs.LG", "math.IT", "stat.ML"], "comment": null, "summary": "Privacy in federated learning is crucial, encompassing two key aspects:\nsafeguarding the privacy of clients' data and maintaining the privacy of the\nfederator's objective from the clients. While the first aspect has been\nextensively studied, the second has received much less attention.\n  We present a novel approach that addresses both concerns simultaneously,\ndrawing inspiration from techniques in knowledge distillation and private\ninformation retrieval to provide strong information-theoretic privacy\nguarantees.\n  Traditional private function computation methods could be used here; however,\nthey are typically limited to linear or polynomial functions. To overcome these\nconstraints, our approach unfolds in three stages. In stage 0, clients perform\nthe necessary computations locally. In stage 1, these results are shared among\nthe clients, and in stage 2, the federator retrieves its desired objective\nwithout compromising the privacy of the clients' data. The crux of the method\nis a carefully designed protocol that combines secret-sharing-based multi-party\ncomputation and a graph-based private information retrieval scheme. We show\nthat our method outperforms existing tools from the literature when properly\nadapted to this setting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u6570\u636e\u548c\u8054\u90a6\u76ee\u6807\u9690\u79c1\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u548c\u79c1\u6709\u4fe1\u606f\u68c0\u7d22\u6280\u672f\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u8054\u90a6\u76ee\u6807\u9690\u79c1\u4fdd\u62a4\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5206\u4e09\u9636\u6bb5\uff1a\u672c\u5730\u8ba1\u7b97\u3001\u5ba2\u6237\u7aef\u5171\u4eab\u7ed3\u679c\u3001\u8054\u90a6\u76ee\u6807\u5b89\u5168\u68c0\u7d22\uff0c\u7ed3\u5408\u591a\u65b9\u8ba1\u7b97\u548c\u56fe\u57fa\u79c1\u6709\u4fe1\u606f\u68c0\u7d22\u3002", "result": "\u65b9\u6cd5\u5728\u9002\u5e94\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u53cc\u91cd\u9690\u79c1\u95ee\u9898\u3002"}}
{"id": "2504.21853", "pdf": "https://arxiv.org/pdf/2504.21853", "abs": "https://arxiv.org/abs/2504.21853", "authors": ["Jiwen Yu", "Yiran Qin", "Haoxuan Che", "Quande Liu", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Hao Chen", "Xihui Liu"], "title": "A Survey of Interactive Generative Video", "categories": ["cs.CV"], "comment": null, "summary": "Interactive Generative Video (IGV) has emerged as a crucial technology in\nresponse to the growing demand for high-quality, interactive video content\nacross various domains. In this paper, we define IGV as a technology that\ncombines generative capabilities to produce diverse high-quality video content\nwith interactive features that enable user engagement through control signals\nand responsive feedback. We survey the current landscape of IGV applications,\nfocusing on three major domains: 1) gaming, where IGV enables infinite\nexploration in virtual worlds; 2) embodied AI, where IGV serves as a\nphysics-aware environment synthesizer for training agents in multimodal\ninteraction with dynamically evolving scenes; and 3) autonomous driving, where\nIGV provides closed-loop simulation capabilities for safety-critical testing\nand validation. To guide future development, we propose a comprehensive\nframework that decomposes an ideal IGV system into five essential modules:\nGeneration, Control, Memory, Dynamics, and Intelligence. Furthermore, we\nsystematically analyze the technical challenges and future directions in\nrealizing each component for an ideal IGV system, such as achieving real-time\ngeneration, enabling open-domain control, maintaining long-term coherence,\nsimulating accurate physics, and integrating causal reasoning. We believe that\nthis systematic analysis will facilitate future research and development in the\nfield of IGV, ultimately advancing the technology toward more sophisticated and\npractical applications.", "AI": {"tldr": "\u672c\u6587\u5b9a\u4e49\u4e86\u4ea4\u4e92\u5f0f\u751f\u6210\u89c6\u9891\uff08IGV\uff09\u6280\u672f\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u6e38\u620f\u3001\u5177\u8eabAI\u548c\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u5e94\u7528\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e94\u4e2a\u6a21\u5757\u7684\u6846\u67b6\uff0c\u5e76\u5206\u6790\u4e86\u6280\u672f\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u6ee1\u8db3\u5bf9\u9ad8\u8d28\u91cf\u3001\u4ea4\u4e92\u5f0f\u89c6\u9891\u5185\u5bb9\u7684\u9700\u6c42\uff0c\u63a8\u52a8IGV\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u5f53\u524dIGV\u5e94\u7528\uff0c\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u751f\u6210\u3001\u63a7\u5236\u3001\u8bb0\u5fc6\u3001\u52a8\u6001\u548c\u667a\u80fd\u4e94\u4e2a\u6a21\u5757\u7684\u6846\u67b6\u3002", "result": "\u603b\u7ed3\u4e86IGV\u5728\u591a\u4e2a\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u6307\u51fa\u4e86\u6280\u672f\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u7cfb\u7edf\u5206\u6790\u6709\u52a9\u4e8e\u63a8\u52a8IGV\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5b9e\u73b0\u66f4\u590d\u6742\u548c\u5b9e\u7528\u7684\u5e94\u7528\u3002"}}
{"id": "2504.21855", "pdf": "https://arxiv.org/pdf/2504.21855", "abs": "https://arxiv.org/abs/2504.21855", "authors": ["Qihao Liu", "Ju He", "Qihang Yu", "Liang-Chieh Chen", "Alan Yuille"], "title": "ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction", "categories": ["cs.CV"], "comment": "Project Page: https://revision-video.github.io/", "summary": "In recent years, video generation has seen significant advancements. However,\nchallenges still persist in generating complex motions and interactions. To\naddress these challenges, we introduce ReVision, a plug-and-play framework that\nexplicitly integrates parameterized 3D physical knowledge into a pretrained\nconditional video generation model, significantly enhancing its ability to\ngenerate high-quality videos with complex motion and interactions.\nSpecifically, ReVision consists of three stages. First, a video diffusion model\nis used to generate a coarse video. Next, we extract a set of 2D and 3D\nfeatures from the coarse video to construct a 3D object-centric representation,\nwhich is then refined by our proposed parameterized physical prior model to\nproduce an accurate 3D motion sequence. Finally, this refined motion sequence\nis fed back into the same video diffusion model as additional conditioning,\nenabling the generation of motion-consistent videos, even in scenarios\ninvolving complex actions and interactions. We validate the effectiveness of\nour approach on Stable Video Diffusion, where ReVision significantly improves\nmotion fidelity and coherence. Remarkably, with only 1.5B parameters, it even\noutperforms a state-of-the-art video generation model with over 13B parameters\non complex video generation by a substantial margin. Our results suggest that,\nby incorporating 3D physical knowledge, even a relatively small video diffusion\nmodel can generate complex motions and interactions with greater realism and\ncontrollability, offering a promising solution for physically plausible video\ngeneration.", "AI": {"tldr": "ReVision\u662f\u4e00\u4e2a\u7ed3\u54083D\u7269\u7406\u77e5\u8bc6\u7684\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u8fd0\u52a8\u548c\u4ea4\u4e92\u89c6\u9891\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u590d\u6742\u8fd0\u52a8\u548c\u4ea4\u4e92\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a\u7c97\u89c6\u9891\u751f\u6210\u30013D\u7279\u5f81\u63d0\u53d6\u4e0e\u7269\u7406\u4f18\u5316\u3001\u53cd\u9988\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002", "result": "\u5728Stable Video Diffusion\u4e0a\u9a8c\u8bc1\uff0cReVision\u663e\u8457\u63d0\u5347\u8fd0\u52a8\u4fdd\u771f\u5ea6\u548c\u8fde\u8d2f\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u66f4\u5927\u6a21\u578b\u3002", "conclusion": "\u7ed3\u54083D\u7269\u7406\u77e5\u8bc6\u7684\u5c0f\u6a21\u578b\u4e5f\u80fd\u9ad8\u6548\u751f\u6210\u590d\u6742\u89c6\u9891\uff0c\u4e3a\u7269\u7406\u5408\u7406\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2504.21228", "pdf": "https://arxiv.org/pdf/2504.21228", "abs": "https://arxiv.org/abs/2504.21228", "authors": ["Rui Wang", "Junda Wu", "Yu Xia", "Tong Yu", "Ruiyi Zhang", "Ryan Rossi", "Lina Yao", "Julian McAuley"], "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCachePrune\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u526aKV\u7f13\u5b58\u4e2d\u7684\u4efb\u52a1\u89e6\u53d1\u795e\u7ecf\u5143\uff0c\u9632\u5fa1LLMs\u7684\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u3002", "motivation": "LLMs\u6613\u53d7\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u56e0\u5176\u65e0\u6cd5\u533a\u5206\u63d0\u793a\u4e2d\u7684\u6570\u636e\u548c\u6307\u4ee4\uff0c\u5bfc\u81f4\u6a21\u578b\u504f\u79bb\u7528\u6237\u6307\u4ee4\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eDPO\u76ee\u6807\u7684\u635f\u5931\u51fd\u6570\u8fdb\u884c\u7279\u5f81\u5f52\u56e0\uff0c\u8bc6\u522b\u5e76\u4fee\u526a\u4efb\u52a1\u89e6\u53d1\u795e\u7ecf\u5143\uff0c\u907f\u514d\u6a21\u578b\u5c06\u63d0\u793a\u4e0a\u4e0b\u6587\u8bef\u8ba4\u4e3a\u6307\u4ee4\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCachePrune\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\uff0c\u4e14\u4e0d\u5f71\u54cd\u54cd\u5e94\u8d28\u91cf\u3002", "conclusion": "CachePrune\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u9c81\u68d2\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u9632\u5fa1\u624b\u6bb5\u3002"}}
{"id": "2504.21067", "pdf": "https://arxiv.org/pdf/2504.21067", "abs": "https://arxiv.org/abs/2504.21067", "authors": ["Yuhan Xie", "Yixi Cai", "Yinqiang Zhang", "Lei Yang", "Jia Pan"], "title": "GauSS-MI: Gaussian Splatting Shannon Mutual Information for Active 3D Reconstruction", "categories": ["cs.GR", "cs.CV", "cs.RO"], "comment": null, "summary": "This research tackles the challenge of real-time active view selection and\nuncertainty quantification on visual quality for active 3D reconstruction.\nVisual quality is a critical aspect of 3D reconstruction. Recent advancements\nsuch as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have\nnotably enhanced the image rendering quality of reconstruction models.\nNonetheless, the efficient and effective acquisition of input images for\nreconstruction-specifically, the selection of the most informative\nviewpoint-remains an open challenge, which is crucial for active\nreconstruction. Existing studies have primarily focused on evaluating geometric\ncompleteness and exploring unobserved or unknown regions, without direct\nevaluation of the visual uncertainty within the reconstruction model. To\naddress this gap, this paper introduces a probabilistic model that quantifies\nvisual uncertainty for each Gaussian. Leveraging Shannon Mutual Information, we\nformulate a criterion, Gaussian Splatting Shannon Mutual Information\n(GauSS-MI), for real-time assessment of visual mutual information from novel\nviewpoints, facilitating the selection of next best view. GauSS-MI is\nimplemented within an active reconstruction system integrated with a view and\nmotion planner. Extensive experiments across various simulated and real-world\nscenes showcase the superior visual quality and reconstruction efficiency\nperformance of the proposed system.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6e85\u5c04\u7684\u89c6\u89c9\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff08GauSS-MI\uff09\uff0c\u7528\u4e8e\u5b9e\u65f6\u9009\u62e9\u6700\u4f73\u89c6\u89d2\uff0c\u63d0\u53473D\u91cd\u5efa\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u51e0\u4f55\u5b8c\u6574\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u91cd\u5efa\u6a21\u578b\u4e2d\u7684\u89c6\u89c9\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u89c6\u89d2\u9009\u62e9\u4e0d\u591f\u9ad8\u6548\u3002", "method": "\u63d0\u51fa\u6982\u7387\u6a21\u578b\u91cf\u5316\u9ad8\u65af\u6e85\u5c04\u7684\u89c6\u89c9\u4e0d\u786e\u5b9a\u6027\uff0c\u5229\u7528\u9999\u519c\u4e92\u4fe1\u606f\u51c6\u5219\uff08GauSS-MI\uff09\u5b9e\u65f6\u8bc4\u4f30\u65b0\u89c6\u89d2\u7684\u89c6\u89c9\u4e92\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u5747\u80fd\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\u548c\u91cd\u5efa\u6548\u7387\u3002", "conclusion": "GauSS-MI\u4e3a\u4e3b\u52a83D\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89c6\u89d2\u9009\u62e9\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u89c6\u89c9\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u7a7a\u767d\u3002"}}
{"id": "2504.21235", "pdf": "https://arxiv.org/pdf/2504.21235", "abs": "https://arxiv.org/abs/2504.21235", "authors": ["Ben Goertzel"], "title": "Efficient Quantum-Safe Homomorphic Encryption for Quantum Computer Programs", "categories": ["quant-ph", "cs.AI"], "comment": null, "summary": "We present a lattice-based scheme for homomorphic evaluation of quantum\nprograms and proofs that remains secure against quantum adversaries. Classical\nhomomorphic encryption is lifted to the quantum setting by replacing\ncomposite-order groups with Module Learning-With-Errors (MLWE) lattices and by\ngeneralizing polynomial functors to bounded natural super functors (BNSFs). A\nsecret depolarizing BNSF mask hides amplitudes, while each quantum state is\nstored as an MLWE ciphertext pair. We formalize security with the qIND-CPA game\nthat allows coherent access to the encryption oracle and give a four-hybrid\nreduction to decisional MLWE.\n  The design also covers practical issues usually left open. A typed QC-bridge\nkeeps classical bits produced by measurements encrypted yet still usable as\ncontrols, with weak-measurement semantics for expectation-value workloads.\nEncrypted Pauli twirls add circuit privacy. If a fixed knowledge base is\nneeded, its axioms are shipped as MLWE \"capsules\"; the evaluator can use them\nbut cannot read them. A rho-calculus driver schedules encrypted tasks across\nseveral QPUs and records an auditable trace on an RChain-style ledger.\n  Performance analysis shows that the extra lattice arithmetic fits inside\ntoday's QPU idle windows: a 100-qubit, depth-10^3 teleportation-based proof\nruns in about 10 ms, the public key (seed only) is 32 bytes, and even a\nCCA-level key stays below 300 kB. A photonic Dirac-3 prototype that executes\nhomomorphic teleportation plus knowledge-base-relative amplitude checks appears\nfeasible with current hardware. These results indicate that fully homomorphic,\nknowledge-base-aware quantum reasoning is compatible with near-term quantum\nclouds and standard post-quantum security assumptions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u683c\u7684\u91cf\u5b50\u7a0b\u5e8f\u540c\u6001\u52a0\u5bc6\u65b9\u6848\uff0c\u652f\u6301\u91cf\u5b50\u5bf9\u6297\u73af\u5883\u4e0b\u7684\u5b89\u5168\u9a8c\u8bc1\u3002", "motivation": "\u5c06\u7ecf\u5178\u540c\u6001\u52a0\u5bc6\u6269\u5c55\u5230\u91cf\u5b50\u9886\u57df\uff0c\u89e3\u51b3\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u5b89\u5168\u4e0e\u9690\u79c1\u95ee\u9898\u3002", "method": "\u4f7f\u7528MLWE\u683c\u548cBNSF\u63a9\u7801\u9690\u85cf\u632f\u5e45\uff0c\u91cf\u5b50\u72b6\u6001\u5b58\u50a8\u4e3aMLWE\u5bc6\u6587\u5bf9\uff0c\u5e76\u901a\u8fc7\u56db\u6df7\u5408\u5f52\u7ea6\u8bc1\u660e\u5b89\u5168\u6027\u3002", "result": "\u6027\u80fd\u5206\u6790\u8868\u660e\uff0c\u65b9\u6848\u9002\u7528\u4e8e\u5f53\u524d\u91cf\u5b50\u786c\u4ef6\uff0c100\u91cf\u5b50\u6bd4\u7279\u7684\u6df1\u5ea6\u8ba1\u7b97\u4ec5\u970010\u6beb\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6848\u8868\u660e\u5168\u540c\u6001\u91cf\u5b50\u63a8\u7406\u4e0e\u8fd1\u91cf\u5b50\u4e91\u53ca\u540e\u91cf\u5b50\u5b89\u5168\u5047\u8bbe\u517c\u5bb9\u3002"}}
{"id": "2504.21199", "pdf": "https://arxiv.org/pdf/2504.21199", "abs": "https://arxiv.org/abs/2504.21199", "authors": ["Terrance Liu", "Eileen Xiao", "Pratiksha Thaker", "Adam Smith", "Zhiwei Steven Wu"], "title": "Generate-then-Verify: Reconstructing Data from Limited Published Statistics", "categories": ["stat.ML", "cs.CR", "cs.LG"], "comment": null, "summary": "We study the problem of reconstructing tabular data from aggregate\nstatistics, in which the attacker aims to identify interesting claims about the\nsensitive data that can be verified with 100% certainty given the aggregates.\nSuccessful attempts in prior work have conducted studies in settings where the\nset of published statistics is rich enough that entire datasets can be\nreconstructed with certainty. In our work, we instead focus on the regime where\nmany possible datasets match the published statistics, making it impossible to\nreconstruct the entire private dataset perfectly (i.e., when approaches in\nprior work fail). We propose the problem of partial data reconstruction, in\nwhich the goal of the adversary is to instead output a $\\textit{subset}$ of\nrows and/or columns that are $\\textit{guaranteed to be correct}$. We introduce\na novel integer programming approach that first $\\textbf{generates}$ a set of\nclaims and then $\\textbf{verifies}$ whether each claim holds for all possible\ndatasets consistent with the published aggregates. We evaluate our approach on\nthe housing-level microdata from the U.S. Decennial Census release,\ndemonstrating that privacy violations can still persist even when information\npublished about such data is relatively sparse.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4ece\u805a\u5408\u7edf\u8ba1\u6570\u636e\u4e2d\u91cd\u5efa\u8868\u683c\u6570\u636e\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u90e8\u5206\u6570\u636e\u91cd\u5efa\u65b9\u6cd5\uff0c\u5373\u4f7f\u65e0\u6cd5\u5b8c\u5168\u91cd\u5efa\u6570\u636e\uff0c\u4e5f\u80fd\u4fdd\u8bc1\u90e8\u5206\u884c\u6216\u5217\u7684\u6b63\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7edf\u8ba1\u6570\u636e\u4e30\u5bcc\u65f6\u53ef\u4ee5\u5b8c\u5168\u91cd\u5efa\u6570\u636e\uff0c\u4f46\u5728\u7edf\u8ba1\u6570\u636e\u7a00\u758f\u65f6\u65e0\u6cd5\u5b9e\u73b0\u3002\u672c\u6587\u5173\u6ce8\u540e\u8005\u60c5\u51b5\uff0c\u63a2\u7d22\u5982\u4f55\u5728\u6570\u636e\u7a00\u758f\u65f6\u4ecd\u80fd\u4fdd\u8bc1\u90e8\u5206\u6570\u636e\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6574\u6570\u89c4\u5212\u7684\u65b9\u6cd5\uff0c\u9996\u5148\u751f\u6210\u4e00\u7ec4\u58f0\u660e\uff0c\u7136\u540e\u9a8c\u8bc1\u8fd9\u4e9b\u58f0\u660e\u662f\u5426\u5728\u6240\u6709\u53ef\u80fd\u7684\u5339\u914d\u6570\u636e\u96c6\u4e2d\u6210\u7acb\u3002", "result": "\u5728U.S. Decennial Census\u7684\u4f4f\u623f\u5fae\u89c2\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\uff0c\u8868\u660e\u5373\u4f7f\u6570\u636e\u53d1\u5e03\u7a00\u758f\uff0c\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u4ecd\u5b58\u5728\u3002", "conclusion": "\u90e8\u5206\u6570\u636e\u91cd\u5efa\u65b9\u6cd5\u5728\u7edf\u8ba1\u6570\u636e\u7a00\u758f\u65f6\u4ecd\u80fd\u6709\u6548\u8bc6\u522b\u9690\u79c1\u6f0f\u6d1e\uff0c\u4e3a\u6570\u636e\u53d1\u5e03\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2504.21209", "pdf": "https://arxiv.org/pdf/2504.21209", "abs": "https://arxiv.org/abs/2504.21209", "authors": ["Xuhang Chen", "Ihsane Olakorede", "Stefan Yu B\u00f6gli", "Wenhao Xu", "Erta Beqiri", "Xuemeng Li", "Chenyu Tang", "Zeyu Gao", "Shuo Gao", "Ari Ercole", "Peter Smielewski"], "title": "Generalised Label-free Artefact Cleaning for Real-time Medical Pulsatile Time Series", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Artefacts compromise clinical decision-making in the use of medical time\nseries. Pulsatile waveforms offer probabilities for accurate artefact\ndetection, yet most approaches rely on supervised manners and overlook\npatient-level distribution shifts. To address these issues, we introduce a\ngeneralised label-free framework, GenClean, for real-time artefact cleaning and\nleverage an in-house dataset of 180,000 ten-second arterial blood pressure\n(ABP) samples for training. We first investigate patient-level generalisation,\ndemonstrating robust performances under both intra- and inter-patient\ndistribution shifts. We further validate its effectiveness through challenging\ncross-disease cohort experiments on the MIMIC-III database. Additionally, we\nextend our method to photoplethysmography (PPG), highlighting its applicability\nto diverse medical pulsatile signals. Finally, its integration into ICM+, a\nclinical research monitoring software, confirms the real-time feasibility of\nour framework, emphasising its practical utility in continuous physiological\nmonitoring. This work provides a foundational step toward precision medicine in\nimproving the reliability of high-resolution medical time series analysis", "AI": {"tldr": "GenClean\u662f\u4e00\u4e2a\u65e0\u9700\u6807\u7b7e\u7684\u901a\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u6e05\u7406\u533b\u7597\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u4f2a\u5f71\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8109\u52a8\u4fe1\u53f7\uff0c\u5e76\u5728\u5b9e\u9645\u4e34\u5e8a\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u533b\u7597\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u4f2a\u5f71\u4f1a\u5f71\u54cd\u4e34\u5e8a\u51b3\u7b56\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\u4e14\u5ffd\u7565\u60a3\u8005\u5c42\u9762\u7684\u5206\u5e03\u53d8\u5316\u3002", "method": "\u63d0\u51faGenClean\u6846\u67b6\uff0c\u5229\u752818\u4e07\u4efd\u52a8\u8109\u8840\u538b\u6837\u672c\u8bad\u7ec3\uff0c\u9a8c\u8bc1\u5176\u5728\u60a3\u8005\u5185\u548c\u60a3\u8005\u95f4\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u6269\u5c55\u5230\u5149\u7535\u5bb9\u79ef\u56fe\u4fe1\u53f7\u3002", "result": "\u5728MIMIC-III\u6570\u636e\u5e93\u7684\u8de8\u75be\u75c5\u961f\u5217\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u96c6\u6210\u5230\u4e34\u5e8a\u7814\u7a76\u76d1\u6d4b\u8f6f\u4ef6ICM+\u4e2d\u3002", "conclusion": "GenClean\u4e3a\u9ad8\u5206\u8fa8\u7387\u533b\u7597\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u7cbe\u51c6\u533b\u5b66\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.21227", "pdf": "https://arxiv.org/pdf/2504.21227", "abs": "https://arxiv.org/abs/2504.21227", "authors": ["Omid Halimi Milani", "Amanda Nikho", "Lauren Mills", "Marouane Tliba", "Ahmet Enis Cetin", "Mohammed H. Elnagar"], "title": "Gradient Attention Map Based Verification of Deep Convolutional Neural Networks with Application to X-ray Image Datasets", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "13 pages, 7 figures, accepted at IEEE VLSI Test Symposium (VTS) 2025", "summary": "Deep learning models have great potential in medical imaging, including\northodontics and skeletal maturity assessment. However, applying a model to\ndata different from its training set can lead to unreliable predictions that\nmay impact patient care. To address this, we propose a comprehensive\nverification framework that evaluates model suitability through multiple\ncomplementary strategies. First, we introduce a Gradient Attention Map\n(GAM)-based approach that analyzes attention patterns using Grad-CAM and\ncompares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine\nSimilarity, Pearson Correlation, KL Divergence, and Wasserstein Distance.\nSecond, we extend verification to early convolutional feature maps, capturing\nstructural mis-alignments missed by attention alone. Finally, we incorporate an\nadditional garbage class into the classification model to explicitly reject\nout-of-distribution inputs. Experimental results demonstrate that these\ncombined methods effectively identify unsuitable models and inputs, promoting\nsafer and more reliable deployment of deep learning in medical imaging.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u79cd\u4e92\u8865\u7b56\u7565\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u9002\u7528\u6027\uff0c\u786e\u4fdd\u9884\u6d4b\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5e94\u7528\u4e8e\u4e0e\u8bad\u7ec3\u96c6\u4e0d\u540c\u7684\u6570\u636e\u53ef\u80fd\u5bfc\u81f4\u4e0d\u53ef\u9760\u9884\u6d4b\uff0c\u5f71\u54cd\u60a3\u8005\u62a4\u7406\u3002", "method": "1. \u57fa\u4e8e\u68af\u5ea6\u6ce8\u610f\u529b\u56fe\uff08GAM\uff09\u7684\u65b9\u6cd5\u5206\u6790\u6ce8\u610f\u529b\u6a21\u5f0f\uff1b2. \u6269\u5c55\u5230\u65e9\u671f\u5377\u79ef\u7279\u5f81\u56fe\uff1b3. \u5728\u5206\u7c7b\u6a21\u578b\u4e2d\u5f15\u5165\u5783\u573e\u7c7b\u4ee5\u62d2\u7edd\u5206\u5e03\u5916\u8f93\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u4e0d\u9002\u7528\u6a21\u578b\u548c\u8f93\u5165\u3002", "conclusion": "\u8be5\u6846\u67b6\u4fc3\u8fdb\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u66f4\u5b89\u5168\u53ef\u9760\u7684\u90e8\u7f72\u3002"}}
{"id": "2504.21331", "pdf": "https://arxiv.org/pdf/2504.21331", "abs": "https://arxiv.org/abs/2504.21331", "authors": ["Alfred Yan", "Muhammad Nur Talha Kilic", "Gert Nolze", "Ankit Agrawal", "Alok Choudhary", "Roberto dos Reis", "Vinayak Dravid"], "title": "Towards Space Group Determination from EBSD Patterns: The Role of Deep Learning and High-throughput Dynamical Simulations", "categories": ["cond-mat.mtrl-sci", "cs.CV"], "comment": "33 pages, preliminary version", "summary": "The design of novel materials hinges on the understanding of\nstructure-property relationships. However, our capability to synthesize a large\nnumber of materials has outpaced the ability and speed needed to characterize\nthem. While the overall chemical constituents can be readily known during\nsynthesis, the structural evolution and characterization of newly synthesized\nsamples remains a bottleneck for the ultimate goal of high throughput\nnanomaterials discovery. Thus, scalable methods for crystal symmetry\ndetermination that can analyze a large volume of material samples within a\nshort time-frame are especially needed. Kikuchi diffraction in the SEM is a\npromising technique for this due to its sensitivity to dynamical scattering,\nwhich may provide information beyond just the seven crystal systems and\nfourteen Bravais lattices. After diffraction patterns are collected from\nmaterial samples, deep learning methods may be able to classify the space group\nsymmetries using the patterns as input, which paired with the elemental\ncomposition, would help enable the determination of the crystal structure. To\ninvestigate the feasibility of this solution, neural networks were trained to\npredict the space group type of background corrected EBSD patterns. Our\nnetworks were first trained and tested on an artificial dataset of EBSD\npatterns of 5,148 different cubic phases, created through physics-based\ndynamical simulations. Next, Maximum Classifier Discrepancy, an unsupervised\ndeep learning-based domain adaptation method, was utilized to train neural\nnetworks to make predictions for experimental EBSD patterns. We introduce a\nrelabeling scheme, which enables our models to achieve accuracy scores higher\nthan 90% on simulated and experimental data, suggesting that neural networks\nare capable of making predictions of crystal symmetry from an EBSD pattern.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u548cKikuchi\u884d\u5c04\u6280\u672f\u5feb\u901f\u786e\u5b9a\u6676\u4f53\u5bf9\u79f0\u6027\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u6750\u6599\u8868\u5f81\u901f\u5ea6\u6ede\u540e\u4e8e\u5408\u6210\u901f\u5ea6\u7684\u95ee\u9898\u3002", "motivation": "\u6750\u6599\u5408\u6210\u901f\u5ea6\u8fdc\u8d85\u8868\u5f81\u901f\u5ea6\uff0c\u4e9f\u9700\u9ad8\u6548\u65b9\u6cd5\u786e\u5b9a\u6676\u4f53\u7ed3\u6784\uff0c\u4ee5\u652f\u6301\u9ad8\u901a\u91cf\u7eb3\u7c73\u6750\u6599\u53d1\u73b0\u3002", "method": "\u7ed3\u5408Kikuchi\u884d\u5c04\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u7a7a\u95f4\u7fa4\u5bf9\u79f0\u6027\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u9a8c\u6570\u636e\u9a8c\u8bc1\u3002", "result": "\u795e\u7ecf\u7f51\u7edc\u5728\u6a21\u62df\u548c\u5b9e\u9a8c\u6570\u636e\u4e0a\u7684\u5206\u7c7b\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u8bc1\u660e\u5176\u53ef\u884c\u6027\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408Kikuchi\u884d\u5c04\u6280\u672f\u53ef\u9ad8\u6548\u9884\u6d4b\u6676\u4f53\u5bf9\u79f0\u6027\uff0c\u4e3a\u6750\u6599\u53d1\u73b0\u63d0\u4f9b\u65b0\u5de5\u5177\u3002"}}
{"id": "2504.21276", "pdf": "https://arxiv.org/pdf/2504.21276", "abs": "https://arxiv.org/abs/2504.21276", "authors": ["Wanyi Chen", "Meng-Wen Su", "Mary L. Cummings"], "title": "Assessing LLM code generation quality through path planning tasks", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "As LLM-generated code grows in popularity, more evaluation is needed to\nassess the risks of using such tools, especially for safety-critical\napplications such as path planning. Existing coding benchmarks are insufficient\nas they do not reflect the context and complexity of safety-critical\napplications. To this end, we assessed six LLMs' abilities to generate the code\nfor three different path-planning algorithms and tested them on three maps of\nvarious difficulties. Our results suggest that LLM-generated code presents\nserious hazards for path planning applications and should not be applied in\nsafety-critical contexts without rigorous testing.", "AI": {"tldr": "\u8bc4\u4f30LLM\u751f\u6210\u4ee3\u7801\u5728\u5b89\u5168\u5173\u952e\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u98ce\u9669\uff0c\u53d1\u73b0\u73b0\u6709\u57fa\u51c6\u4e0d\u8db3\uff0c\u6d4b\u8bd5\u516d\u79cdLLM\u751f\u6210\u4e09\u79cd\u7b97\u6cd5\u7684\u4ee3\u7801\uff0c\u7ed3\u679c\u663e\u793a\u5b58\u5728\u4e25\u91cd\u9690\u60a3\u3002", "motivation": "\u968f\u7740LLM\u751f\u6210\u4ee3\u7801\u7684\u666e\u53ca\uff0c\u9700\u8bc4\u4f30\u5176\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u98ce\u9669\uff0c\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u53cd\u6620\u5b9e\u9645\u590d\u6742\u6027\u3002", "method": "\u6d4b\u8bd5\u516d\u79cdLLM\u751f\u6210\u4e09\u79cd\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u7684\u4ee3\u7801\uff0c\u5e76\u5728\u4e09\u79cd\u96be\u5ea6\u5730\u56fe\u4e0a\u9a8c\u8bc1\u3002", "result": "LLM\u751f\u6210\u7684\u4ee3\u7801\u5728\u8def\u5f84\u89c4\u5212\u4e2d\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u9690\u60a3\u3002", "conclusion": "\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u9700\u4e25\u683c\u6d4b\u8bd5LLM\u751f\u6210\u7684\u4ee3\u7801\uff0c\u907f\u514d\u76f4\u63a5\u4f7f\u7528\u3002"}}
{"id": "2504.21242", "pdf": "https://arxiv.org/pdf/2504.21242", "abs": "https://arxiv.org/abs/2504.21242", "authors": ["Samy Abdel-Ghaffar", "Isaac Galatzer-Levy", "Conor Heneghan", "Xin Liu", "Sarah Kernasovskiy", "Brennan Garrett", "Andrew Barakat", "Daniel McDuff"], "title": "Passive Measurement of Autonomic Arousal in Real-World Settings", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "The autonomic nervous system (ANS) is activated during stress, which can have\nnegative effects on cardiovascular health, sleep, the immune system, and mental\nhealth. While there are ways to quantify ANS activity in laboratories, there is\na paucity of methods that have been validated in real-world contexts. We\npresent the Fitbit Body Response Algorithm, an approach to continuous remote\nmeasurement of ANS activation through widely available remote wrist-based\nsensors. The design was validated via two experiments, a Trier Social Stress\nTest (n = 45) and ecological momentary assessments (EMA) of perceived stress\n(n=87), providing both controlled and ecologically valid test data. Model\nperformance predicting perceived stress when using all available sensor\nmodalities was consistent with expectations (accuracy=0.85) and outperformed\nmodels with access to only a subset of the signals. We discuss and address\nchallenges to sensing that arise in real world settings that do not present in\nconventional lab environments.", "AI": {"tldr": "Fitbit Body Response Algorithm\u901a\u8fc7\u8fdc\u7a0b\u624b\u8155\u4f20\u611f\u5668\u8fde\u7eed\u6d4b\u91cf\u81ea\u4e3b\u795e\u7ecf\u7cfb\u7edf\uff08ANS\uff09\u6d3b\u52a8\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5728\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u7f3a\u4e4f\u771f\u5b9e\u73af\u5883\u4e2d\u7684ANS\u6d3b\u52a8\u6d4b\u91cf\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7Trier Social Stress Test\u548c\u751f\u6001\u77ac\u65f6\u8bc4\u4f30\uff08EMA\uff09\u9a8c\u8bc1\u7b97\u6cd5\uff0c\u7ed3\u5408\u591a\u4f20\u611f\u5668\u6570\u636e\u9884\u6d4b\u611f\u77e5\u538b\u529b\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u611f\u77e5\u538b\u529b\u7684\u51c6\u786e\u7387\u4e3a0.85\uff0c\u4f18\u4e8e\u4ec5\u4f7f\u7528\u90e8\u5206\u4fe1\u53f7\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u6709\u6548\uff0c\u5e76\u89e3\u51b3\u4e86\u5b9e\u9a8c\u5ba4\u73af\u5883\u672a\u6d89\u53ca\u7684\u6311\u6218\u3002"}}
{"id": "2504.21432", "pdf": "https://arxiv.org/pdf/2504.21432", "abs": "https://arxiv.org/abs/2504.21432", "authors": ["Pranav Saxena", "Nishant Raghuvanshi", "Neena Goveas"], "title": "UAV-VLN: End-to-End Vision Language guided Navigation for UAVs", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "A core challenge in AI-guided autonomy is enabling agents to navigate\nrealistically and effectively in previously unseen environments based on\nnatural language commands. We propose UAV-VLN, a novel end-to-end\nVision-Language Navigation (VLN) framework for Unmanned Aerial Vehicles (UAVs)\nthat seamlessly integrates Large Language Models (LLMs) with visual perception\nto facilitate human-interactive navigation. Our system interprets free-form\nnatural language instructions, grounds them into visual observations, and plans\nfeasible aerial trajectories in diverse environments.\n  UAV-VLN leverages the common-sense reasoning capabilities of LLMs to parse\nhigh-level semantic goals, while a vision model detects and localizes\nsemantically relevant objects in the environment. By fusing these modalities,\nthe UAV can reason about spatial relationships, disambiguate references in\nhuman instructions, and plan context-aware behaviors with minimal task-specific\nsupervision. To ensure robust and interpretable decision-making, the framework\nincludes a cross-modal grounding mechanism that aligns linguistic intent with\nvisual context.\n  We evaluate UAV-VLN across diverse indoor and outdoor navigation scenarios,\ndemonstrating its ability to generalize to novel instructions and environments\nwith minimal task-specific training. Our results show significant improvements\nin instruction-following accuracy and trajectory efficiency, highlighting the\npotential of LLM-driven vision-language interfaces for safe, intuitive, and\ngeneralizable UAV autonomy.", "AI": {"tldr": "UAV-VLN\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u89c6\u89c9\u611f\u77e5\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\uff08UAV\uff09\u7684\u81ea\u7136\u8bed\u8a00\u5bfc\u822a\uff0c\u80fd\u591f\u89e3\u6790\u81ea\u7531\u5f62\u5f0f\u7684\u6307\u4ee4\u5e76\u89c4\u5212\u53ef\u884c\u8def\u5f84\u3002", "motivation": "\u89e3\u51b3AI\u81ea\u4e3b\u5bfc\u822a\u4e2d\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u6838\u5fc3\u6311\u6218\u3002", "method": "\u6574\u5408LLM\u7684\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\u548c\u89c6\u89c9\u6a21\u578b\u7684\u5bf9\u8c61\u68c0\u6d4b\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u673a\u5236\u5b9e\u73b0\u8bed\u8a00\u610f\u56fe\u4e0e\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684\u878d\u5408\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u5ba4\u5185\u5916\u5bfc\u822a\u573a\u666f\u4e2d\uff0cUAV-VLN\u8868\u73b0\u51fa\u5bf9\u65b0\u6307\u4ee4\u548c\u73af\u5883\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6307\u4ee4\u9075\u5faa\u51c6\u786e\u6027\u548c\u8f68\u8ff9\u6548\u7387\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u89c6\u89c9\u8bed\u8a00\u63a5\u53e3\u4e3a\u65e0\u4eba\u673a\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u76f4\u89c2\u4e14\u53ef\u6cdb\u5316\u7684\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2504.21243", "pdf": "https://arxiv.org/pdf/2504.21243", "abs": "https://arxiv.org/abs/2504.21243", "authors": ["Yuexin Bian", "Yuanyuan Shi"], "title": "Data-driven operator learning for energy-efficient building control", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "Energy-efficient ventilation control plays a vital role in reducing building\nenergy consumption while ensuring occupant health and comfort. While\nComputational Fluid Dynamics (CFD) simulations offer high-fidelity modeling of\nairflow for building HVAC design, their high computational cost makes them\nimpractical for practical adoption in real-time building management system. In\nthis work, we present a data-driven framework that combines the physical\naccuracy of CFD with the computational efficiency of machine learning to enable\nenergy-efficient building ventilation control. Our method jointly optimizes\nairflow supply rates and vent angles to reduce energy use and adhere to air\nquality constraints. We train a neural operator transformer to learn the\nmapping from building control actions to airflow field distributions using\nhigh-resolution CFD data. This learned operator enables a gradient-based\ncontrol framework capable of optimal decision-making. Experimental results\ndemonstrate that our approach achieves substantial energy savings compared to\nmaximum airflow rate control, rule-based control, and data-driven control based\non regional average CO2 predictions, while consistently maintaining safe indoor\nair quality. These results highlight the practicality and scalability of our\nmethod for enabling safe and energy-efficient building management.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CFD\u7269\u7406\u7cbe\u5ea6\u4e0e\u673a\u5668\u5b66\u4e60\u8ba1\u7b97\u6548\u7387\u7684\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u7b51\u901a\u98ce\u63a7\u5236\uff0c\u5b9e\u73b0\u8282\u80fd\u4e0e\u7a7a\u6c14\u8d28\u91cf\u5e73\u8861\u3002", "motivation": "\u4f20\u7edfCFD\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5b9e\u65f6\u5e94\u7528\u4e8e\u5efa\u7b51\u7ba1\u7406\u7cfb\u7edf\uff0c\u9700\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u8bad\u7ec3\u795e\u7ecf\u7b97\u5b50\u53d8\u6362\u5668\uff0c\u5b66\u4e60\u63a7\u5236\u52a8\u4f5c\u5230\u6c14\u6d41\u5206\u5e03\u7684\u6620\u5c04\uff0c\u57fa\u4e8e\u68af\u5ea6\u4f18\u5316\u901a\u98ce\u53c2\u6570\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0c\u663e\u8457\u8282\u80fd\u4e14\u4fdd\u6301\u7a7a\u6c14\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u8282\u80fd\u7684\u5efa\u7b51\u7ba1\u7406\u3002"}}
{"id": "2504.21530", "pdf": "https://arxiv.org/pdf/2504.21530", "abs": "https://arxiv.org/abs/2504.21530", "authors": ["Haifeng Huang", "Xinyi Chen", "Yilun Chen", "Hao Li", "Xiaoshen Han", "Zehan Wang", "Tai Wang", "Jiangmiao Pang", "Zhou Zhao"], "title": "RoboGround: Robotic Manipulation with Grounded Vision-Language Priors", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Recent advancements in robotic manipulation have highlighted the potential of\nintermediate representations for improving policy generalization. In this work,\nwe explore grounding masks as an effective intermediate representation,\nbalancing two key advantages: (1) effective spatial guidance that specifies\ntarget objects and placement areas while also conveying information about\nobject shape and size, and (2) broad generalization potential driven by\nlarge-scale vision-language models pretrained on diverse grounding datasets. We\nintroduce RoboGround, a grounding-aware robotic manipulation system that\nleverages grounding masks as an intermediate representation to guide policy\nnetworks in object manipulation tasks. To further explore and enhance\ngeneralization, we propose an automated pipeline for generating large-scale,\nsimulated data with a diverse set of objects and instructions. Extensive\nexperiments show the value of our dataset and the effectiveness of grounding\nmasks as intermediate guidance, significantly enhancing the generalization\nabilities of robot policies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8egrounding masks\u7684\u4e2d\u95f4\u8868\u793a\u65b9\u6cd5RoboGround\uff0c\u7528\u4e8e\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u751f\u6210\u5927\u89c4\u6a21\u6a21\u62df\u6570\u636e\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u4e2d\u95f4\u8868\u793a\uff08grounding masks\uff09\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faRoboGround\u7cfb\u7edf\uff0c\u5229\u7528grounding masks\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u6307\u5bfc\u7b56\u7565\u7f51\u7edc\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u7ba1\u9053\u751f\u6210\u5927\u89c4\u6a21\u6a21\u62df\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660egrounding masks\u4f5c\u4e3a\u4e2d\u95f4\u6307\u5bfc\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "grounding masks\u662f\u4e00\u79cd\u6709\u6548\u7684\u4e2d\u95f4\u8868\u793a\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2504.21297", "pdf": "https://arxiv.org/pdf/2504.21297", "abs": "https://arxiv.org/abs/2504.21297", "authors": ["Wenjun Yang", "Eyhab Al-Masri"], "title": "Participatory AI, Public Sector AI, Differential Privacy, Conversational Interfaces, Explainable AI, Citizen Engagement in AI", "categories": ["cs.IT", "cs.AI", "cs.CY", "cs.ET", "math.IT"], "comment": null, "summary": "This paper introduces a conversational interface system that enables\nparticipatory design of differentially private AI systems in public sector\napplications. Addressing the challenge of balancing mathematical privacy\nguarantees with democratic accountability, we propose three key contributions:\n(1) an adaptive $\\epsilon$-selection protocol leveraging TOPSIS multi-criteria\ndecision analysis to align citizen preferences with differential privacy (DP)\nparameters, (2) an explainable noise-injection framework featuring real-time\nMean Absolute Error (MAE) visualizations and GPT-4-powered impact analysis, and\n(3) an integrated legal-compliance mechanism that dynamically modulates privacy\nbudgets based on evolving regulatory constraints. Our results advance\nparticipatory AI practices by demonstrating how conversational interfaces can\nenhance public engagement in algorithmic privacy mechanisms, ensuring that\nprivacy-preserving AI in public sector governance remains both mathematically\nrobust and democratically accountable.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2504.21259", "pdf": "https://arxiv.org/pdf/2504.21259", "abs": "https://arxiv.org/abs/2504.21259", "authors": ["S. Chalavadi", "A. Pastor", "T. Leitch"], "title": "LSTM+Geo with xgBoost Filtering: A Novel Approach for Race and Ethnicity Imputation with Reduced Bias", "categories": ["cs.CY", "cs.LG"], "comment": null, "summary": "Accurate imputation of race and ethnicity (R&E) is crucial for analyzing\ndisparities and informing policy. Methods like Bayesian Improved Surname\nGeocoding (BISG) are widely used but exhibit limitations, including systematic\nmisclassification biases linked to socioeconomic status. This paper introduces\nLSTM+Geo, a novel approach enhancing Long Short-Term Memory (LSTM) networks\nwith census tract geolocation information. Using a large voter dataset, we\ndemonstrate that LSTM+Geo (88.7% accuracy) significantly outperforms standalone\nLSTM (86.4%) and Bayesian methods like BISG (82.9%) and BIFSG (86.8%) in\naccuracy and F1-score on a held-out validation set. LSTM+Geo reduces the rate\nat which non-White individuals are misclassified as White (White FPR 19.3%)\ncompared to name-only LSTMs (White FPR 24.6%). While sophisticated ensemble\nmethods incorporating XGBoost achieve the highest overall accuracy (up to\n89.4%) and lowest White FPR (17.8%), LSTM+Geo offers strong standalone\nperformance with improved bias characteristics compared to baseline models.\nIntegrating LSTM+Geo into an XGBoost ensemble further boosts accuracy,\nhighlighting its utility as both a standalone model and a component for\nadvanced systems. We give a caution at the end regarding the appropriate use of\nthese methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLSTM+Geo\u65b9\u6cd5\uff0c\u7ed3\u5408LSTM\u7f51\u7edc\u548c\u5730\u7406\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u79cd\u65cf\u548c\u6c11\u65cf\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u51c6\u786e\u5206\u7c7b\u79cd\u65cf\u548c\u6c11\u65cf\u5bf9\u5206\u6790\u5dee\u5f02\u548c\u5236\u5b9a\u653f\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5982BISG\uff09\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\u3002", "method": "\u5f15\u5165LSTM+Geo\uff0c\u7ed3\u5408LSTM\u7f51\u7edc\u548c\u4eba\u53e3\u666e\u67e5\u5730\u7406\u4fe1\u606f\uff0c\u4f7f\u7528\u5927\u578b\u9009\u6c11\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "LSTM+Geo\u51c6\u786e\u7387\u8fbe88.7%\uff0c\u4f18\u4e8eLSTM\uff0886.4%\uff09\u548c\u8d1d\u53f6\u65af\u65b9\u6cd5\uff08BISG 82.9%\uff0cBIFSG 86.8%\uff09\uff0c\u5e76\u51cf\u5c11\u975e\u767d\u4eba\u88ab\u8bef\u5206\u7c7b\u4e3a\u767d\u4eba\u7684\u6bd4\u4f8b\u3002", "conclusion": "LSTM+Geo\u4f5c\u4e3a\u72ec\u7acb\u6a21\u578b\u6216\u96c6\u6210\u7ec4\u4ef6\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9700\u8c28\u614e\u4f7f\u7528\u3002"}}
{"id": "2504.21730", "pdf": "https://arxiv.org/pdf/2504.21730", "abs": "https://arxiv.org/abs/2504.21730", "authors": ["Ting Qiao", "Yingjia Wang", "Xing Liu", "Sixing Wu", "Jianbing Li", "Yiming Li"], "title": "Cert-SSB: Toward Certified Sample-Specific Backdoor Defense", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "comment": "15 pages", "summary": "Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an\nattacker manipulates a small portion of the training data to implant hidden\nbackdoors into the model. The compromised model behaves normally on clean\nsamples but misclassifies backdoored samples into the attacker-specified target\nclass, posing a significant threat to real-world DNN applications. Currently,\nseveral empirical defense methods have been proposed to mitigate backdoor\nattacks, but they are often bypassed by more advanced backdoor techniques. In\ncontrast, certified defenses based on randomized smoothing have shown promise\nby adding random noise to training and testing samples to counteract backdoor\nattacks. In this paper, we reveal that existing randomized smoothing defenses\nimplicitly assume that all samples are equidistant from the decision boundary.\nHowever, it may not hold in practice, leading to suboptimal certification\nperformance. To address this issue, we propose a sample-specific certified\nbackdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic\ngradient ascent to optimize the noise magnitude for each sample, ensuring a\nsample-specific noise level that is then applied to multiple poisoned training\nsets to retrain several smoothed models. After that, Cert-SSB aggregates the\npredictions of multiple smoothed models to generate the final robust\nprediction. In particular, in this case, existing certification methods become\ninapplicable since the optimized noise varies across different samples. To\nconquer this challenge, we introduce a storage-update-based certification\nmethod, which dynamically adjusts each sample's certification region to improve\ncertification performance. We conduct extensive experiments on multiple\nbenchmark datasets, demonstrating the effectiveness of our proposed method. Our\ncode is available at https://github.com/NcepuQiaoTing/Cert-SSB.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6837\u672c\u7279\u5b9a\u7684\u8ba4\u8bc1\u540e\u95e8\u9632\u5fa1\u65b9\u6cd5Cert-SSB\uff0c\u901a\u8fc7\u4f18\u5316\u6bcf\u4e2a\u6837\u672c\u7684\u566a\u58f0\u6c34\u5e73\uff0c\u5e76\u7ed3\u5408\u591a\u4e2a\u5e73\u6ed1\u6a21\u578b\u7684\u9884\u6d4b\uff0c\u63d0\u9ad8\u4e86\u5bf9\u540e\u95e8\u653b\u51fb\u7684\u9632\u5fa1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u968f\u673a\u5e73\u6ed1\u9632\u5fa1\u65b9\u6cd5\u5047\u8bbe\u6240\u6709\u6837\u672c\u4e0e\u51b3\u7b56\u8fb9\u754c\u7b49\u8ddd\uff0c\u4f46\u5b9e\u9645\u4e2d\u8fd9\u4e00\u5047\u8bbe\u4e0d\u6210\u7acb\uff0c\u5bfc\u81f4\u9632\u5fa1\u6027\u80fd\u4e0d\u4f73\u3002", "method": "Cert-SSB\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u4e0a\u5347\u4f18\u5316\u6bcf\u4e2a\u6837\u672c\u7684\u566a\u58f0\u6c34\u5e73\uff0c\u8bad\u7ec3\u591a\u4e2a\u5e73\u6ed1\u6a21\u578b\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u8ba4\u8bc1\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCert-SSB\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6709\u6548\u63d0\u5347\u4e86\u9632\u5fa1\u6027\u80fd\u3002", "conclusion": "Cert-SSB\u901a\u8fc7\u6837\u672c\u7279\u5b9a\u7684\u566a\u58f0\u4f18\u5316\u548c\u52a8\u6001\u8ba4\u8bc1\u65b9\u6cd5\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u540e\u95e8\u653b\u51fb\u7684\u9632\u5fa1\u6548\u679c\u3002"}}
{"id": "2504.21323", "pdf": "https://arxiv.org/pdf/2504.21323", "abs": "https://arxiv.org/abs/2504.21323", "authors": ["Chen Wu", "Qian Ma", "Prasenjit Mitra", "Sencun Zhu"], "title": "How to Backdoor the Knowledge Distillation", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Knowledge distillation has become a cornerstone in modern machine learning\nsystems, celebrated for its ability to transfer knowledge from a large, complex\nteacher model to a more efficient student model. Traditionally, this process is\nregarded as secure, assuming the teacher model is clean. This belief stems from\nconventional backdoor attacks relying on poisoned training data with backdoor\ntriggers and attacker-chosen labels, which are not involved in the distillation\nprocess. Instead, knowledge distillation uses the outputs of a clean teacher\nmodel to guide the student model, inherently preventing recognition or response\nto backdoor triggers as intended by an attacker. In this paper, we challenge\nthis assumption by introducing a novel attack methodology that strategically\npoisons the distillation dataset with adversarial examples embedded with\nbackdoor triggers. This technique allows for the stealthy compromise of the\nstudent model while maintaining the integrity of the teacher model. Our\ninnovative approach represents the first successful exploitation of\nvulnerabilities within the knowledge distillation process using clean teacher\nmodels. Through extensive experiments conducted across various datasets and\nattack settings, we demonstrate the robustness, stealthiness, and effectiveness\nof our method. Our findings reveal previously unrecognized vulnerabilities and\npave the way for future research aimed at securing knowledge distillation\nprocesses against backdoor attacks.", "AI": {"tldr": "\u8bba\u6587\u6311\u6218\u4e86\u77e5\u8bc6\u84b8\u998f\u7684\u4f20\u7edf\u5b89\u5168\u5047\u8bbe\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u6837\u672c\u9690\u853d\u5730\u690d\u5165\u540e\u95e8\u89e6\u53d1\u5668\uff0c\u6210\u529f\u5229\u7528\u5e72\u51c0\u6559\u5e08\u6a21\u578b\u4e2d\u7684\u6f0f\u6d1e\u3002", "motivation": "\u4f20\u7edf\u8ba4\u4e3a\u77e5\u8bc6\u84b8\u998f\u662f\u5b89\u5168\u7684\uff0c\u56e0\u4e3a\u6559\u5e08\u6a21\u578b\u5e72\u51c0\u4e14\u4e0d\u6d89\u53ca\u540e\u95e8\u653b\u51fb\u7684\u6570\u636e\u3002\u672c\u6587\u8d28\u7591\u8fd9\u4e00\u5047\u8bbe\uff0c\u63a2\u7d22\u77e5\u8bc6\u84b8\u998f\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u84b8\u998f\u6570\u636e\u96c6\u4e2d\u690d\u5165\u5bf9\u6297\u6837\u672c\u548c\u540e\u95e8\u89e6\u53d1\u5668\uff0c\u9690\u853d\u5730\u7834\u574f\u5b66\u751f\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u9c81\u68d2\u6027\u3001\u9690\u853d\u6027\u548c\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u77e5\u8bc6\u84b8\u998f\u4e2d\u672a\u88ab\u8bc6\u522b\u7684\u6f0f\u6d1e\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u4fdd\u62a4\u77e5\u8bc6\u84b8\u998f\u514d\u53d7\u540e\u95e8\u653b\u51fb\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.21260", "pdf": "https://arxiv.org/pdf/2504.21260", "abs": "https://arxiv.org/abs/2504.21260", "authors": ["Daniel Glover", "Parikshit Pareek", "Deepjyoti Deka", "Anamika Dubey"], "title": "Power Flow Approximations for Multiphase Distribution Networks using Gaussian Processes", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": "5 pages, 7 figures, Accepted at 2025 IEEE PES General Meeting", "summary": "Learning-based approaches are increasingly leveraged to manage and coordinate\nthe operation of grid-edge resources in active power distribution networks.\nAmong these, model-based techniques stand out for their superior data\nefficiency and robustness compared to model-free methods. However, effective\nmodel learning requires a learning-based approximator for the underlying power\nflow model. This study extends existing work by introducing a data-driven power\nflow method based on Gaussian Processes (GPs) to approximate the multiphase\npower flow model, by mapping net load injections to nodal voltages. Simulation\nresults using the IEEE 123-bus and 8500-node distribution test feeders\ndemonstrate that the trained GP model can reliably predict the nonlinear power\nflow solutions with minimal training data. We also conduct a comparative\nanalysis of the training efficiency and testing performance of the proposed\nGP-based power flow approximator against a deep neural network-based\napproximator, highlighting the advantages of our data-efficient approach.\nResults over realistic operating conditions show that despite an 85% reduction\nin the training sample size (corresponding to a 92.8% improvement in training\ntime), GP models produce a 99.9% relative reduction in mean absolute error\ncompared to the baselines of deep neural networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\uff08GPs\uff09\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fd1\u4f3c\u591a\u76f8\u7535\u529b\u6f6e\u6d41\u6a21\u578b\uff0c\u901a\u8fc7\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u53ef\u9760\u9884\u6d4b\u975e\u7ebf\u6027\u7535\u529b\u6f6e\u6d41\u89e3\uff0c\u5e76\u5728\u8bad\u7ec3\u6548\u7387\u548c\u6d4b\u8bd5\u6027\u80fd\u4e0a\u4f18\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u3002", "motivation": "\u6a21\u578b\u5b66\u4e60\u9700\u8981\u9ad8\u6548\u7684\u7535\u529b\u6f6e\u6d41\u6a21\u578b\u8fd1\u4f3c\u5668\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u6548\u7387\u548c\u9c81\u68d2\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u8fc7\u7a0b\uff08GPs\uff09\u6620\u5c04\u51c0\u8d1f\u8f7d\u6ce8\u5165\u5230\u8282\u70b9\u7535\u538b\uff0c\u8fd1\u4f3c\u591a\u76f8\u7535\u529b\u6f6e\u6d41\u6a21\u578b\u3002", "result": "\u5728IEEE 123\u603b\u7ebf\u548c8500\u8282\u70b9\u6d4b\u8bd5\u4e2d\uff0cGP\u6a21\u578b\u4ee5\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u9884\u6d4b\uff0c\u8bad\u7ec3\u6837\u672c\u51cf\u5c1185%\u65f6\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u964d\u4f4e99.9%\u3002", "conclusion": "GP\u65b9\u6cd5\u5728\u6570\u636e\u6548\u7387\u548c\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u9002\u7528\u4e8e\u7535\u529b\u5206\u914d\u7f51\u7edc\u7684\u8d44\u6e90\u7ba1\u7406\u3002"}}
{"id": "2504.21731", "pdf": "https://arxiv.org/pdf/2504.21731", "abs": "https://arxiv.org/abs/2504.21731", "authors": ["Feiyu Lu", "Mengyu Chen", "Hsiang Hsu", "Pranav Deshpande", "Cheng Yao Wang", "Blair MacIntyre"], "title": "Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "In Extended Abstracts of the CHI Conference on Human Factors in\n  Computing Systems (CHI EA '24)", "summary": "Mixed Reality (MR) could assist users' tasks by continuously integrating\nvirtual content with their view of the physical environment. However, where and\nhow to place these content to best support the users has been a challenging\nproblem due to the dynamic nature of MR experiences. In contrast to prior work\nthat investigates optimization-based methods, we are exploring how\nreinforcement learning (RL) could assist with continuous 3D content placement\nthat is aware of users' poses and their surrounding environments. Through an\ninitial exploration and preliminary evaluation, our results demonstrate the\npotential of RL to position content that maximizes the reward for users on the\ngo. We further identify future directions for research that could harness the\npower of RL for personalized and optimized UI and content placement in MR.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u6df7\u5408\u73b0\u5b9e\uff08MR\uff09\u4e2d\u52a8\u6001\u4f18\u53163D\u5185\u5bb9\u5e03\u5c40\uff0c\u4ee5\u9002\u5e94\u7528\u6237\u59ff\u6001\u548c\u73af\u5883\u53d8\u5316\u3002", "motivation": "MR\u4e2d\u865a\u62df\u5185\u5bb9\u7684\u52a8\u6001\u5e03\u5c40\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u5b9e\u65f6\u53d8\u5316\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u7528\u6237\u59ff\u6001\u548c\u73af\u5883\u4fe1\u606f\uff0c\u5b9e\u73b0\u8fde\u7eed3D\u5185\u5bb9\u5e03\u5c40\u4f18\u5316\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0cRL\u80fd\u6709\u6548\u4f18\u5316\u5185\u5bb9\u5e03\u5c40\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "RL\u5728MR\u5185\u5bb9\u5e03\u5c40\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u4e2a\u6027\u5316\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2504.21778", "pdf": "https://arxiv.org/pdf/2504.21778", "abs": "https://arxiv.org/abs/2504.21778", "authors": ["Ayman A. Ameen", "Thomas Richter", "Andr\u00e9 Kaup"], "title": "LoC-LIC: Low Complexity Learned Image Coding Using Hierarchical Feature Transforms", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Current learned image compression models typically exhibit high complexity,\nwhich demands significant computational resources. To overcome these\nchallenges, we propose an innovative approach that employs hierarchical feature\nextraction transforms to significantly reduce complexity while preserving bit\nrate reduction efficiency. Our novel architecture achieves this by using fewer\nchannels for high spatial resolution inputs/feature maps. On the other hand,\nfeature maps with a large number of channels have reduced spatial dimensions,\nthereby cutting down on computational load without sacrificing performance.\nThis strategy effectively reduces the forward pass complexity from \\(1256 \\,\n\\text{kMAC/Pixel}\\) to just \\(270 \\, \\text{kMAC/Pixel}\\). As a result, the\nreduced complexity model can open the way for learned image compression models\nto operate efficiently across various devices and pave the way for the\ndevelopment of new architectures in image compression technology.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5206\u5c42\u7279\u5f81\u63d0\u53d6\u964d\u4f4e\u590d\u6742\u5ea6\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u5f53\u524d\u5b66\u4e60\u7684\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u590d\u6742\u5ea6\u9ad8\uff0c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u5927\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7279\u5f81\u63d0\u53d6\u53d8\u6362\uff0c\u51cf\u5c11\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u8f93\u5165/\u7279\u5f81\u56fe\u7684\u901a\u9053\u6570\uff0c\u540c\u65f6\u964d\u4f4e\u5927\u901a\u9053\u6570\u7279\u5f81\u56fe\u7684\u7a7a\u95f4\u7ef4\u5ea6\u3002", "result": "\u590d\u6742\u5ea6\u4ece1256 kMAC/Pixel\u964d\u81f3270 kMAC/Pixel\uff0c\u6027\u80fd\u672a\u53d7\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u5728\u591a\u79cd\u8bbe\u5907\u4e0a\u9ad8\u6548\u8fd0\u884c\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u5e76\u63a8\u52a8\u4e86\u65b0\u6280\u672f\u67b6\u6784\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.21317", "pdf": "https://arxiv.org/pdf/2504.21317", "abs": "https://arxiv.org/abs/2504.21317", "authors": ["Jiarui Xie", "Yaoyao Fiona Zhao"], "title": "Redundancy Analysis and Mitigation for Machine Learning-Based Process Monitoring of Additive Manufacturing", "categories": ["cs.CE", "cs.LG", "eess.SP"], "comment": "13 pages, 5 figures, 2 tables. Accepted by IDETC-CIE 2025", "summary": "The deployment of machine learning (ML)-based process monitoring systems has\nsignificantly advanced additive manufacturing (AM) by enabling real-time defect\ndetection, quality assessment, and process optimization. However, redundancy is\na critical yet often overlooked challenge in the deployment and operation of\nML-based AM process monitoring systems. Excessive redundancy leads to increased\nequipment costs, compromised model performance, and high computational\nrequirements, posing barriers to industrial adoption. However, existing\nresearch lacks a unified definition of redundancy and a systematic framework\nfor its evaluation and mitigation. This paper defines redundancy in ML-based AM\nprocess monitoring and categorizes it into sample-level, feature-level, and\nmodel-level redundancy. A comprehensive multi-level redundancy mitigation\n(MLRM) framework is proposed, incorporating advanced methods such as data\nregistration, downscaling, cross-modality knowledge transfer, and model pruning\nto systematically reduce redundancy while improving model performance. The\nframework is validated through an ML-based in-situ defect detection case study\nfor directed energy deposition (DED), demonstrating a 91% reduction in latency,\na 47% decrease in error rate, and a 99.4% reduction in storage requirements.\nAdditionally, the proposed approach lowers sensor costs and energy consumption,\nenabling a lightweight, cost-effective, and scalable monitoring system. By\ndefining redundancy and introducing a structured mitigation framework, this\nstudy establishes redundancy analysis and mitigation as a key enabler of\nefficient ML-based process monitoring in production environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ea7\u5197\u4f59\u7f13\u89e3\uff08MLRM\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u589e\u6750\u5236\u9020\u8fc7\u7a0b\u76d1\u63a7\u7cfb\u7edf\u4e2d\u7684\u5197\u4f59\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u5197\u4f59\u7684\u7edf\u4e00\u5b9a\u4e49\u548c\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5bfc\u81f4\u8bbe\u5907\u6210\u672c\u9ad8\u3001\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u548c\u8ba1\u7b97\u9700\u6c42\u5927\uff0c\u963b\u788d\u4e86\u5de5\u4e1a\u5e94\u7528\u3002", "method": "\u5b9a\u4e49\u4e86\u6837\u672c\u7ea7\u3001\u7279\u5f81\u7ea7\u548c\u6a21\u578b\u7ea7\u5197\u4f59\uff0c\u5e76\u63d0\u51faMLRM\u6846\u67b6\uff0c\u7ed3\u5408\u6570\u636e\u6ce8\u518c\u3001\u964d\u7ef4\u3001\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\u548c\u6a21\u578b\u526a\u679d\u7b49\u65b9\u6cd5\u3002", "result": "\u5728\u5b9a\u5411\u80fd\u91cf\u6c89\u79ef\uff08DED\uff09\u6848\u4f8b\u4e2d\uff0c\u5ef6\u8fdf\u51cf\u5c1191%\uff0c\u9519\u8bef\u7387\u964d\u4f4e47%\uff0c\u5b58\u50a8\u9700\u6c42\u51cf\u5c1199.4%\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u4f20\u611f\u5668\u6210\u672c\u548c\u80fd\u8017\u3002", "conclusion": "\u901a\u8fc7\u5b9a\u4e49\u5197\u4f59\u5e76\u63d0\u51fa\u7cfb\u7edf\u7f13\u89e3\u6846\u67b6\uff0c\u8be5\u7814\u7a76\u4e3a\u9ad8\u6548\u673a\u5668\u5b66\u4e60\u76d1\u63a7\u7cfb\u7edf\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5173\u952e\u652f\u6301\u3002"}}
{"id": "2504.21338", "pdf": "https://arxiv.org/pdf/2504.21338", "abs": "https://arxiv.org/abs/2504.21338", "authors": ["Aoi Kato", "Kenta Kojima", "Masahiro Nomura", "Isao Ono"], "title": "A Memetic Algorithm based on Variational Autoencoder for Black-Box Discrete Optimization with Epistasis among Parameters", "categories": ["cs.NE", "cs.LG"], "comment": "IEEE CEC 2025 (Poster)", "summary": "Black-box discrete optimization (BB-DO) problems arise in many real-world\napplications, such as neural architecture search and mathematical model\nestimation. A key challenge in BB-DO is epistasis among parameters where\nmultiple variables must be modified simultaneously to effectively improve the\nobjective function. Estimation of Distribution Algorithms (EDAs) provide a\npowerful framework for tackling BB-DO problems. In particular, an EDA\nleveraging a Variational Autoencoder (VAE) has demonstrated strong performance\non relatively low-dimensional problems with epistasis while reducing\ncomputational cost. Meanwhile, evolutionary algorithms such as DSMGA-II and P3,\nwhich integrate bit-flip-based local search with linkage learning, have shown\nexcellent performance on high-dimensional problems. In this study, we propose a\nnew memetic algorithm that combines VAE-based sampling with local search. The\nproposed method inherits the strengths of both VAE-based EDAs and local\nsearch-based approaches: it effectively handles high-dimensional problems with\nepistasis among parameters without incurring excessive computational overhead.\nExperiments on NK landscapes -- a challenging benchmark for BB-DO involving\nepistasis among parameters -- demonstrate that our method outperforms\nstate-of-the-art VAE-based EDA methods, as well as leading approaches such as\nP3 and DSMGA-II.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408VAE\u91c7\u6837\u4e0e\u5c40\u90e8\u641c\u7d22\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u7ef4\u9ed1\u76d2\u79bb\u6563\u4f18\u5316\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u9ed1\u76d2\u79bb\u6563\u4f18\u5316\u4e2d\u53c2\u6570\u95f4\u76f8\u4e92\u4f9d\u8d56\uff08epistasis\uff09\u7684\u6311\u6218\uff0c\u7ed3\u5408VAE\u548c\u5c40\u90e8\u641c\u7d22\u7684\u4f18\u52bf\u3002", "method": "\u7ed3\u5408VAE\u91c7\u6837\u4e0e\u5c40\u90e8\u641c\u7d22\u7684\u6a21\u56e0\u7b97\u6cd5\uff0c\u7ee7\u627fVAE-EDA\u548c\u5c40\u90e8\u641c\u7d22\u7684\u4f18\u70b9\u3002", "result": "\u5728NK landscapes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709VAE-EDA\u65b9\u6cd5\u53caP3\u3001DSMGA-II\u7b49\u9886\u5148\u65b9\u6cd5\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u6709\u6548\u5904\u7406\u9ad8\u7ef4\u95ee\u9898\uff0c\u8ba1\u7b97\u5f00\u9500\u4f4e\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2504.21366", "pdf": "https://arxiv.org/pdf/2504.21366", "abs": "https://arxiv.org/abs/2504.21366", "authors": ["Yinfeng Yu", "Shiyu Sun"], "title": "DGFNet: End-to-End Audio-Visual Source Separation Based on Dynamic Gating Fusion", "categories": ["cs.SD", "cs.AI"], "comment": "Main paper (9 pages). Accepted for publication by ICMR(International\n  Conference on Multimedia Retrieval) 2025", "summary": "Current Audio-Visual Source Separation methods primarily adopt two design\nstrategies. The first strategy involves fusing audio and visual features at the\nbottleneck layer of the encoder, followed by processing the fused features\nthrough the decoder. However, when there is a significant disparity between the\ntwo modalities, this approach may lead to the loss of critical information. The\nsecond strategy avoids direct fusion and instead relies on the decoder to\nhandle the interaction between audio and visual features. Nonetheless, if the\nencoder fails to integrate information across modalities adequately, the\ndecoder may be unable to effectively capture the complex relationships between\nthem. To address these issues, this paper proposes a dynamic fusion method\nbased on a gating mechanism that dynamically adjusts the modality fusion\ndegree. This approach mitigates the limitations of solely relying on the\ndecoder and facilitates efficient collaboration between audio and visual\nfeatures. Additionally, an audio attention module is introduced to enhance the\nexpressive capacity of audio features, thereby further improving model\nperformance. Experimental results demonstrate that our method achieves\nsignificant performance improvements on two benchmark datasets, validating its\neffectiveness and advantages in Audio-Visual Source Separation tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u95e8\u63a7\u673a\u5236\u7684\u52a8\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u97f3\u9891-\u89c6\u89c9\u6e90\u5206\u79bb\u4e2d\u6a21\u6001\u878d\u5408\u4e0d\u8db3\u6216\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u97f3\u9891\u6ce8\u610f\u529b\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u97f3\u9891\u548c\u89c6\u89c9\u7279\u5f81\u878d\u5408\u65f6\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u6216\u6a21\u6001\u4ea4\u4e92\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u52a8\u6001\u95e8\u63a7\u673a\u5236\u8c03\u6574\u6a21\u6001\u878d\u5408\u7a0b\u5ea6\uff0c\u5e76\u5f15\u5165\u97f3\u9891\u6ce8\u610f\u529b\u6a21\u5757\u589e\u5f3a\u97f3\u9891\u7279\u5f81\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u52a8\u6001\u878d\u5408\u65b9\u6cd5\u548c\u97f3\u9891\u6ce8\u610f\u529b\u6a21\u5757\u6709\u6548\u63d0\u5347\u4e86\u97f3\u9891-\u89c6\u89c9\u6e90\u5206\u79bb\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2504.21411", "pdf": "https://arxiv.org/pdf/2504.21411", "abs": "https://arxiv.org/abs/2504.21411", "authors": ["Xinyi Liu", "Yujie Wang", "Shenhan Zhu", "Fangcheng Fu", "Qingshuo Liu", "Guangming Lin", "Bin Cui"], "title": "Galvatron: An Automatic Distributed System for Efficient Foundation Model Training", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": null, "summary": "Galvatron is a distributed system for efficiently training large-scale\nFoundation Models. It overcomes the complexities of selecting optimal\nparallelism strategies by automatically identifying the most efficient hybrid\nstrategy, incorporating data, tensor, pipeline, sharded data, and sequence\nparallelism, along with recomputation. The system's architecture includes a\nprofiler for hardware and model analysis, a search engine for strategy\noptimization using decision trees and dynamic programming, and a runtime for\nexecuting these strategies efficiently. Benchmarking on various clusters\ndemonstrates Galvatron's superior throughput compared to existing frameworks.\nThis open-source system offers user-friendly interfaces and comprehensive\ndocumentation, making complex distributed training accessible and efficient.\nThe source code of Galvatron is available at\nhttps://github.com/PKU-DAIR/Hetu-Galvatron.", "AI": {"tldr": "Galvatron\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u6548\u8bad\u7ec3\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u52a8\u9009\u62e9\u6700\u4f18\u5e76\u884c\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6846\u67b6\u5728\u8bad\u7ec3\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u65f6\u5e76\u884c\u7b56\u7565\u9009\u62e9\u7684\u590d\u6742\u6027\u548c\u6548\u7387\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u67b6\u6784\u5305\u62ec\u786c\u4ef6\u548c\u6a21\u578b\u5206\u6790\u5668\u3001\u57fa\u4e8e\u51b3\u7b56\u6811\u548c\u52a8\u6001\u7f16\u7a0b\u7684\u7b56\u7565\u4f18\u5316\u641c\u7d22\u5f15\u64ce\uff0c\u4ee5\u53ca\u9ad8\u6548\u6267\u884c\u7b56\u7565\u7684\u8fd0\u884c\u65f6\u3002", "result": "\u5728\u591a\u79cd\u96c6\u7fa4\u4e0a\u6d4b\u8bd5\u663e\u793a\uff0cGalvatron\u7684\u541e\u5410\u91cf\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\u3002", "conclusion": "Galvatron\u901a\u8fc7\u5f00\u6e90\u3001\u7528\u6237\u53cb\u597d\u7684\u63a5\u53e3\u548c\u6587\u6863\uff0c\u4f7f\u590d\u6742\u5206\u5e03\u5f0f\u8bad\u7ec3\u66f4\u6613\u7528\u9ad8\u6548\u3002"}}
{"id": "2504.21419", "pdf": "https://arxiv.org/pdf/2504.21419", "abs": "https://arxiv.org/abs/2504.21419", "authors": ["Damir Filipovic", "Paul Schneider"], "title": "Kernel Density Machines", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH", "62G07, 65D05, 65D15, 65C60, 62G10, 62G20"], "comment": null, "summary": "We introduce kernel density machines (KDM), a novel density ratio estimator\nin a reproducing kernel Hilbert space setting. KDM applies to general\nprobability measures on countably generated measurable spaces without\nrestrictive assumptions on continuity, or the existence of a Lebesgue density.\nFor computational efficiency, we incorporate a low-rank approximation with\nprecisely controlled error that grants scalability to large-sample settings. We\nprovide rigorous theoretical guarantees, including asymptotic consistency, a\nfunctional central limit theorem, and finite-sample error bounds, establishing\na strong foundation for practical use. Empirical results based on simulated and\nreal data demonstrate the efficacy and precision of KDM.", "AI": {"tldr": "Kernel density machines (KDM) \u662f\u4e00\u79cd\u65b0\u9896\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u5668\uff0c\u9002\u7528\u4e8e\u53ef\u6570\u751f\u6210\u53ef\u6d4b\u7a7a\u95f4\u4e0a\u7684\u6982\u7387\u6d4b\u5ea6\uff0c\u65e0\u9700\u8fde\u7eed\u6027\u5047\u8bbe\u6216Lebesgue\u5bc6\u5ea6\u5b58\u5728\u3002\u901a\u8fc7\u4f4e\u79e9\u8fd1\u4f3c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5177\u6709\u4e25\u683c\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u4e25\u683c\u5047\u8bbe\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u6982\u7387\u6d4b\u5ea6\u3002", "method": "\u5728\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u4f7f\u7528\u4f4e\u79e9\u8fd1\u4f3c\uff0c\u63a7\u5236\u8bef\u5dee\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u7406\u8bba\u4fdd\u8bc1\u5305\u62ec\u6e10\u8fd1\u4e00\u81f4\u6027\u3001\u6cdb\u51fd\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u548c\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\uff0c\u5b9e\u8bc1\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u7cbe\u5ea6\u3002", "conclusion": "KDM \u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7406\u8bba\u575a\u5b9e\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u3002"}}
{"id": "2504.21438", "pdf": "https://arxiv.org/pdf/2504.21438", "abs": "https://arxiv.org/abs/2504.21438", "authors": ["St\u00e9phane Lhaut", "Holger Rootz\u00e9n", "Johan Segers"], "title": "Wasserstein-Aitchison GAN for angular measures of multivariate extremes", "categories": ["stat.ML", "cs.LG"], "comment": "38 pages, 11 figures", "summary": "Economically responsible mitigation of multivariate extreme risks -- extreme\nrainfall in a large area, huge variations of many stock prices, widespread\nbreakdowns in transportation systems -- requires estimates of the probabilities\nthat such risks will materialize in the future. This paper develops a new\nmethod, Wasserstein--Aitchison Generative Adversarial Networks (WA-GAN), which\nprovides simulated values of future $d$-dimensional multivariate extreme events\nand which hence can be used to give estimates of such probabilities. The main\nhypothesis is that, after transforming the observations to the unit-Pareto\nscale, their distribution is regularly varying in the sense that the\ndistributions of their radial and angular components (with respect to the\n$L_1$-norm) converge and become asymptotically independent as the radius gets\nlarge. The method is a combination of standard extreme value analysis modeling\nof the tails of the marginal distributions with nonparametric GAN modeling of\nthe angular distribution. For the latter, the angular values are transformed to\nAitchison coordinates in a full $(d-1)$-dimensional linear space, and a\nWasserstein GAN is trained on these coordinates and used to generate new\nvalues. A reverse transformation is then applied to these values and gives\nsimulated values on the original data scale. The method shows good performance\ncompared to other existing methods in the literature, both in terms of\ncapturing the dependence structure of the extremes in the data, as well as in\ngenerating accurate new extremes of the data distribution. The comparison is\nperformed on simulated multivariate extremes from a logistic model in\ndimensions up to 50 and on a 30-dimensional financial data set.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWA-GAN\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6a21\u62df\u591a\u7ef4\u6781\u7aef\u4e8b\u4ef6\uff0c\u5e76\u4f30\u8ba1\u5176\u53d1\u751f\u6982\u7387\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u6781\u503c\u5206\u6790\u4e0e\u975e\u53c2\u6570GAN\u5efa\u6a21\uff0c\u5728\u6a21\u62df\u548c\u91d1\u878d\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7ecf\u6d4e\u4e0a\u8d1f\u8d23\u4efb\u5730\u7f13\u89e3\u591a\u7ef4\u6781\u7aef\u98ce\u9669\uff08\u5982\u6781\u7aef\u964d\u96e8\u3001\u80a1\u7968\u4ef7\u683c\u5927\u5e45\u6ce2\u52a8\u7b49\uff09\u9700\u8981\u4f30\u8ba1\u8fd9\u4e9b\u98ce\u9669\u672a\u6765\u53d1\u751f\u7684\u6982\u7387\u3002", "method": "\u5c06\u89c2\u6d4b\u503c\u8f6c\u6362\u4e3a\u5355\u4f4d\u5e15\u7d2f\u6258\u5c3a\u5ea6\uff0c\u5047\u8bbe\u5176\u5206\u5e03\u662f\u89c4\u5219\u53d8\u5316\u7684\uff0c\u7ed3\u5408\u6781\u503c\u5206\u6790\u548c\u975e\u53c2\u6570GAN\u5efa\u6a21\uff0c\u4f7f\u7528Wasserstein GAN\u751f\u6210\u65b0\u7684\u6781\u7aef\u4e8b\u4ef6\u6a21\u62df\u503c\u3002", "result": "WA-GAN\u5728\u6355\u6349\u6781\u7aef\u6570\u636e\u4f9d\u8d56\u7ed3\u6784\u548c\u751f\u6210\u51c6\u786e\u65b0\u6781\u7aef\u503c\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9ad8\u8fbe50\u7ef4\u7684\u6a21\u62df\u6570\u636e\u548c30\u7ef4\u91d1\u878d\u6570\u636e\u3002", "conclusion": "WA-GAN\u4e3a\u591a\u7ef4\u6781\u7aef\u4e8b\u4ef6\u7684\u6982\u7387\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b0\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.21415", "pdf": "https://arxiv.org/pdf/2504.21415", "abs": "https://arxiv.org/abs/2504.21415", "authors": ["Yi Wang", "Chengyv Wu", "Yang Liao", "Maowei You"], "title": "Optimizing Mouse Dynamics for User Authentication by Machine Learning: Addressing Data Sufficiency, Accuracy-Practicality Trade-off, and Model Performance Challenges", "categories": ["cs.CR", "cs.AI"], "comment": "14pages, 10 figures", "summary": "User authentication is essential to ensure secure access to computer systems,\nyet traditional methods face limitations in usability, cost, and security.\nMouse dynamics authentication, based on the analysis of users' natural\ninteraction behaviors with mouse devices, offers a cost-effective,\nnon-intrusive, and adaptable solution. However, challenges remain in\ndetermining the optimal data volume, balancing accuracy and practicality, and\neffectively capturing temporal behavioral patterns. In this study, we propose a\nstatistical method using Gaussian kernel density estimate (KDE) and\nKullback-Leibler (KL) divergence to estimate the sufficient data volume for\ntraining authentication models. We introduce the Mouse Authentication Unit\n(MAU), leveraging Approximate Entropy (ApEn) to optimize segment length for\nefficient and accurate behavioral representation. Furthermore, we design the\nLocal-Time Mouse Authentication (LT-AMouse) framework, integrating 1D-ResNet\nfor local feature extraction and GRU for modeling long-term temporal\ndependencies. Taking the Balabit and DFL datasets as examples, we significantly\nreduced the data scale, particularly by a factor of 10 for the DFL dataset,\ngreatly alleviating the training burden. Additionally, we determined the\noptimal input recognition unit length for the user authentication system on\ndifferent datasets based on the slope of Approximate Entropy. Training with\nimbalanced samples, our model achieved a successful defense AUC 98.52% for\nblind attack on the DFL dataset and 94.65% on the Balabit dataset, surpassing\nthe current sota performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9f20\u6807\u52a8\u6001\u884c\u4e3a\u7684\u8ba4\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u8ba1\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4f18\u5316\u6570\u636e\u91cf\u548c\u884c\u4e3a\u6a21\u5f0f\u6355\u6349\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba4\u8bc1\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7528\u6237\u8ba4\u8bc1\u65b9\u6cd5\u5728\u53ef\u7528\u6027\u3001\u6210\u672c\u548c\u5b89\u5168\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9f20\u6807\u52a8\u6001\u8ba4\u8bc1\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u975e\u4fb5\u5165\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u6570\u636e\u91cf\u3001\u51c6\u786e\u6027\u4e0e\u5b9e\u7528\u6027\u5e73\u8861\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u6838\u5bc6\u5ea6\u4f30\u8ba1\u548cKL\u6563\u5ea6\u786e\u5b9a\u8bad\u7ec3\u6570\u636e\u91cf\uff0c\u5f15\u5165MAU\u4f18\u5316\u884c\u4e3a\u8868\u793a\uff0c\u8bbe\u8ba1LT-AMouse\u6846\u67b6\u7ed3\u54081D-ResNet\u548cGRU\u63d0\u53d6\u7279\u5f81\u548c\u5efa\u6a21\u65f6\u5e8f\u4f9d\u8d56\u3002", "result": "\u5728Balabit\u548cDFL\u6570\u636e\u96c6\u4e0a\u663e\u8457\u51cf\u5c11\u6570\u636e\u89c4\u6a21\uff08DFL\u51cf\u5c1110\u500d\uff09\uff0c\u8ba4\u8bc1\u7cfb\u7edf\u5728\u76f2\u653b\u51fb\u4e0b\u7684AUC\u8fbe\u523098.52%\uff08DFL\uff09\u548c94.65%\uff08Balabit\uff09\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9f20\u6807\u52a8\u6001\u8ba4\u8bc1\u4e2d\u7684\u6570\u636e\u91cf\u548c\u884c\u4e3a\u6a21\u5f0f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba4\u8bc1\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.21505", "pdf": "https://arxiv.org/pdf/2504.21505", "abs": "https://arxiv.org/abs/2504.21505", "authors": ["Jakob Benjamin Wessel", "Callum J. R. Murphy-Barltrop", "Emma S. Simpson"], "title": "A comparison of generative deep learning methods for multivariate angular simulation", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "With the recent development of new geometric and angular-radial frameworks\nfor multivariate extremes, reliably simulating from angular variables in\nmoderate-to-high dimensions is of increasing importance. Empirical approaches\nhave the benefit of simplicity, and work reasonably well in low dimensions, but\nas the number of variables increases, they can lack the required flexibility\nand scalability. Classical parametric models for angular variables, such as the\nvon Mises-Fisher (vMF) distribution, provide an alternative. Exploiting\nmixtures of vMF distributions increases their flexibility, but there are cases\nwhere even this is not sufficient to capture the intricate features that can\narise in data. Owing to their flexibility, generative deep learning methods are\nable to capture complex data structures; they therefore have the potential to\nbe useful in the simulation of angular variables. In this paper, we explore a\nrange of deep learning approaches for this task, including generative\nadversarial networks, normalizing flows and flow matching. We assess their\nperformance via a range of metrics and make comparisons to the more classical\napproach of using a mixture of vMF distributions. The methods are also applied\nto a metocean data set, demonstrating their applicability to real-world,\ncomplex data structures.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u591a\u5143\u6781\u503c\u5206\u6790\u4e2d\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u751f\u6210\u65b9\u6cd5\uff08\u5982GAN\u3001\u6807\u51c6\u5316\u6d41\u548c\u6d41\u5339\u914d\uff09\u6a21\u62df\u89d2\u53d8\u91cf\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684vMF\u6df7\u5408\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u968f\u7740\u591a\u5143\u6781\u503c\u5206\u6790\u4e2d\u51e0\u4f55\u548c\u89d2\u5f84\u5411\u6846\u67b6\u7684\u53d1\u5c55\uff0c\u9ad8\u7ef4\u89d2\u53d8\u91cf\u7684\u53ef\u9760\u6a21\u62df\u53d8\u5f97\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u4f4e\u7ef4\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9ad8\u7ef4\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08GAN\u3001\u6807\u51c6\u5316\u6d41\u3001\u6d41\u5339\u914d\uff09\u548c\u4f20\u7edf\u7684vMF\u6df7\u5408\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u79cd\u6307\u6807\u8bc4\u4f30\u6027\u80fd\uff0c\u5e76\u5e94\u7528\u4e8e\u5b9e\u9645\u6c14\u8c61\u6d77\u6d0b\u6570\u636e\u96c6\u3002", "result": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6355\u6349\u590d\u6742\u6570\u636e\u7ed3\u6784\uff0c\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4f20\u7edfvMF\u6df7\u5408\u6a21\u578b\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u6a21\u62df\u9ad8\u7ef4\u89d2\u53d8\u91cf\u65f6\u5177\u6709\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5b9e\u9645\u6570\u636e\u3002"}}
{"id": "2504.21428", "pdf": "https://arxiv.org/pdf/2504.21428", "abs": "https://arxiv.org/abs/2504.21428", "authors": ["K\u0131van\u00e7 \u015eerefo\u011flu", "\u00d6nder G\u00fcrcan", "Reyhan Aydo\u011fan"], "title": "UAV Marketplace Simulation Tool for BVLOS Operations", "categories": ["cs.RO", "cs.AI", "cs.DC"], "comment": "3 pages, 2 figures, the 24th International Conference on Autonomous\n  Agents and Multiagent Systems (AAMAS 2025)", "summary": "We present a simulation tool for evaluating team formation in autonomous\nmulti-UAV (Unmanned Aerial Vehicle) missions that operate Beyond Visual Line of\nSight (BVLOS). The tool models UAV collaboration and mission execution in\ndynamic and adversarial conditions, where Byzantine UAVs attempt to disrupt\noperations. Our tool allows researchers to integrate and compare various team\nformation strategies in a controlled environment with configurable mission\nparameters and adversarial behaviors. The log of each simulation run is stored\nin a structured way along with performance metrics so that statistical analysis\ncould be done straightforwardly. The tool is versatile for testing and\nimproving UAV coordination strategies in real-world applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u591a\u65e0\u4eba\u673a\uff08UAV\uff09\u56e2\u961f\u5f62\u6210\u7684\u4eff\u771f\u5de5\u5177\uff0c\u652f\u6301\u8d85\u89c6\u8ddd\uff08BVLOS\uff09\u4efb\u52a1\uff0c\u6a21\u62df\u52a8\u6001\u548c\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u534f\u4f5c\u4e0e\u4efb\u52a1\u6267\u884c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u4e2a\u5de5\u5177\uff0c\u7528\u4e8e\u6d4b\u8bd5\u548c\u6539\u8fdb\u65e0\u4eba\u673a\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u7684\u56e2\u961f\u534f\u4f5c\u7b56\u7565\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u62dc\u5360\u5ead\u65e0\u4eba\u673a\u5e72\u6270\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u4eff\u771f\u5de5\u5177\u5efa\u6a21\u65e0\u4eba\u673a\u534f\u4f5c\u548c\u4efb\u52a1\u6267\u884c\uff0c\u652f\u6301\u914d\u7f6e\u4efb\u52a1\u53c2\u6570\u548c\u5bf9\u6297\u884c\u4e3a\uff0c\u5e76\u8bb0\u5f55\u6a21\u62df\u65e5\u5fd7\u548c\u6027\u80fd\u6307\u6807\u4ee5\u4fbf\u7edf\u8ba1\u5206\u6790\u3002", "result": "\u7ed3\u679c\u662f\u8be5\u5de5\u5177\u80fd\u591f\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u96c6\u6210\u548c\u6bd4\u8f83\u4e0d\u540c\u7684\u56e2\u961f\u5f62\u6210\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u7684\u65e0\u4eba\u673a\u534f\u8c03\u7b56\u7565\u6d4b\u8bd5\u548c\u6539\u8fdb\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u4eff\u771f\u5de5\u5177\u4e3a\u65e0\u4eba\u673a\u56e2\u961f\u534f\u4f5c\u7b56\u7565\u7684\u7814\u7a76\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21527", "pdf": "https://arxiv.org/pdf/2504.21527", "abs": "https://arxiv.org/abs/2504.21527", "authors": ["Sebastian Esche", "Martin Stoll"], "title": "Low-rank computation of the posterior mean in Multi-Output Gaussian Processes", "categories": ["math.NA", "cs.LG", "cs.NA"], "comment": null, "summary": "Gaussian processes (GP) are a versatile tool in machine learning and\ncomputational science. We here consider the case of multi-output Gaussian\nprocesses (MOGP) and present low-rank approaches for efficiently computing the\nposterior mean of a MOGP. Starting from low-rank spatio-temporal data we\nconsider a structured covariance function, assuming separability across space\nand time. This separability, in turn, gives a decomposition of the covariance\nmatrix into a Kronecker product of individual covariance matrices.\nIncorporating the typical noise term to the model then requires the solution of\na large-scale Stein equation for computing the posterior mean. For this, we\npropose efficient low-rank methods based on a combination of a LRPCG method\nwith the Sylvester equation solver KPIK adjusted for solving Stein equations.\nWe test the developed method on real world street network graphs by using graph\nfilters as covariance matrices. Moreover, we propose a degree-weighted average\ncovariance matrix, which can be employed under specific assumptions to achieve\nmore efficient convergence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u79e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u8ba1\u7b97\u591a\u8f93\u51fa\u9ad8\u65af\u8fc7\u7a0b\uff08MOGP\uff09\u7684\u540e\u9a8c\u5747\u503c\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u65f6\u95f4\u53ef\u5206\u79bb\u7684\u534f\u65b9\u5dee\u51fd\u6570\u548cKronecker\u79ef\u5206\u89e3\uff0c\u7ed3\u5408LRPCG\u65b9\u6cd5\u548cKPIK\u6c42\u89e3\u5668\u89e3\u51b3\u5927\u89c4\u6a21Stein\u65b9\u7a0b\u3002", "motivation": "\u591a\u8f93\u51fa\u9ad8\u65af\u8fc7\u7a0b\uff08MOGP\uff09\u5728\u673a\u5668\u5b66\u4e60\u548c\u8ba1\u7b97\u79d1\u5b66\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u8ba1\u7b97\u540e\u9a8c\u5747\u503c\u65f6\u9762\u4e34\u5927\u89c4\u6a21Stein\u65b9\u7a0b\u6c42\u89e3\u7684\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4f4e\u79e9\u65f6\u7a7a\u6570\u636e\u5047\u8bbe\uff0c\u6784\u5efa\u53ef\u5206\u79bb\u7684\u534f\u65b9\u5dee\u51fd\u6570\uff0c\u5206\u89e3\u4e3aKronecker\u79ef\uff0c\u7ed3\u5408LRPCG\u65b9\u6cd5\u548cKPIK\u6c42\u89e3\u5668\u89e3\u51b3Stein\u65b9\u7a0b\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u8857\u9053\u7f51\u7edc\u56fe\u4e0a\u6d4b\u8bd5\uff0c\u4f7f\u7528\u56fe\u6ee4\u6ce2\u5668\u4f5c\u4e3a\u534f\u65b9\u5dee\u77e9\u9635\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u5ea6\u52a0\u6743\u5e73\u5747\u534f\u65b9\u5dee\u77e9\u9635\u4ee5\u63d0\u9ad8\u6536\u655b\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u4f4e\u79e9\u65b9\u6cd5\u80fd\u9ad8\u6548\u8ba1\u7b97MOGP\u540e\u9a8c\u5747\u503c\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.21602", "pdf": "https://arxiv.org/pdf/2504.21602", "abs": "https://arxiv.org/abs/2504.21602", "authors": ["Hannes Reichert", "Benjamin Serfling", "Elijah Sch\u00fcssler", "Kerim Turacan", "Konrad Doll", "Bernhard Sick"], "title": "Real Time Semantic Segmentation of High Resolution Automotive LiDAR Scans", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "In recent studies, numerous previous works emphasize the importance of\nsemantic segmentation of LiDAR data as a critical component to the development\nof driver-assistance systems and autonomous vehicles. However, many\nstate-of-the-art methods are tested on outdated, lower-resolution LiDAR sensors\nand struggle with real-time constraints. This study introduces a novel semantic\nsegmentation framework tailored for modern high-resolution LiDAR sensors that\naddresses both accuracy and real-time processing demands. We propose a novel\nLiDAR dataset collected by a cutting-edge automotive 128 layer LiDAR in urban\ntraffic scenes. Furthermore, we propose a semantic segmentation method\nutilizing surface normals as strong input features. Our approach is bridging\nthe gap between cutting-edge research and practical automotive applications.\nAdditionaly, we provide a Robot Operating System (ROS2) implementation that we\noperate on our research vehicle. Our dataset and code are publicly available:\nhttps://github.com/kav-institute/SemanticLiDAR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9ad8\u5206\u8fa8\u7387LiDAR\u4f20\u611f\u5668\u7684\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u7cbe\u5ea6\u548c\u5b9e\u65f6\u5904\u7406\u9700\u6c42\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u96c6\u548cROS2\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u57fa\u4e8e\u8fc7\u65f6\u7684\u4f4e\u5206\u8fa8\u7387LiDAR\u4f20\u611f\u5668\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u6027\u9700\u6c42\uff0c\u9700\u9488\u5bf9\u73b0\u4ee3\u9ad8\u5206\u8fa8\u7387\u4f20\u611f\u5668\u4f18\u5316\u3002", "method": "\u5229\u7528\u8868\u9762\u6cd5\u7ebf\u4f5c\u4e3a\u8f93\u5165\u7279\u5f81\uff0c\u63d0\u51fa\u65b0\u7684\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e128\u5c42LiDAR\u91c7\u96c6\u6570\u636e\u96c6\u3002", "result": "\u6846\u67b6\u5728\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u586b\u8865\u4e86\u524d\u6cbf\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.21634", "pdf": "https://arxiv.org/pdf/2504.21634", "abs": "https://arxiv.org/abs/2504.21634", "authors": ["Chih-Cheng Rex Yuan", "Bow-Yaw Wang"], "title": "Quantitative Auditing of AI Fairness with Differentially Private Synthetic Data", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "Fairness auditing of AI systems can identify and quantify biases. However,\ntraditional auditing using real-world data raises security and privacy\nconcerns. It exposes auditors to security risks as they become custodians of\nsensitive information and targets for cyberattacks. Privacy risks arise even\nwithout direct breaches, as data analyses can inadvertently expose confidential\ninformation. To address these, we propose a framework that leverages\ndifferentially private synthetic data to audit the fairness of AI systems. By\napplying privacy-preserving mechanisms, it generates synthetic data that\nmirrors the statistical properties of the original dataset while ensuring\nprivacy. This method balances the goal of rigorous fairness auditing and the\nneed for strong privacy protections. Through experiments on real datasets like\nAdult, COMPAS, and Diabetes, we compare fairness metrics of synthetic and real\ndata. By analyzing the alignment and discrepancies between these metrics, we\nassess the capacity of synthetic data to preserve the fairness properties of\nreal data. Our results demonstrate the framework's ability to enable meaningful\nfairness evaluations while safeguarding sensitive information, proving its\napplicability across critical and sensitive domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5dee\u5206\u9690\u79c1\u5408\u6210\u6570\u636e\u7684AI\u516c\u5e73\u6027\u5ba1\u8ba1\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5ba1\u8ba1\u4e2d\u7684\u5b89\u5168\u548c\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5ba1\u8ba1\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u4f1a\u5e26\u6765\u5b89\u5168\u548c\u9690\u79c1\u98ce\u9669\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u6709\u6548\u5ba1\u8ba1\u516c\u5e73\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5dee\u5206\u9690\u79c1\u673a\u5236\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u4fdd\u7559\u539f\u59cb\u6570\u636e\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u540c\u65f6\u786e\u4fdd\u9690\u79c1\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\uff08\u5982Adult\u3001COMPAS\u3001Diabetes\uff09\u4e0a\u9a8c\u8bc1\u4e86\u5408\u6210\u6570\u636e\u80fd\u6709\u6548\u4fdd\u7559\u516c\u5e73\u6027\u6307\u6807\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u62a4\u654f\u611f\u4fe1\u606f\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u516c\u5e73\u6027\u5ba1\u8ba1\uff0c\u9002\u7528\u4e8e\u654f\u611f\u9886\u57df\u3002"}}
{"id": "2504.21454", "pdf": "https://arxiv.org/pdf/2504.21454", "abs": "https://arxiv.org/abs/2504.21454", "authors": ["Federico Nesti", "Gianluca D'Amico", "Mauro Marinoni", "Giorgio Buttazzo"], "title": "SimPRIVE: a Simulation framework for Physical Robot Interaction with Virtual Environments", "categories": ["cs.RO", "cs.AI"], "comment": "Submitted to IEEE ITSC 2025", "summary": "The use of machine learning in cyber-physical systems has attracted the\ninterest of both industry and academia. However, no general solution has yet\nbeen found against the unpredictable behavior of neural networks and\nreinforcement learning agents. Nevertheless, the improvements of\nphoto-realistic simulators have paved the way towards extensive testing of\ncomplex algorithms in different virtual scenarios, which would be expensive and\ndangerous to implement in the real world.\n  This paper presents SimPRIVE, a simulation framework for physical robot\ninteraction with virtual environments, which operates as a vehicle-in-the-loop\nplatform, rendering a virtual world while operating the vehicle in the real\nworld.\n  Using SimPRIVE, any physical mobile robot running on ROS 2 can easily be\nconfigured to move its digital twin in a virtual world built with the Unreal\nEngine 5 graphic engine, which can be populated with objects, people, or other\nvehicles with programmable behavior.\n  SimPRIVE has been designed to accommodate custom or pre-built virtual worlds\nwhile being light-weight to contain execution times and allow fast rendering.\nIts main advantage lies in the possibility of testing complex algorithms on the\nfull software and hardware stack while minimizing the risks and costs of a test\ncampaign. The framework has been validated by testing a reinforcement learning\nagent trained for obstacle avoidance on an AgileX Scout Mini rover that\nnavigates a virtual office environment where everyday objects and people are\nplaced as obstacles. The physical rover moves with no collision in an indoor\nlimited space, thanks to a LiDAR-based heuristic.", "AI": {"tldr": "SimPRIVE\u662f\u4e00\u4e2a\u7528\u4e8e\u7269\u7406\u673a\u5668\u4eba\u4e0e\u865a\u62df\u73af\u5883\u4ea4\u4e92\u7684\u4eff\u771f\u6846\u67b6\uff0c\u652f\u6301ROS 2\u7684\u79fb\u52a8\u673a\u5668\u4eba\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u5728Unreal Engine 5\u6784\u5efa\u7684\u865a\u62df\u4e16\u754c\u4e2d\u8fd0\u884c\uff0c\u7528\u4e8e\u5b89\u5168\u9ad8\u6548\u5730\u6d4b\u8bd5\u590d\u6742\u7b97\u6cd5\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u5728\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u884c\u4e3a\u4e0d\u53ef\u9884\u6d4b\uff0c\u800c\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u6210\u672c\u9ad8\u4e14\u5371\u9669\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u4eff\u771f\u6d4b\u8bd5\u5e73\u53f0\u3002", "method": "\u63d0\u51faSimPRIVE\u6846\u67b6\uff0c\u5c06\u7269\u7406\u673a\u5668\u4eba\u4e0e\u865a\u62df\u73af\u5883\u7ed3\u5408\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u6280\u672f\u8fd0\u884c\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u865a\u62df\u573a\u666f\u548c\u5feb\u901f\u6e32\u67d3\u3002", "result": "\u9a8c\u8bc1\u4e86\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u865a\u62df\u529e\u516c\u5ba4\u73af\u5883\u4e2d\u907f\u969c\u7684\u6709\u6548\u6027\uff0c\u7269\u7406\u673a\u5668\u4eba\u5728\u6709\u9650\u7a7a\u95f4\u5185\u65e0\u78b0\u649e\u8fd0\u884c\u3002", "conclusion": "SimPRIVE\u4e3a\u590d\u6742\u7b97\u6cd5\u7684\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u3001\u4f4e\u98ce\u9669\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u786c\u4ef6\u548c\u8f6f\u4ef6\u5806\u6808\u7684\u5168\u9762\u6d4b\u8bd5\u3002"}}
{"id": "2504.21668", "pdf": "https://arxiv.org/pdf/2504.21668", "abs": "https://arxiv.org/abs/2504.21668", "authors": ["Baolei Zhang", "Haoran Xin", "Minghong Fang", "Zhuqing Liu", "Biao Yi", "Tong Li", "Zheli Liu"], "title": "Traceback of Poisoning Attacks to Retrieval-Augmented Generation", "categories": ["cs.CR", "cs.IR", "cs.LG"], "comment": "Accepted by The Web Conference 2025", "summary": "Large language models (LLMs) integrated with retrieval-augmented generation\n(RAG) systems improve accuracy by leveraging external knowledge sources.\nHowever, recent research has revealed RAG's susceptibility to poisoning\nattacks, where the attacker injects poisoned texts into the knowledge database,\nleading to attacker-desired responses. Existing defenses, which predominantly\nfocus on inference-time mitigation, have proven insufficient against\nsophisticated attacks. In this paper, we introduce RAGForensics, the first\ntraceback system for RAG, designed to identify poisoned texts within the\nknowledge database that are responsible for the attacks. RAGForensics operates\niteratively, first retrieving a subset of texts from the database and then\nutilizing a specially crafted prompt to guide an LLM in detecting potential\npoisoning texts. Empirical evaluations across multiple datasets demonstrate the\neffectiveness of RAGForensics against state-of-the-art poisoning attacks. This\nwork pioneers the traceback of poisoned texts in RAG systems, providing a\npractical and promising defense mechanism to enhance their security.", "AI": {"tldr": "RAGForensics\u662f\u4e00\u79cd\u65b0\u578b\u7684RAG\u7cfb\u7edf\u9632\u5fa1\u673a\u5236\uff0c\u901a\u8fc7\u8ffd\u6eaf\u77e5\u8bc6\u5e93\u4e2d\u7684\u4e2d\u6bd2\u6587\u672c\u6765\u589e\u5f3a\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u6613\u53d7\u4e2d\u6bd2\u653b\u51fb\uff0c\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u590d\u6742\u653b\u51fb\u3002", "method": "RAGForensics\u901a\u8fc7\u8fed\u4ee3\u68c0\u7d22\u77e5\u8bc6\u5e93\u5b50\u96c6\uff0c\u5e76\u5229\u7528\u5b9a\u5236\u63d0\u793a\u5f15\u5bfcLLM\u68c0\u6d4b\u4e2d\u6bd2\u6587\u672c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRAGForensics\u80fd\u6709\u6548\u5bf9\u6297\u5148\u8fdb\u7684\u4e2d\u6bd2\u653b\u51fb\u3002", "conclusion": "RAGForensics\u4e3aRAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u5b89\u5168\u589e\u5f3a\u673a\u5236\u3002"}}
{"id": "2504.21700", "pdf": "https://arxiv.org/pdf/2504.21700", "abs": "https://arxiv.org/abs/2504.21700", "authors": ["Marco Arazzi", "Vignesh Kumar Kembu", "Antonino Nocera", "Vinod P"], "title": "XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models are fundamental actors in the modern IT landscape\ndominated by AI solutions. However, security threats associated with them might\nprevent their reliable adoption in critical application scenarios such as\ngovernment organizations and medical institutions. For this reason, commercial\nLLMs typically undergo a sophisticated censoring mechanism to eliminate any\nharmful output they could possibly produce. In response to this, LLM\nJailbreaking is a significant threat to such protections, and many previous\napproaches have already demonstrated its effectiveness across diverse domains.\nExisting jailbreak proposals mostly adopt a generate-and-test strategy to craft\nmalicious input. To improve the comprehension of censoring mechanisms and\ndesign a targeted jailbreak attack, we propose an Explainable-AI solution that\ncomparatively analyzes the behavior of censored and uncensored models to derive\nunique exploitable alignment patterns. Then, we propose XBreaking, a novel\njailbreak attack that exploits these unique patterns to break the security\nconstraints of LLMs by targeted noise injection. Our thorough experimental\ncampaign returns important insights about the censoring mechanisms and\ndemonstrates the effectiveness and performance of our attack.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91caAI\u7684LLM\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5XBreaking\uff0c\u901a\u8fc7\u5206\u6790\u5ba1\u67e5\u673a\u5236\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u8bbe\u8ba1\u9488\u5bf9\u6027\u653b\u51fb\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u5a01\u80c1\u9650\u5236\u4e86\u5176\u53ef\u9760\u4f7f\u7528\uff0c\u73b0\u6709\u8d8a\u72f1\u65b9\u6cd5\u591a\u4e3a\u751f\u6210-\u6d4b\u8bd5\u7b56\u7565\uff0c\u7f3a\u4e4f\u9488\u5bf9\u6027\u3002", "method": "\u63d0\u51faXBreaking\uff0c\u5229\u7528\u53ef\u89e3\u91caAI\u5206\u6790\u5ba1\u67e5\u4e0e\u672a\u5ba1\u67e5\u6a21\u578b\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u8bbe\u8ba1\u9488\u5bf9\u6027\u566a\u58f0\u6ce8\u5165\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86XBreaking\u7684\u6709\u6548\u6027\u548c\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u5ba1\u67e5\u673a\u5236\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "conclusion": "XBreaking\u4e3aLLM\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u9488\u5bf9\u6027\u653b\u51fb\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.21787", "pdf": "https://arxiv.org/pdf/2504.21787", "abs": "https://arxiv.org/abs/2504.21787", "authors": ["Jaouad Mourtada"], "title": "Estimation of discrete distributions in relative entropy, and the deviations of the missing mass", "categories": ["math.ST", "cs.IT", "cs.LG", "math.IT", "stat.ML", "stat.TH"], "comment": "54 pages", "summary": "We study the problem of estimating a distribution over a finite alphabet from\nan i.i.d. sample, with accuracy measured in relative entropy (Kullback-Leibler\ndivergence). While optimal expected risk bounds are known, high-probability\nguarantees remain less well-understood. First, we analyze the classical Laplace\n(add-$1$) estimator, obtaining matching upper and lower bounds on its\nperformance and showing its optimality among confidence-independent estimators.\nWe then characterize the minimax-optimal high-probability risk achievable by\nany estimator, which is attained via a simple confidence-dependent smoothing\ntechnique. Interestingly, the optimal non-asymptotic risk contains an\nadditional logarithmic factor over the ideal asymptotic risk. Next, motivated\nby scenarios where the alphabet exceeds the sample size, we investigate methods\nthat adapt to the sparsity of the distribution at hand. We introduce an\nestimator using data-dependent smoothing, for which we establish a\nhigh-probability risk bound depending on two effective sparsity parameters. As\npart of the analysis, we also derive a sharp high-probability upper bound on\nthe missing mass.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4ece\u72ec\u7acb\u540c\u5206\u5e03\u6837\u672c\u4e2d\u4f30\u8ba1\u6709\u9650\u5b57\u6bcd\u8868\u4e0a\u7684\u5206\u5e03\u95ee\u9898\uff0c\u4ee5\u76f8\u5bf9\u71b5\uff08KL\u6563\u5ea6\uff09\u8861\u91cf\u51c6\u786e\u6027\u3002\u5206\u6790\u4e86\u7ecf\u5178Laplace\u4f30\u8ba1\u5668\uff0c\u5e76\u63a2\u8ba8\u4e86\u9ad8\u6982\u7387\u98ce\u9669\u6700\u4f18\u6027\u53ca\u7a00\u758f\u5206\u5e03\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5728\u9ad8\u6982\u7387\u4e0b\u5206\u5e03\u4f30\u8ba1\u7684\u6027\u80fd\u754c\u9650\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5b57\u6bcd\u8868\u5927\u5c0f\u8d85\u8fc7\u6837\u672c\u91cf\u7684\u7a00\u758f\u573a\u666f\u4e0b\u3002", "method": "\u5206\u6790\u4e86Laplace\u4f30\u8ba1\u5668\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u7f6e\u4fe1\u4f9d\u8d56\u5e73\u6ed1\u6280\u672f\u548c\u6570\u636e\u4f9d\u8d56\u5e73\u6ed1\u7684\u81ea\u9002\u5e94\u4f30\u8ba1\u5668\u3002", "result": "\u8bc1\u660e\u4e86Laplace\u4f30\u8ba1\u5668\u5728\u7f6e\u4fe1\u72ec\u7acb\u4f30\u8ba1\u5668\u4e2d\u7684\u6700\u4f18\u6027\uff0c\u5e76\u7ed9\u51fa\u4e86\u9ad8\u6982\u7387\u98ce\u9669\u7684\u6700\u4f18\u754c\u9650\uff0c\u53d1\u73b0\u975e\u6e10\u8fd1\u98ce\u9669\u5305\u542b\u989d\u5916\u5bf9\u6570\u56e0\u5b50\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u4f9d\u8d56\u5e73\u6ed1\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7a00\u758f\u5206\u5e03\u7684\u9ad8\u6982\u7387\u98ce\u9669\u754c\u9650\uff0c\u5e76\u63a8\u5bfc\u4e86\u7f3a\u5931\u8d28\u91cf\u7684\u5c16\u9510\u4e0a\u754c\u3002"}}
{"id": "2504.21480", "pdf": "https://arxiv.org/pdf/2504.21480", "abs": "https://arxiv.org/abs/2504.21480", "authors": ["Yuchen Ding", "Hongli Peng", "Xiaoqi Li"], "title": "A Comprehensive Study of Exploitable Patterns in Smart Contracts: From Vulnerability to Defense", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": null, "summary": "With the rapid advancement of blockchain technology, smart contracts have\nenabled the implementation of increasingly complex functionalities. However,\nensuring the security of smart contracts remains a persistent challenge across\nthe stages of development, compilation, and execution. Vulnerabilities within\nsmart contracts not only undermine the security of individual applications but\nalso pose significant risks to the broader blockchain ecosystem, as\ndemonstrated by the growing frequency of attacks since 2016, resulting in\nsubstantial financial losses. This paper provides a comprehensive analysis of\nkey security risks in Ethereum smart contracts, specifically those written in\nSolidity and executed on the Ethereum Virtual Machine (EVM). We focus on two\nprevalent and critical vulnerability types (reentrancy and integer overflow) by\nexamining their underlying mechanisms, replicating attack scenarios, and\nassessing effective countermeasures.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u4ee5\u592a\u574a\u667a\u80fd\u5408\u7ea6\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u91cd\u70b9\u5173\u6ce8\u91cd\u5165\u548c\u6574\u6570\u6ea2\u51fa\u6f0f\u6d1e\uff0c\u63a2\u8ba8\u5176\u673a\u5236\u3001\u653b\u51fb\u573a\u666f\u53ca\u9632\u5fa1\u63aa\u65bd\u3002", "motivation": "\u968f\u7740\u533a\u5757\u94fe\u6280\u672f\u7684\u53d1\u5c55\uff0c\u667a\u80fd\u5408\u7ea6\u5b89\u5168\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u6f0f\u6d1e\u53ef\u80fd\u5bfc\u81f4\u91cd\u5927\u7ecf\u6d4e\u635f\u5931\uff0c\u4e9f\u9700\u6df1\u5165\u7814\u7a76\u3002", "method": "\u7814\u7a76\u4ee5\u592a\u574a\u667a\u80fd\u5408\u7ea6\uff08Solidity\u7f16\u5199\uff0cEVM\u6267\u884c\uff09\u4e2d\u7684\u91cd\u5165\u548c\u6574\u6570\u6ea2\u51fa\u6f0f\u6d1e\uff0c\u5206\u6790\u673a\u5236\u3001\u590d\u73b0\u653b\u51fb\u573a\u666f\u5e76\u8bc4\u4f30\u9632\u5fa1\u65b9\u6cd5\u3002", "result": "\u63ed\u793a\u4e86\u91cd\u5165\u548c\u6574\u6570\u6ea2\u51fa\u6f0f\u6d1e\u7684\u673a\u5236\u53ca\u653b\u51fb\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u6709\u6548\u7684\u9632\u5fa1\u63aa\u65bd\u3002", "conclusion": "\u667a\u80fd\u5408\u7ea6\u5b89\u5168\u98ce\u9669\u9700\u6301\u7eed\u5173\u6ce8\uff0c\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u6f0f\u6d1e\u9632\u5fa1\u7684\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2504.21795", "pdf": "https://arxiv.org/pdf/2504.21795", "abs": "https://arxiv.org/abs/2504.21795", "authors": ["Yuankang Zhao", "Matthew Engelhard"], "title": "Balancing Interpretability and Flexibility in Modeling Diagnostic Trajectories with an Embedded Neural Hawkes Process Model", "categories": ["stat.ML", "cs.LG", "stat.AP"], "comment": null, "summary": "The Hawkes process (HP) is commonly used to model event sequences with\nself-reinforcing dynamics, including electronic health records (EHRs).\nTraditional HPs capture self-reinforcement via parametric impact functions that\ncan be inspected to understand how each event modulates the intensity of\nothers. Neural network-based HPs offer greater flexibility, resulting in\nimproved fit and prediction performance, but at the cost of interpretability,\nwhich is often critical in healthcare. In this work, we aim to understand and\nimprove upon this tradeoff. We propose a novel HP formulation in which impact\nfunctions are modeled by defining a flexible impact kernel, instantiated as a\nneural network, in event embedding space, which allows us to model large-scale\nevent sequences with many event types. This approach is more flexible than\ntraditional HPs yet more interpretable than other neural network approaches,\nand allows us to explicitly trade flexibility for interpretability by adding\ntransformer encoder layers to further contextualize the event embeddings.\nResults show that our method accurately recovers impact functions in\nsimulations, achieves competitive performance on MIMIC-IV procedure dataset,\nand gains clinically meaningful interpretation on XX-EHR with children\ndiagnosis dataset even without transformer layers. This suggests that our\nflexible impact kernel is often sufficient to capture self-reinforcing dynamics\nin EHRs and other data effectively, implying that interpretability can be\nmaintained without loss of performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u970d\u514b\u65af\u8fc7\u7a0b\uff08HP\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u4e8b\u4ef6\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u7075\u6d3b\u5f71\u54cd\u6838\uff08\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\uff09\u6765\u5e73\u8861\u6a21\u578b\u7684\u7075\u6d3b\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u4e8b\u4ef6\u5e8f\u5217\u3002", "motivation": "\u4f20\u7edfHP\u6a21\u578b\u901a\u8fc7\u53c2\u6570\u5316\u5f71\u54cd\u51fd\u6570\u5b9e\u73b0\u81ea\u589e\u5f3a\u52a8\u6001\uff0c\u4f46\u7075\u6d3b\u6027\u4e0d\u8db3\uff1b\u795e\u7ecf\u7f51\u7edcHP\u6a21\u578b\u867d\u7075\u6d3b\u4f46\u727a\u7272\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u53ef\u89e3\u91ca\u6027\u5728\u533b\u7597\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578bHP\u6a21\u578b\uff0c\u5229\u7528\u4e8b\u4ef6\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u5b9a\u4e49\u7075\u6d3b\u5f71\u54cd\u6838\uff0c\u5e76\u53ef\u6dfb\u52a0Transformer\u5c42\u4ee5\u8fdb\u4e00\u6b65\u4e0a\u4e0b\u6587\u5316\u4e8b\u4ef6\u5d4c\u5165\u3002", "result": "\u6a21\u578b\u5728\u6a21\u62df\u4e2d\u51c6\u786e\u6062\u590d\u5f71\u54cd\u51fd\u6570\uff0c\u5728MIMIC-IV\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u513f\u7ae5\u8bca\u65ad\u6570\u636e\u96c6\u4e2d\u83b7\u5f97\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u89e3\u91ca\u3002", "conclusion": "\u7075\u6d3b\u5f71\u54cd\u6838\u80fd\u6709\u6548\u6355\u6349\u81ea\u589e\u5f3a\u52a8\u6001\uff0c\u8868\u660e\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u53ef\u7ef4\u6301\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.21489", "pdf": "https://arxiv.org/pdf/2504.21489", "abs": "https://arxiv.org/abs/2504.21489", "authors": ["Shirin Anlen", "Zuzanna Wojciak"], "title": "TRIED: Truly Innovative and Effective Detection Benchmark, developed by WITNESS", "categories": ["cs.CY", "cs.AI"], "comment": "33 pages", "summary": "The rise of generative AI and deceptive synthetic media threatens the global\ninformation ecosystem, especially across the Global Majority. This report from\nWITNESS highlights the limitations of current AI detection tools, which often\nunderperform in real-world scenarios due to challenges related to\nexplainability, fairness, accessibility, and contextual relevance. In response,\nWITNESS introduces the Truly Innovative and Effective AI Detection (TRIED)\nBenchmark, a new framework for evaluating detection tools based on their\nreal-world impact and capacity for innovation. Drawing on frontline\nexperiences, deceptive AI cases, and global consultations, the report outlines\nhow detection tools must evolve to become truly innovative and relevant by\nmeeting diverse linguistic, cultural, and technological contexts. It offers\npractical guidance for developers, policymakers, and standards bodies to design\naccountable, transparent, and user-centered detection solutions, and\nincorporate sociotechnical considerations into future AI standards, procedures\nand evaluation frameworks. By adopting the TRIED Benchmark, stakeholders can\ndrive innovation, safeguard public trust, strengthen AI literacy, and\ncontribute to a more resilient global information credibility.", "AI": {"tldr": "WITNESS\u62a5\u544a\u6307\u51fa\u5f53\u524dAI\u68c0\u6d4b\u5de5\u5177\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u63d0\u51faTRIED Benchmark\u6846\u67b6\u4ee5\u8bc4\u4f30\u5de5\u5177\u7684\u5b9e\u9645\u5f71\u54cd\u548c\u521b\u65b0\u80fd\u529b\uff0c\u5f3a\u8c03\u9700\u9002\u5e94\u591a\u6837\u5316\u9700\u6c42\u3002", "motivation": "\u751f\u6210\u5f0fAI\u548c\u865a\u5047\u5408\u6210\u5a92\u4f53\u7684\u5d1b\u8d77\u5a01\u80c1\u5168\u7403\u4fe1\u606f\u751f\u6001\u7cfb\u7edf\uff0c\u5c24\u5176\u5728\u53d1\u5c55\u4e2d\u56fd\u5bb6\uff0c\u73b0\u6709\u68c0\u6d4b\u5de5\u5177\u56e0\u53ef\u89e3\u91ca\u6027\u3001\u516c\u5e73\u6027\u7b49\u95ee\u9898\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8e\u524d\u7ebf\u7ecf\u9a8c\u3001\u865a\u5047AI\u6848\u4f8b\u548c\u5168\u7403\u54a8\u8be2\uff0c\u63d0\u51faTRIED Benchmark\u6846\u67b6\uff0c\u8bc4\u4f30\u5de5\u5177\u7684\u5b9e\u7528\u6027\u548c\u521b\u65b0\u6027\u3002", "result": "\u62a5\u544a\u4e3a\u5f00\u53d1\u8005\u3001\u653f\u7b56\u5236\u5b9a\u8005\u7b49\u63d0\u4f9b\u5b9e\u7528\u6307\u5357\uff0c\u5f3a\u8c03\u900f\u660e\u3001\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u68c0\u6d4b\u65b9\u6848\uff0c\u5e76\u7eb3\u5165\u793e\u4f1a\u6280\u672f\u8003\u91cf\u3002", "conclusion": "\u91c7\u7528TRIED Benchmark\u53ef\u63a8\u52a8\u521b\u65b0\u3001\u589e\u5f3a\u516c\u4f17\u4fe1\u4efb\u548cAI\u7d20\u517b\uff0c\u63d0\u5347\u5168\u7403\u4fe1\u606f\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2504.21844", "pdf": "https://arxiv.org/pdf/2504.21844", "abs": "https://arxiv.org/abs/2504.21844", "authors": ["William Sutcliffe", "Marta Calvi", "Simone Capelli", "Jonas Eschle", "Juli\u00e1n Garc\u00eda Pardi\u00f1as", "Abhijit Mathad", "Azusa Uzuki", "Nicola Serra"], "title": "Scalable Multi-Task Learning for Particle Collision Event Reconstruction with Heterogeneous Graph Neural Networks", "categories": ["physics.data-an", "cs.LG", "hep-ex"], "comment": "21 pages, 10 figures, 4 tables", "summary": "The growing luminosity frontier at the Large Hadron Collider is challenging\nthe reconstruction and analysis of particle collision events. Increased\nparticle multiplicities are straining latency and storage requirements at the\ndata acquisition stage, while new complications are emerging, including higher\nbackground levels and more frequent particle vertex misassociations. This in\nturn necessitates the development of more holistic and scalable reconstruction\nmethods that take advantage of recent advances in machine learning. We propose\na novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique\nrepresentations for diverse particle collision relationships and integrated\ngraph pruning layers for scalability. Trained with a multi-task paradigm in an\nenvironment mimicking the LHCb experiment, this HGNN significantly improves\nbeauty hadron reconstruction performance. Notably, it concurrently performs\nparticle vertex association and graph pruning within a single framework. We\nquantify reconstruction and pruning performance, demonstrate enhanced inference\ntime scaling with event complexity, and mitigate potential performance loss\nusing a weighted message passing scheme.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HGNN\uff09\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u578b\u5f3a\u5b50\u5bf9\u649e\u673a\uff08LHC\uff09\u4e2d\u7c92\u5b50\u78b0\u649e\u4e8b\u4ef6\u91cd\u5efa\u7684\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u56fe\u526a\u679d\u5c42\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u578b\u5f3a\u5b50\u5bf9\u649e\u673a\u4eae\u5ea6\u589e\u52a0\uff0c\u7c92\u5b50\u78b0\u649e\u4e8b\u4ef6\u7684\u91cd\u5efa\u548c\u5206\u6790\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u9ad8\u7c92\u5b50\u591a\u91cd\u6027\u3001\u5ef6\u8fdf\u548c\u5b58\u50a8\u9700\u6c42\u589e\u52a0\uff0c\u4ee5\u53ca\u80cc\u666f\u566a\u58f0\u548c\u9876\u70b9\u9519\u8bef\u5173\u8054\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HGNN\uff09\u67b6\u6784\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u56fe\u526a\u679d\u5c42\uff0c\u7528\u4e8e\u7c92\u5b50\u9876\u70b9\u5173\u8054\u548c\u56fe\u5f62\u526a\u679d\u3002", "result": "\u8be5HGNN\u663e\u8457\u63d0\u9ad8\u4e86\u7f8e\u4e3d\u5f3a\u5b50\u91cd\u5efa\u6027\u80fd\uff0c\u5e76\u5728\u5355\u4e00\u6846\u67b6\u5185\u540c\u65f6\u5b8c\u6210\u9876\u70b9\u5173\u8054\u548c\u56fe\u526a\u679d\uff0c\u540c\u65f6\u4f18\u5316\u4e86\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7c92\u5b50\u78b0\u649e\u4e8b\u4ef6\u7684\u91cd\u5efa\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u9ad8\u590d\u6742\u5ea6\u4e8b\u4ef6\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2504.21545", "pdf": "https://arxiv.org/pdf/2504.21545", "abs": "https://arxiv.org/abs/2504.21545", "authors": ["Yangyang Li", "Guanlong Liu", "Ronghua Shang", "Licheng Jiao"], "title": "Meta knowledge assisted Evolutionary Neural Architecture Search", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Evolutionary computation (EC)-based neural architecture search (NAS) has\nachieved remarkable performance in the automatic design of neural\narchitectures. However, the high computational cost associated with evaluating\nsearched architectures poses a challenge for these methods, and a fixed form of\nlearning rate (LR) schedule means greater information loss on diverse searched\narchitectures. This paper introduces an efficient EC-based NAS method to solve\nthese problems via an innovative meta-learning framework. Specifically, a\nmeta-learning-rate (Meta-LR) scheme is used through pretraining to obtain a\nsuitable LR schedule, which guides the training process with lower information\nloss when evaluating each individual. An adaptive surrogate model is designed\nthrough an adaptive threshold to select the potential architectures in a few\nepochs and then evaluate the potential architectures with complete epochs.\nAdditionally, a periodic mutation operator is proposed to increase the\ndiversity of the population, which enhances the generalizability and\nrobustness. Experiments on CIFAR-10, CIFAR-100, and ImageNet1K datasets\ndemonstrate that the proposed method achieves high performance comparable to\nthat of many state-of-the-art peer methods, with lower computational cost and\ngreater robustness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u8ba1\u7b97\uff08EC\uff09\u7684\u9ad8\u6548\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u6846\u67b6\u89e3\u51b3\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u56fa\u5b9a\u5b66\u4e60\u7387\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfEC-NAS\u65b9\u6cd5\u4e2d\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u56fa\u5b9a\u5b66\u4e60\u7387\u5bfc\u81f4\u7684\u4fe1\u606f\u635f\u5931\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5143\u5b66\u4e60\u7387\uff08Meta-LR\uff09\u65b9\u6848\u548c\u81ea\u9002\u5e94\u4ee3\u7406\u6a21\u578b\uff0c\u7ed3\u5408\u5468\u671f\u6027\u53d8\u5f02\u7b97\u5b50\u589e\u5f3a\u591a\u6837\u6027\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cImageNet1K\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u4e14\u9c81\u68d2\u6027\u66f4\u5f3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.21582", "pdf": "https://arxiv.org/pdf/2504.21582", "abs": "https://arxiv.org/abs/2504.21582", "authors": ["Qirui Mi", "Mengyue Yang", "Xiangning Yu", "Zhiyu Zhao", "Cheng Deng", "Bo An", "Haifeng Zhang", "Xu Chen", "Jun Wang"], "title": "MF-LLM: Simulating Collective Decision Dynamics via a Mean-Field Large Language Model Framework", "categories": ["cs.MA", "cs.AI"], "comment": "27 pages, 8 figures, 4 tables", "summary": "Simulating collective decision-making involves more than aggregating\nindividual behaviors; it arises from dynamic interactions among individuals.\nWhile large language models (LLMs) show promise for social simulation, existing\napproaches often exhibit deviations from real-world data. To address this gap,\nwe propose the Mean-Field LLM (MF-LLM) framework, which explicitly models the\nfeedback loop between micro-level decisions and macro-level population. MF-LLM\nalternates between two models: a policy model that generates individual actions\nbased on personal states and group-level information, and a mean field model\nthat updates the population distribution from the latest individual decisions.\nTogether, they produce rollouts that simulate the evolving trajectories of\ncollective decision-making. To better match real-world data, we introduce\nIB-Tune, a fine-tuning method for LLMs grounded in the information bottleneck\nprinciple, which maximizes the relevance of population distributions to future\nactions while minimizing redundancy with historical data. We evaluate MF-LLM on\na real-world social dataset, where it reduces KL divergence to human population\ndistributions by 47 percent over non-mean-field baselines, and enables accurate\ntrend forecasting and intervention planning. It generalizes across seven\ndomains and four LLM backbones, providing a scalable foundation for\nhigh-fidelity social simulation.", "AI": {"tldr": "\u63d0\u51faMF-LLM\u6846\u67b6\uff0c\u901a\u8fc7\u5fae\u89c2\u51b3\u7b56\u4e0e\u5b8f\u89c2\u7fa4\u4f53\u7684\u53cd\u9988\u5faa\u73af\u6a21\u62df\u96c6\u4f53\u51b3\u7b56\uff0c\u7ed3\u5408IB-Tune\u4f18\u5316LLM\uff0c\u663e\u8457\u63d0\u5347\u6a21\u62df\u6548\u679c\u3002", "motivation": "\u73b0\u6709LLM\u65b9\u6cd5\u5728\u6a21\u62df\u96c6\u4f53\u51b3\u7b56\u65f6\u4e0e\u771f\u5b9e\u6570\u636e\u5b58\u5728\u504f\u5dee\uff0c\u9700\u6539\u8fdb\u3002", "method": "MF-LLM\u6846\u67b6\u4ea4\u66ff\u4f7f\u7528\u7b56\u7565\u6a21\u578b\u548c\u5747\u503c\u573a\u6a21\u578b\uff0c\u7ed3\u5408IB-Tune\u5fae\u8c03LLM\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cKL\u6563\u5ea6\u964d\u4f4e47%\uff0c\u652f\u6301\u8d8b\u52bf\u9884\u6d4b\u548c\u5e72\u9884\u89c4\u5212\u3002", "conclusion": "MF-LLM\u4e3a\u9ad8\u4fdd\u771f\u793e\u4f1a\u6a21\u62df\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2504.21585", "pdf": "https://arxiv.org/pdf/2504.21585", "abs": "https://arxiv.org/abs/2504.21585", "authors": ["Yingzhuo Jiang", "Wenjun Huang", "Rongdun Lin", "Chenyang Miao", "Tianfu Sun", "Yunduan Cui"], "title": "Multi-Goal Dexterous Hand Manipulation using Probabilistic Model-based Reinforcement Learning", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper tackles the challenge of learning multi-goal dexterous hand\nmanipulation tasks using model-based Reinforcement Learning. We propose\nGoal-Conditioned Probabilistic Model Predictive Control (GC-PMPC) by designing\nprobabilistic neural network ensembles to describe the high-dimensional\ndexterous hand dynamics and introducing an asynchronous MPC policy to meet the\ncontrol frequency requirements in real-world dexterous hand systems. Extensive\nevaluations on four simulated Shadow Hand manipulation scenarios with randomly\ngenerated goals demonstrate GC-PMPC's superior performance over\nstate-of-the-art baselines. It successfully drives a cable-driven Dexterous\nhand, DexHand 021 with 12 Active DOFs and 5 tactile sensors, to learn\nmanipulating a cubic die to three goal poses within approximately 80 minutes of\ninteractions, demonstrating exceptional learning efficiency and control\nperformance on a cost-effective dexterous hand platform.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff08GC-PMPC\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u76ee\u6807\u7075\u5de7\u624b\u64cd\u63a7\u4efb\u52a1\uff0c\u901a\u8fc7\u6982\u7387\u795e\u7ecf\u7f51\u7edc\u96c6\u5408\u548c\u9ad8\u9891\u5f02\u6b65MPC\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u5ea6\u7075\u5de7\u624b\u52a8\u6001\u5efa\u6a21\u548c\u5b9e\u65f6\u63a7\u5236\u9891\u7387\u8981\u6c42\u7684\u6311\u6218\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u76ee\u6807\u64cd\u63a7\u4efb\u52a1\u5b66\u4e60\u3002", "method": "\u8bbe\u8ba1\u4e86Goal-Conditioned Probabilistic Model Predictive Control (GC-PMPC)\uff0c\u7ed3\u5408\u6982\u7387\u795e\u7ecf\u7f51\u7edc\u96c6\u5408\u548c\u5f02\u6b65MPC\u7b56\u7565\u3002", "result": "\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0cGC-PMPC\u5728\u56db\u79cdShadow Hand\u64cd\u63a7\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645DexHand 021\u5e73\u53f0\u4e0a\u9ad8\u6548\u5b8c\u6210\u4efb\u52a1\u3002", "conclusion": "GC-PMPC\u5728\u7075\u5de7\u624b\u64cd\u63a7\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u4f4e\u6210\u672c\u5e73\u53f0\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21586", "pdf": "https://arxiv.org/pdf/2504.21586", "abs": "https://arxiv.org/abs/2504.21586", "authors": ["Robin Ferede", "Till Blaha", "Erin Lucassen", "Christophe De Wagter", "Guido C. H. E. de Croon"], "title": "One Net to Rule Them All: Domain Randomization in Quadcopter Racing Across Different Platforms", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "In high-speed quadcopter racing, finding a single controller that works well\nacross different platforms remains challenging. This work presents the first\nneural network controller for drone racing that generalizes across physically\ndistinct quadcopters. We demonstrate that a single network, trained with domain\nrandomization, can robustly control various types of quadcopters. The network\nrelies solely on the current state to directly compute motor commands. The\neffectiveness of this generalized controller is validated through real-world\ntests on two substantially different crafts (3-inch and 5-inch race\nquadcopters). We further compare the performance of this generalized controller\nwith controllers specifically trained for the 3-inch and 5-inch drone, using\ntheir identified model parameters with varying levels of domain randomization\n(0%, 10%, 20%, 30%). While the generalized controller shows slightly slower\nspeeds compared to the fine-tuned models, it excels in adaptability across\ndifferent platforms. Our results show that no randomization fails sim-to-real\ntransfer while increasing randomization improves robustness but reduces speed.\nDespite this trade-off, our findings highlight the potential of domain\nrandomization for generalizing controllers, paving the way for universal AI\ncontrollers that can adapt to any platform.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u901a\u7528\u65e0\u4eba\u673a\u7ade\u901f\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u57df\u968f\u673a\u5316\u8bad\u7ec3\uff0c\u80fd\u5728\u4e0d\u540c\u7269\u7406\u7279\u6027\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4e0a\u7a33\u5b9a\u5de5\u4f5c\u3002", "motivation": "\u89e3\u51b3\u9ad8\u901f\u65e0\u4eba\u673a\u7ade\u901f\u4e2d\u5355\u4e00\u63a7\u5236\u5668\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u5e73\u53f0\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u57df\u968f\u673a\u5316\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\uff0c\u4ec5\u4f9d\u8d56\u5f53\u524d\u72b6\u6001\u76f4\u63a5\u8ba1\u7b97\u7535\u673a\u6307\u4ee4\u3002", "result": "\u57283\u82f1\u5bf8\u548c5\u82f1\u5bf8\u7ade\u901f\u65e0\u4eba\u673a\u4e0a\u9a8c\u8bc1\u4e86\u63a7\u5236\u5668\u7684\u901a\u7528\u6027\uff0c\u5c3d\u7ba1\u901f\u5ea6\u7565\u4f4e\u4e8e\u4e13\u7528\u63a7\u5236\u5668\uff0c\u4f46\u9002\u5e94\u6027\u66f4\u5f3a\u3002", "conclusion": "\u57df\u968f\u673a\u5316\u80fd\u6709\u6548\u63d0\u5347\u63a7\u5236\u5668\u7684\u901a\u7528\u6027\uff0c\u4e3a\u901a\u7528AI\u63a7\u5236\u5668\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.21596", "pdf": "https://arxiv.org/pdf/2504.21596", "abs": "https://arxiv.org/abs/2504.21596", "authors": ["Huihui Guo", "Huilong Pi", "Yunchuan Qin", "Zhuo Tang", "Kenli Li"], "title": "Leveraging Pre-trained Large Language Models with Refined Prompting for Online Task and Motion Planning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "With the rapid advancement of artificial intelligence, there is an increasing\ndemand for intelligent robots capable of assisting humans in daily tasks and\nperforming complex operations. Such robots not only require task planning\ncapabilities but must also execute tasks with stability and robustness. In this\npaper, we present a closed-loop task planning and acting system, LLM-PAS, which\nis assisted by a pre-trained Large Language Model (LLM). While LLM-PAS plans\nlong-horizon tasks in a manner similar to traditional task and motion planners,\nit also emphasizes the execution phase of the task. By transferring part of the\nconstraint-checking process from the planning phase to the execution phase,\nLLM-PAS enables exploration of the constraint space and delivers more accurate\nfeedback on environmental anomalies during execution. The reasoning\ncapabilities of the LLM allow it to handle anomalies that cannot be addressed\nby the robust executor. To further enhance the system's ability to assist the\nplanner during replanning, we propose the First Look Prompting (FLP) method,\nwhich induces LLM to generate effective PDDL goals. Through comparative\nprompting experiments and systematic experiments, we demonstrate the\neffectiveness and robustness of LLM-PAS in handling anomalous conditions during\ntask execution.", "AI": {"tldr": "LLM-PAS\u662f\u4e00\u4e2a\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f85\u52a9\u7684\u95ed\u73af\u4efb\u52a1\u89c4\u5212\u4e0e\u6267\u884c\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u90e8\u5206\u7ea6\u675f\u68c0\u67e5\u8f6c\u79fb\u5230\u6267\u884c\u9636\u6bb5\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6267\u884c\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u667a\u80fd\u673a\u5668\u4eba\u9700\u8981\u5177\u5907\u4efb\u52a1\u89c4\u5212\u548c\u7a33\u5b9a\u6267\u884c\u7684\u80fd\u529b\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u4efb\u52a1\u548c\u73af\u5883\u5f02\u5e38\u3002", "method": "LLM-PAS\u7ed3\u5408\u4f20\u7edf\u4efb\u52a1\u89c4\u5212\u4e0e\u6267\u884c\u9636\u6bb5\uff0c\u5229\u7528LLM\u5904\u7406\u5f02\u5e38\uff0c\u5e76\u63d0\u51faFirst Look Prompting\uff08FLP\uff09\u65b9\u6cd5\u4f18\u5316\u89c4\u5212\u76ee\u6807\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLM-PAS\u5728\u5904\u7406\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u5f02\u5e38\u60c5\u51b5\u65f6\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "LLM-PAS\u901a\u8fc7\u95ed\u73af\u89c4\u5212\u548cLLM\u8f85\u52a9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6267\u884c\u80fd\u529b\u3002"}}
{"id": "2504.21695", "pdf": "https://arxiv.org/pdf/2504.21695", "abs": "https://arxiv.org/abs/2504.21695", "authors": ["Stavrow A. Bahnam", "Christophe De Wagter", "Guido C. H. E. de Croon"], "title": "Self-Supervised Monocular Visual Drone Model Identification through Improved Occlusion Handling", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Ego-motion estimation is vital for drones when flying in GPS-denied\nenvironments. Vision-based methods struggle when flight speed increases and\nclose-by objects lead to difficult visual conditions with considerable motion\nblur and large occlusions. To tackle this, vision is typically complemented by\nstate estimation filters that combine a drone model with inertial measurements.\nHowever, these drone models are currently learned in a supervised manner with\nground-truth data from external motion capture systems, limiting scalability to\ndifferent environments and drones. In this work, we propose a self-supervised\nlearning scheme to train a neural-network-based drone model using only onboard\nmonocular video and flight controller data (IMU and motor feedback). We achieve\nthis by first training a self-supervised relative pose estimation model, which\nthen serves as a teacher for the drone model. To allow this to work at high\nspeed close to obstacles, we propose an improved occlusion handling method for\ntraining self-supervised pose estimation models. Due to this method, the root\nmean squared error of resulting odometry estimates is reduced by an average of\n15%. Moreover, the student neural drone model can be successfully obtained from\nthe onboard data. It even becomes more accurate at higher speeds compared to\nits teacher, the self-supervised vision-based model. We demonstrate the value\nof the neural drone model by integrating it into a traditional filter-based VIO\nsystem (ROVIO), resulting in superior odometry accuracy on aggressive 3D racing\ntrajectories near obstacles. Self-supervised learning of ego-motion estimation\nrepresents a significant step toward bridging the gap between flying in\ncontrolled, expensive lab environments and real-world drone applications. The\nfusion of vision and drone models will enable higher-speed flight and improve\nstate estimation, on any drone in any environment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6848\uff0c\u901a\u8fc7\u5355\u76ee\u89c6\u9891\u548c\u98de\u884c\u63a7\u5236\u5668\u6570\u636e\u8bad\u7ec3\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65e0\u4eba\u673a\u6a21\u578b\uff0c\u89e3\u51b3\u4e86GPS\u7f3a\u5931\u73af\u5883\u4e0b\u7684\u8fd0\u52a8\u4f30\u8ba1\u95ee\u9898\u3002", "motivation": "\u5728GPS\u7f3a\u5931\u73af\u5883\u4e2d\uff0c\u65e0\u4eba\u673a\u8fd0\u52a8\u4f30\u8ba1\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u89c6\u89c9\u65b9\u6cd5\u5728\u9ad8\u901f\u98de\u884c\u548c\u590d\u6742\u89c6\u89c9\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u73b0\u6709\u65e0\u4eba\u673a\u6a21\u578b\u4f9d\u8d56\u5916\u90e8\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6848\uff0c\u5148\u8bad\u7ec3\u81ea\u76d1\u7763\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\uff0c\u518d\u8bad\u7ec3\u65e0\u4eba\u673a\u6a21\u578b\u3002\u6539\u8fdb\u906e\u6321\u5904\u7406\u65b9\u6cd5\uff0c\u63d0\u5347\u9ad8\u901f\u98de\u884c\u65f6\u7684\u6027\u80fd\u3002", "result": "\u4f4d\u59ff\u4f30\u8ba1\u7684\u5747\u65b9\u6839\u8bef\u5dee\u5e73\u5747\u964d\u4f4e15%\uff0c\u65e0\u4eba\u673a\u6a21\u578b\u5728\u9ad8\u901f\u98de\u884c\u65f6\u6bd4\u6559\u5e08\u6a21\u578b\u66f4\u51c6\u786e\u3002\u96c6\u6210\u5230VIO\u7cfb\u7edf\u540e\uff0c\u5728\u590d\u6742\u8f68\u8ff9\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u4e3a\u65e0\u4eba\u673a\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u9ad8\u901f\u98de\u884c\u548c\u72b6\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u7f29\u5c0f\u4e86\u5b9e\u9a8c\u5ba4\u4e0e\u771f\u5b9e\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2504.21719", "pdf": "https://arxiv.org/pdf/2504.21719", "abs": "https://arxiv.org/abs/2504.21719", "authors": ["Fay\u00e7al A\u00eft Aoudia", "Jakob Hoydis", "Merlin Nimier-David", "Sebastian Cammerer", "Alexander Keller"], "title": "Sionna RT: Technical Report", "categories": ["cs.IT", "cs.AI", "eess.SP", "math.IT"], "comment": null, "summary": "Sionna is an open-source, GPU-accelerated library that, as of version 0.14,\nincorporates a ray tracer for simulating radio wave propagation. A unique\nfeature of Sionna RT is differentiability, enabling the calculation of\ngradients for the channel impulse responses (CIRs), radio maps, and other\nrelated metrics with respect to system and environmental parameters, such as\nmaterial properties, antenna patterns, and array geometries. The release of\nSionna 1.0 provides a complete overhaul of the ray tracer, significantly\nimproving its speed, memory efficiency, and extensibility. This document\ndetails the algorithms employed by Sionna RT to simulate radio wave propagation\nefficiently, while also addressing their current limitations. Given that the\ncomputation of CIRs and radio maps requires distinct algorithms, these are\ndetailed in separate sections. For CIRs, Sionna RT integrates shooting and\nbouncing of rays (SBR) with the image method and uses a hashing-based mechanism\nto efficiently eliminate duplicate paths. Radio maps are computed using a\npurely SBR-based approach.", "AI": {"tldr": "Sionna 1.0\u662f\u4e00\u4e2a\u5f00\u6e90\u3001GPU\u52a0\u901f\u7684\u5e93\uff0c\u96c6\u6210\u4e86\u53ef\u5fae\u5206\u7684\u5c04\u7ebf\u8ffd\u8e2a\u5668\uff0c\u7528\u4e8e\u6a21\u62df\u65e0\u7ebf\u7535\u6ce2\u4f20\u64ad\uff0c\u5e76\u4f18\u5316\u4e86\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u63d0\u4f9b\u9ad8\u6548\u3001\u53ef\u5fae\u5206\u7684\u65e0\u7ebf\u7535\u6ce2\u4f20\u64ad\u6a21\u62df\u5de5\u5177\uff0c\u652f\u6301\u7cfb\u7edf\u4e0e\u73af\u5883\u53c2\u6570\u7684\u68af\u5ea6\u8ba1\u7b97\u3002", "method": "\u7ed3\u5408SBR\u4e0e\u56fe\u50cf\u65b9\u6cd5\u8ba1\u7b97CIR\uff0c\u4f7f\u7528\u54c8\u5e0c\u673a\u5236\u6d88\u9664\u91cd\u590d\u8def\u5f84\uff1b\u57fa\u4e8eSBR\u8ba1\u7b97\u65e0\u7ebf\u7535\u5730\u56fe\u3002", "result": "\u663e\u8457\u63d0\u5347\u4e86\u5c04\u7ebf\u8ffd\u8e2a\u5668\u7684\u901f\u5ea6\u3001\u5185\u5b58\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "Sionna RT\u4e3a\u65e0\u7ebf\u7535\u6ce2\u4f20\u64ad\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4ecd\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\u3002"}}
{"id": "2504.21772", "pdf": "https://arxiv.org/pdf/2504.21772", "abs": "https://arxiv.org/abs/2504.21772", "authors": ["Minwoo Oh", "Minsu Park", "Eunil Park"], "title": "Solving Copyright Infringement on Short Video Platforms: Novel Datasets and an Audio Restoration Deep Learning Pipeline", "categories": ["cs.MM", "cs.AI"], "comment": "will be presented in IJCAI 2025, 9 pages, 4 tables, 3 figures", "summary": "Short video platforms like YouTube Shorts and TikTok face significant\ncopyright compliance challenges, as infringers frequently embed arbitrary\nbackground music (BGM) to obscure original soundtracks (OST) and evade content\noriginality detection. To tackle this issue, we propose a novel pipeline that\nintegrates Music Source Separation (MSS) and cross-modal video-music retrieval\n(CMVMR). Our approach effectively separates arbitrary BGM from the original\nOST, enabling the restoration of authentic video audio tracks. To support this\nwork, we introduce two domain-specific datasets: OASD-20K for audio separation\nand OSVAR-160 for pipeline evaluation. OASD-20K contains 20,000 audio clips\nfeaturing mixed BGM and OST pairs, while OSVAR160 is a unique benchmark dataset\ncomprising 1,121 video and mixed-audio pairs, specifically designed for short\nvideo restoration tasks. Experimental results demonstrate that our pipeline not\nonly removes arbitrary BGM with high accuracy but also restores OSTs, ensuring\ncontent integrity. This approach provides an ethical and scalable solution to\ncopyright challenges in user-generated content on short video platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u97f3\u4e50\u6e90\u5206\u79bb\u548c\u8de8\u6a21\u6001\u89c6\u9891-\u97f3\u4e50\u68c0\u7d22\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u77ed\u89c6\u9891\u5e73\u53f0\u4e2d\u7684\u7248\u6743\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u4e2a\u4e13\u7528\u6570\u636e\u96c6\u3002", "motivation": "\u77ed\u89c6\u9891\u5e73\u53f0\u4e2d\uff0c\u4fb5\u6743\u8005\u5e38\u901a\u8fc7\u6dfb\u52a0\u80cc\u666f\u97f3\u4e50\u63a9\u76d6\u539f\u58f0\u4ee5\u9003\u907f\u539f\u521b\u68c0\u6d4b\uff0c\u5bfc\u81f4\u7248\u6743\u5408\u89c4\u95ee\u9898\u3002", "method": "\u6574\u5408\u97f3\u4e50\u6e90\u5206\u79bb\uff08MSS\uff09\u548c\u8de8\u6a21\u6001\u89c6\u9891-\u97f3\u4e50\u68c0\u7d22\uff08CMVMR\uff09\uff0c\u5206\u79bb\u80cc\u666f\u97f3\u4e50\u5e76\u6062\u590d\u539f\u58f0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u7cbe\u5ea6\u53bb\u9664\u80cc\u666f\u97f3\u4e50\u5e76\u6062\u590d\u539f\u58f0\uff0c\u786e\u4fdd\u5185\u5bb9\u5b8c\u6574\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u77ed\u89c6\u9891\u5e73\u53f0\u63d0\u4f9b\u4e86\u4f26\u7406\u548c\u53ef\u6269\u5c55\u7684\u7248\u6743\u95ee\u9898\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21848", "pdf": "https://arxiv.org/pdf/2504.21848", "abs": "https://arxiv.org/abs/2504.21848", "authors": ["Atoosa Kasirzadeh", "Iason Gabriel"], "title": "Characterizing AI Agents for Alignment and Governance", "categories": ["cs.CY", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "The creation of effective governance mechanisms for AI agents requires a\ndeeper understanding of their core properties and how these properties relate\nto questions surrounding the deployment and operation of agents in the world.\nThis paper provides a characterization of AI agents that focuses on four\ndimensions: autonomy, efficacy, goal complexity, and generality. We propose\ndifferent gradations for each dimension, and argue that each dimension raises\nunique questions about the design, operation, and governance of these systems.\nMoreover, we draw upon this framework to construct \"agentic profiles\" for\ndifferent kinds of AI agents. These profiles help to illuminate cross-cutting\ntechnical and non-technical governance challenges posed by different classes of\nAI agents, ranging from narrow task-specific assistants to highly autonomous\ngeneral-purpose systems. By mapping out key axes of variation and continuity,\nthis framework provides developers, policymakers, and members of the public\nwith the opportunity to develop governance approaches that better align with\ncollective societal goals.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2aAI\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u56f4\u7ed5\u81ea\u4e3b\u6027\u3001\u6548\u80fd\u3001\u76ee\u6807\u590d\u6742\u6027\u548c\u901a\u7528\u6027\u56db\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u6784\u5efa\u4e86\u4e0d\u540cAI\u4ee3\u7406\u7684\u201c\u4ee3\u7406\u6027\u6863\u6848\u201d\uff0c\u4ee5\u5e2e\u52a9\u89e3\u51b3\u6280\u672f\u548c\u975e\u6280\u672f\u6cbb\u7406\u6311\u6218\u3002", "motivation": "\u7406\u89e3AI\u4ee3\u7406\u7684\u6838\u5fc3\u5c5e\u6027\u53ca\u5176\u4e0e\u6cbb\u7406\u95ee\u9898\u7684\u5173\u7cfb\uff0c\u4ee5\u652f\u6301\u66f4\u6709\u6548\u7684AI\u6cbb\u7406\u673a\u5236\u3002", "method": "\u63d0\u51fa\u56db\u4e2a\u7ef4\u5ea6\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u4e3a\u4e0d\u540cAI\u4ee3\u7406\u6784\u5efa\u4ee3\u7406\u6027\u6863\u6848\u3002", "result": "\u6846\u67b6\u63ed\u793a\u4e86\u4e0d\u540cAI\u4ee3\u7406\u7684\u6cbb\u7406\u6311\u6218\uff0c\u5e76\u4e3a\u5f00\u53d1\u8005\u3001\u653f\u7b56\u5236\u5b9a\u8005\u548c\u516c\u4f17\u63d0\u4f9b\u4e86\u6cbb\u7406\u5de5\u5177\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u7b26\u5408\u793e\u4f1a\u76ee\u6807\u7684AI\u6cbb\u7406\u65b9\u6cd5\u3002"}}
{"id": "2504.21849", "pdf": "https://arxiv.org/pdf/2504.21849", "abs": "https://arxiv.org/abs/2504.21849", "authors": ["Justin B. Bullock", "Janet V. T. Pauketat", "Hsini Huang", "Yi-Fan Wang", "Jacy Reese Anthis"], "title": "Public Opinion and The Rise of Digital Minds: Perceived Risk, Trust, and Regulation Support", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "31 pages, 1 figure, 5 tables, accepted to Public Performance and\n  Management Review", "summary": "Governance institutions must respond to societal risks, including those posed\nby generative AI. This study empirically examines how public trust in\ninstitutions and AI technologies, along with perceived risks, shape preferences\nfor AI regulation. Using the nationally representative 2023 Artificial\nIntelligence, Morality, and Sentience (AIMS) survey, we assess trust in\ngovernment, AI companies, and AI technologies, as well as public support for\nregulatory measures such as slowing AI development or outright bans on advanced\nAI. Our findings reveal broad public support for AI regulation, with risk\nperception playing a significant role in shaping policy preferences.\nIndividuals with higher trust in government favor regulation, while those with\ngreater trust in AI companies and AI technologies are less inclined to support\nrestrictions. Trust in government and perceived risks significantly predict\npreferences for both soft (e.g., slowing development) and strong (e.g., banning\nAI systems) regulatory interventions. These results highlight the importance of\npublic opinion in AI governance. As AI capabilities advance, effective\nregulation will require balancing public concerns about risks with trust in\ninstitutions. This study provides a foundational empirical baseline for\npolicymakers navigating AI governance and underscores the need for further\nresearch into public trust, risk perception, and regulatory strategies in the\nevolving AI landscape.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u516c\u4f17\u5bf9AI\u76d1\u7ba1\u7684\u504f\u597d\uff0c\u53d1\u73b0\u4fe1\u4efb\u653f\u5e9c\u548c\u98ce\u9669\u611f\u77e5\u662f\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5e26\u6765\u7684\u793e\u4f1a\u98ce\u9669\u53ca\u516c\u4f17\u5bf9\u76d1\u7ba1\u7684\u6001\u5ea6\u3002", "method": "\u4f7f\u75282023\u5e74AIMS\u5168\u56fd\u4ee3\u8868\u6027\u8c03\u67e5\u6570\u636e\uff0c\u5206\u6790\u516c\u4f17\u5bf9\u653f\u5e9c\u3001AI\u516c\u53f8\u548c\u6280\u672f\u7684\u4fe1\u4efb\u53ca\u76d1\u7ba1\u652f\u6301\u3002", "result": "\u516c\u4f17\u666e\u904d\u652f\u6301AI\u76d1\u7ba1\uff0c\u4fe1\u4efb\u653f\u5e9c\u8005\u503e\u5411\u76d1\u7ba1\uff0c\u4fe1\u4efbAI\u516c\u53f8\u8005\u53cd\u5bf9\u9650\u5236\u3002\u98ce\u9669\u611f\u77e5\u663e\u8457\u5f71\u54cd\u653f\u7b56\u504f\u597d\u3002", "conclusion": "AI\u6cbb\u7406\u9700\u5e73\u8861\u516c\u4f17\u98ce\u9669\u62c5\u5fe7\u4e0e\u673a\u6784\u4fe1\u4efb\uff0c\u4e3a\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u5b9e\u8bc1\u57fa\u7840\u3002"}}
