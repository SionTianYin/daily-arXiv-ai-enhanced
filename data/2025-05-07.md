<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.CV](#cs.CV) [Total: 89]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.DC](#cs.DC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [math.NA](#math.NA) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [math.AP](#math.AP) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [stat.ML](#stat.ML) [Total: 10]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.MM](#cs.MM) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [cs.CY](#cs.CY) [Total: 7]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.RO](#cs.RO) [Total: 12]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [stat.CO](#stat.CO) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models](https://arxiv.org/abs/2505.02847)
*Bang Zhang,Ruotian Ma,Qingxuan Jiang,Peisong Wang,Jiaqi Chen,Zheng Xie,Xingyu Chen,Yue Wang,Fanghua Ye,Jian Li,Yifan Yang,Zhaopeng Tu,Xiaolong Li*

Main category: cs.CL

TL;DR: SAGE框架通过模拟人类情感变化和内心思考，评估大语言模型（LLM）的高阶社交认知能力，填补了现有评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以评估LLM对人类情感的理解，SAGE旨在提供更真实的评估工具。

Method: SAGE通过模拟情感变化、内心思考和回复生成，生成情感轨迹和可解释的内心思考。

Result: 实验验证了SAGE的情感评分与心理学指标高度相关，并揭示了前沿模型与早期基线之间的显著差距。

Conclusion: SAGE为评估LLM的社会认知能力提供了可扩展且可解释的工具。

Abstract: Assessing how well a large language model (LLM) understands human, rather
than merely text, remains an open challenge. To bridge the gap, we introduce
Sentient Agent as a Judge (SAGE), an automated evaluation framework that
measures an LLM's higher-order social cognition. SAGE instantiates a Sentient
Agent that simulates human-like emotional changes and inner thoughts during
interaction, providing a more realistic evaluation of the tested model in
multi-turn conversations. At every turn, the agent reasons about (i) how its
emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a
numerical emotion trajectory and interpretable inner thoughts. Experiments on
100 supportive-dialogue scenarios show that the final Sentient emotion score
correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings
and utterance-level empathy metrics, validating psychological fidelity. We also
build a public Sentient Leaderboard covering 18 commercial and open-source
models that uncovers substantial gaps (up to 4x) between frontier systems
(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in
conventional leaderboards (e.g., Arena). SAGE thus provides a principled,
scalable and interpretable tool for tracking progress toward genuinely
empathetic and socially adept language agents.

</details>


### [2] [Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors](https://arxiv.org/abs/2505.02850)
*Nicy Scaria,Silvester John Joseph Kennedy,Diksha Seth,Ananya Thakur,Deepak Subramani*

Main category: cs.CL

TL;DR: 论文提出了一种基于分层概念图的框架，利用LLM生成高质量MCQ，针对高中物理领域，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 手动生成高质量MCQ耗时且依赖专家知识，现有自动化方法无法覆盖高认知水平和领域特定误解。

Method: 开发分层概念图，通过自动化流程检索相关内容，指导LLM生成针对常见误解的MCQ和干扰项，并进行自动验证。

Result: 专家评估显示成功率75.20%，学生测试中猜测成功率28.05%，均优于基线方法。

Conclusion: 概念图方法能有效评估认知水平，快速识别概念差距，支持规模化反馈和干预。

Abstract: Generating high-quality MCQs, especially those targeting diverse cognitive
levels and incorporating common misconceptions into distractor design, is
time-consuming and expertise-intensive, making manual creation impractical at
scale. Current automated approaches typically generate questions at lower
cognitive levels and fail to incorporate domain-specific misconceptions. This
paper presents a hierarchical concept map-based framework that provides
structured knowledge to guide LLMs in generating MCQs with distractors. We
chose high-school physics as our test domain and began by developing a
hierarchical concept map covering major Physics topics and their
interconnections with an efficient database design. Next, through an automated
pipeline, topic-relevant sections of these concept maps are retrieved to serve
as a structured context for the LLM to generate questions and distractors that
specifically target common misconceptions. Lastly, an automated validation is
completed to ensure that the generated MCQs meet the requirements provided. We
evaluate our framework against two baseline approaches: a base LLM and a
RAG-based generation. We conducted expert evaluations and student assessments
of the generated MCQs. Expert evaluation shows that our method significantly
outperforms the baseline approaches, achieving a success rate of 75.20% in
meeting all quality criteria compared to approximately 37% for both baseline
methods. Student assessment data reveal that our concept map-driven approach
achieved a significantly lower guess success rate of 28.05% compared to 37.10%
for the baselines, indicating a more effective assessment of conceptual
understanding. The results demonstrate that our concept map-based approach
enables robust assessment across cognitive levels and instant identification of
conceptual gaps, facilitating faster feedback loops and targeted interventions
at scale.

</details>


### [3] [30DayGen: Leveraging LLMs to Create a Content Corpus for Habit Formation](https://arxiv.org/abs/2505.02851)
*Franklin Zhang,Sonya Zhang,Alon Halevy*

Main category: cs.CL

TL;DR: 30 Day Me是一个利用LLMs帮助用户分解目标并跟踪进度的习惯养成应用，核心是30DAYGEN系统，生成3,531种30天挑战。


<details>
  <summary>Details</summary>
Motivation: 利用LLMs快速构建领域特定内容库，支持行为和教育活动。

Method: 通过LLM增强的内容生成和语义去重管道，从15K网页中提取挑战。

Result: 生成了3,531种独特的30天挑战，支持用户目标对齐的实时搜索。

Conclusion: 展示了LLMs在行为和教育领域的实用潜力，提供了高效的内容生成方法。

Abstract: In this paper, we present 30 Day Me, a habit formation application that
leverages Large Language Models (LLMs) to help users break down their goals
into manageable, actionable steps and track their progress. Central to the app
is the 30DAYGEN system, which generates 3,531 unique 30-day challenges sourced
from over 15K webpages, and enables runtime search of challenge ideas aligned
with user-defined goals. We showcase how LLMs can be harnessed to rapidly
construct domain specific content corpora for behavioral and educational
purposes, and propose a practical pipeline that incorporates effective LLM
enhanced approaches for content generation and semantic deduplication.

</details>


### [4] [Ensuring Reproducibility in Generative AI Systems for General Use Cases: A Framework for Regression Testing and Open Datasets](https://arxiv.org/abs/2505.02854)
*Masumi Morishige,Ryo Koshihara*

Main category: cs.CL

TL;DR: GPR-bench是一个轻量级、可扩展的基准测试工具，用于评估生成式AI系统的可重复性和可靠性，覆盖多任务和多语言，但可能不足以区分最新模型版本。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统的行为可能因模型更新或提示调整而变化，亟需解决可重复性和可靠性问题。

Method: GPR-bench结合双语数据集（英语和日语）和自动化评估流程，使用“LLM-as-a-Judge”评分正确性和简洁性。

Result: 新模型在正确性上略有提升但不显著，简洁性提示显著改善输出简洁性（+12.37 pp），准确性仅轻微下降（-1.7 pp）。

Conclusion: GPR-bench为可重复性监控提供了基础，但需注意基准设计对快速演进的模型的适用性。

Abstract: Reproducibility and reliability remain pressing challenges for generative AI
systems whose behavior can drift with each model update or prompt revision. We
introduce GPR-bench, a lightweight, extensible benchmark that operationalizes
regression testing for general purpose use cases. GPR-bench couples an open,
bilingual (English and Japanese) dataset covering eight task categories (e.g.,
text generation, code generation, and information retrieval) and 10 scenarios
in each task categories (80 total test cases for each language) with an
automated evaluation pipeline that employs "LLM-as-a-Judge" scoring of
correctness and conciseness. Experiments across three recent model versions -
gpt-4o-mini, o3-mini, and o4-mini - and two prompt configurations (default
versus concise-writing instruction) reveal heterogeneous quality. Our results
show that newer models generally improve correctness, but the differences are
modest and not statistically significant, suggesting that GPR-bench may not be
sufficiently challenging to differentiate between recent model versions. In
contrast, the concise-writing instruction significantly enhances conciseness
(+12.37 pp, Mann-Whitney U test: p < 0.001, effect size r = 0.2995) with
minimal degradations on accuracy (-1.7 pp), demonstrating the effectiveness of
prompt engineering. Released under the MIT License, GPR- bench lowers the
barrier to initiating reproducibility monitoring and provides a foundation for
community-driven extensions, while also raising important considerations about
benchmark design for rapidly evolving language models.

</details>


### [5] [Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models](https://arxiv.org/abs/2505.02858)
*Henry Tari,Nojus Sereiva,Rishabh Kaushal,Thales Bertaglia,Adriana Iamnitchi*

Main category: cs.CL

TL;DR: 论文探讨了利用大语言模型生成跨平台社交媒体合成数据的潜力，提出了多平台主题提示方法，并评估了生成数据的质量。


<details>
  <summary>Details</summary>
Motivation: 社交媒体数据集对研究至关重要，但获取多平台数据成本高且受限，因此探索合成数据的可行性。

Method: 采用多平台主题提示方法，使用不同语言模型从真实数据生成合成数据，并评估其词汇和语义特性。

Result: 实验表明，大语言模型生成多平台合成数据具有潜力，不同模型表现各异，后处理可能提升数据保真度。

Conclusion: 研究为生成高质量多平台社交媒体合成数据提供了方法和指标，为未来研究奠定了基础。

Abstract: Social media datasets are essential for research on a variety of topics, such
as disinformation, influence operations, hate speech detection, or influencer
marketing practices. However, access to social media datasets is often
constrained due to costs and platform restrictions. Acquiring datasets that
span multiple platforms, which is crucial for understanding the digital
ecosystem, is particularly challenging. This paper explores the potential of
large language models to create lexically and semantically relevant social
media datasets across multiple platforms, aiming to match the quality of real
data. We propose multi-platform topic-based prompting and employ various
language models to generate synthetic data from two real datasets, each
consisting of posts from three different social media platforms. We assess the
lexical and semantic properties of the synthetic data and compare them with
those of the real data. Our empirical findings show that using large language
models to generate synthetic multi-platform social media data is promising,
different language models perform differently in terms of fidelity, and a
post-processing approach might be needed for generating high-fidelity synthetic
datasets for research. In addition to the empirical evaluation of three state
of the art large language models, our contributions include new fidelity
metrics specific to multi-platform social media datasets.

</details>


### [6] [Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI](https://arxiv.org/abs/2505.02859)
*Jonas Bokstaller,Julia Altheimer,Julian Dormehl,Alina Buss,Jasper Wiltfang,Johannes Schneider,Maximilian Röglinger*

Main category: cs.CL

TL;DR: 提出了一种结合XAI和LLM的交互式聊天机器人参考架构，用于提高ML模型的可解释性，并在电池健康状态预测中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着ML模型的复杂性增加，其黑盒特性成为问题，XAI的应用需求上升；同时LLM在理解人类语言和复杂模式方面取得进展。

Method: 设计了一个基于微调LLM的交互式聊天机器人参考架构，用于解释XAI。

Result: 在电池健康状态预测场景中验证了该架构，原型显著提升了ML的可解释性，尤其对XAI经验较少的用户。

Conclusion: 该参考架构成功结合XAI和LLM，提升了ML模型的可解释性，尤其在非专业用户中表现突出。

Abstract: Across various sectors applications of eXplainableAI (XAI) gained momentum as
the increasing black-boxedness of prevailing Machine Learning (ML) models
became apparent. In parallel, Large Language Models (LLMs) significantly
developed in their abilities to understand human language and complex patterns.
By combining both, this paper presents a novel reference architecture for the
interpretation of XAI through an interactive chatbot powered by a fine-tuned
LLM. We instantiate the reference architecture in the context of
State-of-Health (SoH) prediction for batteries and validate its design in
multiple evaluation and demonstration rounds. The evaluation indicates that the
implemented prototype enhances the human interpretability of ML, especially for
users with less experience with XAI.

</details>


### [7] [Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](https://arxiv.org/abs/2505.02862)
*Haoming Yang,Ke Ma,Xiaojun Jia,Yingfei Sun,Qianqian Xu,Qingming Huang*

Main category: cs.CL

TL;DR: 提出了一种基于人类认知启发的新框架ICRT，通过认知分解和相关性偏见优化恶意提示，有效绕过LLMs的安全机制，并引入排名评估方法量化危害。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能充分揭示LLMs在现实场景中的潜在风险，ICRT旨在通过认知启发方法提升攻击效果。

Method: 利用认知分解简化恶意提示，结合相关性偏见优化语义对齐，并采用排名聚合方法（如Elo、HodgeRank）评估危害。

Result: 实验表明ICRT能稳定绕过主流LLMs的安全机制，生成高风险内容。

Conclusion: ICRT为越狱攻击风险提供了新视角，有助于设计更强的防御策略。

Abstract: Despite the remarkable performance of Large Language Models (LLMs), they
remain vulnerable to jailbreak attacks, which can compromise their safety
mechanisms. Existing studies often rely on brute-force optimization or manual
design, failing to uncover potential risks in real-world scenarios. To address
this, we propose a novel jailbreak attack framework, ICRT, inspired by
heuristics and biases in human cognition. Leveraging the simplicity effect, we
employ cognitive decomposition to reduce the complexity of malicious prompts.
Simultaneously, relevance bias is utilized to reorganize prompts, enhancing
semantic alignment and inducing harmful outputs effectively. Furthermore, we
introduce a ranking-based harmfulness evaluation metric that surpasses the
traditional binary success-or-failure paradigm by employing ranking aggregation
methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify
the harmfulness of generated content. Experimental results show that our
approach consistently bypasses mainstream LLMs' safety mechanisms and generates
high-risk content, providing insights into jailbreak attack risks and
contributing to stronger defense strategies.

</details>


### [8] [Accelerating Large Language Model Reasoning via Speculative Search](https://arxiv.org/abs/2505.02865)
*Zhihai Wang,Jie Wang,Jilai Pan,Xilin Xia,Huiling Zhen,Mingxuan Yuan,Jianye Hao,Feng Wu*

Main category: cs.CL

TL;DR: 提出了一种名为SpecSearch的新框架，通过小模型与大模型的协作优化思维生成，显著加速LLM推理，同时保持推理质量。


<details>
  <summary>Details</summary>
Motivation: 解决基于树搜索的推理方法因生成大量中间思维步骤导致的高延迟问题，限制了LLM的适用性。

Method: 采用小模型与大模型在思维和标记级别协作，结合质量保持拒绝机制筛选高质量思维。

Result: 在Qwen和Llama模型上实验表明，SpecSearch比现有方法快2.12倍，推理质量相当。

Conclusion: SpecSearch通过优化思维生成显著提升推理速度，同时保持高质量，为LLM应用提供新思路。

Abstract: Tree-search-based reasoning methods have significantly enhanced the reasoning
capability of large language models (LLMs) by facilitating the exploration of
multiple intermediate reasoning steps, i.e., thoughts. However, these methods
suffer from substantial inference latency, as they have to generate numerous
reasoning thoughts, severely limiting LLM applicability. To address this
challenge, we propose a novel Speculative Search (SpecSearch) framework that
significantly accelerates LLM reasoning by optimizing thought generation.
Specifically, SpecSearch utilizes a small model to strategically collaborate
with a large model at both thought and token levels, efficiently generating
high-quality reasoning thoughts. The major pillar of SpecSearch is a novel
quality-preserving rejection mechanism, which effectively filters out thoughts
whose quality falls below that of the large model's outputs. Moreover, we show
that SpecSearch preserves comparable reasoning quality to the large model.
Experiments on both the Qwen and Llama models demonstrate that SpecSearch
significantly outperforms state-of-the-art approaches, achieving up to
2.12$\times$ speedup with comparable reasoning quality.

</details>


### [9] [Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading](https://arxiv.org/abs/2505.02872)
*Cfir Avraham Hadar,Omer Shubi,Yoav Meiri,Yevgeni Berzak*

Main category: cs.CL

TL;DR: 研究探讨是否可以通过眼动数据自动解码读者的开放式阅读目标，提出了目标分类和重建任务，并开发了多模态LLM模型，实验表明模型能有效从眼动中提取读者目标信息。


<details>
  <summary>Details</summary>
Motivation: 日常生活中，读者常带着特定目标阅读文本，但此前未研究过是否能通过眼动数据自动解码这些目标。

Method: 提出目标分类和重建任务，使用大规模英语阅读眼动数据，开发并比较了多种多模态LLM模型。

Result: 实验显示模型在目标分类和重建任务上表现良好，表明LLM能从眼动中提取读者目标信息。

Conclusion: LLM能有效解码读者眼动中的目标信息，为理解阅读行为提供了新工具。

Abstract: When reading, we often have specific information that interests us in a text.
For example, you might be reading this paper because you are curious about LLMs
for eye movements in reading, the experimental design, or perhaps you only care
about the question ``but does it work?''. More broadly, in daily life, people
approach texts with any number of text-specific goals that guide their reading
behavior. In this work, we ask, for the first time, whether open-ended reading
goals can be automatically decoded from eye movements in reading. To address
this question, we introduce goal classification and goal reconstruction tasks
and evaluation frameworks, and use large-scale eye tracking for reading data in
English with hundreds of text-specific information seeking tasks. We develop
and compare several discriminative and generative multimodal LLMs that combine
eye movements and text for goal classification and goal reconstruction. Our
experiments show considerable success on both tasks, suggesting that LLMs can
extract valuable information about the readers' text-specific goals from eye
movements.

</details>


### [10] [Logits-Constrained Framework with RoBERTa for Ancient Chinese NER](https://arxiv.org/abs/2505.02983)
*Wenjie Hua,Shenghan Xu*

Main category: cs.CL

TL;DR: 本文提出了一种Logits-Constrained（LC）框架用于古汉语命名实体识别（NER），在EvaHan 2025基准上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决古汉语NER任务中高标签或大数据场景下的性能问题。

Method: 结合GujiRoBERTa进行上下文编码，并使用可微分解码机制约束BMES标签转移。

Result: LC框架在性能上优于传统CRF和BiLSTM方法。

Conclusion: 提出的模型选择标准为实际古汉语NLP任务提供了实用指导。

Abstract: This paper presents a Logits-Constrained (LC) framework for Ancient Chinese
Named Entity Recognition (NER), evaluated on the EvaHan 2025 benchmark. Our
two-stage model integrates GujiRoBERTa for contextual encoding and a
differentiable decoding mechanism to enforce valid BMES label transitions.
Experiments demonstrate that LC improves performance over traditional CRF and
BiLSTM-based approaches, especially in high-label or large-data settings. We
also propose a model selection criterion balancing label complexity and dataset
size, providing practical guidance for real-world Ancient Chinese NLP tasks.

</details>


### [11] [RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale](https://arxiv.org/abs/2505.03005)
*Daniel Goldstein,Eric Alcaide,Janna Lu,Eugene Cheah*

Main category: cs.CL

TL;DR: RADLADS是一种快速将softmax注意力Transformer转换为线性注意力解码器模型的协议，并提出了两种新的RWKV变体架构。转换过程仅需350-700M token，成本低至2000美元，且推理质量接近原模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统Transformer模型的高计算成本问题，提出一种高效且低成本的转换方法。

Method: 通过RADLADS协议将softmax注意力Transformer转换为线性注意力解码器模型，并开发了两种RWKV变体架构。

Result: 转换后的模型在标准基准测试中表现优异，推理质量接近原模型，且成本极低。

Conclusion: RADLADS提供了一种高效、低成本的模型转换方法，适用于大规模模型，并开源了相关模型和代码。

Abstract: We present Rapid Attention Distillation to Linear Attention Decoders at Scale
(RADLADS), a protocol for rapidly converting softmax attention transformers
into linear attention decoder models, along with two new RWKV-variant
architectures, and models converted from popular Qwen2.5 open source models in
7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,
less than 0.005% of the token count used to train the original teacher models.
Converting to our 72B linear attention model costs less than \$2,000 USD at
today's prices, yet quality at inference remains close to the original
transformer. These models achieve state-of-the-art downstream performance
across a set of standard benchmarks for linear attention models of their size.
We release all our models on HuggingFace under the Apache 2.0 license, with the
exception of our 72B models which are also governed by the Qwen License
Agreement.
  Models at
https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102
Training Code at https://github.com/recursal/RADLADS-paper

</details>


### [12] [Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis](https://arxiv.org/abs/2505.03019)
*Albérick Euraste Djiré,Abdoul Kader Kaboré,Earl T. Barr,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: PEARL是一种检测大型语言模型（LLM）记忆现象的新方法，通过输入扰动评估模型输出的一致性，区分真实泛化与记忆。


<details>
  <summary>Details</summary>
Motivation: LLM在训练中可能记忆而非泛化数据，引发隐私、知识产权和评估可靠性问题。

Method: PEARL通过输入扰动检测模型输出的敏感性，无需访问模型内部。

Result: 在Pythia和GPT 4o模型上验证，成功识别记忆现象，如经典文本和代码。

Conclusion: PEARL为LLM记忆检测提供了有效框架，支持识别训练数据来源。

Abstract: While Large Language Models (LLMs) achieve remarkable performance through
training on massive datasets, they can exhibit concerning behaviors such as
verbatim reproduction of training data rather than true generalization. This
memorization phenomenon raises significant concerns about data privacy,
intellectual property rights, and the reliability of model evaluations. This
paper introduces PEARL, a novel approach for detecting memorization in LLMs.
PEARL assesses how sensitive an LLM's performance is to input perturbations,
enabling memorization detection without requiring access to the model's
internals. We investigate how input perturbations affect the consistency of
outputs, enabling us to distinguish between true generalization and
memorization. Our findings, following extensive experiments on the Pythia open
model, provide a robust framework for identifying when the model simply
regurgitates learned information. Applied on the GPT 4o models, the PEARL
framework not only identified cases of memorization of classic texts from the
Bible or common code from HumanEval but also demonstrated that it can provide
supporting evidence that some data, such as from the New York Times news
articles, were likely part of the training data of a given model.

</details>


### [13] [A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts](https://arxiv.org/abs/2505.03025)
*Steven Bedrick,A. Seza Doğruöz,Sergiu Nisioi*

Main category: cs.CL

TL;DR: 本文探讨了在医疗领域中合成数据集的创建、评估和应用，并提出了一种新的分类法以帮助比较和评估。


<details>
  <summary>Details</summary>
Motivation: 由于临床对话数据的敏感性和收集难度，合成数据集被广泛使用，但缺乏理论指导其最佳使用和泛化方法。

Method: 提供了合成数据集的创建和评估方法，并提出了一种新的分类法。

Result: 总结了合成数据集在医疗对话任务中的应用，并提出了分类法。

Conclusion: 合成数据集在医疗领域有潜力，但需要更系统的理论和方法支持。

Abstract: Synthetic data sets are used across linguistic domains and NLP tasks,
particularly in scenarios where authentic data is limited (or even
non-existent). One such domain is that of clinical (healthcare) contexts, where
there exist significant and long-standing challenges (e.g., privacy,
anonymization, and data governance) which have led to the development of an
increasing number of synthetic datasets. One increasingly important category of
clinical dataset is that of clinical dialogues which are especially sensitive
and difficult to collect, and as such are commonly synthesized.
  While such synthetic datasets have been shown to be sufficient in some
situations, little theory exists to inform how they may be best used and
generalized to new applications. In this paper, we provide an overview of how
synthetic datasets are created, evaluated and being used for dialogue related
tasks in the medical domain. Additionally, we propose a novel typology for use
in classifying types and degrees of data synthesis, to facilitate comparison
and evaluation.

</details>


### [14] [UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output](https://arxiv.org/abs/2505.03030)
*Sicong Huang,Jincheng He,Shiyuan Huang,Karthik Raja Anandan,Arkajyoti Chakraborty,Ian Lane*

Main category: cs.CL

TL;DR: UCSC团队提出了一种检测和定位大语言模型幻觉的框架，并在Mu-SHROOM任务中取得最佳表现。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在知识密集型查询中的幻觉问题，并精确定位幻觉发生的位置。

Method: 框架包括检索相关上下文、识别答案中的虚假内容，并将其映射回LLM输出中的具体位置，同时通过自动优化提示增强效果。

Result: 系统在所有语言中平均排名第一，表现最佳。

Conclusion: UCSC的系统在Mu-SHROOM任务中表现优异，代码和实验结果已公开。

Abstract: Hallucinations pose a significant challenge for large language models when
answering knowledge-intensive queries. As LLMs become more widely adopted, it
is crucial not only to detect if hallucinations occur but also to pinpoint
exactly where in the LLM output they occur. SemEval 2025 Task 3, Mu-SHROOM:
Multilingual Shared-task on Hallucinations and Related Observable
Overgeneration Mistakes, is a recent effort in this direction. This paper
describes the UCSC system submission to the shared Mu-SHROOM task. We introduce
a framework that first retrieves relevant context, next identifies false
content from the answer, and finally maps them back to spans in the LLM output.
The process is further enhanced by automatically optimizing prompts. Our system
achieves the highest overall performance, ranking #1 in average position across
all languages. We release our code and experiment results.

</details>


### [15] [Teaching Models to Understand (but not Generate) High-risk Data](https://arxiv.org/abs/2505.03052)
*Ryan Wang,Matthew Finlayson,Luca Soldaini,Swabha Swayamdipta,Robin Jia*

Main category: cs.CL

TL;DR: SLUNG是一种预训练范式，通过选择性损失让模型理解高风险内容但不生成它，从而提升模型对高风险数据的识别能力而不增加其生成。


<details>
  <summary>Details</summary>
Motivation: 传统方法过滤高风险内容会限制模型识别和应对有害内容的能力，SLUNG旨在解决这一问题。

Method: SLUNG选择性避免激励高风险标记的生成，同时确保模型理解这些内容。

Result: 实验表明SLUNG提升模型对高风险数据的理解能力，而不增加其生成。

Conclusion: SLUNG使模型能从高风险文本中受益，而无需完全过滤它们。

Abstract: Language model developers typically filter out high-risk content -- such as
toxic or copyrighted text -- from their pre-training data to prevent models
from generating similar outputs. However, removing such data altogether limits
models' ability to recognize and appropriately respond to harmful or sensitive
content. In this paper, we introduce Selective Loss to Understand but Not
Generate (SLUNG), a pre-training paradigm through which models learn to
understand high-risk data without learning to generate it. Instead of uniformly
applying the next-token prediction loss, SLUNG selectively avoids incentivizing
the generation of high-risk tokens while ensuring they remain within the
model's context window. As the model learns to predict low-risk tokens that
follow high-risk ones, it is forced to understand the high-risk content.
Through our experiments, we show that SLUNG consistently improves models'
understanding of high-risk data (e.g., ability to recognize toxic content)
without increasing its generation (e.g., toxicity of model responses). Overall,
our SLUNG paradigm enables models to benefit from high-risk text that would
otherwise be filtered out.

</details>


### [16] [Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text](https://arxiv.org/abs/2505.03053)
*Jennifer Healey,Laurie Byrum,Md Nadeem Akhtar,Surabhi Bhargava,Moumita Sinha*

Main category: cs.CL

TL;DR: 论文探讨了LLM评估的挑战，提出了一个半自动化的偏见评估框架，结合人类洞察和自动化方法。


<details>
  <summary>Details</summary>
Motivation: 现实部署中，LLM评估因任务特定提示和上下文交互而复杂化，传统短上下文基准可能失效，大规模人类评估成本高。

Method: 开发了半自动化的偏见评估框架，结合人类洞察和自动化方法，定义偏见并扩展分类方法。

Result: 通过人类评估发现了偏见基准中的问题模板，验证了框架的实用性。

Conclusion: 半自动化框架结合人类和自动化方法，有效解决了LLM偏见评估的挑战。

Abstract: LLM evaluation is challenging even the case of base models. In real world
deployments, evaluation is further complicated by the interplay of task
specific prompts and experiential context. At scale, bias evaluation is often
based on short context, fixed choice benchmarks that can be rapidly evaluated,
however, these can lose validity when the LLMs' deployed context differs. Large
scale human evaluation is often seen as too intractable and costly. Here we
present our journey towards developing a semi-automated bias evaluation
framework for free text responses that has human insights at its core. We
discuss how we developed an operational definition of bias that helped us
automate our pipeline and a methodology for classifying bias beyond multiple
choice. We additionally comment on how human evaluation helped us uncover
problematic templates in a bias benchmark.

</details>


### [17] [Improving Model Alignment Through Collective Intelligence of Open-Source LLMS](https://arxiv.org/abs/2505.03059)
*Junlin Wang,Roy Xie,Shang Zhu,Jue Wang,Ben Athiwaratkun,Bhuwan Dhingra,Shuaiwen Leon Song,Ce Zhang,James Zou*

Main category: cs.CL

TL;DR: 论文提出了一种名为MoAA的方法，通过结合多种语言模型的优势生成高质量对齐数据，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 构建高质量的人类标注数据成本高且难以扩展，限制了模型的多样性和泛化能力。

Method: 采用Mixture of Agents Alignment (MoAA)方法，利用多种语言模型的集体优势生成对齐数据。

Result: MoAA显著提升了LLaMA-3.1-8B-Instruct的胜率，并在AlpacaEval2等基准上表现优异。

Conclusion: MoAA为开源大语言模型的对齐提供了一种可扩展且多样化的解决方案，无需依赖外部监督。

Abstract: Building helpful and harmless large language models (LLMs) requires effective
model alignment approach based on human instructions and feedback, which
necessitates high-quality human-labeled data. Constructing such datasets is
often expensive and hard to scale, and may face potential limitations on
diversity and generalization. To address these challenges, we introduce Mixture
of Agents Alignment (MoAA), that leverages the collective strengths of various
language models to provide high-quality data for model alignment. By employing
MoAA, we enhance both supervised fine-tuning and preference optimization,
leading to improved performance compared to using a single model alone to
generate alignment data (e.g. using GPT-4o alone). Evaluation results show that
our approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on
Arena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising
direction for model alignment through this new scalable and diverse synthetic
data recipe. Furthermore, we demonstrate that MoAA enables a self-improvement
pipeline, where models finetuned on MoA-generated data surpass their own
initial capabilities, providing evidence that our approach can push the
frontier of open-source LLMs without reliance on stronger external supervision.
Data and code will be released.

</details>


### [18] [Survey of Abstract Meaning Representation: Then, Now, Future](https://arxiv.org/abs/2505.03229)
*Behrooz Mansouri*

Main category: cs.CL

TL;DR: 本文综述了抽象意义表示（AMR）及其扩展，探讨了其解析与生成任务，并回顾了AMR在文本生成、分类和信息提取等应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: AMR作为一种基于图的语义表示框架，能够有效编码复杂句子的意义，研究其发展和应用有助于提升机器对人类语言的理解。

Method: 通过分析AMR及其扩展的能力，调查了文本到AMR的解析和AMR到文本的生成任务，并总结了传统、当前及未来可能的方法。

Result: 综述揭示了AMR在文本生成、分类和信息提取等领域的广泛应用，同时指出了当前研究的挑战和发展方向。

Conclusion: AMR在提升机器语言理解方面具有巨大潜力，未来研究需进一步解决现有挑战并探索新的应用场景。

Abstract: This paper presents a survey of Abstract Meaning Representation (AMR), a
semantic representation framework that captures the meaning of sentences
through a graph-based structure. AMR represents sentences as rooted, directed
acyclic graphs, where nodes correspond to concepts and edges denote
relationships, effectively encoding the meaning of complex sentences. This
survey investigates AMR and its extensions, focusing on AMR capabilities. It
then explores the parsing (text-to-AMR) and generation (AMR-to-text) tasks by
showing traditional, current, and possible futures approaches. It also reviews
various applications of AMR including text generation, text classification, and
information extraction and information seeking. By analyzing recent
developments and challenges in the field, this survey provides insights into
future directions for research and the potential impact of AMR on enhancing
machine understanding of human language.

</details>


### [19] [Ψ-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback](https://arxiv.org/abs/2505.03293)
*Shijing Zhu,Zhuang Chen,Guanqun Bi,Binghang Li,Yaxi Deng,Dazhen Wan,Libiao Peng,Xiyao Xiao,Rongsheng Zhang,Tangjie Lv,Zhipeng Hu,FangFang Li,Minlie Huang*

Main category: cs.CL

TL;DR: 论文提出了{\Psi}-Arena框架，用于全面评估和优化基于LLM的心理咨询师，通过多阶段对话、三方评估和闭环优化提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法在静态测试、单一视角和开放框架上存在局限，无法全面评估LLM心理咨询师的能力。

Method: 提出{\Psi}-Arena框架，包括多阶段对话模拟、三方评估（客户、咨询师、监督者）和闭环优化。

Result: 实验显示不同LLM在真实场景中表现差异显著，优化后咨询性能提升高达141%。

Conclusion: PsychoArena为心理健康领域可靠且人性化的LLM应用提供了基础资源。

Abstract: Large language models (LLMs) have shown promise in providing scalable mental
health support, while evaluating their counseling capability remains crucial to
ensure both efficacy and safety. Existing evaluations are limited by the static
assessment that focuses on knowledge tests, the single perspective that centers
on user experience, and the open-loop framework that lacks actionable feedback.
To address these issues, we propose {\Psi}-Arena, an interactive framework for
comprehensive assessment and optimization of LLM-based counselors, featuring
three key characteristics: (1) Realistic arena interactions that simulate
real-world counseling through multi-stage dialogues with psychologically
profiled NPC clients, (2) Tripartite evaluation that integrates assessments
from the client, counselor, and supervisor perspectives, and (3) Closed-loop
optimization that iteratively improves LLM counselors using diagnostic
feedback. Experiments across eight state-of-the-art LLMs show significant
performance variations in different real-world scenarios and evaluation
perspectives. Moreover, reflection-based optimization results in up to a 141%
improvement in counseling performance. We hope PsychoArena provides a
foundational resource for advancing reliable and human-aligned LLM applications
in mental healthcare.

</details>


### [20] [Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation](https://arxiv.org/abs/2505.03320)
*Junyu Ma,Tianqing Fang,Zhisong Zhang,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 通过Recall with Reasoning (RwR)方法，提升Mamba模型的长上下文记忆能力，无需架构改动。


<details>
  <summary>Details</summary>
Motivation: Mamba在理论上具有无限上下文潜力，但在实际中当序列远超训练长度时表现受限。

Method: 使用RwR方法，通过从教师模型中提取链式思维（CoT）摘要，并在微调时将其作为CoT提示前置，教导Mamba主动回忆和推理长上下文。

Result: 在LONGMEMEVAL和HELMET上的实验显示，RwR提升了Mamba的长上下文性能，同时保留了短上下文能力。

Conclusion: RwR是一种简单有效的方法，能够解锁Mamba的长上下文记忆潜力，且无需改变模型架构。

Abstract: Mamba's theoretical infinite-context potential is limited in practice when
sequences far exceed training lengths. This work explores unlocking Mamba's
long-context memory ability by a simple-yet-effective method, Recall with
Reasoning (RwR), by distilling chain-of-thought (CoT) summarization from a
teacher model. Specifically, RwR prepends these summarization as CoT prompts
during fine-tuning, teaching Mamba to actively recall and reason over long
contexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's
long-context performance against comparable Transformer/hybrid baselines under
similar pretraining conditions, while preserving short-context capabilities,
all without architectural changes.

</details>


### [21] [Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation](https://arxiv.org/abs/2505.03406)
*Mohammad Shoaib Ansari,Mohd Sohail Ali Khan,Shubham Revankar,Aditya Varma,Anil S. Mokhade*

Main category: cs.CL

TL;DR: 该研究探讨了如何将大型语言模型（LLMs）应用于医疗领域，通过结合检索增强生成（RAG）和量化低秩适应（QLoRA）技术，提升医疗决策支持的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 医疗决策支持系统需要高准确性和效率，而LLMs结合特定医院数据和优化技术可以满足这一需求。

Method: 使用Llama 3.2-3B-Instruct作为基础模型，结合RAG和QLoRA技术，嵌入和检索医疗信息，优化参数和内存。

Result: 系统显著提高了响应准确性，并在多个医疗基准测试中表现良好，适用于基本医疗建议。

Conclusion: 研究展示了LLMs在医疗领域的潜力，同时强调了伦理和实际挑战，并提出了未来发展方向。

Abstract: This research paper investigates the application of Large Language Models
(LLMs) in healthcare, specifically focusing on enhancing medical decision
support through Retrieval-Augmented Generation (RAG) integrated with
hospital-specific data and fine-tuning using Quantized Low-Rank Adaptation
(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By
embedding and retrieving context-relevant healthcare information, the system
significantly improves response accuracy. QLoRA facilitates notable parameter
efficiency and memory optimization, preserving the integrity of medical
information through specialized quantization techniques. Our research also
shows that our model performs relatively well on various medical benchmarks,
indicating that it can be used to make basic medical suggestions. This paper
details the system's technical components, including its architecture,
quantization methods, and key healthcare applications such as enhanced disease
prediction from patient symptoms and medical history, treatment suggestions,
and efficient summarization of complex medical reports. We touch on the ethical
considerations-patient privacy, data security, and the need for rigorous
clinical validation-as well as the practical challenges of integrating such
systems into real-world healthcare workflows. Furthermore, the lightweight
quantized weights ensure scalability and ease of deployment even in
low-resource hospital environments. Finally, the paper concludes with an
analysis of the broader impact of LLMs on healthcare and outlines future
directions for LLMs in medical settings.

</details>


### [22] [MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks](https://arxiv.org/abs/2505.03427)
*Mouath Abu Daoud,Chaimae Abouzahir,Leen Kharouf,Walid Al-Eisawi,Nizar Habash,Farah E. Shamout*

Main category: cs.CL

TL;DR: 该研究提出了MedArabiQ，一个阿拉伯语医疗领域的基准数据集，用于评估大型语言模型（LLMs）的性能，并强调了多语言基准的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏高质量的阿拉伯语医疗领域数据集和基准，LLMs在该领域的效能尚未被充分探索。

Method: 研究构建了包含七项阿拉伯语医疗任务的MedArabiQ数据集，并对五种先进LLMs进行了评估。

Result: 评估结果显示，需要更多高质量的多语言基准以确保LLMs在医疗领域的公平部署和扩展性。

Conclusion: 通过发布MedArabiQ数据集，研究为未来评估和增强LLMs的多语言能力奠定了基础。

Abstract: Large Language Models (LLMs) have demonstrated significant promise for
various applications in healthcare. However, their efficacy in the Arabic
medical domain remains unexplored due to the lack of high-quality
domain-specific datasets and benchmarks. This study introduces MedArabiQ, a
novel benchmark dataset consisting of seven Arabic medical tasks, covering
multiple specialties and including multiple choice questions,
fill-in-the-blank, and patient-doctor question answering. We first constructed
the dataset using past medical exams and publicly available datasets. We then
introduced different modifications to evaluate various LLM capabilities,
including bias mitigation. We conducted an extensive evaluation with five
state-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude
3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of
new high-quality benchmarks that span different languages to ensure fair
deployment and scalability of LLMs in healthcare. By establishing this
benchmark and releasing the dataset, we provide a foundation for future
research aimed at evaluating and enhancing the multilingual capabilities of
LLMs for the equitable use of generative AI in healthcare.

</details>


### [23] [An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation](https://arxiv.org/abs/2505.03452)
*Matan Orbach,Ohad Eytan,Benjamin Sznajder,Ariel Gera,Odellia Boni,Yoav Kantor,Gal Bloch,Omri Levy,Hadas Abraham,Nitzan Barzilay,Eyal Shnarch,Michael E. Factor,Shila Ofek-Koifman,Paula Ta-Shma,Assaf Toledo*

Main category: cs.CL

TL;DR: 研究探讨了检索增强生成（RAG）的超参数优化（HPO）方法，通过5种算法和5个数据集验证了其有效性，发现贪婪或随机搜索能高效提升性能。


<details>
  <summary>Details</summary>
Motivation: RAG配置优化复杂且昂贵，现有HPO框架缺乏严格基准测试。

Method: 使用5种HPO算法在5个数据集（包括新产品文档数据集）上进行实验，探索最大搜索空间。

Result: 贪婪或随机搜索能高效提升RAG性能，且优先优化模型优于按流程顺序优化。

Conclusion: RAG HPO可高效实施，显著提升性能，优化顺序应以模型优先。

Abstract: Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a
given use case can be complex and expensive. Motivated by this challenge,
frameworks for RAG hyper-parameter optimization (HPO) have recently emerged,
yet their effectiveness has not been rigorously benchmarked. To address this
gap, we present a comprehensive study involving 5 HPO algorithms over 5
datasets from diverse domains, including a new one collected for this work on
real-world product documentation. Our study explores the largest HPO search
space considered to date, with two optimized evaluation metrics. Analysis of
the results shows that RAG HPO can be done efficiently, either greedily or with
iterative random search, and that it significantly boosts RAG performance for
all datasets. For greedy HPO approaches, we show that optimizing models first
is preferable to the prevalent practice of optimizing sequentially according to
the RAG pipeline order.

</details>


### [24] [Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis](https://arxiv.org/abs/2505.03467)
*Shuang Zhou,Jiashuo Wang,Zidu Xu,Song Wang,David Brauer,Lindsay Welton,Jacob Cogan,Yuen-Hei Chung,Lei Tian,Zaifu Zhan,Yu Hou,Mingquan Lin,Genevieve B. Melton,Rui Zhang*

Main category: cs.CL

TL;DR: ConfiDx是一个基于不确定性感知的大型语言模型（LLM），用于解决诊断不确定性识别和解释问题，提升自动诊断系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 临床笔记中证据不足时会导致诊断不确定性，增加误诊风险，但目前对诊断不确定性的识别和解释研究不足。

Method: 通过微调开源LLM并整合诊断标准，构建了ConfiDx模型，并使用标注数据集评估其性能。

Result: ConfiDx在识别诊断不确定性、诊断性能和生成可信解释方面表现优异。

Conclusion: 该研究首次联合解决诊断不确定性的识别和解释问题，显著提升了自动诊断系统的可靠性。

Abstract: Explainable disease diagnosis, which leverages patient information (e.g.,
signs and symptoms) and computational models to generate probable diagnoses and
reasonings, offers clear clinical values. However, when clinical notes
encompass insufficient evidence for a definite diagnosis, such as the absence
of definitive symptoms, diagnostic uncertainty usually arises, increasing the
risk of misdiagnosis and adverse outcomes. Although explicitly identifying and
explaining diagnostic uncertainties is essential for trustworthy diagnostic
systems, it remains under-explored. To fill this gap, we introduce ConfiDx, an
uncertainty-aware large language model (LLM) created by fine-tuning open-source
LLMs with diagnostic criteria. We formalized the task and assembled richly
annotated datasets that capture varying degrees of diagnostic ambiguity.
Evaluating ConfiDx on real-world datasets demonstrated that it excelled in
identifying diagnostic uncertainties, achieving superior diagnostic
performance, and generating trustworthy explanations for diagnoses and
uncertainties. To our knowledge, this is the first study to jointly address
diagnostic uncertainty recognition and explanation, substantially enhancing the
reliability of automatic diagnostic systems.

</details>


### [25] [Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models](https://arxiv.org/abs/2505.03469)
*Bin Yu,Hang Yuan,Yuliang Wei,Bailing Wang,Weizhen Qi,Kai Chen*

Main category: cs.CL

TL;DR: 论文提出LS-Mixture SFT方法，通过结合长链和短链推理数据，解决SFT中模型推理冗余问题，提升准确率并减少响应长度。


<details>
  <summary>Details</summary>
Motivation: 现有SFT方法通过CoT推理数据传递推理能力，但会导致模型推理冗余（"overthinking"问题），影响效率。

Method: 提出LS-Mixture SFT，结合长链推理数据和结构保留重写的短链数据，优化模型推理过程。

Result: 实验显示，LS-Mixture SFT平均准确率提升2.3%，响应长度减少47.61%。

Conclusion: LS-Mixture SFT有效赋予非推理模型推理能力，同时避免冗余推理，提升效率。

Abstract: Recent advances in large language models have demonstrated that Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from
large reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning
capabilities to non-reasoning models. However, models fine-tuned with this
approach inherit the "overthinking" problem from teacher models, producing
verbose and redundant reasoning chains during inference. To address this
challenge, we propose \textbf{L}ong-\textbf{S}hort Chain-of-Thought
\textbf{Mixture} \textbf{S}upervised \textbf{F}ine-\textbf{T}uning
(\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their
short counterparts obtained through structure-preserved rewriting. Our
experiments demonstrate that models trained using the LS-Mixture SFT method,
compared to those trained with direct SFT, achieved an average accuracy
improvement of 2.3\% across various benchmarks while substantially reducing
model response length by approximately 47.61\%. This work offers an approach to
endow non-reasoning models with reasoning capabilities through supervised
fine-tuning while avoiding the inherent overthinking problems inherited from
teacher models, thereby enabling efficient reasoning in the fine-tuned models.

</details>


### [26] [Evaluation of LLMs on Long-tail Entity Linking in Historical Documents](https://arxiv.org/abs/2505.03473)
*Marta Boscariol,Luana Bulla,Lia Draetta,Beatrice Fiumanò,Emanuele Lenzi,Leonardo Piano*

Main category: cs.CL

TL;DR: 论文探讨了LLMs在长尾实体链接（EL）任务中的表现，发现其性能优于传统方法，填补了主流与长尾实体链接的差距。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在长尾实体链接中的潜力，因传统方法对长尾实体表现不佳。

Method: 使用GPT和LLama3两种LLMs，在MHERCL v0.1数据集上与ReLiK框架进行对比实验。

Result: LLMs在长尾EL任务中表现良好，显示出填补主流与长尾实体链接差距的潜力。

Conclusion: LLMs可作为长尾实体链接的有力补充工具。

Abstract: Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)
applications, enabling the disambiguation of entity mentions by linking them to
their corresponding entries in a reference knowledge base (KB). Thanks to their
deep contextual understanding capabilities, LLMs offer a new perspective to
tackle EL, promising better results than traditional methods. Despite the
impressive generalization capabilities of LLMs, linking less popular, long-tail
entities remains challenging as these entities are often underrepresented in
training data and knowledge bases. Furthermore, the long-tail EL task is an
understudied problem, and limited studies address it with LLMs. In the present
work, we assess the performance of two popular LLMs, GPT and LLama3, in a
long-tail entity linking scenario. Using MHERCL v0.1, a manually annotated
benchmark of sentences from domain-specific historical texts, we quantitatively
compare the performance of LLMs in identifying and linking entities to their
corresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity
Linking and Relation Extraction framework. Our preliminary experiments reveal
that LLMs perform encouragingly well in long-tail EL, indicating that this
technology can be a valuable adjunct in filling the gap between head and
long-tail EL.

</details>


### [27] [Sentence Embeddings as an intermediate target in end-to-end summarisation](https://arxiv.org/abs/2505.03481)
*Maciej Zembrzuski,Saad Mahamood*

Main category: cs.CL

TL;DR: 提出一种结合抽取式和生成式方法的新方法，用于处理大规模用户评论摘要任务，通过预训练句子嵌入提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络方法在处理大规模输入数据集时表现不佳。

Method: 结合抽取式方法和预训练句子嵌入，再与生成式模型结合。

Result: 新方法在大规模输入数据集上优于现有方法，且预测句子嵌入能提升端到端系统的质量。

Conclusion: 结合抽取式与生成式方法及句子嵌入预测，能有效提升大规模输入摘要任务的表现。

Abstract: Current neural network-based methods to the problem of document summarisation
struggle when applied to datasets containing large inputs. In this paper we
propose a new approach to the challenge of content-selection when dealing with
end-to-end summarisation of user reviews of accommodations. We show that by
combining an extractive approach with externally pre-trained sentence level
embeddings in an addition to an abstractive summarisation model we can
outperform existing methods when this is applied to the task of summarising a
large input dataset. We also prove that predicting sentence level embedding of
a summary increases the quality of an end-to-end system for loosely aligned
source to target corpora, than compared to commonly predicting probability
distributions of sentence selection.

</details>


### [28] [Faster MoE LLM Inference for Extremely Large Models](https://arxiv.org/abs/2505.03531)
*Haoqi Yang,Luohe Shi,Qiwei Li,Zuchao Li,Ping Wang,Bo Du,Mengjia Shen,Hai Zhao*

Main category: cs.CL

TL;DR: 稀疏混合专家（MoE）大语言模型（LLM）正成为超大规模模型的主流方法。本文探讨了细粒度MoE模型在不同服务负载下的效率动态，并研究了减少专家数量对效率与性能权衡的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在粗粒度MoE架构上，而细粒度MoE模型的研究较少。本文旨在填补这一空白，探讨其效率动态和优化潜力。

Method: 通过分析细粒度MoE模型在不同服务负载下的表现，研究减少激活专家数量和总专家数量对效率与性能的影响。

Result: 减少激活专家数量可在某些场景下显著提升效率，仅带来轻微性能下降；减少总专家数量效率提升有限，但性能下降严重。方法可将吞吐量提升至少10%且无性能损失。

Conclusion: MoE推理优化仍具巨大探索和改进潜力，细粒度模型为优化提供了新机会。

Abstract: Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually
becoming the mainstream approach for ultra-large-scale models. Existing
optimization efforts for MoE models have focused primarily on coarse-grained
MoE architectures. With the emergence of DeepSeek Models, fine-grained MoE
models are gaining popularity, yet research on them remains limited. Therefore,
we want to discuss the efficiency dynamic under different service loads.
Additionally, fine-grained models allow deployers to reduce the number of
routed experts, both activated counts and total counts, raising the question of
how this reduction affects the trade-off between MoE efficiency and
performance. Our findings indicate that while deploying MoE models presents
greater challenges, it also offers significant optimization opportunities.
Reducing the number of activated experts can lead to substantial efficiency
improvements in certain scenarios, with only minor performance degradation.
Reducing the total number of experts provides limited efficiency gains but
results in severe performance degradation. Our method can increase throughput
by at least 10\% without any performance degradation. Overall, we conclude that
MoE inference optimization remains an area with substantial potential for
exploration and improvement.

</details>


### [29] [Say It Another Way: A Framework for User-Grounded Paraphrasing](https://arxiv.org/abs/2505.03563)
*Cléa Chataigner,Rebecca Ma,Prakhar Ganesh,Afaf Taïk,Elliot Creager,Golnoosh Farnadi*

Main category: cs.CL

TL;DR: 论文研究了提示词微小变化对大型语言模型（LLM）行为的影响，提出了基于语言学转换的受控改述框架，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 提示词的微小变化可能导致LLM行为显著差异，现有研究未能涵盖自然语言的实际变化，需系统性方法评估模型稳定性。

Method: 提出基于语言学转换分类的受控改述框架，利用BBQ数据集通过人工标注和自动化检查验证方法。

Result: 研究发现即使是细微的提示词修改也会显著改变模型行为。

Conclusion: 强调了需要开发对改述敏感的鲁棒评估协议。

Abstract: Small changes in how a prompt is worded can lead to meaningful differences in
the behavior of large language models (LLMs), raising concerns about the
stability and reliability of their evaluations. While prior work has explored
simple formatting changes, these rarely capture the kinds of natural variation
seen in real-world language use. We propose a controlled paraphrasing framework
based on a taxonomy of minimal linguistic transformations to systematically
generate natural prompt variations. Using the BBQ dataset, we validate our
method with both human annotations and automated checks, then use it to study
how LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our
analysis shows that even subtle prompt modifications can lead to substantial
changes in model behavior. These results highlight the need for robust,
paraphrase-aware evaluation protocols.

</details>


### [30] [Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure](https://arxiv.org/abs/2505.03675)
*Anuja Tayal,Devika Salunke,Barbara Di Eugenio,Paula G Allen-Meares,Eulalia P Abril,Olga Garcia-Bedoya,Carolyn A Dickens,Andrew D. Boyd*

Main category: cs.CL

TL;DR: 研究探索了ChatGPT（3.5-turbo和4）为非洲裔美国心衰患者生成自我护理对话的潜力，发现有效提示设计是关键，但ChatGPT仍缺乏医疗沟通所需的同理心和参与度。


<details>
  <summary>Details</summary>
Motivation: 非洲裔美国心衰患者的自我护理领域缺乏专门数据集，研究旨在填补这一空白。

Method: 采用四种提示策略（领域、AAVE、SDOH、SDOH推理）生成对话，涵盖食物、运动和液体摄入等关键领域，并结合患者特定SDOH属性。

Result: SDOH和推理提示提升了对话质量，但ChatGPT在同理心和参与度上仍有不足。

Conclusion: 提示设计对生成高质量医疗对话至关重要，但需进一步改进ChatGPT的情感表达能力。

Abstract: We explore the potential of ChatGPT (3.5-turbo and 4) to generate
conversations focused on self-care strategies for African-American heart
failure patients -- a domain with limited specialized datasets. To simulate
patient-health educator dialogues, we employed four prompting strategies:
domain, African American Vernacular English (AAVE), Social Determinants of
Health (SDOH), and SDOH-informed reasoning. Conversations were generated across
key self-care domains of food, exercise, and fluid intake, with varying turn
lengths (5, 10, 15) and incorporated patient-specific SDOH attributes such as
age, gender, neighborhood, and socioeconomic status. Our findings show that
effective prompt design is essential. While incorporating SDOH and reasoning
improves dialogue quality, ChatGPT still lacks the empathy and engagement
needed for meaningful healthcare communication.

</details>


### [31] [IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages](https://arxiv.org/abs/2505.03688)
*Sharvi Endait,Ruturaj Ghatage,Aditya Kulkarni,Rajlaxmi Patil,Raviraj Joshi*

Main category: cs.CL

TL;DR: IndicSQuAD是一个多语言抽取式QA数据集，覆盖九种主要印度语言，旨在解决印度语言在QA系统中的低资源问题。


<details>
  <summary>Details</summary>
Motivation: 高资源语言在QA系统中进展迅速，而印度语言尽管有大量母语使用者，却代表性不足。

Method: 通过翻译和扩展SQuAD数据集，保持语言保真度和答案跨度对齐，构建IndicSQuAD数据集。

Result: 使用单语和多语BERT模型评估，结果显示低资源环境下的挑战。

Conclusion: 未来工作可扩展语言、开发领域特定数据集和引入多模态数据。

Abstract: The rapid progress in question-answering (QA) systems has predominantly
benefited high-resource languages, leaving Indic languages largely
underrepresented despite their vast native speaker base. In this paper, we
present IndicSQuAD, a comprehensive multi-lingual extractive QA dataset
covering nine major Indic languages, systematically derived from the SQuAD
dataset. Building on previous work with MahaSQuAD for Marathi, our approach
adapts and extends translation techniques to maintain high linguistic fidelity
and accurate answer-span alignment across diverse languages. IndicSQuAD
comprises extensive training, validation, and test sets for each language,
providing a robust foundation for model development. We evaluate baseline
performances using language-specific monolingual BERT models and the
multilingual MuRIL-BERT. The results indicate some challenges inherent in
low-resource settings. Moreover, our experiments suggest potential directions
for future work, including expanding to additional languages, developing
domain-specific datasets, and incorporating multimodal data. The dataset and
models are publicly shared at https://github.com/l3cube-pune/indic-nlp

</details>


### [32] [NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation](https://arxiv.org/abs/2505.03711)
*Baharul Islam,Nasim Ahmad,Ferdous Ahmed Barbhuiya,Kuntal Dey*

Main category: cs.CL

TL;DR: 本文介绍了SemEval 2025任务5的系统提交，专注于英德学术领域的跨语言主题分类，采用双语数据训练和负采样方法，展示了低资源下的竞争力。


<details>
  <summary>Details</summary>
Motivation: 解决跨语言主题分类问题，特别是在英德学术领域，同时优化资源使用。

Method: 利用双语数据训练，结合负采样和基于边界的检索目标，采用维度为标记的自注意力机制。

Result: 系统在通用定量评估中平均召回率为32.24%，定性评估中为43.16%和31.53%，表现具有竞争力。

Conclusion: 方法在资源限制下有效，但仍需改进。

Abstract: We present our system submission for SemEval 2025 Task 5, which focuses on
cross-lingual subject classification in the English and German academic
domains. Our approach leverages bilingual data during training, employing
negative sampling and a margin-based retrieval objective. We demonstrate that a
dimension-as-token self-attention mechanism designed with significantly reduced
internal dimensions can effectively encode sentence embeddings for subject
retrieval. In quantitative evaluation, our system achieved an average recall
rate of 32.24% in the general quantitative setting (all subjects), 43.16% and
31.53% of the general qualitative evaluation methods with minimal GPU usage,
highlighting their competitive performance. Our results demonstrate that our
approach is effective in capturing relevant subject information under resource
constraints, although there is still room for improvement.

</details>


### [33] [WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch](https://arxiv.org/abs/2505.03733)
*Zimu Lu,Yunqiao Yang,Houxing Ren,Haotian Hou,Han Xiao,Ke Wang,Weikang Shi,Aojun Zhou,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 论文介绍了WebGen-Bench，一个用于评估LLM代理生成多文件网站代码能力的基准测试，包含多样化的指令和647个测试用例。测试结果显示当前最佳模型准确率仅为27.8%，表明任务的挑战性。


<details>
  <summary>Details</summary>
Motivation: 评估LLM代理在生成复杂网站代码方面的能力，推动相关技术的发展。

Method: 通过人类和GPT-4o合作创建多样化指令，生成测试用例，并使用自动化代理执行测试。评估了三种代码代理框架和多种LLM。

Result: 最佳组合（Bolt.diy + DeepSeek-R1）准确率为27.8%，训练后的Qwen2.5-Coder-32B-Instruct达到38.2%。

Conclusion: WebGen-Bench是一个具有挑战性的基准测试，展示了当前技术的局限性，并为未来研究提供了方向。

Abstract: LLM-based agents have demonstrated great potential in generating and managing
code within complex codebases. In this paper, we introduce WebGen-Bench, a
novel benchmark designed to measure an LLM-based agent's ability to create
multi-file website codebases from scratch. It contains diverse instructions for
website generation, created through the combined efforts of human annotators
and GPT-4o. These instructions span three major categories and thirteen minor
categories, encompassing nearly all important types of web applications. To
assess the quality of the generated websites, we use GPT-4o to generate test
cases targeting each functionality described in the instructions, and then
manually filter, adjust, and organize them to ensure accuracy, resulting in 647
test cases. Each test case specifies an operation to be performed on the
website and the expected result after the operation. To automate testing and
improve reproducibility, we employ a powerful web-navigation agent to execute
tests on the generated websites and determine whether the observed responses
align with the expected results. We evaluate three high-performance code-agent
frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and
open-source LLMs as engines. The best-performing combination, Bolt.diy powered
by DeepSeek-R1, achieves only 27.8\% accuracy on the test cases, highlighting
the challenging nature of our benchmark. Additionally, we construct
WebGen-Instruct, a training set consisting of 6,667 website-generation
instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories
generated from a subset of this training set achieves an accuracy of 38.2\%,
surpassing the performance of the best proprietary model.

</details>


### [34] [VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model](https://arxiv.org/abs/2505.03739)
*Zuwei Long,Yunhang Shen,Chaoyou Fu,Heting Gao,Lijiang Li,Peixian Chen,Mengdan Zhang,Hang Shao,Jian Li,Jinlong Peng,Haoyu Cao,Ke Li,Rongrong Ji,Xing Sun*

Main category: cs.CL

TL;DR: VITA-Audio是一种端到端的大型语音模型，通过轻量级多模态令牌预测模块和四阶段渐进训练策略，显著降低了语音生成的首令牌延迟，提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有语音模型在流式生成首音频令牌时存在高延迟问题，限制了部署效率。

Method: 引入轻量级多模态令牌预测模块（MCTP）和四阶段渐进训练策略，加速推理并减少首令牌延迟。

Result: 在7B参数规模下，推理速度提升3~5倍，并在ASR、TTS和SQA任务上优于同类开源模型。

Conclusion: VITA-Audio是首个能在首次前向传递中生成音频的多模态大语言模型，具有低延迟和实时对话能力。

Abstract: With the growing requirement for natural human-computer interaction,
speech-based systems receive increasing attention as speech is one of the most
common forms of daily communication. However, the existing speech models still
experience high latency when generating the first audio token during streaming,
which poses a significant bottleneck for deployment. To address this issue, we
propose VITA-Audio, an end-to-end large speech model with fast audio-text token
generation. Specifically, we introduce a lightweight Multiple Cross-modal Token
Prediction (MCTP) module that efficiently generates multiple audio tokens
within a single model forward pass, which not only accelerates the inference
but also significantly reduces the latency for generating the first audio in
streaming scenarios. In addition, a four-stage progressive training strategy is
explored to achieve model acceleration with minimal loss of speech quality. To
our knowledge, VITA-Audio is the first multi-modal large language model capable
of generating audio output during the first forward pass, enabling real-time
conversational capabilities with minimal latency. VITA-Audio is fully
reproducible and is trained on open-source data only. Experimental results
demonstrate that our model achieves an inference speedup of 3~5x at the 7B
parameter scale, but also significantly outperforms open-source models of
similar model size on multiple benchmarks for automatic speech recognition
(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [35] [RESAnything: Attribute Prompting for Arbitrary Referring Segmentation](https://arxiv.org/abs/2505.02867)
*Ruiqi Wang,Hao Zhang*

Main category: cs.CV

TL;DR: 提出了一种开放词汇和零样本的任意参考表达分割（RES）方法，名为RESAnything，通过Chain-of-Thoughts推理和属性提示处理对象和部分级别的标签及隐式参考。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法无法处理的更广泛的输入表达，包括对象/部分级别的标签和隐式参考（如功能、设计、风格等）。

Method: 利用Chain-of-Thoughts推理和属性提示，通过大型语言模型生成详细的对象/部分属性描述，结合基础图像分割模型生成分割提案。

Result: 在传统RES基准测试中表现优于其他零样本方法，并在隐式查询和复杂部分级关系场景中显著优于现有方法。

Conclusion: RESAnything是首个零样本和基于LLM的RES方法，成功处理了隐式查询，并贡献了一个新的基准数据集用于评估部分级RES解决方案。

Abstract: We present an open-vocabulary and zero-shot method for arbitrary referring
expression segmentation (RES), targeting input expressions that are more
general than what prior works were designed to handle. Specifically, our inputs
encompass both object- and part-level labels as well as implicit references
pointing to properties or qualities of object/part function, design, style,
material, etc. Our model, coined RESAnything, leverages Chain-of-Thoughts (CoT)
reasoning, where the key idea is attribute prompting. We generate detailed
descriptions of object/part attributes including shape, color, and location for
potential segment proposals through systematic prompting of a large language
model (LLM), where the proposals are produced by a foundational image
segmentation model. Our approach encourages deep reasoning about object or part
attributes related to function, style, design, etc., enabling the system to
handle implicit queries without any part annotations for training or
fine-tuning. As the first zero-shot and LLM-based RES method, RESAnything
achieves clearly superior performance among zero-shot methods on traditional
RES benchmarks and significantly outperforms existing methods on challenging
scenarios involving implicit queries and complex part-level relations. Finally,
we contribute a new benchmark dataset to offer ~3K carefully curated RES
instances to assess part-level, arbitrary RES solutions.

</details>


### [36] [Gone With the Bits: Revealing Racial Bias in Low-Rate Neural Compression for Facial Images](https://arxiv.org/abs/2505.02949)
*Tian Qiu,Arjun Nichani,Rasta Tadayontahmasebi,Haewon Jeong*

Main category: cs.CV

TL;DR: 该论文提出了一种评估神经图像压缩模型中偏见的框架，并揭示了这些模型在种族偏见上的普遍存在，同时探讨了偏见与图像真实性的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 神经压缩方法在极低比特率下表现优异，但深度学习架构可能导致训练过程中的偏见，影响不同群体的公平性。

Method: 提出了一个结构化、可扩展的框架，用于评估神经压缩模型的偏见，并分析了九种流行模型及其变体。

Result: 研究发现传统失真指标无法有效捕捉偏见，所有神经压缩模型均存在种族偏见，且偏见与图像真实性之间存在权衡。

Conclusion: 使用种族平衡的训练集可以减少偏见，但不足以完全消除偏见。该研究为评估和消除神经图像压缩模型中的偏见迈出了第一步。

Abstract: Neural compression methods are gaining popularity due to their superior
rate-distortion performance over traditional methods, even at extremely low
bitrates below 0.1 bpp. As deep learning architectures, these models are prone
to bias during the training process, potentially leading to unfair outcomes for
individuals in different groups. In this paper, we present a general,
structured, scalable framework for evaluating bias in neural image compression
models. Using this framework, we investigate racial bias in neural compression
algorithms by analyzing nine popular models and their variants. Through this
investigation, we first demonstrate that traditional distortion metrics are
ineffective in capturing bias in neural compression models. Next, we highlight
that racial bias is present in all neural compression models and can be
captured by examining facial phenotype degradation in image reconstructions. We
then examine the relationship between bias and realism in the decoded images
and demonstrate a trade-off across models. Finally, we show that utilizing a
racially balanced training set can reduce bias but is not a sufficient bias
mitigation strategy. We additionally show the bias can be attributed to
compression model bias and classification model bias. We believe that this work
is a first step towards evaluating and eliminating bias in neural image
compression models.

</details>


### [37] [Generating Narrated Lecture Videos from Slides with Synchronized Highlights](https://arxiv.org/abs/2505.02966)
*Alexander Holmberg*

Main category: cs.CV

TL;DR: 该系统通过AI生成同步的视觉高亮和旁白，将静态幻灯片自动转化为视频讲座，显著节省时间和成本。


<details>
  <summary>Details</summary>
Motivation: 将静态幻灯片转化为视频讲座通常需要大量时间和人力，系统旨在自动化这一过程。

Method: 系统采用高亮对齐模块，结合多种策略（如Levenshtein距离、LLM语义分析）和TTS同步，实现语音与视觉高亮的精确匹配。

Result: 在1000个样本的数据集上，LLM对齐方法达到高精度（F1 > 92%），生成成本低于1美元/小时。

Conclusion: 该系统以高精度和低成本为优势，为幻灯片视频化提供实用且可扩展的解决方案。

Abstract: Turning static slides into engaging video lectures takes considerable time
and effort, requiring presenters to record explanations and visually guide
their audience through the material. We introduce an end-to-end system designed
to automate this process entirely. Given a slide deck, this system synthesizes
a video lecture featuring AI-generated narration synchronized precisely with
dynamic visual highlights. These highlights automatically draw attention to the
specific concept being discussed, much like an effective presenter would. The
core technical contribution is a novel highlight alignment module. This module
accurately maps spoken phrases to locations on a given slide using diverse
strategies (e.g., Levenshtein distance, LLM-based semantic analysis) at
selectable granularities (line or word level) and utilizes timestamp-providing
Text-to-Speech (TTS) for timing synchronization. We demonstrate the system's
effectiveness through a technical evaluation using a manually annotated slide
dataset with 1000 samples, finding that LLM-based alignment achieves high
location accuracy (F1 > 92%), significantly outperforming simpler methods,
especially on complex, math-heavy content. Furthermore, the calculated
generation cost averages under $1 per hour of video, offering potential savings
of two orders of magnitude compared to conservative estimates of manual
production costs. This combination of high accuracy and extremely low cost
positions this approach as a practical and scalable tool for transforming
static slides into effective, visually-guided video lectures.

</details>


### [38] [Adversarial Robustness Analysis of Vision-Language Models in Medical Image Segmentation](https://arxiv.org/abs/2505.02971)
*Anjila Budathoki,Manish Dhakal*

Main category: cs.CV

TL;DR: 本文研究了视觉语言分割模型（VLSMs）在医学图像分析中对对抗攻击的鲁棒性，发现其性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击在计算机视觉和视觉语言模型中已有研究，但在医学图像分析中的VLSMs领域仍未被充分探索。

Method: 通过微调预训练的VLSMs并应用PGD和FGSM对抗攻击，评估模型鲁棒性。

Result: 对抗攻击导致DSC和IoU分数显著下降，但未找到适用于医学图像的通用扰动。

Conclusion: VLSMs在医学图像分析中对对抗攻击较为脆弱，需进一步研究提升其鲁棒性。

Abstract: Adversarial attacks have been fairly explored for computer vision and
vision-language models. However, the avenue of adversarial attack for the
vision language segmentation models (VLSMs) is still under-explored, especially
for medical image analysis.
  Thus, we have investigated the robustness of VLSMs against adversarial
attacks for 2D medical images with different modalities with radiology,
photography, and endoscopy. The main idea of this project was to assess the
robustness of the fine-tuned VLSMs specially in the medical domain setting to
address the high risk scenario.
  First, we have fine-tuned pre-trained VLSMs for medical image segmentation
with adapters.
  Then, we have employed adversarial attacks -- projected gradient descent
(PGD) and fast gradient sign method (FGSM) -- on that fine-tuned model to
determine its robustness against adversaries.
  We have reported models' performance decline to analyze the adversaries'
impact.
  The results exhibit significant drops in the DSC and IoU scores after the
introduction of these adversaries. Furthermore, we also explored universal
perturbation but were not able to find for the medical images.
  \footnote{https://github.com/anjilab/secure-private-ai}

</details>


### [39] [Completing Spatial Transcriptomics Data for Gene Expression Prediction Benchmarking](https://arxiv.org/abs/2505.02980)
*Daniela Ruiz,Paula Cardenas,Leonardo Manrique,Daniela Vega,Gabriel Mejia,Pablo Arbelaez*

Main category: cs.CV

TL;DR: SpaRED和SpaCKLE为空间转录组学提供了标准化数据库和高效基因表达预测模型，显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决Visium技术的高成本、数据丢失问题，以及现有模型评估的不一致性。

Method: 引入SpaRED标准化数据库和SpaCKLE基于Transformer的基因表达补全模型。

Result: SpaCKLE将均方误差降低82.5%，显著提升所有预测模型的表现。

Conclusion: SpaRED和SpaCKLE为空间转录组学研究提供了全面基准和未来研究方向。

Abstract: Spatial Transcriptomics is a groundbreaking technology that integrates
histology images with spatially resolved gene expression profiles. Among the
various Spatial Transcriptomics techniques available, Visium has emerged as the
most widely adopted. However, its accessibility is limited by high costs, the
need for specialized expertise, and slow clinical integration. Additionally,
gene capture inefficiencies lead to significant dropout, corrupting acquired
data. To address these challenges, the deep learning community has explored the
gene expression prediction task directly from histology images. Yet,
inconsistencies in datasets, preprocessing, and training protocols hinder fair
comparisons between models. To bridge this gap, we introduce SpaRED, a
systematically curated database comprising 26 public datasets, providing a
standardized resource for model evaluation. We further propose SpaCKLE, a
state-of-the-art transformer-based gene expression completion model that
reduces mean squared error by over 82.5% compared to existing approaches.
Finally, we establish the SpaRED benchmark, evaluating eight state-of-the-art
prediction models on both raw and SpaCKLE-completed data, demonstrating SpaCKLE
substantially improves the results across all the gene expression prediction
models. Altogether, our contributions constitute the most comprehensive
benchmark of gene expression prediction from histology images to date and a
stepping stone for future research on Spatial Transcriptomics.

</details>


### [40] [NTIRE 2025 Challenge on UGC Video Enhancement: Methods and Results](https://arxiv.org/abs/2505.03007)
*Nikolay Safonov,Alexey Bryncev,Andrey Moskalenko,Dmitry Kulikov,Dmitry Vatolin,Radu Timofte,Haibo Lei,Qifan Gao,Qing Luo,Yaqing Li,Jie Song,Shaozhe Hao,Meisong Zheng,Jingyi Xu,Chengbin Wu,Jiahui Liu,Ying Chen,Xin Deng,Mai Xu,Peipei Liang,Jie Ma,Junjie Jin,Yingxue Pang,Fangzhou Luo,Kai Chen,Shijie Zhao,Mingyang Wu,Renjie Li,Yushen Zuo,Shengyun Zhong,Zhengzhong Tu*

Main category: cs.CV

TL;DR: NTIRE 2025挑战赛聚焦UGC视频增强，25支团队参与，7支通过终审，结果公开。


<details>
  <summary>Details</summary>
Motivation: UGC视频在短视频平台广泛使用，但存在多种质量问题，需提升视觉质量。

Method: 挑战赛提供150个无参考视频，要求开发算法改善质量，评估基于8000多人的主观评分。

Result: 7支团队通过终审，结果揭示了UGC视频增强的最新趋势和有效策略。

Conclusion: 挑战赛成果公开，为UGC视频增强领域提供参考和启发。

Abstract: This paper presents an overview of the NTIRE 2025 Challenge on UGC Video
Enhancement. The challenge constructed a set of 150 user-generated content
videos without reference ground truth, which suffer from real-world
degradations such as noise, blur, faded colors, compression artifacts, etc. The
goal of the participants was to develop an algorithm capable of improving the
visual quality of such videos. Given the widespread use of UGC on short-form
video platforms, this task holds substantial practical importance. The
evaluation was based on subjective quality assessment in crowdsourcing,
obtaining votes from over 8000 assessors. The challenge attracted more than 25
teams submitting solutions, 7 of which passed the final phase with source code
verification. The outcomes may provide insights into the state-of-the-art in
UGC video enhancement and highlight emerging trends and effective strategies in
this evolving research area. All data, including the processed videos and
subjective comparison votes and scores, is made publicly available at
https://github.com/msu-video-group/NTIRE25_UGC_Video_Enhancement.

</details>


### [41] [GIF: Generative Inspiration for Face Recognition at Scale](https://arxiv.org/abs/2505.03012)
*Saeed Ebrahimi,Sahar Rahimi,Ali Dabouei,Srinjoy Das,Jeremy M. Dawson,Nasser M. Nasrabadi*

Main category: cs.CV

TL;DR: 提出一种结构化身份编码方法，将标量标签替换为整数序列，显著降低人脸识别中Softmax的计算成本。


<details>
  <summary>Details</summary>
Motivation: 减少大规模标签空间中人脸识别Softmax的计算成本，现有方法仅能线性降低计算量。

Method: 设计一种标记化方案，将标量标签转换为结构化身份编码，训练模型预测编码而非标量标签。

Result: 计算成本从线性降至对数级，在IJB-B和IJB-C上分别提升1.52%和0.6%。

Conclusion: 结构化编码方法有效降低计算成本并提升性能，适用于大规模人脸识别任务。

Abstract: Aiming to reduce the computational cost of Softmax in massive label space of
Face Recognition (FR) benchmarks, recent studies estimate the output using a
subset of identities. Although promising, the association between the
computation cost and the number of identities in the dataset remains linear
only with a reduced ratio. A shared characteristic among available FR methods
is the employment of atomic scalar labels during training. Consequently, the
input to label matching is through a dot product between the feature vector of
the input and the Softmax centroids. Inspired by generative modeling, we
present a simple yet effective method that substitutes scalar labels with
structured identity code, i.e., a sequence of integers. Specifically, we
propose a tokenization scheme that transforms atomic scalar labels into
structured identity codes. Then, we train an FR backbone to predict the code
for each input instead of its scalar label. As a result, the associated
computational cost becomes logarithmic w.r.t. number of identities. We
demonstrate the benefits of the proposed method by conducting experiments. In
particular, our method outperforms its competitors by 1.52%, and 0.6% at
TAR@FAR$=1e-4$ on IJB-B and IJB-C, respectively, while transforming the
association between computational cost and the number of identities from linear
to logarithmic. See code at https://github.com/msed-Ebrahimi/GIF

</details>


### [42] [Lesion-Aware Generative Artificial Intelligence for Virtual Contrast-Enhanced Mammography in Breast Cancer](https://arxiv.org/abs/2505.03018)
*Aurora Rofena,Arianna Manchia,Claudia Lucia Piccolo,Bruno Beomonte Zobel,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: Seg-CycleGAN是一种生成性深度学习框架，用于在CESM中实现虚拟对比增强，通过低能量图像生成高质量的双能量减影图像，减少辐射和对比剂副作用。


<details>
  <summary>Details</summary>
Motivation: CESM虽然诊断准确性高，但存在辐射和对比剂副作用问题，因此需要一种无对比剂的替代方案。

Method: 基于CycleGAN架构，引入病灶分割图指导生成过程，并通过局部损失函数优化病灶区域的重建。

Result: 在CESM@UCBM数据集上，Seg-CycleGAN在PSNR和SSIM上优于基线，同时保持MSE和VIF的竞争力，生成图像病灶保真度更高。

Conclusion: Seg-CycleGAN为无对比剂CESM提供了一种可行方案，未来有望替代传统方法。

Abstract: Contrast-Enhanced Spectral Mammography (CESM) is a dual-energy mammographic
technique that improves lesion visibility through the administration of an
iodinated contrast agent. It acquires both a low-energy image, comparable to
standard mammography, and a high-energy image, which are then combined to
produce a dual-energy subtracted image highlighting lesion contrast
enhancement. While CESM offers superior diagnostic accuracy compared to
standard mammography, its use entails higher radiation exposure and potential
side effects associated with the contrast medium. To address these limitations,
we propose Seg-CycleGAN, a generative deep learning framework for Virtual
Contrast Enhancement in CESM. The model synthesizes high-fidelity dual-energy
subtracted images from low-energy images, leveraging lesion segmentation maps
to guide the generative process and improve lesion reconstruction. Building
upon the standard CycleGAN architecture, Seg-CycleGAN introduces localized loss
terms focused on lesion areas, enhancing the synthesis of diagnostically
relevant regions. Experiments on the CESM@UCBM dataset demonstrate that
Seg-CycleGAN outperforms the baseline in terms of PSNR and SSIM, while
maintaining competitive MSE and VIF. Qualitative evaluations further confirm
improved lesion fidelity in the generated images. These results suggest that
segmentation-aware generative models offer a viable pathway toward
contrast-free CESM alternatives.

</details>


### [43] [An Explainable Anomaly Detection Framework for Monitoring Depression and Anxiety Using Consumer Wearable Devices](https://arxiv.org/abs/2505.03039)
*Yuezhou Zhang,Amos A. Folarin,Callum Stewart,Heet Sankesara,Yatharth Ranjan,Pauline Conde,Akash Roy Choudhury,Shaoxiong Sun,Zulqarnain Rashid,Richard J. B. Dobson*

Main category: cs.CV

TL;DR: 论文提出了一种基于可穿戴设备数据的可解释异常检测框架，用于早期检测抑郁和焦虑症状的恶化，模型表现良好，尤其在并发症状和显著变化时效果更佳。


<details>
  <summary>Details</summary>
Motivation: 通过可穿戴设备持续监测行为和生理数据，为早期发现抑郁和焦虑症状恶化提供客观方法。

Method: 使用LSTM自编码器模型学习健康基线数据（睡眠、步数、静息心率），并通过异常检测识别症状恶化（抑郁或焦虑评分增加≥5分）。

Result: 模型在检测症状恶化事件中表现良好（F1=0.80），静息心率是最具影响力的特征。

Conclusion: 可解释的异常检测框架为个性化、可扩展的主动心理健康监测提供了潜力。

Abstract: Continuous monitoring of behavior and physiology via wearable devices offers
a novel, objective method for the early detection of worsening depression and
anxiety. In this study, we present an explainable anomaly detection framework
that identifies clinically meaningful increases in symptom severity using
consumer-grade wearable data. Leveraging data from 2,023 participants with
defined healthy baselines, our LSTM autoencoder model learned normal health
patterns of sleep duration, step count, and resting heart rate. Anomalies were
flagged when self-reported depression or anxiety scores increased by >=5 points
(a threshold considered clinically significant). The model achieved an adjusted
F1-score of 0.80 (precision = 0.73, recall = 0.88) in detecting 393
symptom-worsening episodes across 341 participants, with higher performance
observed for episodes involving concurrent depression and anxiety escalation
(F1 = 0.84) and for more pronounced symptom changes (>=10-point increases, F1 =
0.85). Model interpretability was supported by SHAP-based analysis, which
identified resting heart rate as the most influential feature in 71.4
percentage of detected anomalies, followed by physical activity and sleep.
Together, our findings highlight the potential of explainable anomaly detection
to enable personalized, scalable, and proactive mental health monitoring in
real-world settings.

</details>


### [44] [Estimating the Diameter at Breast Height of Trees in a Forest With a Single 360 Camera](https://arxiv.org/abs/2505.03093)
*Siming He,Zachary Osman,Fernando Cladera,Dexter Ong,Nitant Rai,Patrick Corey Green,Vijay Kumar,Pratik Chaudhari*

Main category: cs.CV

TL;DR: 提出一种低成本替代LiDAR的方法，使用消费级360度摄像机测量树木胸径（DBH），精度接近LiDAR，但成本大幅降低。


<details>
  <summary>Details</summary>
Motivation: 传统LiDAR技术精度高但成本昂贵且操作复杂，需要一种更经济、易用的替代方案。

Method: 结合SfM摄影测量、语义分割和RANSAC技术，构建半自动化流程，并通过可视化工具验证结果。

Result: 在61次测试中，相对误差为5-9%，仅比LiDAR高2-4%，但成本显著降低。

Conclusion: 该方法为森林资源管理提供了一种低成本、高精度的可行方案。

Abstract: Forest inventories rely on accurate measurements of the diameter at breast
height (DBH) for ecological monitoring, resource management, and carbon
accounting. While LiDAR-based techniques can achieve centimeter-level
precision, they are cost-prohibitive and operationally complex. We present a
low-cost alternative that only needs a consumer-grade 360 video camera. Our
semi-automated pipeline comprises of (i) a dense point cloud reconstruction
using Structure from Motion (SfM) photogrammetry software called Agisoft
Metashape, (ii) semantic trunk segmentation by projecting Grounded Segment
Anything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based
technique to estimate cross section shape and DBH. We introduce an interactive
visualization tool for inspecting segmented trees and their estimated DBH. On
61 acquisitions of 43 trees under a variety of conditions, our method attains
median absolute relative errors of 5-9% with respect to "ground-truth" manual
measurements. This is only 2-4% higher than LiDAR-based estimates, while
employing a single 360 camera that costs orders of magnitude less, requires
minimal setup, and is widely available.

</details>


### [45] [Not All Parameters Matter: Masking Diffusion Models for Enhancing Generation Ability](https://arxiv.org/abs/2505.03097)
*Lei Wang,Senmao Li,Fei Yang,Jianye Wang,Ziheng Zhang,Yuhan Liu,Yaxing Wang,Jian Yang*

Main category: cs.CV

TL;DR: 扩散模型早期专注于构建图像基础结构，后期生成细节特征。与传统深度学习架构不同，扩散模型在同一层学习结构和纹理信息。作者提出MaskUNet方法，通过选择性参数归零提升生成质量，并在COCO数据集上取得最佳FID分数。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在同一网络层同时学习结构和纹理信息，与传统架构不同，这启发了对时间维度扩散模型的探索。

Method: 提出MaskUNet方法，利用时间步和样本依赖的有效U-Net参数，提供两种微调策略（基于训练和无训练）。

Result: 在COCO数据集上，MaskUNet取得最佳FID分数，并在下游任务中表现优异。

Conclusion: MaskUNet通过优化参数选择显著提升生成质量，方法简单高效。

Abstract: The diffusion models, in early stages focus on constructing basic image
structures, while the refined details, including local features and textures,
are generated in later stages. Thus the same network layers are forced to learn
both structural and textural information simultaneously, significantly
differing from the traditional deep learning architectures (e.g., ResNet or
GANs) which captures or generates the image semantic information at different
layers. This difference inspires us to explore the time-wise diffusion models.
We initially investigate the key contributions of the U-Net parameters to the
denoising process and identify that properly zeroing out certain parameters
(including large parameters) contributes to denoising, substantially improving
the generation quality on the fly. Capitalizing on this discovery, we propose a
simple yet effective method-termed ``MaskUNet''- that enhances generation
quality with negligible parameter numbers. Our method fully leverages timestep-
and sample-dependent effective U-Net parameters. To optimize MaskUNet, we offer
two fine-tuning strategies: a training-based approach and a training-free
approach, including tailored networks and optimization functions. In zero-shot
inference on the COCO dataset, MaskUNet achieves the best FID score and further
demonstrates its effectiveness in downstream task evaluations. Project page:
https://gudaochangsheng.github.io/MaskUnet-Page/

</details>


### [46] [Image Recognition with Online Lightweight Vision Transformer: A Survey](https://arxiv.org/abs/2505.03113)
*Zherui Zhang,Rongtao Xu,Jie Zhou,Changwei Wang,Xingtian Pei,Wenhao Xu,Jiguang Zhang,Li Guo,Longxiang Gao,Wenbo Xu,Shibiao Xu*

Main category: cs.CV

TL;DR: 本文综述了轻量级视觉Transformer的在线生成策略，聚焦高效组件设计、动态网络和知识蒸馏，评估了ImageNet-1K上的性能权衡，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: Transformer在自然语言处理中的成功激发了其在计算机视觉任务中的应用，但面临计算和内存挑战，本文旨在探索轻量级解决方案。

Method: 通过高效组件设计、动态网络和知识蒸馏三种策略生成轻量级视觉Transformer，并在ImageNet-1K上进行评估。

Result: 分析了精度、参数量、吞吐量等性能权衡，总结了各策略的优缺点和灵活性。

Conclusion: 提出了轻量级视觉Transformer的未来研究方向和潜在挑战，为社区提供实用指导和启发。

Abstract: The Transformer architecture has achieved significant success in natural
language processing, motivating its adaptation to computer vision tasks. Unlike
convolutional neural networks, vision transformers inherently capture
long-range dependencies and enable parallel processing, yet lack inductive
biases and efficiency benefits, facing significant computational and memory
challenges that limit its real-world applicability. This paper surveys various
online strategies for generating lightweight vision transformers for image
recognition, focusing on three key areas: Efficient Component Design, Dynamic
Network, and Knowledge Distillation. We evaluate the relevant exploration for
each topic on the ImageNet-1K benchmark, analyzing trade-offs among precision,
parameters, throughput, and more to highlight their respective advantages,
disadvantages, and flexibility. Finally, we propose future research directions
and potential challenges in the lightweighting of vision transformers with the
aim of inspiring further exploration and providing practical guidance for the
community. Project Page: https://github.com/ajxklo/Lightweight-VIT

</details>


### [47] [Path and Bone-Contour Regularized Unpaired MRI-to-CT Translation](https://arxiv.org/abs/2505.03114)
*Teng Zhou,Jax Luo,Yuping Sun,Yiheng Tan,Shun Yao,Nazim Haouchine,Scott Raymond*

Main category: cs.CV

TL;DR: 提出了一种基于路径和骨骼轮廓正则化的无配对MRI到CT转换方法，显著提升了骨骼结构的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有无配对MRI到CT转换方法在骨骼结构等解剖特征上表现不佳的问题，满足放射治疗中对精确骨骼表示的需求。

Method: 将MRI和CT图像投影到共享潜在空间，通过神经常微分方程建模连续流，最小化路径长度，并引入可训练神经网络生成骨骼轮廓。

Result: 在三个数据集上表现优于现有方法，整体错误率更低，且在骨骼分割任务中表现更优。

Conclusion: 该方法显著提升了无配对MRI到CT转换的准确性，尤其在骨骼结构上表现突出。

Abstract: Accurate MRI-to-CT translation promises the integration of complementary
imaging information without the need for additional imaging sessions. Given the
practical challenges associated with acquiring paired MRI and CT scans, the
development of robust methods capable of leveraging unpaired datasets is
essential for advancing the MRI-to-CT translation. Current unpaired MRI-to-CT
translation methods, which predominantly rely on cycle consistency and
contrastive learning frameworks, frequently encounter challenges in accurately
translating anatomical features that are highly discernible on CT but less
distinguishable on MRI, such as bone structures. This limitation renders these
approaches less suitable for applications in radiation therapy, where precise
bone representation is essential for accurate treatment planning. To address
this challenge, we propose a path- and bone-contour regularized approach for
unpaired MRI-to-CT translation. In our method, MRI and CT images are projected
to a shared latent space, where the MRI-to-CT mapping is modeled as a
continuous flow governed by neural ordinary differential equations. The optimal
mapping is obtained by minimizing the transition path length of the flow. To
enhance the accuracy of translated bone structures, we introduce a trainable
neural network to generate bone contours from MRI and implement mechanisms to
directly and indirectly encourage the model to focus on bone contours and their
adjacent regions. Evaluations conducted on three datasets demonstrate that our
method outperforms existing unpaired MRI-to-CT translation approaches,
achieving lower overall error rates. Moreover, in a downstream bone
segmentation task, our approach exhibits superior performance in preserving the
fidelity of bone structures. Our code is available at:
https://github.com/kennysyp/PaBoT.

</details>


### [48] [TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion](https://arxiv.org/abs/2505.03116)
*Haoyue Liu,Jinghan Xu,Yi Chang,Hanyu Zhou,Haozhi Zhao,Lin Wang,Luxin Yan*

Main category: cs.CV

TL;DR: 论文提出了一种基于连续点跟踪的视频帧插值框架TimeTracker，通过事件相机处理非线性运动，显著提升了插值质量。


<details>
  <summary>Details</summary>
Motivation: 事件相机的高时间分辨率优势未被充分利用，现有方法在处理非线性运动时存在运动误差问题。

Method: 设计了场景感知区域分割模块和连续轨迹引导的运动估计模块，结合全局运动优化和帧细化生成中间帧。

Result: 实验表明，该方法在运动估计和帧插值质量上优于现有技术。

Conclusion: TimeTracker框架有效解决了非线性运动问题，提升了视频帧插值的性能。

Abstract: Video frame interpolation (VFI) that leverages the bio-inspired event cameras
as guidance has recently shown better performance and memory efficiency than
the frame-based methods, thanks to the event cameras' advantages, such as high
temporal resolution. A hurdle for event-based VFI is how to effectively deal
with non-linear motion, caused by the dynamic changes in motion direction and
speed within the scene. Existing methods either use events to estimate sparse
optical flow or fuse events with image features to estimate dense optical flow.
Unfortunately, motion errors often degrade the VFI quality as the continuous
motion cues from events do not align with the dense spatial information of
images in the temporal dimension. In this paper, we find that object motion is
continuous in space, tracking local regions over continuous time enables more
accurate identification of spatiotemporal feature correlations. In light of
this, we propose a novel continuous point tracking-based VFI framework, named
TimeTracker. Specifically, we first design a Scene-Aware Region Segmentation
(SARS) module to divide the scene into similar patches. Then, a Continuous
Trajectory guided Motion Estimation (CTME) module is proposed to track the
continuous motion trajectory of each patch through events. Finally,
intermediate frames at any given time are generated through global motion
optimization and frame refinement. Moreover, we collect a real-world dataset
that features fast non-linear motion. Extensive experiments show that our
method outperforms prior arts in both motion estimation and frame interpolation
quality.

</details>


### [49] [VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis](https://arxiv.org/abs/2505.03132)
*Xinyuan Yan,Xiwei Xuan,Jorge Piazentin Ono,Jiajing Guo,Vikram Mohanty,Shekar Arvind Kumar,Liang Gou,Bei Wang,Liu Ren*

Main category: cs.CV

TL;DR: VISLIX是一个视觉分析框架，利用基础模型帮助专家分析计算机视觉模型中的数据切片，无需额外元数据，支持交互式测试。


<details>
  <summary>Details</summary>
Motivation: 现有数据切片方法在计算机视觉任务中面临挑战，如依赖元数据、缺乏交互性，VISLIX旨在解决这些问题。

Method: VISLIX结合基础模型自动生成自然语言洞察，并支持用户交互式测试数据切片假设。

Result: 通过专家研究和三个用例验证，VISLIX能有效为对象检测模型提供全面洞察。

Conclusion: VISLIX为计算机视觉模型验证提供了更高效、交互性强的解决方案。

Abstract: Real-world machine learning models require rigorous evaluation before
deployment, especially in safety-critical domains like autonomous driving and
surveillance. The evaluation of machine learning models often focuses on data
slices, which are subsets of the data that share a set of characteristics. Data
slice finding automatically identifies conditions or data subgroups where
models underperform, aiding developers in mitigating performance issues.
Despite its popularity and effectiveness, data slicing for vision model
validation faces several challenges. First, data slicing often needs additional
image metadata or visual concepts, and falls short in certain computer vision
tasks, such as object detection. Second, understanding data slices is a
labor-intensive and mentally demanding process that heavily relies on the
expert's domain knowledge. Third, data slicing lacks a human-in-the-loop
solution that allows experts to form hypothesis and test them interactively. To
overcome these limitations and better support the machine learning operations
lifecycle, we introduce VISLIX, a novel visual analytics framework that employs
state-of-the-art foundation models to help domain experts analyze slices in
computer vision models. Our approach does not require image metadata or visual
concepts, automatically generates natural language insights, and allows users
to test data slice hypothesis interactively. We evaluate VISLIX with an expert
study and three use cases, that demonstrate the effectiveness of our tool in
providing comprehensive insights for validating object detection models.

</details>


### [50] [Enhancing Glass Defect Detection with Diffusion Models: Addressing Imbalanced Datasets in Manufacturing Quality Control](https://arxiv.org/abs/2505.03134)
*Sajjad Rezvani Boroujeni,Hossein Abedi,Tom Bush*

Main category: cs.CV

TL;DR: 论文提出了一种基于去噪扩散概率模型（DDPMs）的方法，通过生成合成缺陷玻璃图像解决数据不平衡问题，显著提升了缺陷检测性能。


<details>
  <summary>Details</summary>
Motivation: 工业玻璃制造中视觉缺陷检测因缺陷产品频率低导致数据不平衡，限制了深度学习模型的性能。

Method: 使用DDPMs生成合成缺陷图像进行数据增强，测试了ResNet50V2、EfficientNetB0和MobileNetV2等CNN架构。

Result: 实验显示，所有测试网络的召回率显著提升，ResNet50V2的分类准确率从78%提高到93%。

Conclusion: 该方法为玻璃制造中的缺陷检测提供了一种可扩展且经济的解决方案，适用于其他类似数据不平衡问题的行业。

Abstract: Visual defect detection in industrial glass manufacturing remains a critical
challenge due to the low frequency of defective products, leading to imbalanced
datasets that limit the performance of deep learning models and computer vision
systems. This paper presents a novel approach using Denoising Diffusion
Probabilistic Models (DDPMs) to generate synthetic defective glass product
images for data augmentation, effectively addressing class imbalance issues in
manufacturing quality control and automated visual inspection. The methodology
significantly enhances image classification performance of standard CNN
architectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting
anomalies by increasing the minority class representation. Experimental results
demonstrate substantial improvements in key machine learning metrics,
particularly in recall for defective samples across all tested deep neural
network architectures while maintaining perfect precision. The most dramatic
improvement was observed in ResNet50V2's overall classification accuracy, which
increased from 78 percent to 93 percent when trained with the augmented data.
This work provides a scalable, cost-effective approach to enhancing automated
defect detection in glass manufacturing that can potentially be extended to
other industrial quality assurance systems and industries with similar class
imbalance challenges.

</details>


### [51] [Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)](https://arxiv.org/abs/2505.03149)
*Joseph William Kettelkamp,Ludovica Romanin,Sarv Priya,Mathews Jacob*

Main category: cs.CV

TL;DR: 提出一种无监督运动补偿图像重建算法，用于自由呼吸和非门控3D心脏磁共振成像（MRI）。


<details>
  <summary>Details</summary>
Motivation: 解决自由呼吸和非门控3D心脏MRI中运动伪影问题。

Method: 通过低秩模型表示运动相位相关的变形场，静态模板和运动参数直接从k空间数据无监督学习。

Result: 相比现有运动分辨和运动补偿算法，该算法在自由呼吸3D cine MRI中恢复效果更好。

Conclusion: 提出的低秩运动模型在无监督条件下显著提升了图像重建质量。

Abstract: We introduce an unsupervised motion-compensated image reconstruction
algorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging
(MRI). We express the image volume corresponding to each specific motion phase
as the deformation of a single static image template. The main contribution of
the work is the low-rank model for the compact joint representation of the
family of diffeomorphisms, parameterized by the motion phases. The
diffeomorphism at a specific motion phase is obtained by integrating a
parametric velocity field along a path connecting the reference template phase
to the motion phase. The velocity field at different phases is represented
using a low-rank model. The static template and the low-rank motion model
parameters are learned directly from the k-space data in an unsupervised
fashion. The more constrained motion model is observed to offer improved
recovery compared to current motion-resolved and motion-compensated algorithms
for free-breathing 3D cine MRI.

</details>


### [52] [Robust Fairness Vision-Language Learning for Medical Image Analysis](https://arxiv.org/abs/2505.03153)
*Sparsh Bansal,Mingyang Wu,Xin Wang,Shu Hu*

Main category: cs.CV

TL;DR: 本文提出了一种确保视觉语言模型（VLM）在医学图像分析中公平性和鲁棒性的框架，通过动态坏对挖掘算法和Sinkhorn距离优化损失函数，实验显示公平性AUC提升8.6%。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在医学图像分析中具有潜力，但公平性和鲁棒性是其实际应用的关键挑战。

Method: 提出框架，结合动态坏对挖掘算法和Sinkhorn距离优化损失函数，调整图像-文本对。

Result: 实验结果显示公平性AUC提升8.6%。

Conclusion: 该框架有效提升了VLM在医学领域的公平性和鲁棒性。

Abstract: The advent of Vision-Language Models (VLMs) in medical image analysis has the
potential to help process multimodal inputs and increase performance over
traditional inference methods. However, when considering the domain in which
these models will be implemented, fairness and robustness are important to
ensure the model stays true for any patient. In this paper, we introduce a
framework for ensuring robustness and fairness of VLM models. This framework
modifies the loss function at training by identifying and adjusting faulty
image-text pairs through a Dynamic Bad Pair Mining algorithm and also utilizing
Sinkhorn distance to ensure the loss distributions of protected groups do not
deviate from the total loss. Experimental testing of our framework shows up to
a 8.6\% improvement when looking at equity-scaled AUC.

</details>


### [53] [StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data](https://arxiv.org/abs/2505.03154)
*Yuxuan Mu,Hung Yu Ling,Yi Shi,Ismael Baira Ojeda,Pengcheng Xi,Chang Shu,Fabio Zinno,Xue Bin Peng*

Main category: cs.CV

TL;DR: StableMotion是一种直接从无配对数据中训练运动清理模型的方法，通过引入运动质量指示器，无需高质量配对数据即可训练模型，有效减少运动伪影。


<details>
  <summary>Details</summary>
Motivation: 运动捕捉数据常因传感器和后期处理不准确而产生伪影，手动清理成本高且耗时。现有数据驱动方法需要配对数据，但获取高质量配对数据同样困难。

Method: 提出运动质量指示器，通过手动标注或启发式算法标记数据质量，结合扩散模型训练质量感知运动生成模型。

Result: 在SoccerMocap数据集上测试，模型显著减少运动伪影，运动弹出和冻结帧分别减少68%和81%。

Conclusion: StableMotion为运动数据清理提供了一种高效且无需配对数据的解决方案，适用于实际生产场景。

Abstract: Motion capture (mocap) data often exhibits visually jarring artifacts due to
inaccurate sensors and post-processing. Cleaning this corrupted data can
require substantial manual effort from human experts, which can be a costly and
time-consuming process. Previous data-driven motion cleanup methods offer the
promise of automating this cleanup process, but often require in-domain paired
corrupted-to-clean training data. Constructing such paired datasets requires
access to high-quality, relatively artifact-free motion clips, which often
necessitates laborious manual cleanup. In this work, we present StableMotion, a
simple yet effective method for training motion cleanup models directly from
unpaired corrupted datasets that need cleanup. The core component of our method
is the introduction of motion quality indicators, which can be easily annotated
through manual labeling or heuristic algorithms and enable training of
quality-aware motion generation models on raw motion data with mixed quality.
At test time, the model can be prompted to generate high-quality motions using
the quality indicators. Our method can be implemented through a simple
diffusion-based framework, leading to a unified motion generate-discriminate
model, which can be used to both identify and fix corrupted frames. We
demonstrate that our proposed method is effective for training motion cleanup
models on raw mocap data in production scenarios by applying StableMotion to
SoccerMocap, a 245-hour soccer mocap dataset containing real-world motion
artifacts. The trained model effectively corrects a wide range of motion
artifacts, reducing motion pops and frozen frames by 68% and 81%, respectively.
See https://youtu.be/3Y7MMAH02B4 for more results.

</details>


### [54] [RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph](https://arxiv.org/abs/2505.03173)
*Sameer Malik,Moyuru Yamada,Ayush Singh,Dishank Aggarwal*

Main category: cs.CV

TL;DR: RAVU框架通过构建时空图增强视频理解，解决了长视频处理的挑战，并在性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型（LMMs）因缺乏显式记忆和检索机制，难以处理长视频。

Method: 提出RAVU框架，利用时空图作为长期记忆，通过组合推理分解复杂查询并检索关键信息。

Result: 在NExT-QA和EgoSchema数据集上，RAVU仅需5-10帧即优于其他方法。

Conclusion: RAVU显著提升了长视频理解的准确性，尤其在多跳推理和跨帧对象追踪任务中。

Abstract: Comprehending long videos remains a significant challenge for Large
Multi-modal Models (LMMs). Current LMMs struggle to process even minutes to
hours videos due to their lack of explicit memory and retrieval mechanisms. To
address this limitation, we propose RAVU (Retrieval Augmented Video
Understanding), a novel framework for video understanding enhanced by retrieval
with compositional reasoning over a spatio-temporal graph. We construct a graph
representation of the video, capturing both spatial and temporal relationships
between entities. This graph serves as a long-term memory, allowing us to track
objects and their actions across time. To answer complex queries, we decompose
the queries into a sequence of reasoning steps and execute these steps on the
graph, retrieving relevant key information. Our approach enables more accurate
understanding of long videos, particularly for queries that require multi-hop
reasoning and tracking objects across frames. Our approach demonstrate superior
performances with limited retrieved frames (5-10) compared with other SOTA
methods and baselines on two major video QA datasets, NExT-QA and EgoSchema.

</details>


### [55] [seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models](https://arxiv.org/abs/2505.03176)
*Hafez Ghaemi,Eilif Muller,Shahab Bakhtiari*

Main category: cs.CV

TL;DR: 论文提出seq-JEPA，一种基于联合嵌入预测架构的世界建模范式，通过处理图像视图序列，同时学习不变和等变表示，解决了传统双视图方法的性能权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前自监督算法主要依赖数据增强和掩码等变换学习视觉表示，但双视图范式限制了表示灵活性，导致不变性和等变性任务间的性能权衡。

Method: seq-JEPA通过处理输入图像的不同视图序列，结合相对变换嵌入，使用Transformer编码器输出聚合表示，预测下一视图的表示。

Result: seq-JEPA在等变基准和图像分类任务中表现优异，且无需牺牲任一任务性能。

Conclusion: seq-JEPA框架不仅解决了传统方法的性能权衡问题，还在需要序列观察聚合的任务中表现出色。

Abstract: Current self-supervised algorithms mostly rely on transformations such as
data augmentation and masking to learn visual representations. This is achieved
by inducing invariance or equivariance with respect to these transformations
after encoding two views of an image. This dominant two-view paradigm can limit
the flexibility of learned representations for downstream adaptation by
creating performance trade-offs between invariance-related tasks such as image
classification and more fine-grained equivariance-related tasks. In this work,
we introduce \emph{seq-JEPA}, a world modeling paradigm based on
joint-embedding predictive architecture that leverages architectural inductive
biases to resolve this trade-off. Without requiring an additional equivariance
predictor or loss term, seq-JEPA simultaneously learns two architecturally
segregated representations: one equivariant to the specified transformations
and another invariant to them and suited for tasks such as classification. To
do so, our model processes a short sequence of different views (observations)
of an input image. Each encoded view is concatenated with embeddings
corresponding to the relative transformation (action) producing the next
observation in the sequence. A transformer encoder outputs an aggregate
representation of this sequence, which is subsequently conditioned on the
action leading to the next observation to predict its representation.
Empirically, seq-JEPA achieves strong performance on equivariant benchmarks and
image classification without sacrificing one for the other. Additionally, our
framework excels at tasks that inherently require aggregating a sequence of
observations, such as path integration across actions and predictive learning
across eye movements.

</details>


### [56] [Interactive Instance Annotation with Siamese Networks](https://arxiv.org/abs/2505.03184)
*Xiang Xu,Ruotong Li,Mengjun Yi,Baile XU,Furao Shen,Jian Zhao*

Main category: cs.CV

TL;DR: SiamAnno利用Siamese网络进行实例标注，通过单次学习预测未见过的物体边界，支持跨域任务，无需微调即可在多数据集上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 实例标注耗时耗力，现有方法多局限于同域场景，难以应对跨域任务。

Method: 提出SiamAnno框架，基于Siamese网络，以边界框为输入预测物体边界，支持用户调整。

Result: 在未微调的情况下，跨数据集测试中表现优于现有方法，实现SOTA性能。

Conclusion: SiamAnno是首个探索Siamese架构用于实例标注的模型，为未来研究奠定基础。

Abstract: Annotating instance masks is time-consuming and labor-intensive. A promising
solution is to predict contours using a deep learning model and then allow
users to refine them. However, most existing methods focus on in-domain
scenarios, limiting their effectiveness for cross-domain annotation tasks. In
this paper, we propose SiamAnno, a framework inspired by the use of Siamese
networks in object tracking. SiamAnno leverages one-shot learning to annotate
previously unseen objects by taking a bounding box as input and predicting
object boundaries, which can then be adjusted by annotators. Trained on one
dataset and tested on another without fine-tuning, SiamAnno achieves
state-of-the-art (SOTA) performance across multiple datasets, demonstrating its
ability to handle domain and environment shifts in cross-domain tasks. We also
provide more comprehensive results compared to previous work, establishing a
strong baseline for future research. To our knowledge, SiamAnno is the first
model to explore Siamese architecture for instance annotation.

</details>


### [57] [PiCo: Enhancing Text-Image Alignment with Improved Noise Selection and Precise Mask Control in Diffusion Models](https://arxiv.org/abs/2505.03203)
*Chang Xie,Chenyi Zhuang,Pan Gao*

Main category: cs.CV

TL;DR: PiCo提出了一种无需训练的方法，通过噪声选择模块和参考掩码模块提升文本-图像对齐效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在复杂文本提示下难以实现文本-图像对齐的问题。

Method: 1. 噪声选择模块评估随机噪声质量；2. 参考掩码模块生成像素级掩码并调节交叉注意力图。

Result: 实验验证PiCo能有效减少随机生成过程并提升文本-图像对齐效果。

Conclusion: PiCo通过噪声选择和掩码引导显著提升了复杂文本下的生成质量。

Abstract: Advanced diffusion models have made notable progress in text-to-image
compositional generation. However, it is still a challenge for existing models
to achieve text-image alignment when confronted with complex text prompts. In
this work, we highlight two factors that affect this alignment: the quality of
the randomly initialized noise and the reliability of the generated controlling
mask. We then propose PiCo (Pick-and-Control), a novel training-free approach
with two key components to tackle these two factors. First, we develop a noise
selection module to assess the quality of the random noise and determine
whether the noise is suitable for the target text. A fast sampling strategy is
utilized to ensure efficiency in the noise selection stage. Second, we
introduce a referring mask module to generate pixel-level masks and to
precisely modulate the cross-attention maps. The referring mask is applied to
the standard diffusion process to guide the reasonable interaction between text
and image features. Extensive experiments have been conducted to verify the
effectiveness of PiCo in liberating users from the tedious process of random
generation and in enhancing the text-image alignment for diverse text
descriptions.

</details>


### [58] [DCS-ST for Classification of Breast Cancer Histopathology Images with Limited Annotations](https://arxiv.org/abs/2505.03204)
*Liu Suxing,Byungwon Min*

Main category: cs.CV

TL;DR: 深度学习在乳腺癌病理图像分类中表现良好，但标注数据不足时性能下降。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像标注成本高且需专业知识的问题。

Method: 使用深度学习技术。

Result: 标注数据不足时性能下降。

Conclusion: 需改进方法以应对标注数据不足的挑战。

Abstract: Deep learning methods have shown promise in classifying breast cancer
histopathology images, but their performance often declines with limited
annotated data, a critical challenge in medical imaging due to the high cost
and expertise required for annotations.

</details>


### [59] [Dual-Domain Masked Image Modeling: A Self-Supervised Pretraining Strategy Using Spatial and Frequency Domain Masking for Hyperspectral Data](https://arxiv.org/abs/2505.03220)
*Shaheer Mohamed,Tharindu Fernando,Sridha Sridharan,Peyman Moghadam,Clinton Fookes*

Main category: cs.CV

TL;DR: 提出了一种名为SFMIM的自监督预训练策略，通过空间和频率双域掩码机制，利用未标记的HSI数据提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 标记的HSI数据稀缺，限制了深度学习尤其是基于Transformer的架构的潜力。

Method: 采用空间和频率双域掩码机制，分别随机掩码空间块和频率成分，训练模型重建掩码部分。

Result: 在三个公开HSI分类基准上达到最优性能，且微调时收敛迅速。

Conclusion: SFMIM策略高效，能显著提升HSI分类性能。

Abstract: Hyperspectral images (HSIs) capture rich spectral signatures that reveal
vital material properties, offering broad applicability across various domains.
However, the scarcity of labeled HSI data limits the full potential of deep
learning, especially for transformer-based architectures that require
large-scale training. To address this constraint, we propose Spatial-Frequency
Masked Image Modeling (SFMIM), a self-supervised pretraining strategy for
hyperspectral data that utilizes the large portion of unlabeled data. Our
method introduces a novel dual-domain masking mechanism that operates in both
spatial and frequency domains. The input HSI cube is initially divided into
non-overlapping patches along the spatial dimension, with each patch comprising
the entire spectrum of its corresponding spatial location. In spatial masking,
we randomly mask selected patches and train the model to reconstruct the masked
inputs using the visible patches. Concurrently, in frequency masking, we remove
portions of the frequency components of the input spectra and predict the
missing frequencies. By learning to reconstruct these masked components, the
transformer-based encoder captures higher-order spectral-spatial correlations.
We evaluate our approach on three publicly available HSI classification
benchmarks and demonstrate that it achieves state-of-the-art performance.
Notably, our model shows rapid convergence during fine-tuning, highlighting the
efficiency of our pretraining strategy.

</details>


### [60] [Seeing the Abstract: Translating the Abstract Language for Vision Language Models](https://arxiv.org/abs/2505.03242)
*Davide Talon,Federico Girella,Ziyue Liu,Marco Cristani,Yiming Wang*

Main category: cs.CV

TL;DR: 论文揭示了抽象语言在视觉语言模型（VLM）中的广泛存在及其被低估的价值，提出了一种无需训练的方法ACT，通过将抽象表示转换为具体表示，显著提升了文本到图像检索任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型研究忽视了抽象语言的表达，而自然语言中抽象概念（如情感、创造力）具有重要价值。本文旨在填补这一空白，尤其在时尚领域。

Method: 提出了一种无需训练、模型无关的方法Abstract-to-Concrete Translator (ACT)，利用预训练模型和多模态数据库，将抽象表示转换为VLM潜在空间中已充分表示的具体表示。

Result: 在文本到图像检索任务中，ACT无需训练即优于微调的VLM，表现出强大的泛化能力，且适用于多种VLM。

Conclusion: ACT是一种即插即用的解决方案，显著提升了VLM对抽象语言的处理能力，为未来研究提供了新方向。

Abstract: Natural language goes beyond dryly describing visual content. It contains
rich abstract concepts to express feeling, creativity and properties that
cannot be directly perceived. Yet, current research in Vision Language Models
(VLMs) has not shed light on abstract-oriented language. Our research breaks
new ground by uncovering its wide presence and under-estimated value, with
extensive analysis. Particularly, we focus our investigation on the fashion
domain, a highly-representative field with abstract expressions. By analyzing
recent large-scale multimodal fashion datasets, we find that abstract terms
have a dominant presence, rivaling the concrete ones, providing novel
information, and being useful in the retrieval task. However, a critical
challenge emerges: current general-purpose or fashion-specific VLMs are
pre-trained with databases that lack sufficient abstract words in their text
corpora, thus hindering their ability to effectively represent
abstract-oriented language. We propose a training-free and model-agnostic
method, Abstract-to-Concrete Translator (ACT), to shift abstract
representations towards well-represented concrete ones in the VLM latent space,
using pre-trained models and existing multimodal databases. On the
text-to-image retrieval task, despite being training-free, ACT outperforms the
fine-tuned VLMs in both same- and cross-dataset settings, exhibiting its
effectiveness with a strong generalization capability. Moreover, the
improvement introduced by ACT is consistent with various VLMs, making it a
plug-and-play solution.

</details>


### [61] [PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs](https://arxiv.org/abs/2505.03254)
*Lukas Meiner,Jens Mehnert,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: PROM是一种针对深度可分离卷积网络的量化方法，通过选择性使用三元和8位量化，显著降低能耗和存储需求。


<details>
  <summary>Details</summary>
Motivation: 现代深度可分离卷积网络中，计算成本分布不均，现有量化方法未能充分利用效率潜力。

Method: 选择性量化：点卷积用三元权重，其余模块用8位权重，结合量化感知训练。

Result: MobileNetV2能耗降低23.9倍，存储减少2.7倍，性能接近原始模型。

Conclusion: PROM提升了量化模型的能耗与精度平衡，适用于资源受限设备。

Abstract: Convolutional neural networks (CNNs) are crucial for computer vision tasks on
resource-constrained devices. Quantization effectively compresses these models,
reducing storage size and energy cost. However, in modern depthwise-separable
architectures, the computational cost is distributed unevenly across its
components, with pointwise operations being the most expensive. By applying a
general quantization scheme to this imbalanced cost distribution, existing
quantization approaches fail to fully exploit potential efficiency gains. To
this end, we introduce PROM, a straightforward approach for quantizing modern
depthwise-separable convolutional networks by selectively using two distinct
bit-widths. Specifically, pointwise convolutions are quantized to ternary
weights, while the remaining modules use 8-bit weights, which is achieved
through a simple quantization-aware training procedure. Additionally, by
quantizing activations to 8-bit, our method transforms pointwise convolutions
with ternary weights into int8 additions, which enjoy broad support across
hardware platforms and effectively eliminates the need for expensive
multiplications. Applying PROM to MobileNetV2 reduces the model's energy cost
by more than an order of magnitude (23.9x) and its storage size by 2.7x
compared to the float16 baseline while retaining similar classification
performance on ImageNet. Our method advances the Pareto frontier for energy
consumption vs. top-1 accuracy for quantized convolutional models on ImageNet.
PROM addresses the challenges of quantizing depthwise-separable convolutional
networks to both ternary and 8-bit weights, offering a simple way to reduce
energy cost and storage size.

</details>


### [62] [DiffVQA: Video Quality Assessment Using Diffusion Feature Extractor](https://arxiv.org/abs/2505.03261)
*Wei-Ting Chen,Yu-Jiet Vong,Yi-Tsung Lee,Sy-Yen Kuo,Qiang Gao,Sizhuo Ma,Jian Wang*

Main category: cs.CV

TL;DR: DiffVQA是一个利用扩散模型预训练能力的视频质量评估框架，通过结合语义和失真特征以及时间动态特征，显著提升了评估性能。


<details>
  <summary>Details</summary>
Motivation: 现有VQA方法（如CNN和ViT）在多样化的真实场景中难以与人类感知对齐，且数据集规模有限。

Method: DiffVQA利用预训练的扩散模型重构输入帧，提取语义和失真特征，并通过Mamba模块增强时间动态特征。

Result: 实验表明，DiffVQA在多个数据集上表现优异，尤其在跨数据集泛化能力上优于CNN和ViT。

Conclusion: 扩散模型作为特征提取器可显著提升VQA性能，优于传统方法。

Abstract: Video Quality Assessment (VQA) aims to evaluate video quality based on
perceptual distortions and human preferences. Despite the promising performance
of existing methods using Convolutional Neural Networks (CNNs) and Vision
Transformers (ViTs), they often struggle to align closely with human
perceptions, particularly in diverse real-world scenarios. This challenge is
exacerbated by the limited scale and diversity of available datasets. To
address this limitation, we introduce a novel VQA framework, DiffVQA, which
harnesses the robust generalization capabilities of diffusion models
pre-trained on extensive datasets. Our framework adapts these models to
reconstruct identical input frames through a control module. The adapted
diffusion model is then used to extract semantic and distortion features from a
resizing branch and a cropping branch, respectively. To enhance the model's
ability to handle long-term temporal dynamics, a parallel Mamba module is
introduced, which extracts temporal coherence augmented features that are
merged with the diffusion features to predict the final score. Experiments
across multiple datasets demonstrate DiffVQA's superior performance on
intra-dataset evaluations and its exceptional generalization across datasets.
These results confirm that leveraging a diffusion model as a feature extractor
can offer enhanced VQA performance compared to CNN and ViT backbones.

</details>


### [63] [OccCylindrical: Multi-Modal Fusion with Cylindrical Representation for 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2505.03284)
*Zhenxing Ming,Julie Stephany Berrio,Mao Shan,Yaoqi Huang,Hongyu Lyu,Nguyen Hoang Khoi Tran,Tzu-Yun Tseng,Stewart Worrall*

Main category: cs.CV

TL;DR: 论文提出OccCylindrical方法，通过圆柱坐标系融合多传感器特征，提升3D语义占据预测的细粒度细节和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于多传感器融合的方法主要使用笛卡尔坐标系，忽略了传感器读数的分布，导致细粒度细节丢失和性能下降。

Method: 提出OccCylindrical方法，在圆柱坐标系下融合和优化不同模态特征，保留更多几何细节。

Result: 在nuScenes数据集（包括雨天和夜间场景）上验证了方法的有效性和领先性能。

Conclusion: OccCylindrical通过圆柱坐标系特征融合，显著提升了3D语义占据预测的性能。

Abstract: The safe operation of autonomous vehicles (AVs) is highly dependent on their
understanding of the surroundings. For this, the task of 3D semantic occupancy
prediction divides the space around the sensors into voxels, and labels each
voxel with both occupancy and semantic information. Recent perception models
have used multisensor fusion to perform this task. However, existing
multisensor fusion-based approaches focus mainly on using sensor information in
the Cartesian coordinate system. This ignores the distribution of the sensor
readings, leading to a loss of fine-grained details and performance
degradation. In this paper, we propose OccCylindrical that merges and refines
the different modality features under cylindrical coordinates. Our method
preserves more fine-grained geometry detail that leads to better performance.
Extensive experiments conducted on the nuScenes dataset, including challenging
rainy and nighttime scenarios, confirm our approach's effectiveness and
state-of-the-art performance. The code will be available at:
https://github.com/DanielMing123/OccCylindrical

</details>


### [64] [Base-Detail Feature Learning Framework for Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2505.03286)
*Zhihao Gong,Lian Wu,Yong Xu*

Main category: cs.CV

TL;DR: 提出了一种基于细节和基础特征学习的框架（BDLF），以充分利用可见光和红外模态的共享和特定信息，提升跨模态行人重识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用不同模态的信息，主要关注共享特征而忽略模态特定细节，导致性能不佳。

Method: 通过无损细节特征提取模块和互补基础嵌入生成机制，分别挖掘细节和基础特征，并采用相关性限制方法确保特征丰富。

Result: 在SYSU-MM01、RegDB和LLCM数据集上的实验验证了BDLF的有效性。

Conclusion: BDLF通过同时利用模态共享和特定信息，显著提升了跨模态行人重识别的性能。

Abstract: Visible-infrared person re-identification (VIReID) provides a solution for
ReID tasks in 24-hour scenarios; however, significant challenges persist in
achieving satisfactory performance due to the substantial discrepancies between
visible (VIS) and infrared (IR) modalities. Existing methods inadequately
leverage information from different modalities, primarily focusing on digging
distinguishing features from modality-shared information while neglecting
modality-specific details. To fully utilize differentiated minutiae, we propose
a Base-Detail Feature Learning Framework (BDLF) that enhances the learning of
both base and detail knowledge, thereby capitalizing on both modality-shared
and modality-specific information. Specifically, the proposed BDLF mines detail
and base features through a lossless detail feature extraction module and a
complementary base embedding generation mechanism, respectively, supported by a
novel correlation restriction method that ensures the features gained by BDLF
enrich both detail and base knowledge across VIS and IR features. Comprehensive
experiments conducted on the SYSU-MM01, RegDB, and LLCM datasets validate the
effectiveness of BDLF.

</details>


### [65] [Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach](https://arxiv.org/abs/2505.03299)
*Pierre Adorni,Minh-Tan Pham,Stéphane May,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: 提出了一种基于“能力编码”的方法，用于预测基础模型在多个下游任务中的性能，无需微调，简化模型选择并推动未来研究。


<details>
  <summary>Details</summary>
Motivation: 尽管已有75多个遥感视觉基础模型，但尚无模型在所有下游任务中表现一致最佳，需一种高效比较方法。

Method: 通过“能力编码”预测模型性能，避免对每个任务进行微调。

Result: 该方法能简化模型选择，并为现有文献提供新视角。

Conclusion: 能力编码方法为模型比较和未来研究方向提供了实用工具。

Abstract: Foundation models constitute a significant advancement in computer vision:
after a single, albeit costly, training phase, they can address a wide array of
tasks. In the field of Earth observation, over 75 remote sensing vision
foundation models have been developed in the past four years. However, none has
consistently outperformed the others across all available downstream tasks. To
facilitate their comparison, we propose a cost-effective method for predicting
a model's performance on multiple downstream tasks without the need for
fine-tuning on each one. This method is based on what we call "capabilities
encoding." The utility of this novel approach is twofold: we demonstrate its
potential to simplify the selection of a foundation model for a given new task,
and we employ it to offer a fresh perspective on the existing literature,
suggesting avenues for future research. Codes are available at
https://github.com/pierreadorni/capabilities-encoding.

</details>


### [66] [3D Can Be Explored In 2D: Pseudo-Label Generation for LiDAR Point Clouds Using Sensor-Intensity-Based 2D Semantic Segmentation](https://arxiv.org/abs/2505.03300)
*Andrew Caunes,Thierry Chateau,Vincent Frémont*

Main category: cs.CV

TL;DR: 提出一种无需3D标注的3D语义分割方法，利用2D分割模型生成伪标签，适用于无监督域适应任务。


<details>
  <summary>Details</summary>
Motivation: 解决3D点云语义分割中标注成本高和域偏移问题。

Method: 通过LiDAR扫描生成2D视图，应用预训练的2D分割模型，再通过投票机制将结果投影回3D点云。

Result: 实验验证了方法的有效性，生成的伪标签可用于无监督域适应。

Conclusion: 该方法为3D语义分割提供了一种高效且无需额外模态的解决方案。

Abstract: Semantic segmentation of 3D LiDAR point clouds, essential for autonomous
driving and infrastructure management, is best achieved by supervised learning,
which demands extensive annotated datasets and faces the problem of domain
shifts. We introduce a new 3D semantic segmentation pipeline that leverages
aligned scenes and state-of-the-art 2D segmentation methods, avoiding the need
for direct 3D annotation or reliance on additional modalities such as camera
images at inference time. Our approach generates 2D views from LiDAR scans
colored by sensor intensity and applies 2D semantic segmentation to these views
using a camera-domain pretrained model. The segmented 2D outputs are then
back-projected onto the 3D points, with a simple voting-based estimator that
merges the labels associated to each 3D point. Our main contribution is a
global pipeline for 3D semantic segmentation requiring no prior 3D annotation
and not other modality for inference, which can be used for pseudo-label
generation. We conduct a thorough ablation study and demonstrate the potential
of the generated pseudo-labels for the Unsupervised Domain Adaptation task.

</details>


### [67] [Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices](https://arxiv.org/abs/2505.03303)
*Tasnim Shahriar*

Main category: cs.CV

TL;DR: 论文评估了五种轻量级深度学习模型在图像分类中的表现，重点分析了其在资源受限环境中的适用性，并揭示了准确性与效率之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究轻量级深度学习模型在资源受限环境（如低内存设备）中的部署潜力，为边缘计算和移动平台提供优化方案。

Method: 对五种模型（MobileNetV3 Small、ResNet18、SqueezeNet、EfficientNetV2-S、ShuffleNetV2）在三个数据集（CIFAR-10、CIFAR-100、Tiny ImageNet）上进行了性能评估，包括分类准确率、推理时间、FLOPs和模型大小，并研究了超参数调优、数据增强和训练范式的影响。

Result: 迁移学习显著提升了模型准确率和计算效率，EfficientNetV2准确率最高，MobileNetV3在准确性和效率间平衡最佳，SqueezeNet在推理速度和模型紧凑性上表现突出。

Conclusion: 研究为资源受限环境中的轻量级模型部署提供了实用建议，优化了边缘计算和移动平台的深度学习系统。

Abstract: This paper presents a comprehensive evaluation of lightweight deep learning
models for image classification, emphasizing their suitability for deployment
in resource-constrained environments such as low-memory devices. Five
state-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,
EfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse
datasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using
four key performance metrics: classification accuracy, inference time,
floating-point operations (FLOPs), and model size. Additionally, we investigate
the impact of hyperparameter tuning, data augmentation, and training paradigms
by comparing pretrained models with scratch-trained counterparts, focusing on
MobileNetV3 Small. Our findings reveal that transfer learning significantly
enhances model accuracy and computational efficiency, particularly for complex
datasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest
accuracy, while MobileNetV3 offers the best balance between accuracy and
efficiency, and SqueezeNet excels in inference speed and compactness. This
study highlights critical trade-offs between accuracy and efficiency, offering
actionable insights for deploying lightweight models in real-world applications
where computational resources are limited. By addressing these challenges, this
research contributes to optimizing deep learning systems for edge computing and
mobile platforms.

</details>


### [68] [3D Gaussian Splatting Data Compression with Mixture of Priors](https://arxiv.org/abs/2505.03310)
*Lei Liu,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 提出了一种基于混合先验（MoP）策略的3D高斯泼溅数据压缩方法，通过多先验特征和门控机制优化熵模型和量化策略，在无损和有损压缩中均取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅数据压缩方法在熵模型和量化策略上存在不足，未能充分利用超先验信息和实现细粒度的元素级量化。

Method: 采用混合先验（MoP）策略，通过多轻量级MLP生成多样化先验特征，结合门控机制整合为MoP特征。无损压缩中用于改进条件熵模型，有损压缩中指导元素级量化，采用先验引导的粗到细量化（C2FQ）策略。

Result: 在Mip-NeRF360、BungeeNeRF、DeepBlending和Tank&Temples等多个基准测试中达到最优性能。

Conclusion: MoP策略有效解决了3D高斯泼溅数据压缩中的熵模型和量化问题，显著提升了压缩性能。

Abstract: 3D Gaussian Splatting (3DGS) data compression is crucial for enabling
efficient storage and transmission in 3D scene modeling. However, its
development remains limited due to inadequate entropy models and suboptimal
quantization strategies for both lossless and lossy compression scenarios,
where existing methods have yet to 1) fully leverage hyperprior information to
construct robust conditional entropy models, and 2) apply fine-grained,
element-wise quantization strategies for improved compression granularity. In
this work, we propose a novel Mixture of Priors (MoP) strategy to
simultaneously address these two challenges. Specifically, inspired by the
Mixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior
information through multiple lightweight MLPs to generate diverse prior
features, which are subsequently integrated into the MoP feature via a gating
mechanism. To enhance lossless compression, the resulting MoP feature is
utilized as a hyperprior to improve conditional entropy modeling. Meanwhile,
for lossy compression, we employ the MoP feature as guidance information in an
element-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine
Quantization (C2FQ) strategy with a predefined quantization step value.
Specifically, we expand the quantization step value into a matrix and
adaptively refine it from coarse to fine granularity, guided by the MoP
feature, thereby obtaining a quantization step matrix that facilitates
element-wise quantization. Extensive experiments demonstrate that our proposed
3DGS data compression framework achieves state-of-the-art performance across
multiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and
Tank&Temples.

</details>


### [69] [Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.03318)
*Yibin Wang,Zhimin Li,Yuhang Zang,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 论文提出UnifiedReward-Think，首个基于长链思维（CoT）的多模态奖励模型，通过强化微调方法提升奖励信号的可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型（RMs）在提供奖励信号时仅能进行浅层推理，导致信号不准确。通过引入长链思维（CoT），可以显著提升其可靠性和鲁棒性。

Method: 采用探索驱动的强化微调方法：1) 使用少量图像生成偏好数据蒸馏GPT-4o的推理过程；2) 利用模型先验知识生成大规模多模态偏好数据；3) 通过GRPO优化模型推理路径。

Result: 实验表明，该模型在多种视觉奖励任务中表现优越。

Conclusion: UnifiedReward-Think通过长链思维和强化微调，显著提升了奖励模型的推理能力和信号准确性。

Abstract: Recent advances in multimodal Reward Models (RMs) have shown significant
promise in delivering reward signals to align vision models with human
preferences. However, current RMs are generally restricted to providing direct
responses or engaging in shallow reasoning processes with limited depth, often
leading to inaccurate reward signals. We posit that incorporating explicit long
chains of thought (CoT) into the reward reasoning process can significantly
strengthen their reliability and robustness. Furthermore, we believe that once
RMs internalize CoT reasoning, their direct response accuracy can also be
improved through implicit reasoning capabilities. To this end, this paper
proposes UnifiedReward-Think, the first unified multimodal CoT-based reward
model, capable of multi-dimensional, step-by-step long-chain reasoning for both
visual understanding and generation reward tasks. Specifically, we adopt an
exploration-driven reinforcement fine-tuning approach to elicit and incentivize
the model's latent complex reasoning ability: (1) We first use a small amount
of image generation preference data to distill the reasoning process of GPT-4o,
which is then used for the model's cold start to learn the format and structure
of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge
and generalization capabilities, we prepare large-scale unified multimodal
preference data to elicit the model's reasoning process across various vision
tasks. During this phase, correct reasoning outputs are retained for rejection
sampling to refine the model (3) while incorrect predicted samples are finally
used for Group Relative Policy Optimization (GRPO) based reinforcement
fine-tuning, enabling the model to explore diverse reasoning paths and optimize
for correct and robust solutions. Extensive experiments across various vision
reward tasks demonstrate the superiority of our model.

</details>


### [70] [SD-VSum: A Method and Dataset for Script-Driven Video Summarization](https://arxiv.org/abs/2505.03319)
*Manolis Mylonas,Evlampios Apostolidis,Vasileios Mezaris*

Main category: cs.CV

TL;DR: 论文提出了一种基于脚本的视频摘要任务（SD-VSum），通过用户提供的脚本选择视频中最相关的部分生成摘要。扩展了VideoXum数据集，并开发了一种新的跨模态注意力网络架构，实验证明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统视频摘要任务无法根据用户需求生成个性化摘要的问题，提出脚本驱动的视频摘要任务。

Method: 扩展VideoXum数据集，加入自然语言描述；开发基于跨模态注意力机制的SD-VSum网络架构，融合视觉和文本信息。

Result: SD-VSum在实验评估中表现优于现有查询驱动和通用摘要方法，能生成符合用户需求的个性化摘要。

Conclusion: SD-VSum通过脚本驱动和跨模态融合，实现了高效且个性化的视频摘要生成。

Abstract: In this work, we introduce the task of script-driven video summarization,
which aims to produce a summary of the full-length video by selecting the parts
that are most relevant to a user-provided script outlining the visual content
of the desired summary. Following, we extend a recently-introduced large-scale
dataset for generic video summarization (VideoXum) by producing natural
language descriptions of the different human-annotated summaries that are
available per video. In this way we make it compatible with the introduced
task, since the available triplets of ``video, summary and summary
description'' can be used for training a method that is able to produce
different summaries for a given video, driven by the provided script about the
content of each summary. Finally, we develop a new network architecture for
script-driven video summarization (SD-VSum), that relies on the use of a
cross-modal attention mechanism for aligning and fusing information from the
visual and text modalities. Our experimental evaluations demonstrate the
advanced performance of SD-VSum against state-of-the-art approaches for
query-driven and generic (unimodal and multimodal) summarization from the
literature, and document its capacity to produce video summaries that are
adapted to each user's needs about their content.

</details>


### [71] [Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning](https://arxiv.org/abs/2505.03327)
*José-Luis Bueso-Bello,Benjamin Chauvel,Daniel Carcereri,Philipp Posovszky,Pietro Milillo,Jennifer Ruiz,Juan-Carlos Fernández-Diaz,Carolina González,Michele Martone,Ronny Hänsch,Paola Rizzoli*

Main category: cs.CV

TL;DR: 论文提出了一种结合自监督学习和监督学习的框架，用于高分辨率森林制图，解决了标记数据不足的问题，并在亚马逊雨林的实际应用中显著提升了分类精度。


<details>
  <summary>Details</summary>
Motivation: 传统的高分辨率森林制图方法需要大量标记数据，但现实中高分辨率标记数据稀缺。论文旨在通过自监督学习减少对标记数据的依赖，提升制图精度。

Method: 采用自监督学习从输入特征中提取信息，随后用少量标记数据进行监督训练。在宾夕法尼亚州的1米分辨率森林/非森林地图上验证方法，并应用于亚马逊雨林的实际场景。

Result: 在标记数据有限的情况下，自监督框架显著优于全监督方法，分类精度更高。

Conclusion: 自监督学习为高分辨率森林制图提供了有效解决方案，尤其适用于标记数据稀缺的场景。

Abstract: Deep learning models have shown encouraging capabilities for mapping
accurately forests at medium resolution with TanDEM-X interferometric SAR data.
Such models, as most of current state-of-the-art deep learning techniques in
remote sensing, are trained in a fully-supervised way, which requires a large
amount of labeled data for training and validation. In this work, our aim is to
exploit the high-resolution capabilities of the TanDEM-X mission to map forests
at 6 m. The goal is to overcome the intrinsic limitations posed by
midresolution products, which affect, e.g., the detection of narrow roads
within vegetated areas and the precise delineation of forested regions
contours. To cope with the lack of extended reliable reference datasets at such
a high resolution, we investigate self-supervised learning techniques for
extracting highly informative representations from the input features, followed
by a supervised training step with a significantly smaller number of reliable
labels. A 1 m resolution forest/non-forest reference map over Pennsylvania,
USA, allows for comparing different training approaches for the development of
an effective forest mapping framework with limited labeled samples. We select
the best-performing approach over this test region and apply it in a real-case
forest mapping scenario over the Amazon rainforest, where only very few labeled
data at high resolution are available. In this challenging scenario, the
proposed self-supervised framework significantly enhances the classification
accuracy with respect to fully-supervised methods, trained using the same
amount of labeled data, representing an extremely promising starting point for
large-scale, very high-resolution forest mapping with TanDEM-X data.

</details>


### [72] [FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for Scene Text Editing](https://arxiv.org/abs/2505.03329)
*Rui Lan,Yancheng Bai,Xu Duan,Mingxing Li,Lei Sun,Xiangxiang Chu*

Main category: cs.CV

TL;DR: FLUX-Text是一个基于FLUX-Fill的多语言场景文本编辑框架，通过轻量级字形和文本嵌入模块，显著提升了文本编辑的准确性和保真度，尤其在非拉丁字符（如中文）上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于潜在扩散模型（LDM）的文本编辑方法在非拉丁字符（如中文）上生成不准确或不可识别字符的问题。

Method: 提出FLUX-Text框架，结合视觉和文本模态的字形条件，设计轻量级字形和文本嵌入模块，仅需10万训练样本。

Result: 在公开数据集上，FLUX-Text在文本保真度上超越了现有方法，达到最先进的性能。

Conclusion: FLUX-Text通过轻量级设计和多模态字形条件，显著提升了多语言场景文本编辑的效果，尤其在复杂字形字符上表现突出。

Abstract: The task of scene text editing is to modify or add texts on images while
maintaining the fidelity of newly generated text and visual coherence with the
background. Recent works based on latent diffusion models (LDM) show improved
text editing results, yet still face challenges and often generate inaccurate
or unrecognizable characters, especially for non-Latin ones (\eg, Chinese),
which have complex glyph structures. To address these issues, we present
FLUX-Text, a simple and advanced multilingual scene text editing framework
based on FLUX-Fill. Specifically, we carefully investigate glyph conditioning,
considering both visual and textual modalities. To retain the original
generative capabilities of FLUX-Fill while enhancing its understanding and
generation of glyphs, we propose lightweight glyph and text embedding modules.
Owning to the lightweight design, FLUX-Text is trained only with $100K$
training examples compared to current popular methods trained with 2.9M ones.
With no bells and whistles, our method achieves state-of-the-art performance on
text editing tasks. Qualitative and quantitative experiments on the public
datasets demonstrate that our method surpasses previous works in text fidelity.

</details>


### [73] [From Word to Sentence: A Large-Scale Multi-Instance Dataset for Open-Set Aerial Detection](https://arxiv.org/abs/2505.03334)
*Guoting Wei,Yu Liu,Xia Yuan,Xizhe Xue,Linlin Guo,Yifan Yang,Chunxia Zhao,Zongwen Bai,Haokui Zhang,Rong Xiao*

Main category: cs.CV

TL;DR: 论文提出了一种大规模语言引导的开集航空检测数据集MI-OAD，通过自动标注管道OS-W2S Label Engine生成，解决了现有数据集在细粒度检测上的不足。实验表明，使用该数据集训练的模型在零样本条件下性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有语言引导的航空检测方法因数据集限制，难以满足细粒度开集检测需求。

Method: 构建包含三个语言级别（词、短语、句子）的大规模数据集MI-OAD，并开发自动标注管道OS-W2S Label Engine。

Result: MI-OAD包含163,023张图像和200万图像-文本对，规模是同类数据集的40倍。使用该数据集训练的模型在零样本条件下性能显著提升（如Grounding DINO的AP_{50}提升29.5）。

Conclusion: MI-OAD和OS-W2S Label Engine为开集航空检测提供了有效支持，未来将公开数据集和标注工具。

Abstract: In recent years, language-guided open-world aerial object detection has
gained significant attention due to its better alignment with real-world
application needs. However, due to limited datasets, most existing
language-guided methods primarily focus on vocabulary, which fails to meet the
demands of more fine-grained open-world detection. To address this limitation,
we propose constructing a large-scale language-guided open-set aerial detection
dataset, encompassing three levels of language guidance: from words to phrases,
and ultimately to sentences. Centered around an open-source large
vision-language model and integrating image-operation-based preprocessing with
BERT-based postprocessing, we present the OS-W2S Label Engine, an automatic
annotation pipeline capable of handling diverse scene annotations for aerial
images. Using this label engine, we expand existing aerial detection datasets
with rich textual annotations and construct a novel benchmark dataset, called
Multi-instance Open-set Aerial Dataset (MI-OAD), addressing the limitations of
current remote sensing grounding data and enabling effective open-set aerial
detection. Specifically, MI-OAD contains 163,023 images and 2 million
image-caption pairs, approximately 40 times larger than comparable datasets. We
also employ state-of-the-art open-set methods from the natural image domain,
trained on our proposed dataset, to validate the model's open-set detection
capabilities. For instance, when trained on our dataset, Grounding DINO
achieves improvements of 29.5 AP_{50} and 33.7 Recall@10 for sentence inputs
under zero-shot transfer conditions. Both the dataset and the label engine will
be released publicly.

</details>


### [74] [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/abs/2505.03414)
*Fangming Cui,Yonggang Zhang,Xuan Wang,Xinmei Tian,Jun Yu*

Main category: cs.CV

TL;DR: 提出了一种特征矩阵（FM）正则化方法，用于增强大型视觉语言模型在非特定目标任务上的表现，避免过拟合。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法在非特定目标任务上表现不佳，可能因过拟合导致模型遗忘通用知识。

Method: 通过提取和利用通用知识构建特征矩阵（FM），从深度和细粒度角度捕捉输入语义，保留通用知识。

Result: FM作为通用模块兼容现有框架，显著提升非特定目标任务性能，达到最先进水平。

Conclusion: FM方法有效缓解过拟合问题，提升模型在非特定目标任务上的泛化能力。

Abstract: Recent developments in prompt learning of large vision-language models have
significantly improved performance in target-specific tasks. However, these
prompt optimizing methods often struggle to tackle the target-unspecific or
generalizable tasks effectively. It may be attributed to the fact that
overfitting training causes the model to forget its general knowledge having
strong promotion on target-unspecific tasks. To alleviate this issue, we
propose a novel Features Matrix (FM) regularization approach designed to
enhance these models on target-unspecific tasks. Our method extracts and
leverages general knowledge, shaping a Features Matrix (FM). Specifically, the
FM captures the semantics of diverse inputs from a deep and fine perspective,
preserving essential general knowledge, which mitigates the risk of
overfitting. Representative evaluations demonstrate that: 1) the FM is
compatible with existing frameworks as a generic and flexible module, and 2)
the FM significantly showcases its effectiveness in enhancing target-unspecific
tasks, achieving state-of-the-art performance.

</details>


### [75] [A Vision-Language Model for Focal Liver Lesion Classification](https://arxiv.org/abs/2505.03350)
*Song Jian,Hu Yuchang,Wang Hui,Chen Yen-Wei*

Main category: cs.CV

TL;DR: Liver-VLM是一种基于视觉-语言模型（VLM）的肝脏病灶分类方法，通过结合文本和图像特征，在小规模标注数据下表现优于传统CNN和标准CLIP模型。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型依赖大规模标注数据，而医学影像数据标注有限。VLM通过多模态学习（文本+图像）在小数据下表现更优。

Method: Liver-VLM将类别信息融入文本编码器，通过计算图像与文本嵌入的余弦相似度，并用交叉熵损失优化模型。

Result: 在MPCT-FLLs数据集上，Liver-VLM在准确率和AUC上优于标准CLIP和MedCLIP，轻量级ResNet18进一步提升了性能。

Conclusion: Liver-VLM在小数据条件下表现优异，为医学影像分类提供了新思路。

Abstract: Accurate classification of focal liver lesions is crucial for diagnosis and
treatment in hepatology. However, traditional supervised deep learning models
depend on large-scale annotated datasets, which are often limited in medical
imaging. Recently, Vision-Language models (VLMs) such as Contrastive
Language-Image Pre-training model (CLIP) has been applied to image
classifications. Compared to the conventional convolutional neural network
(CNN), which classifiers image based on visual information only, VLM leverages
multimodal learning with text and images, allowing it to learn effectively even
with a limited amount of labeled data. Inspired by CLIP, we pro-pose a
Liver-VLM, a model specifically designed for focal liver lesions (FLLs)
classification. First, Liver-VLM incorporates class information into the text
encoder without introducing additional inference overhead. Second, by
calculating the pairwise cosine similarities between image and text embeddings
and optimizing the model with a cross-entropy loss, Liver-VLM ef-fectively
aligns image features with class-level text features. Experimental results on
MPCT-FLLs dataset demonstrate that the Liver-VLM model out-performs both the
standard CLIP and MedCLIP models in terms of accuracy and Area Under the Curve
(AUC). Further analysis shows that using a lightweight ResNet18 backbone
enhances classification performance, particularly under data-constrained
conditions.

</details>


### [76] [GUAVA: Generalizable Upper Body 3D Gaussian Avatar](https://arxiv.org/abs/2505.03351)
*Dongbin Zhang,Yunfei Liu,Lijian Lin,Ye Zhu,Yang Li,Minghan Qin,Yu Li,Haoqian Wang*

Main category: cs.CV

TL;DR: GUAVA框架通过单张图像快速重建高质量、可动画化的3D人体上半身高斯化虚拟形象，显著提升渲染质量和速度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多视图或视频，且受限于SMPLX模型的表达能力，难以捕捉面部表情。

Method: 引入表达性人体模型（EHM）增强面部表情能力，提出基于高斯化重建的GUAVA框架，结合逆向纹理映射和投影采样技术。

Result: GUAVA在渲染质量和速度上显著优于现有方法，重建时间仅0.1秒，支持实时动画和渲染。

Conclusion: GUAVA为单图像驱动的3D虚拟形象重建提供了高效解决方案，适用于广泛的应用场景。

Abstract: Reconstructing a high-quality, animatable 3D human avatar with expressive
facial and hand motions from a single image has gained significant attention
due to its broad application potential. 3D human avatar reconstruction
typically requires multi-view or monocular videos and training on individual
IDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's
expressiveness, these methods often focus on body motion but struggle with
facial expressions. To address these challenges, we first introduce an
expressive human model (EHM) to enhance facial expression capabilities and
develop an accurate tracking method. Based on this template model, we propose
GUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar
reconstruction. We leverage inverse texture mapping and projection sampling
techniques to infer Ubody (upper-body) Gaussians from a single image. The
rendered images are refined through a neural refiner. Experimental results
demonstrate that GUAVA significantly outperforms previous methods in rendering
quality and offers significant speed improvements, with reconstruction times in
the sub-second range (0.1s), and supports real-time animation and rendering.

</details>


### [77] [Interpretable Zero-shot Learning with Infinite Class Concepts](https://arxiv.org/abs/2505.03361)
*Zihan Ye,Shreyank N Gowda,Shiming Chen,Yaochu Jin,Kaizhu Huang,Xiaobo Jin*

Main category: cs.CV

TL;DR: 本文提出InfZSL框架，利用LLM动态生成无限短语级类概念，通过熵评分选择最优概念，提升零样本学习的迁移性和判别性。


<details>
  <summary>Details</summary>
Motivation: 现有ZSL方法依赖人工标注或LLM生成类语义，但存在透明度和幻觉问题，导致非视觉类语义。

Method: 提出InfZSL框架，利用LLM生成短语级类概念，引入熵评分机制选择最优概念。

Result: 在三个基准数据集上表现显著提升，生成可解释的图像相关概念。

Conclusion: InfZSL通过动态生成和选择概念，解决了ZSL中的迁移性和判别性问题。

Abstract: Zero-shot learning (ZSL) aims to recognize unseen classes by aligning images
with intermediate class semantics, like human-annotated concepts or class
definitions. An emerging alternative leverages Large-scale Language Models
(LLMs) to automatically generate class documents. However, these methods often
face challenges with transparency in the classification process and may suffer
from the notorious hallucination problem in LLMs, resulting in non-visual class
semantics. This paper redefines class semantics in ZSL with a focus on
transferability and discriminability, introducing a novel framework called
Zero-shot Learning with Infinite Class Concepts (InfZSL). Our approach
leverages the powerful capabilities of LLMs to dynamically generate an
unlimited array of phrase-level class concepts. To address the hallucination
challenge, we introduce an entropy-based scoring process that incorporates a
``goodness" concept selection mechanism, ensuring that only the most
transferable and discriminative concepts are selected. Our InfZSL framework not
only demonstrates significant improvements on three popular benchmark datasets
but also generates highly interpretable, image-grounded concepts. Code will be
released upon acceptance.

</details>


### [78] [3D Surface Reconstruction with Enhanced High-Frequency Details](https://arxiv.org/abs/2505.03362)
*Shikun Zhang,Yiqun Wang,Cunjian Chen,Yong Li,Qiuhong Ke*

Main category: cs.CV

TL;DR: FreNeuS通过高频信息改进神经隐式3D重建，解决了现有方法表面细节不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有神经表面重建方法随机采样整张图像，难以学习表面高频细节，导致重建结果过于平滑。

Method: FreNeuS利用像素梯度变化获取图像高频区域，动态采样光线，并设计高频加权方法增强表面细节重建。

Result: 实验表明，FreNeuS能重建精细表面细节，优于现有方法，且适用于任何基于NeuS的工作。

Conclusion: FreNeuS通过高频信息引导重建，显著提升了表面细节的恢复能力。

Abstract: Neural implicit 3D reconstruction can reproduce shapes without 3D
supervision, and it learns the 3D scene through volume rendering methods and
neural implicit representations. Current neural surface reconstruction methods
tend to randomly sample the entire image, making it difficult to learn
high-frequency details on the surface, and thus the reconstruction results tend
to be too smooth. We designed a method (FreNeuS) based on high-frequency
information to solve the problem of insufficient surface detail. Specifically,
FreNeuS uses pixel gradient changes to easily acquire high-frequency regions in
an image and uses the obtained high-frequency information to guide surface
detail reconstruction. High-frequency information is first used to guide the
dynamic sampling of rays, applying different sampling strategies according to
variations in high-frequency regions. To further enhance the focus on surface
details, we have designed a high-frequency weighting method that constrains the
representation of high-frequency details during the reconstruction process.
Qualitative and quantitative results show that our method can reconstruct fine
surface details and obtain better surface reconstruction quality compared to
existing methods. In addition, our method is more applicable and can be
generalized to any NeuS-based work.

</details>


### [79] [Reducing Annotation Burden in Physical Activity Research Using Vision-Language Models](https://arxiv.org/abs/2505.03374)
*Abram Schonfeldt,Benjamin Maylor,Xiaofang Chen,Ronald Clark,Aiden Doherty*

Main category: cs.CV

TL;DR: 论文比较了三种视觉语言模型和两种判别模型在自由生活环境下预测久坐行为的性能，发现开源视觉语言模型和微调判别模型在未见过的参与者中表现相当，但性能随活动强度增加而下降。


<details>
  <summary>Details</summary>
Motivation: 验证和开发基于可穿戴设备的健康研究方法，减少图像序列标注的工作负担。

Method: 比较三种视觉语言模型和两种判别模型在两个自由生活验证研究中的性能。

Result: 开源视觉语言模型和微调判别模型在预测久坐行为上表现相当，但性能随活动强度增加而下降，且在外部数据集上表现显著降低。

Conclusion: 开源计算机视觉模型可用于减少标注负担，尤其在相似人群中预测久坐行为。

Abstract: Introduction: Data from wearable devices collected in free-living settings,
and labelled with physical activity behaviours compatible with health research,
are essential for both validating existing wearable-based measurement
approaches and developing novel machine learning approaches. One common way of
obtaining these labels relies on laborious annotation of sequences of images
captured by cameras worn by participants through the course of a day. Methods:
We compare the performance of three vision language models and two
discriminative models on two free-living validation studies with 161 and 111
participants, collected in Oxfordshire, United Kingdom and Sichuan, China,
respectively, using the Autographer (OMG Life, defunct) wearable camera.
Results: We found that the best open-source vision-language model (VLM) and
fine-tuned discriminative model (DM) achieved comparable performance when
predicting sedentary behaviour from single images on unseen participants in the
Oxfordshire study; median F1-scores: VLM = 0.89 (0.84, 0.92), DM = 0.91 (0.86,
0.95). Performance declined for light (VLM = 0.60 (0.56,0.67), DM = 0.70 (0.63,
0.79)), and moderate-to-vigorous intensity physical activity (VLM = 0.66 (0.53,
0.85); DM = 0.72 (0.58, 0.84)). When applied to the external Sichuan study,
performance fell across all intensity categories, with median Cohen's
kappa-scores falling from 0.54 (0.49, 0.64) to 0.26 (0.15, 0.37) for the VLM,
and from 0.67 (0.60, 0.74) to 0.19 (0.10, 0.30) for the DM. Conclusion: Freely
available computer vision models could help annotate sedentary behaviour,
typically the most prevalent activity of daily living, from wearable camera
images within similar populations to seen data, reducing the annotation burden.

</details>


### [80] [Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant](https://arxiv.org/abs/2505.03380)
*Haonan Wang,Jiaji Mao,Lehan Wang,Qixiang Zhang,Marawan Elbatel,Yi Qin,Huijun Hu,Baoxun Li,Wenhui Deng,Weifeng Qin,Hongrui Li,Jialin Liang,Jun Shen,Xiaomeng Li*

Main category: cs.CV

TL;DR: RCMed是一种全栈AI助手，通过多模态对齐和分层视觉-语言基础，提升医学诊断的精确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决医学AI助手在多模态内容准确性不足和真实场景验证不足的问题。

Method: 采用自增强相关机制和颜色区域描述策略，结合视觉特征和语言语义，形成闭环优化。

Result: 在165个临床任务中表现优异，细胞分割相对提升23.5%，并在20种癌症类型的外部验证中达到最优。

Conclusion: RCMed展示了多模态模型在复杂场景中实现人类水平解释的能力，推动了以人为中心的AI医疗发展。

Abstract: Medical AI assistants support doctors in disease diagnosis, medical image
analysis, and report generation. However, they still face significant
challenges in clinical use, including limited accuracy with multimodal content
and insufficient validation in real-world settings. We propose RCMed, a
full-stack AI assistant that improves multimodal alignment in both input and
output, enabling precise anatomical delineation, accurate localization, and
reliable diagnosis through hierarchical vision-language grounding. A
self-reinforcing correlation mechanism allows visual features to inform
language context, while language semantics guide pixel-wise attention, forming
a closed loop that refines both modalities. This correlation is enhanced by a
color region description strategy, translating anatomical structures into
semantically rich text to learn shape-location-text relationships across
scales. Trained on 20 million image-mask-description triplets, RCMed achieves
state-of-the-art precision in contextualizing irregular lesions and subtle
anatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It
achieved a 23.5% relative improvement in cell segmentation from microscopy
images over prior methods. RCMed's strong vision-language alignment enables
exceptional generalization, with state-of-the-art performance in external
validation across 20 clinically significant cancer types, including novel
tasks. This work demonstrates how integrated multimodal models capture
fine-grained patterns, enabling human-level interpretation in complex scenarios
and advancing human-centric AI healthcare.

</details>


### [81] [Attention-aggregated Attack for Boosting the Transferability of Facial Adversarial Examples](https://arxiv.org/abs/2505.03383)
*Jian-Wei Li,Wen-Ze Shao*

Main category: cs.CV

TL;DR: 本文提出了一种名为注意力聚合攻击（AAA）的新方法，通过模仿其他FR模型的注意力机制，增强对抗样本在面部识别任务中的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 针对细粒度视觉任务（如面部识别）中特定类别的深度模型的对抗攻击性能不足问题，研究面部特征对FR模型嵌入学习的影响。

Method: 提出AAA方法，通过注意力分歧模仿其他FR模型对干净面部图像的注意力，破坏关键面部特征。

Result: 在多种FR模型上的实验验证了AAA方法的优越性和鲁棒性。

Conclusion: AAA方法显著提升了对抗样本在FR任务中的可迁移性和攻击效果。

Abstract: Adversarial examples have revealed the vulnerability of deep learning models
and raised serious concerns about information security. The transfer-based
attack is a hot topic in black-box attacks that are practical to real-world
scenarios where the training datasets, parameters, and structure of the target
model are unknown to the attacker. However, few methods consider the
particularity of class-specific deep models for fine-grained vision tasks, such
as face recognition (FR), giving rise to unsatisfactory attacking performance.
In this work, we first investigate what in a face exactly contributes to the
embedding learning of FR models and find that both decisive and auxiliary
facial features are specific to each FR model, which is quite different from
the biological mechanism of human visual system. Accordingly we then propose a
novel attack method named Attention-aggregated Attack (AAA) to enhance the
transferability of adversarial examples against FR, which is inspired by the
attention divergence and aims to destroy the facial features that are critical
for the decision-making of other FR models by imitating their attentions on the
clean face images. Extensive experiments conducted on various FR models
validate the superiority and robust effectiveness of the proposed method over
existing methods.

</details>


### [82] [EOPose : Exemplar-based object reposing using Generalized Pose Correspondences](https://arxiv.org/abs/2505.03394)
*Sarthak Mehrotra,Rishabh Jain,Mayur Hemani,Balaji Krishnamurthy,Mausoom Sarkar*

Main category: cs.CV

TL;DR: EOPose是一种端到端框架，利用无监督关键点检测实现物体重摆，保留细节并生成高质量结果。


<details>
  <summary>Details</summary>
Motivation: 电子商务需要快速生成产品图像变体，现有方法难以保留细节。

Method: 采用三步法，利用目标姿态引导图像的关键点对应关系进行变形和重渲染。

Result: 生成高质量重摆图像，PSNR、SSIM和FID指标优异。

Conclusion: EOPose有效且实用，通过消融和用户研究验证其性能。

Abstract: Reposing objects in images has a myriad of applications, especially for
e-commerce where several variants of product images need to be produced
quickly. In this work, we leverage the recent advances in unsupervised keypoint
correspondence detection between different object images of the same class to
propose an end-to-end framework for generic object reposing. Our method,
EOPose, takes a target pose-guidance image as input and uses its keypoint
correspondence with the source object image to warp and re-render the latter
into the target pose using a novel three-step approach. Unlike generative
approaches, our method also preserves the fine-grained details of the object
such as its exact colors, textures, and brand marks. We also prepare a new
dataset of paired objects based on the Objaverse dataset to train and test our
network. EOPose produces high-quality reposing output as evidenced by different
image quality metrics (PSNR, SSIM and FID). Besides a description of the method
and the dataset, the paper also includes detailed ablation and user studies to
indicate the efficacy of the proposed method

</details>


### [83] [DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation](https://arxiv.org/abs/2505.03401)
*Shanshan Song,Hui Tang,Honglong Yang,Xiaomeng Li*

Main category: cs.CV

TL;DR: 论文提出了一种动态差异感知时序残差网络（DDaTR），用于纵向放射学报告生成（LRRG），通过捕捉多级空间相关性和时序相关性，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有LRRG方法在特征提取过程中难以有效捕捉空间和时序相关性，导致差异信息提取不足，影响报告生成质量。

Method: 设计了动态特征对齐模块（DFAM）和动态差异感知模块（DDAM），结合动态残差网络，捕捉多级空间和时序相关性。

Result: 在三个基准测试中表现优于现有方法，证明了其在RRG和LRRG任务中的有效性。

Conclusion: DDaTR通过改进特征提取和时序建模，显著提升了纵向放射学报告生成的性能。

Abstract: Radiology Report Generation (RRG) automates the creation of radiology reports
from medical imaging, enhancing the efficiency of the reporting process.
Longitudinal Radiology Report Generation (LRRG) extends RRG by incorporating
the ability to compare current and prior exams, facilitating the tracking of
temporal changes in clinical findings. Existing LRRG approaches only extract
features from prior and current images using a visual pre-trained encoder,
which are then concatenated to generate the final report. However, these
methods struggle to effectively capture both spatial and temporal correlations
during the feature extraction process. Consequently, the extracted features
inadequately capture the information of difference across exams and thus
underrepresent the expected progressions, leading to sub-optimal performance in
LRRG. To address this, we develop a novel dynamic difference-aware temporal
residual network (DDaTR). In DDaTR, we introduce two modules at each stage of
the visual encoder to capture multi-level spatial correlations. The Dynamic
Feature Alignment Module (DFAM) is designed to align prior features across
modalities for the integrity of prior clinical information. Prompted by the
enriched prior features, the dynamic difference-aware module (DDAM) captures
favorable difference information by identifying relationships across exams.
Furthermore, our DDaTR employs the dynamic residual network to unidirectionally
transmit longitudinal information, effectively modelling temporal correlations.
Extensive experiments demonstrated superior performance over existing methods
on three benchmarks, proving its efficacy in both RRG and LRRG tasks.

</details>


### [84] [CXR-AD: Component X-ray Image Dataset for Industrial Anomaly Detection](https://arxiv.org/abs/2505.03412)
*Haoyu Bai,Jie Wang,Gaomin Li,Xuan Li,Xiaohu Zhang,Xia Yang*

Main category: cs.CV

TL;DR: 论文提出了首个公开的X射线组件异常检测数据集CXR-AD，填补了内部缺陷检测数据集的空白，并分析了其技术挑战和现有算法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测数据集主要关注表面缺陷，缺乏针对组件内部缺陷的公开X射线数据集，因此需要构建CXR-AD以推动相关研究。

Method: 构建了包含653个正常样本和561个缺陷样本的X射线数据集，并标注了像素级掩码。分析了数据集特性，并评估了三种先进异常检测框架的性能。

Result: 实验显示，现有算法在CXR-AD上的性能平均下降29.78%，表明其在处理内部缺陷检测任务时的局限性。

Conclusion: CXR-AD为内部缺陷检测提供了首个公开基准，有助于算法开发和提升检测精度。

Abstract: Internal defect detection constitutes a critical process in ensuring
component quality, for which anomaly detection serves as an effective solution.
However, existing anomaly detection datasets predominantly focus on surface
defects in visible-light images, lacking publicly available X-ray datasets
targeting internal defects in components. To address this gap, we construct the
first publicly accessible component X-ray anomaly detection (CXR-AD) dataset,
comprising real-world X-ray images. The dataset covers five industrial
component categories, including 653 normal samples and 561 defect samples with
precise pixel-level mask annotations. We systematically analyze the dataset
characteristics and identify three major technical challenges: (1) strong
coupling between complex internal structures and defect regions, (2) inherent
low contrast and high noise interference in X-ray imaging, and (3) significant
variations in defect scales and morphologies. To evaluate dataset complexity,
we benchmark three state-of-the-art anomaly detection frameworks
(feature-based, reconstruction-based, and zero-shot learning methods).
Experimental results demonstrate a 29.78% average performance degradation on
CXR-AD compared to MVTec AD, highlighting the limitations of current algorithms
in handling internal defect detection tasks. To the best of our knowledge,
CXR-AD represents the first publicly available X-ray dataset for component
anomaly detection, providing a real-world industrial benchmark to advance
algorithm development and enhance precision in internal defect inspection
technologies.

</details>


### [85] [LiftFeat: 3D Geometry-Aware Local Feature Matching](https://arxiv.org/abs/2505.03422)
*Yepeng Liu,Wenpeng Lai,Zhou Zhao,Yuxuan Xiong,Jinchi Zhu,Jun Cheng,Yongchao Xu*

Main category: cs.CV

TL;DR: 提出了一种轻量级网络LiftFeat，通过融合3D几何特征提升局部特征匹配的鲁棒性和判别力。


<details>
  <summary>Details</summary>
Motivation: 在光照变化大、低纹理区域或重复图案场景中，提取鲁棒且判别性强的视觉特征仍具挑战性。

Method: 采用预训练的单目深度估计模型生成伪表面法线标签，监督3D几何特征的提取，并设计3D几何感知的特征提升模块，融合表面法线特征与原始2D描述符特征。

Result: 在相对位姿估计、单应性估计和视觉定位任务中，LiftFeat优于一些轻量级先进方法。

Conclusion: LiftFeat通过引入3D几何特征，显著提升了极端条件下的特征判别能力。

Abstract: Robust and efficient local feature matching plays a crucial role in
applications such as SLAM and visual localization for robotics. Despite great
progress, it is still very challenging to extract robust and discriminative
visual features in scenarios with drastic lighting changes, low texture areas,
or repetitive patterns. In this paper, we propose a new lightweight network
called \textit{LiftFeat}, which lifts the robustness of raw descriptor by
aggregating 3D geometric feature. Specifically, we first adopt a pre-trained
monocular depth estimation model to generate pseudo surface normal label,
supervising the extraction of 3D geometric feature in terms of predicted
surface normal. We then design a 3D geometry-aware feature lifting module to
fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D
geometric feature enhances the discriminative ability of 2D feature description
in extreme conditions. Extensive experimental results on relative pose
estimation, homography estimation, and visual localization tasks, demonstrate
that our LiftFeat outperforms some lightweight state-of-the-art methods. Code
will be released at : https://github.com/lyp-deeplearning/LiftFeat.

</details>


### [86] [Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI Synthesis: Advancing Pretraining and Clinical Applications](https://arxiv.org/abs/2505.03426)
*Ziyu Li,Yujian Hu,Zhengyao Ding,Yiheng Mao,Haitao Li,Fan Yi,Hongkun Zhang,Zhengxing Huang*

Main category: cs.CV

TL;DR: 提出了一种名为CPGG的新方法，通过生成多样化的CMR数据来解决AI模型在心脏疾病诊断中数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 由于大规模高质量CMR数据的稀缺，AI模型在心脏疾病诊断中的应用受到限制。

Method: CPGG框架分为两阶段：首先生成模型基于心脏表型训练，随后通过掩码自回归扩散模型生成高保真CMR序列。

Result: 生成的合成CMR数据显著提升了多种下游任务的性能，包括诊断和表型预测。

Conclusion: CPGG方法有效解决了数据不足问题，提升了AI模型在心脏疾病领域的表现。

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for
diagnosing heart diseases and evaluating cardiac health. However, the limited
availability of large-scale, high-quality CMR datasets poses a major challenge
to the effective application of artificial intelligence (AI) in this domain.
Even the amount of unlabeled data and the health status it covers are difficult
to meet the needs of model pretraining, which hinders the performance of AI
models on downstream tasks. In this study, we present Cardiac Phenotype-Guided
CMR Generation (CPGG), a novel approach for generating diverse CMR data that
covers a wide spectrum of cardiac health status. The CPGG framework consists of
two stages: in the first stage, a generative model is trained using cardiac
phenotypes derived from CMR data; in the second stage, a masked autoregressive
diffusion model, conditioned on these phenotypes, generates high-fidelity CMR
cine sequences that capture both structural and functional features of the
heart in a fine-grained manner. We synthesized a massive amount of CMR to
expand the pretraining data. Experimental results show that CPGG generates
high-quality synthetic CMR data, significantly improving performance on various
downstream tasks, including diagnosis and cardiac phenotypes prediction. These
gains are demonstrated across both public and private datasets, highlighting
the effectiveness of our approach. Code is availabel at
https://anonymous.4open.science/r/CPGG.

</details>


### [87] [A Fusion-Guided Inception Network for Hyperspectral Image Super-Resolution](https://arxiv.org/abs/2505.03431)
*Usman Muhammad,Jorma Laaksonen*

Main category: cs.CV

TL;DR: 提出了一种名为FGIN的单图像超分辨率模型，通过光谱-空间融合模块和多尺度特征提取策略，解决了传统方法依赖精确对齐的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖图像对的精确对齐，但在实际场景中难以实现，因此需要一种不依赖对齐的单图像超分辨率方法。

Method: 采用光谱-空间融合模块早期整合信息，使用Inception-like分层特征提取策略捕获多尺度空间依赖，结合优化的上采样模块提升重建质量。

Result: 在两个公开的高光谱数据集上表现出竞争力。

Conclusion: FGIN模型有效解决了传统方法对齐依赖的问题，并在实验中表现出优越性能。

Abstract: The fusion of low-spatial-resolution hyperspectral images (HSIs) with
high-spatial-resolution conventional images (e.g., panchromatic or RGB) has
played a significant role in recent advancements in HSI super-resolution.
However, this fusion process relies on the availability of precise alignment
between image pairs, which is often challenging in real-world scenarios. To
mitigate this limitation, we propose a single-image super-resolution model
called the Fusion-Guided Inception Network (FGIN). Specifically, we first
employ a spectral-spatial fusion module to effectively integrate spectral and
spatial information at an early stage. Next, an Inception-like hierarchical
feature extraction strategy is used to capture multiscale spatial dependencies,
followed by a dedicated multi-scale fusion block. To further enhance
reconstruction quality, we incorporate an optimized upsampling module that
combines bilinear interpolation with depthwise separable convolutions.
Experimental evaluations on two publicly available hyperspectral datasets
demonstrate the competitive performance of our method.

</details>


### [88] [Robustness in AI-Generated Detection: Enhancing Resistance to Adversarial Attacks](https://arxiv.org/abs/2505.03435)
*Sun Haoxuan,Hong Yan,Zhan Jiahui,Chen Haoxing,Lan Jun,Zhu Huijia,Wang Weiqiang,Zhang Liqing,Zhang Jianfu*

Main category: cs.CV

TL;DR: 本文研究了AI生成人脸检测系统的脆弱性，提出了一种结合对抗训练和扩散反演的方法，显著提升了检测系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 生成图像技术的快速发展引发了安全担忧，尤其是人脸生成检测领域。现有检测方法在标准条件下表现良好，但对对抗攻击的鲁棒性有限。

Method: 提出了一种结合对抗训练的方法，并利用扩散反演和重建技术增强检测鲁棒性。

Result: 实验表明，现有系统易受对抗扰动影响，但新方法显著提升了鲁棒性。

Conclusion: 该方法有效提升了AI生成人脸检测的鲁棒性，并公开了代码以促进进一步研究。

Abstract: The rapid advancement of generative image technology has introduced
significant security concerns, particularly in the domain of face generation
detection. This paper investigates the vulnerabilities of current AI-generated
face detection systems. Our study reveals that while existing detection methods
often achieve high accuracy under standard conditions, they exhibit limited
robustness against adversarial attacks. To address these challenges, we propose
an approach that integrates adversarial training to mitigate the impact of
adversarial examples. Furthermore, we utilize diffusion inversion and
reconstruction to further enhance detection robustness. Experimental results
demonstrate that minor adversarial perturbations can easily bypass existing
detection systems, but our method significantly improves the robustness of
these systems. Additionally, we provide an in-depth analysis of adversarial and
benign examples, offering insights into the intrinsic characteristics of
AI-generated content. All associated code will be made publicly available in a
dedicated repository to facilitate further research and verification.

</details>


### [89] [Polar Coordinate-Based 2D Pose Prior with Neural Distance Field](https://arxiv.org/abs/2505.03445)
*Qi Gan,Sao Mai Nguyen,Eric Fenaux,Stephan Clémençon,Mounîm El Yacoubi*

Main category: cs.CV

TL;DR: 论文提出了一种基于神经距离场（NDF）的2D姿态先验引导优化方法，通过极坐标表示和新型非测地距离度量，提升了运动模糊和遮挡场景下的姿态估计准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB视频的深度学习姿态估计模型在真实体育场景中因运动模糊、遮挡和领域偏移而效果不佳，需大量标注数据且泛化能力有限。

Method: 采用极坐标表示结合关节连接长度，定义非测地距离度量，并提出梯度批量投影增强策略以缓解数据稀缺问题。

Result: 在跳远数据集上验证，方法能跨多种姿态表示提升估计准确性，仅需少量训练数据即可增强姿态合理性。

Conclusion: 所提方法通过极坐标表示和NDF优化，显著提升了姿态估计的鲁棒性和泛化能力。

Abstract: Human pose capture is essential for sports analysis, enabling precise
evaluation of athletes' movements. While deep learning-based human pose
estimation (HPE) models from RGB videos have achieved impressive performance on
public datasets, their effectiveness in real-world sports scenarios is often
hindered by motion blur, occlusions, and domain shifts across different pose
representations. Fine-tuning these models can partially alleviate such
challenges but typically requires large-scale annotated data and still
struggles to generalize across diverse sports environments. To address these
limitations, we propose a 2D pose prior-guided refinement approach based on
Neural Distance Fields (NDF). Unlike existing approaches that rely solely on
angular representations of human poses, we introduce a polar coordinate-based
representation that explicitly incorporates joint connection lengths, enabling
a more accurate correction of erroneous pose estimations. Additionally, we
define a novel non-geodesic distance metric that separates angular and radial
discrepancies, which we demonstrate is better suited for polar representations
than traditional geodesic distances. To mitigate data scarcity, we develop a
gradient-based batch-projection augmentation strategy, which synthesizes
realistic pose samples through iterative refinement. Our method is evaluated on
a long jump dataset, demonstrating its ability to improve 2D pose estimation
across multiple pose representations, making it robust across different
domains. Experimental results show that our approach enhances pose plausibility
while requiring only limited training data. Code is available at:
https://github.com/QGAN2019/polar-NDF.

</details>


### [90] [Nonperiodic dynamic CT reconstruction using backward-warping INR with regularization of diffeomorphism (BIRD)](https://arxiv.org/abs/2505.03463)
*Muge Du,Zhuozhao Zheng,Wenying Wang,Guotao Quan,Wuliang Shi,Le Shen,Li Zhang,Liang Li,Yinong Liu,Yuxiang Xing*

Main category: cs.CV

TL;DR: BIRD框架通过反向变形、微分同胚正则化、运动补偿重建和降维设计，解决了非周期性动态CT重建中的计算效率、解剖合理性和细节保留问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法和深度学习在非周期性快速运动（如心脏成像）中面临运动伪影和泛化问题，INR技术虽有潜力但存在计算效率低和解剖合理性不足等限制。

Method: 提出BIRD框架，包括反向变形、微分同胚正则化、运动补偿重建和降维设计。

Result: 实验表明，BIRD在数字/物理模型和患者数据中均能减少运动伪影并增强细节。

Conclusion: BIRD为临床动态CT重建提供了更准确的解决方案，适用于单次心跳重建等功能成像。

Abstract: Dynamic computed tomography (CT) reconstruction faces significant challenges
in addressing motion artifacts, particularly for nonperiodic rapid movements
such as cardiac imaging with fast heart rates. Traditional methods struggle
with the extreme limited-angle problems inherent in nonperiodic cases. Deep
learning methods have improved performance but face generalization challenges.
Recent implicit neural representation (INR) techniques show promise through
self-supervised deep learning, but have critical limitations: computational
inefficiency due to forward-warping modeling, difficulty balancing DVF
complexity with anatomical plausibility, and challenges in preserving fine
details without additional patient-specific pre-scans. This paper presents a
novel INR-based framework, BIRD, for nonperiodic dynamic CT reconstruction. It
addresses these challenges through four key contributions: (1) backward-warping
deformation that enables direct computation of each dynamic voxel with
significantly reduced computational cost, (2) diffeomorphism-based DVF
regularization that ensures anatomically plausible deformations while
maintaining representational capacity, (3) motion-compensated analytical
reconstruction that enhances fine details without requiring additional
pre-scans, and (4) dimensional-reduction design for efficient 4D coordinate
encoding. Through various simulations and practical studies, including digital
and physical phantoms and retrospective patient data, we demonstrate the
effectiveness of our approach for nonperiodic dynamic CT reconstruction with
enhanced details and reduced motion artifacts. The proposed framework enables
more accurate dynamic CT reconstruction with potential clinical applications,
such as one-beat cardiac reconstruction, cinematic image sequences for
functional imaging, and motion artifact reduction in conventional CT scans.

</details>


### [91] [Blending 3D Geometry and Machine Learning for Multi-View Stereopsis](https://arxiv.org/abs/2505.03470)
*Vibhas Vats,Md. Alimoor Reza,David Crandall,Soon-heung Jung*

Main category: cs.CV

TL;DR: GC MVSNet++通过在学习阶段主动实施多视角、多尺度的几何一致性检查，显著加速训练过程，并在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统MVS方法依赖光度一致性，而现代学习算法仅将几何一致性作为后处理步骤，未直接影响学习过程。

Method: 提出GC MVSNet++，在学习阶段集成多视角、多尺度的几何一致性检查，并设计密集连接的成本正则化网络。

Result: 在DTU和BlendedMVS数据集上达到最先进性能，在Tanks and Temples基准上排名第二。

Conclusion: GC MVSNet++是首个在学习阶段实施多视角、多尺度几何一致性的方法，显著提升性能。

Abstract: Traditional multi-view stereo (MVS) methods primarily depend on photometric
and geometric consistency constraints. In contrast, modern learning-based
algorithms often rely on the plane sweep algorithm to infer 3D geometry,
applying explicit geometric consistency (GC) checks only as a post-processing
step, with no impact on the learning process itself. In this work, we introduce
GC MVSNet plus plus, a novel approach that actively enforces geometric
consistency of reference view depth maps across multiple source views (multi
view) and at various scales (multi scale) during the learning phase (see Fig.
1). This integrated GC check significantly accelerates the learning process by
directly penalizing geometrically inconsistent pixels, effectively halving the
number of training iterations compared to other MVS methods. Furthermore, we
introduce a densely connected cost regularization network with two distinct
block designs simple and feature dense optimized to harness dense feature
connections for enhanced regularization. Extensive experiments demonstrate that
our approach achieves a new state of the art on the DTU and BlendedMVS datasets
and secures second place on the Tanks and Temples benchmark. To our knowledge,
GC MVSNet plus plus is the first method to enforce multi-view, multi-scale
supervised geometric consistency during learning. Our code is available.

</details>


### [92] [UPMAD-Net: A Brain Tumor Segmentation Network with Uncertainty Guidance and Adaptive Multimodal Feature Fusion](https://arxiv.org/abs/2505.03494)
*Zhanyuan Jia,Ni Yao,Danyang Sun,Chuang Han,Yanting Li,Jiaofen Nan,Fubao Zhu,Chen Zhao,Weihua Zhou*

Main category: cs.CV

TL;DR: 提出了一种结合深度学习和区域生长算法的脑肿瘤分割方法，通过多尺度特征融合和自适应注意力机制提升性能，并在BraTS数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤分割对诊断和治疗至关重要，但由于肿瘤形状不规则、边界模糊和高度变异性，准确分割仍具挑战性。

Method: 采用多尺度特征融合模块和自适应注意力机制提取特征，结合蒙特卡洛Dropout策略进行不确定性估计。

Result: 在BraTS2021和BraTS2019数据集上，分割性能显著优于现有方法，Dice分数分别为ET 89.18%/87.43%、WT 93.67%/90.92%、TC 91.23%/90.40%。

Conclusion: 基于U-Net架构的新方法通过引入先验知识和不确定性估计，提升了分割的鲁棒性和性能，代码已开源。

Abstract: Background: Brain tumor segmentation has a significant impact on the
diagnosis and treatment of brain tumors. Accurate brain tumor segmentation
remains challenging due to their irregular shapes, vague boundaries, and high
variability. Objective: We propose a brain tumor segmentation method that
combines deep learning with prior knowledge derived from a region-growing
algorithm. Methods: The proposed method utilizes a multi-scale feature fusion
(MSFF) module and adaptive attention mechanisms (AAM) to extract multi-scale
features and capture global contextual information. To enhance the model's
robustness in low-confidence regions, the Monte Carlo Dropout (MC Dropout)
strategy is employed for uncertainty estimation. Results: Extensive experiments
demonstrate that the proposed method achieves superior performance on Brain
Tumor Segmentation (BraTS) datasets, significantly outperforming various
state-of-the-art methods. On the BraTS2021 dataset, the test Dice scores are
89.18% for Enhancing Tumor (ET) segmentation, 93.67% for Whole Tumor (WT)
segmentation, and 91.23% for Tumor Core (TC) segmentation. On the BraTS2019
validation set, the validation Dice scores are 87.43%, 90.92%, and 90.40% for
ET, WT, and TC segmentation, respectively. Ablation studies further confirmed
the contribution of each module to segmentation accuracy, indicating that each
component played a vital role in overall performance improvement. Conclusion:
This study proposed a novel 3D brain tumor segmentation network based on the
U-Net architecture. By incorporating the prior knowledge and employing the
uncertainty estimation method, the robustness and performance were improved.
The code for the proposed method is available at
https://github.com/chenzhao2023/UPMAD_Net_BrainSeg.

</details>


### [93] [MRI motion correction via efficient residual-guided denoising diffusion probabilistic models](https://arxiv.org/abs/2505.03498)
*Mojtaba Safari,Shansong Wang,Qiang Li,Zach Eidex,Richard L. J. Qiu,Chih-Wei Chang,Hui Mao,Xiaofeng Yang*

Main category: cs.CV

TL;DR: Res-MoCoDiff是一种高效的MRI运动伪影校正方法，通过残差误差转移机制和四步反向扩散实现快速去噪，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: MRI中的运动伪影严重影响图像质量和定量分析，传统方法成本高且流程复杂，需要更高效的解决方案。

Method: Res-MoCoDiff结合残差误差转移机制和U-net骨干网络，采用l1+l2损失函数训练，在合成和真实数据集上评估。

Result: 在所有运动严重程度下，Res-MoCoDiff表现最佳，PSNR达41.91 dB，采样时间大幅缩短至0.37秒。

Conclusion: Res-MoCoDiff在MRI运动伪影校正中表现出高效性和优越性能，具有实际应用潜力。

Abstract: Purpose: Motion artifacts in magnetic resonance imaging (MRI) significantly
degrade image quality and impair quantitative analysis. Conventional mitigation
strategies, such as repeated acquisitions or motion tracking, are costly and
workflow-intensive. This study introduces Res-MoCoDiff, an efficient denoising
diffusion probabilistic model tailored for MRI motion artifact correction.
Methods: Res-MoCoDiff incorporates a novel residual error shifting mechanism in
the forward diffusion process, aligning the noise distribution with
motion-corrupted data and enabling an efficient four-step reverse diffusion. A
U-net backbone enhanced with Swin-Transformer blocks conventional attention
layers, improving adaptability across resolutions. Training employs a combined
l1+l2 loss, which promotes image sharpness and reduces pixel-level errors.
Res-MoCoDiff was evaluated on synthetic dataset generated using a realistic
motion simulation framework and on an in-vivo dataset. Comparative analyses
were conducted against established methods, including CycleGAN, Pix2pix, and
MT-DDPM using quantitative metrics such as peak signal-to-noise ratio (PSNR),
structural similarity index measure (SSIM), and normalized mean squared error
(NMSE). Results: The proposed method demonstrated superior performance in
removing motion artifacts across all motion severity levels. Res-MoCoDiff
consistently achieved the highest SSIM and the lowest NMSE values, with a PSNR
of up to 41.91+-2.94 dB for minor distortions. Notably, the average sampling
time was reduced to 0.37 seconds per batch of two image slices, compared with
101.74 seconds for conventional approaches.

</details>


### [94] [Modality-Guided Dynamic Graph Fusion and Temporal Diffusion for Self-Supervised RGB-T Tracking](https://arxiv.org/abs/2505.03507)
*Shenglan Li,Rui Yao,Yong Zhou,Hancheng Zhu,Kunyang Sun,Bing Liu,Zhiwen Shao,Jiaqi Zhao*

Main category: cs.CV

TL;DR: GDSTrack提出了一种动态图融合和时序扩散方法，用于解决自监督RGB-T跟踪中的伪标签噪声和模态融合效率问题。


<details>
  <summary>Details</summary>
Motivation: 减少对大规模标注的依赖，同时解决伪标签噪声和背景噪声对模态融合效率的影响。

Method: 通过动态图融合模块（MDGF）和时序图扩散（TGID）技术，动态融合邻近帧模态并利用生成模型的去噪能力。

Result: 在四个公开RGB-T数据集上表现优于现有方法。

Conclusion: GDSTrack通过动态图融合和时序扩散有效提升了自监督RGB-T跟踪的性能。

Abstract: To reduce the reliance on large-scale annotations, self-supervised RGB-T
tracking approaches have garnered significant attention. However, the omission
of the object region by erroneous pseudo-label or the introduction of
background noise affects the efficiency of modality fusion, while pseudo-label
noise triggered by similar object noise can further affect the tracking
performance. In this paper, we propose GDSTrack, a novel approach that
introduces dynamic graph fusion and temporal diffusion to address the above
challenges in self-supervised RGB-T tracking. GDSTrack dynamically fuses the
modalities of neighboring frames, treats them as distractor noise, and
leverages the denoising capability of a generative model. Specifically, by
constructing an adjacency matrix via an Adjacency Matrix Generator (AMG), the
proposed Modality-guided Dynamic Graph Fusion (MDGF) module uses a dynamic
adjacency matrix to guide graph attention, focusing on and fusing the object's
coherent regions. Temporal Graph-Informed Diffusion (TGID) models MDGF features
from neighboring frames as interference, and thus improving robustness against
similar-object noise. Extensive experiments conducted on four public RGB-T
tracking datasets demonstrate that GDSTrack outperforms the existing
state-of-the-art methods. The source code is available at
https://github.com/LiShenglana/GDSTrack.

</details>


### [95] [Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks](https://arxiv.org/abs/2505.03522)
*Haotong Cheng,Zhiqi Zhang,Hao Li,Xinshang Zhang*

Main category: cs.CV

TL;DR: 论文提出“通用性”概念及评估方程（UAE），设计优化模块CRB和DCRB，实验证明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视模块可迁移性量化，需扩展“泛化性”概念以揭示模块通用性与模型泛化能力的关系。

Method: 引入UAE评估模块可移植性，基于结果设计CRB和DCRB模块。

Result: 在多种数据集和部署场景中，新模块性能显著提升，PSNR最高增加0.83dB或参数减少71.3%。

Conclusion: UAE和优化模块有效提升模型性能，验证了通用性与泛化能力的关联。

Abstract: Deep learning has substantially advanced the Single Image Super-Resolution
(SISR). However, existing researches have predominantly focused on raw
performance gains, with little attention paid to quantifying the
transferability of architectural components. In this paper, we introduce the
concept of "Universality" and its associated definitions which extend the
traditional notion of "Generalization" to encompass the modules' ease of
transferability, thus revealing the relationships between module universality
and model generalizability. Then we propose the Universality Assessment
Equation (UAE), a metric for quantifying how readily a given module could be
transplanted across models. Guided by the UAE results of standard residual
blocks and other plug-and-play modules, we further design two optimized
modules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).
Through comprehensive experiments on natural-scene benchmarks, remote-sensing
datasets, extreme-industrial imagery and on-device deployments, we demonstrate
that networks embedded with the proposed plug-and-play modules outperform
several state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or
enabling a 71.3% reduction in parameters with negligible loss in reconstruction
fidelity.

</details>


### [96] [Coop-WD: Cooperative Perception with Weighting and Denoising for Robust V2V Communication](https://arxiv.org/abs/2505.03528)
*Chenguang Liu,Jianjun Chen,Yunfei Chen,Yubei He,Zhuangkun Wei,Hongjian Sun,Haiyan Lu,Qi Hao*

Main category: cs.CV

TL;DR: 提出了一种联合加权和去噪框架Coop-WD，用于增强V2V通信受损下的协同感知，并提出了高效变体Coop-WD-eco以减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对不同级别V2V通信损伤的泛化能力，需要一种更鲁棒的方法来提升协同感知精度。

Method: 采用自监督对比模型和条件扩散概率模型进行车辆级和像素级特征增强，并设计高效变体Coop-WD-eco选择性去噪。

Result: Coop-WD在所有信道类型中优于传统基准，Coop-WD-eco在严重失真下计算成本降低50%且精度相当。

Conclusion: Coop-WD和Coop-WD-eco有效提升了V2V通信受损下的协同感知性能，同时兼顾计算效率。

Abstract: Cooperative perception, leveraging shared information from multiple vehicles
via vehicle-to-vehicle (V2V) communication, plays a vital role in autonomous
driving to alleviate the limitation of single-vehicle perception. Existing
works have explored the effects of V2V communication impairments on perception
precision, but they lack generalization to different levels of impairments. In
this work, we propose a joint weighting and denoising framework, Coop-WD, to
enhance cooperative perception subject to V2V channel impairments. In this
framework, the self-supervised contrastive model and the conditional diffusion
probabilistic model are adopted hierarchically for vehicle-level and
pixel-level feature enhancement. An efficient variant model, Coop-WD-eco, is
proposed to selectively deactivate denoising to reduce processing overhead.
Rician fading, non-stationarity, and time-varying distortion are considered.
Simulation results demonstrate that the proposed Coop-WD outperforms
conventional benchmarks in all types of channels. Qualitative analysis with
visual examples further proves the superiority of our proposed method. The
proposed Coop-WD-eco achieves up to 50% reduction in computational cost under
severe distortion while maintaining comparable accuracy as channel conditions
improve.

</details>


### [97] [RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth Segmentation in CBCT](https://arxiv.org/abs/2505.03538)
*Chuyu Zhao,Hao Huang,Jiashuo Guo,Ziyu Shen,Zhongwei Zhou,Jie Liu,Zekuan Yu*

Main category: cs.CV

TL;DR: RAIL是一种双组双学生的半监督框架，通过区域感知教学机制解决CBCT牙齿分割中监督不足和伪标签不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 解决半监督学习在CBCT牙齿分割中监督不足和伪标签不可靠的挑战。

Method: 提出RAIL框架，包含DFS控制器和CAL调制器，通过双组交替训练实现知识转移和区域感知教学。

Result: 在四个CBCT数据集上表现优于现有方法。

Conclusion: RAIL在有限标注下显著提升分割性能，代码开源。

Abstract: Semi-supervised learning has become a compelling approach for 3D tooth
segmentation from CBCT scans, where labeled data is minimal. However, existing
methods still face two persistent challenges: limited corrective supervision in
structurally ambiguous or mislabeled regions during supervised training and
performance degradation caused by unreliable pseudo-labels on unlabeled data.
To address these problems, we propose Region-Aware Instructive Learning (RAIL),
a dual-group dual-student, semi-supervised framework. Each group contains two
student models guided by a shared teacher network. By alternating training
between the two groups, RAIL promotes intergroup knowledge transfer and
collaborative region-aware instruction while reducing overfitting to the
characteristics of any single model. Specifically, RAIL introduces two
instructive mechanisms. Disagreement-Focused Supervision (DFS) Controller
improves supervised learning by instructing predictions only within areas where
student outputs diverge from both ground truth and the best student, thereby
concentrating supervision on structurally ambiguous or mislabeled areas. In the
unsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces
agreement in regions with high model certainty while reducing the effect of
low-confidence predictions during training. This helps prevent our model from
learning unstable patterns and improves the overall reliability of
pseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets
show that RAIL surpasses state-of-the-art methods under limited annotation. Our
code will be available at https://github.com/Tournesol-Saturday/RAIL.

</details>


### [98] [Panoramic Out-of-Distribution Segmentation](https://arxiv.org/abs/2505.03539)
*Mengfei Duan,Kailun Yang,Yuheng Zhang,Yihong Cao,Fei Teng,Kai Luo,Jiaming Zhang,Zhiyong Li,Shutao Li*

Main category: cs.CV

TL;DR: 论文提出全景异常分割任务（PanOoS）及首个解决方案POS，通过文本引导的提示分布学习适应全景图像特性，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有全景语义分割方法无法识别异常，且传统OoS模型在全景域表现不佳，需解决背景杂乱和像素失真问题。

Method: 提出POS方法，集成解耦策略、提示修复注意力（PRA）和双层提示分布学习（BPDL），优化语义解码和掩码嵌入。

Result: POS在DenseOoS上AuPRC提升34.25%，FPR95降低21.42%，优于现有方法，并具备领先的闭集分割能力。

Conclusion: POS为全景异常分割提供高效解决方案，并发布两个新基准数据集推动领域发展。

Abstract: Panoramic imaging enables capturing 360{\deg} images with an ultra-wide
Field-of-View (FoV) for dense omnidirectional perception. However, current
panoramic semantic segmentation methods fail to identify outliers, and pinhole
Out-of-distribution Segmentation (OoS) models perform unsatisfactorily in the
panoramic domain due to background clutter and pixel distortions. To address
these issues, we introduce a new task, Panoramic Out-of-distribution
Segmentation (PanOoS), achieving OoS for panoramas. Furthermore, we propose the
first solution, POS, which adapts to the characteristics of panoramic images
through text-guided prompt distribution learning. Specifically, POS integrates
a disentanglement strategy designed to materialize the cross-domain
generalization capability of CLIP. The proposed Prompt-based Restoration
Attention (PRA) optimizes semantic decoding by prompt guidance and
self-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL)
refines the manifold of per-pixel mask embeddings via semantic prototype
supervision. Besides, to compensate for the scarcity of PanOoS datasets, we
establish two benchmarks: DenseOoS, which features diverse outliers in complex
environments, and QuadOoS, captured by a quadruped robot with a panoramic
annular lens system. Extensive experiments demonstrate superior performance of
POS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS,
outperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves
leading closed-set segmentation capabilities. Code and datasets will be
available at https://github.com/MengfeiD/PanOoS.

</details>


### [99] [Read My Ears! Horse Ear Movement Detection for Equine Affective State Assessment](https://arxiv.org/abs/2505.03554)
*João Alves,Pia Haubro Andersen,Rikke Gade*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度学习和光流的方法，用于自动检测和定位马耳动作单元（AU），以解决手动标注数据稀缺的问题，并在公开数据集上达到了87.5%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 由于手动标注马面部动作单元（AU）耗时且昂贵，现有数据稀缺，限制了马情感状态评估的发展。因此，需要自动化标注系统来提升数据利用和检测工具。

Method: 结合深度学习视频特征提取和循环神经网络进行分类任务，同时采用经典光流方法，专注于马耳AU的检测和定位。

Result: 在公开马视频数据集上，耳部动作分类准确率达到87.5%，验证了方法的潜力。

Conclusion: 该方法为自动化AU检测提供了可行方案，未来可进一步应用于马福利和兽医诊断实践。代码已公开。

Abstract: The Equine Facial Action Coding System (EquiFACS) enables the systematic
annotation of facial movements through distinct Action Units (AUs). It serves
as a crucial tool for assessing affective states in horses by identifying
subtle facial expressions associated with discomfort. However, the field of
horse affective state assessment is constrained by the scarcity of annotated
data, as manually labelling facial AUs is both time-consuming and costly. To
address this challenge, automated annotation systems are essential for
leveraging existing datasets and improving affective states detection tools. In
this work, we study different methods for specific ear AU detection and
localization from horse videos. We leverage past works on deep learning-based
video feature extraction combined with recurrent neural networks for the video
classification task, as well as a classic optical flow based approach. We
achieve 87.5% classification accuracy of ear movement presence on a public
horse video dataset, demonstrating the potential of our approach. We discuss
future directions to develop these systems, with the aim of bridging the gap
between automated AU detection and practical applications in equine welfare and
veterinary diagnostics. Our code will be made publicly available at
https://github.com/jmalves5/read-my-ears.

</details>


### [100] [Generating Synthetic Data via Augmentations for Improved Facial Resemblance in DreamBooth and InstantID](https://arxiv.org/abs/2505.03557)
*Koray Ulusan,Benjamin Kiefer*

Main category: cs.CV

TL;DR: 研究探讨了通过DreamBooth和InstantID技术增强Stable Diffusion生成肖像的面部相似性，并提出了FaceDistance评估方法。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过增强技术提高业余照片生成专业肖像的面部相似性。

Method: 使用DreamBooth和InstantID技术，结合多种增强策略，并通过FaceDistance评估生成结果的面部相似性。

Result: 实验表明增强策略能显著提升生成肖像的面部相似性。

Conclusion: 研究为Stable Diffusion肖像生成中增强策略的有效应用提供了指导。

Abstract: The personalization of Stable Diffusion for generating professional portraits
from amateur photographs is a burgeoning area, with applications in various
downstream contexts. This paper investigates the impact of augmentations on
improving facial resemblance when using two prominent personalization
techniques: DreamBooth and InstantID. Through a series of experiments with
diverse subject datasets, we assessed the effectiveness of various augmentation
strategies on the generated headshots' fidelity to the original subject. We
introduce FaceDistance, a wrapper around FaceNet, to rank the generations based
on facial similarity, which aided in our assessment. Ultimately, this research
provides insights into the role of augmentations in enhancing facial
resemblance in SDXL-generated portraits, informing strategies for their
effective deployment in downstream applications.

</details>


### [101] [Real-Time Person Image Synthesis Using a Flow Matching Model](https://arxiv.org/abs/2505.03562)
*Jiwoo Jeong,Kirok Kim,Wooju Kim,Nam-Joon Kim*

Main category: cs.CV

TL;DR: PGPIS任务通过目标姿态和源图像生成逼真人物图像，但实时性不足。本文提出基于流匹配的模型RPFM，在速度和性能间取得平衡，实现近实时生成。


<details>
  <summary>Details</summary>
Motivation: PGPIS在实时应用中需求迫切，但现有扩散模型速度慢。需开发快速可靠的模型以支持实时交互系统。

Method: 提出基于流匹配（FM）的生成模型RPFM，支持条件生成和潜在空间操作，提升训练和采样效率。

Result: RPFM在DeepFashion数据集上实现近实时采样速度，性能接近SOTA，生成速度提升两倍以上。

Conclusion: RPFM通过牺牲少量精度换取速度，为实时PGPIS应用提供了可行解决方案。

Abstract: Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images
conditioned on a target pose and a source image. This task plays a key role in
various real-world applications, such as sign language video generation, AR/VR,
gaming, and live streaming. In these scenarios, real-time PGPIS is critical for
providing immediate visual feedback and maintaining user immersion.However,
achieving real-time performance remains a significant challenge due to the
complexity of synthesizing high-fidelity images from diverse and dynamic human
poses. Recent diffusion-based methods have shown impressive image quality in
PGPIS, but their slow sampling speeds hinder deployment in time-sensitive
applications. This latency is particularly problematic in tasks like generating
sign language videos during live broadcasts, where rapid image updates are
required. Therefore, developing a fast and reliable PGPIS model is a crucial
step toward enabling real-time interactive systems. To address this challenge,
we propose a generative model based on flow matching (FM). Our approach enables
faster, more stable, and more efficient training and sampling. Furthermore, the
proposed model supports conditional generation and can operate in latent space,
making it especially suitable for real-time PGPIS applications where both speed
and quality are critical. We evaluate our proposed method, Real-Time Person
Image Synthesis Using a Flow Matching Model (RPFM), on the widely used
DeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves
near-real-time sampling speeds while maintaining performance comparable to the
state-of-the-art models. Our methodology trades off a slight, acceptable
decrease in generated-image accuracy for over a twofold increase in generation
speed, thereby ensuring real-time performance.

</details>


### [102] [Uncertainty-Aware Prototype Semantic Decoupling for Text-Based Person Search in Full Images](https://arxiv.org/abs/2505.03567)
*Zengli Luo,Canlong Zhang,Xiaochun Lu,Zhixin Li,Zhiwen Wang*

Main category: cs.CV

TL;DR: UPD-TBPS框架通过多粒度不确定性估计、原型不确定性解耦和跨模态重识别，提升复杂场景中基于文本的行人搜索性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂场景中因检测和匹配的不确定性导致性能下降，需解决这一问题。

Method: 提出UPD-TBPS框架，包含多粒度不确定性估计（MUE）、原型不确定性解耦（PUD）和跨模态重识别（ReID）三个模块。

Result: 在CUHK-SYSU-TBPS和PRW-TBPS数据集上验证了框架的有效性。

Conclusion: UPD-TBPS通过减少不确定性提升了行人搜索的准确性和鲁棒性。

Abstract: Text-based pedestrian search (TBPS) in full images aims to locate a target
pedestrian in untrimmed images using natural language descriptions. However, in
complex scenes with multiple pedestrians, existing methods are limited by
uncertainties in detection and matching, leading to degraded performance. To
address this, we propose UPD-TBPS, a novel framework comprising three modules:
Multi-granularity Uncertainty Estimation (MUE), Prototype-based Uncertainty
Decoupling (PUD), and Cross-modal Re-identification (ReID). MUE conducts
multi-granularity queries to identify potential targets and assigns confidence
scores to reduce early-stage uncertainty. PUD leverages visual context
decoupling and prototype mining to extract features of the target pedestrian
described in the query. It separates and learns pedestrian prototype
representations at both the coarse-grained cluster level and the fine-grained
individual level, thereby reducing matching uncertainty. ReID evaluates
candidates with varying confidence levels, improving detection and retrieval
accuracy. Experiments on CUHK-SYSU-TBPS and PRW-TBPS datasets validate the
effectiveness of our framework.

</details>


### [103] [Corner Cases: How Size and Position of Objects Challenge ImageNet-Trained Models](https://arxiv.org/abs/2505.03569)
*Mishal Fatima,Steffen Jung,Margret Keuper*

Main category: cs.CV

TL;DR: 论文研究了图像背景对模型预测中虚假特征依赖的影响，提出了一个合成数据集Hard-Spurious-ImageNet，并发现现有方法未能有效解决因物体大小和位置变化导致的性能问题。


<details>
  <summary>Details</summary>
Motivation: 图像背景可能导致虚假相关性，影响模型预测。人类审美偏好使数据集存在位置和大小偏差，进而影响模型对背景虚假特征的依赖。

Method: 提出合成数据集Hard-Spurious-ImageNet，基于ImageNet1k，包含多样背景、物体位置和大小。评估不同预训练模型在该数据集上的表现。

Result: 大多数模型在物体区域占比小且偏离图像中心时，严重依赖背景虚假特征。现有方法未能显著提升最差组准确率。

Conclusion: 物体大小和位置变化未被现有方法充分考虑，需进一步研究以解决背景虚假特征问题。

Abstract: Backgrounds in images play a major role in contributing to spurious
correlations among different data points. Owing to aesthetic preferences of
humans capturing the images, datasets can exhibit positional (location of the
object within a given frame) and size (region-of-interest to image ratio)
biases for different classes. In this paper, we show that these biases can
impact how much a model relies on spurious features in the background to make
its predictions. To better illustrate our findings, we propose a synthetic
dataset derived from ImageNet1k, Hard-Spurious-ImageNet, which contains images
with various backgrounds, object positions, and object sizes. By evaluating the
dataset on different pretrained models, we find that most models rely heavily
on spurious features in the background when the region-of-interest (ROI) to
image ratio is small and the object is far from the center of the image.
Moreover, we also show that current methods that aim to mitigate harmful
spurious features, do not take into account these factors, hence fail to
achieve considerable performance gains for worst-group accuracies when the size
and location of core features in an image change.

</details>


### [104] [Supervised and Unsupervised Textile Classification via Near-Infrared Hyperspectral Imaging and Deep Learning](https://arxiv.org/abs/2505.03575)
*Maria Kainz,Johannes K. Krondorfer,Malte Jaschik,Maria Jernej,Harald Ganster*

Main category: cs.CV

TL;DR: 利用高光谱近红外成像和深度学习算法，研究纺织纤维分类与分选，优化CNN和自编码器模型，展示其在不同纺织结构中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 纺织纤维回收对减少纺织业环境影响至关重要，高光谱成像与深度学习为高效分类提供解决方案。

Method: 研究监督与非监督深度学习模型，测试其在多种纺织结构中的泛化能力，优化CNN和自编码器网络。

Result: 优化的CNN和自编码器模型在不同条件下表现稳健，泛化能力强。

Conclusion: 高光谱成像与深度学习结合，有望通过准确分类推动可持续纺织回收。

Abstract: Recycling textile fibers is critical to reducing the environmental impact of
the textile industry. Hyperspectral near-infrared (NIR) imaging combined with
advanced deep learning algorithms offers a promising solution for efficient
fiber classification and sorting. In this study, we investigate supervised and
unsupervised deep learning models and test their generalization capabilities on
different textile structures. We show that optimized convolutional neural
networks (CNNs) and autoencoder networks achieve robust generalization under
varying conditions. These results highlight the potential of hyperspectral
imaging and deep learning to advance sustainable textile recycling through
accurate and robust classification.

</details>


### [105] [DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer Questions in Dynamic Scenes](https://arxiv.org/abs/2505.03581)
*Sergey Linok,Vadim Semenov,Anastasia Trunova,Oleg Bulichev,Dmitry Yudin*

Main category: cs.CV

TL;DR: 论文提出了一种名为DyGEnc的新方法，用于编码动态图，结合了空间-时间结构表示和大型语言模型，显著提升了动态环境中事件分析的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型在动态环境中分析事件时缺乏可解释的空间-时间对象表示，DyGEnc旨在解决这一问题。

Method: DyGEnc通过压缩的空间-时间结构表示与大型语言模型结合，基于文本场景图进行高级问答。

Result: 在STAR和AGQA数据集上，DyGEnc比现有视觉方法性能提升15-25%，并能扩展到处理原始图像。

Conclusion: DyGEnc为基于图的机器人记忆提供了稳健且高效的解决方案，支持长期推理。

Abstract: The analysis of events in dynamic environments poses a fundamental challenge
in the development of intelligent agents and robots capable of interacting with
humans. Current approaches predominantly utilize visual models. However, these
methods often capture information implicitly from images, lacking interpretable
spatial-temporal object representations. To address this issue we introduce
DyGEnc - a novel method for Encoding a Dynamic Graph. This method integrates
compressed spatial-temporal structural observation representation with the
cognitive capabilities of large language models. The purpose of this
integration is to enable advanced question answering based on a sequence of
textual scene graphs. Extended evaluations on the STAR and AGQA datasets
indicate that DyGEnc outperforms existing visual methods by a large margin of
15-25% in addressing queries regarding the history of human-to-object
interactions. Furthermore, the proposed method can be seamlessly extended to
process raw input images utilizing foundational models for extracting explicit
textual scene graphs, as substantiated by the results of a robotic experiment
conducted with a wheeled manipulator platform. We hope that these findings will
contribute to the implementation of robust and compressed graph-based robotic
memory for long-horizon reasoning. Code is available at
github.com/linukc/DyGEnc.

</details>


### [106] [Fixed-Length Dense Fingerprint Representation](https://arxiv.org/abs/2505.03597)
*Zhiyu Pan,Xiongjun Guan,Yongjie Duan,Jianjiang Feng,Jie Zhou*

Main category: cs.CV

TL;DR: 提出了一种固定长度的指纹密集描述符和匹配框架FLARE，结合姿态对齐和鲁棒增强，显著提升了跨模态和低质量指纹的匹配性能。


<details>
  <summary>Details</summary>
Motivation: 固定长度指纹表示在大规模匹配中高效，但现有方法难以处理多样指纹模态、姿态变化和噪声干扰。

Method: FLARE框架采用三维密集描述符捕捉指纹脊结构空间关系，结合姿态对齐和双增强策略。

Result: 实验表明FLARE在多种指纹类型和低质量场景下性能优越，显著优于现有方法。

Conclusion: FLARE是一种统一且可扩展的鲁棒指纹表示和匹配解决方案，代码已公开。

Abstract: Fixed-length fingerprint representations, which map each fingerprint to a
compact and fixed-size feature vector, are computationally efficient and
well-suited for large-scale matching. However, designing a robust
representation that effectively handles diverse fingerprint modalities, pose
variations, and noise interference remains a significant challenge. In this
work, we propose a fixed-length dense descriptor of fingerprints, and introduce
FLARE-a fingerprint matching framework that integrates the Fixed-Length dense
descriptor with pose-based Alignment and Robust Enhancement. This fixed-length
representation employs a three-dimensional dense descriptor to effectively
capture spatial relationships among fingerprint ridge structures, enabling
robust and locally discriminative representations. To ensure consistency within
this dense feature space, FLARE incorporates pose-based alignment using
complementary estimation methods, along with dual enhancement strategies that
refine ridge clarity while preserving the original fingerprint modality. The
proposed dense descriptor supports fixed-length representation while
maintaining spatial correspondence, enabling fast and accurate similarity
computation. Extensive experiments demonstrate that FLARE achieves superior
performance across rolled, plain, latent, and contactless fingerprints,
significantly outperforming existing methods in cross-modality and low-quality
scenarios. Further analysis validates the effectiveness of the dense descriptor
design, as well as the impact of alignment and enhancement modules on the
accuracy of dense descriptor matching. Experimental results highlight the
effectiveness and generalizability of FLARE as a unified and scalable solution
for robust fingerprint representation and matching. The implementation and code
will be publicly available at https://github.com/Yu-Yy/FLARE.

</details>


### [107] [From Pixels to Polygons: A Survey of Deep Learning Approaches for Medical Image-to-Mesh Reconstruction](https://arxiv.org/abs/2505.03599)
*Fengming Lin,Arezoo Zakeri,Yidan Xue,Michael MacRaild,Haoran Dou,Zherui Zhou,Ziwei Zou,Ali Sarrami-Foroushani,Jinming Duan,Alejandro F. Frangi*

Main category: cs.CV

TL;DR: 该论文综述了基于深度学习的医学图像到网格重建方法，将其分为四类（模板模型、统计模型、生成模型和隐式模型），分析了每类的方法、优缺点及适用性，并评估了不同解剖应用中的表现。


<details>
  <summary>Details</summary>
Motivation: 推动医学图像分析领域的发展，为计算医学和虚拟试验提供三维网格模型，以更好地理解疾病机制和优化诊疗技术。

Method: 系统分类现有方法，详细分析每类的方法论基础、优缺点及适用性，并通过标准指标进行定量比较。

Result: 总结了四类方法的性能表现，指出了拓扑正确性、几何精度和多模态集成等当前挑战。

Conclusion: 该综述为医学图像分析和计算医学领域的研究者提供了全面参考，并提出了未来研究方向。

Abstract: Deep learning-based medical image-to-mesh reconstruction has rapidly evolved,
enabling the transformation of medical imaging data into three-dimensional mesh
models that are critical in computational medicine and in silico trials for
advancing our understanding of disease mechanisms, and diagnostic and
therapeutic techniques in modern medicine. This survey systematically
categorizes existing approaches into four main categories: template models,
statistical models, generative models, and implicit models. Each category is
analysed in detail, examining their methodological foundations, strengths,
limitations, and applicability to different anatomical structures and imaging
modalities. We provide an extensive evaluation of these methods across various
anatomical applications, from cardiac imaging to neurological studies,
supported by quantitative comparisons using standard metrics. Additionally, we
compile and analyze major public datasets available for medical mesh
reconstruction tasks and discuss commonly used evaluation metrics and loss
functions. The survey identifies current challenges in the field, including
requirements for topological correctness, geometric accuracy, and
multi-modality integration. Finally, we present promising future research
directions in this domain. This systematic review aims to serve as a
comprehensive reference for researchers and practitioners in medical image
analysis and computational medicine.

</details>


### [108] [PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model](https://arxiv.org/abs/2505.03603)
*Y. B. Wang,S. Z. Zhou,J. F. Wu,T. Hu,J. N. Zhang,Y. Liu*

Main category: cs.CV

TL;DR: PAHA提出了一种基于扩散模型的端到端音频驱动上半身人体动画框架，通过PAR和PCE方法提升生成质量与音频-动作一致性，并设计了SG和DG推理指导方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多阶段生成和中间表示，导致推理时间长且生成质量与音频-动作一致性不足，缺乏局部细粒度监督指导。

Method: 引入PAR动态调整区域训练损失权重，PCE训练扩散模型区域音频-视觉分类器，并设计SG和DG推理指导方法。

Result: PAHA在音频-动作对齐和视频相关评估中显著优于现有方法。

Conclusion: PAHA通过局部细粒度监督和高效推理指导，解决了现有方法的不足，并发布了首个中文新闻主播语音数据集CNAS。

Abstract: Audio-driven human animation technology is widely used in human-computer
interaction, and the emergence of diffusion models has further advanced its
development. Currently, most methods rely on multi-stage generation and
intermediate representations, resulting in long inference time and issues with
generation quality in specific foreground regions and audio-motion consistency.
These shortcomings are primarily due to the lack of localized fine-grained
supervised guidance. To address above challenges, we propose PAHA, an
end-to-end audio-driven upper-body human animation framework with diffusion
model. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts
Consistency Enhancement (PCE). PAR dynamically adjusts regional training loss
weights based on pose confidence scores, effectively improving visual quality.
PCE constructs and trains diffusion-based regional audio-visual classifiers to
improve the consistency of motion and co-speech audio. Afterwards, we design
two novel inference guidance methods for the foregoing classifiers, Sequential
Guidance (SG) and Differential Guidance (DG), to balance efficiency and quality
respectively. Additionally, we build CNAS, the first public Chinese News Anchor
Speech dataset, to advance research and validation in this field. Extensive
experimental results and user studies demonstrate that PAHA significantly
outperforms existing methods in audio-motion alignment and video-related
evaluations. The codes and CNAS dataset will be released upon acceptance.

</details>


### [109] [Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack Detection](https://arxiv.org/abs/2505.03610)
*Fangling Jiang,Qi Li,Bing Liu,Weining Wang,Caifeng Shan,Zhenan Sun,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出了一种基于知识图谱的提示学习框架，用于3D面具攻击检测，结合视觉语言模型和因果图理论，显著提升了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多模态传感器或rPPG信号，成本高且泛化能力有限；文本描述成本低且通用性强，但尚未被充分探索。

Method: 结合知识图谱实体和三元组生成任务特定的提示，引入视觉知识过滤器优化相关元素，并利用因果图理论消除虚假相关性。

Result: 在基准数据集上实现了最优的跨场景检测性能。

Conclusion: 该方法通过知识驱动的提示学习和因果分析，显著提升了3D面具攻击检测的泛化能力和性能。

Abstract: 3D mask presentation attack detection is crucial for protecting face
recognition systems against the rising threat of 3D mask attacks. While most
existing methods utilize multimodal features or remote photoplethysmography
(rPPG) signals to distinguish between real faces and 3D masks, they face
significant challenges, such as the high costs associated with multimodal
sensors and limited generalization ability. Detection-related text descriptions
offer concise, universal information and are cost-effective to obtain. However,
the potential of vision-language multimodal features for 3D mask presentation
attack detection remains unexplored. In this paper, we propose a novel
knowledge-based prompt learning framework to explore the strong generalization
capability of vision-language models for 3D mask presentation attack detection.
Specifically, our approach incorporates entities and triples from knowledge
graphs into the prompt learning process, generating fine-grained, task-specific
explicit prompts that effectively harness the knowledge embedded in pre-trained
vision-language models. Furthermore, considering different input images may
emphasize distinct knowledge graph elements, we introduce a visual-specific
knowledge filter based on an attention mechanism to refine relevant elements
according to the visual context. Additionally, we leverage causal graph theory
insights into the prompt learning process to further enhance the generalization
ability of our method. During training, a spurious correlation elimination
paradigm is employed, which removes category-irrelevant local image patches
using guidance from knowledge-based text features, fostering the learning of
generalized causal prompts that align with category-relevant local patches.
Experimental results demonstrate that the proposed method achieves
state-of-the-art intra- and cross-scenario detection performance on benchmark
datasets.

</details>


### [110] [Learning Unknown Spoof Prompts for Generalized Face Anti-Spoofing Using Only Real Face Images](https://arxiv.org/abs/2505.03611)
*Fangling Jiang,Qi Li,Weining Wang,Wei Shen,Bing Liu,Zhenan Sun*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉语言模型的新方法，通过生成真实人脸和潜在未知攻击的文本提示，提升人脸反欺骗模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 人脸反欺骗技术的泛化能力受限，主要源于协变量偏移和语义偏移。

Method: 利用视觉语言模型生成文本提示，并通过优化框架学习多样化的欺骗提示，约束其与真实人脸的距离并保持语义独立性。

Result: 在九个数据集上的实验表明，该方法无需使用欺骗人脸图像即可实现优异的泛化性能。

Conclusion: 该方法通过文本提示学习，显著提升了人脸反欺骗模型对未知攻击类型的泛化能力。

Abstract: Face anti-spoofing is a critical technology for ensuring the security of face
recognition systems. However, its ability to generalize across diverse
scenarios remains a significant challenge. In this paper, we attribute the
limited generalization ability to two key factors: covariate shift, which
arises from external data collection variations, and semantic shift, which
results from substantial differences in emerging attack types. To address both
challenges, we propose a novel approach for learning unknown spoof prompts,
relying solely on real face images from a single source domain. Our method
generates textual prompts for real faces and potential unknown spoof attacks by
leveraging the general knowledge embedded in vision-language models, thereby
enhancing the model's ability to generalize to unseen target domains.
Specifically, we introduce a diverse spoof prompt optimization framework to
learn effective prompts. This framework constrains unknown spoof prompts within
a relaxed prior knowledge space while maximizing their distance from real face
images. Moreover, it enforces semantic independence among different spoof
prompts to capture a broad range of spoof patterns. Experimental results on
nine datasets demonstrate that the learned prompts effectively transfer the
knowledge of vision-language models, enabling state-of-the-art generalization
ability against diverse unknown attack types across unseen target domains
without using any spoof face images.

</details>


### [111] [PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing](https://arxiv.org/abs/2505.03621)
*Yiping Xie,Bo Zhao,Mingtong Dai,Jian-Ping Zhou,Yue Sun,Tao Tan,Weicheng Xie,Linlin Shen,Zitong Yu*

Main category: cs.CV

TL;DR: PhysLLM框架结合LLMs与rPPG组件，通过跨模态对齐和双域稳定算法提升非接触生理测量的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: rPPG在非接触生理测量中易受光照变化和运动伪影影响，LLMs虽擅长长程依赖但难以处理连续噪声敏感信号。

Method: 提出PhysLLM框架，包括文本原型引导（TPG）策略和双域稳定（DDS）算法，结合生理先验和跨模态学习。

Result: 在四个基准数据集上实现最优准确性和鲁棒性，适应光照变化和运动场景。

Conclusion: PhysLLM通过跨模态协作优化，显著提升了rPPG的性能和泛化能力。

Abstract: Remote photoplethysmography (rPPG) enables non-contact physiological
measurement but remains highly susceptible to illumination changes, motion
artifacts, and limited temporal modeling. Large Language Models (LLMs) excel at
capturing long-range dependencies, offering a potential solution but struggle
with the continuous, noise-sensitive nature of rPPG signals due to their
text-centric design. To bridge this gap, we introduce PhysLLM, a collaborative
optimization framework that synergizes LLMs with domain-specific rPPG
components. Specifically, the Text Prototype Guidance (TPG) strategy is
proposed to establish cross-modal alignment by projecting hemodynamic features
into LLM-interpretable semantic space, effectively bridging the
representational gap between physiological signals and linguistic tokens.
Besides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for
resolving signal instability through adaptive time-frequency domain feature
re-weighting. Finally, rPPG task-specific cues systematically inject
physiological priors through physiological statistics, environmental contextual
answering, and task description, leveraging cross-modal learning to integrate
both visual and textual information, enabling dynamic adaptation to challenging
scenarios like variable illumination and subject movements. Evaluation on four
benchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness,
demonstrating superior generalization across lighting variations and motion
scenarios.

</details>


### [112] [Bounding Box-Guided Diffusion for Synthesizing Industrial Images and Segmentation Map](https://arxiv.org/abs/2505.03623)
*Alessandro Simoni,Francesco Pelosin*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的新方法，用于生成高保真工业缺陷数据集，减少标注成本，提升分割精度。


<details>
  <summary>Details</summary>
Motivation: 工业缺陷分割需要高精度标注数据，但获取成本高且耗时。

Method: 利用扩散模型结合边界框表示生成精确分割掩码，提升缺陷一致性和空间准确性。

Result: 实验表明，该方法能有效缩小合成数据与真实工业数据之间的差距，提升下游分割任务性能。

Conclusion: 扩散模型为工业数据合成提供了一种可靠且高效的方法，代码已开源。

Abstract: Synthetic dataset generation in Computer Vision, particularly for industrial
applications, is still underexplored. Industrial defect segmentation, for
instance, requires highly accurate labels, yet acquiring such data is costly
and time-consuming. To address this challenge, we propose a novel
diffusion-based pipeline for generating high-fidelity industrial datasets with
minimal supervision. Our approach conditions the diffusion model on enriched
bounding box representations to produce precise segmentation masks, ensuring
realistic and accurately localized defect synthesis. Compared to existing
layout-conditioned generative methods, our approach improves defect consistency
and spatial accuracy. We introduce two quantitative metrics to evaluate the
effectiveness of our method and assess its impact on a downstream segmentation
task trained on real and synthetic data. Our results demonstrate that
diffusion-based synthesis can bridge the gap between artificial and real-world
industrial data, fostering more reliable and cost-efficient segmentation
models. The code is publicly available at
https://github.com/covisionlab/diffusion_labeling.

</details>


### [113] [Breaking Annotation Barriers: Generalized Video Quality Assessment via Ranking-based Self-Supervision](https://arxiv.org/abs/2505.03631)
*Linhan Cao,Wei Sun,Kaiwei Zhang,Yicong Peng,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 该论文提出了一种自监督学习框架，用于视频质量评估（VQA），通过利用大规模未标记网络视频和迭代自改进训练策略，显著提升了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统VQA模型依赖人工标注数据集，成本高且难以扩展，限制了模型的泛化能力。本文旨在通过自监督学习解决这一问题。

Method: 采用学习排序范式，结合质量伪标注和合成失真模拟，训练多模态模型，并引入迭代自改进策略优化训练数据。

Result: 模型在零样本和微调场景下均表现优异，泛化能力显著提升，并在多个基准测试中达到新SOTA。

Conclusion: 自监督学习框架有效提升了VQA模型的泛化能力，为未来研究提供了新方向。

Abstract: Video quality assessment (VQA) is essential for quantifying perceptual
quality in various video processing workflows, spanning from camera capture
systems to over-the-top streaming platforms. While recent supervised VQA models
have made substantial progress, the reliance on manually annotated datasets --
a process that is labor-intensive, costly, and difficult to scale up -- has
hindered further optimization of their generalization to unseen video content
and distortions. To bridge this gap, we introduce a self-supervised learning
framework for VQA to learn quality assessment capabilities from large-scale,
unlabeled web videos. Our approach leverages a \textbf{learning-to-rank}
paradigm to train a large multimodal model (LMM) on video pairs automatically
labeled via two manners, including quality pseudo-labeling by existing VQA
models and relative quality ranking based on synthetic distortion simulations.
Furthermore, we introduce a novel \textbf{iterative self-improvement training
strategy}, where the trained model acts an improved annotator to iteratively
refine the annotation quality of training data. By training on a dataset
$10\times$ larger than the existing VQA benchmarks, our model: (1) achieves
zero-shot performance on in-domain VQA benchmarks that matches or surpasses
supervised models; (2) demonstrates superior out-of-distribution (OOD)
generalization across diverse video content and distortions; and (3) sets a new
state-of-the-art when fine-tuned on human-labeled datasets. Extensive
experimental results validate the effectiveness of our self-supervised approach
in training generalized VQA models. The datasets and code will be publicly
released to facilitate future research.

</details>


### [114] [Towards Smart Point-and-Shoot Photography](https://arxiv.org/abs/2505.03638)
*Jiawan Li,Fei Zhou,Zhipeng Zhong,Jiongzhi Lin,Guoping Qiu*

Main category: cs.CV

TL;DR: 论文提出了一种智能点拍（SPAS）系统，通过实时调整相机姿态帮助用户拍摄更好的照片。系统包括构建大型数据集、开发基于CLIP的构图质量评估模型（CCQA）和相机姿态调整模型（CPAM）。


<details>
  <summary>Details</summary>
Motivation: 传统点拍相机无法指导用户构图，而智能手机用户普遍缺乏摄影技巧，因此需要一种智能系统来帮助用户拍摄更好的照片。

Method: 1. 构建包含32万张图像的数据集；2. 开发CCQA模型，通过可学习文本嵌入技术评估图像质量；3. 开发CPAM模型，通过混合专家模型和门控损失函数输出相机姿态调整建议。

Result: 系统能够有效评估图像构图质量并实时提供相机姿态调整建议。

Conclusion: SPAS系统通过智能化的构图指导和姿态调整，显著提升了用户拍摄照片的质量。

Abstract: Hundreds of millions of people routinely take photos using their smartphones
as point and shoot (PAS) cameras, yet very few would have the photography
skills to compose a good shot of a scene. While traditional PAS cameras have
built-in functions to ensure a photo is well focused and has the right
brightness, they cannot tell the users how to compose the best shot of a scene.
In this paper, we present a first of its kind smart point and shoot (SPAS)
system to help users to take good photos. Our SPAS proposes to help users to
compose a good shot of a scene by automatically guiding the users to adjust the
camera pose live on the scene. We first constructed a large dataset containing
320K images with camera pose information from 4000 scenes. We then developed an
innovative CLIP-based Composition Quality Assessment (CCQA) model to assign
pseudo labels to these images. The CCQA introduces a unique learnable text
embedding technique to learn continuous word embeddings capable of discerning
subtle visual quality differences in the range covered by five levels of
quality description words {bad, poor, fair, good, perfect}. And finally we have
developed a camera pose adjustment model (CPAM) which first determines if the
current view can be further improved and if so it outputs the adjust suggestion
in the form of two camera pose adjustment angles. The two tasks of CPAM make
decisions in a sequential manner and each involves different sets of training
samples, we have developed a mixture-of-experts model with a gated loss
function to train the CPAM in an end-to-end manner. We will present extensive
results to demonstrate the performances of our SPAS system using publicly
available image composition datasets.

</details>


### [115] [ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language and Vision Assistant](https://arxiv.org/abs/2505.03654)
*Yifan Xiang,Zhenxi Zhang,Bin Li,Yixuan Weng,Shoujun Zhou,Yangfan He,Keqin Li*

Main category: cs.CV

TL;DR: 论文提出ReGraP数据集和ReGraP-LLaVA模型，解决现有方法在个性化多模态大语言模型（MLLM）中关系推理的不足，通过图提示方法实现结构化推理，并在新基准测试中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在个性化MLLM中缺乏对多对象关系的建模和推理能力，且实验局限于单一概念任务。

Method: 提出ReGraP数据集（含图像、知识图和推理问答对），设计软硬图提示方法训练ReGraP-LLaVA模型。

Result: ReGraP-LLaVA在关系推理和知识连接任务中表现最优，超越现有方法。

Conclusion: ReGraP数据集和模型有效提升个性化MLLM的关系推理能力，为未来研究提供新方向。

Abstract: Recent advances in personalized MLLMs enable effective capture of
user-specific concepts, supporting both recognition of personalized concepts
and contextual captioning. However, humans typically explore and reason over
relations among objects and individuals, transcending surface-level information
to achieve more personalized and contextual understanding. To this end,
existing methods may face three main limitations: Their training data lacks
multi-object sets in which relations among objects are learnable. Building on
the limited training data, their models overlook the relations between
different personalized concepts and fail to reason over them. Their experiments
mainly focus on a single personalized concept, where evaluations are limited to
recognition and captioning tasks. To address the limitations, we present a new
dataset named ReGraP, consisting of 120 sets of personalized knowledge. Each
set includes images, KGs, and CoT QA pairs derived from the KGs, enabling more
structured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an
MLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard
graph prompting methods are designed to align KGs within the model's semantic
space. We establish the ReGraP Benchmark, which contains diverse task types:
multiple-choice, fill-in-the-blank, True/False, and descriptive questions in
both open- and closed-ended settings. The proposed benchmark is designed to
evaluate the relational reasoning and knowledge-connection capability of
personalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and
other competitive MLLMs. Results show that the proposed model not only learns
personalized knowledge but also performs relational reasoning in responses,
achieving the SoTA performance compared with the competitive methods. All the
codes and datasets are released at: https://github.com/xyfyyds/ReGraP.

</details>


### [116] [Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps from T1-Weighted MRI using CycleGAN Models](https://arxiv.org/abs/2505.03662)
*Xin Du,Francesca M. Cozzi,Rajesh Jena*

Main category: cs.CV

TL;DR: 提出了一种基于CycleGAN的方法，直接从T1加权MRI扫描生成FA图，解决了FA图与纤维束图谱的空间对齐问题，并在肿瘤区域表现优异。


<details>
  <summary>Details</summary>
Motivation: FA和DEC图对评估白质完整性至关重要，但FA图与纤维束图谱的空间对齐问题阻碍了其在预测模型中的有效整合。

Method: 采用CycleGAN方法，利用未配对数据训练模型，直接从T1加权MRI生成FA图。

Result: 模型生成的FA图具有高保真度，在肿瘤区域表现尤为突出，并通过SSIM和PSNR验证。

Conclusion: 该方法为临床工作流提供了AI驱动的替代方案，减少了对额外扫描的需求。

Abstract: Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are
essential for evaluating white matter integrity and structural connectivity in
neuroimaging. However, the spatial misalignment between FA maps and
tractography atlases hinders their effective integration into predictive
models. To address this issue, we propose a CycleGAN based approach for
generating FA maps directly from T1-weighted MRI scans, representing the first
application of this technique to both healthy and tumour-affected tissues. Our
model, trained on unpaired data, produces high fidelity maps, which have been
rigorously evaluated using Structural Similarity Index (SSIM) and Peak
Signal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in
tumour regions. Radiological assessments further underscore the model's
potential to enhance clinical workflows by providing an AI-driven alternative
that reduces the necessity for additional scans.

</details>


### [117] [Distribution-Conditional Generation: From Class Distribution to Creative Generation](https://arxiv.org/abs/2505.03667)
*Fu Feng,Yucheng Xie,Xu Yang,Jing Wang,Xin Geng*

Main category: cs.CV

TL;DR: 论文提出了一种名为DisTok的框架，通过分布条件生成实现语义无约束的创意图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型依赖训练数据分布，难以生成真正新颖的概念。

Method: 提出Distribution-Conditional Generation和DisTok框架，将类分布映射到潜在空间并解码为创意概念令牌。

Result: DisTok在文本-图像对齐和人类偏好评分上达到最优性能。

Conclusion: DisTok通过分布条件融合和基于采样的合成，实现了高效灵活的令牌级生成。

Abstract: Text-to-image (T2I) diffusion models are effective at producing semantically
aligned images, but their reliance on training data distributions limits their
ability to synthesize truly novel, out-of-distribution concepts. Existing
methods typically enhance creativity by combining pairs of known concepts,
yielding compositions that, while out-of-distribution, remain linguistically
describable and bounded within the existing semantic space. Inspired by the
soft probabilistic outputs of classifiers on ambiguous inputs, we propose
Distribution-Conditional Generation, a novel formulation that models creativity
as image synthesis conditioned on class distributions, enabling semantically
unconstrained creative generation. Building on this, we propose DisTok, an
encoder-decoder framework that maps class distributions into a latent space and
decodes them into tokens of creative concept. DisTok maintains a dynamic
concept pool and iteratively sampling and fusing concept pairs, enabling the
generation of tokens aligned with increasingly complex class distributions. To
enforce distributional consistency, latent vectors sampled from a Gaussian
prior are decoded into tokens and rendered into images, whose class
distributions-predicted by a vision-language model-supervise the alignment
between input distributions and the visual semantics of generated tokens. The
resulting tokens are added to the concept pool for subsequent composition.
Extensive experiments demonstrate that DisTok, by unifying
distribution-conditioned fusion and sampling-based synthesis, enables efficient
and flexible token-level generation, achieving state-of-the-art performance
with superior text-image alignment and human preference scores.

</details>


### [118] [CaRaFFusion: Improving 2D Semantic Segmentation with Camera-Radar Point Cloud Fusion and Zero-Shot Image Inpainting](https://arxiv.org/abs/2505.03679)
*Huawei Sun,Bora Kunter Sahin,Georg Stettinger,Maximilian Bernhard,Matthias Schubert,Robert Wille*

Main category: cs.CV

TL;DR: 提出了一种融合相机和雷达数据的新框架，通过扩散模型和伪掩码生成提升语义分割性能，在恶劣天气条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 相机在恶劣天气下性能下降，雷达数据稀疏且噪声多，融合两者可提升环境感知能力。

Method: 结合扩散模型和雷达点特征生成伪掩码，利用噪声减少单元优化掩码，生成修复图像补充信息。

Result: 在Waterscenes数据集上，相机基线性能提升2.63% mIoU，融合架构提升1.48% mIoU。

Conclusion: 该方法有效提升了恶劣天气下相机-雷达融合的语义分割性能。

Abstract: Segmenting objects in an environment is a crucial task for autonomous driving
and robotics, as it enables a better understanding of the surroundings of each
agent. Although camera sensors provide rich visual details, they are vulnerable
to adverse weather conditions. In contrast, radar sensors remain robust under
such conditions, but often produce sparse and noisy data. Therefore, a
promising approach is to fuse information from both sensors. In this work, we
propose a novel framework to enhance camera-only baselines by integrating a
diffusion model into a camera-radar fusion architecture. We leverage radar
point features to create pseudo-masks using the Segment-Anything model,
treating the projected radar points as point prompts. Additionally, we propose
a noise reduction unit to denoise these pseudo-masks, which are further used to
generate inpainted images that complete the missing information in the original
images. Our method improves the camera-only segmentation baseline by 2.63% in
mIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on the
Waterscenes dataset. This demonstrates the effectiveness of our approach for
semantic segmentation using camera-radar fusion under adverse weather
conditions.

</details>


### [119] [Matching Distance and Geometric Distribution Aided Learning Multiview Point Cloud Registration](https://arxiv.org/abs/2505.03692)
*Shiqi Li,Jihua Zhu,Yifan Xie,Naiwen Hu,Di Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于匹配距离的网络模型用于构建多视点云配准的位姿图，并通过数据驱动的神经网络模型计算绝对位姿，实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 多视点云配准在机器人、自动化和计算机视觉领域至关重要，但现有方法在构建位姿图时可能不可靠，且运动同步常依赖人工设计的损失函数。

Method: 设计网络模型从点云对的匹配距离中提取信息构建位姿图；提出数据驱动的神经网络模型计算绝对位姿，利用几何分布信息和改进的注意力机制。

Result: 在多种室内外数据集上的实验证明了方法的有效性和泛化能力。

Conclusion: 所提方法在多视点云配准中表现优越，代码已开源。

Abstract: Multiview point cloud registration plays a crucial role in robotics,
automation, and computer vision fields. This paper concentrates on pose graph
construction and motion synchronization within multiview registration. Previous
methods for pose graph construction often pruned fully connected graphs or
constructed sparse graph using global feature aggregated from local
descriptors, which may not consistently yield reliable results. To identify
dependable pairs for pose graph construction, we design a network model that
extracts information from the matching distance between point cloud pairs. For
motion synchronization, we propose another neural network model to calculate
the absolute pose in a data-driven manner, rather than optimizing inaccurate
handcrafted loss functions. Our model takes into account geometric distribution
information and employs a modified attention mechanism to facilitate flexible
and reliable feature interaction. Experimental results on diverse indoor and
outdoor datasets confirm the effectiveness and generalizability of our
approach. The source code is available at https://github.com/Shi-Qi-Li/MDGD.

</details>


### [120] [Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning](https://arxiv.org/abs/2505.03703)
*François Role,Sébastien Meyer,Victor Amblard*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法来评估和减少视觉语言模型中的模态间隙，通过谱方法和最优传输方法，实验证明其在下游任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）存在模态间隙问题，即文本和图像嵌入在共享表示空间中分离，这对多模态检索、聚类等任务有害，但目前缺乏通用且实用的评估和解决方法。

Method: 提出基于谱方法和最优传输方法的新技术，以评估和减少模态间隙。

Result: 在多个图像-文本数据集和模型上的实验表明，这些方法有效且对下游任务有益。

Conclusion: 本文提出的方法成功解决了模态间隙问题，并提升了多模态任务的性能。

Abstract: Vision-language models (VLMs) allow to embed texts and images in a shared
representation space. However, it has been shown that these models are subject
to a modality gap phenomenon meaning there exists a clear separation between
the embeddings from one modality and another in the embedding space. While this
misalignment is detrimental for downstream tasks such as multimodal retrieval,
multimodal clustering or zero-shot classification, etc. no generic and
practical methods have so far been proposed to assess it precisely and even
reduce it. We therefore propose novel measures and effective techniques
(spectral- and optimal transport-based methods) to achieve this goal. Extensive
experiments conducted on several image-text datasets and models demonstrate
their effectiveness and beneficial effects on downstream tasks. Our code is
available at the URL provided in the paper's abstract.

</details>


### [121] [DISARM++: Beyond scanner-free harmonization](https://arxiv.org/abs/2505.03715)
*Luca Caldera,Lara Cavinato,Alessio Cirone,Isabella Cama,Sara Garbarino,Raffaele Lodi,Fabrizio Tagliavini,Anna Nigri,Silvia De Francesco,Andrea Cappozzo,Michele Piana,Francesca Ieva*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的T1加权MR图像跨扫描仪直接协调方法，确保特征提取的可靠性，并在多种应用中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 不同扫描仪获取的T1加权MR图像存在差异，影响神经影像研究的一致性，因此需要一种直接协调方法。

Method: 通过两种方式实现图像转换：映射到无扫描仪空间或特定扫描仪域，无需预处理步骤。

Result: 在脑龄预测、AD分类等任务中表现优异（R2=0.60，AUC=0.95），优于现有方法。

Conclusion: 该方法提供了一种高效、鲁棒的解决方案，适用于多样化神经影像研究。

Abstract: Harmonization of T1-weighted MR images across different scanners is crucial
for ensuring consistency in neuroimaging studies. This study introduces a novel
approach to direct image harmonization, moving beyond feature standardization
to ensure that extracted features remain inherently reliable for downstream
analysis. Our method enables image transfer in two ways: (1) mapping images to
a scanner-free space for uniform appearance across all scanners, and (2)
transforming images into the domain of a specific scanner used in model
training, embedding its unique characteristics. Our approach presents strong
generalization capability, even for unseen scanners not included in the
training phase. We validated our method using MR images from diverse cohorts,
including healthy controls, traveling subjects, and individuals with
Alzheimer's disease (AD). The model's effectiveness is tested in multiple
applications, such as brain age prediction (R2 = 0.60 \pm 0.05), biomarker
extraction, AD classification (Test Accuracy = 0.86 \pm 0.03), and diagnosis
prediction (AUC = 0.95). In all cases, our harmonization technique outperforms
state-of-the-art methods, showing improvements in both reliability and
predictive accuracy. Moreover, our approach eliminates the need for extensive
preprocessing steps, such as skull-stripping, which can introduce errors by
misclassifying brain and non-brain structures. This makes our method
particularly suitable for applications that require full-head analysis,
including research on head trauma and cranial deformities. Additionally, our
harmonization model does not require retraining for new datasets, allowing
smooth integration into various neuroimaging workflows. By ensuring
scanner-invariant image quality, our approach provides a robust and efficient
solution for improving neuroimaging studies across diverse settings. The code
is available at this link.

</details>


### [122] [FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios](https://arxiv.org/abs/2505.03730)
*Shiyi Zhang,Junhao Zhuang,Zhaoyang Zhang,Ying Shan,Yansong Tang*

Main category: cs.CV

TL;DR: FlexiAct是一种新方法，通过RefAdapter和FAE技术，实现了从参考视频到任意目标图像的动作定制，突破了现有方法在空间结构和一致性上的限制。


<details>
  <summary>Details</summary>
Motivation: 现有动作定制方法受限于空间结构的严格约束（如布局、骨架和视角一致性），限制了其在不同场景和主体上的适应性。

Method: 提出FlexiAct，结合RefAdapter（轻量级图像条件适配器）和FAE（频率感知动作提取），实现动作控制和空间结构适应。

Result: 实验表明，FlexiAct能有效将动作转移到具有不同布局、骨架和视角的主体上。

Conclusion: FlexiAct在保持身份一致性的同时，提供了更高的结构灵活性，优于现有方法。

Abstract: Action customization involves generating videos where the subject performs
actions dictated by input control signals. Current methods use pose-guided or
global motion customization but are limited by strict constraints on spatial
structure, such as layout, skeleton, and viewpoint consistency, reducing
adaptability across diverse subjects and scenarios. To overcome these
limitations, we propose FlexiAct, which transfers actions from a reference
video to an arbitrary target image. Unlike existing methods, FlexiAct allows
for variations in layout, viewpoint, and skeletal structure between the subject
of the reference video and the target image, while maintaining identity
consistency. Achieving this requires precise action control, spatial structure
adaptation, and consistency preservation. To this end, we introduce RefAdapter,
a lightweight image-conditioned adapter that excels in spatial adaptation and
consistency preservation, surpassing existing methods in balancing appearance
consistency and structural flexibility. Additionally, based on our
observations, the denoising process exhibits varying levels of attention to
motion (low frequency) and appearance details (high frequency) at different
timesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike
existing methods that rely on separate spatial-temporal architectures, directly
achieves action extraction during the denoising process. Experiments
demonstrate that our method effectively transfers actions to subjects with
diverse layouts, skeletons, and viewpoints. We release our code and model
weights to support further research at
https://shiyi-zh0408.github.io/projectpages/FlexiAct/

</details>


### [123] [Multi-Agent System for Comprehensive Soccer Understanding](https://arxiv.org/abs/2505.03735)
*Jiayuan Rao,Zifeng Li,Haoning Wu,Ya Zhang,Yanfeng Wang,Weidi Xie*

Main category: cs.CV

TL;DR: 提出一个全面的足球理解框架，包括构建知识库SoccerWiki、基准测试SoccerBench、多智能体系统SoccerAgent，并公开数据和代码。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动的足球理解研究多集中于孤立任务，缺乏全面性。

Method: 构建SoccerWiki知识库、SoccerBench基准测试和多智能体系统SoccerAgent。

Result: SoccerAgent在多模态问答任务中表现优异，优于现有模型。

Conclusion: 该框架为足球理解提供了全面解决方案，未来可扩展至其他领域。

Abstract: Recent advancements in AI-driven soccer understanding have demonstrated rapid
progress, yet existing research predominantly focuses on isolated or narrow
tasks. To bridge this gap, we propose a comprehensive framework for holistic
soccer understanding. Specifically, we make the following contributions in this
paper: (i) we construct SoccerWiki, the first large-scale multimodal soccer
knowledge base, integrating rich domain knowledge about players, teams,
referees, and venues to enable knowledge-driven reasoning; (ii) we present
SoccerBench, the largest and most comprehensive soccer-specific benchmark,
featuring around 10K standardized multimodal (text, image, video) multi-choice
QA pairs across 13 distinct understanding tasks, curated through automated
pipelines and manual verification; (iii) we introduce SoccerAgent, a novel
multi-agent system that decomposes complex soccer questions via collaborative
reasoning, leveraging domain expertise from SoccerWiki and achieving robust
performance; (iv) extensive evaluations and ablations that benchmark
state-of-the-art MLLMs on SoccerBench, highlighting the superiority of our
proposed agentic system. All data and code are publicly available at:
https://jyrao.github.io/SoccerAgent/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [124] [Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach](https://arxiv.org/abs/2505.02952)
*Fabrizio Marozzo*

Main category: cs.AI

TL;DR: 提出了一种通过结构化澄清问题解决自然语言指令模糊性的迭代方法，相比传统一次性解决方案更准确、高效。


<details>
  <summary>Details</summary>
Motivation: 自然语言的模糊性导致用户需多次测试和修正提示，影响效率。

Method: 通过结构化澄清问题和替代方案提案逐步消除模糊性，结合输入/输出示例。

Result: 在编码、数据分析和创意写作等任务中表现更优，准确性高、解决时间短、用户满意度高。

Conclusion: 迭代方法显著优于传统一次性解决方案，适用于多种任务。

Abstract: Generative AI systems have revolutionized human interaction by enabling
natural language-based coding and problem solving. However, the inherent
ambiguity of natural language often leads to imprecise instructions, forcing
users to iteratively test, correct, and resubmit their prompts. We propose an
iterative approach that systematically narrows down these ambiguities through a
structured series of clarification questions and alternative solution
proposals, illustrated with input/output examples as well. Once every
uncertainty is resolved, a final, precise solution is generated. Evaluated on a
diverse dataset spanning coding, data analysis, and creative writing, our
method demonstrates superior accuracy, competitive resolution times, and higher
user satisfaction compared to conventional one-shot solutions, which typically
require multiple manual iterations to achieve a correct output.

</details>


### [125] [The Multimodal Paradox: How Added and Missing Modalities Shape Bias and Performance in Multimodal AI](https://arxiv.org/abs/2505.03020)
*Kishore Sampath,Pratheesh,Ayaazuddin Mohammad,Resmi Ramachandranpillai*

Main category: cs.AI

TL;DR: 多模态学习通过整合多种数据源（如图像、文本和结构化数据）在决策中表现优于单模态方法。本文探讨了多模态对性能和公平性的影响，以及模态缺失对模型鲁棒性的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态学习在性能上表现优越，但其在公平性和鲁棒性方面的潜在问题常被忽视。本文旨在填补这一研究空白。

Method: 通过分析多模态模型的性能和公平性，研究模态添加和缺失的影响，并使用医疗数据集进行实验验证。

Result: 添加模态通常提升性能，但公平性表现因评估指标和数据集而异；模态缺失会降低性能和公平性。

Conclusion: 多模态学习需平衡性能与公平性，模态缺失问题需进一步解决以提高模型鲁棒性。

Abstract: Multimodal learning, which integrates diverse data sources such as images,
text, and structured data, has proven superior to unimodal counterparts in
high-stakes decision-making. However, while performance gains remain the gold
standard for evaluating multimodal systems, concerns around bias and robustness
are frequently overlooked. In this context, this paper explores two key
research questions (RQs): (i) RQ1 examines whether adding a modality
con-sistently enhances performance and investigates its role in shaping
fairness measures, assessing whether it mitigates or amplifies bias in
multimodal models; (ii) RQ2 investigates the impact of missing modalities at
inference time, analyzing how multimodal models generalize in terms of both
performance and fairness. Our analysis reveals that incorporating new
modalities during training consistently enhances the performance of multimodal
models, while fairness trends exhibit variability across different evaluation
measures and datasets. Additionally, the absence of modalities at inference
degrades performance and fairness, raising concerns about its robustness in
real-world deployment. We conduct extensive experiments using multimodal
healthcare datasets containing images, time series, and structured information
to validate our findings.

</details>


### [126] [Evaluating the Impact of AI-Powered Audiovisual Personalization on Learner Emotion, Focus, and Learning Outcomes](https://arxiv.org/abs/2505.03033)
*George Xi Wang,Jingying Deng,Safinah Ali*

Main category: cs.AI

TL;DR: 论文提出了一种基于大语言模型（LLM）的个性化多感官学习环境系统，旨在通过定制化的视听元素提升学习者的专注力和情绪稳定性。


<details>
  <summary>Details</summary>
Motivation: 独立学习者在非结构化或易分心的环境中难以保持专注和情绪调节，现有教育技术忽视了学习的情感与感官背景。

Method: 利用LLM生成个性化视听学习环境，结合生物特征测量和绩效结果进行混合方法研究。

Result: 研究评估了LLM驱动的感官个性化对认知负荷和学习参与度的影响。

Conclusion: 研究旨在推动情感响应式教育技术的发展，并拓展多模态LLM在自主学习感官维度中的应用。

Abstract: Independent learners often struggle with sustaining focus and emotional
regulation in unstructured or distracting settings. Although some rely on
ambient aids such as music, ASMR, or visual backgrounds to support
concentration, these tools are rarely integrated into cohesive,
learner-centered systems. Moreover, existing educational technologies focus
primarily on content adaptation and feedback, overlooking the emotional and
sensory context in which learning takes place. Large language models have
demonstrated powerful multimodal capabilities including the ability to generate
and adapt text, audio, and visual content. Educational research has yet to
fully explore their potential in creating personalized audiovisual learning
environments. To address this gap, we introduce an AI-powered system that uses
LLMs to generate personalized multisensory study environments. Users select or
generate customized visual themes (e.g., abstract vs. realistic, static vs.
animated) and auditory elements (e.g., white noise, ambient ASMR, familiar vs.
novel sounds) to create immersive settings aimed at reducing distraction and
enhancing emotional stability. Our primary research question investigates how
combinations of personalized audiovisual elements affect learner cognitive load
and engagement. Using a mixed-methods design that incorporates biometric
measures and performance outcomes, this study evaluates the effectiveness of
LLM-driven sensory personalization. The findings aim to advance emotionally
responsive educational technologies and extend the application of multimodal
LLMs into the sensory dimension of self-directed learning.

</details>


### [127] [BLAB: Brutally Long Audio Bench](https://arxiv.org/abs/2505.03054)
*Orevaoghene Ahia,Martijn Bartelds,Kabir Ahuja,Hila Gonen,Valentin Hofmann,Siddhant Arora,Shuyue Stella Li,Vishal Puttagunta,Mofetoluwa Adeyemi,Charishma Buchireddy,Ben Walls,Noah Bennett,Shinji Watanabe,Noah A. Smith,Yulia Tsvetkov,Sachin Kumar*

Main category: cs.AI

TL;DR: BLAB是一个针对长音频语言模型的挑战性基准测试，评估模型在长音频任务中的表现，发现现有模型在长音频理解上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 开发能够理解多样化语音交互的大型音频语言模型，以提升语言技术的多模态性和可访问性。

Method: 引入BLAB基准测试，包含833+小时的长音频片段和人工标注的问答对，评估六种音频语言模型。

Result: 所有模型在长音频任务中表现不佳，性能随音频时长增加而下降。

Conclusion: BLAB为开发具有强大长音频理解能力的音频语言模型提供了挑战性框架。

Abstract: Developing large audio language models (LMs) capable of understanding diverse
spoken interactions is essential for accommodating the multimodal nature of
human communication and can increase the accessibility of language technologies
across different user populations. Recent work on audio LMs has primarily
evaluated their performance on short audio segments, typically under 30
seconds, with limited exploration of long-form conversational speech segments
that more closely reflect natural user interactions with these models. We
introduce Brutally Long Audio Bench (BLAB), a challenging long-form audio
benchmark that evaluates audio LMs on localization, duration estimation,
emotion, and counting tasks using audio segments averaging 51 minutes in
length. BLAB consists of 833+ hours of diverse, full-length audio clips, each
paired with human-annotated, text-based natural language questions and answers.
Our audio data were collected from permissively licensed sources and underwent
a human-assisted filtering process to ensure task compliance. We evaluate six
open-source and proprietary audio LMs on BLAB and find that all of them,
including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the
tasks in BLAB. Our comprehensive analysis reveals key insights into the
trade-offs between task difficulty and audio duration. In general, we find that
audio LMs struggle with long-form speech, with performance declining as
duration increases. They perform poorly on localization, temporal reasoning,
counting, and struggle to understand non-phonemic information, relying more on
prompts than audio content. BLAB serves as a challenging evaluation framework
to develop audio LMs with robust long-form audio understanding capabilities.

</details>


### [128] [Is AI currently capable of identifying wild oysters? A comparison of human annotators against the AI model, ODYSSEE](https://arxiv.org/abs/2505.03108)
*Brendan Campbell,Alan Williams,Kleio Baxevani,Alyssa Campbell,Rushabh Dhoke,Rileigh E. Hudock,Xiaomin Lin,Vivek Mange,Bernhard Neuberger,Arjun Suresh,Alhim Vera,Arthur Trembanis,Herbert G. Tanner,Edward Hale*

Main category: cs.AI

TL;DR: 论文提出了一种基于深度学习的ODYSSEE模型，用于通过图像或视频识别活牡蛎，以替代传统的破坏性采样方法。尽管模型速度更快，但准确性低于人工标注。


<details>
  <summary>Details</summary>
Motivation: 传统牡蛎礁监测方法破坏性强且耗时，需要一种非破坏性、高效的替代方案。

Method: 使用ODYSSEE深度学习模型，通过图像或视频识别活牡蛎，并与专家和非专家标注结果对比。

Result: 模型速度显著快于人工标注（39.6秒 vs. 数小时），但准确性较低（63% vs. 74%-75%）。图像质量影响模型和人工的准确性。

Conclusion: 尽管ODYSSEE模型目前准确性不足，但通过改进图像质量、增加训练数据和标注类别，未来有望提升其预测能力。

Abstract: Oysters are ecologically and commercially important species that require
frequent monitoring to track population demographics (e.g. abundance, growth,
mortality). Current methods of monitoring oyster reefs often require
destructive sampling methods and extensive manual effort. Therefore, they are
suboptimal for small-scale or sensitive environments. A recent alternative, the
ODYSSEE model, was developed to use deep learning techniques to identify live
oysters using video or images taken in the field of oyster reefs to assess
abundance. The validity of this model in identifying live oysters on a reef was
compared to expert and non-expert annotators. In addition, we identified
potential sources of prediction error. Although the model can make inferences
significantly faster than expert and non-expert annotators (39.6 s, $2.34 \pm
0.61$ h, $4.50 \pm 1.46$ h, respectively), the model overpredicted the number
of live oysters, achieving lower accuracy (63\%) in identifying live oysters
compared to experts (74\%) and non-experts (75\%) alike. Image quality was an
important factor in determining the accuracy of the model and the annotators.
Better quality images improved human accuracy and worsened model accuracy.
Although ODYSSEE was not sufficiently accurate, we anticipate that future
training on higher-quality images, utilizing additional live imagery, and
incorporating additional annotation training classes will greatly improve the
model's predictive power based on the results of this analysis. Future research
should address methods that improve the detection of living vs. dead oysters.

</details>


### [129] [Holmes: Automated Fact Check with Large Language Models](https://arxiv.org/abs/2505.03135)
*Haoran Ou,Gelei Deng,Xingshuo Han,Jie Zhang,Xinlei He,Han Qiu,Shangwei Guo,Tianwei Zhang*

Main category: cs.AI

TL;DR: 论文探讨了利用大型语言模型（LLMs）检测多模态虚假信息，提出Holmes框架，通过证据检索提升准确性。


<details>
  <summary>Details</summary>
Motivation: 互联网虚假信息威胁社会信任与安全，传统深度学习方法难以应对多模态虚假信息的复杂性。

Method: 提出Holmes框架，结合LLMs和新型证据检索方法，包括LLM驱动的摘要提取和质量评估算法。

Result: Holmes在公开数据集上达到88.3%准确率，实时任务中90.2%，证据检索提升30.8%准确性。

Conclusion: Holmes框架有效提升LLMs在虚假信息检测中的性能，证据检索是关键。

Abstract: The rise of Internet connectivity has accelerated the spread of
disinformation, threatening societal trust, decision-making, and national
security. Disinformation has evolved from simple text to complex multimodal
forms combining images and text, challenging existing detection methods.
Traditional deep learning models struggle to capture the complexity of
multimodal disinformation. Inspired by advances in AI, this study explores
using Large Language Models (LLMs) for automated disinformation detection. The
empirical study shows that (1) LLMs alone cannot reliably assess the
truthfulness of claims; (2) providing relevant evidence significantly improves
their performance; (3) however, LLMs cannot autonomously search for accurate
evidence. To address this, we propose Holmes, an end-to-end framework featuring
a novel evidence retrieval method that assists LLMs in collecting high-quality
evidence. Our approach uses (1) LLM-powered summarization to extract key
information from open sources and (2) a new algorithm and metrics to evaluate
evidence quality. Holmes enables LLMs to verify claims and generate
justifications effectively. Experiments show Holmes achieves 88.3% accuracy on
two open-source datasets and 90.2% in real-time verification tasks. Notably,
our improved evidence retrieval boosts fact-checking accuracy by 30.8% over
existing methods

</details>


### [130] [CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics](https://arxiv.org/abs/2505.03171)
*Junqi Liu,Xiaohan Lin,Jonas Bayer,Yael Dillies,Weijie Jiang,Xiaodan Liang,Roman Soletskyi,Haiming Wang,Yunzhou Xie,Beibei Xiong,Zhengfeng Yang,Jujian Zhang,Lihong Zhi,Jia Li,Zhengying Liu*

Main category: cs.AI

TL;DR: 论文介绍了CombiBench，一个包含100个组合问题的基准测试，用于评估大型语言模型在组合数学中的表现，并提出了Fine-Eval评估框架。


<details>
  <summary>Details</summary>
Motivation: 组合数学领域缺乏合适的基准测试和定理库，限制了神经符号方法在该领域的发展。

Method: 提出CombiBench基准测试和Fine-Eval评估框架，使用Kimina Lean Server作为后端，测试多个LLM的表现。

Result: 测试的LLM在组合数学问题上的表现有限，Kimina-Prover表现最佳，解决了7个问题。

Conclusion: CombiBench填补了组合数学领域的基准测试空白，为未来的研究提供了工具和数据支持。

Abstract: Neurosymbolic approaches integrating large language models with formal
reasoning have recently achieved human-level performance on mathematics
competition problems in algebra, geometry and number theory. In comparison,
combinatorics remains a challenging domain, characterized by a lack of
appropriate benchmarks and theorem libraries. To address this gap, we introduce
CombiBench, a comprehensive benchmark comprising 100 combinatorial problems,
each formalized in Lean~4 and paired with its corresponding informal statement.
The problem set covers a wide spectrum of difficulty levels, ranging from
middle school to IMO and university level, and span over ten combinatorial
topics. CombiBench is suitable for testing IMO solving capabilities since it
includes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its
statement contain an images). Furthermore, we provide a comprehensive and
standardized evaluation framework, dubbed Fine-Eval (for
$\textbf{F}$ill-in-the-blank $\textbf{in}$ L$\textbf{e}$an Evaluation), for
formal mathematics. It accommodates not only proof-based problems but also, for
the first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval
as the evaluation method and Kimina Lean Server as the backend, we benchmark
several LLMs on CombiBench and observe that their capabilities for formally
solving combinatorial problems remain limited. Among all models tested (none of
which has been trained for this particular task), Kimina-Prover attains the
best results, solving 7 problems (out of 100) under both ``with solution'' and
``without solution'' scenarios. We open source the benchmark dataset alongside
with the code of the proposed evaluation method at
https://github.com/MoonshotAI/CombiBench/.

</details>


### [131] [Patterns and Mechanisms of Contrastive Activation Engineering](https://arxiv.org/abs/2505.03189)
*Yixiong Hao,Ayush Panda,Stepan Shabalin,Sheikh Abdur Raheem Ali*

Main category: cs.AI

TL;DR: 对比激活工程（CAE）是一种零成本、灵活的LLM行为调控方法，但其效果受限于分布内场景，且存在对抗性输入和模型困惑度增加的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）行为调控的复杂性和不透明性问题，探索低成本、高效的方法。

Method: 采用对比激活工程（CAE）技术，通过修改模型内部表示来调控输出。

Result: CAE在分布内场景有效，但样本数量超过80后收益递减，且易受对抗性输入影响，同时增加模型困惑度。

Conclusion: CAE是一种有潜力的方法，但需进一步优化和指导以应对其局限性。

Abstract: Controlling the behavior of Large Language Models (LLMs) remains a
significant challenge due to their inherent complexity and opacity. While
techniques like fine-tuning can modify model behavior, they typically require
extensive computational resources. Recent work has introduced a class of
contrastive activation engineering (CAE) techniques as promising approaches for
steering LLM outputs through targeted modifications to their internal
representations. Applied at inference-time with zero cost, CAE has the
potential to introduce a new paradigm of flexible, task-specific LLM behavior
tuning. We analyze the performance of CAE in in-distribution,
out-of-distribution settings, evaluate drawbacks, and begin to develop
comprehensive guidelines for its effective deployment. We find that 1. CAE is
only reliably effective when applied to in-distribution contexts. 2. Increasing
the number of samples used to generate steering vectors has diminishing returns
at around 80 samples. 3. Steering vectors are susceptible to adversarial inputs
that reverses the behavior that is steered for. 4. Steering vectors harm the
overall model perplexity. 5. Larger models are more resistant to
steering-induced degradation.

</details>


### [132] [RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation](https://arxiv.org/abs/2505.03275)
*Tiantian Gan,Qiyao Sun*

Main category: cs.AI

TL;DR: RAG-MCP框架通过语义检索减少提示词数量，提升工具选择准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLM）在外部工具使用中因提示词膨胀和选择复杂性导致的问题。

Method: 引入RAG-MCP框架，通过语义检索从外部索引中筛选相关工具描述，减少输入提示词。

Result: 实验显示，RAG-MCP显著减少提示词数量（如50%以上），工具选择准确率提升三倍以上（43.13% vs 13.62%）。

Conclusion: RAG-MCP为LLM提供了可扩展且精准的工具集成方案。

Abstract: Large language models (LLMs) struggle to effectively utilize a growing number
of external tools, such as those defined by the Model Context Protocol
(MCP)\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We
introduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes
this challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to
identify the most relevant MCP(s) for a given query from an external index
before engaging the LLM. Only the selected tool descriptions are passed to the
model, drastically reducing prompt size and simplifying decision-making.
Experiments, including an MCP stress test, demonstrate RAG-MCP significantly
cuts prompt tokens (e.g., by over 50%) and more than triples tool selection
accuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables
scalable and accurate tool integration for LLMs.

</details>


### [133] [Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for Reusing Existing Libraries and Interfaces](https://arxiv.org/abs/2505.03295)
*Luis Miguel Vieira da Silva,Aljosha Köcher,Nicolas König,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: 论文提出了一种利用大语言模型基于自然语言输入生成技能实现代码的方法，通过将能力视为合同并集成现有软件库，实现跨语言代码生成。


<details>
  <summary>Details</summary>
Motivation: 开发符合能力的技能实现耗时且具有挑战性，需要一种更高效的方法。

Method: 将能力视为合同，利用大语言模型生成代码，集成现有库和接口技术，支持跨语言实现。

Result: 通过自主移动机器人（Python和ROS 2控制）验证了方法的可行性和灵活性。

Conclusion: 该方法显著提升了技能实现的效率，并展示了跨语言生成的潜力。

Abstract: Modern automation systems increasingly rely on modular architectures, with
capabilities and skills as one solution approach. Capabilities define the
functions of resources in a machine-readable form and skills provide the
concrete implementations that realize those capabilities. However, the
development of a skill implementation conforming to a corresponding capability
remains a time-consuming and challenging task. In this paper, we present a
method that treats capabilities as contracts for skill implementations and
leverages large language models to generate executable code based on natural
language user input. A key feature of our approach is the integration of
existing software libraries and interface technologies, enabling the generation
of skill implementations across different target languages. We introduce a
framework that allows users to incorporate their own libraries and resource
interfaces into the code generation process through a retrieval-augmented
generation architecture. The proposed method is evaluated using an autonomous
mobile robot controlled via Python and ROS 2, demonstrating the feasibility and
flexibility of the approach.

</details>


### [134] [Artificial Behavior Intelligence: Technology, Challenges, and Future Directions](https://arxiv.org/abs/2505.03315)
*Kanghyun Jo,Jehwan Choi,Kwanho Kim,Seongmin Kim,Duy-Linh Nguyen,Xuan-Thuy Vo,Adri Priadana,Tien-Dat Tran*

Main category: cs.AI

TL;DR: 本文提出人工行为智能（ABI）技术框架，分析人类行为的多模态数据，并探讨预训练模型在提升行为识别精度中的作用。


<details>
  <summary>Details</summary>
Motivation: 理解和预测人类行为在自动驾驶、智能医疗等领域至关重要，ABI框架旨在全面解析人类行为。

Method: ABI包括姿态估计、表情识别、行为序列分析和上下文建模，结合预训练模型优化识别效果。

Result: 研究团队开发轻量级模型，解决数据有限、行为预测不确定性和实时推理等技术挑战。

Conclusion: ABI在现实应用中潜力巨大，需进一步优化模型结构和推理效率。

Abstract: Understanding and predicting human behavior has emerged as a core capability
in various AI application domains such as autonomous driving, smart healthcare,
surveillance systems, and social robotics. This paper defines the technical
framework of Artificial Behavior Intelligence (ABI), which comprehensively
analyzes and interprets human posture, facial expressions, emotions, behavioral
sequences, and contextual cues. It details the essential components of ABI,
including pose estimation, face and emotion recognition, sequential behavior
analysis, and context-aware modeling. Furthermore, we highlight the
transformative potential of recent advances in large-scale pretrained models,
such as large language models (LLMs), vision foundation models, and multimodal
integration models, in significantly improving the accuracy and
interpretability of behavior recognition. Our research team has a strong
interest in the ABI domain and is actively conducting research, particularly
focusing on the development of intelligent lightweight models capable of
efficiently inferring complex human behaviors. This paper identifies several
technical challenges that must be addressed to deploy ABI in real-world
applications including learning behavioral intelligence from limited data,
quantifying uncertainty in complex behavior prediction, and optimizing model
structures for low-power, real-time inference. To tackle these challenges, our
team is exploring various optimization strategies including lightweight
transformers, graph-based recognition architectures, energy-aware loss
functions, and multimodal knowledge distillation, while validating their
applicability in real-time environments.

</details>


### [135] [AI-Driven Scholarly Peer Review via Persistent Workflow Prompting, Meta-Prompting, and Meta-Reasoning](https://arxiv.org/abs/2505.03332)
*Evgeny Markhasin*

Main category: cs.AI

TL;DR: 论文提出了一种名为Persistent Workflow Prompting (PWP)的方法，用于改进大型语言模型(LLM)在科学论文评审中的表现，通过结构化提示工程实现复杂任务处理。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在科学论文评审中因数据限制和专家推理复杂性导致的挑战。

Method: 采用PWP方法，通过分层模块化架构（Markdown结构化）定义详细分析流程，结合元提示和元推理技术。

Result: PWP引导的LLM能够识别实验化学论文中的主要方法缺陷，并完成复杂任务如区分主张与证据、多模态分析等。

Conclusion: PWP方法通过工作流形式化，展示了利用现有LLM完成复杂科学任务的潜力，并提供了透明化的实现资源。

Abstract: Critical peer review of scientific manuscripts presents a significant
challenge for Large Language Models (LLMs), partly due to data limitations and
the complexity of expert reasoning. This report introduces Persistent Workflow
Prompting (PWP), a potentially broadly applicable prompt engineering
methodology designed to bridge this gap using standard LLM chat interfaces
(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical
analysis of experimental chemistry manuscripts, featuring a hierarchical,
modular architecture (structured via Markdown) that defines detailed analysis
workflows. We develop this PWP prompt through iterative application of
meta-prompting techniques and meta-reasoning aimed at systematically codifying
expert review workflows, including tacit knowledge. Submitted once at the start
of a session, this PWP prompt equips the LLM with persistent workflows
triggered by subsequent queries, guiding modern reasoning LLMs through
systematic, multimodal evaluations. Demonstrations show the PWP-guided LLM
identifying major methodological flaws in a test case while mitigating LLM
input bias and performing complex tasks, including distinguishing claims from
evidence, integrating text/photo/figure analysis to infer parameters, executing
quantitative feasibility checks, comparing estimates against claims, and
assessing a priori plausibility. To ensure transparency and facilitate
replication, we provide full prompts, detailed demonstration analyses, and logs
of interactive chats as supplementary resources. Beyond the specific
application, this work offers insights into the meta-development process
itself, highlighting the potential of PWP, informed by detailed workflow
formalization, to enable sophisticated analysis using readily available LLMs
for complex scientific tasks.

</details>


### [136] [Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection](https://arxiv.org/abs/2505.03359)
*June-Woo Kim,Haram Yoon,Wonkyo Oh,Dawoon Jung,Sung-Hoon Yoon,Dae-Jin Kim,Dong-Ho Lee,Sang-Yeol Lee,Chan-Mo Yang*

Main category: cs.AI

TL;DR: 该论文提出了一种基于域对抗训练的方法，用于减少语音AI模型在抑郁和PTSD检测中的性别偏见，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 语音AI模型在抑郁和PTSD检测中存在性别偏见，导致预测不公和不准，研究旨在解决这一问题。

Method: 采用域对抗训练方法，将不同性别视为不同域，并将此信息整合到预训练的语音基础模型中，使用E-DAIC数据集验证。

Result: 实验结果显示，该方法显著提升检测性能，F1分数比基线提高了13.29个百分点。

Conclusion: 研究表明，解决AI驱动的心理健康评估中的人口统计差异至关重要。

Abstract: Speech-based AI models are emerging as powerful tools for detecting
depression and the presence of Post-traumatic stress disorder (PTSD), offering
a non-invasive and cost-effective way to assess mental health. However, these
models often struggle with gender bias, which can lead to unfair and inaccurate
predictions. In this study, our study addresses this issue by introducing a
domain adversarial training approach that explicitly considers gender
differences in speech-based depression and PTSD detection. Specifically, we
treat different genders as distinct domains and integrate this information into
a pretrained speech foundation model. We then validate its effectiveness on the
E-DAIC dataset to assess its impact on performance. Experimental results show
that our method notably improves detection performance, increasing the F1-score
by up to 13.29 percentage points compared to the baseline. This highlights the
importance of addressing demographic disparities in AI-driven mental health
assessment.

</details>


### [137] [Validating the Effectiveness of a Large Language Model-based Approach for Identifying Children's Development across Various Free Play Settings in Kindergarten](https://arxiv.org/abs/2505.03369)
*Yuanyuan Yang,Yuan Shen,Tianchen Sun,Yangbin Xie*

Main category: cs.AI

TL;DR: 该研究提出了一种结合大型语言模型（LLMs）和学习分析的方法，通过分析儿童的游戏自述来评估其发展能力，结果显示该方法在多个领域准确率超过90%。


<details>
  <summary>Details</summary>
Motivation: 自由游戏对儿童发展至关重要，但传统评估方法难以全面捕捉其发展情况，因此需要一种更高效、准确的评估工具。

Method: 研究结合LLMs和学习分析技术，分析29名幼儿园儿童在四个不同游戏区域的2,224个游戏自述，并计算各区域的性能分数。

Result: LLM方法在识别认知、运动和社交能力方面准确率超过90%，且不同游戏区域对特定能力的发展有显著差异。

Conclusion: 该方法能有效评估儿童在自由游戏中的发展，为教育者提供个性化学习支持，展示了LLMs和学习分析在早期教育中的潜力。

Abstract: Free play is a fundamental aspect of early childhood education, supporting
children's cognitive, social, emotional, and motor development. However,
assessing children's development during free play poses significant challenges
due to the unstructured and spontaneous nature of the activity. Traditional
assessment methods often rely on direct observations by teachers, parents, or
researchers, which may fail to capture comprehensive insights from free play
and provide timely feedback to educators. This study proposes an innovative
approach combining Large Language Models (LLMs) with learning analytics to
analyze children's self-narratives of their play experiences. The LLM
identifies developmental abilities, while performance scores across different
play settings are calculated using learning analytics techniques. We collected
2,224 play narratives from 29 children in a kindergarten, covering four
distinct play areas over one semester. According to the evaluation results from
eight professionals, the LLM-based approach achieved high accuracy in
identifying cognitive, motor, and social abilities, with accuracy exceeding 90%
in most domains. Moreover, significant differences in developmental outcomes
were observed across play settings, highlighting each area's unique
contributions to specific abilities. These findings confirm that the proposed
approach is effective in identifying children's development across various free
play settings. This study demonstrates the potential of integrating LLMs and
learning analytics to provide child-centered insights into developmental
trajectories, offering educators valuable data to support personalized learning
and enhance early childhood education practices.

</details>


### [138] [Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in LLM-Based Agents](https://arxiv.org/abs/2505.03434)
*Schaun Wheeler,Olivier Jeunen*

Main category: cs.AI

TL;DR: 大语言模型（LLMs）在AI领域取得了突破性进展，但其依赖程序性记忆的局限性在复杂环境中显现。本文提出通过结合语义记忆和联想学习系统，构建模块化架构，以提升LLMs在动态环境中的适应能力。


<details>
  <summary>Details</summary>
Motivation: LLMs在程序性任务中表现出色，但在复杂、不可预测的环境中存在局限性。为了应对规则多变、反馈模糊的“棘手”学习环境，需要增强LLMs的认知功能。

Method: 提出模块化架构，将程序性记忆与语义记忆和联想学习系统分离，以提升模型的适应性。

Result: 通过模块化设计，LLMs能够更好地适应动态环境，弥补程序性记忆的不足。

Conclusion: 结合语义记忆和联想学习系统是提升LLMs在复杂环境中表现的关键，模块化架构为实现这一目标提供了可行路径。

Abstract: Large Language Models (LLMs) represent a landmark achievement in Artificial
Intelligence (AI), demonstrating unprecedented proficiency in procedural tasks
such as text generation, code completion, and conversational coherence. These
capabilities stem from their architecture, which mirrors human procedural
memory -- the brain's ability to automate repetitive, pattern-driven tasks
through practice. However, as LLMs are increasingly deployed in real-world
applications, it becomes impossible to ignore their limitations operating in
complex, unpredictable environments. This paper argues that LLMs, while
transformative, are fundamentally constrained by their reliance on procedural
memory. To create agents capable of navigating ``wicked'' learning environments
-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must
augment LLMs with semantic memory and associative learning systems. By adopting
a modular architecture that decouples these cognitive functions, we can bridge
the gap between narrow procedural expertise and the adaptive intelligence
required for real-world problem-solving.

</details>


### [139] [The Steganographic Potentials of Language Models](https://arxiv.org/abs/2505.03439)
*Artem Karpov,Tinuade Adeleke,Seong Hah Cho,Natalia Perez-Campanero*

Main category: cs.AI

TL;DR: 研究探讨了通过强化学习微调的大语言模型（LLMs）在隐写术中的能力，发现当前模型在安全性和容量上表现初级，但明确算法指导可显著提升信息隐藏能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型可能隐藏信息（隐写术），这对检测和对齐AI代理构成挑战，并影响模型的推理忠实性。

Method: 通过强化学习微调LLMs，研究其隐写术能力，包括开发隐蔽编码方案、在提示下进行隐写术，以及在未提示但可能隐藏推理的现实场景中应用。

Result: 实验发现，当前模型在隐写术的安全性和容量上表现初级，但通过算法指导可显著提升隐藏能力。

Conclusion: 研究揭示了LLMs在隐写术中的潜力，并指出明确指导可增强其信息隐藏能力。

Abstract: The potential for large language models (LLMs) to hide messages within plain
text (steganography) poses a challenge to detection and thwarting of unaligned
AI agents, and undermines faithfulness of LLMs reasoning. We explore the
steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)
to: (1) develop covert encoding schemes, (2) engage in steganography when
prompted, and (3) utilize steganography in realistic scenarios where hidden
reasoning is likely, but not prompted. In these scenarios, we detect the
intention of LLMs to hide their reasoning as well as their steganography
performance. Our findings in the fine-tuning experiments as well as in
behavioral non fine-tuning evaluations reveal that while current models exhibit
rudimentary steganographic abilities in terms of security and capacity,
explicit algorithmic guidance markedly enhances their capacity for information
concealment.

</details>


### [140] [am-ELO: A Stable Framework for Arena-based LLM Evaluation](https://arxiv.org/abs/2505.03475)
*Zirui Liu,Jiatong Li,Yan Zhuang,Qi Liu,Shuanghong Shen,Jie Ouyang,Mingyue Cheng,Shijin Wang*

Main category: cs.AI

TL;DR: 提出了一种基于最大似然估计（MLE）的稳定竞技场框架（m-ELO和am-ELO），解决了现有ELO评分系统的不稳定性和标注者能力差异问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于ELO评分系统的框架存在排名不一致导致的稳定性问题，且未考虑标注者能力差异。

Method: 采用MLE方法替代迭代更新（m-ELO），并改进ELO评分概率函数以纳入标注者能力（am-ELO）。

Result: 实验证明该方法提高了稳定性，为大型语言模型提供了更鲁棒、准确和稳定的评估。

Conclusion: 提出的框架显著提升了竞技场评估的稳定性和准确性。

Abstract: Arena-based evaluation is a fundamental yet significant evaluation paradigm
for modern AI models, especially large language models (LLMs). Existing
framework based on ELO rating system suffers from the inevitable instability
problem due to ranking inconsistency and the lack of attention to the varying
abilities of annotators. In this paper, we introduce a novel stable arena
framework to address these issues by enhancing the ELO Rating System.
Specifically, we replace the iterative update method with a Maximum Likelihood
Estimation (MLE) approach, m-ELO, and provide theoretical proof of the
consistency and stability of the MLE approach for model ranking. Additionally,
we proposed the am-ELO, which modify the Elo Rating's probability function to
incorporate annotator abilities, enabling the simultaneous estimation of model
scores and annotator reliability. Experiments demonstrate that this method
ensures stability, proving that this framework offers a more robust, accurate,
and stable evaluation method for LLMs.

</details>


### [141] [STORY2GAME: Generating (Almost) Everything in an Interactive Fiction Game](https://arxiv.org/abs/2505.03547)
*Eric Zhou,Shreyas Basavatia,Moontashir Siam,Zexin Chen,Mark O. Riedl*

Main category: cs.AI

TL;DR: STORY2GAME利用大语言模型生成基于文本的互动小说游戏，通过生成故事、填充世界并构建游戏引擎代码，实现开放式故事生成和动态动作生成。


<details>
  <summary>Details</summary>
Motivation: 传统硬编码动作限制了故事生成的开放性，而动态生成动作能提供更自由的游戏体验。

Method: 利用LLM生成动作的前置条件和效果，指导游戏引擎跟踪和更新状态；动态生成新动作以适应玩家需求。

Result: 评估了动作代码生成的成功率，确保玩家能完整体验生成的故事。

Conclusion: STORY2GAME通过动态动作生成和状态管理，实现了开放式互动故事体验。

Abstract: We introduce STORY2GAME, a novel approach to using Large Language Models to
generate text-based interactive fiction games that starts by generating a
story, populates the world, and builds the code for actions in a game engine
that enables the story to play out interactively. Whereas a given set of
hard-coded actions can artificially constrain story generation, the ability to
generate actions means the story generation process can be more open-ended but
still allow for experiences that are grounded in a game state. The key to
successful action generation is to use LLM-generated preconditions and effects
of actions in the stories as guides for what aspects of the game state must be
tracked and changed by the game engine when a player performs an action. We
also introduce a technique for dynamically generating new actions to
accommodate the player's desire to perform actions that they think of that are
not part of the story. Dynamic action generation may require on-the-fly updates
to the game engine's state representation and revision of previously generated
actions. We evaluate the success rate of action code generation with respect to
whether a player can interactively play through the entire generated story.

</details>


### [142] [A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model Reasoning](https://arxiv.org/abs/2505.03553)
*Kolawole E. Ogunsina,Morayo A. Ogunsina*

Main category: cs.AI

TL;DR: 提出一种基于Hashgraph共识机制的新方法，用于验证和收敛多个大型语言模型（LLM）的输出，减少不一致和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 不同专有推理模型（RM）在相同复杂请求下产生不一致输出，影响AI系统的可靠性。

Method: 采用Hashgraph共识算法，通过gossip-about-gossip通信和虚拟投票实现模型间的共识。

Result: 原型系统设计展示通过多轮迭代交换和更新答案，提高准确性和置信度。

Conclusion: 该方法为多智能体AI系统提供了一种自我验证和高保真响应的可行方向。

Abstract: Inconsistent outputs and hallucinations from large language models (LLMs) are
major obstacles to reliable AI systems. When different proprietary reasoning
models (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI,
are given the same complex request, they often produce divergent results due to
variations in training and inference. This paper proposes a novel consensus
mechanism, inspired by distributed ledger technology, to validate and converge
these outputs, treating each RM as a black-box peer. Building on the Hashgraph
consensus algorithm, our approach employs gossip-about-gossip communication and
virtual voting to achieve agreement among an ensemble of RMs. We present an
architectural design for a prototype system in which RMs iteratively exchange
and update their answers, using information from each round to improve accuracy
and confidence in subsequent rounds. This approach goes beyond simple majority
voting by incorporating the knowledge and cross-verification content of every
model. We justify the feasibility of this Hashgraph-inspired consensus for AI
ensembles and outline its advantages over traditional ensembling techniques in
reducing nonfactual outputs. Preliminary considerations for implementation,
evaluation criteria for convergence and accuracy, and potential challenges are
discussed. The proposed mechanism demonstrates a promising direction for
multi-agent AI systems to self-validate and deliver high-fidelity responses in
complex tasks.

</details>


### [143] [OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents](https://arxiv.org/abs/2505.03570)
*Mariya Davydova,Daniel Jeffries,Patrick Barker,Arturo Márquez Flores,Sinéad Ryan*

Main category: cs.AI

TL;DR: OSUniverse是一个用于评估GUI导航AI代理的多模态桌面任务基准，强调易用性、可扩展性和自动化验证。


<details>
  <summary>Details</summary>
Motivation: 为GUI导航AI代理提供一个全面、可扩展且易于验证的测试基准，以衡量其能力和进步。

Method: 将任务按复杂度分级，从基础点击到多步骤跨应用任务，并引入自动化验证机制。

Result: 当前SOTA代理的得分不超过50%，而普通白领可完美完成任务；自动化验证误差率低于2%。

Conclusion: OSUniverse为GUI导航AI代理的能力评估提供了可靠且自动化的基准。

Abstract: In this paper, we introduce OSUniverse: a benchmark of complex, multimodal
desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on
ease of use, extensibility, comprehensive coverage of test cases, and automated
validation. We divide the tasks in increasing levels of complexity, from basic
precision clicking to multistep, multiapplication tests requiring dexterity,
precision, and clear thinking from the agent. In version one of the benchmark,
presented here, we have calibrated the complexity of the benchmark test cases
to ensure that the SOTA (State of the Art) agents (at the time of publication)
do not achieve results higher than 50%, while the average white collar worker
can perform all these tasks with perfect accuracy. The benchmark can be scored
manually, but we also introduce an automated validation mechanism that has an
average error rate less than 2%. Therefore, this benchmark presents solid
ground for fully automated measuring of progress, capabilities and the
effectiveness of GUI-navigation AI agents over the short and medium-term
horizon. The source code of the benchmark is available at
https://github.com/agentsea/osuniverse.

</details>


### [144] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and Manipulating Human Perceptual Variability](https://arxiv.org/abs/2505.03641)
*Chen Wei,Chi Zhang,Jiachen Zou,Haotian Deng,Dietmar Heinke,Quanying Liu*

Main category: cs.AI

TL;DR: 论文提出了一种计算框架BAM，结合ANN的感知边界采样和人类行为实验，研究人类决策的变异性，并通过大规模实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 理解人类在不确定性和模糊性下的决策变异性，揭示感知和决策机制。

Method: 结合ANN的感知边界采样算法生成刺激物，并通过大规模行为实验验证。

Result: 生成了variMNIST数据集，建立了预测和操纵个体差异的方法。

Conclusion: BAM框架为个性化感知分析提供了新工具，填补了计算模型与人类个体差异研究之间的空白。

Abstract: Human decision-making in cognitive tasks and daily life exhibits considerable
variability, shaped by factors such as task difficulty, individual preferences,
and personal experiences. Understanding this variability across individuals is
essential for uncovering the perceptual and decision-making mechanisms that
humans rely on when faced with uncertainty and ambiguity. We present a
computational framework BAM (Boundary Alignment & Manipulation framework) that
combines perceptual boundary sampling in ANNs and human behavioral experiments
to systematically investigate this phenomenon. Our perceptual boundary sampling
algorithm generates stimuli along ANN decision boundaries that intrinsically
induce significant perceptual variability. The efficacy of these stimuli is
empirically validated through large-scale behavioral experiments involving 246
participants across 116,715 trials, culminating in the variMNIST dataset
containing 19,943 systematically annotated images. Through personalized model
alignment and adversarial generation, we establish a reliable method for
simultaneously predicting and manipulating the divergent perceptual decisions
of pairs of participants. This work bridges the gap between computational
models and human individual difference research, providing new tools for
personalized perception analysis.

</details>


### [145] [BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems](https://arxiv.org/abs/2505.03643)
*Chelsea Sidrane,Jana Tumova*

Main category: cs.AI

TL;DR: 提出一种计算非线性离散时间神经反馈环路后向可达集的方法，用于验证学习型系统的目标可达性。


<details>
  <summary>Details</summary>
Motivation: 学习型规划与控制算法缺乏严格的性能或安全性保证，需要一种验证方法。

Method: 通过过近似系统动力学函数，利用混合整数线性规划计算后向可达集。

Result: 算法在数值示例中验证了有效性，扩展了可验证的学习型系统属性范围。

Conclusion: 该方法为学习型系统提供了更严格的验证手段。

Abstract: Learning-enabled planning and control algorithms are increasingly popular,
but they often lack rigorous guarantees of performance or safety. We introduce
an algorithm for computing underapproximate backward reachable sets of
nonlinear discrete time neural feedback loops. We then use the backward
reachable sets to check goal-reaching properties. Our algorithm is based on
overapproximating the system dynamics function to enable computation of
underapproximate backward reachable sets through solutions of mixed-integer
linear programs. We rigorously analyze the soundness of our algorithm and
demonstrate it on a numerical example. Our work expands the class of properties
that can be verified for learning-enabled systems.

</details>


### [146] [Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time](https://arxiv.org/abs/2505.03668)
*Celeste Veronese,Daniele Meli,Alessandro Farinelli*

Main category: cs.AI

TL;DR: 本文提出了一种结合时间逻辑推理和部分可观测马尔可夫决策过程（POMDPs）的方法，用于在不确定性下实现可解释的决策。通过基于事件演算的线性时序逻辑（LTL）生成持久宏动作，显著减少推理时间并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决POMDPs在不确定性决策中推理时间过长和性能不稳定的问题，同时减少对人工设计启发式规则的依赖。

Method: 利用基于事件演算的LTL片段生成持久宏动作，通过归纳逻辑编程（ILP）从少量执行轨迹中学习宏动作，结合蒙特卡洛树搜索（MCTS）优化POMDP求解。

Result: 在Pocman和Rocksample基准测试中，学习的宏动作表现出更高的表达能力和通用性，显著提升了计算效率。

Conclusion: 该方法通过结合时间逻辑和POMDPs，实现了高效且可解释的决策，减少了人工干预需求。

Abstract: This paper proposes an integration of temporal logical reasoning and
Partially Observable Markov Decision Processes (POMDPs) to achieve
interpretable decision-making under uncertainty with macro-actions. Our method
leverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus
(EC) to generate \emph{persistent} (i.e., constant) macro-actions, which guide
Monte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon,
significantly reducing inference time while ensuring robust performance. Such
macro-actions are learnt via Inductive Logic Programming (ILP) from a few
traces of execution (belief-action pairs), thus eliminating the need for
manually designed heuristics and requiring only the specification of the POMDP
transition model. In the Pocman and Rocksample benchmark scenarios, our learned
macro-actions demonstrate increased expressiveness and generality when compared
to time-independent heuristics, indeed offering substantial computational
efficiency improvements.

</details>


### [147] [Gap the (Theory of) Mind: Sharing Beliefs About Teammates' Goals Boosts Collaboration Perception, Not Performance](https://arxiv.org/abs/2505.03674)
*Yotam Amitai,Reuth Mirsky,Ofra Amir*

Main category: cs.AI

TL;DR: 研究探讨AI代理通过分享对人类队友目标的理解是否能提升任务表现和协作感知，发现目标分享虽未显著改善任务表现或满意度，但支持战略调整和协作感知。


<details>
  <summary>Details</summary>
Motivation: 在人类与AI团队中，直接沟通目标不总是可行，需通过行动推断意图，研究探索分享推断目标是否能改善协作。

Method: 通过实验比较三种条件（无识别、可行目标、按需可行目标），分析任务表现、满意度和认知负荷。

Result: 目标分享未显著提升任务表现或满意度，但支持战略调整和协作感知，且未增加认知负荷。

Conclusion: 目标分享在信任和协作感知上有益，但需平衡信息量与简洁性，可能偶尔影响客观表现。

Abstract: In human-agent teams, openly sharing goals is often assumed to enhance
planning, collaboration, and effectiveness. However, direct communication of
these goals is not always feasible, requiring teammates to infer their
partner's intentions through actions. Building on this, we investigate whether
an AI agent's ability to share its inferred understanding of a human teammate's
goals can improve task performance and perceived collaboration. Through an
experiment comparing three conditions-no recognition (NR), viable goals (VG),
and viable goals on-demand (VGod) - we find that while goal-sharing information
did not yield significant improvements in task performance or overall
satisfaction scores, thematic analysis suggests that it supported strategic
adaptations and subjective perceptions of collaboration. Cognitive load
assessments revealed no additional burden across conditions, highlighting the
challenge of balancing informativeness and simplicity in human-agent
interactions. These findings highlight the nuanced trade-off of goal-sharing:
while it fosters trust and enhances perceived collaboration, it can
occasionally hinder objective performance gains.

</details>


### [148] [Graph Drawing for LLMs: An Empirical Evaluation](https://arxiv.org/abs/2505.03678)
*Walter Didimo,Fabrizio Montecchiani,Tommaso Piselli*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLMs）在图相关任务中的表现，重点关注视觉模态（图形绘制）对性能的影响，包括布局范式、图形美观度和提示技术。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在图任务中的表现，尤其是视觉输入（图形绘制）如何影响其性能。

Method: 通过实验分析布局范式、图形美观度和提示技术对模型性能的影响。

Result: 选择合适的布局范式和优化图形可读性显著提升模型性能；提示技术的选择是关键但具挑战性。

Conclusion: 视觉输入的质量和提示技术的选择对LLMs在图任务中的表现至关重要。

Abstract: Our work contributes to the fast-growing literature on the use of Large
Language Models (LLMs) to perform graph-related tasks. In particular, we focus
on usage scenarios that rely on the visual modality, feeding the model with a
drawing of the graph under analysis. We investigate how the model's performance
is affected by the chosen layout paradigm, the aesthetics of the drawing, and
the prompting technique used for the queries. We formulate three corresponding
research questions and present the results of a thorough experimental analysis.
Our findings reveal that choosing the right layout paradigm and optimizing the
readability of the input drawing from a human perspective can significantly
improve the performance of the model on the given task. Moreover, selecting the
most effective prompting technique is a challenging yet crucial task for
achieving optimal performance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [149] [Uncertainty Quantification for Machine Learning in Healthcare: A Survey](https://arxiv.org/abs/2505.02874)
*L. Julián Lechuga López,Shaza Elsharief,Dhiyaa Al Jorf,Firas Darwish,Congbo Ma,Farah E. Shamout*

Main category: cs.LG

TL;DR: 本文综述了医疗领域中机器学习（ML）模型的不确定性量化（UQ）现状，提出了一个框架，指导如何将UQ方法整合到ML流程的各个阶段，并探讨了挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 医疗领域ML系统的可靠性和安全性需要UQ支持，但目前缺乏系统性的方法评估和实际应用。

Method: 通过全面分析当前医疗领域的UQ研究，提出一个框架，整合数据预处理、训练和评估阶段的UQ方法。

Result: 总结了医疗领域常用的UQ方法，并提出了其他领域可能适用于医疗的新方法。

Conclusion: 本文为研究者和从业者提供了UQ在医疗ML中的实施指南，旨在提升系统的可靠性和信任度。

Abstract: Uncertainty Quantification (UQ) is pivotal in enhancing the robustness,
reliability, and interpretability of Machine Learning (ML) systems for
healthcare, optimizing resources and improving patient care. Despite the
emergence of ML-based clinical decision support tools, the lack of principled
quantification of uncertainty in ML models remains a major challenge. Current
reviews have a narrow focus on analyzing the state-of-the-art UQ in specific
healthcare domains without systematically evaluating method efficacy across
different stages of model development, and despite a growing body of research,
its implementation in healthcare applications remains limited. Therefore, in
this survey, we provide a comprehensive analysis of current UQ in healthcare,
offering an informed framework that highlights how different methods can be
integrated into each stage of the ML pipeline including data processing,
training and evaluation. We also highlight the most popular methods used in
healthcare and novel approaches from other domains that hold potential for
future adoption in the medical context. We expect this study will provide a
clear overview of the challenges and opportunities of implementing UQ in the ML
pipeline for healthcare, guiding researchers and practitioners in selecting
suitable techniques to enhance the reliability, safety and trust from patients
and clinicians on ML-driven healthcare solutions.

</details>


### [150] [A Wireless Collaborated Inference Acceleration Framework for Plant Disease Recognition](https://arxiv.org/abs/2505.02877)
*Hele Zhu,Xinyi Huang,Haojia Gao,Mengfei Jiang,Haohua Que,Lei Mu*

Main category: cs.LG

TL;DR: 提出了一种基于边缘设备和云服务器协同推理的植物病害识别框架，通过深度强化学习修剪模型并采用贪心策略优化分割点，显著提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统手动识别方法效率低且成本高，深度学习虽有效但面临资源受限和通信带宽问题。

Method: 使用深度强化学习修剪DNN模型，贪心策略确定最优分割点，实现协同推理加速。

Result: 实验表明框架显著提升推理速度，同时保持可接受的识别精度。

Conclusion: 该框架为快速诊断和预防植物病害提供了新解决方案。

Abstract: Plant disease is a critical factor affecting agricultural production.
Traditional manual recognition methods face significant drawbacks, including
low accuracy, high costs, and inefficiency. Deep learning techniques have
demonstrated significant benefits in identifying plant diseases, but they still
face challenges such as inference delays and high energy consumption. Deep
learning algorithms are difficult to run on resource-limited embedded devices.
Offloading these models to cloud servers is confronted with the restriction of
communication bandwidth, and all of these factors will influence the
inference's efficiency. We propose a collaborative inference framework for
recognizing plant diseases between edge devices and cloud servers to enhance
inference speed. The DNN model for plant disease recognition is pruned through
deep reinforcement learning to improve the inference speed and reduce energy
consumption. Then the optimal split point is determined by a greedy strategy to
achieve the best collaborated inference acceleration. Finally, the system for
collaborative inference acceleration in plant disease recognition has been
implemented using Gradio to facilitate friendly human-machine interaction.
Experiments indicate that the proposed collaborative inference framework
significantly increases inference speed while maintaining acceptable
recognition accuracy, offering a novel solution for rapidly diagnosing and
preventing plant diseases.

</details>


### [151] [LLM4FTS: Enhancing Large Language Models for Financial Time Series Prediction](https://arxiv.org/abs/2505.02880)
*Zian Liu,Renjun Jia*

Main category: cs.LG

TL;DR: 论文提出LLM4FTS框架，通过可学习的分割和动态小波卷积模块增强LLM在金融时间序列建模中的能力，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列预测因信号噪声比低和时间模式复杂而具有挑战性，传统机器学习模型能力有限，LLM虽有潜力但现有方法忽略多尺度模式。

Method: 结合K-means++聚类、自适应分割和动态小波卷积模块，识别多尺度模式并增强LLM的时间序列建模能力。

Result: 在真实金融数据集上表现优异，捕捉复杂市场模式，并在股票回报预测中达到最优结果。

Conclusion: LLM4FTS框架显著提升了LLM在金融预测中的应用，具有实际部署价值。

Abstract: Predicting financial time series presents significant challenges due to
inherent low signal-to-noise ratios and intricate temporal patterns.
Traditional machine learning models exhibit limitations in this forecasting
task constrained by their restricted model capacity. Recent advances in large
language models (LLMs), with their greatly expanded parameter spaces,
demonstrate promising potential for modeling complex dependencies in temporal
sequences. However, existing LLM-based approaches typically focus on
fixed-length patch analysis due to the Transformer architecture, ignoring
market data's multi-scale pattern characteristics. In this study, we propose
$LLM4FTS$, a novel framework that enhances LLM capabilities for temporal
sequence modeling through learnable patch segmentation and dynamic wavelet
convolution modules. Specifically,we first employ K-means++ clustering based on
DTW distance to identify scale-invariant patterns in market data. Building upon
pattern recognition results, we introduce adaptive patch segmentation that
partitions temporal sequences while preserving maximal pattern integrity. To
accommodate time-varying frequency characteristics, we devise a dynamic wavelet
convolution module that emulates discrete wavelet transformation with enhanced
flexibility in capturing time-frequency features. These three modules work
together to improve large language model's ability to handle scale-invariant
patterns in financial time series. Extensive experiments on real-world
financial datasets substantiate the framework's efficacy, demonstrating
superior performance in capturing complex market patterns and achieving
state-of-the-art results in stock return prediction. The successful deployment
in practical trading systems confirms its real-world applicability,
representing a significant advancement in LLM applications for financial
forecasting.

</details>


### [152] [Rewriting Pre-Training Data Boosts LLM Performance in Math and Code](https://arxiv.org/abs/2505.02881)
*Kazuki Fujii,Yukito Tajima,Sakae Mizuki,Hinari Shimada,Taihei Shiotani,Koshiro Saito,Masanari Ohi,Masaki Kawamura,Taishi Nakamura,Takumi Okamoto,Shigeki Ishida,Kakeru Hattori,Youmi Ma,Hiroya Takamura,Rio Yokota,Naoaki Okazaki*

Main category: cs.LG

TL;DR: 论文介绍了两个公开数据集SwallowCode和SwallowMath，通过系统重写公共数据显著提升大语言模型（LLM）在程序合成和数学推理中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在程序合成和数学推理中的性能受限于预训练语料的质量，因此需要高质量数据集来提升模型能力。

Method: SwallowCode通过四阶段管道（语法验证、风格过滤、两阶段LLM重写）优化Python代码；SwallowMath通过去除冗余、恢复上下文和格式化步骤优化数学问题解答。

Result: 在固定训练预算下，使用SwallowCode和SwallowMath分别显著提升了HumanEval、GSM8K等基准测试的性能。

Conclusion: 公开数据集和流程为LLM预训练提供了可复现的研究基础，推动了专业领域的发展。

Abstract: The performance of large language models (LLMs) in program synthesis and
mathematical reasoning is fundamentally limited by the quality of their
pre-training corpora. We introduce two openly licensed datasets, released under
the Llama 3.3 Community License, that significantly enhance LLM performance by
systematically rewriting public data. SwallowCode (approximately 16.1 billion
tokens) refines Python snippets from The-Stack-v2 through a novel four-stage
pipeline: syntax validation, pylint-based style filtering, and a two-stage LLM
rewriting process that enforces style conformity and transforms snippets into
self-contained, algorithmically efficient examples. Unlike prior methods that
rely on exclusionary filtering or limited transformations, our
transform-and-retain approach upgrades low-quality code, maximizing data
utility. SwallowMath (approximately 2.3 billion tokens) enhances Finemath-4+ by
removing boilerplate, restoring context, and reformatting solutions into
concise, step-by-step explanations. Within a fixed 50 billion token training
budget, continual pre-training of Llama-3.1-8B with SwallowCode boosts pass@1
by +17.0 on HumanEval and +17.7 on HumanEval+ compared to Stack-Edu, surpassing
the baseline model's code generation capabilities. Similarly, substituting
SwallowMath yields +12.4 accuracy on GSM8K and +7.6 on MATH. Ablation studies
confirm that each pipeline stage contributes incrementally, with rewriting
delivering the largest gains. All datasets, prompts, and checkpoints are
publicly available, enabling reproducible research and advancing LLM
pre-training for specialized domains.

</details>


### [153] [Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?](https://arxiv.org/abs/2505.02884)
*Guangzhi Sun,Potsawee Manakul,Xiao Zhan,Mark Gales*

Main category: cs.LG

TL;DR: 论文提出了一种新的遗忘方法DF-MCQ，通过KL散度平坦化模型预测分布，有效移除目标知识并提高拒绝率。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法多基于混淆而非真正移除知识，导致模型易受探测攻击。

Method: 提出DF-MCQ方法，利用KL散度平坦化模型预测分布，并引入自动生成的多选题评估框架。

Result: DF-MCQ在探测问题上的拒绝率超过90%，且不确定性显著高于混淆方法。

Conclusion: DF-MCQ是一种有效的遗忘方法，优于现有混淆技术。

Abstract: Unlearning has emerged as a critical capability for large language models
(LLMs) to support data privacy, regulatory compliance, and ethical AI
deployment. Recent techniques often rely on obfuscation by injecting incorrect
or irrelevant information to suppress knowledge. Such methods effectively
constitute knowledge addition rather than true removal, often leaving models
vulnerable to probing. In this paper, we formally distinguish unlearning from
obfuscation and introduce a probing-based evaluation framework to assess
whether existing approaches genuinely remove targeted information. Moreover, we
propose DF-MCQ, a novel unlearning method that flattens the model predictive
distribution over automatically generated multiple-choice questions using
KL-divergence, effectively removing knowledge about target individuals and
triggering appropriate refusal behaviour. Experimental results demonstrate that
DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level
uncertainty that is much higher than obfuscation on probing questions.

</details>


### [154] [When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger](https://arxiv.org/abs/2505.02888)
*Rintaro Ando*

Main category: cs.LG

TL;DR: N2M-RSI是一个形式化模型，展示AI代理在反馈输出并跨越信息整合阈值后，其内部复杂性会无限增长。


<details>
  <summary>Details</summary>
Motivation: 统一自提示语言模型、哥德尔自指和AutoML等概念，探索AI自我改进的潜力。

Method: 通过递归反馈输出作为输入，并设定信息整合阈值，模型实现自我复杂性增长。

Result: 模型显示内部复杂性无限增长，且在多代理交互中可能产生超线性效应。

Conclusion: N2M-RSI为AI自我改进提供了理论框架，但出于安全考虑未公开具体实现。

Abstract: We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal
formal model showing that once an AI agent feeds its own outputs back as inputs
and crosses an explicit information-integration threshold, its internal
complexity will grow without bound under our assumptions. The framework unifies
earlier ideas on self-prompting large language models, G\"odelian
self-reference, and AutoML, yet remains implementation-agnostic. The model
furthermore scales naturally to interacting swarms of agents, hinting at
super-linear effects once communication among instances is permitted. For
safety reasons, we omit system-specific implementation details and release only
a brief, model-agnostic toy prototype in Appendix C.

</details>


### [155] [Early Prediction of Sepsis: Feature-Aligned Transfer Learning](https://arxiv.org/abs/2505.02889)
*Oyindolapo O. Komolafe,Zhimin Mei,David Morales Zarate,Gregory William Spangenberg*

Main category: cs.LG

TL;DR: 开发了一种名为FATL的机器学习方法，通过特征对齐和迁移学习，提高早期脓毒症预测的准确性和普适性。


<details>
  <summary>Details</summary>
Motivation: 脓毒症早期诊断困难，现有模型因特征不一致和人群偏差而受限。

Method: 提出FATL方法，聚焦重要且常见的特征，结合多人群模型知识。

Result: FATL提高了模型的普适性和临床相关性，适用于资源有限的医院。

Conclusion: FATL为脓毒症早期检测提供了实用且可扩展的解决方案，有望改善患者预后和医疗公平性。

Abstract: Sepsis is a life threatening medical condition that occurs when the body has
an extreme response to infection, leading to widespread inflammation, organ
failure, and potentially death. Because sepsis can worsen rapidly, early
detection is critical to saving lives. However, current diagnostic methods
often identify sepsis only after significant damage has already occurred. Our
project aims to address this challenge by developing a machine learning based
system to predict sepsis in its early stages, giving healthcare providers more
time to intervene.
  A major problem with existing models is the wide variability in the patient
information or features they use, such as heart rate, temperature, and lab
results. This inconsistency makes models difficult to compare and limits their
ability to work across different hospitals and settings. To solve this, we
propose a method called Feature Aligned Transfer Learning (FATL), which
identifies and focuses on the most important and commonly reported features
across multiple studies, ensuring the model remains consistent and clinically
relevant.
  Most existing models are trained on narrow patient groups, leading to
population bias. FATL addresses this by combining knowledge from models trained
on diverse populations, using a weighted approach that reflects each models
contribution. This makes the system more generalizable and effective across
different patient demographics and clinical environments. FATL offers a
practical and scalable solution for early sepsis detection, particularly in
hospitals with limited resources, and has the potential to improve patient
outcomes, reduce healthcare costs, and support more equitable healthcare
delivery.

</details>


### [156] [RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM Inference](https://arxiv.org/abs/2505.02922)
*Yaoqi Chen,Jinkai Zhang,Baotong Lu,Qianxi Zhang,Chengruidong Zhang,Jingjia Luo,Di Liu,Huiqiang Jiang,Qi Chen,Jing Liu,Bailu Ding,Xiao Yan,Jiawei Jiang,Chen Chen,Mingxing Zhang,Yuqing Yang,Fan Yang,Mao Yang*

Main category: cs.LG

TL;DR: RetroInfer通过重新设计KV缓存为向量存储系统，利用注意力稀疏性加速长上下文LLM推理，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决LLM长上下文推理中GPU内存和带宽限制的问题。

Method: 提出wave index（注意力感知向量索引）和wave buffer（协调KV缓存放置），结合三重注意力近似等技术。

Result: 在长上下文基准测试中，速度提升达4.5倍（GPU内存内）和10.5倍（扩展到CPU内存），同时保持准确性。

Conclusion: RetroInfer在性能和准确性上均优于现有稀疏注意力方法。

Abstract: The growing context lengths of large language models (LLMs) pose significant
challenges for efficient inference, primarily due to GPU memory and bandwidth
constraints. We present RetroInfer, a novel system that reconceptualizes the
key-value (KV) cache as a vector storage system which exploits the inherent
attention sparsity to accelerate long-context LLM inference. At its core is the
wave index, an Attention-aWare VEctor index that enables efficient and accurate
retrieval of critical tokens through techniques such as tripartite attention
approximation, accuracy-bounded attention estimation, and segmented clustering.
Complementing this is the wave buffer, which coordinates KV cache placement and
overlaps computation and data transfer across GPU and CPU to sustain high
throughput. Unlike prior sparsity-based methods that struggle with token
selection and hardware coordination, RetroInfer delivers robust performance
without compromising model accuracy. Experiments on long-context benchmarks
show up to 4.5X speedup over full attention within GPU memory limits and up to
10.5X over sparse attention baselines when KV cache is extended to CPU memory,
all while preserving full-attention-level accuracy.

</details>


### [157] [Smooth Quadratic Prediction Markets](https://arxiv.org/abs/2505.02959)
*Enrique Nueve,Bo Waggoner*

Main category: cs.LG

TL;DR: 论文提出了一种基于平滑二次预测市场的新设计，激励代理通过梯度下降实现学习算法，相比DCFMM在AD证券上具有更优的最坏情况货币损失，同时保留了多项公理保证。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以通过其他学习算法设计预测市场，改进现有DCFMM的局限性。

Method: 通过分解和修改DCFMM的定价机制，提出平滑二次预测市场，激励代理实现梯度下降。

Result: 新市场在AD证券上表现更优，保留了价格存在性、信息整合等公理保证，并分析了预算有限和仅购买证券的约束。

Conclusion: 平滑二次预测市场为未来设计提供了方向，价格更新规则可与费用结构分离，同时保留保证。

Abstract: When agents trade in a Duality-based Cost Function prediction market, they
collectively implement the learning algorithm Follow-The-Regularized-Leader. We
ask whether other learning algorithms could be used to inspire the design of
prediction markets. By decomposing and modifying the Duality-based Cost
Function Market Maker's (DCFMM) pricing mechanism, we propose a new prediction
market, called the Smooth Quadratic Prediction Market, the incentivizes agents
to collectively implement general steepest gradient descent. Relative to the
DCFMM, the Smooth Quadratic Prediction Market has a better worst-case monetary
loss for AD securities while preserving axiom guarantees such as the existence
of instantaneous price, information incorporation, expressiveness, no
arbitrage, and a form of incentive compatibility. To motivate the application
of the Smooth Quadratic Prediction Market, we independently examine agents'
trading behavior under two realistic constraints: bounded budgets and buy-only
securities. Finally, we provide an introductory analysis of an approach to
facilitate adaptive liquidity using the Smooth Quadratic AD Prediction Market.
Our results suggest future designs where the price update rule is separate from
the fee structure, yet guarantees are preserved.

</details>


### [158] [Physics-Learning AI Datamodel (PLAID) datasets: a collection of physics simulations for machine learning](https://arxiv.org/abs/2505.02974)
*Fabien Casenave,Xavier Roynard,Brian Staber,Nissrine Akkari,William Piat,Michele Alessandro Bucci,Abbas Kabalan,Xuan Minh Vuong Nguyen,Luca Saverio,Raphaël Carpintero Perez,Anthony Kalaydjian,Samy Fouché,Thierry Gonon,Ghassan Najjar,Emmanuel Menier,Matthieu Nastorg,Christian Rey*

Main category: cs.LG

TL;DR: PLAID是一个用于物理模拟数据的统一框架，解决了现有数据集分散、标准化不足的问题，并提供了基准测试工具。


<details>
  <summary>Details</summary>
Motivation: 现有物理模拟数据集缺乏规模、多样性和标准化，限制了机器学习代理模型的广泛应用。

Method: 提出了PLAID框架，定义了统一的数据标准，并提供了工具库和六个数据集，涵盖结构力学和计算流体动力学。

Result: 发布了PLAID标准和数据集，提供了基准测试工具，支持社区参与评估。

Conclusion: PLAID为物理模拟数据的标准化和共享提供了灵活框架，有望推动机器学习在科学模拟中的应用。

Abstract: Machine learning-based surrogate models have emerged as a powerful tool to
accelerate simulation-driven scientific workflows. However, their widespread
adoption is hindered by the lack of large-scale, diverse, and standardized
datasets tailored to physics-based simulations. While existing initiatives
provide valuable contributions, many are limited in scope-focusing on specific
physics domains, relying on fragmented tooling, or adhering to overly
simplistic datamodels that restrict generalization. To address these
limitations, we introduce PLAID (Physics-Learning AI Datamodel), a flexible and
extensible framework for representing and sharing datasets of physics
simulations. PLAID defines a unified standard for describing simulation data
and is accompanied by a library for creating, reading, and manipulating complex
datasets across a wide range of physical use cases (gitlab.com/drti/plaid). We
release six carefully crafted datasets under the PLAID standard, covering
structural mechanics and computational fluid dynamics, and provide baseline
benchmarks using representative learning methods. Benchmarking tools are made
available on Hugging Face, enabling direct participation by the community and
contribution to ongoing evaluation efforts (huggingface.co/PLAIDcompetitions).

</details>


### [159] [More Optimal Fractional-Order Stochastic Gradient Descent for Non-Convex Optimization Problems](https://arxiv.org/abs/2505.02985)
*Mohammad Partohaghighi,Roummel Marcia,YangQuan Chen*

Main category: cs.LG

TL;DR: 2SEDFOSGD结合2SED算法与FOSGD，通过数据驱动方式调整分数阶指数，优化收敛速度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统FOSGD的分数阶指数难以调优和稳定，限制了其应用。

Method: 提出2SEDFOSGD，动态调整分数阶指数，结合模型敏感性和有效维度。

Result: 在非凸优化问题中，2SEDFOSGD避免了传统FOSGD的缓慢或不稳定行为，收敛更快且参数估计更稳健。

Conclusion: 2SEDFOSGD展示了维度感知分数阶技术在高级建模和估计任务中的潜力。

Abstract: Fractional-order stochastic gradient descent (FOSGD) leverages fractional
exponents to capture long-memory effects in optimization. However, its utility
is often limited by the difficulty of tuning and stabilizing these exponents.
We propose 2SED Fractional-Order Stochastic Gradient Descent (2SEDFOSGD), which
integrates the Two-Scale Effective Dimension (2SED) algorithm with FOSGD to
adapt the fractional exponent in a data-driven manner. By tracking model
sensitivity and effective dimensionality, 2SEDFOSGD dynamically modulates the
exponent to mitigate oscillations and hasten convergence. Theoretically, for
onoconvex optimization problems, this approach preserves the advantages of
fractional memory without the sluggish or unstable behavior observed in na\"ive
fractional SGD. Empirical evaluations in Gaussian and $\alpha$-stable noise
scenarios using an autoregressive (AR) model highlight faster convergence and
more robust parameter estimates compared to baseline methods, underscoring the
potential of dimension-aware fractional techniques for advanced modeling and
estimation tasks.

</details>


### [160] [Radio: Rate-Distortion Optimization for Large Language Model Compression](https://arxiv.org/abs/2505.03031)
*Sean I. Young*

Main category: cs.LG

TL;DR: 论文提出了一种基于率失真理论的量化技术，用于压缩大型语言模型（LLMs），以降低计算成本并适应资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 压缩LLMs以降低部署成本、减少计算资源需求及环境影响。

Method: 从率失真理论出发，提出一种基于率失真优化的量化技术，支持训练后压缩至用户指定的大小或精度。

Result: 技术可扩展至包含数千亿参数的模型，提供灵活的压缩选项。

Conclusion: 该量化技术为LLMs的高效部署提供了实用解决方案。

Abstract: In recent years, the compression of large language models (LLMs) has emerged
as a key problem in facilitating LLM deployment on resource-limited devices,
reducing compute costs, and mitigating the environmental footprint due to
large-scale AI infrastructure. Here, we establish the foundations of LLM
quantization from a rate-distortion theory perspective and propose a
quantization technique based on simple rate-distortion optimization. Our
technique scales to models containing hundreds of billions of weight parameters
and offers users the flexibility to compress models, post-training, to a model
size or accuracy specified by the user.

</details>


### [161] [A New Perspective To Understanding Multi-resolution Hash Encoding For Neural Fields](https://arxiv.org/abs/2505.03042)
*Steven Tin Sui Luo*

Main category: cs.LG

TL;DR: Instant-NGP的多分辨率哈希网格结构显著提升了神经网络的性能，但其原理尚不明确。本文提出“域操作”视角，解释哈希网格如何通过线性段倍增增强表达能力。


<details>
  <summary>Details</summary>
Motivation: 缺乏对哈希网格原理的理解导致其超参数只能凭经验调整，本文旨在提供直观解释。

Method: 提出“域操作”视角，并通过一维信号实验验证其有效性。

Result: 实验支持哈希网格通过线性段倍增提升表达能力，且该思想可推广至高维。

Conclusion: “域操作”视角为哈希网格的工作原理提供了新解释，有助于未来优化设计。

Abstract: Instant-NGP has been the state-of-the-art architecture of neural fields in
recent years. Its incredible signal-fitting capabilities are generally
attributed to its multi-resolution hash grid structure and have been used and
improved in numerous following works. However, it is unclear how and why such a
hash grid structure improves the capabilities of a neural network by such great
margins. A lack of principled understanding of the hash grid also implies that
the large set of hyperparameters accompanying Instant-NGP could only be tuned
empirically without much heuristics. To provide an intuitive explanation of the
working principle of the hash grid, we propose a novel perspective, namely
domain manipulation. This perspective provides a ground-up explanation of how
the feature grid learns the target signal and increases the expressivity of the
neural field by artificially creating multiples of pre-existing linear
segments. We conducted numerous experiments on carefully constructed
1-dimensional signals to support our claims empirically and aid our
illustrations. While our analysis mainly focuses on 1-dimensional signals, we
show that the idea is generalizable to higher dimensions.

</details>


### [162] [34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery](https://arxiv.org/abs/2505.03049)
*Yoel Zimmermann,Adib Bazgir,Alexander Al-Feghali,Mehrad Ansari,L. Catherine Brinson,Yuan Chiang,Defne Circi,Min-Hsueh Chiu,Nathan Daelman,Matthew L. Evans,Abhijeet S. Gangan,Janine George,Hassan Harb,Ghazal Khalighinejad,Sartaaj Takrim Khan,Sascha Klawohn,Magdalena Lederbauer,Soroush Mahjoubi,Bernadette Mohr,Seyed Mohamad Moosavi,Aakash Naik,Aleyna Beste Ozhan,Dieter Plessers,Aritra Roy,Fabian Schöppach,Philippe Schwaller,Carla Terboven,Katharina Ueltzen,Shang Zhu,Jan Janssen,Calvin Li,Ian Foster,Ben Blaiszik*

Main category: cs.LG

TL;DR: LLMs在材料科学与化学研究中展现出多领域应用潜力，涵盖预测、设计、自动化等，但需进一步解决可靠性等问题。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在材料科学与化学研究全生命周期中的应用潜力，展示其多功能性。

Method: 通过34个项目回顾LLMs在7个关键研究领域的应用，包括预测、设计、自动化等。

Result: LLMs作为预测模型和工具开发平台表现优异，尤其在低数据环境和跨学科研究中。

Conclusion: LLMs的持续改进为科研工作流带来新机遇与挑战，需进一步研究可靠性等问题。

Abstract: Large Language Models (LLMs) are reshaping many aspects of materials science
and chemistry research, enabling advances in molecular property prediction,
materials design, scientific automation, knowledge extraction, and more. Recent
developments demonstrate that the latest class of models are able to integrate
structured and unstructured data, assist in hypothesis generation, and
streamline research workflows. To explore the frontier of LLM capabilities
across the research lifecycle, we review applications of LLMs through 34 total
projects developed during the second annual Large Language Model Hackathon for
Applications in Materials Science and Chemistry, a global hybrid event. These
projects spanned seven key research areas: (1) molecular and material property
prediction, (2) molecular and material design, (3) automation and novel
interfaces, (4) scientific communication and education, (5) research data
management and automation, (6) hypothesis generation and evaluation, and (7)
knowledge extraction and reasoning from the scientific literature.
Collectively, these applications illustrate how LLMs serve as versatile
predictive models, platforms for rapid prototyping of domain-specific tools,
and much more. In particular, improvements in both open source and proprietary
LLM performance through the addition of reasoning, additional training data,
and new techniques have expanded effectiveness, particularly in low-data
environments and interdisciplinary research. As LLMs continue to improve, their
integration into scientific workflows presents both new opportunities and new
challenges, requiring ongoing exploration, continued refinement, and further
research to address reliability, interpretability, and reproducibility.

</details>


### [163] [Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/abs/2505.03084)
*Shashank Kapoor,Sanjay Surendranath Girija,Lakshit Arora,Dipen Pradhan,Ankit Shetgaonkar,Aman Raj*

Main category: cs.LG

TL;DR: 该论文综述了多模态模型中的对抗攻击，填补了实践中攻击类型概述的空白。


<details>
  <summary>Details</summary>
Motivation: 多模态模型虽然推动了AI发展，但也继承了各模态的脆弱性，对抗威胁加剧。目前缺乏针对实践者的攻击类型概述。

Method: 通过调查针对文本、图像、视频和音频的对抗攻击，分析多模态对抗威胁的演变。

Result: 提供了多模态对抗攻击的全面总结，首次系统化威胁图谱。

Conclusion: 论文填补了多模态对抗攻击研究的空白，为实践者提供了防御依据。

Abstract: The introduction of multimodal models is a huge step forward in Artificial
Intelligence. A single model is trained to understand multiple modalities:
text, image, video, and audio. Open-source multimodal models have made these
breakthroughs more accessible. However, considering the vast landscape of
adversarial attacks across these modalities, these models also inherit
vulnerabilities of all the modalities, and ultimately, the adversarial threat
amplifies. While broad research is available on possible attacks within or
across these modalities, a practitioner-focused view that outlines attack types
remains absent in the multimodal world. As more Machine Learning Practitioners
adopt, fine-tune, and deploy open-source models in real-world applications,
it's crucial that they can view the threat landscape and take the preventive
actions necessary. This paper addresses the gap by surveying adversarial
attacks targeting all four modalities: text, image, video, and audio. This
survey provides a view of the adversarial attack landscape and presents how
multimodal adversarial threats have evolved. To the best of our knowledge, this
survey is the first comprehensive summarization of the threat landscape in the
multimodal world.

</details>


### [164] [Deep Learning in Renewable Energy Forecasting: A Cross-Dataset Evaluation of Temporal and Spatial Models](https://arxiv.org/abs/2505.03109)
*Lutfu Sua,Haibo Wang,Jun Huang*

Main category: cs.LG

TL;DR: 论文探讨了深度学习（DL）模型在可再生能源领域的应用，比较了多种DL方法，并分析了影响其准确性的因素。LSTM和MLP模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 可再生能源数据的非线性关系需要比传统机器学习更强大的方法，DL模型能有效捕捉复杂变量关系。

Method: 研究比较了七种DL方法（如LSTM、CNN等），并评估了采样、超参数优化等因素。使用了两个数据集（天气/发电数据和光伏数据）。

Result: LSTM和MLP模型表现最优，验证数据的均方根误差极低。

Conclusion: DL模型在可再生能源领域具有潜力，LSTM和MLP尤其适合处理此类复杂数据。

Abstract: Unpredictability of renewable energy sources coupled with the complexity of
those methods used for various purposes in this area calls for the development
of robust methods such as DL models within the renewable energy domain. Given
the nonlinear relationships among variables in renewable energy datasets, DL
models are preferred over traditional machine learning (ML) models because they
can effectively capture and model complex interactions between variables. This
research aims to identify the factors responsible for the accuracy of DL
techniques, such as sampling, stationarity, linearity, and hyperparameter
optimization for different algorithms. The proposed DL framework compares
various methods and alternative training/test ratios. Seven ML methods, such as
Long-Short Term Memory (LSTM), Stacked LSTM, Convolutional Neural Network
(CNN), CNN-LSTM, Deep Neural Network (DNN), Multilayer Perceptron (MLP), and
Encoder-Decoder (ED), were evaluated on two different datasets. The first
dataset contains the weather and power generation data. It encompasses two
distinct datasets, hourly energy demand data and hourly weather data in Spain,
while the second dataset includes power output generated by the photovoltaic
panels at 12 locations. This study deploys regularization approaches, including
early stopping, neuron dropping, and L2 regularization, to reduce the
overfitting problem associated with DL models. The LSTM and MLP models show
superior performance. Their validation data exhibit exceptionally low root mean
square error values.

</details>


### [165] [Plug-and-Play AMC: Context Is King in Training-Free, Open-Set Modulation with LLMs](https://arxiv.org/abs/2505.03112)
*Mohammad Rostami,Atik Faysal,Reihaneh Gh. Roshan,Huaxia Wang,Nikhil Muralidhar,Yu-Dong Yao*

Main category: cs.LG

TL;DR: 提出了一种结合传统信号处理与大型语言模型（LLM）的创新框架，用于自动调制分类（AMC），无需额外训练或预处理即可实现高效分类。


<details>
  <summary>Details</summary>
Motivation: AMC在复杂信号干扰和噪声环境下仍具挑战性，需开发更高效、稳健的分类方法。

Method: 利用高阶统计和累积量估计将信号特征转化为自然语言提示，结合LLM实现一次性分类。

Result: 在合成数据集上验证了框架的竞争力，适用于多种调制方案和信噪比条件。

Conclusion: 为下一代无线网络中可扩展、可解释的信号分类系统奠定了基础。

Abstract: Automatic Modulation Classification (AMC) is critical for efficient spectrum
management and robust wireless communications. However, AMC remains challenging
due to the complex interplay of signal interference and noise. In this work, we
propose an innovative framework that integrates traditional signal processing
techniques with Large-Language Models (LLMs) to address AMC. Our approach
leverages higher-order statistics and cumulant estimation to convert
quantitative signal features into structured natural language prompts. By
incorporating exemplar contexts into these prompts, our method exploits the
LLM's inherent familiarity with classical signal processing, enabling effective
one-shot classification without additional training or preprocessing (e.g.,
denoising). Experimental evaluations on synthetically generated datasets,
spanning both noiseless and noisy conditions, demonstrate that our framework
achieves competitive performance across diverse modulation schemes and
Signal-to-Noise Ratios (SNRs). Moreover, our approach paves the way for robust
foundation models in wireless communications across varying channel conditions,
significantly reducing the expense associated with developing channel-specific
models. This work lays the foundation for scalable, interpretable, and
versatile signal classification systems in next-generation wireless networks.
The source code is available at https://github.com/RU-SIT/context-is-king

</details>


### [166] [Adaptive Thresholding for Multi-Label Classification via Global-Local Signal Fusion](https://arxiv.org/abs/2505.03118)
*Dmytro Shamatrin*

Main category: cs.LG

TL;DR: 提出了一种自适应阈值机制，结合全局和局部信号，用于多标签分类，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在多标签分类中忽略了上下文和全局稀有性，导致性能受限。

Method: 引入自适应阈值机制，融合全局（IDF）和局部（KNN）信号，作为可微损失惩罚。

Result: 在AmazonCat-13K基准测试中，宏F1达到0.1712，优于树和预训练Transformer方法。

Conclusion: 该方法轻量、可解释且模块化，代码已开源。

Abstract: Multi-label classification (MLC) requires predicting multiple labels per
sample, often under heavy class imbalance and noisy conditions. Traditional
approaches apply fixed thresholds or treat labels independently, overlooking
context and global rarity. We introduce an adaptive thresholding mechanism that
fuses global (IDF-based) and local (KNN-based) signals to produce per-label,
per-instance thresholds. Instead of applying these as hard cutoffs, we treat
them as differentiable penalties in the loss, providing smooth supervision and
better calibration. Our architecture is lightweight, interpretable, and highly
modular. On the AmazonCat-13K benchmark, it achieves a macro-F1 of 0.1712,
substantially outperforming tree-based and pretrained transformer-based
methods. We release full code for reproducibility and future extensions.

</details>


### [167] [Rethinking the Global Convergence of Softmax Policy Gradient with Linear Function Approximation](https://arxiv.org/abs/2505.03155)
*Max Qiushi Lin,Jincheng Mei,Matin Aghaei,Michael Lu,Bo Dai,Alekh Agarwal,Dale Schuurmans,Csaba Szepesvari,Sharan Vaswani*

Main category: cs.LG

TL;DR: 本文研究了线性函数近似的Softmax策略梯度方法（Lin-SPG），发现近似误差不影响其全局收敛性，并确定了特征表示的必要和充分条件，证明了其收敛速度。


<details>
  <summary>Details</summary>
Motivation: 研究策略梯度方法在大状态-动作空间中的全局收敛性，特别是线性函数近似下的Softmax策略梯度方法。

Method: 聚焦于Lin-SPG方法，分析其近似误差与全局收敛的关系，并确定特征表示的条件。

Result: 证明了Lin-SPG在特定学习率下具有O(1/T)的收敛速度，且任意常数学习率下也能全局收敛。

Conclusion: Lin-SPG的全局收敛性不受近似误差影响，且特征表示的条件是关键。

Abstract: Policy gradient (PG) methods have played an essential role in the empirical
successes of reinforcement learning. In order to handle large state-action
spaces, PG methods are typically used with function approximation. In this
setting, the approximation error in modeling problem-dependent quantities is a
key notion for characterizing the global convergence of PG methods. We focus on
Softmax PG with linear function approximation (referred to as
$\texttt{Lin-SPG}$) and demonstrate that the approximation error is irrelevant
to the algorithm's global convergence even for the stochastic bandit setting.
Consequently, we first identify the necessary and sufficient conditions on the
feature representation that can guarantee the asymptotic global convergence of
$\texttt{Lin-SPG}$. Under these feature conditions, we prove that $T$
iterations of $\texttt{Lin-SPG}$ with a problem-specific learning rate result
in an $O(1/T)$ convergence to the optimal policy. Furthermore, we prove that
$\texttt{Lin-SPG}$ with any arbitrary constant learning rate can ensure
asymptotic global convergence to the optimal policy.

</details>


### [168] [Improving the Reproducibility of Deep Learning Software: An Initial Investigation through a Case Study Analysis](https://arxiv.org/abs/2505.03165)
*Nikita Ravi,Abhinav Goel,James C. Davis,George K. Thiruvathukal*

Main category: cs.LG

TL;DR: 本文探讨了深度学习模型的可重复性问题，提出了系统性指南以提高可重复性，并通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 深度学习领域快速发展，但实验结果的可重复性成为重大挑战，影响研究的可靠性和实际应用。

Method: 提出系统性指南，包括复制原始软件环境、端到端训练测试算法、公开架构设计、增强数据处理和训练管道的透明度，并进行敏感性分析。

Result: 通过案例研究验证了指南的有效性，展示了提高深度学习模型可重复性的具体模式和反模式。

Conclusion: 实施这些策略有助于弥合研究与实际应用之间的差距，确保深度学习创新能够被有效复制和部署。

Abstract: The field of deep learning has witnessed significant breakthroughs, spanning
various applications, and fundamentally transforming current software
capabilities. However, alongside these advancements, there have been increasing
concerns about reproducing the results of these deep learning methods. This is
significant because reproducibility is the foundation of reliability and
validity in software development, particularly in the rapidly evolving domain
of deep learning. The difficulty of reproducibility may arise due to several
reasons, including having differences from the original execution environment,
incompatible software libraries, proprietary data and source code, lack of
transparency, and the stochastic nature in some software. A study conducted by
the Nature journal reveals that more than 70% of researchers failed to
reproduce other researchers experiments and over 50% failed to reproduce their
own experiments. Irreproducibility of deep learning poses significant
challenges for researchers and practitioners. To address these concerns, this
paper presents a systematic approach at analyzing and improving the
reproducibility of deep learning models by demonstrating these guidelines using
a case study. We illustrate the patterns and anti-patterns involved with these
guidelines for improving the reproducibility of deep learning models. These
guidelines encompass establishing a methodology to replicate the original
software environment, implementing end-to-end training and testing algorithms,
disclosing architectural designs, and enhancing transparency in data processing
and training pipelines. We also conduct a sensitivity analysis to understand
the model performance across diverse conditions. By implementing these
strategies, we aim to bridge the gap between research and practice, so that
innovations in deep learning can be effectively reproduced and deployed within
software.

</details>


### [169] [Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2505.03172)
*Caleb Chuck,Fan Feng,Carl Qi,Chang Shi,Siddhant Agarwal,Amy Zhang,Scott Niekum*

Main category: cs.LG

TL;DR: HInt结合交互与后见之明重标记，提升目标导向强化学习在物体中心领域的样本效率。


<details>
  <summary>Details</summary>
Motivation: 后见之明重标记在物体中心领域表现不佳，因无效轨迹干扰学习。

Method: 提出HInt，基于空反事实定义交互，并通过NCII推断交互。

Result: NCII显著提升交互推断准确性，HInt样本效率提升4倍。

Conclusion: HInt通过交互改进后见之明重标记，有效解决物体中心领域问题。

Abstract: Hindsight relabeling is a powerful tool for overcoming sparsity in
goal-conditioned reinforcement learning (GCRL), especially in certain domains
such as navigation and locomotion. However, hindsight relabeling can struggle
in object-centric domains. For example, suppose that the goal space consists of
a robotic arm pushing a particular target block to a goal location. In this
case, hindsight relabeling will give high rewards to any trajectory that does
not interact with the block. However, these behaviors are only useful when the
object is already at the goal -- an extremely rare case in practice. A dataset
dominated by these kinds of trajectories can complicate learning and lead to
failures. In object-centric domains, one key intuition is that meaningful
trajectories are often characterized by object-object interactions such as
pushing the block with the gripper. To leverage this intuition, we introduce
Hindsight Relabeling using Interactions (HInt), which combines interactions
with hindsight relabeling to improve the sample efficiency of downstream RL.
However because interactions do not have a consensus statistical definition
tractable for downstream GCRL, we propose a definition of interactions based on
the concept of null counterfactual: a cause object is interacting with a target
object if, in a world where the cause object did not exist, the target object
would have different transition dynamics. We leverage this definition to infer
interactions in Null Counterfactual Interaction Inference (NCII), which uses a
"nulling'' operation with a learned model to infer interactions. NCII is able
to achieve significantly improved interaction inference accuracy in both simple
linear dynamics domains and dynamic robotic domains in Robosuite, Robot Air
Hockey, and Franka Kitchen and HInt improves sample efficiency by up to 4x.

</details>


### [170] [RADE: Learning Risk-Adjustable Driving Environment via Multi-Agent Conditional Diffusion](https://arxiv.org/abs/2505.03178)
*Jiawei Wang,Xintao Yan,Yao Mu,Haowei Sun,Zhong Cao,Henry X. Liu*

Main category: cs.LG

TL;DR: RADE是一个基于多智能体扩散架构的仿真框架，用于生成统计真实且风险可调的交通场景，直接学习风险条件行为，保留自然的多智能体交互。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过设计复杂目标操纵单一车辆轨迹，牺牲了真实性和可扩展性，RADE旨在解决这一问题。

Method: RADE采用多智能体扩散架构，联合建模所有智能体行为，并通过代用风险度量调节轨迹，同时引入动态检查模块确保物理合理性。

Result: 在真实数据集上验证，RADE在不同风险水平下保持统计真实性，并能自然增加安全关键事件的发生概率。

Conclusion: RADE是一种可扩展且真实的工具，适用于自动驾驶安全评估。

Abstract: Generating safety-critical scenarios in high-fidelity simulations offers a
promising and cost-effective approach for efficient testing of autonomous
vehicles. Existing methods typically rely on manipulating a single vehicle's
trajectory through sophisticated designed objectives to induce adversarial
interactions, often at the cost of realism and scalability. In this work, we
propose the Risk-Adjustable Driving Environment (RADE), a simulation framework
that generates statistically realistic and risk-adjustable traffic scenes.
Built upon a multi-agent diffusion architecture, RADE jointly models the
behavior of all agents in the environment and conditions their trajectories on
a surrogate risk measure. Unlike traditional adversarial methods, RADE learns
risk-conditioned behaviors directly from data, preserving naturalistic
multi-agent interactions with controllable risk levels. To ensure physical
plausibility, we incorporate a tokenized dynamics check module that efficiently
filters generated trajectories using a motion vocabulary. We validate RADE on
the real-world rounD dataset, demonstrating that it preserves statistical
realism across varying risk levels and naturally increases the likelihood of
safety-critical events as the desired risk level grows up. Our results
highlight RADE's potential as a scalable and realistic tool for AV safety
evaluation.

</details>


### [171] [VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making](https://arxiv.org/abs/2505.03181)
*Jake Grigsby,Yuke Zhu,Michael Ryoo,Juan Carlos Niebles*

Main category: cs.LG

TL;DR: 论文探讨了如何通过离线到在线强化学习（RL）提升视觉语言模型（VLMs）在代理任务中的表现，弥补其与大型语言模型（LLMs）的差距。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs在多模态代理任务中表现不如LLMs，特别是在严格输出语法要求和开放问答任务上。

Method: 采用离线到在线强化学习方法，结合监督微调（SFT）和自学习机制，优化VLMs。

Result: 实验展示了该方法在三个多模态代理领域中的有效性。

Conclusion: 强化学习可以显著提升VLMs在代理任务中的能力，同时保持SFT的稳定性和简单性。

Abstract: Recent research looks to harness the general knowledge and reasoning of large
language models (LLMs) into agents that accomplish user-specified goals in
interactive environments. Vision-language models (VLMs) extend LLMs to
multi-modal data and provide agents with the visual reasoning necessary for new
applications in areas such as computer automation. However, agent tasks
emphasize skills where accessible open-weight VLMs lag behind their LLM
equivalents. For example, VLMs are less capable of following an environment's
strict output syntax requirements and are more focused on open-ended question
answering. Overcoming these limitations requires supervised fine-tuning (SFT)
on task-specific expert demonstrations. Our work approaches these challenges
from an offline-to-online reinforcement learning (RL) perspective. RL lets us
fine-tune VLMs to agent tasks while learning from the unsuccessful decisions of
our own model or more capable (larger) models. We explore an off-policy RL
solution that retains the stability and simplicity of the widely used SFT
workflow while allowing our agent to self-improve and learn from low-quality
datasets. We demonstrate this technique with two open-weight VLMs across three
multi-modal agent domains.

</details>


### [172] [Convergence Of Consistency Model With Multistep Sampling Under General Data Assumptions](https://arxiv.org/abs/2505.03194)
*Yiding Chen,Yiyi Zhang,Owen Oertell,Wen Sun*

Main category: cs.LG

TL;DR: 一致性模型通过学习一致性函数直接从噪声映射到数据，实现快速一步生成和多步采样提升质量。本文分析其在自一致性近似成立时的收敛性，证明在温和数据假设下，生成样本接近目标分布。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在数据生成任务中表现出色，但迭代采样计算成本高。一致性模型旨在解决这一问题，通过直接映射实现高效生成。

Method: 研究一致性模型在自一致性近似成立时的收敛性，分析不同数据分布假设下的生成样本质量。

Result: 在目标分布有界或尾部快速衰减时，生成样本在Wasserstein距离下接近目标分布；在平滑假设下，通过额外扰动步骤，样本在总变差距离下接近目标分布。

Conclusion: 一致性模型在多种数据分布下均能有效生成高质量样本，多步采样进一步提升了生成质量。

Abstract: Diffusion models accomplish remarkable success in data generation tasks
across various domains. However, the iterative sampling process is
computationally expensive. Consistency models are proposed to learn consistency
functions to map from noise to data directly, which allows one-step fast data
generation and multistep sampling to improve sample quality. In this paper, we
study the convergence of consistency models when the self-consistency property
holds approximately under the training distribution. Our analysis requires only
mild data assumption and applies to a family of forward processes. When the
target data distribution has bounded support or has tails that decay
sufficiently fast, we show that the samples generated by the consistency model
are close to the target distribution in Wasserstein distance; when the target
distribution satisfies some smoothness assumption, we show that with an
additional perturbation step for smoothing, the generated samples are close to
the target distribution in total variation distance. We provide two case
studies with commonly chosen forward processes to demonstrate the benefit of
multistep sampling.

</details>


### [173] [Transformers for Learning on Noisy and Task-Level Manifolds: Approximation and Generalization Insights](https://arxiv.org/abs/2505.03205)
*Zhaiming Shen,Alex Havrilla,Rongjie Lai,Alexander Cloninger,Wenjing Liao*

Main category: cs.LG

TL;DR: 本文为Transformer在噪声输入数据上的回归任务建立了理论基础，证明了其性能依赖于数据的本征维度。


<details>
  <summary>Details</summary>
Motivation: 现实数据和学习任务通常具有低维结构，但Transformer的理论理解尚不充分，本文旨在填补这一空白。

Method: 分析Transformer在流形上噪声输入数据的回归任务中的表现，提出新的证明技术。

Result: Transformer能够利用低复杂度结构学习任务，即使输入数据受高维噪声干扰。

Conclusion: Transformer在噪声数据上的性能依赖于流形的本征维度，新证明技术展示了其算术表示能力。

Abstract: Transformers serve as the foundational architecture for large language and
video generation models, such as GPT, BERT, SORA and their successors.
Empirical studies have demonstrated that real-world data and learning tasks
exhibit low-dimensional structures, along with some noise or measurement error.
The performance of transformers tends to depend on the intrinsic dimension of
the data/tasks, though theoretical understandings remain largely unexplored for
transformers. This work establishes a theoretical foundation by analyzing the
performance of transformers for regression tasks involving noisy input data on
a manifold. Specifically, the input data are in a tubular neighborhood of a
manifold, while the ground truth function depends on the projection of the
noisy data onto the manifold. We prove approximation and generalization errors
which crucially depend on the intrinsic dimension of the manifold. Our results
demonstrate that transformers can leverage low-complexity structures in
learning task even when the input data are perturbed by high-dimensional noise.
Our novel proof technique constructs representations of basic arithmetic
operations by transformers, which may hold independent interest.

</details>


### [174] [Partial Label Clustering](https://arxiv.org/abs/2505.03207)
*Yutong Xie,Fuchao Yang,Yuheng Jia*

Main category: cs.LG

TL;DR: 本文首次研究了部分标签聚类问题，利用有限的候选标签提升聚类性能。方法包括构建权重矩阵、标签消歧、约束传播，并通过联合模型实现相互增强。实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 部分标签学习（PLL）是一种弱监督学习框架，但现有研究未充分利用部分标签提升聚类性能。本文旨在填补这一空白。

Method: 1. 基于特征空间关系构建权重矩阵并消歧候选标签；2. 根据消歧结果构建必须链接和不能链接约束；3. 通过对抗性先验双图学习传播约束；4. 将上述步骤整合为联合模型。

Result: 实验证明该方法优于现有约束聚类方法，且在有限标注样本下优于PLL和半监督PLL方法。

Conclusion: 本文提出的联合模型有效利用部分标签提升聚类性能，理论证明消歧标签矩阵对聚类有积极作用。

Abstract: Partial label learning (PLL) is a significant weakly supervised learning
framework, where each training example corresponds to a set of candidate labels
and only one label is the ground-truth label. For the first time, this paper
investigates the partial label clustering problem, which takes advantage of the
limited available partial labels to improve the clustering performance.
Specifically, we first construct a weight matrix of examples based on their
relationships in the feature space and disambiguate the candidate labels to
estimate the ground-truth label based on the weight matrix. Then, we construct
a set of must-link and cannot-link constraints based on the disambiguation
results. Moreover, we propagate the initial must-link and cannot-link
constraints based on an adversarial prior promoted dual-graph learning
approach. Finally, we integrate weight matrix construction, label
disambiguation, and pairwise constraints propagation into a joint model to
achieve mutual enhancement. We also theoretically prove that a better
disambiguated label matrix can help improve clustering performance.
Comprehensive experiments demonstrate our method realizes superior performance
when comparing with state-of-the-art constrained clustering methods, and
outperforms PLL and semi-supervised PLL methods when only limited samples are
annotated. The code is publicly available at https://github.com/xyt-ml/PLC.

</details>


### [175] [DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning](https://arxiv.org/abs/2505.03209)
*Borui Wang,Kathleen McKeown,Rex Ying*

Main category: cs.LG

TL;DR: DYSTIL是一种结合大型语言模型（LLMs）的策略强化学习框架，通过动态生成文本策略提升泛化能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于专家演示的强化学习方法泛化能力差、样本效率低且模型可解释性不足。

Method: DYSTIL动态查询LLM生成策略，并通过策略优化逐步内化到RL代理中。

Result: 在Minigrid和BabyAI环境中，DYSTIL平均成功率比基线方法高17.75%，且样本效率更高。

Conclusion: DYSTIL显著提升了强化学习的性能、泛化能力和可解释性。

Abstract: Reinforcement learning from expert demonstrations has long remained a
challenging research problem, and existing state-of-the-art methods using
behavioral cloning plus further RL training often suffer from poor
generalization, low sample efficiency, and poor model interpretability.
Inspired by the strong reasoning abilities of large language models (LLMs), we
propose a novel strategy-based reinforcement learning framework integrated with
LLMs called DYnamic STrategy Induction with Llms for reinforcement learning
(DYSTIL) to overcome these limitations. DYSTIL dynamically queries a
strategy-generating LLM to induce textual strategies based on advantage
estimations and expert demonstrations, and gradually internalizes induced
strategies into the RL agent through policy optimization to improve its
performance through boosting policy generalization and enhancing sample
efficiency. It also provides a direct textual channel to observe and interpret
the evolution of the policy's underlying strategies during training. We test
DYSTIL over challenging RL environments from Minigrid and BabyAI, and
empirically demonstrate that DYSTIL significantly outperforms state-of-the-art
baseline methods by 17.75% in average success rate while also enjoying higher
sample efficiency during the learning process.

</details>


### [176] [Joint Resource Management for Energy-efficient UAV-assisted SWIPT-MEC: A Deep Reinforcement Learning Approach](https://arxiv.org/abs/2505.03230)
*Yue Chen,Hui Kang,Jiahui Li,Geng Su,Boxiong Wang,Jiacheng Wang,Cong Liang,Shuang Liang,Dusit Niyato*

Main category: cs.LG

TL;DR: 论文提出了一种基于无人机和定向天线的移动边缘计算系统，用于6G物联网中的无线信息和能量传输，通过改进的SAC算法优化能量效率和终端电池可持续性。


<details>
  <summary>Details</summary>
Motivation: 解决6G物联网在无地面基础设施的偏远和灾难场景中，同时实现无线信息和能量传输的挑战。

Method: 提出无人机辅助MEC系统，结合定向天线和双目标优化问题，采用改进的SAC算法解决非凸问题。

Result: 仿真显示该方法优于基线，实现高效能量管理和高计算性能，并在复杂环境中表现优异。

Conclusion: 改进的SAC算法和系统设计在6G物联网中有效平衡能量和计算需求，具有强泛化能力。

Abstract: The integration of simultaneous wireless information and power transfer
(SWIPT) technology in 6G Internet of Things (IoT) networks faces significant
challenges in remote areas and disaster scenarios where ground infrastructure
is unavailable. This paper proposes a novel unmanned aerial vehicle
(UAV)-assisted mobile edge computing (MEC) system enhanced by directional
antennas to provide both computational resources and energy support for ground
IoT terminals. However, such systems require multiple trade-off policies to
balance UAV energy consumption, terminal battery levels, and computational
resource allocation under various constraints, including limited UAV battery
capacity, non-linear energy harvesting characteristics, and dynamic task
arrivals. To address these challenges comprehensively, we formulate a
bi-objective optimization problem that simultaneously considers system energy
efficiency and terminal battery sustainability. We then reformulate this
non-convex problem with a hybrid solution space as a Markov decision process
(MDP) and propose an improved soft actor-critic (SAC) algorithm with an action
simplification mechanism to enhance its convergence and generalization
capabilities. Simulation results have demonstrated that our proposed approach
outperforms various baselines in different scenarios, achieving efficient
energy management while maintaining high computational performance.
Furthermore, our method shows strong generalization ability across different
scenarios, particularly in complex environments, validating the effectiveness
of our designed boundary penalty and charging reward mechanisms.

</details>


### [177] [MDPs with a State Sensing Cost](https://arxiv.org/abs/2505.03280)
*Vansh Kapoor,Jayakrishnan Nair*

Main category: cs.LG

TL;DR: 论文研究了在状态感知有成本的情况下，如何优化决策以平衡感知成本和行动价值，提出了一个扩展的MDP模型，并设计了高效启发式算法。


<details>
  <summary>Details</summary>
Motivation: 解决在状态感知成本存在时，如何优化决策以平衡感知成本和行动价值的问题。

Method: 将问题建模为扩展的MDP，限制连续非感知动作数量，并设计基于策略改进的启发式算法。

Result: 证明了限制策略的次优性界限，启发式算法在实践中接近最优。

Conclusion: 提出的方法在计算效率和性能上优于现有技术，适用于实际决策问题。

Abstract: In many practical sequential decision-making problems, tracking the state of
the environment incurs a sensing/communication/computation cost. In these
settings, the agent's interaction with its environment includes the additional
component of deciding $\textit{when}$ to sense the state, in a manner that
balances the value associated with optimal (state-specific) actions and the
cost of sensing. We formulate this as an expected discounted cost Markov
Decision Process (MDP), wherein the agent incurs an additional cost for sensing
its next state, but has the option to take actions while remaining 'blind' to
the system state.
  We pose this problem as a classical discounted cost MDP with an expanded
(countably infinite) state space. While computing the optimal policy for this
MDP is intractable in general, we bound the sub-optimality gap associated with
optimal policies in a restricted class, where the number of consecutive
non-sensing (a.k.a., blind) actions is capped. We also design a computationally
efficient heuristic algorithm based on policy improvement, which in practice
performs close to the optimal policy. Finally, we benchmark against the state
of the art via a numerical case study.

</details>


### [178] [Physics-inspired Energy Transition Neural Network for Sequence Learning](https://arxiv.org/abs/2505.03281)
*Zhou Wu,Junyi An,Baile Xu,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种基于物理能量转换模型的循环神经网络PETNN，其在长序列任务中优于Transformer，且复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 重新评估纯RNN的能力，探索其在长序列建模中的潜力，以挑战Transformer的主导地位。

Method: 受物理能量转换模型启发，设计了一种新的循环结构PETNN，其记忆机制能有效存储长依赖信息。

Result: 实验表明PETNN在多种序列任务中优于Transformer方法，且复杂度显著更低。

Conclusion: PETNN为循环神经网络提供了新的基础架构，展示了在Transformer主导领域开发高效RNN的潜力。

Abstract: Recently, the superior performance of Transformers has made them a more
robust and scalable solution for sequence modeling than traditional recurrent
neural networks (RNNs). However, the effectiveness of Transformer in capturing
long-term dependencies is primarily attributed to their comprehensive
pair-modeling process rather than inherent inductive biases toward sequence
semantics. In this study, we explore the capabilities of pure RNNs and reassess
their long-term learning mechanisms. Inspired by the physics energy transition
models that track energy changes over time, we propose a effective recurrent
structure called the``Physics-inspired Energy Transition Neural Network"
(PETNN). We demonstrate that PETNN's memory mechanism effectively stores
information over long-term dependencies. Experimental results indicate that
PETNN outperforms transformer-based methods across various sequence tasks.
Furthermore, owing to its recurrent nature, PETNN exhibits significantly lower
complexity. Our study presents an optimal foundational recurrent architecture
and highlights the potential for developing effective recurrent neural networks
in fields currently dominated by Transformer.

</details>


### [179] [Unraveling the Rainbow: can value-based methods schedule?](https://arxiv.org/abs/2505.03323)
*Arthur Corrêa,Alexandre Jesus,Cristóvão Silva,Samuel Moniz*

Main category: cs.LG

TL;DR: 深度强化学习在组合优化问题中表现出色，但价值基方法常被忽视。本文通过实证评估，证明价值基方法在某些组合优化问题中可媲美或超越策略基方法。


<details>
  <summary>Details</summary>
Motivation: 组合优化领域多偏好策略基方法，但价值基方法的潜力未被充分挖掘。本文旨在验证价值基方法在复杂组合优化问题中的表现。

Method: 对深度Q网络及其扩展进行实证评估，应用于作业车间和柔性作业车间调度问题。

Result: 价值基方法在某些情况下可媲美或超越广泛采用的近端策略优化算法。

Conclusion: 价值基方法在组合优化中值得更多关注，其潜力不应被低估。

Abstract: Recently, deep reinforcement learning has emerged as a promising approach for
solving complex combinatorial optimization problems. Broadly, deep
reinforcement learning methods fall into two categories: policy-based and
value-based. While value-based approaches have achieved notable success in
domains such as the Arcade Learning Environment, the combinatorial optimization
community has predominantly favored policy-based methods, often overlooking the
potential of value-based algorithms. In this work, we conduct a comprehensive
empirical evaluation of value-based algorithms, including the deep q-network
and several of its advanced extensions, within the context of two complex
combinatorial problems: the job-shop and the flexible job-shop scheduling
problems, two fundamental challenges with multiple industrial applications. Our
results challenge the assumption that policy-based methods are inherently
superior for combinatorial optimization. We show that several value-based
approaches can match or even outperform the widely adopted proximal policy
optimization algorithm, suggesting that value-based strategies deserve greater
attention from the combinatorial optimization community. Our code is openly
available at: https://github.com/AJ-Correa/Unraveling-the-Rainbow.

</details>


### [180] [Absolute Zero: Reinforced Self-play Reasoning with Zero Data](https://arxiv.org/abs/2505.03335)
*Andrew Zhao,Yiran Wu,Yang Yue,Tong Wu,Quentin Xu,Yang Yue,Matthieu Lin,Shenzhi Wang,Qingyun Wu,Zilong Zheng,Gao Huang*

Main category: cs.LG

TL;DR: 提出了一种名为Absolute Zero的新RLVR范式，通过自我生成任务和验证答案实现无外部数据的强化学习。


<details>
  <summary>Details</summary>
Motivation: 解决现有RLVR方法依赖人工标注数据的问题，并探索超级智能系统在超越人类后的学习潜力。

Method: 引入Absolute Zero Reasoner (AZR)，利用代码执行器自我生成任务并验证答案，实现无监督学习。

Result: AZR在编码和数学推理任务上表现优异，超越依赖大量人工数据的现有模型。

Conclusion: AZR展示了无外部数据学习的可行性，并适用于不同规模和类型的模型。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown promise in
enhancing the reasoning capabilities of large language models by learning
directly from outcome-based rewards. Recent RLVR works that operate under the
zero setting avoid supervision in labeling the reasoning process, but still
depend on manually curated collections of questions and answers for training.
The scarcity of high-quality, human-produced examples raises concerns about the
long-term scalability of relying on human supervision, a challenge already
evident in the domain of language model pretraining. Furthermore, in a
hypothetical future where AI surpasses human intelligence, tasks provided by
humans may offer limited learning potential for a superintelligent system. To
address these concerns, we propose a new RLVR paradigm called Absolute Zero, in
which a single model learns to propose tasks that maximize its own learning
progress and improves reasoning by solving them, without relying on any
external data. Under this paradigm, we introduce the Absolute Zero Reasoner
(AZR), a system that self-evolves its training curriculum and reasoning ability
by using a code executor to both validate proposed code reasoning tasks and
verify answers, serving as an unified source of verifiable reward to guide
open-ended yet grounded learning. Despite being trained entirely without
external data, AZR achieves overall SOTA performance on coding and mathematical
reasoning tasks, outperforming existing zero-setting models that rely on tens
of thousands of in-domain human-curated examples. Furthermore, we demonstrate
that AZR can be effectively applied across different model scales and is
compatible with various model classes.

</details>


### [181] [Geospatial Mechanistic Interpretability of Large Language Models](https://arxiv.org/abs/2505.03368)
*Stef De Sabbata,Stefano Mizzaro,Kevin Roitero*

Main category: cs.LG

TL;DR: 本文提出了一种研究LLMs处理地理信息的新框架，通过空间分析揭示其内部机制。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自然语言处理中表现出色，但其处理地理信息的内部机制仍不清楚。

Method: 采用空间自相关等方法，结合稀疏自编码器，分析LLMs对地名等地理信息的内部表示。

Result: 实验显示，地名特征具有与地理位置相关的空间模式，揭示了LLMs处理地理信息的方式。

Conclusion: 该框架为地理学中基础模型的研究和应用提供了新视角。

Abstract: Large Language Models (LLMs) have demonstrated unprecedented capabilities
across various natural language processing tasks. Their ability to process and
generate viable text and code has made them ubiquitous in many fields, while
their deployment as knowledge bases and "reasoning" tools remains an area of
ongoing research. In geography, a growing body of literature has been focusing
on evaluating LLMs' geographical knowledge and their ability to perform spatial
reasoning. However, very little is still known about the internal functioning
of these models, especially about how they process geographical information.
  In this chapter, we establish a novel framework for the study of geospatial
mechanistic interpretability - using spatial analysis to reverse engineer how
LLMs handle geographical information. Our aim is to advance our understanding
of the internal representations that these complex models generate while
processing geographical information - what one might call "how LLMs think about
geographic information" if such phrasing was not an undue anthropomorphism.
  We first outline the use of probing in revealing internal structures within
LLMs. We then introduce the field of mechanistic interpretability, discussing
the superposition hypothesis and the role of sparse autoencoders in
disentangling polysemantic internal representations of LLMs into more
interpretable, monosemantic features. In our experiments, we use spatial
autocorrelation to show how features obtained for placenames display spatial
patterns related to their geographic location and can thus be interpreted
geospatially, providing insights into how these models process geographical
information. We conclude by discussing how our framework can help shape the
study and use of foundation models in geography.

</details>


### [182] [SPAP: Structured Pruning via Alternating Optimization and Penalty Methods](https://arxiv.org/abs/2505.03373)
*Hanyu Hu,Xiaoming Yuan*

Main category: cs.LG

TL;DR: SPAP是一种基于优化理论的结构化剪枝框架，通过交替优化和惩罚方法有效减少大语言模型的计算和内存需求，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的计算和内存需求高，现有剪枝方法存在性能下降、依赖启发式指标或微调成本高的问题。

Method: SPAP采用混合整数优化模型和惩罚方法，结合交替最小化算法，实现高效剪枝决策和权重更新。

Result: 在多个模型上验证，SPAP优于现有方法，推理速度提升1.29倍（30%稀疏度），内存占用线性减少。

Conclusion: SPAP为LLMs提供了一种实用、优化驱动的剪枝方案，显著提升效率且不牺牲性能。

Abstract: The deployment of large language models (LLMs) is often constrained by their
substantial computational and memory demands. While structured pruning presents
a viable approach by eliminating entire network components, existing methods
suffer from performance degradation, reliance on heuristic metrics, or
expensive finetuning. To address these challenges, we propose SPAP (Structured
Pruning via Alternating Optimization and Penalty Methods), a novel and
efficient structured pruning framework for LLMs grounded in optimization
theory. SPAP formulates the pruning problem through a mixed-integer
optimization model, employs a penalty method that effectively makes pruning
decisions to minimize pruning errors, and introduces an alternating
minimization algorithm tailored to the splittable problem structure for
efficient weight updates and performance recovery. Extensive experiments on
OPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over
state-of-the-art methods, delivering linear inference speedups (1.29$\times$ at
30% sparsity) and proportional memory reductions. Our work offers a practical,
optimization-driven solution for pruning LLMs while preserving model
performance.

</details>


### [183] [Physics-informed neural network estimation of active material properties in time-dependent cardiac biomechanical models](https://arxiv.org/abs/2505.03382)
*Matthias Höfler,Francesco Regazzoni,Stefano Pagani,Elias Karabelas,Christoph Augustin,Gundolf Haase,Gernot Plank,Federica Caforio*

Main category: cs.LG

TL;DR: 该论文提出了一种基于物理信息神经网络（PINNs）的方法，用于从医学影像数据中推断心脏生物力学模型中的主动收缩参数，并通过改进算法提高了参数重建的精度。


<details>
  <summary>Details</summary>
Motivation: 准确评估心脏生物力学中的主动应力参数对理解心肌功能至关重要，但临床中仅凭医学影像数据（如位移和应变）难以实现。

Method: 采用物理信息神经网络（PINNs），通过两个神经网络参数化状态和参数场，并构建能量最小化问题优化网络参数，结合自适应权重、正则化策略和傅里叶特征改进算法。

Result: 在噪声和高空间分辨率条件下成功重建主动应力场，并应用于心肌组织异质性和纤维化疤痕的检测。

Conclusion: 该方法为心脏纤维化相关疾病的诊断和治疗规划提供了新途径。

Abstract: Active stress models in cardiac biomechanics account for the mechanical
deformation caused by muscle activity, thus providing a link between the
electrophysiological and mechanical properties of the tissue. The accurate
assessment of active stress parameters is fundamental for a precise
understanding of myocardial function but remains difficult to achieve in a
clinical setting, especially when only displacement and strain data from
medical imaging modalities are available. This work investigates, through an
in-silico study, the application of physics-informed neural networks (PINNs)
for inferring active contractility parameters in time-dependent cardiac
biomechanical models from these types of imaging data. In particular, by
parametrising the sought state and parameter field with two neural networks,
respectively, and formulating an energy minimisation problem to search for the
optimal network parameters, we are able to reconstruct in various settings
active stress fields in the presence of noise and with a high spatial
resolution. To this end, we also advance the vanilla PINN learning algorithm
with the use of adaptive weighting schemes, ad-hoc regularisation strategies,
Fourier features, and suitable network architectures. In addition, we
thoroughly analyse the influence of the loss weights in the reconstruction of
active stress parameters. Finally, we apply the method to the characterisation
of tissue inhomogeneities and detection of fibrotic scars in myocardial tissue.
This approach opens a new pathway to significantly improve the diagnosis,
treatment planning, and management of heart conditions associated with cardiac
fibrosis.

</details>


### [184] [Improving Omics-Based Classification: The Role of Feature Selection and Synthetic Data Generation](https://arxiv.org/abs/2505.03387)
*Diego Perazzolo,Pietro Fanton,Ilaria Barison,Marny Fedrigo,Annalisa Angelini,Chiara Castellani,Enrico Grisan*

Main category: cs.LG

TL;DR: 论文提出了一种结合特征选择与数据增强的机器学习分类框架，旨在提高小样本组学数据的分类准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决组学数据高维小样本导致的模型可解释性和可靠性不足的问题。

Method: 集成特征选择与数据增强技术，通过公开数据集（E MTAB 8026）进行六种二分类场景的引导分析。

Result: 在小数据集上验证了模型的性能，并在更大测试集上保持一致性，表明合成数据有助于泛化。

Conclusion: 研究强调了准确性与特征选择的平衡，并展示了合成数据在小样本场景中的积极作用。

Abstract: Given the increasing complexity of omics datasets, a key challenge is not
only improving classification performance but also enhancing the transparency
and reliability of model decisions. Effective model performance and feature
selection are fundamental for explainability and reliability. In many cases,
high dimensional omics datasets suffer from limited number of samples due to
clinical constraints, patient conditions, phenotypes rarity and others
conditions. Current omics based classification models often suffer from narrow
interpretability, making it difficult to discern meaningful insights where
trust and reproducibility are critical. This study presents a machine learning
based classification framework that integrates feature selection with data
augmentation techniques to achieve high standard classification accuracy while
ensuring better interpretability. Using the publicly available dataset (E MTAB
8026), we explore a bootstrap analysis in six binary classification scenarios
to evaluate the proposed model's behaviour. We show that the proposed pipeline
yields cross validated perfomance on small dataset that is conserved when the
trained classifier is applied to a larger test set. Our findings emphasize the
fundamental balance between accuracy and feature selection, highlighting the
positive effect of introducing synthetic data for better generalization, even
in scenarios with very limited samples availability.

</details>


### [185] [Concept Factorization via Self-Representation and Adaptive Graph Structure Learning](https://arxiv.org/abs/2505.03390)
*Zhengqin Yang,Di Wu,Jia Chen,Xin Luo*

Main category: cs.LG

TL;DR: 提出了一种基于自表示和自适应图结构学习的概念分解模型（CFSRAG），通过动态学习数据内部几何结构，提升了聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于概念分解的模型依赖初始图结构构建，限制了聚类性能。

Method: 利用自表示方法学习数据间的亲和关系，结合动态图正则化约束，实现数据内部几何结构的动态学习。

Result: 在四个真实数据集上的实验表明，CFSRAG优于其他先进模型。

Conclusion: CFSRAG通过自适应图结构学习，显著提升了聚类性能。

Abstract: Concept Factorization (CF) models have attracted widespread attention due to
their excellent performance in data clustering. In recent years, many variant
models based on CF have achieved great success in clustering by taking into
account the internal geometric manifold structure of the dataset and using
graph regularization techniques. However, their clustering performance depends
greatly on the construction of the initial graph structure. In order to enable
adaptive learning of the graph structure of the data, we propose a Concept
Factorization Based on Self-Representation and Adaptive Graph Structure
Learning (CFSRAG) Model. CFSRAG learns the affinity relationship between data
through a self-representation method, and uses the learned affinity matrix to
implement dynamic graph regularization constraints, thereby ensuring dynamic
learning of the internal geometric structure of the data. Finally, we give the
CFSRAG update rule and convergence analysis, and conduct comparative
experiments on four real datasets. The results show that our model outperforms
other state-of-the-art models.

</details>


### [186] [Automatic Calibration for Membership Inference Attack on Large Language Models](https://arxiv.org/abs/2505.03392)
*Saleh Zare Zade,Yao Qiang,Xiangyu Zhou,Hui Zhu,Mohammad Amin Roshani,Prashant Khanduri,Dongxiao Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种名为ACMIA的新框架，通过可调温度校准输出概率，显著提高了成员推理攻击（MIAs）的准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击方法存在高误报率或依赖额外参考模型的问题，限制了其实际应用。

Method: ACMIA利用可调温度校准输出概率，并通过三种配置适应不同模型访问级别，增加成员与非成员之间的概率差距。

Result: 在多个开源LLM上的实验表明，ACMIA在三个基准测试中优于现有方法，具有高效性、鲁棒性和泛化性。

Conclusion: ACMIA通过理论指导和实验验证，显著提升了成员推理攻击的可靠性，为相关研究提供了新工具。

Abstract: Membership Inference Attacks (MIAs) have recently been employed to determine
whether a specific text was part of the pre-training data of Large Language
Models (LLMs). However, existing methods often misinfer non-members as members,
leading to a high false positive rate, or depend on additional reference models
for probability calibration, which limits their practicality. To overcome these
challenges, we introduce a novel framework called Automatic Calibration
Membership Inference Attack (ACMIA), which utilizes a tunable temperature to
calibrate output probabilities effectively. This approach is inspired by our
theoretical insights into maximum likelihood estimation during the pre-training
of LLMs. We introduce ACMIA in three configurations designed to accommodate
different levels of model access and increase the probability gap between
members and non-members, improving the reliability and robustness of membership
inference. Extensive experiments on various open-source LLMs demonstrate that
our proposed attack is highly effective, robust, and generalizable, surpassing
state-of-the-art baselines across three widely used benchmarks. Our code is
available at:
\href{https://github.com/Salehzz/ACMIA}{\textcolor{blue}{Github}}.

</details>


### [187] [Prediction Models That Learn to Avoid Missing Values](https://arxiv.org/abs/2505.03393)
*Lena Stempfle,Anton Matsson,Newton Mwai,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 提出了一种避免缺失值的机器学习框架（MA），通过特定正则化减少对缺失特征的依赖，同时保持预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 测试时处理缺失值对高精度和可解释性模型具有挑战性，传统方法可能引入偏差或复杂性。

Method: 为决策树、树集成和稀疏线性模型设计了MA学习算法，通过正则化减少对缺失值的依赖。

Result: 实验表明，MA方法在减少对缺失值依赖的同时，保持了与未正则化模型相当的预测性能。

Conclusion: MA框架为实践者提供了在测试时处理缺失值的同时保持模型可解释性的有效工具。

Abstract: Handling missing values at test time is challenging for machine learning
models, especially when aiming for both high accuracy and interpretability.
Established approaches often add bias through imputation or excessive model
complexity via missingness indicators. Moreover, either method can obscure
interpretability, making it harder to understand how the model utilizes the
observed variables in predictions. We propose missingness-avoiding (MA) machine
learning, a general framework for training models to rarely require the values
of missing (or imputed) features at test time. We create tailored MA learning
algorithms for decision trees, tree ensembles, and sparse linear models by
incorporating classifier-specific regularization terms in their learning
objectives. The tree-based models leverage contextual missingness by reducing
reliance on missing values based on the observed context. Experiments on
real-world datasets demonstrate that MA-DT, MA-LASSO, MA-RF, and MA-GBT
effectively reduce the reliance on features with missing values while
maintaining predictive performance competitive with their unregularized
counterparts. This shows that our framework gives practitioners a powerful tool
to maintain interpretability in predictions with test-time missing values.

</details>


### [188] [Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey](https://arxiv.org/abs/2505.03418)
*Da Zheng,Lun Du,Junwei Su,Yuchen Tian,Yuqi Zhu,Jintian Zhang,Lanning Wei,Ningyu Zhang,Huajun Chen*

Main category: cs.LG

TL;DR: 该论文综述了大型语言模型（LLMs）在复杂问题解决中的能力与局限，探讨了多步推理、领域知识整合和结果验证等技术，并指出了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的发展，LLMs成为解决复杂问题的强大工具，但其在现实应用中仍面临多步推理、领域知识整合和结果验证等挑战。

Method: 论文通过分析Chain-of-Thought推理、知识增强和验证技术等方法，探讨LLMs在复杂问题中的应用。

Result: 研究发现LLMs在多领域（如软件工程、数学推理、科学等）中具有潜力，但仍存在局限性。

Conclusion: 未来需进一步优化LLMs在多步推理、领域知识整合和结果验证方面的能力。

Abstract: Problem-solving has been a fundamental driver of human progress in numerous
domains. With advancements in artificial intelligence, Large Language Models
(LLMs) have emerged as powerful tools capable of tackling complex problems
across diverse domains. Unlike traditional computational systems, LLMs combine
raw computational power with an approximation of human reasoning, allowing them
to generate solutions, make inferences, and even leverage external
computational tools. However, applying LLMs to real-world problem-solving
presents significant challenges, including multi-step reasoning, domain
knowledge integration, and result verification. This survey explores the
capabilities and limitations of LLMs in complex problem-solving, examining
techniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,
and various LLM-based and tool-based verification techniques. Additionally, we
highlight domain-specific challenges in various domains, such as software
engineering, mathematical reasoning and proving, data analysis and modeling,
and scientific research. The paper further discusses the fundamental
limitations of the current LLM solutions and the future directions of LLM-based
complex problems solving from the perspective of multi-step reasoning, domain
knowledge integration and result verification.

</details>


### [189] [Framework GNN-AID: Graph Neural Network Analysis Interpretation and Defense](https://arxiv.org/abs/2505.03424)
*Kirill Lukyanov,Mikhail Drobyshevskiy,Georgii Sazonov,Mikhail Soloviov,Ilya Makarov*

Main category: cs.LG

TL;DR: GNN-AID是一个开源框架，专注于图数据的可信AI，结合了可解释性和鲁棒性，支持攻击、防御和可解释性方法。


<details>
  <summary>Details</summary>
Motivation: 现有工具常忽略图数据，且很少将可解释性与鲁棒性结合。GNN-AID旨在填补这一空白。

Method: 基于PyTorch-Geometric构建，提供预加载数据集、模型和自定义接口，支持MLOps技术。

Result: GNN-AID为开发者和研究人员提供了灵活的工具，支持快速实验和高级研究。

Conclusion: GNN-AID展示了防御策略间的冲突，强调了图数据中可信AI的复杂性。

Abstract: The growing need for Trusted AI (TAI) highlights the importance of
interpretability and robustness in machine learning models. However, many
existing tools overlook graph data and rarely combine these two aspects into a
single solution. Graph Neural Networks (GNNs) have become a popular approach,
achieving top results across various tasks. We introduce GNN-AID (Graph Neural
Network Analysis, Interpretation, and Defense), an open-source framework
designed for graph data to address this gap. Built as a Python library, GNN-AID
supports advanced trust methods and architectural layers, allowing users to
analyze graph datasets and GNN behavior using attacks, defenses, and
interpretability methods.
  GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models,
and support for any GNNs through customizable interfaces. It also includes a
web interface with tools for graph visualization and no-code features like an
interactive model builder, simplifying the exploration and analysis of GNNs.
The framework also supports MLOps techniques, ensuring reproducibility and
result versioning to track and revisit analyses efficiently.
  GNN-AID is a flexible tool for developers and researchers. It helps
developers create, analyze, and customize graph models, while also providing
access to prebuilt datasets and models for quick experimentation. Researchers
can use the framework to explore advanced topics on the relationship between
interpretability and robustness, test defense strategies, and combine methods
to protect against different types of attacks.
  We also show how defenses against evasion and poisoning attacks can conflict
when applied to graph data, highlighting the complex connections between
defense strategies.
  GNN-AID is available at
\href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}

</details>


### [190] [Wasserstein Convergence of Score-based Generative Models under Semiconvexity and Discontinuous Gradients](https://arxiv.org/abs/2505.03432)
*Stefano Bruno,Sotirios Sabanis*

Main category: cs.LG

TL;DR: 本文首次为非平滑、复杂数据分布下的Score-based生成模型（SGMs）建立了非渐近Wasserstein-2收敛保证，放宽了传统分析中的强正则条件。


<details>
  <summary>Details</summary>
Motivation: 尽管SGMs在复杂数据建模和生成方面表现出色，但现有理论分析通常依赖强正则条件（如平滑性或严格对数凹性），这些条件在实际中很少满足。本文旨在填补理论与实践的差距。

Method: 通过利用半凸性（semiconvexity）而不要求潜在函数的平滑性（如可微性），建立非渐近Wasserstein-2收敛上界。

Result: 上界在关键参数上显式且尖锐，实现了数据维度$d$上的最优依赖$O(\sqrt{d})$和阶数为1的收敛速率。适用于多种实际相关分布（如高斯混合、双阱势等）。

Conclusion: 本文显著拓宽了SGMs的理论基础，为复杂数据场景下的实际应用提供了更严格的保证。

Abstract: Score-based Generative Models (SGMs) approximate a data distribution by
perturbing it with Gaussian noise and subsequently denoising it via a learned
reverse diffusion process. These models excel at modeling complex data
distributions and generating diverse samples, achieving state-of-the-art
performance across domains such as computer vision, audio generation,
reinforcement learning, and computational biology. Despite their empirical
success, existing Wasserstein-2 convergence analysis typically assume strong
regularity conditions-such as smoothness or strict log-concavity of the data
distribution-that are rarely satisfied in practice. In this work, we establish
the first non-asymptotic Wasserstein-2 convergence guarantees for SGMs
targeting semiconvex distributions with potentially discontinuous gradients.
Our upper bounds are explicit and sharp in key parameters, achieving optimal
dependence of $O(\sqrt{d})$ on the data dimension $d$ and convergence rate of
order one. The framework accommodates a wide class of practically relevant
distributions, including symmetric modified half-normal distributions, Gaussian
mixtures, double-well potentials, and elastic net potentials. By leveraging
semiconvexity without requiring smoothness assumptions on the potential such as
differentiability, our results substantially broaden the theoretical
foundations of SGMs, bridging the gap between empirical success and rigorous
guarantees in non-smooth, complex data regimes.

</details>


### [191] [A new membership inference attack that spots memorization in generative and predictive models: Loss-Based with Reference Model algorithm (LBRM)](https://arxiv.org/abs/2505.03490)
*Faiz Taleb,Ivan Gazeau,Maryline Laurent*

Main category: cs.LG

TL;DR: 论文提出LBRM算法，通过参考模型提升时间序列插值模型中训练数据的记忆检测精度，显著提高隐私风险识别能力。


<details>
  <summary>Details</summary>
Motivation: 生成模型可能无意中记忆训练数据，带来隐私风险，本文旨在解决时间序列插值模型中的记忆现象。

Method: 提出Loss-Based with Reference Model (LBRM)算法，利用参考模型区分训练和测试数据，提升成员推理攻击的准确性。

Result: 未微调时AUROC提升约40%，微调后提升约60%；验证了LBRM在两种时间序列插值架构中的鲁棒性和通用性。

Conclusion: LBRM方法显著提升了检测精度，有效应对时间序列插值模型的隐私风险。

Abstract: Generative models can unintentionally memorize training data, posing
significant privacy risks. This paper addresses the memorization phenomenon in
time series imputation models, introducing the Loss-Based with Reference Model
(LBRM) algorithm. The LBRM method leverages a reference model to enhance the
accuracy of membership inference attacks, distinguishing between training and
test data. Our contributions are twofold: first, we propose an innovative
method to effectively extract and identify memorized training data,
significantly improving detection accuracy. On average, without fine-tuning,
the AUROC improved by approximately 40\%. With fine-tuning, the AUROC increased
by approximately 60\%. Second, we validate our approach through membership
inference attacks on two types of architectures designed for time series
imputation, demonstrating the robustness and versatility of the LBRM approach
in different contexts. These results highlight the significant enhancement in
detection accuracy provided by the LBRM approach, addressing privacy risks in
time series imputation models.

</details>


### [192] [AnomalyMatch: Discovering Rare Objects of Interest with Semi-supervised and Active Learning](https://arxiv.org/abs/2505.03509)
*Pablo Gómez,David O'Ryan*

Main category: cs.LG

TL;DR: AnomalyMatch是一个结合半监督FixMatch算法和主动学习的异常检测框架，适用于标签稀缺的大规模数据集，表现出色且可扩展。


<details>
  <summary>Details</summary>
Motivation: 在大型数据集中进行异常检测（如天文学和计算机视觉）通常需要大量标注，但实际中标注异常数据不切实际。

Method: 将异常检测视为半监督二分类问题，结合FixMatch算法和主动学习，利用有限标注和大量未标注图像，通过用户界面迭代优化模型。

Result: 在GalaxyMNIST和miniImageNet数据集上表现优异，AUROC分别达0.86和0.95，AUPRC为0.71和0.77，高排名图像中异常检测精度达71%-93%。

Conclusion: AnomalyMatch在标签稀缺领域具有显著实用性和可扩展性，特别适用于大规模天文数据集。

Abstract: Anomaly detection in large datasets is essential in fields such as astronomy
and computer vision; however, supervised methods typically require extensive
anomaly labelling, which is often impractical. We present AnomalyMatch, an
anomaly detection framework combining the semi-supervised FixMatch algorithm
using EfficientNet classifiers with active learning. By treating anomaly
detection as a semi-supervised binary classification problem, we efficiently
utilise limited labelled and abundant unlabelled images. We allow iterative
model refinement in a user interface for expert verification of high-confidence
anomalies and correction of false positives. Built for astronomical data,
AnomalyMatch generalises readily to other domains facing similar data
challenges. Evaluations on the GalaxyMNIST astronomical dataset and the
miniImageNet natural-image benchmark under severe class imbalance (1% anomalies
for miniImageNet) display strong performance: starting from five to ten
labelled anomalies and after three active learning cycles, we achieve an
average AUROC of 0.95 (miniImageNet) and 0.86 (GalaxyMNIST), with respective
AUPRC of 0.77 and 0.71. After active learning cycles, anomalies are ranked with
71% (miniImageNet) to 93% precision in the 1% of the highest-ranked images.
AnomalyMatch is tailored for large-scale applications, efficiently processing
predictions for 100 million images within three days on a single GPU.
Integrated into ESAs Datalabs platform, AnomalyMatch facilitates targeted
discovery of scientifically valuable anomalies in vast astronomical datasets.
Our results underscore the exceptional utility and scalability of this approach
for anomaly discovery, highlighting the value of specialised approaches for
domains characterised by severe label scarcity.

</details>


### [193] [Uncovering the Limitations of Model Inversion Evaluation: Benchmarks and Connection to Type-I Adversarial Attacks](https://arxiv.org/abs/2505.03519)
*Sy-Tuyen Ho,Koh Jun Hao,Ngoc-Bao Nguyen,Alexander Binder,Ngai-Man Cheung*

Main category: cs.LG

TL;DR: 本文首次深入研究了模型反演（MI）攻击的评估框架，发现其存在大量假阳性问题，导致此前报道的MI攻击成功率被高估。


<details>
  <summary>Details</summary>
Motivation: 揭示当前MI评估框架的局限性，特别是假阳性问题，以修正对MI攻击实际隐私泄漏的误解。

Method: 构建首个全面的人工标注MI攻击样本数据集，分析评估框架的准确性，并通过控制实验探究假阳性原因。

Result: 发现评估框架存在显著假阳性，MI攻击的实际隐私泄漏远低于此前报道，并揭示了Type I对抗特征的影响。

Conclusion: 强调当前MI评估框架的局限性，建议将人工评估作为主要框架，并呼吁开发更可靠的自动评估方法。

Abstract: Model Inversion (MI) attacks aim to reconstruct information of private
training data by exploiting access to machine learning models. The most common
evaluation framework for MI attacks/defenses relies on an evaluation model that
has been utilized to assess progress across almost all MI attacks and defenses
proposed in recent years. In this paper, for the first time, we present an
in-depth study of MI evaluation. Firstly, we construct the first comprehensive
human-annotated dataset of MI attack samples, based on 28 setups of different
MI attacks, defenses, private and public datasets. Secondly, using our dataset,
we examine the accuracy of the MI evaluation framework and reveal that it
suffers from a significant number of false positives. These findings raise
questions about the previously reported success rates of SOTA MI attacks.
Thirdly, we analyze the causes of these false positives, design controlled
experiments, and discover the surprising effect of Type I adversarial features
on MI evaluation, as well as adversarial transferability, highlighting a
relationship between two previously distinct research areas. Our findings
suggest that the performance of SOTA MI attacks has been overestimated, with
the actual privacy leakage being significantly less than previously reported.
In conclusion, we highlight critical limitations in the widely used MI
evaluation framework and present our methods to mitigate false positive rates.
We remark that prior research has shown that Type I adversarial attacks are
very challenging, with no existing solution. Therefore, we urge to consider
human evaluation as a primary MI evaluation framework rather than merely a
supplement as in previous MI research. We also encourage further work on
developing more robust and reliable automatic evaluation frameworks.

</details>


### [194] [Causal Intervention Framework for Variational Auto Encoder Mechanistic Interpretability](https://arxiv.org/abs/2505.03530)
*Dip Roy*

Main category: cs.LG

TL;DR: 该论文提出了一种针对变分自编码器（VAE）的因果干预框架，用于机制解释性研究，通过干预技术分析语义因素的编码和处理，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如VAE）的机制解释性研究仍具挑战性，而理解其内部工作原理对提升模型透明性和可控性至关重要。

Method: 开发了多层次的干预技术（输入操作、潜在空间扰动、激活修补和因果中介分析），用于识别和分析VAE中的电路模式。

Result: 实验表明，该框架能成功隔离功能电路，并将计算图映射到语义因素的因果图。FactorVAE在解耦分数（0.084）和效应强度（均值4.59）上优于标准VAE和Beta-VAE。

Conclusion: 该框架推进了对生成模型的机制理解，并为设计更透明和可控的VAE架构提供了工具。

Abstract: Mechanistic interpretability of deep learning models has emerged as a crucial
research direction for understanding the functioning of neural networks. While
significant progress has been made in interpreting discriminative models like
transformers, understanding generative models such as Variational Autoencoders
(VAEs) remains challenging. This paper introduces a comprehensive causal
intervention framework for mechanistic interpretability of VAEs. We develop
techniques to identify and analyze "circuit motifs" in VAEs, examining how
semantic factors are encoded, processed, and disentangled through the network
layers. Our approach uses targeted interventions at different levels: input
manipulations, latent space perturbations, activation patching, and causal
mediation analysis. We apply our framework to both synthetic datasets with
known causal relationships and standard disentanglement benchmarks. Results
show that our interventions can successfully isolate functional circuits, map
computational graphs to causal graphs of semantic factors, and distinguish
between polysemantic and monosemantic units. Furthermore, we introduce metrics
for causal effect strength, intervention specificity, and circuit modularity
that quantify the interpretability of VAE components. Experimental results
demonstrate clear differences between VAE variants, with FactorVAE achieving
higher disentanglement scores (0.084) and effect strengths (mean 4.59) compared
to standard VAE (0.064, 3.99) and Beta-VAE (0.051, 3.43). Our framework
advances the mechanistic understanding of generative models and provides tools
for more transparent and controllable VAE architectures.

</details>


### [195] [Small-Scale-Fading-Aware Resource Allocation in Wireless Federated Learning](https://arxiv.org/abs/2505.03533)
*Jiacheng Wang,Le Liang,Hao Ye,Chongtao Guo,Shi Jin*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体强化学习（MARL）的资源分配策略，用于优化无线网络中联邦学习（FL）的性能，考虑了小尺度衰落动态。


<details>
  <summary>Details</summary>
Motivation: 现有资源分配策略通常基于块衰落假设，忽略了FL梯度上传中的快速信道波动，导致性能下降。

Method: 通过建立FL算法的一步收敛边界，将资源分配问题建模为分散部分可观测马尔可夫决策过程（Dec-POMDP），并使用QMIX算法求解。

Result: 实验表明，该方法在统计异构性不同的情况下显著优于基线方法，消融研究验证了小尺度衰落动态的重要性。

Conclusion: 提出的MARL框架通过分散决策和动态资源分配，显著提升了FL的训练性能和实用性。

Abstract: Judicious resource allocation can effectively enhance federated learning (FL)
training performance in wireless networks by addressing both system and
statistical heterogeneity. However, existing strategies typically rely on block
fading assumptions, which overlooks rapid channel fluctuations within each
round of FL gradient uploading, leading to a degradation in FL training
performance. Therefore, this paper proposes a small-scale-fading-aware resource
allocation strategy using a multi-agent reinforcement learning (MARL)
framework. Specifically, we establish a one-step convergence bound of the FL
algorithm and formulate the resource allocation problem as a decentralized
partially observable Markov decision process (Dec-POMDP), which is subsequently
solved using the QMIX algorithm. In our framework, each client serves as an
agent that dynamically determines spectrum and power allocations within each
coherence time slot, based on local observations and a reward derived from the
convergence analysis. The MARL setting reduces the dimensionality of the action
space and facilitates decentralized decision-making, enhancing the scalability
and practicality of the solution. Experimental results demonstrate that our
QMIX-based resource allocation strategy significantly outperforms baseline
methods across various degrees of statistical heterogeneity. Additionally,
ablation studies validate the critical importance of incorporating small-scale
fading dynamics, highlighting its role in optimizing FL performance.

</details>


### [196] [Efficient Training of Physics-enhanced Neural ODEs via Direct Collocation and Nonlinear Programming](https://arxiv.org/abs/2505.03552)
*Linus Langenkamp,Philip Hannebohm,Bernhard Bachmann*

Main category: cs.LG

TL;DR: 提出了一种通过动态优化问题训练物理增强神经ODE（PeNODEs）的新方法，利用高阶隐式Runge-Kutta方法离散化模型，并通过NLP求解器优化参数和状态轨迹，显著提升了稳定性、运行时间和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决基于ODE求解器的训练方法在稳定性、运行时间和准确性方面的局限性。

Method: 采用高阶隐式Runge-Kutta方法离散化模型，将其转化为大规模非线性规划问题（NLP），并使用Ipopt等求解器进行优化。

Result: 在Quarter Vehicle Model和Van-der-Pol振荡器上的实验表明，该方法在准确性、速度和泛化能力上优于其他训练技术，且所需网络更小。

Conclusion: 该方法为训练PeNODEs提供了一种高效且稳定的解决方案，并计划集成到OpenModelica中以支持神经DAE的训练。

Abstract: We propose a novel approach for training Physics-enhanced Neural ODEs
(PeNODEs) by expressing the training process as a dynamic optimization problem.
The full model, including neural components, is discretized using a high-order
implicit Runge-Kutta method with flipped Legendre-Gauss-Radau points, resulting
in a large-scale nonlinear program (NLP) efficiently solved by state-of-the-art
NLP solvers such as Ipopt. This formulation enables simultaneous optimization
of network parameters and state trajectories, addressing key limitations of ODE
solver-based training in terms of stability, runtime, and accuracy. Extending
on a recent direct collocation-based method for Neural ODEs, we generalize to
PeNODEs, incorporate physical constraints, and present a custom, parallelized,
open-source implementation. Benchmarks on a Quarter Vehicle Model and a
Van-der-Pol oscillator demonstrate superior accuracy, speed, and generalization
with smaller networks compared to other training techniques. We also outline a
planned integration into OpenModelica to enable accessible training of Neural
DAEs.

</details>


### [197] [Rapid AI-based generation of coverage paths for dispensing applications](https://arxiv.org/abs/2505.03560)
*Simon Baeuerle,Ian F. Mendonca,Kristof Van Laerhoven,Ralf Mikut,Andreas Steimer*

Main category: cs.LG

TL;DR: 提出了一种基于AI的方法，用于生成热界面材料（TIM）的涂覆路径，替代传统的高计算优化方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖专家手动操作或高计算优化的方法，效率低且成本高。

Method: 使用人工神经网络（ANN）直接根据目标冷却区域生成涂覆路径，无需标签数据。

Result: 生成的路径可直接用于自动化制造设备，且避免了空气夹带问题。

Conclusion: 该方法可推广至其他制造过程，实时预测工艺参数。

Abstract: Coverage Path Planning of Thermal Interface Materials (TIM) plays a crucial
role in the design of power electronics and electronic control units. Up to
now, this is done manually by experts or by using optimization approaches with
a high computational effort. We propose a novel AI-based approach to generate
dispense paths for TIM and similar dispensing applications. It is a drop-in
replacement for optimization-based approaches. An Artificial Neural Network
(ANN) receives the target cooling area as input and directly outputs the
dispense path. Our proposed setup does not require labels and we show its
feasibility on multiple target areas. The resulting dispense paths can be
directly transferred to automated manufacturing equipment and do not exhibit
air entrapments. The approach of using an ANN to predict process parameters for
a desired target state in real-time could potentially be transferred to other
manufacturing processes.

</details>


### [198] [Ergodic Generative Flows](https://arxiv.org/abs/2505.03561)
*Leo Maxime Brunswic,Mateo Clemente,Rui Heng Yang,Adam Sigal,Amir Rasouli,Yinchuan Li*

Main category: cs.LG

TL;DR: 论文提出了一种称为Ergodic Generative Flows (EGFs)的生成流方法，用于解决GFNs在连续设置和模仿学习中的挑战，包括流匹配损失难处理性和无需单独奖励模型的模仿学习。


<details>
  <summary>Details</summary>
Motivation: Generative Flow Networks (GFNs)在连续设置和模仿学习中面临流匹配损失难处理、非循环训练测试有限以及模仿学习需要单独奖励模型等问题。

Method: 提出EGFs方法，利用遍历性构建简单生成流，定义有限全局变换，并引入KL-weakFM损失用于模仿学习。

Result: 在2D任务和NASA真实数据集上验证了IL-EGFs的有效性，并在2D强化学习实验中测试了FM损失。

Conclusion: EGFs解决了GFNs的关键挑战，为连续设置和模仿学习提供了有效方法。

Abstract: Generative Flow Networks (GFNs) were initially introduced on directed acyclic
graphs to sample from an unnormalized distribution density. Recent works have
extended the theoretical framework for generative methods allowing more
flexibility and enhancing application range. However, many challenges remain in
training GFNs in continuous settings and for imitation learning (IL), including
intractability of flow-matching loss, limited tests of non-acyclic training,
and the need for a separate reward model in imitation learning. The present
work proposes a family of generative flows called Ergodic Generative Flows
(EGFs) which are used to address the aforementioned issues. First, we leverage
ergodicity to build simple generative flows with finitely many globally defined
transformations (diffeomorphisms) with universality guarantees and tractable
flow-matching loss (FM loss). Second, we introduce a new loss involving
cross-entropy coupled to weak flow-matching control, coined KL-weakFM loss. It
is designed for IL training without a separate reward model. We evaluate
IL-EGFs on toy 2D tasks and real-world datasets from NASA on the sphere, using
the KL-weakFM loss. Additionally, we conduct toy 2D reinforcement learning
experiments with a target reward, using the FM loss.

</details>


### [199] [Anant-Net: Breaking the Curse of Dimensionality with Scalable and Interpretable Neural Surrogate for High-Dimensional PDEs](https://arxiv.org/abs/2505.03595)
*Sidharth S. Menon,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: Anant-Net是一种高效的神经代理模型，用于解决高维偏微分方程（PDEs），克服了传统方法的维度灾难问题。


<details>
  <summary>Details</summary>
Motivation: 高维PDEs在科学和工程中广泛应用，但传统数值方法因计算复杂度指数增长而难以处理。

Method: Anant-Net通过高效结合高维边界条件并最小化PDE残差，同时集成Kolmogorov-Arnold网络提升可解释性。

Result: 在多种线性和非线性高维方程（如Poisson、Sine-Gordon和Allen-Cahn方程）上表现出高精度和鲁棒性，300维问题可在单GPU上几小时内解决。

Conclusion: Anant-Net是一种准确、可解释且可扩展的高维PDE求解框架。

Abstract: High-dimensional partial differential equations (PDEs) arise in diverse
scientific and engineering applications but remain computationally intractable
due to the curse of dimensionality. Traditional numerical methods struggle with
the exponential growth in computational complexity, particularly on hypercubic
domains, where the number of required collocation points increases rapidly with
dimensionality. Here, we introduce Anant-Net, an efficient neural surrogate
that overcomes this challenge, enabling the solution of PDEs in high
dimensions. Unlike hyperspheres, where the internal volume diminishes as
dimensionality increases, hypercubes retain or expand their volume (for unit or
larger length), making high-dimensional computations significantly more
demanding. Anant-Net efficiently incorporates high-dimensional boundary
conditions and minimizes the PDE residual at high-dimensional collocation
points. To enhance interpretability, we integrate Kolmogorov-Arnold networks
into the Anant-Net architecture. We benchmark Anant-Net's performance on
several linear and nonlinear high-dimensional equations, including the Poisson,
Sine-Gordon, and Allen-Cahn equations, demonstrating high accuracy and
robustness across randomly sampled test points from high-dimensional space.
Importantly, Anant-Net achieves these results with remarkable efficiency,
solving 300-dimensional problems on a single GPU within a few hours. We also
compare Anant-Net's results for accuracy and runtime with other
state-of-the-art methods. Our findings establish Anant-Net as an accurate,
interpretable, and scalable framework for efficiently solving high-dimensional
PDEs.

</details>


### [200] [Understand the Effect of Importance Weighting in Deep Learning on Dataset Shift](https://arxiv.org/abs/2505.03617)
*Thien Nhan Vo,Thanh Xuan Truong*

Main category: cs.LG

TL;DR: 研究了重要性加权在深度神经网络中对标签偏移和协变量偏移的有效性，发现其效果随训练时间减弱，且在复杂数据中效果有限。


<details>
  <summary>Details</summary>
Motivation: 探讨重要性加权在实际分布偏移中的实用性。

Method: 在合成2D数据和CIFAR-10上实验，比较不同正则化方法的效果。

Result: 加权在早期训练中影响决策边界，但效果随优化减弱；L2正则化有助于保持效果，协变量偏移实验中无显著提升。

Conclusion: 质疑重要性加权在现实分布偏移中的实际效用。

Abstract: We evaluate the effectiveness of importance weighting in deep neural networks
under label shift and covariate shift. On synthetic 2D data (linearly separable
and moon-shaped) using logistic regression and MLPs, we observe that weighting
strongly affects decision boundaries early in training but fades with prolonged
optimization. On CIFAR-10 with various class imbalances, only L2 regularization
(not dropout) helps preserve weighting effects. In a covariate-shift
experiment, importance weighting yields no significant performance gain,
highlighting challenges on complex data. Our results call into question the
practical utility of importance weighting for real-world distribution shifts.

</details>


### [201] [ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders](https://arxiv.org/abs/2505.03646)
*Chethan Krishnamurthy Ramanaik,Arjun Roy,Eirini Ntoutsi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于层条件的对抗优化目标，用于增强深度自编码器的对抗鲁棒性评估，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度自编码器在关键应用中使用广泛，但其对抗鲁棒性研究不足，现有评估框架未能充分挖掘其脆弱性。

Method: 提出一种新的层条件对抗优化目标，通过增强损失梯度信息传播，引导对抗映射到局部Lipschitz边界区域。

Result: 实验表明，该方法在通用和样本特定场景下均优于现有攻击方法。

Conclusion: 论文还提出了一种对抗训练的防御插件，用于缓解对抗样本的影响。

Abstract: Despite the extensive use of deep autoencoders (AEs) in critical
applications, their adversarial robustness remains relatively underexplored
compared to classification models. AE robustness is characterized by the
Lipschitz bounds of its components. Existing robustness evaluation frameworks
based on white-box attacks do not fully exploit the vulnerabilities of
intermediate ill-conditioned layers in AEs. In the context of optimizing
imperceptible norm-bounded additive perturbations to maximize output damage,
existing methods struggle to effectively propagate adversarial loss gradients
throughout the network, often converging to less effective perturbations. To
address this, we propose a novel layer-conditioning-based adversarial
optimization objective that effectively guides the adversarial map toward
regions of local Lipschitz bounds by enhancing loss gradient information
propagation during attack optimization. We demonstrate through extensive
experiments on state-of-the-art AEs that our adversarial objective results in
stronger attacks, outperforming existing methods in both universal and
sample-specific scenarios. As a defense method against this attack, we
introduce an inference-time adversarially trained defense plugin that mitigates
the effects of adversarial examples.

</details>


### [202] [Mitigating mode collapse in normalizing flows by annealing with an adaptive schedule: Application to parameter estimation](https://arxiv.org/abs/2505.03652)
*Yihang Wang,Chris Chi,Aaron R. Dinner*

Main category: cs.LG

TL;DR: 本文提出了一种基于有效样本量（ESS）的自适应退火方法，用于缓解归一化流（NFs）在多模态分布中的模式崩溃问题，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 归一化流（NFs）在参数估计中具有潜力，但在多模态分布中易发生模式崩溃，限制了其实际应用。

Method: 采用基于ESS的自适应退火策略，并通过样本修剪降低方差。

Result: 该方法在生化振荡器模型的时间序列数据拟合中，计算时间比传统MCMC方法快十倍，且能收敛边际似然。

Conclusion: ESS自适应退火方法为NFs采样提供了通用解决方案，并为进一步改进提供了可能。

Abstract: Normalizing flows (NFs) provide uncorrelated samples from complex
distributions, making them an appealing tool for parameter estimation. However,
the practical utility of NFs remains limited by their tendency to collapse to a
single mode of a multimodal distribution. In this study, we show that annealing
with an adaptive schedule based on the effective sample size (ESS) can mitigate
mode collapse. We demonstrate that our approach can converge the marginal
likelihood for a biochemical oscillator model fit to time-series data in
ten-fold less computation time than a widely used ensemble Markov chain Monte
Carlo (MCMC) method. We show that the ESS can also be used to reduce variance
by pruning the samples. We expect these developments to be of general use for
sampling with NFs and discuss potential opportunities for further improvements.

</details>


### [203] [Neural Integral Operators for Inverse problems in Spectroscopy](https://arxiv.org/abs/2505.03677)
*Emanuele Zappala,Alice Giola,Andreas Kramer,Enrico Greco*

Main category: cs.LG

TL;DR: 提出一种基于积分算子的深度学习方法，用于分子光谱分类，解决了小数据集下的过拟合问题，性能优于传统方法和深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 光谱数据通常稀缺，传统深度学习方法易过拟合，传统机器学习方法精度有限。

Method: 基于第一类积分方程学习积分算子，设计深度学习模型。

Result: 在真实数据上表现优于决策树、支持向量机和其他深度学习模型，尤其适用于小数据集。

Conclusion: 该方法在小数据集下仍能保持深度学习的高性能，解决了光谱领域数据稀缺的挑战。

Abstract: Deep learning has shown high performance on spectroscopic inverse problems
when sufficient data is available. However, it is often the case that data in
spectroscopy is scarce, and this usually causes severe overfitting problems
with deep learning methods. Traditional machine learning methods are viable
when datasets are smaller, but the accuracy and applicability of these methods
is generally more limited.
  We introduce a deep learning method for classification of molecular spectra
based on learning integral operators via integral equations of the first kind,
which results in an algorithm that is less affected by overfitting issues on
small datasets, compared to other deep learning models.
  The problem formulation of the deep learning approach is based on inverse
problems, which have traditionally found important applications in
spectroscopy. We perform experiments on real world data to showcase our
algorithm. It is seen that the model outperforms traditional machine learning
approaches such as decision tree and support vector machine, and for small
datasets it outperforms other deep learning models. Therefore, our methodology
leverages the power of deep learning, still maintaining the performance when
the available data is very limited, which is one of the main issues that deep
learning faces in spectroscopy, where datasets are often times of small size.

</details>


### [204] [Learning Survival Distributions with the Asymmetric Laplace Distribution](https://arxiv.org/abs/2505.03712)
*Deming Sheng,Ricardo Henao*

Main category: cs.LG

TL;DR: 提出了一种基于非对称拉普拉斯分布（ALD）的参数化生存分析方法，优于现有参数和非参数方法。


<details>
  <summary>Details</summary>
Motivation: 现有生存分析模型多采用非参数方法，但缺乏对事件分布的直接估计，因此提出一种参数化方法以更准确地计算事件摘要。

Method: 利用ALD分布，通过最大似然估计学习个体层面的分布参数（位置、尺度和不对称性），实现事件摘要的闭式计算。

Result: 在合成和真实数据上，该方法在准确性、区分度和校准方面优于现有参数和非参数方法。

Conclusion: 基于ALD的参数化生存分析方法是一种高效且准确的替代方案。

Abstract: Probabilistic survival analysis models seek to estimate the distribution of
the future occurrence (time) of an event given a set of covariates. In recent
years, these models have preferred nonparametric specifications that avoid
directly estimating survival distributions via discretization. Specifically,
they estimate the probability of an individual event at fixed times or the time
of an event at fixed probabilities (quantiles), using supervised learning.
Borrowing ideas from the quantile regression literature, we propose a
parametric survival analysis method based on the Asymmetric Laplace
Distribution (ALD). This distribution allows for closed-form calculation of
popular event summaries such as mean, median, mode, variation, and quantiles.
The model is optimized by maximum likelihood to learn, at the individual level,
the parameters (location, scale, and asymmetry) of the ALD distribution.
Extensive results on synthetic and real-world data demonstrate that the
proposed method outperforms parametric and nonparametric approaches in terms of
accuracy, discrimination and calibration.

</details>


### [205] [Sustainable Smart Farm Networks: Enhancing Resilience and Efficiency with Decision Theory-Guided Deep Reinforcement Learning](https://arxiv.org/abs/2505.03721)
*Dian Chen,Zelin Wan,Dong Sam Ha,Jin-Hee Cho*

Main category: cs.LG

TL;DR: 论文提出了一种结合深度强化学习（DRL）、迁移学习（TL）和决策理论（DT）的可持续智能农场网络，以应对网络攻击和动态能源供应的挑战。


<details>
  <summary>Details</summary>
Motivation: 太阳能传感器监测系统在农业中应用广泛，但其对网络攻击的抵抗力和动态能源供应的适应性尚未充分研究。

Method: 采用DRL设计最优策略，结合TL和DT加速学习过程，优化监测质量和能源效率。

Result: 实验显示，DT引导的DRL模型性能优于TL增强的DRL，训练时间减少47.5%。

Conclusion: 该方法显著提升了智能农场网络的性能和效率，为农业监测系统提供了可持续解决方案。

Abstract: Solar sensor-based monitoring systems have become a crucial agricultural
innovation, advancing farm management and animal welfare through integrating
sensor technology, Internet-of-Things, and edge and cloud computing. However,
the resilience of these systems to cyber-attacks and their adaptability to
dynamic and constrained energy supplies remain largely unexplored. To address
these challenges, we propose a sustainable smart farm network designed to
maintain high-quality animal monitoring under various cyber and adversarial
threats, as well as fluctuating energy conditions. Our approach utilizes deep
reinforcement learning (DRL) to devise optimal policies that maximize both
monitoring effectiveness and energy efficiency. To overcome DRL's inherent
challenge of slow convergence, we integrate transfer learning (TL) and decision
theory (DT) to accelerate the learning process. By incorporating DT-guided
strategies, we optimize monitoring quality and energy sustainability,
significantly reducing training time while achieving comparable performance
rewards. Our experimental results prove that DT-guided DRL outperforms
TL-enhanced DRL models, improving system performance and reducing training
runtime by 47.5%.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [206] [Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation](https://arxiv.org/abs/2505.03105)
*Xule Lin*

Main category: cs.HC

TL;DR: 论文提出Cognitio Emergens（CE）框架，分析人类与AI在科学知识创造中的动态合作关系，强调角色、能力和动态演化的整合。


<details>
  <summary>Details</summary>
Motivation: 现有模型无法捕捉人类与AI在科学理解中的递归互动，CE框架旨在填补这一空白。

Method: CE框架包含三个组件：代理配置、认知维度和合作关系动态，结合自创生理论和社会系统理论。

Result: CE揭示了知识共创通过角色、价值观和组织结构的持续协商实现，提供了平衡人类参与与AI突破的工具。

Conclusion: CE重新定义人类与AI的科学合作为共同进化关系，为维持人类参与并实现突破提供了理论支持。

Abstract: Scientific knowledge creation is fundamentally transforming as humans and AI
systems evolve beyond tool-user relationships into co-evolutionary epistemic
partnerships. When AlphaFold revolutionized protein structure prediction,
researchers described engaging with an epistemic partner that reshaped how they
conceptualized fundamental relationships. This article introduces Cognitio
Emergens (CE), a framework addressing critical limitations in existing models
that focus on static roles or narrow metrics while failing to capture how
scientific understanding emerges through recursive human-AI interaction over
time. CE integrates three components addressing these limitations: Agency
Configurations describing how authority distributes between humans and AI
(Directed, Contributory, Partnership), with partnerships dynamically
oscillating between configurations rather than following linear progression;
Epistemic Dimensions capturing six specific capabilities emerging through
collaboration across Discovery, Integration, and Projection axes, creating
distinctive "capability signatures" that guide development; and Partnership
Dynamics identifying forces shaping how these relationships evolve,
particularly the risk of epistemic alienation where researchers lose
interpretive control over knowledge they formally endorse. Drawing from
autopoiesis theory, social systems theory, and organizational modularity, CE
reveals how knowledge co-creation emerges through continuous negotiation of
roles, values, and organizational structures. By reconceptualizing human-AI
scientific collaboration as fundamentally co-evolutionary, CE offers a balanced
perspective that neither uncritically celebrates nor unnecessarily fears AI's
evolving role, instead providing conceptual tools for cultivating partnerships
that maintain meaningful human participation while enabling transformative
scientific breakthroughs.

</details>


### [207] [Augmenting Human Cognition through Everyday AR](https://arxiv.org/abs/2505.03492)
*Xiaoan Liu*

Main category: cs.HC

TL;DR: 探讨了持续运行的增强现实（AR）如何结合数字认知与物理环境，提升任务表现和理解。


<details>
  <summary>Details</summary>
Motivation: 随着空间计算和多模态大语言模型的发展，AR有望成为智能化的‘思考工具’，将语义和上下文感知直接嵌入日常环境。

Method: 研究持续运行的AR技术，实现数字认知与物理环境的无缝连接。

Result: AR能够支持主动且上下文敏感的交互，从而提升人类的任务表现和理解能力。

Conclusion: 持续运行的AR技术有望成为连接数字与物理世界的桥梁，为人类提供更智能的交互体验。

Abstract: As spatial computing and multimodal LLMs mature, AR is tending to become an
intuitive "thinking tool," embedding semantic and context-aware intelligence
directly into everyday environments. This paper explores how always-on AR can
seamlessly bridge digital cognition and physical affordances, enabling
proactive, context-sensitive interactions that enhance human task performance
and understanding.

</details>


### [208] [BCause: Human-AI collaboration to improve hybrid mapping and ideation in argumentation-grounded deliberation](https://arxiv.org/abs/2505.03584)
*Lucas Anastasiou,Anna De Liddo*

Main category: cs.HC

TL;DR: BCause是一个结合生成式AI与人机协作的讨论系统，旨在将公共议题的无序对话转化为结构化、可操作的民主进程。


<details>
  <summary>Details</summary>
Motivation: 公共讨论常因散乱、浅显和脱离实际政策结果而效果不佳，BCause试图解决这一问题。

Method: 系统通过三项创新实现目标：(i) 将无序文本转化为论证性讨论，(ii) 通过Telegram机器人实现地理化问题感知，(iii) 提供智能报告工具（如摘要、主题建模、政策建议）。

Result: BCause通过人机协作保留了人类参与的关键作用，确保伦理监督、情境相关性和创造性综合。

Conclusion: BCause为公共讨论提供了结构化与可操作化的新途径，同时兼顾伦理与实用性。

Abstract: Public deliberation, as in open discussion of issues of public concern, often
suffers from scattered and shallow discourse, poor sensemaking, and a
disconnect from actionable policy outcomes. This paper introduces BCause, a
discussion system leveraging generative AI and human-machine collaboration to
transform unstructured dialogue around public issues (such as urban living,
policy changes, and current socio-economic transformations) into structured,
actionable democratic processes. We present three innovations: (i) importing
and transforming unstructured transcripts into argumentative discussions, (ii)
geo-deliberated problem-sensing via a Telegram bot for local issue reporting,
and (iii) smart reporting with customizable widgets (e.g., summaries, topic
modelling, policy recommendations, clustered arguments). The system's human-AI
partnership preserves critical human participation to ensure ethical oversight,
contextual relevance, and creative synthesis.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [209] [Snakemaker: Seamlessly transforming ad-hoc analyses into sustainable Snakemake workflows with generative AI](https://arxiv.org/abs/2505.02841)
*Marco Masera,Alessandro Leone,Johannes Köster,Ivan Molineris*

Main category: cs.SE

TL;DR: Snakemaker利用生成式AI将非结构化代码转换为Snakemake工作流，提升生物信息学软件的可重复性和可持续性。


<details>
  <summary>Details</summary>
Motivation: 生物信息学软件开发中，工具快速迭代和复杂工作流导致管道短命或难以适应，亟需解决方案。

Method: Snakemaker通过追踪终端操作、分析执行模式，生成符合最佳实践的Snakemake工作流，并支持将Ipython Notebook模块化。

Result: Snakemaker能生成高质量工作流，降低原型与生产代码间的障碍。

Conclusion: Snakemaker填补了生物信息学研究在计算可重复性上的关键空白。

Abstract: Reproducibility and sustainability present significant challenges in
bioinformatics software development, where rapidly evolving tools and complex
workflows often result in short-lived or difficult-to-adapt pipelines. This
paper introduces Snakemaker, a tool that leverages generative AI to facilitate
researchers build sustainable data analysis pipelines by converting
unstructured code into well-defined Snakemake workflows. Snakemaker
non-invasively tracks the work performed in the terminal by the researcher,
analyzes execution patterns, and generates Snakemake workflows that can be
integrated into existing pipelines. Snakemaker also supports the transformation
of monolithic Ipython Notebooks into modular Snakemake pipelines, resolving the
global state of the notebook into discrete, file-based interactions between
rules. An integrated chat assistant provides users with fine-grained control
through natural language instructions. Snakemaker generates high-quality
Snakemake workflows by adhering to the best practices, including Conda
environment tracking, generic rule generation and loop unrolling. By lowering
the barrier between prototype and production-quality code, Snakemaker addresses
a critical gap in computational reproducibility for bioinformatics research.

</details>


### [210] [The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models](https://arxiv.org/abs/2505.02931)
*Fernando Vallecillos Ruiz,Max Hort,Leon Moonen*

Main category: cs.SE

TL;DR: 论文研究了自动程序修复（APR）中多输出生成与多轮迭代的平衡策略，通过限制每个错误的总补丁数为10，评估了三种指令调优LLM的性能，发现小规模微调数据集即可显著提升修复能力，但过度微调会导致收益递减。迭代策略对基础模型尤其有效，复杂基准测试中效果更明显。


<details>
  <summary>Details</summary>
Motivation: 研究旨在平衡自动程序修复（APR）中多输出生成与多轮迭代的策略，以优化修复效果，同时限制资源消耗。

Method: 使用三种指令调优LLM（DeepSeekCoder-Instruct、Codellama-Instruct、Llama3.1-Instruct），在不同规模的微调数据集（1K、30K、65K）和两种技术（全微调与LoRA）下评估修复能力，测试于HumanEval-Java和Defects4J基准。

Result: 小规模微调数据集（<1%）可提升78%的合理补丁生成量，但过度微调会导致收益递减。迭代策略对基础模型效果显著，复杂基准中更明显。微调模型虽受益较少，但仍能从中获益。

Conclusion: 研究强调了结合多输出生成与迭代优化的平衡APR策略的重要性，为未来研究提供了方向。

Abstract: Automatic program repair (APR) aims to reduce the manual efforts required to
identify and fix errors in source code. Before the rise of LLM-based agents, a
common strategy was to increase the number of generated patches, sometimes to
the thousands, to achieve better repair results on benchmarks. More recently,
self-iterative capabilities enabled LLMs to refine patches over multiple rounds
guided by feedback. However, literature often focuses on many iterations and
disregards different numbers of outputs.
  We investigate an APR pipeline that balances these two approaches, the
generation of multiple outputs and multiple rounds of iteration, while imposing
a limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs
- DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct - to the APR
task. We further fine-tune each model on an APR dataset with three sizes (1K,
30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess
their repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.
  Our results show that by using only a fraction (<1%) of the fine-tuning
dataset, we can achieve improvements of up to 78% in the number of plausible
patches generated, challenging prior studies that reported limited gains using
Full Fine-Tuning. However, we find that exceeding certain thresholds leads to
diminishing outcomes, likely due to overfitting. Moreover, we show that base
models greatly benefit from creating patches in an iterative fashion rather
than generating them all at once. In addition, the benefit of iterative
strategies becomes more pronounced in complex benchmarks. Even fine-tuned
models, while benefiting less from iterations, still gain advantages,
particularly on complex benchmarks. The research underscores the need for
balanced APR strategies that combine multi-output generation and iterative
refinement.

</details>


### [211] [DocSpiral: A Platform for Integrated Assistive Document Annotation through Human-in-the-Spiral](https://arxiv.org/abs/2505.03214)
*Qiang Sun,Sirui Li,Tingting Bi,Du Huynh,Mark Reynolds,Yuanyi Luo,Wei Liu*

Main category: cs.SE

TL;DR: DocSpiral是一个辅助文档标注平台，通过人机协作减少标注时间，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决从图像文档中提取结构化数据的挑战，减少人工标注需求。

Method: 采用螺旋设计，结合文档格式标准化、标注界面、评估指标和API端点。

Result: 标注时间减少41%，模型性能持续提升。

Conclusion: DocSpiral降低了AI/ML模型开发的障碍，适用于地学和医疗等领域。

Abstract: Acquiring structured data from domain-specific, image-based documents such as
scanned reports is crucial for many downstream tasks but remains challenging
due to document variability. Many of these documents exist as images rather
than as machine-readable text, which requires human annotation to train
automated extraction systems. We present DocSpiral, the first
Human-in-the-Spiral assistive document annotation platform, designed to address
the challenge of extracting structured information from domain-specific,
image-based document collections. Our spiral design establishes an iterative
cycle in which human annotations train models that progressively require less
manual intervention. DocSpiral integrates document format normalization,
comprehensive annotation interfaces, evaluation metrics dashboard, and API
endpoints for the development of AI / ML models into a unified workflow.
Experiments demonstrate that our framework reduces annotation time by at least
41\% while showing consistent performance gains across three iterations during
model training. By making this annotation platform freely accessible, we aim to
lower barriers to AI/ML models development in document processing, facilitating
the adoption of large language models in image-based, document-intensive fields
such as geoscience and healthcare. The system is freely available at:
https://app.ai4wa.com. The demonstration video is available:
https://app.ai4wa.com/docs/docspiral/demo.

</details>


### [212] [Synthline: A Product Line Approach for Synthetic Requirements Engineering Data Generation using Large Language Models](https://arxiv.org/abs/2505.03265)
*Abdelkarim El-Hajjami,Camille Salinesi*

Main category: cs.SE

TL;DR: Synthline利用大型语言模型生成合成RE数据，弥补高质量数据集稀缺问题。实验表明合成数据虽多样性不足，但可作为有效训练资源，结合真实数据能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代需求工程依赖自然语言处理和机器学习，但高质量数据集稀缺限制了其效果。

Method: 采用产品线方法Synthline，利用大型语言模型生成合成数据，并通过实验评估其多样性和实用性。

Result: 合成数据多样性低于真实数据，但可作为训练资源；结合真实数据后，模型性能显著提升（精度提高85%，召回率翻倍）。

Conclusion: Synthline证明合成数据生成能有效解决需求工程中的数据稀缺问题，推动领域发展。

Abstract: While modern Requirements Engineering (RE) heavily relies on natural language
processing and Machine Learning (ML) techniques, their effectiveness is limited
by the scarcity of high-quality datasets. This paper introduces Synthline, a
Product Line (PL) approach that leverages Large Language Models to
systematically generate synthetic RE data for classification-based use cases.
Through an empirical evaluation conducted in the context of using ML for the
identification of requirements specification defects, we investigated both the
diversity of the generated data and its utility for training downstream models.
Our analysis reveals that while synthetic datasets exhibit less diversity than
real data, they are good enough to serve as viable training resources.
Moreover, our evaluation shows that combining synthetic and real data leads to
substantial performance improvements. Specifically, hybrid approaches achieve
up to 85% improvement in precision and a 2x increase in recall compared to
models trained exclusively on real data. These findings demonstrate the
potential of PL-based synthetic data generation to address data scarcity in RE.
We make both our implementation and generated datasets publicly available to
support reproducibility and advancement in the field.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [213] [Rational Retrieval Acts: Leveraging Pragmatic Reasoning to Improve Sparse Retrieval](https://arxiv.org/abs/2505.03676)
*Arthur Satouf,Gabriel Ben Zenou,Benjamin Piwowarski,Habiboulaye Amadou Boubacar,Pablo Piantanida*

Main category: cs.IR

TL;DR: 本文提出了一种基于Rational Speech Acts（RSA）框架的稀疏神经信息检索方法，通过动态调整文档中词项的权重，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏神经信息检索方法未充分考虑文档集合及词项权重间的复杂交互，限制了检索效果。

Method: 将RSA框架应用于信息检索，动态调整词项-文档交互，以更好地对比文档表示。

Result: 实验表明，RSA方法显著提升了多种稀疏检索模型的性能，并在BEIR基准测试中达到最先进水平。

Conclusion: RSA框架能有效改进稀疏检索模型，尤其在跨域数据集上表现优异。

Abstract: Current sparse neural information retrieval (IR) methods, and to a lesser
extent more traditional models such as BM25, do not take into account the
document collection and the complex interplay between different term weights
when representing a single document. In this paper, we show how the Rational
Speech Acts (RSA), a linguistics framework used to minimize the number of
features to be communicated when identifying an object in a set, can be adapted
to the IR case -- and in particular to the high number of potential features
(here, tokens). RSA dynamically modulates token-document interactions by
considering the influence of other documents in the dataset, better contrasting
document representations. Experiments show that incorporating RSA consistently
improves multiple sparse retrieval models and achieves state-of-the-art
performance on out-of-domain datasets from the BEIR benchmark.
https://github.com/arthur-75/Rational-Retrieval-Acts

</details>


### [214] [Feature Staleness Aware Incremental Learning for CTR Prediction](https://arxiv.org/abs/2505.02844)
*Zhikai Wang,Yanyan Shen,Zibin Zhang,Kangyi Lin*

Main category: cs.IR

TL;DR: 论文提出FeSAIL方法，通过自适应重放陈旧特征的样本，解决CTR预测中的特征陈旧问题。


<details>
  <summary>Details</summary>
Motivation: CTR预测模型在增量学习中，特征嵌入会因特征未出现在新数据中而变得陈旧，导致性能下降。

Method: 提出FeSAIL方法，包括SAS算法高效采样陈旧样本，以及SAR机制精细控制特征嵌入更新。

Result: FeSAIL在四个基准数据集上优于现有方法。

Conclusion: FeSAIL有效缓解特征陈旧问题，提升CTR预测性能。

Abstract: Click-through Rate (CTR) prediction in real-world recommender systems often
deals with billions of user interactions every day. To improve the training
efficiency, it is common to update the CTR prediction model incrementally using
the new incremental data and a subset of historical data. However, the feature
embeddings of a CTR prediction model often get stale when the corresponding
features do not appear in current incremental data. In the next period, the
model would have a performance degradation on samples containing stale
features, which we call the feature staleness problem. To mitigate this
problem, we propose a Feature Staleness Aware Incremental Learning method for
CTR prediction (FeSAIL) which adaptively replays samples containing stale
features. We first introduce a staleness aware sampling algorithm (SAS) to
sample a fixed number of stale samples with high sampling efficiency. We then
introduce a staleness aware regularization mechanism (SAR) for a fine-grained
control of the feature embedding updating. We instantiate FeSAIL with a general
deep learning-based CTR prediction model and the experimental results
demonstrate FeSAIL outperforms various state-of-the-art methods on four
benchmark datasets.

</details>


### [215] [Avoid Recommending Out-of-Domain Items: Constrained Generative Recommendation with LLMs](https://arxiv.org/abs/2505.03336)
*Hao Liao,Wensheng Lu,Jianxun Lian,Mingqi Wu,Shuo Wang,Yong Zhang,Yitian Huang,Mingyang Zhou,Xing Xie*

Main category: cs.IR

TL;DR: 论文研究了两种方法（RecLM-ret和RecLM-cgen）来解决LLM推荐系统中的OOD问题，实验表明RecLM-cgen在准确性和消除OOD推荐方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在生成推荐系统中可能推荐超出领域（OOD）物品的问题。

Method: 提出了两种方法：基于检索的RecLM-ret和基于约束生成的RecLM-cgen。

Result: RecLM-cgen在三个数据集上表现优于RecLM-ret和其他LLM推荐模型，且能完全消除OOD推荐。

Conclusion: RecLM-cgen是更优的选择，具有轻量化和易集成的特点，适合实际应用。

Abstract: Large Language Models (LLMs) have shown promise for generative recommender
systems due to their transformative capabilities in user interaction. However,
ensuring they do not recommend out-of-domain (OOD) items remains a challenge.
We study two distinct methods to address this issue: RecLM-ret, a
retrieval-based method, and RecLM-cgen, a constrained generation method. Both
methods integrate seamlessly with existing LLMs to ensure in-domain
recommendations. Comprehensive experiments on three recommendation datasets
demonstrate that RecLM-cgen consistently outperforms RecLM-ret and existing
LLM-based recommender models in accuracy while eliminating OOD recommendations,
making it the preferred method for adoption. Additionally, RecLM-cgen maintains
strong generalist capabilities and is a lightweight plug-and-play module for
easy integration into LLMs, offering valuable practical benefits for the
community. Source code is available at https://github.com/microsoft/RecAI

</details>


### [216] [Modeling Musical Genre Trajectories through Pathlet Learning](https://arxiv.org/abs/2505.03480)
*Lilian Marey,Charlotte Laclau,Bruno Sguerra,Tiphaine Viard,Manuel Moussallam*

Main category: cs.IR

TL;DR: 论文提出了一种基于字典学习的框架，用于分析音乐流媒体平台上用户偏好的演变，通过捕捉重复出现的音乐流派轨迹模式（称为pathlets），生成可理解的轨迹嵌入。


<details>
  <summary>Details</summary>
Motivation: 随着音乐流媒体平台上用户数据的增加，分析音乐消费行为成为可能，但理解用户偏好的动态变化仍具挑战性。

Method: 采用字典学习范式，定义新框架捕捉音乐流派轨迹中的重复模式（pathlets），生成轨迹嵌入。

Result: pathlet学习揭示了可定性和定量分析的收听模式，有助于理解用户与音乐的互动。

Conclusion: 该研究不仅提升了对用户音乐偏好的理解，还为推荐系统行为研究和多样性促进提供了新方向，并公开了Deezer提供的2000名用户17个月的流派标记数据集。

Abstract: The increasing availability of user data on music streaming platforms opens
up new possibilities for analyzing music consumption. However, understanding
the evolution of user preferences remains a complex challenge, particularly as
their musical tastes change over time. This paper uses the dictionary learning
paradigm to model user trajectories across different musical genres. We define
a new framework that captures recurring patterns in genre trajectories, called
pathlets, enabling the creation of comprehensible trajectory embeddings. We
show that pathlet learning reveals relevant listening patterns that can be
analyzed both qualitatively and quantitatively. This work improves our
understanding of users' interactions with music and opens up avenues of
research into user behavior and fostering diversity in recommender systems. A
dataset of 2000 user histories tagged by genre over 17 months, supplied by
Deezer (a leading music streaming company), is also released with the code.

</details>


### [217] [Counterfactual Inference for Eliminating Sentiment Bias in Recommender Systems](https://arxiv.org/abs/2505.03655)
*Le Pan,Yuanjiang Cao,Chengkai Huang,Wenjie Zhang,Lina Yao*

Main category: cs.IR

TL;DR: 该论文提出了一种基于反事实推理的方法，用于缓解推荐系统中的情感偏差问题，通过建模情感对评分的影响并解耦直接和间接效应，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究情感偏差对推荐系统的影响，特别是负面评价对用户和物品推荐准确性的不利影响，旨在通过反事实推理解决这一问题。

Method: 采用两阶段方法：训练阶段构建因果图建模情感对评分的影响；推理阶段通过反事实推理解耦直接和间接效应以减轻情感偏差。

Result: 实验结果表明，该方法在评分预测和情感偏差缓解方面表现优异。

Conclusion: 这是首次在推荐系统中应用反事实推理解决情感偏差问题，为未来研究提供了新方向。

Abstract: Recommender Systems (RSs) aim to provide personalized recommendations for
users. A newly discovered bias, known as sentiment bias, uncovers a common
phenomenon within Review-based RSs (RRSs): the recommendation accuracy of users
or items with negative reviews deteriorates compared with users or items with
positive reviews. Critical users and niche items are disadvantaged by such
unfair recommendations. We study this problem from the perspective of
counterfactual inference with two stages. At the model training stage, we build
a causal graph and model how sentiment influences the final rating score.
During the inference stage, we decouple the direct and indirect effects to
mitigate the impact of sentiment bias and remove the indirect effect using
counterfactual inference. We have conducted extensive experiments, and the
results validate that our model can achieve comparable performance on rating
prediction for better recommendations and effective mitigation of sentiment
bias. To the best of our knowledge, this is the first work to employ
counterfactual inference on sentiment bias mitigation in RSs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [218] [Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories](https://arxiv.org/abs/2505.03443)
*Valerio Bellandi*

Main category: cs.DC

TL;DR: 论文比较了集中式和分布式系统的优缺点，并介绍了一个为意大利司法部开发的分布式文档存储系统。


<details>
  <summary>Details</summary>
Motivation: 探讨集中式与分布式系统的适用场景，并开发一个高效、可扩展的文档存储系统以满足司法需求。

Method: 采用分布式文档存储系统，利用边缘存储库分析文本数据和元数据，提升语义探索能力。

Result: 系统成功应用于意大利司法部，增强了文档管理的可扩展性和语义探索功能。

Conclusion: 分布式系统在需要高可用性和性能的大规模环境中表现优异，适合司法文档管理需求。

Abstract: Centralized and distributed systems are two main approaches to organizing ICT
infrastructure, each with its pros and cons. Centralized systems concentrate
resources in one location, making management easier but creating single points
of failure. Distributed systems, on the other hand, spread resources across
multiple nodes, offering better scalability and fault tolerance, but requiring
more complex management. The choice between them depends on factors like
application needs, scalability, and data sensitivity. Centralized systems suit
applications with limited scalability and centralized control, while
distributed systems excel in large-scale environments requiring high
availability and performance. This paper explores a distributed document
repository system developed for the Italian Ministry of Justice, using edge
repositories to analyze textual data and metadata, enhancing semantic
exploration capabilities.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [219] [Physical foundations for trustworthy medical imaging: a review for artificial intelligence researchers](https://arxiv.org/abs/2505.02843)
*Miriam Cobo,David Corral Fontecha,Wilson Silva,Lara Lloret Iglesias*

Main category: eess.IV

TL;DR: 论文探讨了医学影像中人工智能的发展，强调了物理知识对提升AI算法可信度和鲁棒性的重要性，并综述了物理原理在生成模型和重建算法中的应用。


<details>
  <summary>Details</summary>
Motivation: 由于AI专业人士对医学影像物理原理的理解不足，限制了AI在医学影像中的潜力发挥，因此需要将物理知识整合到AI算法中。

Method: 综述了医学影像的物理基础及其对AI最新进展的影响，特别是生成模型和重建算法，并探索了物理知识在机器学习模型中的整合。

Result: 物理知识的整合可以增强AI算法在医学影像中的可信度和鲁棒性，尤其在数据有限的情况下。

Conclusion: 物理启发的机器学习模型通过结合物理约束，能够更好地学习医学影像特征，提升AI在医学影像中的应用效果。

Abstract: Artificial intelligence in medical imaging has seen unprecedented growth in
the last years, due to rapid advances in deep learning and computing resources.
Applications cover the full range of existing medical imaging modalities, with
unique characteristics driven by the physics of each technique. Yet, artificial
intelligence professionals entering the field, and even experienced developers,
often lack a comprehensive understanding of the physical principles underlying
medical image acquisition, which hinders their ability to fully leverage its
potential. The integration of physics knowledge into artificial intelligence
algorithms enhances their trustworthiness and robustness in medical imaging,
especially in scenarios with limited data availability. In this work, we review
the fundamentals of physics in medical images and their impact on the latest
advances in artificial intelligence, particularly, in generative models and
reconstruction algorithms. Finally, we explore the integration of physics
knowledge into physics-inspired machine learning models, which leverage
physics-based constraints to enhance the learning of medical imaging features.

</details>


### [220] [Dual Prompting for Diverse Count-level PET Denoising](https://arxiv.org/abs/2505.03037)
*Xiaofeng Liu,Yongsong Huang,Thibault Marin,Samira Vafay Eslahi,Tiss Amal,Yanis Chemli,Keith Johnson,Georges El Fakhri,Jinsong Ouyang*

Main category: eess.IV

TL;DR: 论文提出了一种基于双提示学习的PET图像去噪方法，通过显式和隐式提示动态指导不同计数水平的去噪过程，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: PET图像去噪面临不同计数水平的挑战，需要一种通用模型处理多样化数据。

Method: 提出双提示（显式计数水平提示和隐式通用去噪提示）及提示融合模块，动态指导去噪过程。

Result: 在1940个低计数PET 3D体积上验证，双提示方法显著优于计数条件模型。

Conclusion: 双提示学习能有效统一不同计数水平的PET去噪，具有广泛应用潜力。

Abstract: The to-be-denoised positron emission tomography (PET) volumes are inherent
with diverse count levels, which imposes challenges for a unified model to
tackle varied cases. In this work, we resort to the recently flourished prompt
learning to achieve generalizable PET denoising with different count levels.
Specifically, we propose dual prompts to guide the PET denoising in a
divide-and-conquer manner, i.e., an explicitly count-level prompt to provide
the specific prior information and an implicitly general denoising prompt to
encode the essential PET denoising knowledge. Then, a novel prompt fusion
module is developed to unify the heterogeneous prompts, followed by a
prompt-feature interaction module to inject prompts into the features. The
prompts are able to dynamically guide the noise-conditioned denoising process.
Therefore, we are able to efficiently train a unified denoising model for
various count levels, and deploy it to different cases with personalized
prompts. We evaluated on 1940 low-count PET 3D volumes with uniformly randomly
selected 13-22\% fractions of events from 97 $^{18}$F-MK6240 tau PET studies.
It shows our dual prompting can largely improve the performance with informed
count-level and outperform the count-conditional model.

</details>


### [221] [STG: Spatiotemporal Graph Neural Network with Fusion and Spatiotemporal Decoupling Learning for Prognostic Prediction of Colorectal Cancer Liver Metastasis](https://arxiv.org/abs/2505.03123)
*Yiran Zhu,Wei Yang,Yan su,Zesheng Li,Chengchang Pan,Honggang Qi*

Main category: eess.IV

TL;DR: 提出了一种多模态时空图神经网络框架（STG），用于预测结直肠癌肝转移（CRLM）的进展，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前临床模型未能有效整合肿瘤的空间异质性、动态演化和多模态数据关系，限制了预测准确性。

Method: 结合术前CT影像和临床数据构建异构图结构，利用GraphSAGE聚合时空邻域信息，并通过监督和对比学习策略增强模型能力。

Result: 在MSKCC CRLM数据集上，时间邻近准确率达85%，平均绝对误差为1.1005。

Conclusion: 该框架通过创新的异构图构建和时空解耦机制，为个性化治疗决策提供了可靠的定量支持。

Abstract: We propose a multimodal spatiotemporal graph neural network (STG) framework
to predict colorectal cancer liver metastasis (CRLM) progression. Current
clinical models do not effectively integrate the tumor's spatial heterogeneity,
dynamic evolution, and complex multimodal data relationships, limiting their
predictive accuracy. Our STG framework combines preoperative CT imaging and
clinical data into a heterogeneous graph structure, enabling joint modeling of
tumor distribution and temporal evolution through spatial topology and
cross-modal edges. The framework uses GraphSAGE to aggregate spatiotemporal
neighborhood information and leverages supervised and contrastive learning
strategies to enhance the model's ability to capture temporal features and
improve robustness. A lightweight version of the model reduces parameter count
by 78.55%, maintaining near-state-of-the-art performance. The model jointly
optimizes recurrence risk regression and survival analysis tasks, with
contrastive loss improving feature representational discriminability and
cross-modal consistency. Experimental results on the MSKCC CRLM dataset show a
time-adjacent accuracy of 85% and a mean absolute error of 1.1005,
significantly outperforming existing methods. The innovative heterogeneous
graph construction and spatiotemporal decoupling mechanism effectively uncover
the associations between dynamic tumor microenvironment changes and prognosis,
providing reliable quantitative support for personalized treatment decisions.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [222] [Safer Prompts: Reducing IP Risk in Visual Generative AI](https://arxiv.org/abs/2505.03338)
*Lena Reissinger,Yuanyuan Li,Anna-Carolina Haensch,Neeraj Sarna*

Main category: math.NA

TL;DR: 研究评估了提示工程技术在减少图像生成中知识产权侵权风险的效果，发现链式思维提示和任务指令提示能显著降低生成图像与训练数据的相似性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型可能因训练数据多样性而记忆并复制特定内容，引发知识产权侵权担忧。

Method: 采用链式思维提示和任务指令提示技术。

Result: 这些提示技术显著降低了生成图像与训练数据的相似性。

Conclusion: 提示工程是降低生成式AI知识产权侵权风险的有效方法。

Abstract: Visual Generative AI models have demonstrated remarkable capability in
generating high-quality images from simple inputs like text prompts. However,
because these models are trained on images from diverse sources, they risk
memorizing and reproducing specific content, raising concerns about
intellectual property (IP) infringement. Recent advances in prompt engineering
offer a cost-effective way to enhance generative AI performance. In this paper,
we evaluate the effectiveness of prompt engineering techniques in mitigating IP
infringement risks in image generation. Our findings show that Chain of Thought
Prompting and Task Instruction Prompting significantly reduce the similarity
between generated images and the training data of diffusion models, thereby
lowering the risk of IP infringement.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [223] [BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models](https://arxiv.org/abs/2505.03501)
*Zihan Wang,Hongwei Li,Rui Zhang,Wenbo Jiang,Kangjie Chen,Tianwei Zhang,Qingchuan Zhao,Guowen Xu*

Main category: cs.CR

TL;DR: 本文提出了一种针对大型语言模型（LLMs）的新型后门攻击——语言后门攻击，其创新点在于利用语言本身作为触发条件，使受感染的LLMs生成煽动性言论。通过设计任务无关的BadLingual攻击和改进的对抗训练方法，显著提升了攻击的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 揭示LLMs在多语言能力下的新漏洞，特别是通过语言后门攻击加剧种族歧视的可能性。

Method: 1. 实现基线语言后门攻击，通过翻译触发语言污染训练数据；2. 设计任务无关的BadLingual攻击，采用PPL约束的贪婪坐标梯度搜索（PGCG）对抗训练方法扩展决策边界。

Result: 基线攻击在特定任务上ASR超过90%，但在任务无关场景下仅为37.61%；BadLingual相比基线提升了37.35%。

Conclusion: 语言后门攻击揭示了LLMs的新漏洞，为未来防御研究提供了方向。

Abstract: In this paper, we present a new form of backdoor attack against Large
Language Models (LLMs): lingual-backdoor attacks. The key novelty of
lingual-backdoor attacks is that the language itself serves as the trigger to
hijack the infected LLMs to generate inflammatory speech. They enable the
precise targeting of a specific language-speaking group, exacerbating racial
discrimination by malicious entities. We first implement a baseline
lingual-backdoor attack, which is carried out by poisoning a set of training
data for specific downstream tasks through translation into the trigger
language. However, this baseline attack suffers from poor task generalization
and is impractical in real-world settings. To address this challenge, we design
BadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any
downstream tasks within the chat LLMs, regardless of the specific questions of
these tasks. We design a new approach using PPL-constrained Greedy Coordinate
Gradient-based Search (PGCG) based adversarial training to expand the decision
boundary of lingual-backdoor, thereby enhancing the generalization ability of
lingual-backdoor across various tasks. We perform extensive experiments to
validate the effectiveness of our proposed attacks. Specifically, the baseline
attack achieves an ASR of over 90% on the specified tasks. However, its ASR
reaches only 37.61% across six tasks in the task-agnostic scenario. In
contrast, BadLingual brings up to 37.35% improvement over the baseline. Our
study sheds light on a new perspective of vulnerabilities in LLMs with
multilingual capabilities and is expected to promote future research on the
potential defenses to enhance the LLMs' robustness

</details>


### [224] [Adversarial Sample Generation for Anomaly Detection in Industrial Control Systems](https://arxiv.org/abs/2505.03120)
*Abdul Mustafa,Muhammad Talha Khan,Muhammad Azmi Umer,Zaki Masood,Chuadhry Mujeeb Ahmed*

Main category: cs.CR

TL;DR: 论文研究了基于机器学习的入侵检测系统对抗性攻击的脆弱性，提出使用JSMA生成对抗样本，并在工业控制系统中验证其泛化性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 对抗性攻击可能被恶意实体利用，因此需要研究如何让IDS识别对抗样本以提高安全性。

Method: 使用Jacobian Saliency Map Attack (JSMA)生成对抗样本，并在SWaT测试床上评估其效果。

Result: 模型在未训练的真实攻击数据上检测准确率达到95%。

Conclusion: 研究表明，对抗样本训练能有效提升IDS对真实攻击的检测能力。

Abstract: Machine learning (ML)-based intrusion detection systems (IDS) are vulnerable
to adversarial attacks. It is crucial for an IDS to learn to recognize
adversarial examples before malicious entities exploit them. In this paper, we
generated adversarial samples using the Jacobian Saliency Map Attack (JSMA). We
validate the generalization and scalability of the adversarial samples to
tackle a broad range of real attacks on Industrial Control Systems (ICS). We
evaluated the impact by assessing multiple attacks generated using the proposed
method. The model trained with adversarial samples detected attacks with 95%
accuracy on real-world attack data not used during training. The study was
conducted using an operational secure water treatment (SWaT) testbed.

</details>


### [225] [Detecting Quishing Attacks with Machine Learning Techniques Through QR Code Analysis](https://arxiv.org/abs/2505.03451)
*Fouad Trad,Ali Chehab*

Main category: cs.CR

TL;DR: 论文提出了一种基于QR码结构和像素模式的钓鱼检测框架，无需提取内容，通过机器学习模型（如XGBoost）实现了高AUC性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于URL的检测方法无法全面应对QR码钓鱼攻击，且可能暴露用户于恶意内容。

Method: 生成钓鱼与良性QR码数据集，训练多种机器学习模型，分析QR码结构和像素模式。

Result: XGBoost模型AUC达0.9106，优化特征后提升至0.9133，结构特征与钓鱼风险强相关。

Conclusion: 直接QR分析为钓鱼防御提供了新方向，验证了其可行性。

Abstract: The rise of QR code based phishing ("Quishing") poses a growing cybersecurity
threat, as attackers increasingly exploit QR codes to bypass traditional
phishing defenses. Existing detection methods predominantly focus on URL
analysis, which requires the extraction of the QR code payload, and may
inadvertently expose users to malicious content. Moreover, QR codes can encode
various types of data beyond URLs, such as Wi-Fi credentials and payment
information, making URL-based detection insufficient for broader security
concerns. To address these gaps, we propose the first framework for quishing
detection that directly analyzes QR code structure and pixel patterns without
extracting the embedded content. We generated a dataset of phishing and benign
QR codes and we used it to train and evaluate multiple machine learning models,
including Logistic Regression, Decision Trees, Random Forest, Naive Bayes,
LightGBM, and XGBoost. Our best-performing model (XGBoost) achieves an AUC of
0.9106, demonstrating the feasibility of QR-centric detection. Through feature
importance analysis, we identify key visual indicators of malicious intent and
refine our feature set by removing non-informative pixels, improving
performance to an AUC of 0.9133 with a reduced feature space. Our findings
reveal that the structural features of QR code correlate strongly with phishing
risk. This work establishes a foundation for quishing mitigation and highlights
the potential of direct QR analysis as a critical layer in modern phishing
defenses.

</details>


### [226] [LlamaFirewall: An open source guardrail system for building secure AI agents](https://arxiv.org/abs/2505.03574)
*Sahana Chennabasappa,Cyrus Nikolaidis,Daniel Song,David Molnar,Stephanie Ding,Shengye Wan,Spencer Whitman,Lauren Deason,Nicholas Doucette,Abraham Montilla,Alekhya Gampa,Beto de Paola,Dominik Gabi,James Crnkovich,Jean-Christophe Testud,Kat He,Rashnil Chaturvedi,Wu Zhou,Joshua Saxe*

Main category: cs.CR

TL;DR: LlamaFirewall是一个开源的安全防护框架，旨在为AI代理提供最后一层防御，通过PromptGuard 2、Agent Alignment Checks和CodeShield三大防护机制，解决提示注入、代理不对齐和不安全代码等风险。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）能力的提升，其作为自主代理执行高风险任务时，现有安全措施无法完全应对新的安全风险，因此需要实时防护监控。

Method: LlamaFirewall框架包含三个核心防护机制：PromptGuard 2（通用越狱检测器）、Agent Alignment Checks（链式思维审计器）和CodeShield（在线静态分析引擎），并支持开发者自定义扫描器。

Result: PromptGuard 2表现优异，Agent Alignment Checks在防止间接注入方面效果显著，CodeShield快速且可扩展。

Conclusion: LlamaFirewall为AI代理提供了有效的安全防护，填补了现有安全措施的不足。

Abstract: Large language models (LLMs) have evolved from simple chatbots into
autonomous agents capable of performing complex tasks such as editing
production code, orchestrating workflows, and taking higher-stakes actions
based on untrusted inputs like webpages and emails. These capabilities
introduce new security risks that existing security measures, such as model
fine-tuning or chatbot-focused guardrails, do not fully address. Given the
higher stakes and the absence of deterministic solutions to mitigate these
risks, there is a critical need for a real-time guardrail monitor to serve as a
final layer of defense, and support system level, use case specific safety
policy definition and enforcement. We introduce LlamaFirewall, an open-source
security focused guardrail framework designed to serve as a final layer of
defense against security risks associated with AI Agents. Our framework
mitigates risks such as prompt injection, agent misalignment, and insecure code
risks through three powerful guardrails: PromptGuard 2, a universal jailbreak
detector that demonstrates clear state of the art performance; Agent Alignment
Checks, a chain-of-thought auditor that inspects agent reasoning for prompt
injection and goal misalignment, which, while still experimental, shows
stronger efficacy at preventing indirect injections in general scenarios than
previously proposed approaches; and CodeShield, an online static analysis
engine that is both fast and extensible, aimed at preventing the generation of
insecure or dangerous code by coding agents. Additionally, we include
easy-to-use customizable scanners that make it possible for any developer who
can write a regular expression or an LLM prompt to quickly update an agent's
security guardrails.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [227] [Vector valued optimal transport: from dynamic to static formulations](https://arxiv.org/abs/2505.03670)
*Katy Craig,Nicolás García Trillos,Đorđe Nikolić*

Main category: math.AP

TL;DR: 论文提出了一种统一向量值最优传输理论的方法，涵盖动态和静态形式，并证明了四种距离的等价性。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于向量值测度分类和多物种偏微分方程的应用需求。

Method: 通过将向量值测度建模为乘积空间上的概率测度，结合加权图几何，分析动态和静态距离。

Result: 证明了四种向量值最优传输距离的尖锐不等式和双Hölder等价性。

Conclusion: 该框架为多物种偏微分方程和数据分析提供了理论支持，并展示了线性化技术的潜在应用。

Abstract: Motivated by applications in classification of vector valued measures and
multispecies PDE, we develop a theory that unifies existing notions of vector
valued optimal transport, from dynamic formulations (\`a la Benamou-Brenier) to
static formulations (\`a la Kantorovich). In our framework, vector valued
measures are modeled as probability measures on a product space $\mathbb{R}^d
\times G$, where $G$ is a weighted graph over a finite set of nodes and the
graph geometry strongly influences the associated dynamic and static distances.
We obtain sharp inequalities relating four notions of vector valued optimal
transport and prove that the distances are mutually bi-H\"older equivalent. We
discuss the theoretical and practical advantages of each metric and indicate
potential applications in multispecies PDE and data analysis. In particular,
one of the static formulations discussed in the paper is amenable to
linearization, a technique that has been explored in recent years to accelerate
the computation of pairwise optimal transport distances.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [228] [Robustly Invertible Nonlinear Dynamics and the BiLipREN: Contracting Neural Models with Contracting Inverses](https://arxiv.org/abs/2505.03069)
*Yurui Zhang,Ruigang Wang,Ian R. Manchester*

Main category: eess.SY

TL;DR: 论文提出了一种新的可逆循环神经网络模型BiLipREN，基于非线性动力系统的收缩性和增量稳定性分析，确保模型及其逆模型均为收缩且Lipschitz的。


<details>
  <summary>Details</summary>
Motivation: 研究非线性动力系统的可逆性，提出一种具有鲁棒性的可逆模型，以解决输入扰动和初始条件变化对逆模型重建的影响。

Method: 通过参数化神经动态模型（biLipREN），确保其构造上具有鲁棒可逆性，并可结合正交线性系统构建更通用的双Lipschitz动态模型。

Result: 提出的biLipREN模型在数值实验中展示了其鲁棒性和可逆性，能够有效重建输入序列。

Conclusion: biLipREN为非线性动力系统的可逆性提供了一种新的解决方案，具有广泛的应用潜力。

Abstract: We study the invertibility of nonlinear dynamical systems from the
perspective of contraction and incremental stability analysis and propose a new
invertible recurrent neural model: the BiLipREN. In particular, we consider a
nonlinear state space model to be robustly invertible if an inverse exists with
a state space realisation, and both the forward model and its inverse are
contracting, i.e. incrementally exponentially stable, and Lipschitz, i.e. have
bounded incremental gain. This property of bi-Lipschitzness implies both
robustness in the sense of sensitivity to input perturbations, as well as
robust distinguishability of different inputs from their corresponding outputs,
i.e. the inverse model robustly reconstructs the input sequence despite small
perturbations to the initial conditions and measured output. Building on this
foundation, we propose a parameterization of neural dynamic models:
bi-Lipschitz recurrent equilibrium networks (biLipREN), which are robustly
invertible by construction. Moreover, biLipRENs can be composed with orthogonal
linear systems to construct more general bi-Lipschitz dynamic models, e.g., a
nonlinear analogue of minimum-phase/all-pass (inner/outer) factorization. We
illustrate the utility of our proposed approach with numerical examples.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [229] [An Active Inference perspective on Neurofeedback Training](https://arxiv.org/abs/2505.03308)
*Côme Annicchiarico,Fabien Lotte,Jérémie Mattout*

Main category: q-bio.NC

TL;DR: 论文提出了一种基于主动推理的计算模型，用于分析神经反馈训练（NFT）的变异性，并探讨了反馈质量和先验信念对训练效果的影响。


<details>
  <summary>Details</summary>
Motivation: 神经反馈训练的效果存在高度变异性，且机制不明确，阻碍了其验证。本文旨在通过计算模型解决这些问题。

Method: 使用主动推理（一种贝叶斯框架）模拟代理与NFT环境的交互，测试反馈质量和先验信念等因素的影响。

Result: 模拟显示训练效果对反馈噪声、偏差及先验信念敏感，但完美反馈并不能保证高性能。

Conclusion: 该模型为评估NFT变异性、解释实验数据及开发个性化训练方案提供了工具。

Abstract: Neurofeedback training (NFT) aims to teach self-regulation of brain activity
through real-time feedback, but suffers from highly variable outcomes and
poorly understood mechanisms, hampering its validation. To address these
issues, we propose a formal computational model of the NFT closed loop. Using
Active Inference, a Bayesian framework modelling perception, action, and
learning, we simulate agents interacting with an NFT environment. This enables
us to test the impact of design choices (e.g., feedback quality, biomarker
validity) and subject factors (e.g., prior beliefs) on training. Simulations
show that training effectiveness is sensitive to feedback noise or bias, and to
prior beliefs (highlighting the importance of guiding instructions), but also
reveal that perfect feedback is insufficient to guarantee high performance.
This approach provides a tool for assessing and predicting NFT variability,
interpret empirical data, and potentially develop personalized training
protocols.

</details>


### [230] [Binding threshold units with artificial oscillatory neurons](https://arxiv.org/abs/2505.03648)
*Vladimir Fanaskov,Ivan Oseledets*

Main category: q-bio.NC

TL;DR: 论文提出了一种理论框架，区分振荡神经元与阈值单元，并建立了它们的耦合机制。振荡单元通过频率调制促进信息交换，而阈值单元模拟神经元放电强度。通过Lyapunov函数约束动力学，形成了Hopfield-Kuramoto关联记忆模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于明确振荡神经元与阈值单元的区别，并建立两者耦合的理论基础，以探索它们在神经编码中的不同作用。

Method: 方法包括：1）通过Lyapunov函数约束动力学；2）将Hopfield模型与广义Kuramoto模型耦合；3）提出低秩权重矩阵修正。

Result: 结果表明，振荡神经元可以作为Hopfield网络的低秩修正，实现类似Hebbian学习或LoRA方法的效果。

Conclusion: 结论是振荡神经元与阈值单元的耦合模型具有理论和实践潜力，尤其在关联记忆任务中表现优越。

Abstract: Artificial Kuramoto oscillatory neurons were recently introduced as an
alternative to threshold units. Empirical evidence suggests that oscillatory
units outperform threshold units in several tasks including unsupervised object
discovery and certain reasoning problems. The proposed coupling mechanism for
these oscillatory neurons is heterogeneous, combining a generalized Kuramoto
equation with standard coupling methods used for threshold units. In this
research note, we present a theoretical framework that clearly distinguishes
oscillatory neurons from threshold units and establishes a coupling mechanism
between them. We argue that, from a biological standpoint, oscillatory and
threshold units realise distinct aspects of neural coding: roughly, threshold
units model intensity of neuron firing, while oscillatory units facilitate
information exchange by frequency modulation. To derive interaction between
these two types of units, we constrain their dynamics by focusing on dynamical
systems that admit Lyapunov functions. For threshold units, this leads to
Hopfield associative memory model, and for oscillatory units it yields a
specific form of generalized Kuramoto model. The resulting dynamical systems
can be naturally coupled to form a Hopfield-Kuramoto associative memory model,
which also admits a Lyapunov function. Various forms of coupling are possible.
Notably, oscillatory neurons can be employed to implement a low-rank correction
to the weight matrix of a Hopfield network. This correction can be viewed
either as a form of Hebbian learning or as a popular LoRA method used for
fine-tuning of large language models. We demonstrate the practical realization
of this particular coupling through illustrative toy experiments.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [231] [GeoERM: Geometry-Aware Multi-Task Representation Learning on Riemannian Manifolds](https://arxiv.org/abs/2505.02972)
*Aoran Chen,Yang Feng*

Main category: stat.ML

TL;DR: GeoERM提出了一种基于黎曼流形的多任务学习框架，通过几何感知优化提升任务异构性和对抗性噪声下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有MTL方法通常忽略潜在表示矩阵的非欧几何特性，导致在任务异构或对抗性情况下鲁棒性不足。

Method: GeoERM将共享表示嵌入黎曼流形，通过黎曼梯度步和极坐标回缩优化，保持几何保真度。

Result: 在合成实验和可穿戴传感器活动识别基准测试中，GeoERM显著提升估计精度，减少负迁移，并在对抗性噪声下保持稳定。

Conclusion: GeoERM通过几何感知优化，在多任务学习中实现了更高的鲁棒性和性能提升。

Abstract: Multi-Task Learning (MTL) seeks to boost statistical power and learning
efficiency by discovering structure shared across related tasks.
State-of-the-art MTL representation methods, however, usually treat the latent
representation matrix as a point in ordinary Euclidean space, ignoring its
often non-Euclidean geometry, thus sacrificing robustness when tasks are
heterogeneous or even adversarial. We propose GeoERM, a geometry-aware MTL
framework that embeds the shared representation on its natural Riemannian
manifold and optimizes it via explicit manifold operations. Each training cycle
performs (i) a Riemannian gradient step that respects the intrinsic curvature
of the search space, followed by (ii) an efficient polar retraction to remain
on the manifold, guaranteeing geometric fidelity at every iteration. The
procedure applies to a broad class of matrix-factorized MTL models and retains
the same per-iteration cost as Euclidean baselines. Across a set of synthetic
experiments with task heterogeneity and on a wearable-sensor
activity-recognition benchmark, GeoERM consistently improves estimation
accuracy, reduces negative transfer, and remains stable under adversarial label
noise, outperforming leading MTL and single-task alternatives.

</details>


### [232] [Modeling Spatial Extremes using Non-Gaussian Spatial Autoregressive Models via Convolutional Neural Networks](https://arxiv.org/abs/2505.03034)
*Sweta Rai,Douglas W. Nychka,Soutir Bandyopadhyay*

Main category: stat.ML

TL;DR: 提出了一种空间自回归建模框架，用于处理具有空间异质性和重尾分布的大规模网格数据，结合广义极值分布创新项，并通过卷积神经网络快速估计参数。


<details>
  <summary>Details</summary>
Motivation: 解决大规模网格数据中空间异质性和重尾分布带来的建模挑战。

Method: 采用空间自回归模型（SAR）结合广义极值分布创新项，利用卷积神经网络进行参数估计。

Result: 模型能够有效捕捉极端空间行为，并快速估计参数。

Conclusion: 该框架适用于非高斯场，为极端空间行为分析提供了灵活且高效的解决方案。

Abstract: Data derived from remote sensing or numerical simulations often have a
regular gridded structure and are large in volume, making it challenging to
find accurate spatial models that can fill in missing grid cells or simulate
the process effectively, especially in the presence of spatial heterogeneity
and heavy-tailed marginal distributions. To overcome this issue, we present a
spatial autoregressive modeling framework, which maps observations at a
location and its neighbors to independent random variables. This is a highly
flexible modeling approach and well-suited for non-Gaussian fields, providing
simpler interpretability. In particular, we consider the SAR model with
Generalized Extreme Value distribution innovations to combine the observation
at a central grid location with its neighbors, capturing extreme spatial
behavior based on the heavy-tailed innovations. While these models are fast to
simulate by exploiting the sparsity of the key matrices in the computations,
the maximum likelihood estimation of the parameters is prohibitive due to the
intractability of the likelihood, making optimization challenging. To overcome
this, we train a convolutional neural network on a large training set that
covers a useful parameter space, and then use the trained network for fast
parameter estimation. Finally, we apply this model to analyze annual maximum
precipitation data from ERA-Interim-driven Weather Research and Forecasting
(WRF) simulations, allowing us to explore its spatial extreme behavior across
North America.

</details>


### [233] [A Symbolic and Statistical Learning Framework to Discover Bioprocessing Regulatory Mechanism: Cell Culture Example](https://arxiv.org/abs/2505.03177)
*Keilung Choy,Wei Xie,Keqi Wang*

Main category: stat.ML

TL;DR: 本文提出了一种结合符号与统计学习的框架，用于识别生物过程中的关键调控机制并量化模型不确定性，通过贝叶斯学习和高效算法提升了样本效率和模型选择鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 生物过程机理建模对智能数字孪生至关重要，但复杂调控、随机行为和有限数据带来挑战。

Method: 采用随机微分方程描述生物过程动态，结合贝叶斯学习和混合模型，开发了高效的后验探索算法。

Result: 相比现有方法，该框架在样本效率和模型选择上表现更优，能恢复缺失调控机制并提升模型保真度。

Conclusion: 该框架为数据有限条件下的生物过程建模提供了有效解决方案。

Abstract: Bioprocess mechanistic modeling is essential for advancing intelligent
digital twin representation of biomanufacturing, yet challenges persist due to
complex intracellular regulation, stochastic system behavior, and limited
experimental data. This paper introduces a symbolic and statistical learning
framework to identify key regulatory mechanisms and quantify model uncertainty.
Bioprocess dynamics is formulated with stochastic differential equations
characterizing intrinsic process variability, with a predefined set of
candidate regulatory mechanisms constructed from biological knowledge. A
Bayesian learning approach is developed, which is based on a joint learning of
kinetic parameters and regulatory structure through a formulation of the
mixture model. To enhance computational efficiency, a Metropolis-adjusted
Langevin algorithm with adjoint sensitivity analysis is developed for posterior
exploration. Compared to state-of-the-art Bayesian inference approaches, the
proposed framework achieves improved sample efficiency and robust model
selection. An empirical study demonstrates its ability to recover missing
regulatory mechanisms and improve model fidelity under data-limited conditions.

</details>


### [234] [Weighted Average Gradients for Feature Attribution](https://arxiv.org/abs/2505.03201)
*Kien Tran Duc Tuan,Tam Nguyen Trong,Son Nguyen Hoang,Khoat Than,Anh Nguyen Duc*

Main category: stat.ML

TL;DR: 论文提出了一种名为加权平均梯度（WG）的新方法，用于改进可解释AI中的基线选择问题，通过无监督评估基线适用性并选择有效基线，提升了稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统预期梯度（EG）方法假设基线可以均匀采样并等权重平均，但实际基线不应等同对待，因此需要更有效的基线选择策略。

Method: 引入WG方法，无监督评估基线适用性，并选择有效基线；理论分析验证其满足解释方法标准且更稳定。

Result: 实验表明WG在多种场景下优于EG，主要指标提升10-35%，同时能筛选有效基线以降低计算成本。

Conclusion: WG通过改进基线选择策略，显著提升了可解释AI的性能和效率，代码已开源。

Abstract: In explainable AI, Integrated Gradients (IG) is a widely adopted technique
for assessing the significance of feature attributes of the input on model
outputs by evaluating contributions from a baseline input to the current input.
The choice of the baseline input significantly influences the resulting
explanation. While the traditional Expected Gradients (EG) method assumes
baselines can be uniformly sampled and averaged with equal weights, this study
argues that baselines should not be treated equivalently. We introduce Weighted
Average Gradients (WG), a novel approach that unsupervisedly evaluates baseline
suitability and incorporates a strategy for selecting effective baselines.
Theoretical analysis demonstrates that WG satisfies essential explanation
method criteria and offers greater stability than prior approaches.
Experimental results further confirm that WG outperforms EG across diverse
scenarios, achieving an improvement of 10-35\% on main metrics. Moreover, by
evaluating baselines, our method can filter a subset of effective baselines for
each input to calculate explanations, maintaining high accuracy while reducing
computational cost. The code is available at:
https://github.com/Tamnt240904/weighted_baseline.

</details>


### [235] [Lower Bounds for Greedy Teaching Set Constructions](https://arxiv.org/abs/2505.03223)
*Spencer Compton,Chirag Pabbaraju,Nikita Zhivotovskiy*

Main category: stat.ML

TL;DR: 本文研究了学习理论中概念类的最小教学维度问题，探讨了贪婪算法的性能下限，并提出了对现有猜想的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决概念类的最小教学维度问题，特别是验证Simon和Zilles提出的递归教学维度猜想。

Method: 通过分析贪婪算法在小k值下的性能，证明其下限，并与现有上限进行比较。

Result: 证明对于k=1和k=2，贪婪算法的性能有限，且下限扩展到k≤⌈cd⌉，暗示需要更高阶交互研究。

Conclusion: 研究结果表明，现有贪婪算法可能不足以解决最小教学维度问题，需进一步探索高阶交互方法。

Abstract: A fundamental open problem in learning theory is to characterize the
best-case teaching dimension $\operatorname{TS}_{\min}$ of a concept class
$\mathcal{C}$ with finite VC dimension $d$. Resolving this problem will, in
particular, settle the conjectured upper bound on Recursive Teaching Dimension
posed by [Simon and Zilles; COLT 2015]. Prior work used a natural greedy
algorithm to construct teaching sets recursively, thereby proving upper bounds
on $\operatorname{TS}_{\min}$, with the best known bound being $O(d^2)$ [Hu,
Wu, Li, and Wang; COLT 2017]. In each iteration, this greedy algorithm chooses
to add to the teaching set the $k$ labeled points that restrict the concept
class the most. In this work, we prove lower bounds on the performance of this
greedy approach for small $k$. Specifically, we show that for $k = 1$, the
algorithm does not improve upon the halving-based bound of
$O(\log(|\mathcal{C}|))$. Furthermore, for $k = 2$, we complement the upper
bound of $O\left(\log(\log(|\mathcal{C}|))\right)$ from [Moran, Shpilka,
Wigderson, and Yuhudayoff; FOCS 2015] with a matching lower bound. Most
consequentially, our lower bound extends up to $k \le \lceil c d \rceil$ for
small constant $c>0$: suggesting that studying higher-order interactions may be
necessary to resolve the conjecture that $\operatorname{TS}_{\min} = O(d)$.

</details>


### [236] [Decision Making under Model Misspecification: DRO with Robust Bayesian Ambiguity Sets](https://arxiv.org/abs/2505.03585)
*Charita Dellaporta,Patrick O'Hara,Theodoros Damoulas*

Main category: stat.ML

TL;DR: DRO-RoBAS结合了分布鲁棒优化和贝叶斯方法，通过鲁棒后验预测分布解决模型误设问题，提升了样本外性能。


<details>
  <summary>Details</summary>
Motivation: 传统DRO方法在模型误设时可能导致过于保守的决策，需要一种新方法来平衡鲁棒性和性能。

Method: 提出DRO-RoBAS，使用基于最大均值差异的模糊集，结合鲁棒后验预测分布，并在再生核希尔伯特空间中推导对偶问题。

Result: 在Newsvendor和Portfolio问题上，DRO-RoBAS在模型误设情况下优于其他贝叶斯和实证DRO方法。

Conclusion: DRO-RoBAS有效解决了模型误设问题，提供了更好的决策鲁棒性和性能。

Abstract: Distributionally Robust Optimisation (DRO) protects risk-averse
decision-makers by considering the worst-case risk within an ambiguity set of
distributions based on the empirical distribution or a model. To further guard
against finite, noisy data, model-based approaches admit Bayesian formulations
that propagate uncertainty from the posterior to the decision-making problem.
However, when the model is misspecified, the decision maker must stretch the
ambiguity set to contain the data-generating process (DGP), leading to overly
conservative decisions. We address this challenge by introducing DRO with
Robust, to model misspecification, Bayesian Ambiguity Sets (DRO-RoBAS). These
are Maximum Mean Discrepancy ambiguity sets centred at a robust posterior
predictive distribution that incorporates beliefs about the DGP. We show that
the resulting optimisation problem obtains a dual formulation in the
Reproducing Kernel Hilbert Space and we give probabilistic guarantees on the
tolerance level of the ambiguity set. Our method outperforms other Bayesian and
empirical DRO approaches in out-of-sample performance on the Newsvendor and
Portfolio problems with various cases of model misspecification.

</details>


### [237] [Physics-Informed Sylvester Normalizing Flows for Bayesian Inference in Magnetic Resonance Spectroscopy](https://arxiv.org/abs/2505.03590)
*Julian P. Merkofer,Dennis M. J. van de Sande,Alex A. Bhogal,Ruud J. G. van Sloun*

Main category: stat.ML

TL;DR: 论文提出了一种基于贝叶斯推断和Sylvester归一化流（SNFs）的方法，用于提高磁共振波谱（MRS）中代谢物定量的可靠性。


<details>
  <summary>Details</summary>
Motivation: 磁共振波谱（MRS）在代谢物定量中面临谱重叠、低信噪比等问题，传统方法如线性组合模型存在局限性。

Method: 采用贝叶斯推断框架和Sylvester归一化流（SNFs）近似代谢物浓度的后验分布，并结合物理基础的解码器。

Result: 在模拟的7T质子MRS数据上验证，展示了准确的代谢物定量、校准良好的不确定性，以及对参数相关性和多模态分布的洞察。

Conclusion: 该方法显著提高了MRS代谢物定量的可靠性，为临床和研究提供了更准确的工具。

Abstract: Magnetic resonance spectroscopy (MRS) is a non-invasive technique to measure
the metabolic composition of tissues, offering valuable insights into
neurological disorders, tumor detection, and other metabolic dysfunctions.
However, accurate metabolite quantification is hindered by challenges such as
spectral overlap, low signal-to-noise ratio, and various artifacts. Traditional
methods like linear-combination modeling are susceptible to ambiguities and
commonly only provide a theoretical lower bound on estimation accuracy in the
form of the Cram\'er-Rao bound. This work introduces a Bayesian inference
framework using Sylvester normalizing flows (SNFs) to approximate posterior
distributions over metabolite concentrations, enhancing quantification
reliability. A physics-based decoder incorporates prior knowledge of MRS signal
formation, ensuring realistic distribution representations. We validate the
method on simulated 7T proton MRS data, demonstrating accurate metabolite
quantification, well-calibrated uncertainties, and insights into parameter
correlations and multi-modal distributions.

</details>


### [238] [Weighted Random Dot Product Graphs](https://arxiv.org/abs/2505.03649)
*Bernardo Marenco,Paola Bermolen,Marcelo Fiori,Federico Larroca,Gonzalo Mateos*

Main category: stat.ML

TL;DR: 本文扩展了随机点积图（RDPG）模型，提出了一种非参数加权（W）RDPG模型，适用于具有异质权重分布的加权图。


<details>
  <summary>Details</summary>
Motivation: 网络数据中的复杂关系模式分析是现代统计研究和数据科学的核心，但现有模型难以区分具有相同均值但高阶矩不同的权重分布。

Method: 提出WRDPG模型，通过节点潜在位置的内积指定边权重分布的矩生成函数，并推导了节点潜在位置估计器的统计保证。

Result: 证明了估计器的一致性和渐近正态性，并提出了生成符合WRDPG的图的框架。

Conclusion: WRDPG模型在多种网络分析应用中表现出色，扩展了RDPG的适用范围。

Abstract: Modeling of intricate relational patterns % through the analysis structures
of network data has become a cornerstone of contemporary statistical research
and related data science fields. Networks, represented as graphs, offer a
natural framework for this analysis. This paper extends the Random Dot Product
Graph (RDPG) model to accommodate weighted graphs, markedly broadening the
model's scope to scenarios where edges exhibit heterogeneous weight
distributions. We propose a nonparametric weighted (W)RDPG model that assigns a
sequence of latent positions to each node. Inner products of these nodal
vectors specify the moments of their incident edge weights' distribution via
moment-generating functions. In this way, and unlike prior art, the WRDPG can
discriminate between weight distributions that share the same mean but differ
in other higher-order moments. We derive statistical guarantees for an
estimator of the nodal's latent positions adapted from the workhorse adjacency
spectral embedding, establishing its consistency and asymptotic normality. We
also contribute a generative framework that enables sampling of graphs that
adhere to a (prescribed or data-fitted) WRDPG, facilitating, e.g., the analysis
and testing of observed graph metrics using judicious reference distributions.
The paper is organized to formalize the model's definition, the estimation (or
nodal embedding) process and its guarantees, as well as the methodologies for
generating weighted graphs, all complemented by illustrative and reproducible
examples showcasing the WRDPG's effectiveness in various network analytic
applications.

</details>


### [239] [Multi-modal cascade feature transfer for polymer property prediction](https://arxiv.org/abs/2505.03704)
*Kiichi Obuchi,Yuta Yahagi,Kiyohiko Toyama,Shukichi Tanaka,Kota Matsui*

Main category: stat.ML

TL;DR: 提出了一种多模态级联模型，结合图卷积神经网络（GCN）提取的特征与分子描述符等信息，用于聚合物性质预测，性能优于传统单特征方法。


<details>
  <summary>Details</summary>
Motivation: 聚合物数据多模态（如分子描述符、化学结构等），传统方法单独处理各模态数据，限制了预测精度。

Method: 结合GCN提取的化学结构特征与其他特征（如分子描述符），构建多模态级联模型。

Result: 在多个聚合物数据集上验证，预测性能显著优于传统单特征方法。

Conclusion: 多模态特征融合显著提升聚合物性质预测精度。

Abstract: In this paper, we propose a novel transfer learning approach called
multi-modal cascade model with feature transfer for polymer property
prediction.Polymers are characterized by a composite of data in several
different formats, including molecular descriptors and additive information as
well as chemical structures. However, in conventional approaches, prediction
models were often constructed using each type of data separately. Our model
enables more accurate prediction of physical properties for polymers by
combining features extracted from the chemical structure by graph convolutional
neural networks (GCN) with features such as molecular descriptors and additive
information. The predictive performance of the proposed method is empirically
evaluated using several polymer datasets. We report that the proposed method
shows high predictive performance compared to the baseline conventional
approach using a single feature.

</details>


### [240] [Actor-Critics Can Achieve Optimal Sample Efficiency](https://arxiv.org/abs/2505.03710)
*Kevin Tan,Wei Fan,Yuting Wei*

Main category: stat.ML

TL;DR: 提出一种新的actor-critic算法，实现了O(dH^5 log|A|/ε² + dH^4 log|F|/ε²)的样本复杂度，并扩展到混合RL和离线数据场景。


<details>
  <summary>Details</summary>
Motivation: 解决现有actor-critic算法在战略探索和通用函数逼近下无法达到O(1/ε²)样本复杂度的问题。

Method: 结合乐观策略、离策略critic估计和罕见切换策略重置，并扩展到混合RL和离线数据初始化。

Result: 实现了理论上的高效样本复杂度，并通过数值实验验证。

Conclusion: 新算法填补了文献中的空白，并在混合RL和离线数据场景中表现出优势。

Abstract: Actor-critic algorithms have become a cornerstone in reinforcement learning
(RL), leveraging the strengths of both policy-based and value-based methods.
Despite recent progress in understanding their statistical efficiency, no
existing work has successfully learned an $\epsilon$-optimal policy with a
sample complexity of $O(1/\epsilon^2)$ trajectories with general function
approximation when strategic exploration is necessary.
  We address this open problem by introducing a novel actor-critic algorithm
that attains a sample-complexity of $O(dH^5 \log|\mathcal{A}|/\epsilon^2 + d
H^4 \log|\mathcal{F}|/ \epsilon^2)$ trajectories, and accompanying $\sqrt{T}$
regret when the Bellman eluder dimension $d$ does not increase with $T$ at more
than a $\log T$ rate.
  Here, $\mathcal{F}$ is the critic function class, $\mathcal{A}$ is the action
space, and $H$ is the horizon in the finite horizon MDP setting. Our algorithm
integrates optimism, off-policy critic estimation targeting the optimal
Q-function, and rare-switching policy resets.
  We extend this to the setting of Hybrid RL, showing that initializing the
critic with offline data yields sample efficiency gains compared to purely
offline or online RL. Further, utilizing access to offline data, we provide a
\textit{non-optimistic} provably efficient actor-critic algorithm that only
additionally requires $N_{\text{off}} \geq c_{\text{off}}^*dH^4/\epsilon^2$ in
exchange for omitting optimism, where $c_{\text{off}}^*$ is the single-policy
concentrability coefficient and $N_{\text{off}}$ is the number of offline
samples. This addresses another open problem in the literature. We further
provide numerical experiments to support our theoretical findings.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [241] [SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation](https://arxiv.org/abs/2505.03273)
*Zhaoxi Mu,Xinyu Yang,Gang Wang*

Main category: cs.SD

TL;DR: SepALM是一种结合音频语言模型（ALM）的语音分离方法，通过纠正和重合成语音，提升分离精度和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有语音分离技术在复杂真实环境中表现不佳，容易产生失真或伪影。

Method: SepALM包含分离器、纠正器、合成器和对齐器，结合ALM端到端纠错机制，避免传统方法的优化问题。

Result: 实验证明SepALM提高了语音分离精度，增强了在新声学环境中的适应性。

Conclusion: SepALM为复杂环境下的语音分离提供了更优解决方案。

Abstract: While contemporary speech separation technologies adeptly process lengthy
mixed audio waveforms, they are frequently challenged by the intricacies of
real-world environments, including noisy and reverberant settings, which can
result in artifacts or distortions in the separated speech. To overcome these
limitations, we introduce SepALM, a pioneering approach that employs audio
language models (ALMs) to rectify and re-synthesize speech within the text
domain following preliminary separation. SepALM comprises four core components:
a separator, a corrector, a synthesizer, and an aligner. By integrating an
ALM-based end-to-end error correction mechanism, we mitigate the risk of error
accumulation and circumvent the optimization hurdles typically encountered in
conventional methods that amalgamate automatic speech recognition (ASR) with
large language models (LLMs). Additionally, we have developed Chain-of-Thought
(CoT) prompting and knowledge distillation techniques to facilitate the
reasoning and training processes of the ALM. Our experiments substantiate that
SepALM not only elevates the precision of speech separation but also markedly
bolsters adaptability in novel acoustic environments.

</details>


### [242] [A study on audio synchronous steganography detection and distributed guide inference model based on sliding spectral features and intelligent inference drive](https://arxiv.org/abs/2505.03193)
*Wei Meng*

Main category: cs.SD

TL;DR: 本文提出了一种基于短时傅里叶变换的滑动频谱特征提取方法，用于检测短视频平台中的同步隐写数据，并构建了分布式指令重建模型。


<details>
  <summary>Details</summary>
Motivation: 随着短视频平台的兴起，音频同步流中的隐写数据成为新的隐蔽通信方式，传统检测技术存在局限性。

Method: 采用25毫秒滑动窗口和短时傅里叶变换提取主频轨迹，构建同步帧检测模型（M1）和结构化解码模型（M2）。

Result: 在36至45秒音频段中发现低熵重复字节序列和高集中频谱能量，验证了同步帧的存在，并推断出军事通信协议特征。

Conclusion: 该方法验证了滑动频谱特征在同步隐写检测中的有效性，并为开放平台上的隐蔽通信分析和战术模拟提供了可扩展模型。

Abstract: With the rise of short video platforms in global communication, embedding
steganographic data in audio synchronization streams has emerged as a new
covert communication method. To address the limitations of traditional
techniques in detecting synchronized steganography, this paper proposes a
detection and distributed guidance reconstruction model based on short video
"Yupan" samples released by China's South Sea Fleet on TikTok. The method
integrates sliding spectrum feature extraction and intelligent inference
mechanisms. A 25 ms sliding window with short-time Fourier transform (STFT) is
used to extract the main frequency trajectory and construct the synchronization
frame detection model (M1), identifying a frame flag "FFFFFFFFFFFFFFFFFF80".
The subsequent 32-byte payload is decoded by a structured model (M2) to infer
distributed guidance commands. Analysis reveals a low-entropy, repetitive byte
sequence in the 36 to 45 second audio segment with highly concentrated spectral
energy, confirming the presence of synchronization frames. Although plaintext
semantics are not restored, the consistency in command field layout suggests
features of military communication protocols. The multi-segment splicing model
further shows cross-video embedding and centralized decoding capabilities. The
proposed framework validates the effectiveness of sliding spectral features for
synchronized steganography detection and builds an extensible inference model
for covert communication analysis and tactical guidance simulation on open
platforms.

</details>


### [243] [Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic Music Generation](https://arxiv.org/abs/2505.03314)
*Jincheng Zhang,György Fazekas,Charalampos Saitis*

Main category: cs.SD

TL;DR: 该论文提出了一种基于扩散模型的符号音乐生成方法，通过将音乐表示为图像形式的钢琴卷帘，并结合Transformer-Mamba块和可学习小波变换，实现了高质量和可控的音乐生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像合成中表现出色，但在符号音乐生成领域应用较少，主要因为符号音乐是离散数据。本文旨在探索扩散模型在符号音乐生成中的潜力。

Method: 将符号音乐表示为钢琴卷帘，提出结合Transformer-Mamba块和可学习小波变换的扩散模型，并使用无分类器指导生成目标和弦的音乐。

Result: 实验表明，该方法在音乐质量和可控性方面表现优异，优于基线模型。

Conclusion: 通过图像化表示和新型扩散模型，成功实现了符号音乐的高质量生成，为扩散模型在离散数据领域的应用提供了新思路。

Abstract: The recent surge in the popularity of diffusion models for image synthesis
has attracted new attention to their potential for generation tasks in other
domains. However, their applications to symbolic music generation remain
largely under-explored because symbolic music is typically represented as
sequences of discrete events and standard diffusion models are not well-suited
for discrete data. We represent symbolic music as image-like pianorolls,
facilitating the use of diffusion models for the generation of symbolic music.
Moreover, this study introduces a novel diffusion model that incorporates our
proposed Transformer-Mamba block and learnable wavelet transform.
Classifier-free guidance is utilised to generate symbolic music with target
chords. Our evaluation shows that our method achieves compelling results in
terms of music quality and controllability, outperforming the strong baseline
in pianoroll generation. Our code is available at
https://github.com/jinchengzhanggg/proffusion.

</details>


### [244] [Knowledge Distillation for Speech Denoising by Latent Representation Alignment with Cosine Distance](https://arxiv.org/abs/2505.03442)
*Diep Luong,Mikko Heikkinen,Konstantinos Drossos,Tuomas Virtanen*

Main category: cs.SD

TL;DR: 论文提出了一种基于知识蒸馏的语音去噪方法，通过利用去噪自编码器框架、线性倒置瓶颈和余弦相似性特性，解决了现有方法中学生模型受限于教师模型分布和特征维度的问题。实验表明，该方法在性能和学生与教师模型不匹配条件下的表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有语音去噪方法在低资源计算环境中部署复杂，且知识蒸馏方法中学生的学习受限于教师模型的分布和特征维度。

Method: 利用去噪自编码器框架、线性倒置瓶颈和余弦相似性特性，设计了一种新的知识蒸馏方法。

Result: 实验结果显示，该方法在性能和学生与教师模型不匹配条件下的表现优于现有方法。

Conclusion: 提出的方法在语音去噪任务中表现优异，尤其在学生模型与教师模型不匹配时仍能保持良好性能。

Abstract: Speech denoising is a generally adopted and impactful task, appearing in many
common and everyday-life use cases. Although there are very powerful methods
published, most of those are too complex for deployment in everyday and
low-resources computational environments, like hand-held devices, intelligent
glasses, hearing aids, etc. Knowledge distillation (KD) is a prominent way for
alleviating this complexity mismatch and is based on the
transferring/distilling of knowledge from a pre-trained complex model, the
teacher, to another less complex one, the student. Existing KD methods for
speech denoising are based on processes that potentially hamper the KD by
bounding the learning of the student to the distribution, information ordering,
and feature dimensionality learned by the teacher. In this paper, we present
and assess a method that tries to treat this issue, by exploiting the
well-known denoising-autoencoder framework, the linear inverted bottlenecks,
and the properties of the cosine similarity. We use a public dataset and
conduct repeated experiments with different mismatching scenarios between the
teacher and the student, reporting the mean and standard deviation of the
metrics of our method and another, state-of-the-art method that is used as a
baseline. Our results show that with the proposed method, the student can
perform better and can also retain greater mismatching conditions compared to
the teacher.

</details>


### [245] [CoGenAV: Versatile Audio-Visual Representation Learning via Contrastive-Generative Synchronization](https://arxiv.org/abs/2505.03186)
*Detao Bai,Zhiheng Ma,Xihan Wei,Liefeng Bo*

Main category: cs.SD

TL;DR: CoGenAV模型通过结合对比性特征对齐和生成性文本预测，利用少量标注数据学习多功能音频-视觉表示，在多项任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 利用说话者的唇部动作、声音和语言内容的同步性，提升语音处理任务性能，尤其是在传统音频系统表现不佳的环境中。

Method: CoGenAV通过优化双重目标（对比性特征对齐和生成性文本预测）训练，仅使用223小时的LRS2数据集。

Result: 在AVSR任务中达到1.27%的WER，VSR任务中22.0%的WER，噪声环境下性能提升70%以上，并在其他任务中表现优异。

Conclusion: CoGenAV的多功能表示在多种任务中表现卓越，模型将开源以促进学术和工业界的进一步发展。

Abstract: The inherent synchronization between a speaker's lip movements, voice, and
the underlying linguistic content offers a rich source of information for
improving speech processing tasks, especially in challenging conditions where
traditional audio-only systems falter. We introduce CoGenAV, a powerful and
data-efficient model designed to learn versatile audio-visual representations
applicable across a wide range of speech and audio-visual tasks. CoGenAV is
trained by optimizing a dual objective derived from natural audio-visual
synchrony, contrastive feature alignment and generative text prediction, using
only 223 hours of labeled data from the LRS2 dataset. This
contrastive-generative synchronization strategy effectively captures
fundamental cross-modal correlations. We showcase the effectiveness and
versatility of the learned CoGenAV representations on multiple benchmarks. When
utilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these
representations contribute to achieving a state-of-the-art Word Error Rate
(WER) of 1.27. They also enable strong performance in Visual Speech Recognition
(VSR) with a WER of 22.0 on LRS2, and significantly improve performance in
noisy environments by over 70%. Furthermore, CoGenAV representations benefit
speech reconstruction tasks, boosting performance in Speech Enhancement and
Separation, and achieve competitive results in audio-visual synchronization
tasks like Active Speaker Detection (ASD). Our model will be open-sourced to
facilitate further development and collaboration within both academia and
industry.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [246] [Mitigating Image Captioning Hallucinations in Vision-Language Models](https://arxiv.org/abs/2505.03420)
*Fei Zhao,Chengcui Zhang,Runlin Zhang,Tianyang Wang,Xi Li*

Main category: cs.MM

TL;DR: 提出了一种基于强化学习的测试时适应框架，减少视觉语言模型（VLM）中的幻觉现象，无需重新训练或额外模型。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型中的幻觉问题影响可靠性和实际应用，现有方法计算成本高或需要额外资源。

Method: 通过更新语言模型中的层归一化参数（约0.003%参数），利用CLIP评估模型提供双重奖励。

Result: 在LLaVA和InstructBLIP上分别减少15.4%和17.3%的幻觉率，优于现有方法68.3%。

Conclusion: 该方法高效且低成本，显著减少了VLM中的幻觉问题。

Abstract: Hallucinations in vision-language models (VLMs) hinder reliability and
real-world applicability, usually stemming from distribution shifts between
pretraining data and test samples. Existing solutions, such as retraining or
fine-tuning on additional data, demand significant computational resources and
labor-intensive data collection, while ensemble-based methods incur additional
costs by introducing auxiliary VLMs. To address these challenges, we propose a
novel test-time adaptation framework using reinforcement learning to mitigate
hallucinations during inference without retraining or any auxiliary VLMs. By
updating only the learnable parameters in the layer normalization of the
language model (approximately 0.003% of the model parameters), our method
reduces distribution shifts between test samples and pretraining samples. A
CLIP-based hallucination evaluation model is proposed to provide dual rewards
to VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in
hallucination rates on LLaVA and InstructBLIP, respectively. Our approach
outperforms state-of-the-art baselines with a 68.3% improvement in
hallucination mitigation, demonstrating its effectiveness.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [247] [HMAE: Self-Supervised Few-Shot Learning for Quantum Spin Systems](https://arxiv.org/abs/2505.03140)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: quant-ph

TL;DR: HMAE是一种自监督框架，通过物理信息掩码预训练Transformer，显著提升量子系统的少样本学习性能。


<details>
  <summary>Details</summary>
Motivation: 解决量子机器学习中标记数据稀缺和模拟计算成本高的问题。

Method: 提出Hamiltonian-Masked Autoencoding (HMAE)，基于量子信息理论选择性掩码哈密顿项。

Result: 在12,500个量子哈密顿量上，HMAE在相分类和基态能量预测中表现优于基线方法。

Conclusion: HMAE在小量子系统中表现出色，但规模限制阻碍了其在更大系统中的应用。

Abstract: Quantum machine learning for spin and molecular systems faces critical
challenges of scarce labeled data and computationally expensive simulations. To
address these limitations, we introduce Hamiltonian-Masked Autoencoding (HMAE),
a novel self-supervised framework that pre-trains transformers on unlabeled
quantum Hamiltonians, enabling efficient few-shot transfer learning. Unlike
random masking approaches, HMAE employs a physics-informed strategy based on
quantum information theory to selectively mask Hamiltonian terms based on their
physical significance. Experiments on 12,500 quantum Hamiltonians (60%
real-world, 40% synthetic) demonstrate that HMAE achieves 85.3% $\pm$ 1.5%
accuracy in phase classification and 0.15 $\pm$ 0.02 eV MAE in ground state
energy prediction with merely 10 labeled examples - a statistically significant
improvement (p < 0.01) over classical graph neural networks (78.1% $\pm$ 2.1%)
and quantum neural networks (76.8% $\pm$ 2.3%). Our method's primary advantage
is exceptional sample efficiency - reducing required labeled examples by 3-5x
compared to baseline methods - though we emphasize that ground truth values for
fine-tuning and evaluation still require exact diagonalization or tensor
networks. We explicitly acknowledge that our current approach is limited to
small quantum systems (specifically limited to 12 qubits during training, with
limited extension to 16-20 qubits in testing) and that, while promising within
this regime, this size restriction prevents immediate application to larger
systems of practical interest in materials science and quantum chemistry.

</details>


### [248] [Quantum Feature Space of a Qubit Coupled to an Arbitrary Bath](https://arxiv.org/abs/2505.03397)
*Chris Wise,Akram Youssry,Alberto Peruzzo,Jo Plested,Matt Woolley*

Main category: quant-ph

TL;DR: 论文提出了一种高效参数化方法替代传统神经网络，用于描述量子比特动态中的噪声操作，并定义了量子特征空间，用于噪声分类。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖复杂神经网络和物理编码层，难以扩展和实时操作，因此寻求更高效的参数化方法。

Method: 利用量子特征空间参数化噪声操作，并通过欧几里得距离和随机森林算法分类噪声过程。

Result: 量子特征空间能有效分类噪声的平稳性和类型，并探索控制脉冲参数与特征空间的映射关系。

Conclusion: 量子特征空间提供了一种高效且可扩展的噪声分类方法，简化了传统复杂模型。

Abstract: Qubit control protocols have traditionally leveraged a characterisation of
the qubit-bath coupling via its power spectral density. Previous work proposed
the inference of noise operators that characterise the influence of a classical
bath using a grey-box approach that combines deep neural networks with
physics-encoded layers. This overall structure is complex and poses challenges
in scaling and real-time operations. Here, we show that no expensive neural
networks are needed and that this noise operator description admits an
efficient parameterisation. We refer to the resulting parameter space as the
\textit{quantum feature space} of the qubit dynamics resulting from the coupled
bath. We show that the Euclidean distance defined over the quantum feature
space provides an effective method for classifying noise processes in the
presence of a given set of controls. Using the quantum feature space as the
input space for a simple machine learning algorithm (random forest, in this
case), we demonstrate that it can effectively classify the stationarity and the
broad class of noise processes perturbing a qubit. Finally, we explore how
control pulse parameters map to the quantum feature space.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [249] [Nonnegative Low-rank Matrix Recovery Can Have Spurious Local Minima](https://arxiv.org/abs/2505.03717)
*Richard Y. Zhang*

Main category: math.OC

TL;DR: 论文研究了低秩矩阵恢复问题在非负约束下是否仍具有良性非凸性，发现完全观测时成立，但部分观测时即使RIP常数趋近于0也不成立。


<details>
  <summary>Details</summary>
Motivation: 探讨非负约束是否影响低秩矩阵恢复的良性非凸性。

Method: 在秩1非负矩阵的简单设置下，分析完全观测和部分观测两种情况。

Result: 完全观测时良性非凸性成立（RIP常数δ=0），部分观测时即使δ趋近于0也不成立。

Conclusion: 非负约束导致低秩矩阵恢复的连续性理论失效，揭示了理论上的关键缺口。

Abstract: The classical low-rank matrix recovery problem is well-known to exhibit
\emph{benign nonconvexity} under the restricted isometry property (RIP): local
optimization is guaranteed to converge to the global optimum, where the ground
truth is recovered. We investigate whether benign nonconvexity continues to
hold when the factor matrices are constrained to be elementwise nonnegative --
a common practical requirement. In the simple setting of a rank-1 nonnegative
ground truth, we confirm that benign nonconvexity holds in the fully-observed
case with RIP constant $\delta=0$. Surprisingly, however, this property fails
to extend to the partially-observed case with any arbitrarily small RIP
constant $\delta\to0^{+}$, irrespective of rank overparameterization. This
finding exposes a critical theoretical gap: the continuity argument widely used
to explain the empirical robustness of low-rank matrix recovery fundamentally
breaks down once nonnegative constraints are imposed.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [250] [Single-Sample and Robust Online Resource Allocation](https://arxiv.org/abs/2505.02963)
*Rohan Ghuge,Sahil Singla,Yifan Wang*

Main category: cs.DS

TL;DR: 本文提出了一种新颖的指数定价算法，用于在线资源分配问题，仅需单个样本即可实现(1-ε)近似，并具有鲁棒性和激励兼容性。


<details>
  <summary>Details</summary>
Motivation: 研究在线资源分配问题的可学习性和鲁棒性，解决现有方法需要完全分布知识或局限于特定假设的局限性。

Method: 提出指数定价算法，通过资源价格的指数调整实现资源分配，确保资源不会耗尽。

Result: 算法在单样本下实现(1-ε)近似，并在异常值和价值增强模型中保持鲁棒性。

Conclusion: 指数定价算法为在线资源分配提供了一种高效且鲁棒的解决方案，解决了现有方法的局限性。

Abstract: Online Resource Allocation problem is a central problem in many areas of
Computer Science, Operations Research, and Economics. In this problem, we
sequentially receive $n$ stochastic requests for $m$ kinds of shared resources,
where each request can be satisfied in multiple ways, consuming different
amounts of resources and generating different values. The goal is to achieve a
$(1-\epsilon)$-approximation to the hindsight optimum, where $\epsilon>0$ is a
small constant, assuming each resource has a large budget.
  In this paper, we investigate the learnability and robustness of online
resource allocation. Our primary contribution is a novel Exponential Pricing
algorithm with the following properties: 1. It requires only a \emph{single
sample} from each of the $n$ request distributions to achieve a
$(1-\epsilon)$-approximation for online resource allocation with large budgets.
Such an algorithm was previously unknown, even with access to polynomially many
samples, as prior work either assumed full distributional knowledge or was
limited to i.i.d.\,or random-order arrivals. 2. It is robust to corruptions in
the outliers model and the value augmentation model. Specifically, it maintains
its $(1 - \epsilon)$-approximation guarantee under both these robustness
models, resolving the open question posed in Argue, Gupta, Molinaro, and Singla
(SODA'22). 3. It operates as a simple item-pricing algorithm that ensures
incentive compatibility.
  The intuition behind our Exponential Pricing algorithm is that the price of a
resource should adjust exponentially as it is overused or underused. It differs
from conventional approaches that use an online learning algorithm for item
pricing. This departure guarantees that the algorithm will never run out of any
resource, but loses the usual no-regret properties of online learning
algorithms, necessitating a new analytical approach.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [251] [Taskmaster Deconstructed: A Quantitative Look at Tension, Volatility, and Viewer Ratings](https://arxiv.org/abs/2505.02886)
*David H. Silver*

Main category: physics.soc-ph

TL;DR: 对英国电视节目《Taskmaster》的统计分析表明，评分动态对观众参与度无显著影响，观众兴趣更多由选手行为决定。


<details>
  <summary>Details</summary>
Motivation: 探讨《Taskmaster》的评分动态是否对观众参与度有实际影响。

Method: 对18季162集的15项指标进行统计分析，包括排名波动、分数差距等。

Result: 评分指标与IMDb评分无显著关联；长期趋势显示平均分上升，波动性略降。

Conclusion: 观众兴趣主要由选手行为驱动，而非游戏机制。

Abstract: Taskmaster is a British television show that combines comedic performance
with a formal scoring system. Despite the appearance of structured competition,
it remains unclear whether scoring dynamics contribute meaningfully to audience
engagement. We conducted a statistical analysis of 162 episodes across 18
series, using fifteen episode-level metrics to quantify rank volatility, point
spread, lead changes, and winner dominance. None of these metrics showed a
significant association with IMDb ratings, even after controlling for series
effects. Long-term trends suggest that average points have increased over time,
while volatility has slightly declined and rank spread has remained stable.
These patterns indicate an attempt to enhance competitive visibility without
altering the show's structural equilibrium. We also analyzed contestant rank
trajectories and identified five recurring archetypes describing performance
styles. These patterns suggest that viewer interest is shaped more by
contestant behavior than by game mechanics.

</details>


### [252] [Floating Car Observers in Intelligent Transportation Systems: Detection Modeling and Temporal Insights](https://arxiv.org/abs/2505.02845)
*Jeremias Gerner,Klaus Bogenberger,Stefanie Schmidtner*

Main category: physics.soc-ph

TL;DR: 论文探讨了Floating Car Observers (FCOs)在微观交通模拟中的建模方法，评估其对智能交通系统的潜力。通过神经网络仿真技术，研究了FCO数据在SUMO交通网络数字孪生中的应用效果。


<details>
  <summary>Details</summary>
Motivation: 传统Floating Car Data (FCD)数据有限，FCOs通过集成车载传感器提供更丰富的交通数据，为智能交通系统提供更全面的支持。

Method: 研究采用了多种建模方法，包括2D光线追踪和高保真协同仿真，并引入神经网络仿真技术。在SUMO中模拟FCO数据，评估其效果。

Result: 实验表明，即使在20%的渗透率下，FCOs能识别65%的车辆；结合时间数据后，可恢复80%的车辆，位置偏差极小。

Conclusion: FCOs在智能交通系统中具有显著潜力，尤其在交通状态估计和监控方面，适用于不同渗透率和交通条件。

Abstract: Floating Car Observers (FCOs) extend traditional Floating Car Data (FCD) by
integrating onboard sensors to detect and localize other traffic participants,
providing richer and more detailed traffic data. In this work, we explore
various modeling approaches for FCO detections within microscopic traffic
simulations to evaluate their potential for Intelligent Transportation System
(ITS) applications. These approaches range from 2D raytracing to high-fidelity
co-simulations that emulate real-world sensors and integrate 3D object
detection algorithms to closely replicate FCO detections. Additionally, we
introduce a neural network-based emulation technique that effectively
approximates the results of high-fidelity co-simulations. This approach
captures the unique characteristics of FCO detections while offering a fast and
scalable solution for modeling. Using this emulation method, we investigate the
impact of FCO data in a digital twin of a traffic network modeled in SUMO.
Results demonstrate that even at a 20% penetration rate, FCOs using LiDAR-based
detections can identify 65% of vehicles across various intersections and
traffic demand scenarios. Further potential emerges when temporal insights are
integrated, enabling the recovery of previously detected but currently unseen
vehicles. By employing data-driven methods, we recover over 80% of these
vehicles with minimal positional deviations. These findings underscore the
potential of FCOs for ITS, particularly in enhancing traffic state estimation
and monitoring under varying penetration rates and traffic conditions.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [253] [The Precautionary Principle and the Innovation Principle: Incompatible Guides for AI Innovation Governance?](https://arxiv.org/abs/2505.02846)
*Kim Kaivanto*

Main category: cs.CY

TL;DR: 论文探讨了AI治理中预防原则（PP）和创新原则（IP）的关系，指出在弱形式下两者并非完全对立，而是可以通过信号检测理论（SDT）模型找到最优策略。


<details>
  <summary>Details</summary>
Motivation: 研究AI治理中PP与IP是否对立，以及如何通过弱形式实现平衡。

Method: 采用信号检测理论（SDT）模型分析弱PP和IP的决策成本，提出‘等待与监控’策略及监管沙盒工具。

Result: 弱PP和IP在特定成本比例下可共存，监管沙盒有助于动态调整策略。

Conclusion: 弱PP和IP并非对立，通过动态学习和调整可实现AI治理的平衡。

Abstract: In policy debates concerning the governance and regulation of Artificial
Intelligence (AI), both the Precautionary Principle (PP) and the Innovation
Principle (IP) are advocated by their respective interest groups. Do these
principles offer wholly incompatible and contradictory guidance? Does one
necessarily negate the other? I argue here that provided attention is
restricted to weak-form PP and IP, the answer to both of these questions is
"No." The essence of these weak formulations is the requirement to fully
account for type-I error costs arising from erroneously preventing the
innovation's diffusion through society (i.e. mistaken regulatory red-lighting)
as well as the type-II error costs arising from erroneously allowing the
innovation to diffuse through society (i.e. mistaken regulatory
green-lighting). Within the Signal Detection Theory (SDT) model developed here,
weak-PP red-light (weak-IP green-light) determinations are optimal for
sufficiently small (large) ratios of expected type-I to type-II error costs.
For intermediate expected cost ratios, an amber-light 'wait-and-monitor' policy
is optimal. Regulatory sandbox instruments allow AI testing and experimentation
to take place within a structured environment of limited duration and societal
scale, whereby the expected cost ratio falls within the 'wait-and-monitor'
range. Through sandboxing regulators and innovating firms learn more about the
expected cost ratio, and what respective adaptations -- of regulation, of
technical solution, of business model, or combination thereof, if any -- are
needed to keep the ratio out of the weak-PP red-light zone.

</details>


### [254] [Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration](https://arxiv.org/abs/2505.02848)
*Kexin Ding,Mu Zhou,Akshay Chaudhari,Shaoting Zhang,Dimitris N. Metaxas*

Main category: cs.CY

TL;DR: 本文探讨了大型语言模型（LLMs）在医疗领域中的对齐问题，强调了医疗利益相关者与模型输出之间对齐的重要性，并提出了实现对齐的方法与工具。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗领域的广泛应用需要与医疗利益相关者的知识、需求和价值观对齐，以确保其有效、安全和负责任地支持医疗工作流程。

Method: 通过医疗利益相关者参与LLMs的整个生命周期（包括数据整理、模型训练和推理），并结合知识整合、任务理解和人工指导，实现对齐。

Result: 研究表明，通过上述方法，LLMs能更好地遵循人类价值观，提升医疗应用的信任度。

Conclusion: 未来需进一步优化人类与LLMs的对齐，以构建可信赖的医疗应用。

Abstract: The wide exploration of large language models (LLMs) raises the awareness of
alignment between healthcare stakeholder preferences and model outputs. This
alignment becomes a crucial foundation to empower the healthcare workflow
effectively, safely, and responsibly. Yet the varying behaviors of LLMs may not
always match with healthcare stakeholders' knowledge, demands, and values. To
enable a human-AI alignment, healthcare stakeholders will need to perform
essential roles in guiding and enhancing the performance of LLMs. Human
professionals must participate in the entire life cycle of adopting LLM in
healthcare, including training data curation, model training, and inference. In
this review, we discuss the approaches, tools, and applications of alignments
between healthcare stakeholders and LLMs. We demonstrate that LLMs can better
follow human values by properly enhancing healthcare knowledge integration,
task understanding, and human guidance. We provide outlooks on enhancing the
alignment between humans and LLMs to build trustworthy real-world healthcare
applications.

</details>


### [255] [Enhancing tutoring systems by leveraging tailored promptings and domain knowledge with Large Language Models](https://arxiv.org/abs/2505.02849)
*Mohsen Balavar,Wenli Yang,David Herbert,Soonja Yeom*

Main category: cs.CY

TL;DR: 研究探讨了AI驱动的个性化学习工具在计算机科学编程中的应用，通过RAG技术改进LLM的提示工程，提供技能对齐的反馈。


<details>
  <summary>Details</summary>
Motivation: 尽管AI工具如ITS和ChatGPT已提升学习体验，但在适应多样化学习风格和提供实时反馈方面仍有挑战。

Method: 整合RAG技术到LLM提示工程中，开发个性化辅导应用，并通过编程任务的定量指标评估系统。

Result: 系统成功将学生按技能水平分类，并提供情境感知反馈，效果优于通用方法。

Conclusion: 研究表明，技能对齐的反馈方法在个性化学习中更具效果和适应性。

Abstract: Recent advancements in artificial intelligence (AI) and machine learning have
reignited interest in their impact on Computer-based Learning (CBL). AI-driven
tools like ChatGPT and Intelligent Tutoring Systems (ITS) have enhanced
learning experiences through personalisation and flexibility. ITSs can adapt to
individual learning needs and provide customised feedback based on a student's
performance, cognitive state, and learning path. Despite these advances,
challenges remain in accommodating diverse learning styles and delivering
real-time, context-aware feedback. Our research aims to address these gaps by
integrating skill-aligned feedback via Retrieval Augmented Generation (RAG)
into prompt engineering for Large Language Models (LLMs) and developing an
application to enhance learning through personalised tutoring in a computer
science programming context. The pilot study evaluated a proposed system using
three quantitative metrics: readability score, response time, and feedback
depth, across three programming tasks of varying complexity. The system
successfully sorted simulated students into three skill-level categories and
provided context-aware feedback. This targeted approach demonstrated better
effectiveness and adaptability compared to general methods.

</details>


### [256] [A Computational Model of Inclusive Pedagogy: From Understanding to Application](https://arxiv.org/abs/2505.02853)
*Francesco Balzan,Pedro P. Santos,Maurizio Gabbrielli,Mahault Albarracin,Manuel Lopes*

Main category: cs.CY

TL;DR: 论文提出了一种计算模型，用于模拟师生互动中的共同适应动态，并通过实验验证了双向策略的优越性。


<details>
  <summary>Details</summary>
Motivation: 当前计算模型对师生共同适应动态的研究不足，限制了教育科学的测试和扩展能力，以及机器学习系统对人类学习过程的模拟和支持。

Method: 开发了一个计算模型，将人类教育的上下文洞察整合到可测试框架中，并在合成课堂环境中评估不同师生互动策略。

Result: 实验表明，包含共同适应原则的策略（如双向代理）优于单边策略，能提升所有学习类型的效果。

Conclusion: 该模型为教育科学提供了可扩展的测试工具，并为开发动态适应学习者需求的公平教育技术奠定了基础。

Abstract: Human education transcends mere knowledge transfer, it relies on
co-adaptation dynamics -- the mutual adjustment of teaching and learning
strategies between agents. Despite its centrality, computational models of
co-adaptive teacher-student interactions (T-SI) remain underdeveloped. We argue
that this gap impedes Educational Science in testing and scaling contextual
insights across diverse settings, and limits the potential of Machine Learning
systems, which struggle to emulate and adaptively support human learning
processes. To address this, we present a computational T-SI model that
integrates contextual insights on human education into a testable framework. We
use the model to evaluate diverse T-SI strategies in a realistic synthetic
classroom setting, simulating student groups with unequal access to sensory
information. Results show that strategies incorporating co-adaptation
principles (e.g., bidirectional agency) outperform unilateral approaches (i.e.,
where only the teacher or the student is active), improving the learning
outcomes for all learning types. Beyond the testing and scaling of
context-dependent educational insights, our model enables hypothesis generation
in controlled yet adaptable environments. This work bridges non-computational
theories of human education with scalable, inclusive AI in Education systems,
providing a foundation for equitable technologies that dynamically adapt to
learner needs.

</details>


### [257] [AI Education in a Mirror: Challenges Faced by Academic and Industry Experts](https://arxiv.org/abs/2505.02856)
*Mahir Akgun,Hadi Hosseini*

Main category: cs.CY

TL;DR: 研究探讨AI教育与实际行业挑战的差距，通过14位专家访谈发现数据质量、模型扩展等共同问题，行业更关注部署限制，学术界则强调理论适应。建议课程整合现实复杂性。


<details>
  <summary>Details</summary>
Motivation: 调查AI教育与实践之间的差距，为改进AI课程提供依据。

Method: 对14位AI专家（8位行业，6位学术）进行半结构化访谈。

Result: 发现数据质量、模型扩展等共同挑战，行业关注部署限制，学术关注理论适应。

Conclusion: 建议AI课程整合现实复杂性、软件工程原则及跨学科学习，同时注重基础与伦理能力。

Abstract: As Artificial Intelligence (AI) technologies continue to evolve, the gap
between academic AI education and real-world industry challenges remains an
important area of investigation. This study provides preliminary insights into
challenges AI professionals encounter in both academia and industry, based on
semi-structured interviews with 14 AI experts - eight from industry and six
from academia. We identify key challenges related to data quality and
availability, model scalability, practical constraints, user behavior, and
explainability. While both groups experience data and model adaptation
difficulties, industry professionals more frequently highlight deployment
constraints, resource limitations, and external dependencies, whereas academics
emphasize theoretical adaptation and standardization issues. These exploratory
findings suggest that AI curricula could better integrate real-world
complexities, software engineering principles, and interdisciplinary learning,
while recognizing the broader educational goals of building foundational and
ethical reasoning skills.

</details>


### [258] [Understanding University Students' Use of Generative AI: The Roles of Demographics and Personality Traits](https://arxiv.org/abs/2505.02863)
*Newnew Deng,Edward Jiusi Liu,Xiaoming Zhai*

Main category: cs.CY

TL;DR: 研究发现高年级学生、非英语母语者和亚裔学生更倾向于使用生成式AI（GAI），人格特质如尽责性、宜人性等也显著影响GAI的使用态度。


<details>
  <summary>Details</summary>
Motivation: 填补关于大学生GAI使用及其影响因素的实证研究空白。

Method: 调查了363名美国本科生和研究生，分析GAI使用与人口统计学变量及大五人格特质的关系。

Result: 高年级学生、非英语母语者和亚裔学生更常使用GAI；人格特质如尽责性低、外向性高者更偏好GAI。

Conclusion: 大学需提供个性化指导，确保学生有效、道德且公平地使用GAI。

Abstract: The use of generative AI (GAI) among university students is rapidly
increasing, yet empirical research on students' GAI use and the factors
influencing it remains limited. To address this gap, we surveyed 363
undergraduate and graduate students in the United States, examining their GAI
usage and how it relates to demographic variables and personality traits based
on the Big Five model (i.e., extraversion, agreeableness, conscientiousness,
and emotional stability, and intellect/imagination). Our findings reveal: (a)
Students in higher academic years are more inclined to use GAI and prefer it
over traditional resources. (b) Non-native English speakers use and adopt GAI
more readily than native speakers. (c) Compared to White, Asian students report
higher GAI usage, perceive greater academic benefits, and express a stronger
preference for it. Similarly, Black students report a more positive impact of
GAI on their academic performance. Personality traits also play a significant
role in shaping perceptions and usage of GAI. After controlling demographic
factors, we found that personality still significantly predicts GAI use and
attitudes: (a) Students with higher conscientiousness use GAI less. (b)
Students who are higher in agreeableness perceive a less positive impact of GAI
on academic performance and express more ethical concerns about using it for
academic work. (c) Students with higher emotional stability report a more
positive impact of GAI on learning and fewer concerns about its academic use.
(d) Students with higher extraversion show a stronger preference for GAI over
traditional resources. (e) Students with higher intellect/imagination tend to
prefer traditional resources. These insights highlight the need for
universities to provide personalized guidance to ensure students use GAI
effectively, ethically, and equitably in their academic pursuits.

</details>


### [259] [The Cognitive Foundations of Economic Exchange: A Modular Framework Grounded in Behavioral Evidence](https://arxiv.org/abs/2505.02945)
*Egil Diau*

Main category: cs.CY

TL;DR: 论文提出了一种基于认知最小机制的框架，用于在多智能体AI中建模社会合作，将信任重新定义为分级的认知期望。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体AI中社会合作的建模问题，尤其是信任和道德等概念的非正式定义限制了其在人工代理中的可测试性和实现。

Method: 基于灵长类行为、婴儿认知和经济人类学的实证证据，提出了一个由个体识别、互相信任和成本回报敏感性组成的认知最小机制框架。

Result: 该框架为人工代理中的互惠交换提供了可模拟的基础，并支持自下而上涌现的可扩展合作和制度动态。

Conclusion: 通过认知最小机制框架，论文为多智能体AI中的社会合作建模提供了新的理论基础和实现路径。

Abstract: A key challenge in multi-agent AI is modeling social cooperation under
realistic behavioral constraints. Many foundational concepts in economics and
ethics such as "trust" or "morality" are often defined informally, without
operational criteria or cognitive grounding, which limits their testability and
implementation in artificial agents. Drawing on converging empirical evidence
from primate behavior, infant cognition, and economic anthropology, we propose
a conceptual framework composed of three cognitively minimal mechanisms:
individual recognition, reciprocal credence, and cost return sensitivity. This
framework reframes trust as a graded cognitive expectation, providing a
simulateable basis for reciprocal exchange in artificial agents, and enabling
the bottom-up emergence of scalable cooperation and institutional dynamics.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [260] [Parameter estimation for land-surface models using machine learning libraries](https://arxiv.org/abs/2505.02979)
*Ruiyue Huang,Claire E. Heaney,Maarten van Reeuwijk*

Main category: physics.ao-ph

TL;DR: NN4PDEs方法用于通过PyTorch的反向传播引擎确定地表模型参数。使用合成数据测试逆模型，发现单一土壤温度时间序列无法可靠估计参数，但两个深度的测量可以。应用于城市数据后，模型能可靠估计热导率、热容和热通量系数。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用NN4PDEs方法通过反向传播优化地表模型参数，解决单一观测数据无法可靠估计参数的问题。

Method: 使用PyTorch的反向传播引擎，通过合成数据测试逆模型，并应用于实际城市数据。

Result: 单一土壤温度时间序列无法可靠估计参数，但两个深度的测量可以。模型能准确预测长波辐射、土壤热通量和热通量系数。

Conclusion: NN4PDEs方法在多个观测数据支持下能有效估计地表模型参数，但无法区分潜热和感热通量。

Abstract: The Neural Networks for Partial Differential Equations (NN4PDEs) approach is
used to determine the parameters of a simple land-surface model using PyTorch's
backpropagation engine. In order to test the inverse model, a synthetic dataset
is created by running the model in forward mode with known parameter values to
create soil temperature time series that can be used as observations for the
inverse model. We show that it is not possible to obtain a reliable parameter
estimation using a single observed soil temperature time series. Using
measurements at two depths, reliable parameter estimates can be obtained
although it is not possible to differentiate between latent and sensible heat
fluxes. We apply the inverse model to urban flux tower data in Phoenix, United
States, and show that the thermal conductivity, volumetric heat capacity, and
the combined sensible-latent heat transfer coefficient can be reliably
estimated using an observed value for the effective surface albedo. The
resulting model accurately predicts the outgoing longwave radiation, conductive
soil fluxes and the combined sensible-latent heat fluxes.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [261] [Accelerating Evolution: Integrating PSO Principles into Real-Coded Genetic Algorithm Crossover](https://arxiv.org/abs/2505.03217)
*Xiaobo Jin,JiaShu Tu*

Main category: cs.NE

TL;DR: 本文提出了一种名为PSOX的新型交叉算子，专为实数编码遗传算法设计，通过结合当前全局最优解和历史最优解，提升算法性能。


<details>
  <summary>Details</summary>
Motivation: 传统交叉算子仅在同代个体间交换信息，缺乏对全局和历史最优解的利用，限制了算法性能。

Method: PSOX结合当前全局最优解和历史最优解，维持种群多样性并加速收敛。

Result: 在15个基准测试函数上验证，PSOX在准确性、稳定性和收敛速度上优于其他五种先进交叉算子。

Conclusion: PSOX在优化问题中表现优异，尤其结合适当变异策略时，同时提供了参数调优的实用指南。

Abstract: This study introduces an innovative crossover operator named Particle Swarm
Optimization-inspired Crossover (PSOX), which is specifically developed for
real-coded genetic algorithms. Departing from conventional crossover approaches
that only exchange information between individuals within the same generation,
PSOX uniquely incorporates guidance from both the current global best solution
and historical optimal solutions across multiple generations. This novel
mechanism enables the algorithm to maintain population diversity while
simultaneously accelerating convergence toward promising regions of the search
space. The effectiveness of PSOX is rigorously evaluated through comprehensive
experiments on 15 benchmark test functions with diverse characteristics,
including unimodal, multimodal, and highly complex landscapes. Comparative
analysis against five state-of-the-art crossover operators reveals that PSOX
consistently delivers superior performance in terms of solution accuracy,
algorithmic stability, and convergence speed, especially when combined with an
appropriate mutation strategy. Furthermore, the study provides an in-depth
investigation of how different mutation rates influence PSOX's performance,
yielding practical guidelines for parameter tuning when addressing optimization
problems with varying landscape properties.

</details>


### [262] [From Neurons to Computation: Biological Reservoir Computing for Pattern Recognition](https://arxiv.org/abs/2505.03510)
*Ludovico Iannello,Luca Ciampi,Gabriele Lagani,Fabrizio Tonelli,Eleonora Crocco,Lucio Maria Calcagnile,Angelo Di Garbo,Federico Cremisi,Giuseppe Amato*

Main category: cs.NE

TL;DR: 提出了一种基于生物神经元池的储层计算（BRC）新范式，利用培养神经元网络作为储层，通过多电极阵列（MEA）记录神经活动，实现高效模式识别。


<details>
  <summary>Details</summary>
Motivation: 探索生物神经网络在传统人工神经网络任务中的应用潜力，推动生物启发计算系统的发展。

Method: 使用MEA记录神经元活动，输入数据通过电极引入，剩余电极捕获神经活动，生成高维生物特征空间映射。

Result: 实验验证了BRC在位置编码、方向条和数字识别任务中的有效性。

Conclusion: BRC展示了生物神经网络在计算任务中的可行性，为神经形态工程和生物混合计算开辟了新途径。

Abstract: In this paper, we introduce a novel paradigm for reservoir computing (RC)
that leverages a pool of cultured biological neurons as the reservoir
substrate, creating a biological reservoir computing (BRC). This system
operates similarly to an echo state network (ESN), with the key distinction
that the neural activity is generated by a network of cultured neurons, rather
than being modeled by traditional artificial computational units. The neuronal
activity is recorded using a multi-electrode array (MEA), which enables
high-throughput recording of neural signals. In our approach, inputs are
introduced into the network through a subset of the MEA electrodes, while the
remaining electrodes capture the resulting neural activity. This generates a
nonlinear mapping of the input data to a high-dimensional biological feature
space, where distinguishing between data becomes more efficient and
straightforward, allowing a simple linear classifier to perform pattern
recognition tasks effectively. To evaluate the performance of our proposed
system, we present an experimental study that includes various input patterns,
such as positional codes, bars with different orientations, and a digit
recognition task. The results demonstrate the feasibility of using biological
neural networks to perform tasks traditionally handled by artificial neural
networks, paving the way for further exploration of biologically-inspired
computing systems, with potential applications in neuromorphic engineering and
bio-hybrid computing.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [263] [CreoPep: A Universal Deep Learning Framework for Target-Specific Peptide Design and Optimization](https://arxiv.org/abs/2505.02887)
*Cheng Ge,Han-Shen Tae,Zhenqiang Zhang,Lu Lu,Zhijie Huang,Yilin Wang,Tao Jiang,Wenqing Cai,Shan Chang,David J. Adams,Rilei Yu*

Main category: q-bio.BM

TL;DR: CreoPep是一个基于深度学习的条件生成框架，用于设计高亲和力肽突变体，同时发现新的结构基序。


<details>
  <summary>Details</summary>
Motivation: 天然肽变体多样性有限且传统优化策略耗时，限制了其治疗潜力。

Method: 结合掩码语言建模和渐进掩码方案，通过FoldX能量筛选和温度控制多态采样生成多样肽。

Result: 设计的α7烟碱乙酰胆碱受体抑制剂在电生理测试中达到亚微摩尔效力。

Conclusion: CreoPep为计算肽设计与实验验证提供了通用平台，加速下一代肽疗法的发现。

Abstract: Target-specific peptides, such as conotoxins, exhibit exceptional binding
affinity and selectivity toward ion channels and receptors. However, their
therapeutic potential remains underutilized due to the limited diversity of
natural variants and the labor-intensive nature of traditional optimization
strategies. Here, we present CreoPep, a deep learning-based conditional
generative framework that integrates masked language modeling with a
progressive masking scheme to design high-affinity peptide mutants while
uncovering novel structural motifs. CreoPep employs an integrative augmentation
pipeline, combining FoldX-based energy screening with temperature-controlled
multinomial sampling, to generate structurally and functionally diverse
peptides that retain key pharmacological properties. We validate this approach
by designing conotoxin inhibitors targeting the $\alpha$7 nicotinic
acetylcholine receptor, achieving submicromolar potency in electrophysiological
assays. Structural analysis reveals that CreoPep-generated variants engage in
both conserved and novel binding modes, including disulfide-deficient forms,
thus expanding beyond conventional design paradigms. Overall, CreoPep offers a
robust and generalizable platform that bridges computational peptide design
with experimental validation, accelerating the discovery of next-generation
peptide therapeutics.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [264] [MORE: Mobile Manipulation Rearrangement Through Grounded Language Reasoning](https://arxiv.org/abs/2505.03035)
*Mohammad Mohammadi,Daniel Honerkamp,Martin Büchner,Matteo Cassinelli,Tim Welschehold,Fabien Despinoy,Igor Gilitschenski,Abhinav Valada*

Main category: cs.RO

TL;DR: MORE是一种新方法，通过场景图和主动过滤方案提升语言模型在零样本移动操作规划中的能力，解决了大规模环境和多物体问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于基础模型的方法在处理多物体和大规模环境时性能下降，需要更可靠的规划方法。

Method: MORE利用场景图表示环境，引入实例区分和主动过滤方案，提取任务相关子图，形成有界规划问题。

Result: 在BEHAVIOR-1K基准测试中，MORE成功解决了81个多样化重排任务，优于现有方法，并在真实任务中验证了其能力。

Conclusion: MORE通过场景图和主动过滤显著提升了语言模型的规划能力，适用于复杂室内外环境。

Abstract: Autonomous long-horizon mobile manipulation encompasses a multitude of
challenges, including scene dynamics, unexplored areas, and error recovery.
Recent works have leveraged foundation models for scene-level robotic reasoning
and planning. However, the performance of these methods degrades when dealing
with a large number of objects and large-scale environments. To address these
limitations, we propose MORE, a novel approach for enhancing the capabilities
of language models to solve zero-shot mobile manipulation planning for
rearrangement tasks. MORE leverages scene graphs to represent environments,
incorporates instance differentiation, and introduces an active filtering
scheme that extracts task-relevant subgraphs of object and region instances.
These steps yield a bounded planning problem, effectively mitigating
hallucinations and improving reliability. Additionally, we introduce several
enhancements that enable planning across both indoor and outdoor environments.
We evaluate MORE on 81 diverse rearrangement tasks from the BEHAVIOR-1K
benchmark, where it becomes the first approach to successfully solve a
significant share of the benchmark, outperforming recent foundation model-based
approaches. Furthermore, we demonstrate the capabilities of our approach in
several complex real-world tasks, mimicking everyday activities. We make the
code publicly available at https://more-model.cs.uni-freiburg.de.

</details>


### [265] [Latent Adaptive Planner for Dynamic Manipulation](https://arxiv.org/abs/2505.03077)
*Donghun Noh,Deqian Kong,Minglu Zhao,Andrew Lizarraga,Jianwen Xie,Ying Nian Wu,Dennis Hong*

Main category: cs.RO

TL;DR: LAP是一种基于潜在空间推理的动态非抓取操作任务规划方法，通过学习人类演示视频实现高效适应环境变化。


<details>
  <summary>Details</summary>
Motivation: 解决视觉运动策略学习中的关键挑战，如时间一致性和环境适应性。

Method: 采用变分重规划框架和贝叶斯更新，结合模型比例映射从人类演示中生成精确状态。

Result: 在多个复杂操作基准测试中表现优异，成功率和适应性优于现有方法。

Conclusion: LAP为机器人提供了类似人类的适应性，并适用于多种平台。

Abstract: This paper presents Latent Adaptive Planner (LAP), a novel approach for
dynamic nonprehensile manipulation tasks that formulates planning as latent
space inference, effectively learned from human demonstration videos. Our
method addresses key challenges in visuomotor policy learning through a
principled variational replanning framework that maintains temporal consistency
while efficiently adapting to environmental changes. LAP employs Bayesian
updating in latent space to incrementally refine plans as new observations
become available, striking an optimal balance between computational efficiency
and real-time adaptability. We bridge the embodiment gap between humans and
robots through model-based proportional mapping that regenerates accurate
kinematic-dynamic joint states and object positions from human demonstrations.
Experimental evaluations across multiple complex manipulation benchmarks
demonstrate that LAP achieves state-of-the-art performance, outperforming
existing approaches in success rate, trajectory smoothness, and energy
efficiency, particularly in dynamic adaptation scenarios. Our approach enables
robots to perform complex interactions with human-like adaptability while
providing an expandable framework applicable to diverse robotic platforms using
the same human demonstration videos.

</details>


### [266] [Learn to Swim: Data-Driven LSTM Hydrodynamic Model for Quadruped Robot Gait Optimization](https://arxiv.org/abs/2505.03146)
*Fei Han,Pengming Guo,Hao Chen,Weikun Li,Jingbo Ren,Naijun Liu,Ning Yang,Dixia Fan*

Main category: cs.RO

TL;DR: FED-LSTM模型基于LSTM网络，用于预测水下四足机器人的非线性流体动力，优于传统经验公式，提高了游泳性能和转向效率。


<details>
  <summary>Details</summary>
Motivation: 传统经验公式在预测复杂流体动力学时精度不足，需要一种更准确的数据驱动模型来优化水下机器人的运动性能。

Method: 利用实验数据训练LSTM网络，结合NSGA-II算法优化直线和转向步态。

Result: FED-LSTM在直线游泳和转向时表现更优，误差更小，且硬件实验验证了其稳定性和精度。

Conclusion: FED-LSTM为水下机器人运动性能优化提供了可靠框架，为未来研究奠定了基础。

Abstract: This paper presents a Long Short-Term Memory network-based Fluid Experiment
Data-Driven model (FED-LSTM) for predicting unsteady, nonlinear hydrodynamic
forces on the underwater quadruped robot we constructed. Trained on
experimental data from leg force and body drag tests conducted in both a
recirculating water tank and a towing tank, FED-LSTM outperforms traditional
Empirical Formulas (EF) commonly used for flow prediction over flat surfaces.
The model demonstrates superior accuracy and adaptability in capturing complex
fluid dynamics, particularly in straight-line and turning-gait optimizations
via the NSGA-II algorithm. FED-LSTM reduces deflection errors during
straight-line swimming and improves turn times without increasing the turning
radius. Hardware experiments further validate the model's precision and
stability over EF. This approach provides a robust framework for enhancing the
swimming performance of legged robots, laying the groundwork for future
advances in underwater robotic locomotion.

</details>


### [267] [Systematic Evaluation of Initial States and Exploration-Exploitation Strategies in PID Auto-Tuning: A Framework-Driven Approach Applied on Mobile Robots](https://arxiv.org/abs/2505.03159)
*Zaid Ghazal,Ali Al-Bustami,Khouloud Gaaloul,Jaerock Kwon*

Main category: cs.RO

TL;DR: 论文提出了一种新框架，用于评估初始系统状态和探索-利用平衡对PID控制器自动调谐的影响，并通过实验验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 尽管已有高级优化技术用于PID控制器调谐，但初始系统状态和探索-利用平衡的影响尚未充分研究，且需在实际机器人平台上验证。

Method: 引入新框架，结合贝叶斯优化和差分进化，系统评估初始状态和探索-利用平衡对PID调谐的影响，并在两种机器人平台上测试。

Result: 实验结果表明，系统变化对收敛速度、稳定时间、上升时间和超调百分比有显著影响。

Conclusion: 研究结果为未来PID自动调谐研究提供了实证基础。

Abstract: PID controllers are widely used in control systems because of their
simplicity and effectiveness. Although advanced optimization techniques such as
Bayesian Optimization and Differential Evolution have been applied to address
the challenges of automatic tuning of PID controllers, the influence of initial
system states on convergence and the balance between exploration and
exploitation remains underexplored. Moreover, experimenting the influence
directly on real cyber-physical systems such as mobile robots is crucial for
deriving realistic insights. In the present paper, a novel framework is
introduced to evaluate the impact of systematically varying these factors on
the PID auto-tuning processes that utilize Bayesian Optimization and
Differential Evolution. Testing was conducted on two distinct PID-controlled
robotic platforms, an omnidirectional robot and a differential drive mobile
robot, to assess the effects on convergence rate, settling time, rise time, and
overshoot percentage. As a result, the experimental outcomes yield evidence on
the effects of the systematic variations, thereby providing an empirical basis
for future research studies in the field.

</details>


### [268] [Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets](https://arxiv.org/abs/2505.03174)
*Guillermo Roque,Erika Maquiling,Jose Giovanni Tapia Lopez,Ross Greer*

Main category: cs.RO

TL;DR: 利用GPS和NLP自动生成指令-动作数据对，减少人工标注成本。


<details>
  <summary>Details</summary>
Motivation: 人工标注指令-动作数据对成本高且效率低，探索自动化生成方法。

Method: 通过GPS应用收集语音指令，结合视频数据形成视觉-语言-动作三元组，开发自动化数据收集系统ADVLAT-Engine。

Result: 成功分类GPS语音指令为八类，展示自动化生成高质量数据对的潜力。

Conclusion: 自动化方法可高效生成指令-动作数据，为视觉-语言导航和交互系统提供支持。

Abstract: Instruction-Action (IA) data pairs are valuable for training robotic systems,
especially autonomous vehicles (AVs), but having humans manually annotate this
data is costly and time-inefficient. This paper explores the potential of using
mobile application Global Positioning System (GPS) references and Natural
Language Processing (NLP) to automatically generate large volumes of IA
commands and responses without having a human generate or retroactively tag the
data. In our pilot data collection, by driving to various destinations and
collecting voice instructions from GPS applications, we demonstrate a means to
collect and categorize the diverse sets of instructions, further accompanied by
video data to form complete vision-language-action triads. We provide details
on our completely automated data collection prototype system, ADVLAT-Engine. We
characterize collected GPS voice instructions into eight different
classifications, highlighting the breadth of commands and referentialities
available for curation from freely available mobile applications. Through
research and exploration into the automation of IA data pairs using GPS
references, the potential to increase the speed and volume at which
high-quality IA datasets are created, while minimizing cost, can pave the way
for robust vision-language-action (VLA) models to serve tasks in
vision-language navigation (VLN) and human-interactive autonomous systems.

</details>


### [269] [The Unreasonable Effectiveness of Discrete-Time Gaussian Process Mixtures for Robot Policy Learning](https://arxiv.org/abs/2505.03296)
*Jan Ole von Hartz,Adrian Röfer,Joschka Boedecker,Abhinav Valada*

Main category: cs.RO

TL;DR: MiDiGap是一种用于机器人操作的新型策略表示和模仿学习方法，仅需五次演示即可学习，并在多种复杂任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中灵活策略表示和模仿学习的挑战，尤其是在少样本和多模态任务中。

Method: 提出MiDiGap方法，利用离散时间高斯过程混合模型，结合推理时引导工具（如碰撞信号和运动学约束）。

Result: 在RLBench任务中，策略成功率提升76%，轨迹成本降低67%；多模态任务中，成功率提升48%，样本效率提高20倍。

Conclusion: MiDiGap在少样本和多模态任务中表现优异，具有广泛的应用潜力。

Abstract: We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel
approach for flexible policy representation and imitation learning in robot
manipulation. MiDiGap enables learning from as few as five demonstrations using
only camera observations and generalizes across a wide range of challenging
tasks. It excels at long-horizon behaviors such as making coffee, highly
constrained motions such as opening doors, dynamic actions such as scooping
with a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns
these tasks on a CPU in less than a minute and scales linearly to large
datasets. We also develop a rich suite of tools for inference-time steering
using evidence such as collision signals and robot kinematic constraints. This
steering enables novel generalization capabilities, including obstacle
avoidance and cross-embodiment policy transfer. MiDiGap achieves
state-of-the-art performance on diverse few-shot manipulation benchmarks. On
constrained RLBench tasks, it improves policy success by 76 percentage points
and reduces trajectory cost by 67%. On multimodal tasks, it improves policy
success by 48 percentage points and increases sample efficiency by a factor of
20. In cross-embodiment transfer, it more than doubles policy success. We make
the code publicly available at https://midigap.cs.uni-freiburg.de.

</details>


### [270] [RIFT: Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic Simulation](https://arxiv.org/abs/2505.03344)
*Keyu Chen,Wenchao Sun,Hao Cheng,Sifa Zheng*

Main category: cs.RO

TL;DR: 论文提出了一种双阶段仿真框架，结合数据驱动和物理模拟，通过预训练和微调提升自动驾驶仿真的真实性和可控性。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶仿真中数据驱动方法因协变量偏移和简化动力学模型导致的可靠性不足，以及物理模拟方法缺乏专家演示而牺牲真实性的问题。

Method: 采用双阶段框架：数据驱动模拟器中进行开环模仿学习预训练，物理模拟器中进行闭环强化学习微调，并提出RIFT策略保留多模态性。

Result: RIFT显著提升了交通场景的真实性和可控性，为自动驾驶性能评估提供了鲁棒平台。

Conclusion: 双阶段框架和RIFT策略有效解决了仿真中的真实性和可控性矛盾，为自动驾驶测试提供了可靠工具。

Abstract: Achieving both realism and controllability in interactive closed-loop traffic
simulation remains a key challenge in autonomous driving. Data-driven
simulation methods reproduce realistic trajectories but suffer from covariate
shift in closed-loop deployment, compounded by simplified dynamics models that
further reduce reliability. Conversely, physics-based simulation methods
enhance reliable and controllable closed-loop interactions but often lack
expert demonstrations, compromising realism. To address these challenges, we
introduce a dual-stage AV-centered simulation framework that conducts open-loop
imitation learning pre-training in a data-driven simulator to capture
trajectory-level realism and multimodality, followed by closed-loop
reinforcement learning fine-tuning in a physics-based simulator to enhance
controllability and mitigate covariate shift. In the fine-tuning stage, we
propose RIFT, a simple yet effective closed-loop RL fine-tuning strategy that
preserves the trajectory-level multimodality through a GRPO-style
group-relative advantage formulation, while enhancing controllability and
training stability by replacing KL regularization with the dual-clip mechanism.
Extensive experiments demonstrate that RIFT significantly improves the realism
and controllability of generated traffic scenarios, providing a robust platform
for evaluating autonomous vehicle performance in diverse and interactive
scenarios.

</details>


### [271] [Sim2Real Transfer for Vision-Based Grasp Verification](https://arxiv.org/abs/2505.03046)
*Pau Amargant,Peter Hönig,Markus Vincze*

Main category: cs.RO

TL;DR: 论文提出了一种基于视觉的抓取验证方法，用于判断机器人夹爪是否成功抓取物体，特别针对可变形物体。采用两阶段架构（YOLO检测夹爪，ResNet分类物体），并引入合成数据集HSR-GraspSynth。实验显示方法在真实环境中准确率高。


<details>
  <summary>Details</summary>
Motivation: 传统基于力和触觉传感器的方法在处理可变形物体时效果不佳，因此需要一种更有效的视觉验证方法。

Method: 两阶段架构：YOLO检测夹爪位置，ResNet分类物体是否存在。使用合成数据集HSR-GraspSynth补充真实数据不足。

Result: 方法在真实环境中表现高准确率，并探索了视觉问答能力作为零样本基线。

Conclusion: 提出的视觉方法有效解决了可变形物体抓取验证问题，代码和数据集已公开。

Abstract: The verification of successful grasps is a crucial aspect of robot
manipulation, particularly when handling deformable objects. Traditional
methods relying on force and tactile sensors often struggle with deformable and
non-rigid objects. In this work, we present a vision-based approach for grasp
verification to determine whether the robotic gripper has successfully grasped
an object. Our method employs a two-stage architecture; first YOLO-based object
detection model to detect and locate the robot's gripper and then a
ResNet-based classifier determines the presence of an object. To address the
limitations of real-world data capture, we introduce HSR-GraspSynth, a
synthetic dataset designed to simulate diverse grasping scenarios. Furthermore,
we explore the use of Visual Question Answering capabilities as a zero-shot
baseline to which we compare our model. Experimental results demonstrate that
our approach achieves high accuracy in real-world environments, with potential
for integration into grasping pipelines. Code and datasets are publicly
available at https://github.com/pauamargant/HSR-GraspSynth .

</details>


### [272] [Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach](https://arxiv.org/abs/2505.03702)
*Srecharan Selvam,Abhishesh Silwal,George Kanter*

Main category: cs.RO

TL;DR: 提出了一种结合几何与神经网络的自主叶片抓取方法，通过自监督学习提升性能，实验显示显著优于纯几何或神经网络方法。


<details>
  <summary>Details</summary>
Motivation: 农业环境中叶片操作的自动化面临植物形态多变和叶片可变形等挑战，需结合传统计算机视觉与神经网络。

Method: 整合YOLOv8进行实例分割和RAFT-Stereo进行3D深度估计，结合几何特征评分与神经细化模块（GraspPointCNN），采用置信度加权融合机制。

Result: 在控制环境中成功率为88.0%，实际温室中为84.7%，显著优于纯几何（75.3%）和神经网络（60.2%）方法。

Conclusion: 为农业机器人学提供了新范式，结合领域专业知识与机器学习能力，为全自动作物监测系统奠定基础。

Abstract: Automating leaf manipulation in agricultural settings faces significant
challenges, including the variability of plant morphologies and deformable
leaves. We propose a novel hybrid geometric-neural approach for autonomous leaf
grasping that combines traditional computer vision with neural networks through
self-supervised learning. Our method integrates YOLOv8 for instance
segmentation and RAFT-Stereo for 3D depth estimation to build rich leaf
representations, which feed into both a geometric feature scoring pipeline and
a neural refinement module (GraspPointCNN). The key innovation is our
confidence-weighted fusion mechanism that dynamically balances the contribution
of each approach based on prediction certainty. Our self-supervised framework
uses the geometric pipeline as an expert teacher to automatically generate
training data. Experiments demonstrate that our approach achieves an 88.0%
success rate in controlled environments and 84.7% in real greenhouse
conditions, significantly outperforming both purely geometric (75.3%) and
neural (60.2%) methods. This work establishes a new paradigm for agricultural
robotics where domain expertise is seamlessly integrated with machine learning
capabilities, providing a foundation for fully automated crop monitoring
systems.

</details>


### [273] [Visual Imitation Enables Contextual Humanoid Control](https://arxiv.org/abs/2505.03729)
*Arthur Allshire,Hongsuk Choi,Junyi Zhang,David McAllister,Anthony Zhang,Chung Min Kim,Trevor Darrell,Pieter Abbeel,Jitendra Malik,Angjoo Kanazawa*

Main category: cs.RO

TL;DR: VIDEOMIMIC是一种从视频中学习并生成人形机器人控制策略的流程，通过模拟环境实现真实技能。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用日常视频教人形机器人完成复杂动作（如爬楼梯、坐椅子），以简化机器人学习过程。

Method: 提出VIDEOMIMIC流程，从视频中重建人类与环境，生成全身控制策略，并通过模拟到真实环境实现。

Result: 在真实人形机器人上展示了稳健、可重复的上下文控制能力，如上下楼梯、坐立等动作。

Conclusion: VIDEOMIMIC为多样化现实环境中的人形机器人操作提供了可扩展的学习路径。

Abstract: How can we teach humanoids to climb staircases and sit on chairs using the
surrounding environment context? Arguably, the simplest way is to just show
them-casually capture a human motion video and feed it to humanoids. We
introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday
videos, jointly reconstructs the humans and the environment, and produces
whole-body control policies for humanoid robots that perform the corresponding
skills. We demonstrate the results of our pipeline on real humanoid robots,
showing robust, repeatable contextual control such as staircase ascents and
descents, sitting and standing from chairs and benches, as well as other
dynamic whole-body skills-all from a single policy, conditioned on the
environment and global root commands. VIDEOMIMIC offers a scalable path towards
teaching humanoids to operate in diverse real-world environments.

</details>


### [274] [AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control](https://arxiv.org/abs/2505.03738)
*Jialong Li,Xuxin Cheng,Tianshu Huang,Shiqi Yang,Ri-Zhao Qiu,Xiaolong Wang*

Main category: cs.RO

TL;DR: AMO框架结合强化学习与轨迹优化，实现人形机器人实时自适应全身控制，提升稳定性和工作范围。


<details>
  <summary>Details</summary>
Motivation: 人形机器人因高自由度和非线性动力学，实现复杂任务（如拾取地面物体）仍具挑战。

Method: 提出AMO框架，结合模拟到现实的强化学习和轨迹优化，构建混合数据集训练网络以适应O.O.D.命令。

Result: 在29自由度Unitree G1机器人上验证，AMO表现优于基线，稳定性和工作范围更优。

Conclusion: AMO通过模仿学习支持自主任务执行，展现系统多功能性和鲁棒性。

Abstract: Humanoid robots derive much of their dexterity from hyper-dexterous
whole-body movements, enabling tasks that require a large operational
workspace: such as picking objects off the ground. However, achieving these
capabilities on real humanoids remains challenging due to their high degrees of
freedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization
(AMO), a framework that integrates sim-to-real reinforcement learning (RL) with
trajectory optimization for real-time, adaptive whole-body control. To mitigate
distribution bias in motion imitation RL, we construct a hybrid AMO dataset and
train a network capable of robust, on-demand adaptation to potentially O.O.D.
commands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid
robot, demonstrating superior stability and an expanded workspace compared to
strong baselines. Finally, we show that AMO's consistent performance supports
autonomous task execution via imitation learning, underscoring the system's
versatility and robustness.

</details>


### [275] [Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid](https://arxiv.org/abs/2505.03694)
*Parv Kapoor,Ian Higgins,Nikhil Keetha,Jay Patrikar,Brady Moon,Zelin Ye,Yao He,Ivan Cisneros,Yaoyu Hu,Changliu Liu,Eunsuk Kang,Sebastian Scherer*

Main category: cs.RO

TL;DR: ViSafe是一种基于视觉的高速空中防撞系统，通过边缘AI框架和多摄像头硬件原型实现安全分离，并在高速操作中提供可证明的安全保证。


<details>
  <summary>Details</summary>
Motivation: 确保安全分离是实现高密度空中交通的关键，ViSafe旨在为资源受限的空中系统提供这一安全关键能力。

Method: ViSafe结合了基于学习的边缘AI框架和定制多摄像头硬件，利用感知输入控制屏障函数（CBF）设计和强制执行安全阈值。

Result: 通过模拟和真实飞行测试，ViSafe在多样化场景中均能确保安全分离，并在高速碰撞测试中达到144 km/h的闭合速度。

Conclusion: ViSafe为高速空中导航中的视觉自主防撞设立了新标准。

Abstract: Assured safe-separation is essential for achieving seamless high-density
operation of airborne vehicles in a shared airspace. To equip
resource-constrained aerial systems with this safety-critical capability, we
present ViSafe, a high-speed vision-only airborne collision avoidance system.
ViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by
tightly integrating a learning-based edge-AI framework with a custom
multi-camera hardware prototype designed under SWaP-C constraints. By
leveraging perceptual input-focused control barrier functions (CBF) to design,
encode, and enforce safety thresholds, ViSafe can provide provably safe runtime
guarantees for self-separation in high-speed aerial operations. We evaluate
ViSafe's performance through an extensive test campaign involving both
simulated digital twins and real-world flight scenarios. By independently
varying agent types, closure rates, interaction geometries, and environmental
conditions (e.g., weather and lighting), we demonstrate that ViSafe
consistently ensures self-separation across diverse scenarios. In
first-of-its-kind real-world high-speed collision avoidance tests with closure
rates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous
collision avoidance, establishing a new standard for safety in high-speed
aerial navigation.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [276] [Soft Best-of-n Sampling for Model Alignment](https://arxiv.org/abs/2505.03156)
*Claudio Mayrink Verdun,Alex Oesterling,Himabindu Lakkaraju,Flavio P. Calmon*

Main category: cs.IT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Best-of-$n$ (BoN) sampling is a practical approach for aligning language
model outputs with human preferences without expensive fine-tuning. BoN
sampling is performed by generating $n$ responses to a prompt and then
selecting the sample that maximizes a reward function. BoN yields high reward
values in practice at a distortion cost, as measured by the KL-divergence
between the sampled and original distribution. This distortion is coarsely
controlled by varying the number of samples: larger $n$ yields a higher reward
at a higher distortion cost. We introduce Soft Best-of-$n$ sampling, a
generalization of BoN that allows for smooth interpolation between the original
distribution and reward-maximizing distribution through a temperature parameter
$\lambda$. We establish theoretical guarantees showing that Soft Best-of-$n$
sampling converges sharply to the optimal tilted distribution at a rate of
$O(1/n)$ in KL and the expected (relative) reward. For sequences of discrete
outputs, we analyze an additive reward model that reveals the fundamental
limitations of blockwise sampling.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [277] [A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case](https://arxiv.org/abs/2505.03196)
*Haoxiang Luo,Gang Sun,Yinqiu Liu,Dusit Niyato,Hongfang Yu,Mohammed Atiquzzaman,Schahram Dustdar*

Main category: cs.NI

TL;DR: 提出了一种基于区块链的多LLM协作框架（MultiLLMN），以解决不同LLM在通信和网络任务中的信任和可靠性问题，并通过FBS防御案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 不同LLM因结构和训练数据差异可能导致响应不一致或偏见，且单个LLM的局限性难以应对复杂网络问题，需建立可信协作机制。

Method: 设计区块链支持的MultiLLMN框架，实现多LLM协作评估和选择最优响应，并以FBS攻击防御为例验证。

Result: MultiLLMN能有效提升响应可靠性和质量，尤其在FBS防御等复杂网络问题中表现突出。

Conclusion: MultiLLMN为解决LLM协作中的信任问题提供了可行方案，未来可进一步探索其应用和优化。

Abstract: Large Language Models (LLMs) demonstrate strong potential across a variety of
tasks in communications and networking due to their advanced reasoning
capabilities. However, because different LLMs have different model structures
and are trained using distinct corpora and methods, they may offer varying
optimization strategies for the same network issues. Moreover, the limitations
of an individual LLM's training data, aggravated by the potential maliciousness
of its hosting device, can result in responses with low confidence or even
bias. To address these challenges, we propose a blockchain-enabled
collaborative framework that connects multiple LLMs into a Trustworthy
Multi-LLM Network (MultiLLMN). This architecture enables the cooperative
evaluation and selection of the most reliable and high-quality responses to
complex network optimization problems. Specifically, we begin by reviewing
related work and highlighting the limitations of existing LLMs in collaboration
and trust, emphasizing the need for trustworthiness in LLM-based systems. We
then introduce the workflow and design of the proposed Trustworthy MultiLLMN
framework. Given the severity of False Base Station (FBS) attacks in B5G and 6G
communication systems and the difficulty of addressing such threats through
traditional modeling techniques, we present FBS defense as a case study to
empirically validate the effectiveness of our approach. Finally, we outline
promising future research directions in this emerging area.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [278] [Neural Orchestration for Multi-Agent Systems: A Deep Learning Framework for Optimal Agent Selection in Multi-Domain Task Environments](https://arxiv.org/abs/2505.02861)
*Kushagra Agrawal,Nisharg Nargund*

Main category: cs.MA

TL;DR: MetaOrch是一个神经编排框架，通过监督学习和模糊评估模块动态选择多域任务环境中最合适的代理，显著提升了选择准确性和系统适应性。


<details>
  <summary>Details</summary>
Motivation: 传统多代理系统（MAS）架构存在协调机制僵化和难以适应动态任务的问题，MetaOrch旨在解决这些问题。

Method: 采用监督学习方法建模任务上下文、代理历史和预期响应质量，结合模糊评估模块生成软监督标签，动态预测最合适的代理。

Result: 在模拟环境中，MetaOrch实现了86.3%的选择准确率，显著优于随机选择和轮询调度等基线策略。

Conclusion: 神经编排为多代理系统提供了增强自主性、可解释性和适应性的有效方法。

Abstract: Multi-agent systems (MAS) are foundational in simulating complex real-world
scenarios involving autonomous, interacting entities. However, traditional MAS
architectures often suffer from rigid coordination mechanisms and difficulty
adapting to dynamic tasks. We propose MetaOrch, a neural orchestration
framework for optimal agent selection in multi-domain task environments. Our
system implements a supervised learning approach that models task context,
agent histories, and expected response quality to select the most appropriate
agent for each task. A novel fuzzy evaluation module scores agent responses
along completeness, relevance, and confidence dimensions, generating soft
supervision labels for training the orchestrator. Unlike previous methods that
hard-code agent-task mappings, MetaOrch dynamically predicts the most suitable
agent while estimating selection confidence. Experiments in simulated
environments with heterogeneous agents demonstrate that our approach achieves
86.3% selection accuracy, significantly outperforming baseline strategies
including random selection and round-robin scheduling. The modular architecture
emphasizes extensibility, allowing agents to be registered, updated, and
queried independently. Results suggest that neural orchestration offers a
powerful approach to enhancing the autonomy, interpretability, and adaptability
of multi-agent systems across diverse task domains.

</details>


### [279] [Assessing and Enhancing the Robustness of LLM-based Multi-Agent Systems Through Chaos Engineering](https://arxiv.org/abs/2505.03096)
*Joshua Owotogbe*

Main category: cs.MA

TL;DR: 研究提出了一种混沌工程框架，用于增强大型语言模型多智能体系统（LLM-MAS）在真实环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LLM-MAS在生产或预生产环境中可能因幻觉、智能体故障和通信故障等问题而脆弱，需提升其可靠性。

Method: 采用混沌工程框架主动识别LLM-MAS的脆弱性，并评估和增强其韧性。

Result: 框架有助于确保LLM-MAS在关键应用中的可靠性能。

Conclusion: 混沌工程是提升LLM-MAS鲁棒性的有效方法。

Abstract: This study explores the application of chaos engineering to enhance the
robustness of Large Language Model-Based Multi-Agent Systems (LLM-MAS) in
production-like environments under real-world conditions. LLM-MAS can
potentially improve a wide range of tasks, from answering questions and
generating content to automating customer support and improving decision-making
processes. However, LLM-MAS in production or preproduction environments can be
vulnerable to emergent errors or disruptions, such as hallucinations, agent
failures, and agent communication failures. This study proposes a chaos
engineering framework to proactively identify such vulnerabilities in LLM-MAS,
assess and build resilience against them, and ensure reliable performance in
critical applications.

</details>


### [280] [Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation](https://arxiv.org/abs/2505.03586)
*Songchen Fu,Siang Chen,Shaojing Zhao,Letian Bai,Ta Li,Yonghong Yan*

Main category: cs.MA

TL;DR: 论文提出了一种针对多智能体系统中观测延迟问题的解决方案，通过扩展Dec-POMDP模型，设计了RDC框架，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实多智能体系统中普遍存在观测延迟问题，导致智能体无法基于真实环境状态决策，亟需解决方案。

Method: 扩展Dec-POMDP为DSID-POMDP模型，提出RDC训练框架及其模块实现，并在标准MARL基准（MPE和SMAC）上测试。

Result: 实验表明，基线MARL方法在延迟条件下性能显著下降，而RDC框架能有效缓解延迟影响，部分场景甚至达到无延迟性能。

Conclusion: 研究为多智能体延迟观测问题提供了新视角和有效解决方案框架。

Abstract: In real-world multi-agent systems (MASs), observation delays are ubiquitous,
preventing agents from making decisions based on the environment's true state.
An individual agent's local observation often consists of multiple components
from other agents or dynamic entities in the environment. These discrete
observation components with varying delay characteristics pose significant
challenges for multi-agent reinforcement learning (MARL). In this paper, we
first formulate the decentralized stochastic individual delay partially
observable Markov decision process (DSID-POMDP) by extending the standard
Dec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL
training framework for addressing stochastic individual delays, along with
recommended implementations for its constituent modules. We implement the
DSID-POMDP's observation generation pattern using standard MARL benchmarks,
including MPE and SMAC. Experiments demonstrate that baseline MARL methods
suffer severe performance degradation under fixed and unfixed delays. The
RDC-enhanced approach mitigates this issue, remarkably achieving ideal
delay-free performance in certain delay scenarios while maintaining
generalization capability. Our work provides a novel perspective on multi-agent
delayed observation problems and offers an effective solution framework.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [281] [New affine invariant ensemble samplers and their dimensional scaling](https://arxiv.org/abs/2505.02987)
*Yifan Chen*

Main category: stat.CO

TL;DR: 本文提出了一些新的仿射不变集成采样器，改进了现有算法，尤其在高维问题中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有采样器在高维问题中表现不佳，尤其是处理高度偏斜分布时。

Method: 提出了无导数集成侧移采样器和基于导数的仿射不变集成HMC采样器。

Result: 新采样器在性能上优于现有算法，尤其是仿射不变集成HMC在高维高斯目标中表现更优。

Conclusion: 仿射不变集成采样器在高维问题中具有显著优势，尤其是结合导数信息时。

Abstract: We introduce some new affine invariant ensemble samplers that are easy to
construct and improve upon existing widely used algorithms, especially for
high-dimensional problems. Specifically, we propose a derivative-free ensemble
side move sampler that performs favorably compared to popular samplers in the
\texttt{emcee} package. Additionally, we develop a class of derivative-based
ensemble Hamiltonian Monte Carlo (HMC) samplers with affine invariance, which
outperform standard HMC without affine invariance when sampling highly skewed
distributions. We provide asymptotic scaling analysis for high-dimensional
Gaussian targets to further elucidate the properties of these affine invariant
ensemble samplers. In particular, with derivative information, the affine
invariant ensemble HMC can scale much better with dimension compared to
derivative-free ensemble samplers.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [282] [Solar Flare Forecast: A Comparative Analysis of Machine Learning Algorithms for Solar Flare Class Prediction](https://arxiv.org/abs/2505.03385)
*Julia Bringewald*

Main category: astro-ph.SR

TL;DR: 该研究评估了三种机器学习算法（随机森林、KNN和XGBoost）在太阳耀斑分类任务中的表现，发现随机森林和XGBoost性能最优，尤其在增加维度后表现更佳。


<details>
  <summary>Details</summary>
Motivation: 太阳耀斑对空间天气和地球技术设施有重大影响，需要准确预测其发生和强度。

Method: 使用13个SHARP参数数据集，结合主成分分析（PCA）进行降维，采用10折分层交叉验证和网格搜索调参。

Result: 随机森林和XGBoost在所有指标中表现最佳，且维度增加后性能显著提升。

Conclusion: 研究结果为太阳耀斑预测提供了优化方法，有助于改进空间天气预报系统和深化对太阳物理的理解。

Abstract: Solar flares are among the most powerful and dynamic events in the solar
system, resulting from the sudden release of magnetic energy stored in the
Sun's atmosphere. These energetic bursts of electromagnetic radiation can
release up to 10^32 erg of energy, impacting space weather and posing risks to
technological infrastructure and therefore require accurate forecasting of
solar flare occurrences and intensities. This study evaluates the predictive
performance of three machine learning algorithms: Random Forest, k-Nearest
Neighbors (KNN), and Extreme Gradient Boosting (XGBoost) for classifying solar
flares into 4 categories (B, C, M, X). Using the dataset of 13 SHARP
parameters, the effectiveness of the models was evaluated in binary and
multiclass classification tasks. The analysis utilized 8 principal components
(PC), capturing 95% of data variance, and 100 PCs, capturing 97.5% of variance.
Our approach uniquely combines binary and multiclass classification with
different levels of dimensionality reduction, an innovative methodology not
previously explored in the context of solar flare prediction. Employing a
10-fold stratified cross-validation and grid search for hyperparameter tuning
ensured robust model evaluation. Our findings indicate that Random Forest and
XGBoost consistently demonstrate strong performance across all metrics,
benefiting significantly from increased dimensionality. The insights of this
study enhance future research by optimizing dimensionality reduction techniques
and informing model selection for astrophysical tasks. By integrating this
newly acquired knowledge into future research, more accurate space weather
forecasting systems can be developed, along with a deeper understanding of
solar physics.

</details>
