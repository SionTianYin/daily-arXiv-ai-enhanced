<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 41]
- [cs.CV](#cs.CV) [Total: 65]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.LG](#cs.LG) [Total: 87]
- [cs.DL](#cs.DL) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.IR](#cs.IR) [Total: 12]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 3]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.RO](#cs.RO) [Total: 7]
- [cs.GR](#cs.GR) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.SD](#cs.SD) [Total: 5]
- [math.NA](#math.NA) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.CR](#cs.CR) [Total: 8]
- [cs.SE](#cs.SE) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [It's the same but not the same: Do LLMs distinguish Spanish varieties?](https://arxiv.org/abs/2504.20049)
*Marina Mayor-Rocher,Cristina Pozo,Nina Melero,Gonzalo Martínez,María Grandury,Pedro Reviriego*

Main category: cs.CL

TL;DR: 研究评估了九种语言模型识别七种西班牙语变体的能力，发现GPT-4o是唯一能识别西班牙语多样性的模型。


<details>
  <summary>Details</summary>
Motivation: 西班牙语存在丰富的地区变体，研究旨在评估语言模型识别这些变体的能力。

Method: 通过多项选择题测试九种模型对七种西班牙语变体的识别能力。

Result: 所有模型中，GPT-4o表现最佳，能识别西班牙语的多样性。

Conclusion: GPT-4o是目前唯一能有效识别西班牙语地区变体的语言模型。

Abstract: In recent years, large language models (LLMs) have demonstrated a high
capacity for understanding and generating text in Spanish. However, with five
hundred million native speakers, Spanish is not a homogeneous language but
rather one rich in diatopic variations spanning both sides of the Atlantic. For
this reason, in this study, we evaluate the ability of nine language models to
identify and distinguish the morphosyntactic and lexical peculiarities of seven
varieties of Spanish (Andean, Antillean, Continental Caribbean, Chilean,
Peninsular, Mexican and Central American and Rioplatense) through a
multiple-choice test. The results indicate that the Peninsular Spanish variety
is the best identified by all models and that, among them, GPT-4o is the only
model capable of recognizing the variability of the Spanish language.
  --
  En los \'ultimos a\~nos, los grandes modelos de lenguaje (LLMs, por sus
siglas en ingl\'es) han demostrado una alta capacidad para comprender y generar
texto en espa\~nol. Sin embargo, con quinientos millones de hablantes nativos,
la espa\~nola no es una lengua homog\'enea, sino rica en variedades
diat\'opicas que se extienden a ambos lados del Atl\'antico. Por todo ello,
evaluamos en este trabajo la capacidad de nueve modelos de lenguaje de
identificar y discernir las peculiaridades morfosint\'acticas y l\'exicas de
siete variedades de espa\~nol (andino, antillano, caribe\~no continental,
chileno, espa\~nol peninsular, mexicano y centroamericano y rioplatense)
mediante un test de respuesta m\'ultiple. Los resultados obtenidos indican que
la variedad de espa\~nol peninsular es la mejor identificada por todos los
modelos y que, de entre todos, GPT-4o es el \'unico modelo capaz de identificar
la variabilidad de la lengua espa\~nola.

</details>


### [2] [Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts](https://arxiv.org/abs/2504.20051)
*Frances Laureano De Leon,Harish Tayyar Madabushi,Mark G. Lee*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型处理多词表达歧义的能力，发现即使是GPT-4等先进模型在多词表达的检测和语义任务中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 多词表达具有非组合意义和句法不规则性，其字面和惯用意义可能导致显著语义变化。研究旨在验证大型语言模型是否能处理此类语言细微差别。

Method: 通过评估英语、葡萄牙语和加利西亚语中的多词表达，使用新颖的代码切换数据集和任务，对比模型性能。

Result: 大型语言模型在多词表达任务中表现不佳，尤其是新任务，GPT-4未能超越xlm-roBERTa-base基线。

Conclusion: 多词表达，尤其是歧义性表达，仍是模型的挑战。

Abstract: Multiword expressions, characterised by non-compositional meanings and
syntactic irregularities, are an example of nuanced language. These expressions
can be used literally or idiomatically, leading to significant changes in
meaning. While large language models have demonstrated strong performance
across many tasks, their ability to handle such linguistic subtleties remains
uncertain. Therefore, this study evaluates how state-of-the-art language models
process the ambiguity of potentially idiomatic multiword expressions,
particularly in contexts that are less frequent, where models are less likely
to rely on memorisation. By evaluating models across in Portuguese and
Galician, in addition to English, and using a novel code-switched dataset and a
novel task, we find that large language models, despite their strengths,
struggle with nuanced language. In particular, we find that the latest models,
including GPT-4, fail to outperform the xlm-roBERTa-base baselines in both
detection and semantic tasks, with especially poor performance on the novel
tasks we introduce, despite its similarity to existing tasks. Overall, our
results demonstrate that multiword expressions, especially those which are
ambiguous, continue to be a challenge to models.

</details>


### [3] [Understanding and Mitigating Risks of Generative AI in Financial Services](https://arxiv.org/abs/2504.20086)
*Sebastian Gehrmann,Claire Huang,Xian Teng,Sergei Yurovski,Iyanuoluwa Shode,Chirag S. Patel,Arjun Bhorkar,Naveen Thomas,John Doucette,David Rosenberg,Mark Dredze,David Rabinowitz*

Main category: cs.CL

TL;DR: 论文探讨了生成式AI（GenAI）产品在金融领域的特定内容安全考量，提出了AI内容风险分类法，并评估现有开源技术护栏的覆盖情况。


<details>
  <summary>Details</summary>
Motivation: 当前对GenAI安全性的讨论多集中于通用领域的毒性、偏见等问题，而忽视了专业领域的法律和监管要求。本文旨在填补金融服务业的内容安全研究空白。

Method: 提出金融领域的AI内容风险分类法，并通过红队测试数据评估现有开源技术护栏的覆盖能力。

Result: 现有护栏未能检测到大部分讨论的内容风险。

Conclusion: 金融领域的GenAI需结合行业法规和治理要求，现有技术护栏需改进以覆盖专业领域风险。

Abstract: To responsibly develop Generative AI (GenAI) products, it is critical to
define the scope of acceptable inputs and outputs. What constitutes a "safe"
response is an actively debated question. Academic work puts an outsized focus
on evaluating models by themselves for general purpose aspects such as
toxicity, bias, and fairness, especially in conversational applications being
used by a broad audience. In contrast, less focus is put on considering
sociotechnical systems in specialized domains. Yet, those specialized systems
can be subject to extensive and well-understood legal and regulatory scrutiny.
These product-specific considerations need to be set in industry-specific laws,
regulations, and corporate governance requirements. In this paper, we aim to
highlight AI content safety considerations specific to the financial services
domain and outline an associated AI content risk taxonomy. We compare this
taxonomy to existing work in this space and discuss implications of risk
category violations on various stakeholders. We evaluate how existing
open-source technical guardrail solutions cover this taxonomy by assessing them
on data collected via red-teaming activities. Our results demonstrate that
these guardrails fail to detect most of the content risks we discuss.

</details>


### [4] [Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models](https://arxiv.org/abs/2504.20157)
*Zae Myung Kim,Chanwoo Park,Vipul Raheja,Dongyeop Kang*

Main category: cs.CL

TL;DR: Meta Policy Optimization (MPO) 是一个通过动态调整奖励模型提示来解决奖励黑客和依赖人工提示工程问题的框架。


<details>
  <summary>Details</summary>
Motivation: 现有奖励对齐方法存在奖励黑客和依赖人工提示工程的局限性，需要更鲁棒和自适应的解决方案。

Method: MPO 引入元奖励模型，动态调整奖励提示以保持对齐，减少人工干预。

Result: MPO 性能优于或等同于手工设计的奖励提示，且适用于多种任务。

Conclusion: MPO 为 LLM 的奖励对齐提供了更稳健和自适应的策略，具有广泛适用性。

Abstract: Reward-based alignment methods for large language models (LLMs) face two key
limitations: vulnerability to reward hacking, where models exploit flaws in the
reward signal; and reliance on brittle, labor-intensive prompt engineering when
LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a
framework that addresses these challenges by integrating a meta-reward model
that dynamically refines the reward model's prompt throughout training. In MPO,
the meta-reward model monitors the evolving training context and continuously
adjusts the reward model's prompt to maintain high alignment, providing an
adaptive reward signal that resists exploitation by the policy. This
meta-learning approach promotes a more stable policy optimization, and greatly
reduces the need for manual reward prompt design. It yields performance on par
with or better than models guided by extensively hand-crafted reward prompts.
Furthermore, we show that MPO maintains its effectiveness across diverse tasks,
such as question answering and mathematical reasoning, without requiring
specialized reward designs. Beyond standard RLAIF, MPO's meta-learning
formulation is readily extensible to higher-level alignment frameworks.
Overall, this method addresses theoretical and practical challenges in
reward-based RL alignment for LLMs, paving the way for more robust and
adaptable alignment strategies. The code and models will be publicly shared.

</details>


### [5] [MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools](https://arxiv.org/abs/2504.20168)
*Nishant Subramani,Jason Eisner,Justin Svegliato,Benjamin Van Durme,Yu Su,Sam Thomson*

Main category: cs.CL

TL;DR: 提出了一种新型模型内部置信度估计器（MICE），通过解码语言模型中间层并计算相似度得分，提升工具调用时的置信度评估。实验表明MICE在校准误差和工具调用效用上优于基线。


<details>
  <summary>Details</summary>
Motivation: 工具调用代理需兼顾实用性和安全性，但现有模型置信度校准不佳，需改进。

Method: MICE解码语言模型各中间层，计算生成内容与最终输出的相似度，通过分类器评估置信度。

Result: 在STE数据集上，MICE在校准误差和工具调用效用上优于基线，且能零样本泛化到新API。

Conclusion: MICE显著提升工具调用的置信度评估，代码已开源。

Abstract: Tool-using agents that act in the world need to be both useful and safe.
Well-calibrated model confidences can be used to weigh the risk versus reward
of potential actions, but prior work shows that many models are poorly
calibrated. Inspired by interpretability literature exploring the internals of
models, we propose a novel class of model-internal confidence estimators (MICE)
to better assess confidence when calling tools. MICE first decodes from each
intermediate layer of the language model using logitLens and then computes
similarity scores between each layer's generation and the final output. These
features are fed into a learned probabilistic classifier to assess confidence
in the decoded output. On the simulated trial and error (STE) tool-calling
dataset using Llama3 models, we find that MICE beats or matches the baselines
on smoothed expected calibration error. Using MICE confidences to determine
whether to call a tool significantly improves over strong baselines on a new
metric, expected tool-calling utility. Further experiments show that MICE is
sample-efficient, can generalize zero-shot to unseen APIs, and results in
higher tool-calling utility in scenarios with varying risk levels. Our code is
open source, available at https://github.com/microsoft/mice_for_cats.

</details>


### [6] [A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports](https://arxiv.org/abs/2504.20220)
*Henning Schäfer,Cynthia S. Schmidt,Johannes Wutzkowsky,Kamil Lorek,Lea Reinartz,Johannes Rückert,Christian Temme,Britta Böckmann,Peter A. Horn,Christoph M. Friedrich*

Main category: cs.CL

TL;DR: 提出了一种开源流程，用于从扫描文档中提取和分类复选框数据，以减少人工转录错误和工作量。


<details>
  <summary>Details</summary>
Motivation: 医疗记录中仍依赖纸质文档，手动转录耗时且易错，需高效自动化解决方案。

Method: 整合复选框检测、多语言OCR和视觉语言模型（VLMs），适用于复选框丰富的文档。

Result: 与2017-2024年的黄金标准相比，实现了高精度和召回率，减少行政负担。

Conclusion: 开源流程可推广至其他复选框文档，提升数据转录效率和准确性。

Abstract: Despite the growing adoption of electronic health records, many processes
still rely on paper documents, reflecting the heterogeneous real-world
conditions in which healthcare is delivered. The manual transcription process
is time-consuming and prone to errors when transferring paper-based data to
digital formats. To streamline this workflow, this study presents an
open-source pipeline that extracts and categorizes checkbox data from scanned
documents. Demonstrated on transfusion reaction reports, the design supports
adaptation to other checkbox-rich document types. The proposed method
integrates checkbox detection, multilingual optical character recognition (OCR)
and multilingual vision-language models (VLMs). The pipeline achieves high
precision and recall compared against annually compiled gold-standards from
2017 to 2024. The result is a reduction in administrative workload and accurate
regulatory reporting. The open-source availability of this pipeline encourages
self-hosted parsing of checkbox forms.

</details>


### [7] [A Platform for Generating Educational Activities to Teach English as a Second Language](https://arxiv.org/abs/2504.20251)
*Aiala Rosá,Santiago Góngora,Juan Pablo Filevich,Ignacio Sastre,Laura Musto,Brian Carpenter,Luis Chiruzzo*

Main category: cs.CL

TL;DR: 介绍了一个基于自然语言处理技术的英语教学平台，支持生成游戏和练习，并提供内容编辑功能，未来计划扩展图像和文本生成。


<details>
  <summary>Details</summary>
Motivation: 为英语作为外语的教学提供多样化的教育活动，利用NLP技术提升教学效果。

Method: 平台结合半自动生成和人工编辑内容，支持教师输入文本生成复杂活动，并计划迁移到更强大的服务器。

Result: 平台已部署，能够生成多样化的教学活动，并支持内容编辑和扩展。

Conclusion: 平台展示了NLP在教育中的应用潜力，未来将进一步扩展功能和性能。

Abstract: We present a platform for the generation of educational activities oriented
to teaching English as a foreign language. The different activities -- games
and language practice exercises -- are strongly based on Natural Language
Processing techniques. The platform offers the possibility of playing
out-of-the-box games, generated from resources created semi-automatically and
then manually curated. It can also generate games or exercises of greater
complexity from texts entered by teachers, providing a stage of review and
edition of the generated content before use. As a way of expanding the variety
of activities in the platform, we are currently experimenting with image and
text generation. In order to integrate them and improve the performance of
other neural tools already integrated, we are working on migrating the platform
to a more powerful server. In this paper we describe the development of our
platform and its deployment for end users, discussing the challenges faced and
how we overcame them, and also detail our future work plans.

</details>


### [8] [Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi](https://arxiv.org/abs/2504.20276)
*Dandan Chen Kaptur,Yue Huang,Xuejun Ryan Ji,Yanhui Guo,Bradley Kaptur*

Main category: cs.CL

TL;DR: 研究比较了GPT-4和Kimi在系统综述中的表现，发现其性能受数据量和问题复杂度影响。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（如GPT-4和Kimi）在系统综述中的表现，并与人工生成的代码进行对比。

Method: 通过比较LLM生成的代码与人工生成的代码，分析其性能差异。

Result: LLM的性能因数据量和问题复杂度而波动。

Conclusion: LLMs在系统综述中的应用需考虑数据量和问题复杂度的影响。

Abstract: This research delved into GPT-4 and Kimi, two Large Language Models (LLMs),
for systematic reviews. We evaluated their performance by comparing
LLM-generated codes with human-generated codes from a peer-reviewed systematic
review on assessment. Our findings suggested that the performance of LLMs
fluctuates by data volume and question complexity for systematic reviews.

</details>


### [9] [UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions](https://arxiv.org/abs/2504.20304)
*Xiulin Yang,Zhuoxuan Ju,Lanni Bu,Zoey Liu,Nathan Schneider*

Main category: cs.CL

TL;DR: 本文介绍了UD-English-CHILDES，这是首个基于CHILDES数据的官方发布的通用依存树库，统一了11名儿童及其照顾者的48k句子的标注，并提供了1M银标准句子。


<details>
  <summary>Details</summary>
Motivation: CHILDES是广泛使用的儿童及儿童导向语音转录资源，但缺乏一致的依存标注。本文旨在提供一个统一且一致的标注资源。

Method: 通过统一和协调11名儿童及其照顾者的依存标注数据，并在UD v2框架下验证现有金标准标注。

Result: 生成了包含48k金标准句子和1M银标准句子的UD-English-CHILDES树库。

Conclusion: UD-English-CHILDES为计算和语言学研究提供了一个一致的资源。

Abstract: CHILDES is a widely used resource of transcribed child and child-directed
speech. This paper introduces UD-English-CHILDES, the first officially released
Universal Dependencies (UD) treebank derived from previously
dependency-annotated CHILDES data with consistent and unified annotation
guidelines. Our corpus harmonizes annotations from 11 children and their
caregivers, totaling over 48k sentences. We validate existing gold-standard
annotations under the UD v2 framework and provide an additional 1M
silver-standard sentences, offering a consistent resource for computational and
linguistic research.

</details>


### [10] [Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation](https://arxiv.org/abs/2504.20323)
*Chao-Lin Liu,Po-Hsien Wu,Yi-Ting Yu*

Main category: cs.CL

TL;DR: 提出一种基于法律文章共引用的方法，用于解决法律推荐系统中标注数据不足的问题，特别是在劳动纠纷等专业领域。


<details>
  <summary>Details</summary>
Motivation: 解决专业法律领域中标注数据稀缺的问题，提升法律推荐系统的自动化标注能力。

Method: 利用法律文章在案件中的共引用关系建立相似性，结合文本嵌入模型和BiLSTM模块进行案例推荐。

Result: 实验表明，该方法能有效推荐相似劳动纠纷案例，验证了共引用方法的有效性。

Conclusion: 该研究为法律文档的自动化标注提供了新思路，尤其适用于法律数据库不完善的领域。

Abstract: This report addresses the challenge of limited labeled datasets for
developing legal recommender systems, particularly in specialized domains like
labor disputes. We propose a new approach leveraging the co-citation of legal
articles within cases to establish similarity and enable algorithmic
annotation. This method draws a parallel to the concept of case co-citation,
utilizing cited precedents as indicators of shared legal issues. To evaluate
the labeled results, we employ a system that recommends similar cases based on
plaintiffs' accusations, defendants' rebuttals, and points of disputes. The
evaluation demonstrates that the recommender, with finetuned text embedding
models and a reasonable BiLSTM module can recommend labor cases whose
similarity was measured by the co-citation of the legal articles. This research
contributes to the development of automated annotation techniques for legal
documents, particularly in areas with limited access to comprehensive legal
databases.

</details>


### [11] [Local Prompt Optimization](https://arxiv.org/abs/2504.20355)
*Yash Jain,Vishal Chowdhary*

Main category: cs.CL

TL;DR: 论文提出局部提示优化（LPO）方法，通过聚焦关键优化令牌提升提示优化效率，显著提升数学推理和BIG-bench Hard基准性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法全局优化所有令牌，导致优化空间过大且指导不足，需改进。

Method: LPO识别提示中的优化令牌，仅优化这些令牌，与通用自动提示工程方法兼容。

Result: 在GSM8k、MultiArith和BIG-bench Hard基准上性能显著提升，且收敛速度更快。

Conclusion: LPO通过局部优化有效提升提示优化效率和性能。

Abstract: In recent years, the use of prompts to guide the output of Large Language
Models have increased dramatically. However, even the best of experts struggle
to choose the correct words to stitch up a prompt for the desired task. To
solve this, LLM driven prompt optimization emerged as an important problem.
Existing prompt optimization methods optimize a prompt globally, where in all
the prompt tokens have to be optimized over a large vocabulary while solving a
complex task. The large optimization space (tokens) leads to insufficient
guidance for a better prompt. In this work, we introduce Local Prompt
Optimization (LPO) that integrates with any general automatic prompt
engineering method. We identify the optimization tokens in a prompt and nudge
the LLM to focus only on those tokens in its optimization step. We observe
remarkable performance improvements on Math Reasoning (GSM8k and MultiArith)
and BIG-bench Hard benchmarks across various automatic prompt engineering
methods. Further, we show that LPO converges to the optimal prompt faster than
global methods.

</details>


### [12] [What Causes Knowledge Loss in Multilingual Language Models?](https://arxiv.org/abs/2504.20356)
*Maria Khelli,Samuel Cahyawijaya,Ayu Purwarianti,Genta Indra Winata*

Main category: cs.CL

TL;DR: 研究探讨了多语言NLP模型中的灾难性遗忘问题，通过LoRA适配器评估参数共享对跨语言迁移的影响。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理多语言数据时难以模拟真实场景，导致灾难性遗忘问题。研究旨在探索语言差异对表示学习的影响。

Method: 使用52种语言和不同等级的LoRA适配器，评估非共享、部分共享和完全共享参数的效果。

Result: 非拉丁文字的语言更易受灾难性遗忘影响，而拉丁文字语言则更利于跨语言迁移。

Conclusion: 参数共享（如LoRA适配器）可缓解灾难性遗忘，但语言脚本类型是关键因素。

Abstract: Cross-lingual transfer in natural language processing (NLP) models enhances
multilingual performance by leveraging shared linguistic knowledge. However,
traditional methods that process all data simultaneously often fail to mimic
real-world scenarios, leading to challenges like catastrophic forgetting, where
fine-tuning on new tasks degrades performance on previously learned ones. Our
study explores this issue in multilingual contexts, focusing on linguistic
differences affecting representational learning rather than just model
parameters. We experiment with 52 languages using LoRA adapters of varying
ranks to evaluate non-shared, partially shared, and fully shared parameters.
Our aim is to see if parameter sharing through adapters can mitigate forgetting
while preserving prior knowledge. We find that languages using non-Latin
scripts are more susceptible to catastrophic forgetting, whereas those written
in Latin script facilitate more effective cross-lingual transfer.

</details>


### [13] [DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation](https://arxiv.org/abs/2504.20371)
*Zhibo Man,Yuanmeng Chen,Yujie Zhang,Yufeng Chen,Jinan Xu*

Main category: cs.CL

TL;DR: 论文提出了一种评估LLMs在多领域翻译中消歧能力的框架DMDTEval，包括构建测试集、设计提示模板和评估指标，揭示了关键发现。


<details>
  <summary>Details</summary>
Motivation: LLMs在多领域翻译中表现不佳，词义歧义问题突出，需评估其消歧能力。

Method: 构建多领域歧义词测试集，设计多样化提示模板和精确评估指标，测试多种LLMs。

Result: 实验揭示了关键发现，为改进LLMs消歧能力的研究提供了方向。

Conclusion: DMDTEval框架为评估和提升LLMs在多领域翻译中的消歧能力奠定了基础。

Abstract: Currently, Large Language Models (LLMs) have achieved remarkable results in
machine translation. However, their performance in multi-domain translation
(MDT) is less satisfactory; the meanings of words can vary across different
domains, highlighting the significant ambiguity inherent in MDT. Therefore,
evaluating the disambiguation ability of LLMs in MDT remains an open problem.
To this end, we present an evaluation and analysis of LLMs on disambiguation in
multi-domain translation (DMDTEval), our systematic evaluation framework
consisting of three critical aspects: (1) we construct a translation test set
with multi-domain ambiguous word annotation, (2) we curate a diverse set of
disambiguation prompting templates, and (3) we design precise disambiguation
metrics, and study the efficacy of various prompting strategies on multiple
state-of-the-art LLMs. Our extensive experiments reveal a number of crucial
findings that we believe will pave the way and also facilitate further research
in the critical area of improving the disambiguation of LLMs.

</details>


### [14] [On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?](https://arxiv.org/abs/2504.20444)
*Mika Hämäläinen*

Main category: cs.CL

TL;DR: 研究三种商业LLM（ChatGPT、Gemini和Claude）中的首因效应，通过改造Asch（1946）实验发现，不同模型在不同实验条件下对候选人的偏好存在差异。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM是否像人类一样受首因效应影响，即在描述顺序不同时是否表现出偏好差异。

Method: 通过两种实验设计：1）同时呈现两个候选人描述；2）分别呈现描述，测试模型对200对候选人的选择偏好。

Result: 实验1中，ChatGPT偏好正面形容词先出现的候选人，Gemini无偏好，Claude拒绝选择；实验2中，ChatGPT和Claude多评分相等，否则偏好负面形容词先出现的候选人，Gemini则更偏好负面形容词先出现的候选人。

Conclusion: 不同LLM对首因效应的反应各异，表明模型设计可能影响其决策行为。

Abstract: We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and
Claude. We do this by repurposing the famous experiment Asch (1946) conducted
using human subjects. The experiment is simple, given two candidates with equal
descriptions which one is preferred if one description has positive adjectives
first before negative ones and another description has negative adjectives
followed by positive ones. We test this in two experiments. In one experiment,
LLMs are given both candidates simultaneously in the same prompt, and in
another experiment, LLMs are given both candidates separately. We test all the
models with 200 candidate pairs. We found that, in the first experiment,
ChatGPT preferred the candidate with positive adjectives listed first, while
Gemini preferred both equally often. Claude refused to make a choice. In the
second experiment, ChatGPT and Claude were most likely to rank both candidates
equally. In the case where they did not give an equal rating, both showed a
clear preference to a candidate that had negative adjectives listed first.
Gemini was most likely to prefer a candidate with negative adjectives listed
first.

</details>


### [15] [Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs](https://arxiv.org/abs/2504.20451)
*Daniel Lee,Harsh Sharma,Jieun Han,Sunny Jeong,Alice Oh,Vered Shwartz*

Main category: cs.CL

TL;DR: LLMs在英韩翻译中优于传统机器翻译系统，但在需要文化适应的实体翻译上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究知识密集和实体丰富的文本在英韩翻译中的挑战，特别是文化适应问题。

Method: 评估13种模型（LLMs和MT模型），结合自动指标和双语标注者的人工评估，构建错误分类法。

Result: LLMs表现优于传统MT系统，但在实体翻译和文化适应上存在不足，错误主要集中在实体名称和错误响应。

Conclusion: 揭示了自动评估指标的局限性，为未来研究文化敏感的机器翻译提供了方向。

Abstract: Translating knowledge-intensive and entity-rich text between English and
Korean requires transcreation to preserve language-specific and cultural
nuances beyond literal, phonetic or word-for-word conversion. We evaluate 13
models (LLMs and MT models) using automatic metrics and human assessment by
bilingual annotators. Our findings show LLMs outperform traditional MT systems
but struggle with entity translation requiring cultural adaptation. By
constructing an error taxonomy, we identify incorrect responses and entity name
errors as key issues, with performance varying by entity type and popularity
level. This work exposes gaps in automatic evaluation metrics and hope to
enable future work in completing culturally-nuanced machine translation.

</details>


### [16] [Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models](https://arxiv.org/abs/2504.20469)
*Enfa Fane,Mihai Surdeanu,Eduardo Blanco,Steven R. Corman*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在零样本条件下对新闻叙事中实体框架角色的分类能力，通过分层方法和优化输入上下文与提示策略，取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 研究新闻叙事如何框架实体对理解媒体对社会事件认知的影响至关重要，探索LLMs在此任务中的潜力。

Method: 采用分层分类方法，先识别广泛角色再细化角色，并系统评估输入上下文、提示策略和任务分解的影响。

Result: 分层方法优于单步分类，主角色准确率达89.4%，精确匹配率为34.5%。

Conclusion: 优化提示设计和输入上下文对提升LLMs在实体框架任务中的性能至关重要。

Abstract: Understanding how news narratives frame entities is crucial for studying
media's impact on societal perceptions of events. In this paper, we evaluate
the zero-shot capabilities of large language models (LLMs) in classifying
framing roles. Through systematic experimentation, we assess the effects of
input context, prompting strategies, and task decomposition. Our findings show
that a hierarchical approach of first identifying broad roles and then
fine-grained roles, outperforms single-step classification. We also demonstrate
that optimal input contexts and prompts vary across task levels, highlighting
the need for subtask-specific strategies. We achieve a Main Role Accuracy of
89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of our
approach. Our findings emphasize the importance of tailored prompt design and
input context optimization for improving LLM performance in entity framing.

</details>


### [17] [Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training](https://arxiv.org/abs/2504.20484)
*Linjuan Wu,Haoran Wei,Huan Lin,Tianhao Li,Baosong Yang,Weiming Lu*

Main category: cs.CL

TL;DR: CrossIC-PT通过语义相关的双语文本增强跨语言迁移，提升多语言性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言迁移方法受限于平行资源，覆盖范围有限。

Method: 利用语义相关的双语文本进行上下文预训练，采用分段策略保持上下文连贯性。

Result: 在三种模型和六种目标语言上性能提升显著，最高达3.99%。

Conclusion: CrossIC-PT是一种简单且可扩展的方法，有效提升多语言模型性能。

Abstract: Large language models (LLMs) exhibit remarkable multilingual capabilities
despite English-dominated pre-training, attributed to cross-lingual mechanisms
during pre-training. Existing methods for enhancing cross-lingual transfer
remain constrained by parallel resources, suffering from limited linguistic and
domain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT),
a simple and scalable approach that enhances cross-lingual transfer by
leveraging semantically related bilingual texts via simple next-word
prediction. We construct CrossIC-PT samples by interleaving semantic-related
bilingual Wikipedia documents into a single context window. To access window
size constraints, we implement a systematic segmentation policy to split long
bilingual document pairs into chunks while adjusting the sliding window
mechanism to preserve contextual coherence. We further extend data availability
through a semantic retrieval framework to construct CrossIC-PT samples from
web-crawled corpus. Experimental results demonstrate that CrossIC-PT improves
multilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and
Qwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%,
3.99%, and 1.95%, respectively, with additional improvements after data
augmentation.

</details>


### [18] [UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation](https://arxiv.org/abs/2504.20500)
*Huimin Lu,Masaru Isonuma,Junichiro Mori,Ichiro Sakata*

Main category: cs.CL

TL;DR: UniDetox是一种通用的去毒方法，适用于多种大型语言模型（LLM），无需针对不同模型单独调参。


<details>
  <summary>Details</summary>
Motivation: 现有去毒方法通常针对特定模型或模型家族，且需权衡去毒效果与语言建模性能，调参复杂。

Method: 提出一种基于对比解码的数据蒸馏技术，生成合成文本数据，通过微调实现通用去毒。

Result: 实验表明，从GPT-2蒸馏的去毒文本可有效应用于OPT、Falcon和LLaMA-2等更大模型，且无需单独调参。

Conclusion: UniDetox不仅实现通用去毒，还减少政治偏见内容，为LLM去毒提供了新思路。

Abstract: We present UniDetox, a universally applicable method designed to mitigate
toxicity across various large language models (LLMs). Previous detoxification
methods are typically model-specific, addressing only individual models or
model families, and require careful hyperparameter tuning due to the trade-off
between detoxification efficacy and language modeling performance. In contrast,
UniDetox provides a detoxification technique that can be universally applied to
a wide range of LLMs without the need for separate model-specific tuning.
Specifically, we propose a novel and efficient dataset distillation technique
for detoxification using contrastive decoding. This approach distills
detoxifying representations in the form of synthetic text data, enabling
universal detoxification of any LLM through fine-tuning with the distilled
text. Our experiments demonstrate that the detoxifying text distilled from
GPT-2 can effectively detoxify larger models, including OPT, Falcon, and
LLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter
tuning for each model, as a single hyperparameter configuration can be
seamlessly applied across different models. Additionally, analysis of the
detoxifying text reveals a reduction in politically biased content, providing
insights into the attributes necessary for effective detoxification of LLMs.

</details>


### [19] [Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records](https://arxiv.org/abs/2504.20547)
*Jesus Lovon,Thouria Ben-Haddi,Jules Di Scala,Jose G. Moreno,Lynda Tamine*

Main category: cs.CL

TL;DR: 论文重新评估了MIMIC-IV医疗数据基准，将其整合到Hugging Face库中，并探索了将表格数据转换为文本的方法。实验表明，微调的文本模型在患者死亡率任务上表现优于零样本LLM。


<details>
  <summary>Details</summary>
Motivation: 医疗领域缺乏标准化的文本评估基准，限制了自然语言模型在健康相关任务中的应用。

Method: 整合MIMIC-IV数据到Hugging Face库，探索表格数据转文本的模板方法，并比较微调与零样本LLM的表现。

Result: 微调文本模型在患者死亡率任务上表现优于零样本LLM，后者难以有效利用EHR数据。

Conclusion: 文本方法在医疗领域有潜力，但零样本LLM仍需改进。

Abstract: The lack of standardized evaluation benchmarks in the medical domain for text
inputs can be a barrier to widely adopting and leveraging the potential of
natural language models for health-related downstream tasks. This paper
revisited an openly available MIMIC-IV benchmark for electronic health records
(EHRs) to address this issue. First, we integrate the MIMIC-IV data within the
Hugging Face datasets library to allow an easy share and use of this
collection. Second, we investigate the application of templates to convert EHR
tabular data to text. Experiments using fine-tuned and zero-shot LLMs on the
mortality of patients task show that fine-tuned text-based models are
competitive against robust tabular classifiers. In contrast, zero-shot LLMs
struggle to leverage EHR representations. This study underlines the potential
of text-based approaches in the medical field and highlights areas for further
improvement.

</details>


### [20] [BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters](https://arxiv.org/abs/2504.20552)
*Baz Roland,Kristina Malyseva,Anna Pappa,Tristan Cazenave*

Main category: cs.CL

TL;DR: BrAIcht是一个基于德国LeoLM的AI对话代理，能够生成类似剧作家Bertolt Brecht风格的对话。通过QLoRA技术微调，使用Brecht和其他类似风格的德国剧本数据集，性能表现优异。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个能够模仿Bertolt Brecht独特风格的AI对话代理，以丰富创意写作和戏剧领域。

Method: 使用德国LeoLM（70亿参数）和Llama2的修改版本，通过QLoRA技术进行参数高效微调，数据集包括29部Brecht剧本和907部类似风格的德国剧本。

Result: 基于BLEU分数和困惑度评估，BrAIcht在生成Brecht风格对话方面表现出色。

Conclusion: BrAIcht成功实现了模仿Brecht风格的目标，为创意写作和戏剧提供了新的工具。

Abstract: This project introduces BrAIcht, an AI conversational agent that creates
dialogues in the distinctive style of the famous German playwright Bertolt
Brecht. BrAIcht is fine-tuned using German LeoLM, a large language model with 7
billion parameters and a modified version of the base Llama2 suitable for
German language tasks. For fine-tuning, 29 plays of Bertolt Brecht and 907 of
other German plays that are stylistically similar to Bertolt Brecht are used to
form a more di-erse dataset. Due to the limited memory capacity, a
parameterefficient fine-tuning technique called QLoRA is implemented to train
the large language model. The results, based on BLEU score and perplexity, show
very promising performance of BrAIcht in generating dialogues in the style of
Bertolt Brecht.

</details>


### [21] [ClonEval: An Open Voice Cloning Benchmark](https://arxiv.org/abs/2504.20581)
*Iwona Christop,Tomasz Kuczyński,Marek Kubis*

Main category: cs.CL

TL;DR: 论文介绍了一个新的语音克隆文本转语音模型基准，包括评估协议、开源库和排行榜。


<details>
  <summary>Details</summary>
Motivation: 为语音克隆模型提供一个标准化的评估工具和平台，促进模型性能的比较和改进。

Method: 设计了评估协议，开发了开源库用于性能评估，并建立了排行榜展示结果。

Result: 提出了一个完整的语音克隆模型评估框架，包括工具和平台。

Conclusion: 该基准为语音克隆领域的研究提供了实用的评估工具，有助于推动技术进步。

Abstract: We present a novel benchmark for voice cloning text-to-speech models. The
benchmark consists of an evaluation protocol, an open-source library for
assessing the performance of voice cloning models, and an accompanying
leaderboard. The paper discusses design considerations and presents a detailed
description of the evaluation procedure. The usage of the software library is
explained, along with the organization of results on the leaderboard.

</details>


### [22] [TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models](https://arxiv.org/abs/2504.20605)
*Mihai Nadas,Laura Diosan,Andrei Piscoran,Andreea Tomescu*

Main category: cs.CL

TL;DR: TF1-EN-3M是一个由8B参数模型生成的300万英语寓言数据集，包含结构化叙事和明确道德教训，填补了NLP领域空白。


<details>
  <summary>Details</summary>
Motivation: 现代NLP缺乏结合连贯叙事和明确道德教训的大规模结构化语料库，TF1-EN-3M旨在解决这一问题。

Method: 使用组合提示引擎生成遵循六槽模板的寓言，并通过混合评估流程（GPT评分和无参考指标）优化模型选择。

Result: 8B参数的Llama-3变体在质量和速度上表现最佳，单GPU即可高效生成高质量寓言。

Conclusion: TF1-EN-3M为指令遵循、叙事智能等研究提供资源，证明大规模道德叙事无需依赖专有大模型。

Abstract: Moral stories are a time-tested vehicle for transmitting values, yet modern
NLP lacks a large, structured corpus that couples coherent narratives with
explicit ethical lessons. We close this gap with TF1-EN-3M, the first open
dataset of three million English-language fables generated exclusively by
instruction-tuned models no larger than 8B parameters. Each story follows a
six-slot scaffold (character -> trait -> setting -> conflict -> resolution ->
moral), produced through a combinatorial prompt engine that guarantees genre
fidelity while covering a broad thematic space.
  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores
grammar, creativity, moral clarity, and template adherence with (ii)
reference-free diversity and readability metrics. Among ten open-weight
candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed
trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)
at approximately 13.5 cents per 1,000 fables.
  We release the dataset, generation code, evaluation scripts, and full
metadata under a permissive license, enabling exact reproducibility and cost
benchmarking. TF1-EN-3M opens avenues for research in instruction following,
narrative intelligence, value alignment, and child-friendly educational AI,
demonstrating that large-scale moral storytelling no longer requires
proprietary giant models.

</details>


### [23] [WenyanGPT: A Large Language Model for Classical Chinese Tasks](https://arxiv.org/abs/2504.20609)
*Xinyu Yao,Mengdi Wang,Bo Chen,Xiaobing Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种针对文言文的语言处理解决方案，通过预训练和指令微调构建了WenyanGPT模型，并开发了评估数据集WenyanBENCH，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言处理模型主要针对现代汉语，对文言文处理效果不佳，阻碍了古代文学的研究与传承。

Method: 在LLaMA3-8B-Chinese模型基础上进行预训练和指令微调，构建WenyanGPT模型，并开发评估数据集WenyanBENCH。

Result: 实验表明，WenyanGPT在文言文任务上显著优于现有先进模型。

Conclusion: WenyanGPT为文言文处理提供了高效解决方案，相关数据和模型已公开以推动领域发展。

Abstract: Classical Chinese, as the core carrier of Chinese culture, plays a crucial
role in the inheritance and study of ancient literature. However, existing
natural language processing models primarily optimize for Modern Chinese,
resulting in inadequate performance on Classical Chinese. This paper presents a
comprehensive solution for Classical Chinese language processing. By continuing
pre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we
construct a large language model, WenyanGPT, which is specifically designed for
Classical Chinese tasks. Additionally, we develop an evaluation benchmark
dataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that
WenyanGPT significantly outperforms current advanced LLMs in various Classical
Chinese tasks. We make the model's training data, instruction fine-tuning
data\footnote, and evaluation benchmark dataset publicly available to promote
further research and development in the field of Classical Chinese processing.

</details>


### [24] [Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations](https://arxiv.org/abs/2504.20643)
*Moran Mizrahi,Chen Shani,Gabriel Stanovsky,Dan Jurafsky,Dafna Shahaf*

Main category: cs.CL

TL;DR: 本文提出了一种结合LLMs与结构化表示和认知启发操作的新方法，以生成更具创造性和多样性的想法，并在烹饪领域展示了其效果。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多任务中表现出色，但在创造力方面仍有不足。本文旨在通过结构化表示和认知启发操作提升LLMs的创造力。

Method: 通过耦合LLMs与结构化表示和认知启发操作，生成更抽象和多样化的想法。在烹饪领域应用了DishCOVER模型。

Result: 实验表明，DishCOVER生成的食谱比GPT-4o更具多样性。专家评估显示，其输出在新颖性上显著优于GPT-4o。

Conclusion: 本文方法在创造力生成方面优于GPT-4o，并希望激发更多关于AI结构化创造力的研究。

Abstract: Large Language Models (LLMs) excel at countless tasks, yet struggle with
creativity. In this paper, we introduce a novel approach that couples LLMs with
structured representations and cognitively inspired manipulations to generate
more creative and diverse ideas. Our notion of creativity goes beyond
superficial token-level variations; rather, we explicitly recombine structured
representations of existing ideas, allowing our algorithm to effectively
explore the more abstract landscape of ideas. We demonstrate our approach in
the culinary domain with DishCOVER, a model that generates creative recipes.
Experiments comparing our model's results to those of GPT-4o show greater
diversity. Domain expert evaluations reveal that our outputs, which are mostly
coherent and feasible culinary creations, significantly surpass GPT-4o in terms
of novelty, thus outperforming it in creative generation. We hope our work
inspires further research into structured creativity in AI.

</details>


### [25] [A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages](https://arxiv.org/abs/2504.20668)
*Ivan Vykopal,Martin Hyben,Robert Moro,Michal Gregor,Jakub Simko*

Main category: cs.CL

TL;DR: 该研究提出了一种利用大语言模型（LLMs）检索和评估已核实信息的方法，以减少重复工作并提升事实核查效率。


<details>
  <summary>Details</summary>
Motivation: 在线虚假信息泛滥，事实核查者需高效验证以避免冗余工作。

Method: 使用LLMs筛选无关事实核查并生成摘要，辅助核查者快速判断。

Result: LLMs能有效过滤无关信息，减少工作量并优化流程。

Conclusion: 该方法显著提升了事实核查的效率和响应速度。

Abstract: Online disinformation poses a global challenge, placing significant demands
on fact-checkers who must verify claims efficiently to prevent the spread of
false information. A major issue in this process is the redundant verification
of already fact-checked claims, which increases workload and delays responses
to newly emerging claims. This research introduces an approach that retrieves
previously fact-checked claims, evaluates their relevance to a given input, and
provides supplementary information to support fact-checkers. Our method employs
large language models (LLMs) to filter irrelevant fact-checks and generate
concise summaries and explanations, enabling fact-checkers to faster assess
whether a claim has been verified before. In addition, we evaluate our approach
through both automatic and human assessments, where humans interact with the
developed tool to review its effectiveness. Our results demonstrate that LLMs
are able to filter out many irrelevant fact-checks and, therefore, reduce
effort and streamline the fact-checking process.

</details>


### [26] [Non-native Children's Automatic Speech Assessment Challenge (NOCASA)](https://arxiv.org/abs/2504.20678)
*Yaroslav Getman,Tamás Grósz,Mikko Kurimo,Giampiero Salvi*

Main category: cs.CL

TL;DR: NOCASA竞赛要求开发系统评估非母语儿童单词语音，提供数据集TeflonNorL2和基线模型，最佳模型UAR为36.37%。


<details>
  <summary>Details</summary>
Motivation: 解决非母语儿童发音评估中的数据不足和类别不平衡问题。

Method: 提供数据集TeflonNorL2和两种基线模型（SVM和wav2vec 2.0）。

Result: wav2vec 2.0模型表现最佳，UAR为36.37%。

Conclusion: NOCASA竞赛推动了发音评估技术的发展，但仍有改进空间。

Abstract: This paper presents the "Non-native Children's Automatic Speech Assessment"
(NOCASA) - a data competition part of the IEEE MLSP 2025 conference. NOCASA
challenges participants to develop new systems that can assess single-word
pronunciations of young second language (L2) learners as part of a gamified
pronunciation training app. To achieve this, several issues must be addressed,
most notably the limited nature of available training data and the highly
unbalanced distribution among the pronunciation level categories. To expedite
the development, we provide a pseudo-anonymized training data (TeflonNorL2),
containing 10,334 recordings from 44 speakers attempting to pronounce 205
distinct Norwegian words, human-rated on a 1 to 5 scale (number of stars that
should be given in the game). In addition to the data, two already trained
systems are released as official baselines: an SVM classifier trained on the
ComParE_16 acoustic feature set and a multi-task wav2vec 2.0 model. The latter
achieves the best performance on the challenge test set, with an unweighted
average recall (UAR) of 36.37%.

</details>


### [27] [Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?](https://arxiv.org/abs/2504.20679)
*Wing Yan Li,Zeqiang Wang,Jon Johnson,Suparna De*

Main category: cs.CL

TL;DR: 论文提出了一种新的信息检索任务，用于识别纵向社会科学调查中的语义等效问题，并比较了多种无监督方法的效果。


<details>
  <summary>Details</summary>
Motivation: 解决纵向调查中概念和子概念不一致以及词汇结构演变带来的挑战，以支持社会科学研究。

Method: 研究了多种无监督方法，包括概率模型、语言模型线性探测和预训练的IR专用神经网络。

Result: IR专用神经网络表现最佳，其他方法效果相近；重排序仅带来微小改进（F1分数最多提高0.07）。

Conclusion: 研究为社会科学纵向研究的协调提供了进一步支持，但模型对高词汇重叠问题的敏感性较低。

Abstract: Automated detection of semantically equivalent questions in longitudinal
social science surveys is crucial for long-term studies informing empirical
research in the social, economic, and health sciences. Retrieving equivalent
questions faces dual challenges: inconsistent representation of theoretical
constructs (i.e. concept/sub-concept) across studies as well as between
question and response options, and the evolution of vocabulary and structure in
longitudinal text. To address these challenges, our multi-disciplinary
collaboration of computer scientists and survey specialists presents a new
information retrieval (IR) task of identifying concept (e.g. Housing, Job,
etc.) equivalence across question and response options to harmonise
longitudinal population studies. This paper investigates multiple unsupervised
approaches on a survey dataset spanning 1946-2020, including probabilistic
models, linear probing of language models, and pre-trained neural networks
specialised for IR. We show that IR-specialised neural models achieve the
highest overall performance with other approaches performing comparably.
Additionally, the re-ranking of the probabilistic model's results with neural
models only introduces modest improvements of 0.07 at most in F1-score.
Qualitative post-hoc evaluation by survey specialists shows that models
generally have a low sensitivity to questions with high lexical overlap,
particularly in cases where sub-concepts are mismatched. Altogether, our
analysis serves to further research on harmonising longitudinal studies in
social science.

</details>


### [28] [Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?](https://arxiv.org/abs/2504.20699)
*Evangelia Gogoulou,Shorouq Zahra,Liane Guillou,Luise Dürlich,Joakim Nivre*

Main category: cs.CL

TL;DR: 论文研究了LLMs在翻译和释义任务中检测幻觉的能力，发现性能因模型而异，但提示选择影响不大。NLI模型表现相近，表明LLM检测器并非唯一可行方案。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs生成内容中常见的幻觉问题，评估其在特定任务中的检测能力。

Method: 使用HalluciGen任务评估开源LLMs在翻译和释义任务中的幻觉检测能力，研究模型大小、指令调整和提示选择的影响。

Result: 模型性能因任务和语言而异，但提示选择影响较小；NLI模型表现与LLMs相当。

Conclusion: LLM检测器并非唯一有效方案，NLI模型在幻觉检测中同样可行。

Abstract: A frequently observed problem with LLMs is their tendency to generate output
that is nonsensical, illogical, or factually incorrect, often referred to
broadly as hallucination. Building on the recently proposed HalluciGen task for
hallucination detection and generation, we evaluate a suite of open-access LLMs
on their ability to detect intrinsic hallucinations in two conditional
generation tasks: translation and paraphrasing. We study how model performance
varies across tasks and language and we investigate the impact of model size,
instruction tuning, and prompt choice. We find that performance varies across
models but is consistent across prompts. Finally, we find that NLI models
perform comparably well, suggesting that LLM-based detectors are not the only
viable option for this specific task.

</details>


### [29] [BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification](https://arxiv.org/abs/2504.20703)
*Foteini Papadopoulou,Osman Mutlu,Neris Özen,Bas H. M. van der Velden,Iris Hendrickx,Ali Hürriyetoğlu*

Main category: cs.CL

TL;DR: 论文提出了针对SemEval-2025任务9的食品危害检测系统，通过文本增强技术提升少数类别的分类性能，并比较了不同模型的效果。


<details>
  <summary>Details</summary>
Motivation: 解决食品召回事件报告中少数类别分类性能不佳的问题。

Method: 采用三种词级数据增强技术（同义词替换、随机词交换、上下文词插入），并在多种模型上进行比较。

Result: Transformer模型整体表现更好，但增强技术未显著提升整体性能；BERT模型在细粒度类别上表现显著提升，上下文词插入技术使少数危害类别的预测准确率提高6%。

Conclusion: 针对少数类别的定向增强可以提升Transformer模型的性能。

Abstract: This paper presents our system developed for the SemEval-2025 Task 9: The
Food Hazard Detection Challenge. The shared task's objective is to evaluate
explainable classification systems for classifying hazards and products in two
levels of granularity from food recall incident reports. In this work, we
propose text augmentation techniques as a way to improve poor performance on
minority classes and compare their effect for each category on various
transformer and machine learning models. We explore three word-level data
augmentation techniques, namely synonym replacement, random word swapping, and
contextual word insertion. The results show that transformer models tend to
have a better overall performance. None of the three augmentation techniques
consistently improved overall performance for classifying hazards and products.
We observed a statistically significant improvement (P < 0.05) in the
fine-grained categories when using the BERT model to compare the baseline with
each augmented model. Compared to the baseline, the contextual words insertion
augmentation improved the accuracy of predictions for the minority hazard
classes by 6%. This suggests that targeted augmentation of minority classes can
improve the performance of transformer models.

</details>


### [30] [Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think](https://arxiv.org/abs/2504.20708)
*Hasan Abed Al Kader Hammoud,Hani Itani,Bernard Ghanem*

Main category: cs.CL

TL;DR: 论文提出了一种通过分析中间推理步骤（子思维）来评估大语言模型（LLM）的方法，发现聚合不同子思维的答案能显著提高准确性。


<details>
  <summary>Details</summary>
Motivation: 挑战传统仅依赖最终答案的评估方式，探讨最终答案是否能代表模型的最优结论，以及不同推理路径是否会产生不同结果。

Method: 将推理轨迹分段为子思维，从每个子思维生成延续并提取答案，选择最频繁的答案（众数）作为最终结果。

Result: 实验表明，该方法在多个LLM和数学推理数据集上显著提升准确性，最高达13%和10%。

Conclusion: 分析子思维一致性可揭示模型信心和正确性，为识别不可靠答案提供潜力。

Abstract: Large Language Models (LLMs) leverage step-by-step reasoning to solve complex
problems. Standard evaluation practice involves generating a complete reasoning
trace and assessing the correctness of the final answer presented at its
conclusion. In this paper, we challenge the reliance on the final answer by
posing the following two questions: Does the final answer reliably represent
the model's optimal conclusion? Can alternative reasoning paths yield different
results? To answer these questions, we analyze intermediate reasoning steps,
termed subthoughts, and propose a method based on our findings. Our approach
involves segmenting a reasoning trace into sequential subthoughts based on
linguistic cues. We start by prompting the model to generate continuations from
the end-point of each intermediate subthought. We extract a potential answer
from every completed continuation originating from different subthoughts. We
find that aggregating these answers by selecting the most frequent one (the
mode) often yields significantly higher accuracy compared to relying solely on
the answer derived from the original complete trace. Analyzing the consistency
among the answers derived from different subthoughts reveals characteristics
that correlate with the model's confidence and correctness, suggesting
potential for identifying less reliable answers. Our experiments across various
LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025)
show consistent accuracy improvements, with gains reaching up to 13\% and 10\%
respectively. Implementation is available at:
https://github.com/hammoudhasan/SubthoughtReasoner.

</details>


### [31] [UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities](https://arxiv.org/abs/2504.20734)
*Woongyeong Yeo,Kangsan Kim,Soyeong Jeong,Jinheon Baek,Sung Ju Hwang*

Main category: cs.CL

TL;DR: UniversalRAG是一个新的RAG框架，支持从多模态异构知识源中检索和整合信息，解决了现有RAG方法局限于单一模态的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法通常局限于单一模态，无法满足现实世界中多样化查询的需求。

Method: 提出了一种模态感知路由机制，动态选择最适合的模态特定语料库，并在其中进行针对性检索，同时按粒度组织模态。

Result: 在8个多模态基准测试中，UniversalRAG优于单一模态和统一基线方法。

Conclusion: UniversalRAG通过多模态和粒度感知的检索机制，显著提升了RAG的适用性和性能。

Abstract: Retrieval-Augmented Generation (RAG) has shown substantial promise in
improving factual accuracy by grounding model responses with external knowledge
relevant to queries. However, most existing RAG approaches are limited to a
text-only corpus, and while recent efforts have extended RAG to other
modalities such as images and videos, they typically operate over a single
modality-specific corpus. In contrast, real-world queries vary widely in the
type of knowledge they require, which a single type of knowledge source cannot
address. To address this, we introduce UniversalRAG, a novel RAG framework
designed to retrieve and integrate knowledge from heterogeneous sources with
diverse modalities and granularities. Specifically, motivated by the
observation that forcing all modalities into a unified representation space
derived from a single combined corpus causes a modality gap, where the
retrieval tends to favor items from the same modality as the query, we propose
a modality-aware routing mechanism that dynamically identifies the most
appropriate modality-specific corpus and performs targeted retrieval within it.
Also, beyond modality, we organize each modality into multiple granularity
levels, enabling fine-tuned retrieval tailored to the complexity and scope of
the query. We validate UniversalRAG on 8 benchmarks spanning multiple
modalities, showing its superiority over modality-specific and unified
baselines.

</details>


### [32] [Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers](https://arxiv.org/abs/2504.20752)
*Roman Abramov,Felix Steinbauer,Gjergji Kasneci*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Transformers have achieved great success in numerous NLP tasks but continue
to exhibit notable gaps in multi-step factual reasoning, especially when
real-world knowledge is sparse. Recent advances in grokking have demonstrated
that neural networks can transition from memorizing to perfectly generalizing
once they detect underlying logical patterns - yet these studies have primarily
used small, synthetic tasks. In this paper, for the first time, we extend
grokking to real-world factual data and address the challenge of dataset
sparsity by augmenting existing knowledge graphs with carefully designed
synthetic data to raise the ratio $\phi_r$ of inferred facts to atomic facts
above the threshold required for grokking. Surprisingly, we find that even
factually incorrect synthetic data can strengthen emergent reasoning circuits
rather than degrade accuracy, as it forces the model to rely on relational
structure rather than memorization. When evaluated on multi-hop reasoning
benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -
substantially improving over strong baselines and matching or exceeding current
state-of-the-art results. We further provide an in-depth analysis of how
increasing $\phi_r$ drives the formation of generalizing circuits inside
Transformers. Our findings suggest that grokking-based data augmentation can
unlock implicit multi-hop reasoning capabilities, opening the door to more
robust and interpretable factual reasoning in large-scale language models.

</details>


### [33] [Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption](https://arxiv.org/abs/2504.20769)
*Wenxiao Wang,Parsa Hosseini,Soheil Feizi*

Main category: cs.CL

TL;DR: 链式防御思维提示显著提升大语言模型在非推理任务中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用链式思维提示增强的推理能力，提升大语言模型在非推理任务中的鲁棒性。

Method: 提出链式防御思维提示方法，仅需提供少量结构化防御推理示例。

Result: 在自然问题任务中，GPT-4o的准确率从标准提示下的3%提升至50%。

Conclusion: 链式防御思维提示简单有效，显著提升模型对抗参考数据污染的鲁棒性。

Abstract: Chain-of-thought prompting has demonstrated great success in facilitating the
reasoning abilities of large language models. In this work, we explore how
these enhanced reasoning abilities can be exploited to improve the robustness
of large language models in tasks that are not necessarily reasoning-focused.
In particular, we show how a wide range of large language models exhibit
significantly improved robustness against reference corruption using a simple
method called chain-of-defensive-thought, where only a few exemplars with
structured and defensive reasoning are provided as demonstrations. Empirically,
the improvements can be astounding, especially given the simplicity and
applicability of the method. For example, in the Natural Questions task, the
accuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting
when 1 out of 10 references provided is corrupted with prompt injection
attacks. In contrast, GPT-4o using chain-of-defensive-thought prompting
maintains an accuracy of 50%.

</details>


### [34] [Turing Machine Evaluation for Large Language Model](https://arxiv.org/abs/2504.20771)
*Haitao Wu,Zongbo Han,Huaxi Huang,Changqing Zhang*

Main category: cs.CL

TL;DR: 该研究提出了一种基于通用图灵机（UTM）模拟的评估框架TMBench，用于系统评估大语言模型（LLMs）的核心计算推理能力，发现其性能与其他推理基准强相关。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，对其核心计算推理能力（如准确理解规则和执行逻辑计算）的评估变得至关重要。

Method: 提出基于UTM模拟的评估框架TMBench，要求LLMs在多步计算中严格遵循指令并跟踪动态状态。

Result: TMBench具有知识无关性、可调难度和无限实例生成能力，模型性能与其他推理基准强相关（Pearson系数0.73）。

Conclusion: 计算推理能力是衡量LLMs深层能力的重要维度，TMBench为标准化评估提供了有效工具。

Abstract: With the rapid development and widespread application of Large Language
Models (LLMs), rigorous evaluation has become particularly crucial. This
research adopts a novel perspective, focusing on evaluating the core
computational reasoning ability of LLMs, defined as the capacity of model to
accurately understand rules, and execute logically computing operations. This
capability assesses the reliability of LLMs as precise executors, and is
critical to advanced tasks such as complex code generation and multi-step
problem-solving. We propose an evaluation framework based on Universal Turing
Machine (UTM) simulation. This framework requires LLMs to strictly follow
instructions and track dynamic states, such as tape content and read/write head
position, during multi-step computations. To enable standardized evaluation, we
developed TMBench, a benchmark for systematically studying the computational
reasoning capabilities of LLMs. TMBench provides several key advantages,
including knowledge-agnostic evaluation, adjustable difficulty, foundational
coverage through Turing machine encoding, and unlimited capacity for instance
generation, ensuring scalability as models continue to evolve. We find that
model performance on TMBench correlates strongly with performance on other
recognized reasoning benchmarks (Pearson correlation coefficient is 0.73),
clearly demonstrating that computational reasoning is a significant dimension
for measuring the deep capabilities of LLMs. Code and data are available at
https://github.com/HaitaoWuTJU/Turing-Machine-Bench.

</details>


### [35] [Universal language model with the intervention of quantum theory](https://arxiv.org/abs/2504.20839)
*D. -F. Qin*

Main category: cs.CL

TL;DR: 论文探讨了基于量子力学的语言建模，提出将量子力学引入符号-意义对以构建自然语言表示模型，并尝试用量子统计改进词嵌入技术。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用量子力学的数学框架解释和改进语言建模中的词嵌入技术，探索自然语言的量子特性及其物理性来源。

Method: 通过量子力学和量子统计理论，研究自然语言的数学表示、演化及统计特性，并构建实验代码验证可行性。

Result: 论文指出量子理论可用于自然语言建模，并讨论了其在生成模型和量子计算机中的潜在应用。

Conclusion: 量子力学为语言建模提供了新的理论视角，未来可能在生成模型和量子计算中有广泛应用。

Abstract: This paper examines language modeling based on the theory of quantum
mechanics. It focuses on the introduction of quantum mechanics into the
symbol-meaning pairs of language in order to build a representation model of
natural language. At the same time, it is realized that word embedding, which
is widely used as a basic technique for statistical language modeling, can be
explained and improved by the mathematical framework of quantum mechanics. On
this basis, this paper continues to try to use quantum statistics and other
related theories to study the mathematical representation, natural evolution
and statistical properties of natural language. It is also assumed that the
source of such quantum properties is the physicality of information. The
feasibility of using quantum theory to model natural language is pointed out
through the construction of a experimental code. The paper discusses, in terms
of applications, the possible help of the theory in constructing generative
models that are popular nowadays. A preliminary discussion of future
applications of the theory to quantum computers is also presented.

</details>


### [36] [JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry](https://arxiv.org/abs/2504.20849)
*Anum Afzal,Alexandre Mercier,Florian Matthes*

Main category: cs.CL

TL;DR: 论文研究了基于LLM的数据到文本方法，旨在生成高质量且多样化的营销文本，并提出了评估多样性的指标JaccDiv。


<details>
  <summary>Details</summary>
Motivation: 传统生成方法容易陷入重复模式，导致内容单调，无法满足在线平台的需求。

Method: 利用T5、GPT-3.5、GPT-4和LLaMa2等语言模型，结合微调、少样本和零样本方法生成多样化文本。

Result: 提出了JaccDiv指标评估文本多样性，方法适用于多个领域。

Conclusion: LLM方法能有效提升营销文本的多样性和质量，适用于广泛场景。

Abstract: Online platforms are increasingly interested in using Data-to-Text
technologies to generate content and help their users. Unfortunately,
traditional generative methods often fall into repetitive patterns, resulting
in monotonous galleries of texts after only a few iterations. In this paper, we
investigate LLM-based data-to-text approaches to automatically generate
marketing texts that are of sufficient quality and diverse enough for broad
adoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in
conjunction with fine-tuning, few-shot, and zero-shot approaches to set a
baseline for diverse marketing texts. We also introduce a metric JaccDiv to
evaluate the diversity of a set of texts. This research extends its relevance
beyond the music industry, proving beneficial in various fields where
repetitive automated content generation is prevalent.

</details>


### [37] [DYNAMAX: Dynamic computing for Transformers and Mamba based architectures](https://arxiv.org/abs/2504.20922)
*Miguel Nogales,Matteo Gambella,Manuel Roveri*

Main category: cs.CL

TL;DR: DYNAMAX框架首次将早期退出机制应用于Mamba架构，并展示了其作为高效分类器的潜力，在计算成本和性能之间取得了平衡。


<details>
  <summary>Details</summary>
Motivation: 探索早期退出机制在Mamba架构中的应用，以降低计算成本和延迟，同时保持预测准确性。

Method: 在Mamba架构中集成早期退出机制，并将其作为分类器用于Mamba和Transformer模型，通过实验验证其效果。

Result: Mamba作为早期退出分类器表现出高效性和适应性，在计算节省和性能平衡方面优于传统方法。

Conclusion: Mamba架构的动态处理能力为资源受限环境中的高效推理提供了新途径，具有变革潜力。

Abstract: Early exits (EEs) offer a promising approach to reducing computational costs
and latency by dynamically terminating inference once a satisfactory prediction
confidence on a data sample is achieved. Although many works integrate EEs into
encoder-only Transformers, their application to decoder-only architectures and,
more importantly, Mamba models, a novel family of state-space architectures in
the LLM realm, remains insufficiently explored. This work introduces DYNAMAX,
the first framework to exploit the unique properties of Mamba architectures for
early exit mechanisms. We not only integrate EEs into Mamba but also repurpose
Mamba as an efficient EE classifier for both Mamba-based and transformer-based
LLMs, showcasing its versatility. Our experiments employ the Mistral 7B
transformer compared to the Codestral 7B Mamba model, using data sets such as
TruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and
consistency. The results highlight the adaptability of Mamba as a powerful EE
classifier and its efficiency in balancing computational cost and performance
quality across NLP tasks. By leveraging Mamba's inherent design for dynamic
processing, we open pathways for scalable and efficient inference in embedded
applications and resource-constrained environments. This study underscores the
transformative potential of Mamba in redefining dynamic computing paradigms for
LLMs.

</details>


### [38] [Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models](https://arxiv.org/abs/2504.20946)
*Tyler McDonald,Ali Emami*

Main category: cs.CL

TL;DR: 论文提出了一种名为Trace-of-Thought Prompting的零样本提示工程方法，旨在通过开源模型（参数≤7B）提升算术推理能力，性能提升高达125%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在专业领域（如算术推理）的应用存在计算成本高和闭源模型适应性差的问题，开源模型能更高效地优化资源使用。

Method: 引入Trace-of-Thought Prompting方法，指导LLMs通过可观察的子问题解决过程增强算术推理能力。

Result: 在参数≤7B的开源模型与GPT-4结合使用时，性能提升高达125%，并提供了问题解决过程的新视角。

Conclusion: 开源模型在AI研究民主化和高质量计算语言学应用的可及性方面具有巨大潜力。

Abstract: As Large Language Models (LLMs) continue to be leveraged for daily tasks,
prompt engineering remains an active field of contribution within computational
linguistics, particularly in domains requiring specialized knowledge such as
arithmetic reasoning. While these LLMs are optimized for a variety of tasks,
their exhaustive employment may become computationally or financially
cumbersome for small teams. Additionally, complete reliance on proprietary,
closed-source models often limits customization and adaptability, posing
significant challenges in research and application scalability. Instead, by
leveraging open-source models at or below 7 billion parameters, we can optimize
our resource usage while still observing remarkable gains over standard
prompting approaches. To cultivate this notion, we introduce Trace-of-Thought
Prompting, a simple, zero-shot prompt engineering method that instructs LLMs to
create observable subproblems using critical problem-solving, specifically
designed to enhance arithmetic reasoning capabilities. When applied to
open-source models in tandem with GPT-4, we observe that Trace-of-Thought not
only allows novel insight into the problem-solving process but also introduces
performance gains as large as 125% on language models at or below 7 billion
parameters. This approach underscores the potential of open-source initiatives
in democratizing AI research and improving the accessibility of high-quality
computational linguistics applications.

</details>


### [39] [Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models](https://arxiv.org/abs/2504.20951)
*Maryna Vyshnyvetska*

Main category: cs.CL

TL;DR: 提出了一种名为“信息引力”的理论模型，用场论和时空几何的物理工具描述大型语言模型（LLM）的文本生成过程。


<details>
  <summary>Details</summary>
Motivation: 解释LLM行为中的现象，如幻觉、对查询表述的敏感性以及采样温度对输出多样性的影响。

Method: 将查询视为具有“信息质量”的对象，通过弯曲模型的语义空间形成引力势阱，吸引生成过程中的标记。

Result: 模型为LLM行为中的多种现象提供了机制性解释。

Conclusion: “信息引力”模型为理解LLM的文本生成过程提供了新的理论框架。

Abstract: We propose a theoretical model called "information gravity" to describe the
text generation process in large language models (LLMs). The model uses
physical apparatus from field theory and spacetime geometry to formalize the
interaction between user queries and the probability distribution of generated
tokens. A query is viewed as an object with "information mass" that curves the
semantic space of the model, creating gravitational potential wells that
"attract" tokens during generation. This model offers a mechanism to explain
several observed phenomena in LLM behavior, including hallucinations (emerging
from low-density semantic voids), sensitivity to query formulation (due to
semantic field curvature changes), and the influence of sampling temperature on
output diversity.

</details>


### [40] [OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification](https://arxiv.org/abs/2504.20964)
*Shangyu Li,Juyong Jiang,Tiancheng Zhao,Jiasi Shen*

Main category: cs.CL

TL;DR: OSVBench是一个新基准，用于评估大型语言模型（LLM）在操作系统内核验证任务中生成完整规范代码的能力。基于Hyperkernel，包含245个复杂任务，当前LLM表现有限。


<details>
  <summary>Details</summary>
Motivation: 为操作系统验证任务中的规范代码生成问题提供一个评估基准，填补现有研究的空白。

Method: 将规范生成问题转化为程序合成问题，限定语法和语义范围，要求LLM基于验证假设和高层功能描述生成完整规范。

Result: 对12个LLM的评估显示其在长上下文代码生成任务中表现有限，性能差异显著。

Conclusion: OSVBench揭示了当前LLM在操作系统验证任务中的局限性，为未来研究提供了工具和方向。

Abstract: We introduce OSVBench, a new benchmark for evaluating Large Language Models
(LLMs) in generating complete specification code pertaining to operating system
kernel verification tasks. The benchmark first defines the specification
generation problem into a program synthesis problem within a confined scope of
syntax and semantics by providing LLMs with the programming model. The LLMs are
required to understand the provided verification assumption and the potential
syntax and semantics space to search for, then generate the complete
specification for the potentially buggy operating system code implementation
under the guidance of the high-level functional description of the operating
system. This benchmark is built upon a real-world operating system kernel,
Hyperkernel, and consists of 245 complex specification generation tasks in
total, each is a long context task of about 20k-30k tokens. Our comprehensive
evaluation of 12 LLMs exhibits the limited performance of the current LLMs on
the specification generation tasks for operating system verification.
Significant disparities in their performance on the benchmark highlight
differences in their ability to handle long-context code generation tasks. The
evaluation toolkit and benchmark are available at
https://github.com/lishangyu-hkust/OSVBench.

</details>


### [41] [SetKE: Knowledge Editing for Knowledge Elements Overlap](https://arxiv.org/abs/2504.20972)
*Yifan Wei,Xiaoyan Yu,Ran Song,Hao Peng,Angsheng Li*

Main category: cs.CL

TL;DR: 论文提出了一种新的知识编辑方法SetKE，解决现有方法在处理知识元素重叠（KEO）时的性能下降问题，并引入数据集EditSet作为基准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在知识更新时面临传统方法（如微调和增量学习）的局限性，而现有知识编辑（KE）方法忽视了知识元素重叠（KEO）现象，导致编辑冲突和性能下降。

Method: 提出知识集编辑（KSE）框架和SetKE方法，通过同时编辑三元组集合来解决KEO问题。

Result: 实验表明，SetKE在主流LLMs上优于现有方法，特别是在KEO场景中。

Conclusion: SetKE为知识编辑提供了更高效的解决方案，EditSet数据集为未来研究提供了基准。

Abstract: Large Language Models (LLMs) excel in tasks such as retrieval and question
answering but require updates to incorporate new knowledge and reduce
inaccuracies and hallucinations. Traditional updating methods, like fine-tuning
and incremental learning, face challenges such as overfitting and high
computational costs. Knowledge Editing (KE) provides a promising alternative
but often overlooks the Knowledge Element Overlap (KEO) phenomenon, where
multiple triplets share common elements, leading to editing conflicts. We
identify the prevalence of KEO in existing KE datasets and show its significant
impact on current KE methods, causing performance degradation in handling such
triplets. To address this, we propose a new formulation, Knowledge Set Editing
(KSE), and introduce SetKE, a method that edits sets of triplets
simultaneously. Experimental results demonstrate that SetKE outperforms
existing methods in KEO scenarios on mainstream LLMs. Additionally, we
introduce EditSet, a dataset containing KEO triplets, providing a comprehensive
benchmark.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [42] [Can Geometry Save Central Views for Sports Field Registration?](https://arxiv.org/abs/2504.20052)
*Floriane Magera,Thomas Hoyoux,Martin Castin,Olivier Barnich,Anthony Cioppa,Marc Van Droogenbroeck*

Main category: cs.CV

TL;DR: 提出了一种利用圆形标记进行运动场注册的新方法，解决了现有方法在特写镜头中仅依赖线性标记的局限性。


<details>
  <summary>Details</summary>
Motivation: 运动场注册通常依赖稀疏且分布不均的线性标记，特写镜头中圆形标记的利用不足，导致注册困难。

Method: 通过从圆形对应关系中推导点和线，将圆形标记纳入线性方程组，用于运动场注册和图像标注。

Result: 实验表明，该方法能有效补充高性能检测器，在困难场景中实现运动场注册。

Conclusion: 该方法扩展了运动场注册的能力，特别是在特写镜头中利用圆形标记的场景。

Abstract: Single-frame sports field registration often serves as the foundation for
extracting 3D information from broadcast videos, enabling applications related
to sports analytics, refereeing, or fan engagement. As sports fields have
rigorous specifications in terms of shape and dimensions of their line, circle
and point components, sports field markings are commonly used as calibration
targets for this task. However, because of the sparse and uneven distribution
of field markings, close-up camera views around central areas of the field
often depict only line and circle markings. On these views, sports field
registration is challenging for the vast majority of existing methods, as they
focus on leveraging line field markings and their intersections. It is indeed a
challenge to include circle correspondences in a set of linear equations. In
this work, we propose a novel method to derive a set of points and lines from
circle correspondences, enabling the exploitation of circle correspondences for
both sports field registration and image annotation. In our experiments, we
illustrate the benefits of our bottom-up geometric method against
top-performing detectors and show that our method successfully complements
them, enabling sports field registration in difficult scenarios.

</details>


### [43] [Marmot: Multi-Agent Reasoning for Multi-Object Self-Correcting in Improving Image-Text Alignment](https://arxiv.org/abs/2504.20054)
*Jiayang Sun,Hongbo Wang,Jie Cao,Huaibo Huang,Ran He*

Main category: cs.CV

TL;DR: Marmot框架通过多智能体推理提升复杂多对象场景中的图像生成准确性，解决计数、属性和空间关系问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在多对象场景中难以准确处理计数、属性和空间关系，需要一种通用且高效的解决方案。

Method: 采用分治策略，将自校正任务分解为三个维度（计数、属性、空间关系），并构建多智能体编辑系统，结合决策-执行-验证机制和像素域拼接平滑器。

Result: 实验表明，Marmot显著提升了图像生成任务中对象计数、属性分配和空间关系的准确性。

Conclusion: Marmot框架通过多智能体推理和优化技术，有效解决了复杂多对象场景中的图像生成问题。

Abstract: While diffusion models excel at generating high-quality images, they often
struggle with accurate counting, attributes, and spatial relationships in
complex multi-object scenes. To address these challenges, we propose Marmot, a
novel and generalizable framework that employs Multi-Agent Reasoning for
Multi-Object Self-Correcting, enhancing image-text alignment and facilitating
more coherent multi-object image editing. Our framework adopts a
divide-and-conquer strategy that decomposes the self-correction task into three
critical dimensions (counting, attributes, and spatial relationships), and
further divided into object-level subtasks. We construct a multi-agent editing
system featuring a decision-execution-verification mechanism, effectively
mitigating inter-object interference and enhancing editing reliability. To
resolve the problem of subtask integration, we propose a Pixel-Domain Stitching
Smoother that employs mask-guided two-stage latent space optimization. This
innovation enables parallel processing of subtask results, thereby enhancing
runtime efficiency while eliminating multi-stage distortion accumulation.
Extensive experiments demonstrate that Marmot significantly improves accuracy
in object counting, attribute assignment, and spatial relationships for image
generation tasks.

</details>


### [44] [Edge-Based Learning for Improved Classification Under Adversarial Noise](https://arxiv.org/abs/2504.20077)
*Manish Kansana,Keyan Alexander Rahimi,Elias Hossain,Iman Dehzangi,Noorbakhsh Amiri Golilarz*

Main category: cs.CV

TL;DR: 研究探讨了对抗性噪声对图像分类的影响，发现基于边缘特征的训练能提升模型对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 对抗性噪声会误导深度学习模型，研究旨在探索通过特定图像特征（如边缘）提升模型鲁棒性的方法。

Method: 使用FGSM生成对抗噪声，分别在原始图像和边缘图像上训练模型，并测试其对抗噪声的鲁棒性。

Result: 边缘模型对对抗性攻击更具抵抗力，但原始数据重新训练后的准确率提升略高于边缘数据。

Conclusion: 基于边缘的学习可增强模型对抗性扰动的鲁棒性。

Abstract: Adversarial noise introduces small perturbations in images, misleading deep
learning models into misclassification and significantly impacting recognition
accuracy. In this study, we analyzed the effects of Fast Gradient Sign Method
(FGSM) adversarial noise on image classification and investigated whether
training on specific image features can improve robustness. We hypothesize that
while adversarial noise perturbs various regions of an image, edges may remain
relatively stable and provide essential structural information for
classification. To test this, we conducted a series of experiments using brain
tumor and COVID datasets. Initially, we trained the models on clean images and
then introduced subtle adversarial perturbations, which caused deep learning
models to significantly misclassify the images. Retraining on a combination of
clean and noisy images led to improved performance. To evaluate the robustness
of the edge features, we extracted edges from the original/clean images and
trained the models exclusively on edge-based representations. When noise was
introduced to the images, the edge-based models demonstrated greater resilience
to adversarial attacks compared to those trained on the original or clean
images. These results suggest that while adversarial noise is able to exploit
complex non-edge regions significantly more than edges, the improvement in the
accuracy after retraining is marginally more in the original data as compared
to the edges. Thus, leveraging edge-based learning can improve the resilience
of deep learning models against adversarial perturbations.

</details>


### [45] [VideoMultiAgents: A Multi-Agent Framework for Video Question Answering](https://arxiv.org/abs/2504.20091)
*Noriyuki Kugo,Xiang Li,Zixin Li,Ashish Gupta,Arpandeep Khatua,Nidhish Jain,Chaitanya Patel,Yuta Kyuragi,Masamoto Tanabiki,Kazuki Kozuka,Ehsan Adeli*

Main category: cs.CV

TL;DR: VideoMultiAgents框架通过多模态代理提升视频问答性能，结合问题引导的标题生成，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答方法依赖单一模型处理帧级标题，难以捕捉时空和交互上下文。

Method: 引入VideoMultiAgents框架，整合视觉、场景图和文本处理代理，辅以问题引导的标题生成。

Result: 在Intent-QA、EgoSchema和NExT-QA数据集上分别提升6.2%、3.4%和0.4%。

Conclusion: 多代理协同和问题引导标题生成显著提升视频问答性能。

Abstract: Video Question Answering (VQA) inherently relies on multimodal reasoning,
integrating visual, temporal, and linguistic cues to achieve a deeper
understanding of video content. However, many existing methods rely on feeding
frame-level captions into a single model, making it difficult to adequately
capture temporal and interactive contexts. To address this limitation, we
introduce VideoMultiAgents, a framework that integrates specialized agents for
vision, scene graph analysis, and text processing. It enhances video
understanding leveraging complementary multimodal reasoning from independently
operating agents. Our approach is also supplemented with a question-guided
caption generation, which produces captions that highlight objects, actions,
and temporal transitions directly relevant to a given query, thus improving the
answer accuracy. Experimental results demonstrate that our method achieves
state-of-the-art performance on Intent-QA (79.0%, +6.2% over previous SOTA),
EgoSchema subset (75.4%, +3.4%), and NExT-QA (79.6%, +0.4%).

</details>


### [46] [Long-Distance Field Demonstration of Imaging-Free Drone Identification in Intracity Environments](https://arxiv.org/abs/2504.20097)
*Junran Guo,Tonglin Mu,Keyuan Li,Jianing Li,Ziyang Luo,Ye Chen,Xiaodong Fan,Jinquan Huang,Minjie Liu,Jinbei Zhang,Ruoyang Qi,Naiting Gu,Shihai Sun*

Main category: cs.CV

TL;DR: 论文提出了一种结合残差神经网络（ResNet）与D²SP²-LiDAR的新方法，显著提升了小型目标（如无人机）的远距离检测能力，检测范围扩展至5公里，并实现了高精度的姿态和类型识别。


<details>
  <summary>Details</summary>
Motivation: 传统基于成像的方法在远距离检测小型目标时存在分辨率、功耗和成本限制，而现有的D²SP²-LiDAR技术检测范围有限，因此需要一种更高效的方法。

Method: 通过将ResNet与D²SP²-LiDAR结合，并优化观测模型，实现了远距离高精度检测。

Result: 实验表明，该方法在5公里范围内实现了94.93%的姿态识别准确率和97.99%的类型分类准确率，优于传统成像方法。

Conclusion: 该方法展示了成像无关技术在远距离小型目标检测中的潜力，适用于实际场景。

Abstract: Detecting small objects, such as drones, over long distances presents a
significant challenge with broad implications for security, surveillance,
environmental monitoring, and autonomous systems. Traditional imaging-based
methods rely on high-resolution image acquisition, but are often constrained by
range, power consumption, and cost. In contrast, data-driven
single-photon-single-pixel light detection and ranging
(\text{D\textsuperscript{2}SP\textsuperscript{2}-LiDAR}) provides an
imaging-free alternative, directly enabling target identification while
reducing system complexity and cost. However, its detection range has been
limited to a few hundred meters. Here, we introduce a novel integration of
residual neural networks (ResNet) with
\text{D\textsuperscript{2}SP\textsuperscript{2}-LiDAR}, incorporating a refined
observation model to extend the detection range to 5~\si{\kilo\meter} in an
intracity environment while enabling high-accuracy identification of drone
poses and types. Experimental results demonstrate that our approach not only
outperforms conventional imaging-based recognition systems, but also achieves
94.93\% pose identification accuracy and 97.99\% type classification accuracy,
even under weak signal conditions with long distances and low signal-to-noise
ratios (SNRs). These findings highlight the potential of imaging-free methods
for robust long-range detection of small targets in real-world scenarios.

</details>


### [47] [An on-production high-resolution longitudinal neonatal fingerprint database in Brazil](https://arxiv.org/abs/2504.20104)
*Luiz F. P. Southier,Marcelo Filipak,Luiz A. Zanlorensi,Ildefonso Wasilevski,Fabio Favarim,Jefferson T. Oliva,Marcelo Teixeira,Dalcimar Casanova*

Main category: cs.CV

TL;DR: 研究旨在开发新生儿指纹的高质量生物识别数据库，以支持机器学习模型训练，解决新生儿指纹因生长变化导致的识别难题。


<details>
  <summary>Details</summary>
Motivation: 新生儿期对生存至关重要，但现有生物识别系统因生理变化（如指纹生长）难以准确识别，缺乏长期数据支持。

Method: 设计并开发多阶段采集的新生儿指纹数据库，用于训练和评估深度学习模型，预测指纹特征变化。

Result: 预期该数据库将支持开发比传统缩放方法更准确的深度学习模型，提高新生儿指纹识别可靠性。

Conclusion: 研究为针对新生儿独特生长轨迹的生物识别系统奠定了基础，有望提升识别准确性。

Abstract: The neonatal period is critical for survival, requiring accurate and early
identification to enable timely interventions such as vaccinations, HIV
treatment, and nutrition programs. Biometric solutions offer potential for
child protection by helping to prevent baby swaps, locate missing children, and
support national identity systems. However, developing effective biometric
identification systems for newborns remains a major challenge due to the
physiological variability caused by finger growth, weight changes, and skin
texture alterations during early development. Current literature has attempted
to address these issues by applying scaling factors to emulate growth-induced
distortions in minutiae maps, but such approaches fail to capture the complex
and non-linear growth patterns of infants. A key barrier to progress in this
domain is the lack of comprehensive, longitudinal biometric datasets capturing
the evolution of neonatal fingerprints over time. This study addresses this gap
by focusing on designing and developing a high-quality biometric database of
neonatal fingerprints, acquired at multiple early life stages. The dataset is
intended to support the training and evaluation of machine learning models
aimed at emulating the effects of growth on biometric features. We hypothesize
that such a dataset will enable the development of more robust and accurate
Deep Learning-based models, capable of predicting changes in the minutiae map
with higher fidelity than conventional scaling-based methods. Ultimately, this
effort lays the groundwork for more reliable biometric identification systems
tailored to the unique developmental trajectory of newborns.

</details>


### [48] [Forging and Removing Latent-Noise Diffusion Watermarks Using a Single Image](https://arxiv.org/abs/2504.20111)
*Anubhav Jain,Yuya Kobayashi,Naoki Murata,Yuhta Takida,Takashi Shibuya,Yuki Mitsufuji,Niv Cohen,Nasir Memon,Julian Togelius*

Main category: cs.CV

TL;DR: 提出一种针对扩散模型水印的黑盒对抗攻击方法，仅需单一样本即可伪造或移除水印，揭示现有水印方案的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有水印技术依赖初始噪声嵌入密钥，被认为难以移除或伪造，但缺乏对黑盒攻击的鲁棒性研究。

Method: 基于图像与初始噪声的多对一映射关系，通过扰动图像进入或退出水印区域，实现伪造或移除水印。

Result: 在多种水印方案（Tree-Ring、RingID等）和扩散模型（SDv1.4、SDv2.0）上验证了攻击的有效性。

Conclusion: 现有水印方法存在漏洞，需进一步研究改进其安全性。

Abstract: Watermarking techniques are vital for protecting intellectual property and
preventing fraudulent use of media. Most previous watermarking schemes designed
for diffusion models embed a secret key in the initial noise. The resulting
pattern is often considered hard to remove and forge into unrelated images. In
this paper, we propose a black-box adversarial attack without presuming access
to the diffusion model weights. Our attack uses only a single watermarked
example and is based on a simple observation: there is a many-to-one mapping
between images and initial noises. There are regions in the clean image latent
space pertaining to each watermark that get mapped to the same initial noise
when inverted. Based on this intuition, we propose an adversarial attack to
forge the watermark by introducing perturbations to the images such that we can
enter the region of watermarked images. We show that we can also apply a
similar approach for watermark removal by learning perturbations to exit this
region. We report results on multiple watermarking schemes (Tree-Ring, RingID,
WIND, and Gaussian Shading) across two diffusion models (SDv1.4 and SDv2.0).
Our results demonstrate the effectiveness of the attack and expose
vulnerabilities in the watermarking methods, motivating future research on
improving them.

</details>


### [49] [A Transformer-based Multimodal Fusion Model for Efficient Crowd Counting Using Visual and Wireless Signals](https://arxiv.org/abs/2504.20178)
*Zhe Cui,Yuli Li,Le-Nam Tran*

Main category: cs.CV

TL;DR: TransFusion是一种基于多模态融合的人群计数模型，结合了CSI和图像数据，利用Transformer和CNN的优势，显著提升了计数精度。


<details>
  <summary>Details</summary>
Motivation: 现有单模态人群计数模型存在信息丢失和性能不足的问题，需要更全面的数据融合方法。

Method: 通过Transformer网络融合CSI和图像数据，并引入CNN补充局部细节特征。

Result: 实验表明TransFusion在计数精度和效率上均表现优异。

Conclusion: TransFusion通过多模态融合和混合网络架构，显著提升了人群计数的准确性和鲁棒性。

Abstract: Current crowd-counting models often rely on single-modal inputs, such as
visual images or wireless signal data, which can result in significant
information loss and suboptimal recognition performance. To address these
shortcomings, we propose TransFusion, a novel multimodal fusion-based
crowd-counting model that integrates Channel State Information (CSI) with image
data. By leveraging the powerful capabilities of Transformer networks,
TransFusion effectively combines these two distinct data modalities, enabling
the capture of comprehensive global contextual information that is critical for
accurate crowd estimation. However, while transformers are well capable of
capturing global features, they potentially fail to identify finer-grained,
local details essential for precise crowd counting. To mitigate this, we
incorporate Convolutional Neural Networks (CNNs) into the model architecture,
enhancing its ability to extract detailed local features that complement the
global context provided by the Transformer. Extensive experimental evaluations
demonstrate that TransFusion achieves high accuracy with minimal counting
errors while maintaining superior efficiency.

</details>


### [50] [Integration Flow Models](https://arxiv.org/abs/2504.20179)
*Jingjing Wang,Dan Zhang,Joshua Luo,Yin Yang,Feng Luo*

Main category: cs.CV

TL;DR: 提出Integration Flow，直接学习ODE轨迹路径的积分，避免数值求解ODE的误差，提升生成质量和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统ODE生成模型因数值求解误差或训练不稳定问题，限制了样本质量。

Method: 直接学习ODE轨迹的积分，引入目标状态作为锚点指导反向动态。

Result: 在CIFAR10和ImageNet上，Integration Flow显著提升现有ODE模型的性能，如扩散模型和Rectified Flows。

Conclusion: Integration Flow通过统一结构和理论证明，显著提升了ODE生成模型的稳定性和准确性。

Abstract: Ordinary differential equation (ODE) based generative models have emerged as
a powerful approach for producing high-quality samples in many applications.
However, the ODE-based methods either suffer the discretization error of
numerical solvers of ODE, which restricts the quality of samples when only a
few NFEs are used, or struggle with training instability. In this paper, we
proposed Integration Flow, which directly learns the integral of ODE-based
trajectory paths without solving the ODE functions. Moreover, Integration Flow
explicitly incorporates the target state $\mathbf{x}_0$ as the anchor state in
guiding the reverse-time dynamics. We have theoretically proven this can
contribute to both stability and accuracy. To the best of our knowledge,
Integration Flow is the first model with a unified structure to estimate
ODE-based generative models and the first to show the exact straightness of
1-Rectified Flow without reflow. Through theoretical analysis and empirical
evaluations, we show that Integration Flows achieve improved performance when
it is applied to existing ODE-based models, such as diffusion models, Rectified
Flows, and PFGM++. Specifically, Integration Flow achieves one-step generation
on CIFAR10 with FIDs of 2.86 for the Variance Exploding (VE) diffusion model,
3.36 for rectified flow without reflow, and 2.91 for PFGM++; and on ImageNet
with FIDs of 4.09 for VE diffusion model, 4.35 for rectified flow without
reflow and 4.15 for PFGM++.

</details>


### [51] [Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains](https://arxiv.org/abs/2504.20199)
*Juntian Zhang,Chuanqi cheng,Yuhan Liu,Wei Liu,Jian Luan,Rui Yan*

Main category: cs.CV

TL;DR: 论文提出Focus-Centric Visual Chain范式，提升视觉语言模型在多图像任务中的性能，并通过Focus-Centric Data Synthesis方法构建VISC-150K数据集，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多图像输入复杂，现有视觉语言模型性能下降，需提升模型在多图像任务中的感知、理解和推理能力。

Method: 提出Focus-Centric Visual Chain范式，采用Focus-Centric Data Synthesis方法合成高质量数据，构建VISC-150K数据集。

Result: 在七个多图像基准测试中，平均性能提升3.16%和2.24%，且不影响通用视觉语言能力。

Conclusion: 研究为处理复杂视觉场景的视觉语言系统提供了重要进展。

Abstract: Vision-language models (VLMs) achieve remarkable success in single-image
tasks. However, real-world scenarios often involve intricate multi-image
inputs, leading to a notable performance decline as models struggle to
disentangle critical information scattered across complex visual features. In
this work, we propose Focus-Centric Visual Chain, a novel paradigm that
enhances VLMs'perception, comprehension, and reasoning abilities in multi-image
scenarios. To facilitate this paradigm, we propose Focus-Centric Data
Synthesis, a scalable bottom-up approach for synthesizing high-quality data
with elaborate reasoning paths. Through this approach, We construct VISC-150K,
a large-scale dataset with reasoning data in the form of Focus-Centric Visual
Chain, specifically designed for multi-image tasks. Experimental results on
seven multi-image benchmarks demonstrate that our method achieves average
performance gains of 3.16% and 2.24% across two distinct model architectures,
without compromising the general vision-language capabilities. our study
represents a significant step toward more robust and capable vision-language
systems that can handle complex visual scenarios.

</details>


### [52] [Remote Sensing Imagery for Flood Detection: Exploration of Augmentation Strategies](https://arxiv.org/abs/2504.20203)
*Vladyslav Polushko,Damjan Hatic,Ronald Rösch,Thomas März,Markus Rauhut,Andreas Weinmann*

Main category: cs.CV

TL;DR: 论文探讨了利用不同数据增强策略优化深度学习网络在RGB图像中检测河流洪水的效果。


<details>
  <summary>Details</summary>
Motivation: 洪水是全球性问题，快速有效响应需要准确及时的受灾区域信息。遥感图像结合特定检测方法可提升洪水检测精度。

Method: 使用BlessemFlood21数据集，测试从基础到复杂（如光学畸变）的数据增强策略，优化深度学习分割网络的训练。

Result: 通过识别有效的数据增强策略，改进了洪水检测的深度学习模型训练过程。

Conclusion: 数据增强策略对提升洪水检测精度具有重要意义，为未来研究提供了优化方向。

Abstract: Floods cause serious problems around the world. Responding quickly and
effectively requires accurate and timely information about the affected areas.
The effective use of Remote Sensing images for accurate flood detection
requires specific detection methods. Typically, Deep Neural Networks are
employed, which are trained on specific datasets. For the purpose of river
flood detection in RGB imagery, we use the BlessemFlood21 dataset. We here
explore the use of different augmentation strategies, ranging from basic
approaches to more complex techniques, including optical distortion. By
identifying effective strategies, we aim to refine the training process of
state-of-the-art Deep Learning segmentation networks.

</details>


### [53] [FreBIS: Frequency-Based Stratification for Neural Implicit Surface Representations](https://arxiv.org/abs/2504.20222)
*Naoko Sawada,Pedro Miraldo,Suhas Lohit,Tim K. Marks,Moitreya Chatterjee*

Main category: cs.CV

TL;DR: 论文提出了一种名为FreBIS的新型神经隐式表面表示方法，通过频率分层编码器解决复杂场景中表面信息捕捉的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统神经隐式表面表示方法使用单一编码器捕捉场景中所有频率的表面信息，难以处理复杂多变的表面。

Method: FreBIS将场景按表面频率分层，每层由专用编码器处理，并通过冗余感知权重模块促进编码器间的互补性。

Result: 在BlendedMVS数据集上的实验表明，FreBIS显著提升了3D表面重建质量和渲染保真度。

Conclusion: FreBIS通过频率分层和互补编码器设计，有效提升了复杂场景的表面重建效果。

Abstract: Neural implicit surface representation techniques are in high demand for
advancing technologies in augmented reality/virtual reality, digital twins,
autonomous navigation, and many other fields. With their ability to model
object surfaces in a scene as a continuous function, such techniques have made
remarkable strides recently, especially over classical 3D surface
reconstruction methods, such as those that use voxels or point clouds. However,
these methods struggle with scenes that have varied and complex surfaces
principally because they model any given scene with a single encoder network
that is tasked to capture all of low through high-surface frequency information
in the scene simultaneously. In this work, we propose a novel, neural implicit
surface representation approach called FreBIS to overcome this challenge.
FreBIS works by stratifying the scene based on the frequency of surfaces into
multiple frequency levels, with each level (or a group of levels) encoded by a
dedicated encoder. Moreover, FreBIS encourages these encoders to capture
complementary information by promoting mutual dissimilarity of the encoded
features via a novel, redundancy-aware weighting module. Empirical evaluations
on the challenging BlendedMVS dataset indicate that replacing the standard
encoder in an off-the-shelf neural surface reconstruction method with our
frequency-stratified encoders yields significant improvements. These
enhancements are evident both in the quality of the reconstructed 3D surfaces
and in the fidelity of their renderings from any viewpoint.

</details>


### [54] [Improving trajectory continuity in drone-based crowd monitoring using a set of minimal-cost techniques and deep discriminative correlation filters](https://arxiv.org/abs/2504.20234)
*Bartosz Ptak,Marek Kraft*

Main category: cs.CV

TL;DR: 提出了一种基于点距离度量的无人机人群监测在线跟踪算法，显著提升了轨迹连续性和计数可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统检测-分配跟踪方法存在误检、漏检和身份切换问题，导致计数精度下降和分析困难。

Method: 改进SORT框架，引入点距离度量，结合相机运动补偿、高度感知分配和分类轨迹验证，并集成DDCF提升计算效率。

Result: 在DroneCrowd和UP-COUNT-TRACK数据集上，计数误差分别降至23%和15%，身份切换显著减少。

Conclusion: 该方法在跟踪精度和计数可靠性上优于基线在线跟踪器和离线优化方法。

Abstract: Drone-based crowd monitoring is the key technology for applications in
surveillance, public safety, and event management. However, maintaining
tracking continuity and consistency remains a significant challenge.
Traditional detection-assignment tracking methods struggle with false
positives, false negatives, and frequent identity switches, leading to degraded
counting accuracy and making in-depth analysis impossible. This paper
introduces a point-oriented online tracking algorithm that improves trajectory
continuity and counting reliability in drone-based crowd monitoring. Our method
builds on the Simple Online and Real-time Tracking (SORT) framework, replacing
the original bounding-box assignment with a point-distance metric. The
algorithm is enhanced with three cost-effective techniques: camera motion
compensation, altitude-aware assignment, and classification-based trajectory
validation. Further, Deep Discriminative Correlation Filters (DDCF) that re-use
spatial feature maps from localisation algorithms for increased computational
efficiency through neural network resource sharing are integrated to refine
object tracking by reducing noise and handling missed detections. The proposed
method is evaluated on the DroneCrowd and newly shared UP-COUNT-TRACK datasets,
demonstrating substantial improvements in tracking metrics, reducing counting
errors to 23% and 15%, respectively. The results also indicate a significant
reduction of identity switches while maintaining high tracking accuracy,
outperforming baseline online trackers and even an offline greedy optimisation
method.

</details>


### [55] [Physics-Informed Diffusion Models for SAR Ship Wake Generation from Text Prompts](https://arxiv.org/abs/2504.20241)
*Kamirul Kamirul,Odysseas Pappas,Alin Achim*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型的方法，用于高效生成合成孔径雷达（SAR）图像中的船舶尾迹，以解决标注数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 由于标注数据稀缺，传统物理模拟方法效率低且限制了端到端学习，因此需要一种更高效的方法。

Method: 使用扩散模型，通过物理模拟器生成的数据进行训练，将模拟器生成的图像与参数文本提示配对。

Result: 模型能够生成逼真的开尔文尾迹模式，且推理速度显著快于物理模拟器。

Conclusion: 扩散模型在快速可控的尾迹图像生成方面具有潜力，为海事SAR分析的端到端下游任务提供了新可能性。

Abstract: Detecting ship presence via wake signatures in SAR imagery is attracting
considerable research interest, but limited annotated data availability poses
significant challenges for supervised learning. Physics-based simulations are
commonly used to address this data scarcity, although they are slow and
constrain end-to-end learning. In this work, we explore a new direction for
more efficient and end-to-end SAR ship wake simulation using a diffusion model
trained on data generated by a physics-based simulator. The training dataset is
built by pairing images produced by the simulator with text prompts derived
from simulation parameters. Experimental result show that the model generates
realistic Kelvin wake patterns and achieves significantly faster inference than
the physics-based simulator. These results highlight the potential of diffusion
models for fast and controllable wake image generation, opening new
possibilities for end-to-end downstream tasks in maritime SAR analysis.

</details>


### [56] [Image Interpolation with Score-based Riemannian Metrics of Diffusion Models](https://arxiv.org/abs/2504.20288)
*Shinnosuke Saito,Takashi Matsubara*

Main category: cs.CV

TL;DR: 论文提出了一种新框架，将预训练扩散模型的数据空间视为黎曼流形，利用评分函数导出度量，提升了图像插值的真实性和提示忠实度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在内容生成方面表现优异，但缺乏利用数据流形的实用方法，而其他深度生成模型通常具备潜在空间。

Method: 将预训练扩散模型的数据空间建模为黎曼流形，通过评分函数定义度量。

Result: 在MNIST和Stable Diffusion上的实验表明，该方法生成的图像插值更真实、噪声更少且更忠实于提示。

Conclusion: 该几何感知方法在内容生成和编辑方面具有潜力。

Abstract: Diffusion models excel in content generation by implicitly learning the data
manifold, yet they lack a practical method to leverage this manifold - unlike
other deep generative models equipped with latent spaces. This paper introduces
a novel framework that treats the data space of pre-trained diffusion models as
a Riemannian manifold, with a metric derived from the score function.
Experiments with MNIST and Stable Diffusion show that this geometry-aware
approach yields image interpolations that are more realistic, less noisy, and
more faithful to prompts than existing methods, demonstrating its potential for
improved content generation and editing.

</details>


### [57] [DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral Remote Sensing Imagery of the Andes](https://arxiv.org/abs/2504.20303)
*Junlin Guo,James R. Zimmer-Dauphinee,Jordan M. Nieusma,Siqi Lu,Quan Liu,Ruining Deng,Can Cui,Jialin Yue,Yizhe Lin,Tianyuan Yao,Juming Xiong,Junchao Zhu,Chongyu Qu,Yuechen Yang,Mitchell Wilkes,Xiao Wang,Parker VanValkenburgh,Steven A. Wernke,Yuankai Huo*

Main category: cs.CV

TL;DR: DeepAndes是一个基于Transformer的视觉基础模型，专为安第斯考古学设计，通过自监督学习优化多光谱卫星图像分析。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在标注细粒度考古特征时面临挑战，且现有模型多针对RGB图像而非多光谱数据。

Method: 采用定制化的DINOv2自监督学习算法，训练于300万张多光谱卫星图像。

Result: 在少样本学习场景下，DeepAndes在分类、检索和分割任务中表现优异。

Conclusion: 大规模自监督预训练在考古遥感中效果显著，DeepAndes为安第斯地区提供了首个专用基础模型。

Abstract: By mapping sites at large scales using remotely sensed data, archaeologists
can generate unique insights into long-term demographic trends, inter-regional
social networks, and past adaptations to climate change. Remote sensing surveys
complement field-based approaches, and their reach can be especially great when
combined with deep learning and computer vision techniques. However,
conventional supervised deep learning methods face challenges in annotating
fine-grained archaeological features at scale. While recent vision foundation
models have shown remarkable success in learning large-scale remote sensing
data with minimal annotations, most off-the-shelf solutions are designed for
RGB images rather than multi-spectral satellite imagery, such as the 8-band
data used in our study. In this paper, we introduce DeepAndes, a
transformer-based vision foundation model trained on three million
multi-spectral satellite images, specifically tailored for Andean archaeology.
DeepAndes incorporates a customized DINOv2 self-supervised learning algorithm
optimized for 8-band multi-spectral imagery, marking the first foundation model
designed explicitly for the Andes region. We evaluate its image understanding
performance through imbalanced image classification, image instance retrieval,
and pixel-level semantic segmentation tasks. Our experiments show that
DeepAndes achieves superior F1 scores, mean average precision, and Dice scores
in few-shot learning scenarios, significantly outperforming models trained from
scratch or pre-trained on smaller datasets. This underscores the effectiveness
of large-scale self-supervised pre-training in archaeological remote sensing.
Codes will be available on https://github.com/geopacha/DeepAndes.

</details>


### [58] [Dynamic Contextual Attention Network: Transforming Spatial Representations into Adaptive Insights for Endoscopic Polyp Diagnosis](https://arxiv.org/abs/2504.20306)
*Teja Krishna Cherukuri,Nagur Shareef Shaik,Sribhuvan Reddy Yellu,Jun-Won Chung,Dong Hye Ye*

Main category: cs.CV

TL;DR: 提出动态上下文注意力网络（DCAN），通过注意力机制提升结肠息肉检测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统内窥镜成像在息肉定位和上下文感知方面存在不足，影响诊断的可解释性。

Method: DCAN将空间表征转化为自适应上下文信息，无需显式定位模块即可增强关键区域注意力。

Result: DCAN提高了分类过程的决策可解释性和整体诊断性能。

Conclusion: DCAN有望提升结肠癌检测的可靠性，改善患者预后。

Abstract: Colorectal polyps are key indicators for early detection of colorectal
cancer. However, traditional endoscopic imaging often struggles with accurate
polyp localization and lacks comprehensive contextual awareness, which can
limit the explainability of diagnoses. To address these issues, we propose the
Dynamic Contextual Attention Network (DCAN). This novel approach transforms
spatial representations into adaptive contextual insights, using an attention
mechanism that enhances focus on critical polyp regions without explicit
localization modules. By integrating contextual awareness into the
classification process, DCAN improves decision interpretability and overall
diagnostic performance. This advancement in imaging could lead to more reliable
colorectal cancer detection, enabling better patient outcomes.

</details>


### [59] [Fine Grain Classification: Connecting Meta using Cross-Contrastive pre-training](https://arxiv.org/abs/2504.20322)
*Sumit Mamtani,Yash Thesia*

Main category: cs.CV

TL;DR: 论文提出了一种利用元信息辅助细粒度视觉分类的统一框架，通过跨对比预训练联合学习视觉和元信息，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 细粒度视觉分类仅依赖外观信息难以准确区分子类别，因此需要引入元信息辅助识别。

Method: 采用三个编码器（图像、文本和元信息）进行跨对比预训练，对齐嵌入表示，随后微调图像和元信息编码器进行分类任务。

Result: 在NABirds数据集上，框架利用元信息将性能提升7.83%，准确率达到84.44%，优于现有方法。

Conclusion: 元信息的引入显著提升了细粒度视觉分类性能，证明了框架的有效性。

Abstract: Fine-grained visual classification aims to recognize objects belonging to
multiple subordinate categories within a super-category. However, this remains
a challenging problem, as appearance information alone is often insufficient to
accurately differentiate between fine-grained visual categories. To address
this, we propose a novel and unified framework that leverages meta-information
to assist fine-grained identification. We tackle the joint learning of visual
and meta-information through cross-contrastive pre-training. In the first
stage, we employ three encoders for images, text, and meta-information,
aligning their projected embeddings to achieve better representations. We then
fine-tune the image and meta-information encoders for the classification task.
Experiments on the NABirds dataset demonstrate that our framework effectively
utilizes meta-information to enhance fine-grained recognition performance. With
the addition of meta-information, our framework surpasses the current baseline
on NABirds by 7.83%. Furthermore, it achieves an accuracy of 84.44% on the
NABirds dataset, outperforming many existing state-of-the-art approaches that
utilize meta-information.

</details>


### [60] [MicarVLMoE: A Modern Gated Cross-Aligned Vision-Language Mixture of Experts Model for Medical Image Captioning and Report Generation](https://arxiv.org/abs/2504.20343)
*Amaan Izhar,Nurul Japar,Norisma Idris,Ting Dang*

Main category: cs.CV

TL;DR: MicarVLMoE模型通过多尺度视觉编码器和专家混合解码器，解决了医学图像报告中细粒度特征提取和多模态对齐问题，并在多种影像类型上取得先进结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在细粒度特征提取、多模态对齐和跨影像类型泛化方面表现不足，主要局限于胸部X光片。

Method: 提出MicarVLMoE模型，包括多尺度视觉编码器（MSVE）、多头双分支潜在注意力模块（MDLA）和调制专家混合解码器（MoE）。

Result: 在COVCTR、MMR、PGROSS和ROCO数据集上取得先进结果，提升了临床准确性、跨模态对齐和模型可解释性。

Conclusion: MicarVLMoE模型有效解决了医学图像报告中的关键挑战，并在多种影像类型上表现出色。

Abstract: Medical image reporting (MIR) aims to generate structured clinical
descriptions from radiological images. Existing methods struggle with
fine-grained feature extraction, multimodal alignment, and generalization
across diverse imaging types, often relying on vanilla transformers and
focusing primarily on chest X-rays. We propose MicarVLMoE, a vision-language
mixture-of-experts model with gated cross-aligned fusion, designed to address
these limitations. Our architecture includes: (i) a multiscale vision encoder
(MSVE) for capturing anatomical details at varying resolutions, (ii) a
multihead dual-branch latent attention (MDLA) module for vision-language
alignment through latent bottleneck representations, and (iii) a modulated
mixture-of-experts (MoE) decoder for adaptive expert specialization. We extend
MIR to CT scans, retinal imaging, MRI scans, and gross pathology images,
reporting state-of-the-art results on COVCTR, MMR, PGROSS, and ROCO datasets.
Extensive experiments and ablations confirm improved clinical accuracy,
cross-modal alignment, and model interpretability. Code is available at
https://github.com/AI-14/micar-vl-moe.

</details>


### [61] [TTTFusion: A Test-Time Training-Based Strategy for Multimodal Medical Image Fusion in Surgical Robots](https://arxiv.org/abs/2504.20362)
*Qinhua Xie,Hao Tang*

Main category: cs.CV

TL;DR: TTTFusion是一种基于测试时训练（TTT）的图像融合策略，通过动态调整模型参数提升多模态医学图像的融合质量。


<details>
  <summary>Details</summary>
Motivation: 提升手术机器人处理多模态医学图像的能力，解决传统方法在实时性、细粒度特征提取和边缘保留方面的不足。

Method: 采用测试时训练（TTT）策略，在推理阶段动态调整模型参数，优化输入图像数据的融合效果。

Result: 实验表明，TTTFusion在多模态图像融合质量上显著优于传统方法，尤其在细粒度特征提取和边缘保留方面表现突出。

Conclusion: TTTFusion不仅提高了图像融合精度，还为手术机器人实时图像处理提供了创新技术方案。

Abstract: With the increasing use of surgical robots in clinical practice, enhancing
their ability to process multimodal medical images has become a key research
challenge. Although traditional medical image fusion methods have made progress
in improving fusion accuracy, they still face significant challenges in
real-time performance, fine-grained feature extraction, and edge
preservation.In this paper, we introduce TTTFusion, a Test-Time Training
(TTT)-based image fusion strategy that dynamically adjusts model parameters
during inference to efficiently fuse multimodal medical images. By adapting the
model during the test phase, our method optimizes the parameters based on the
input image data, leading to improved accuracy and better detail preservation
in the fusion results.Experimental results demonstrate that TTTFusion
significantly enhances the fusion quality of multimodal images compared to
traditional fusion methods, particularly in fine-grained feature extraction and
edge preservation. This approach not only improves image fusion accuracy but
also offers a novel technical solution for real-time image processing in
surgical robots.

</details>


### [62] [Inception: Jailbreak the Memory Mechanism of Text-to-Image Generation Systems](https://arxiv.org/abs/2504.20376)
*Shiqian Zhao,Jiayang Liu,Yiming Li,Runyi Hu,Xiaojun Jia,Wenshu Fan,Xinfeng Li,Jie Zhang,Wei Dong,Tianwei Zhang,Luu Anh Tuan*

Main category: cs.CV

TL;DR: 论文揭示了文本到图像生成系统中的记忆机制加剧了越狱攻击的风险，并提出了一种名为Inception的多轮攻击方法，通过分块和递归策略实现高成功率。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成系统（如DALL·E 3）的记忆机制虽实用，但安全性分析不足，存在越狱攻击风险。

Method: 提出Inception攻击方法，分块输入恶意提示并利用递归策略，确保生成的图像语义一致且攻击成功率高。

Result: 实验表明，Inception在攻击成功率上比现有方法高出14%。

Conclusion: 记忆机制的安全漏洞需引起重视，Inception为多轮攻击提供了新思路。

Abstract: Currently, the memory mechanism has been widely and successfully exploited in
online text-to-image (T2I) generation systems ($e.g.$, DALL$\cdot$E 3) for
alleviating the growing tokenization burden and capturing key information in
multi-turn interactions. Despite its practicality, its security analyses have
fallen far behind. In this paper, we reveal that this mechanism exacerbates the
risk of jailbreak attacks. Different from previous attacks that fuse the unsafe
target prompt into one ultimate adversarial prompt, which can be easily
detected or may generate non-unsafe images due to under- or over-optimization,
we propose Inception, the first multi-turn jailbreak attack against the memory
mechanism in real-world text-to-image generation systems. Inception embeds the
malice at the inception of the chat session turn by turn, leveraging the
mechanism that T2I generation systems retrieve key information in their memory.
Specifically, Inception mainly consists of two modules. It first segments the
unsafe prompt into chunks, which are subsequently fed to the system in multiple
turns, serving as pseudo-gradients for directive optimization. Specifically, we
develop a series of segmentation policies that ensure the images generated are
semantically consistent with the target prompt. Secondly, after segmentation,
to overcome the challenge of the inseparability of minimum unsafe words, we
propose recursion, a strategy that makes minimum unsafe words subdivisible.
Collectively, segmentation and recursion ensure that all the request prompts
are benign but can lead to malicious outcomes. We conduct experiments on the
real-world text-to-image generation system ($i.e.$, DALL$\cdot$E 3) to validate
the effectiveness of Inception. The results indicate that Inception surpasses
the state-of-the-art by a 14\% margin in attack success rate.

</details>


### [63] [Sparse2DGS: Geometry-Prioritized Gaussian Splatting for Surface Reconstruction from Sparse Views](https://arxiv.org/abs/2504.20378)
*Jiang Wu,Rui Li,Yu Zhu,Rong Guo,Jinqiu Sun,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于稀疏输入视图的高斯泼溅表面重建方法Sparse2DGS，通过结合几何优先增强方案，解决了稀疏视图几何优化的不适定问题，显著优于现有方法且速度更快。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖密集视图或稀疏的Structure-from-Motion点初始化，而基于学习的多视图立体（MVS）虽提供密集3D点，但直接结合高斯泼溅会导致次优结果。

Method: 提出Sparse2DGS，一种MVS初始化的高斯泼溅管道，结合几何优先增强方案，实现直接且稳健的几何学习。

Result: Sparse2DGS显著优于现有方法，且比基于NeRF的微调方法快2倍。

Conclusion: Sparse2DGS通过几何优先增强方案，在稀疏视图下实现了完整且准确的表面重建。

Abstract: We present a Gaussian Splatting method for surface reconstruction using
sparse input views. Previous methods relying on dense views struggle with
extremely sparse Structure-from-Motion points for initialization. While
learning-based Multi-view Stereo (MVS) provides dense 3D points, directly
combining it with Gaussian Splatting leads to suboptimal results due to the
ill-posed nature of sparse-view geometric optimization. We propose Sparse2DGS,
an MVS-initialized Gaussian Splatting pipeline for complete and accurate
reconstruction. Our key insight is to incorporate the geometric-prioritized
enhancement schemes, allowing for direct and robust geometric learning under
ill-posed conditions. Sparse2DGS outperforms existing methods by notable
margins while being ${2}\times$ faster than the NeRF-based fine-tuning
approach.

</details>


### [64] [GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting](https://arxiv.org/abs/2504.20379)
*Jongwon Lee,Timothy Bretl*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯点云（3DGS）的场景表示方法，用于快速定位查询图像的位置和姿态，显著降低了推理时间和估计误差。


<details>
  <summary>Details</summary>
Motivation: 现有方法在定位图像时计算量大且耗时，需要一种更高效的解决方案。

Method: 1. 使用3DGS生成初始姿态的合成RGBD图像；2. 建立查询图像与合成图像的2D-2D对应关系；3. 利用深度信息将2D-2D对应提升为2D-3D对应，并通过PnP求解最终姿态。

Result: 在三个数据集上测试，推理时间从10秒降至0.1秒，姿态估计误差显著降低，且对初始姿态误差容忍度高。

Conclusion: 该方法高效且鲁棒，适用于大规模场景的图像定位任务。

Abstract: In this paper, we present a method for localizing a query image with respect
to a precomputed 3D Gaussian Splatting (3DGS) scene representation. First, the
method uses 3DGS to render a synthetic RGBD image at some initial pose
estimate. Second, it establishes 2D-2D correspondences between the query image
and this synthetic image. Third, it uses the depth map to lift the 2D-2D
correspondences to 2D-3D correspondences and solves a perspective-n-point (PnP)
problem to produce a final pose estimate. Results from evaluation across three
existing datasets with 38 scenes and over 2,700 test images show that our
method significantly reduces both inference time (by over two orders of
magnitude, from more than 10 seconds to as fast as 0.1 seconds) and estimation
error compared to baseline methods that use photometric loss minimization.
Results also show that our method tolerates large errors in the initial pose
estimate of up to 55{\deg} in rotation and 1.1 units in translation (normalized
by scene scale), achieving final pose errors of less than 5{\deg} in rotation
and 0.05 units in translation on 90% of images from the Synthetic NeRF and
Mip-NeRF360 datasets and on 42% of images from the more challenging Tanks and
Temples dataset.

</details>


### [65] [Neural Stereo Video Compression with Hybrid Disparity Compensation](https://arxiv.org/abs/2504.20383)
*Shiyin Jiang,Zhenghao Chen,Minghao Han,Xingyu Zhou,Leheng Zhang,Shuhang Gu*

Main category: cs.CV

TL;DR: 提出了一种混合视差补偿（HDC）策略，结合显式像素位移和隐式交叉注意力机制，用于立体视频压缩（SVC），并构建了一个端到端优化的神经框架。


<details>
  <summary>Details</summary>
Motivation: 现有视差补偿策略分为显式水平位移和隐式交叉注意力机制，但各有局限性。HDC旨在结合两者优势，更全面地捕捉视差信息。

Method: HDC先通过融合水平位移特征生成相似性图，再将其归一化为显式像素级注意力分数，用于隐式特征对齐。基于HDC，构建了HDC-FER和HDC-EM模块，优化了特征提取和熵建模。

Result: 在KITTI 2012、KITTI 2015和Nagoya等SVC基准测试中，该框架优于传统和神经SVC方法。

Conclusion: HDC策略有效结合了显式和隐式视差补偿的优势，显著提升了立体视频压缩性能。

Abstract: Disparity compensation represents the primary strategy in stereo video
compression (SVC) for exploiting cross-view redundancy. These mechanisms can be
broadly categorized into two types: one that employs explicit horizontal
shifting, and another that utilizes an implicit cross-attention mechanism to
reduce cross-view disparity redundancy. In this work, we propose a hybrid
disparity compensation (HDC) strategy that leverages explicit pixel
displacement as a robust prior feature to simplify optimization and perform
implicit cross-attention mechanisms for subsequent warping operations, thereby
capturing a broader range of disparity information. Specifically, HDC first
computes a similarity map by fusing the horizontally shifted cross-view
features to capture pixel displacement information. This similarity map is then
normalized into an "explicit pixel-wise attention score" to perform the
cross-attention mechanism, implicitly aligning features from one view to
another. Building upon HDC, we introduce a novel end-to-end optimized neural
stereo video compression framework, which integrates HDC-based modules into key
coding operations, including cross-view feature extraction and reconstruction
(HDC-FER) and cross-view entropy modeling (HDC-EM). Extensive experiments on
SVC benchmarks, including KITTI 2012, KITTI 2015, and Nagoya, which cover both
autonomous driving and general scenes, demonstrate that our framework
outperforms both neural and traditional SVC methodologies.

</details>


### [66] [FiLA-Video: Spatio-Temporal Compression for Fine-Grained Long Video Understanding](https://arxiv.org/abs/2504.20384)
*Yanan Guo,Wenhui Dong,Jun Song,Shiding Zhu,Xuan Zhang,Hanqing Yang,Yingbo Wang,Yang Du,Xianing Chen,Bo Zheng*

Main category: cs.CV

TL;DR: FiLA-Video提出了一种轻量级动态权重多帧融合策略，通过自适应整合多帧信息并保留关键视频内容，同时降低计算成本，解决了长视频理解中的冗余和计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频理解中的特征压缩方法存在冗余信息或计算开销大的问题，限制了长视频理解的性能。

Method: 采用动态权重多帧融合策略和关键帧选择策略，结合长视频训练数据生成方法。

Result: 实验表明，FiLA-Video在长视频理解中实现了更高的效率和准确性。

Conclusion: FiLA-Video通过优化帧融合和关键帧选择，显著提升了长视频理解的性能。

Abstract: Recent advancements in video understanding within visual large language
models (VLLMs) have led to notable progress. However, the complexity of video
data and contextual processing limitations still hinder long-video
comprehension. A common approach is video feature compression to reduce token
input to large language models, yet many methods either fail to prioritize
essential features, leading to redundant inter-frame information, or introduce
computationally expensive modules.To address these issues, we propose
FiLA(Fine-grained Vision Language Model)-Video, a novel framework that
leverages a lightweight dynamic-weight multi-frame fusion strategy, which
adaptively integrates multiple frames into a single representation while
preserving key video information and reducing computational costs. To enhance
frame selection for fusion, we introduce a keyframe selection strategy,
effectively identifying informative frames from a larger pool for improved
summarization. Additionally, we present a simple yet effective long-video
training data generation strategy, boosting model performance without extensive
manual annotation. Experimental results demonstrate that FiLA-Video achieves
superior efficiency and accuracy in long-video comprehension compared to
existing methods.

</details>


### [67] [GarmentX: Autoregressive Parametric Representations for High-Fidelity 3D Garment Generation](https://arxiv.org/abs/2504.20409)
*Jingfeng Guo,Jinnan Chen,Weikai Chen,Zhenyu Sun,Lanjiong Li,Baozhu Zhao,Lingting Zhu,Xin Wang,Qi Liu*

Main category: cs.CV

TL;DR: GarmentX是一个从单张图像生成多样化、高保真且可穿戴3D服装的新框架，通过参数化表示和自回归模型解决传统方法的问题。


<details>
  <summary>Details</summary>
Motivation: 传统服装重建方法直接预测2D图案边缘及其连接性，导致自相交和物理不可行的服装结构。GarmentX旨在解决这些问题。

Method: 引入结构化且可编辑的参数化表示，结合自回归模型预测服装参数，并使用自动生成的大规模数据集GarmentX。

Result: 在几何保真度和输入图像对齐方面达到最先进性能，显著优于现有方法。

Conclusion: GarmentX框架和数据集为3D服装生成提供了高效且可编辑的解决方案。

Abstract: This work presents GarmentX, a novel framework for generating diverse,
high-fidelity, and wearable 3D garments from a single input image. Traditional
garment reconstruction methods directly predict 2D pattern edges and their
connectivity, an overly unconstrained approach that often leads to severe
self-intersections and physically implausible garment structures. In contrast,
GarmentX introduces a structured and editable parametric representation
compatible with GarmentCode, ensuring that the decoded sewing patterns always
form valid, simulation-ready 3D garments while allowing for intuitive
modifications of garment shape and style. To achieve this, we employ a masked
autoregressive model that sequentially predicts garment parameters, leveraging
autoregressive modeling for structured generation while mitigating
inconsistencies in direct pattern prediction. Additionally, we introduce
GarmentX dataset, a large-scale dataset of 378,682 garment parameter-image
pairs, constructed through an automatic data generation pipeline that
synthesizes diverse and high-quality garment images conditioned on parametric
garment representations. Through integrating our method with GarmentX dataset,
we achieve state-of-the-art performance in geometric fidelity and input image
alignment, significantly outperforming prior approaches. We will release
GarmentX dataset upon publication.

</details>


### [68] [Plant Disease Detection through Multimodal Large Language Models and Convolutional Neural Networks](https://arxiv.org/abs/2504.20419)
*Konstantinos I. Roumeliotis,Ranjan Sapkota,Manoj Karkee,Nikolaos D. Tselikas,Dimitrios K. Nasiopoulos*

Main category: cs.CV

TL;DR: 研究探讨了结合多模态大语言模型（GPT-4o）与卷积神经网络（CNNs）在植物病害分类中的效果，发现微调后的GPT-4o性能略优于ResNet-50，但零样本表现较差。


<details>
  <summary>Details</summary>
Motivation: 解决农业自动化中作物监测和病害管理的挑战，尤其是通过早期检测系统。

Method: 使用PlantVillage数据集，评估了GPT-4o和ResNet-50在零样本、少样本和渐进微调场景下的性能。

Result: 微调后的GPT-4o在苹果叶图像分类中达到98.12%准确率，优于ResNet-50的96.88%，但零样本表现较差。

Conclusion: 多模态大语言模型在自动化病害检测中具有潜力，可提升精准农业系统的智能化和可扩展性。

Abstract: Automation in agriculture plays a vital role in addressing challenges related
to crop monitoring and disease management, particularly through early detection
systems. This study investigates the effectiveness of combining multimodal
Large Language Models (LLMs), specifically GPT-4o, with Convolutional Neural
Networks (CNNs) for automated plant disease classification using leaf imagery.
Leveraging the PlantVillage dataset, we systematically evaluate model
performance across zero-shot, few-shot, and progressive fine-tuning scenarios.
A comparative analysis between GPT-4o and the widely used ResNet-50 model was
conducted across three resolutions (100, 150, and 256 pixels) and two plant
species (apple and corn). Results indicate that fine-tuned GPT-4o models
achieved slightly better performance compared to the performance of ResNet-50,
achieving up to 98.12% classification accuracy on apple leaf images, compared
to 96.88% achieved by ResNet-50, with improved generalization and near-zero
training loss. However, zero-shot performance of GPT-4o was significantly
lower, underscoring the need for minimal training. Additional evaluations on
cross-resolution and cross-plant generalization revealed the models'
adaptability and limitations when applied to new domains. The findings
highlight the promise of integrating multimodal LLMs into automated disease
detection pipelines, enhancing the scalability and intelligence of precision
agriculture systems while reducing the dependence on large, labeled datasets
and high-resolution sensor infrastructure. Large Language Models, Vision
Language Models, LLMs and CNNs, Disease Detection with Vision Language Models,
VLMs

</details>


### [69] [AI Assisted Cervical Cancer Screening for Cytology Samples in Developing Countries](https://arxiv.org/abs/2504.20435)
*Love Panta,Suraj Prasai,Karishma Malla Vaidya,Shyam Shrestha,Suresh Manandhar*

Main category: cs.CV

TL;DR: 本文提出了一种结合低成本显微镜和高效AI算法的自动化宫颈癌筛查方法，显著提升了筛查的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统液基细胞学检查（LBC）劳动密集、依赖专家且易出错，亟需更高效的筛查方法。

Method: 采用低成本显微镜捕获图像，通过AI流程（图像拼接、细胞分割和分类）处理，结合轻量级UNet分割模型和CvT分类模型。

Result: 系统在SIPaKMeD数据集上准确分类五种细胞类型，性能优于现有方法。

Conclusion: 该方法为宫颈癌筛查提供了高效、准确的解决方案。

Abstract: Cervical cancer remains a significant health challenge, with high incidence
and mortality rates, particularly in transitioning countries. Conventional
Liquid-Based Cytology(LBC) is a labor-intensive process, requires expert
pathologists and is highly prone to errors, highlighting the need for more
efficient screening methods. This paper introduces an innovative approach that
integrates low-cost biological microscopes with our simple and efficient AI
algorithms for automated whole-slide analysis. Our system uses a motorized
microscope to capture cytology images, which are then processed through an AI
pipeline involving image stitching, cell segmentation, and classification. We
utilize the lightweight UNet-based model involving human-in-the-loop approach
to train our segmentation model with minimal ROIs. CvT-based classification
model, trained on the SIPaKMeD dataset, accurately categorizes five cell types.
Our framework offers enhanced accuracy and efficiency in cervical cancer
screening compared to various state-of-art methods, as demonstrated by
different evaluation metrics.

</details>


### [70] [PixelHacker: Image Inpainting with Structural and Semantic Consistency](https://arxiv.org/abs/2504.20438)
*Ziyang Xu,Kangsheng Duan,Xiaolei Shen,Zhifeng Ding,Wenyu Liu,Xiaohu Ruan,Xiaoxin Chen,Xinggang Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为PixelHacker的扩散模型，通过潜在类别指导解决图像修复中的结构和语义问题，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂结构和语义修复上表现不佳，导致生成结果存在伪影和逻辑错误。

Method: 构建大规模图像-掩码数据集，通过潜在类别指导编码前景和背景特征，并在去噪过程中注入这些特征。

Result: PixelHacker在Places2、CelebA-HQ和FFHQ等数据集上全面超越现有方法，表现出色。

Conclusion: PixelHacker通过潜在类别指导显著提升了图像修复的结构和语义一致性。

Abstract: Image inpainting is a fundamental research area between image editing and
image generation. Recent state-of-the-art (SOTA) methods have explored novel
attention mechanisms, lightweight architectures, and context-aware modeling,
demonstrating impressive performance. However, they often struggle with complex
structure (e.g., texture, shape, spatial relations) and semantics (e.g., color
consistency, object restoration, and logical correctness), leading to artifacts
and inappropriate generation. To address this challenge, we design a simple yet
effective inpainting paradigm called latent categories guidance, and further
propose a diffusion-based model named PixelHacker. Specifically, we first
construct a large dataset containing 14 million image-mask pairs by annotating
foreground and background (potential 116 and 21 categories, respectively).
Then, we encode potential foreground and background representations separately
through two fixed-size embeddings, and intermittently inject these features
into the denoising process via linear attention. Finally, by pre-training on
our dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker.
Extensive experiments show that PixelHacker comprehensively outperforms the
SOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits
remarkable consistency in both structure and semantics. Project page at
https://hustvl.github.io/projects/PixelHacker.

</details>


### [71] [LMM4Gen3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs](https://arxiv.org/abs/2504.20466)
*Woo Yi Yang,Jiarui Wang,Sijing Wu,Huiyu Duan,Yuxin Zhu,Liu Yang,Kang Fu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 论文提出了Gen3DHF基准和LMME3DHF模型，用于评估AI生成的3D人脸质量，并在质量和真实性预测、视觉问答及显著区域识别方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 评估AI生成的3D人脸质量具有挑战性，因人类感知的主观性和对脸部特征的敏感性。

Method: 引入Gen3DHF基准（2000个视频和4000个MOS评分），并基于此提出LMME3DHF模型进行多任务评估。

Result: LMME3DHF在质量评分预测和显著区域识别上优于现有方法，且与人类感知一致。

Conclusion: Gen3DHF和LMME3DHF为AI生成3D人脸的质量评估提供了有效工具。

Abstract: The rapid advancement in generative artificial intelligence have enabled the
creation of 3D human faces (HFs) for applications including media production,
virtual reality, security, healthcare, and game development, etc. However,
assessing the quality and realism of these AI-generated 3D human faces remains
a significant challenge due to the subjective nature of human perception and
innate perceptual sensitivity to facial features. To this end, we conduct a
comprehensive study on the quality assessment of AI-generated 3D human faces.
We first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of
AI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS)
collected across two dimensions, i.e., quality and authenticity, 2,000
distortion-aware saliency maps and distortion descriptions. Based on Gen3DHF,
we propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating
3DHF capable of quality and authenticity score prediction, distortion-aware
visual question answering, and distortion-aware saliency prediction.
Experimental results show that LMME3DHF achieves state-of-the-art performance,
surpassing existing methods in both accurately predicting quality scores for
AI-generated 3D human faces and effectively identifying distortion-aware
salient regions and distortion types, while maintaining strong alignment with
human perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be
released upon the publication.

</details>


### [72] [Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception](https://arxiv.org/abs/2504.20468)
*Yuanchen Wu,Lu Zhang,Hang Yao,Junlong Du,Ke Yan,Shouhong Ding,Yunsheng Wu,Xiaoqiang Li*

Main category: cs.CV

TL;DR: 论文提出Antidote框架，通过合成数据驱动的方法缓解大视觉语言模型（LVLMs）在处理反事实预设问题（CPQs）时的幻觉问题，并构建了CP-Bench基准进行评估。


<details>
  <summary>Details</summary>
Motivation: LVLMs在跨模态任务中表现优异，但存在生成反事实响应的幻觉问题，尤其是对反事实预设问题的处理能力较弱。

Method: 引入Antidote框架，利用合成数据将事实先验融入问题以实现自我修正，并将缓解过程解耦为偏好优化问题。

Result: Antidote显著提升了LVLMs在CP-Bench上的性能（50%以上），并在其他基准上也有显著改进。

Conclusion: Antidote是一种无需外部监督的有效方法，能同时缓解多种幻觉问题，且不会引发灾难性遗忘。

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive results across
various cross-modal tasks. However, hallucinations, i.e., the models generating
counterfactual responses, remain a challenge. Though recent studies have
attempted to alleviate object perception hallucinations, they focus on the
models' response generation, and overlooking the task question itself. This
paper discusses the vulnerability of LVLMs in solving counterfactual
presupposition questions (CPQs), where the models are prone to accept the
presuppositions of counterfactual objects and produce severe hallucinatory
responses. To this end, we introduce "Antidote", a unified, synthetic
data-driven post-training framework for mitigating both types of hallucination
above. It leverages synthetic data to incorporate factual priors into questions
to achieve self-correction, and decouple the mitigation process into a
preference optimization problem. Furthermore, we construct "CP-Bench", a novel
benchmark to evaluate LVLMs' ability to correctly handle CPQs and produce
factual responses. Applied to the LLaVA series, Antidote can simultaneously
enhance performance on CP-Bench by over 50%, POPE by 1.8-3.3%, and CHAIR & SHR
by 30-50%, all without relying on external supervision from stronger LVLMs or
human feedback and introducing noticeable catastrophic forgetting issues.

</details>


### [73] [Large-scale visual SLAM for in-the-wild videos](https://arxiv.org/abs/2504.20496)
*Shuo Sun,Torsten Sattler,Malcolm Mielle,Achim J. Lilienthal,Martin Magnusson*

Main category: cs.CV

TL;DR: 该论文提出了一种鲁棒的3D场景重建方法，针对非约束视频中的相机姿态估计和场景重建问题，通过改进现有视觉SLAM方法，结合深度估计和全局优化，实现了更一致的大规模重建。


<details>
  <summary>Details</summary>
Motivation: 现有视觉SLAM方法在非约束视频（如快速旋转、纹理缺失区域和动态物体）中表现不佳，限制了机器人部署的灵活性。

Method: 结合深度视觉里程计、自动恢复相机内参、动态物体掩码、单目深度估计正则化、全局捆绑调整和闭环检测。

Result: 在多种环境中实现了大规模连续的3D模型重建，相比基线方法更一致且准确。

Conclusion: 该方法为非约束视频的3D重建提供了新的基准，显著提升了重建的一致性和鲁棒性。

Abstract: Accurate and robust 3D scene reconstruction from casual, in-the-wild videos
can significantly simplify robot deployment to new environments. However,
reliable camera pose estimation and scene reconstruction from such
unconstrained videos remains an open challenge. Existing visual-only SLAM
methods perform well on benchmark datasets but struggle with real-world footage
which often exhibits uncontrolled motion including rapid rotations and pure
forward movements, textureless regions, and dynamic objects. We analyze the
limitations of current methods and introduce a robust pipeline designed to
improve 3D reconstruction from casual videos. We build upon recent deep visual
odometry methods but increase robustness in several ways. Camera intrinsics are
automatically recovered from the first few frames using structure-from-motion.
Dynamic objects and less-constrained areas are masked with a predictive model.
Additionally, we leverage monocular depth estimates to regularize bundle
adjustment, mitigating errors in low-parallax situations. Finally, we integrate
place recognition and loop closure to reduce long-term drift and refine both
intrinsics and pose estimates through global bundle adjustment. We demonstrate
large-scale contiguous 3D models from several online videos in various
environments. In contrast, baseline methods typically produce locally
inconsistent results at several points, producing separate segments or
distorted maps. In lieu of ground-truth pose data, we evaluate map consistency,
execution time and visual accuracy of re-rendered NeRF models. Our proposed
system establishes a new baseline for visual reconstruction from casual
uncontrolled videos found online, demonstrating more consistent reconstructions
over longer sequences of in-the-wild videos than previously achieved.

</details>


### [74] [Style-Adaptive Detection Transformer for Single-Source Domain Generalized Object Detection](https://arxiv.org/abs/2504.20498)
*Jianhong Han,Yupei Wang,Liang Chen*

Main category: cs.CV

TL;DR: SA-DETR是一种基于DETR的检测器，通过动态风格适配和对象感知对比学习，提升单源域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖数据增强和特征对齐，但无法覆盖所有未见域。DETR在域适应任务中表现优异，但在单源域泛化中尚未探索。

Method: 提出风格适配器动态投影目标域风格，结合对象感知对比学习模块提取域不变特征。

Result: 在五种天气场景中表现出优异的泛化能力。

Conclusion: SA-DETR在单源域泛化任务中具有显著优势。

Abstract: Single-source Domain Generalization (SDG) in object detection aims to develop
a detector using only data from a source domain that can exhibit strong
generalization capability when applied to unseen target domains. Existing
methods are built upon CNN-based detectors and primarily improve robustness by
employing carefully designed data augmentation strategies integrated with
feature alignment techniques. However, data augmentation methods have inherent
drawbacks; they are only effective when the augmented sample distribution
approximates or covers the unseen scenarios, thus failing to enhance
generalization across all unseen domains. Furthermore, while the recent
Detection Transformer (DETR) has demonstrated superior generalization
capability in domain adaptation tasks due to its efficient global information
extraction, its potential in SDG tasks remains unexplored. To this end, we
introduce a strong DETR-based detector named the Style-Adaptive Detection
Transformer (SA-DETR) for SDG in object detection. Specifically, we present a
domain style adapter that projects the style representation of the unseen
target domain into the training domain, enabling dynamic style adaptation.
Then, we propose an object-aware contrastive learning module to guide the
detector in extracting domain-invariant features through contrastive learning.
By using object-aware gating masks to constrain feature aggregation in both
spatial and semantic dimensions, this module achieves cross-domain contrast of
instance-level features, thereby enhancing generalization. Extensive
experiments demonstrate the superior performance and generalization capability
of SA-DETR across five different weather scenarios. Code is released at
https://github.com/h751410234/SA-DETR.

</details>


### [75] [MambaMoE: Mixture-of-Spectral-Spatial-Experts State Space Model for Hyperspectral Image Classification](https://arxiv.org/abs/2504.20509)
*Yichu Xu,Di Wang,Hongzan Jiao,Lefei Zhang,Liangpei Zhang*

Main category: cs.CV

TL;DR: MambaMoE是一种新型的混合专家框架，用于高光谱图像分类，通过自适应光谱-空间建模和不确定性引导学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有Mamba方法忽视了高光谱场景中异质物体的光谱和空间方向特性，导致分类性能受限。

Method: 提出MambaMoE框架，包含混合Mamba专家块（MoMEB）和不确定性引导校正学习（UGCL）策略。

Result: 在多个公开高光谱基准测试中，MambaMoE在准确性和效率上均达到最优。

Conclusion: MambaMoE显著提升了高光谱分类性能，尤其是针对Mamba方法的改进。

Abstract: The Mamba model has recently demonstrated strong potential in hyperspectral
image (HSI) classification, owing to its ability to perform context modeling
with linear computational complexity. However, existing Mamba-based methods
usually neglect the spectral and spatial directional characteristics related to
heterogeneous objects in hyperspectral scenes, leading to limited
classification performance. To address these issues, we propose MambaMoE, a
novel spectral-spatial mixture-of-experts framework, representing the first
MoE-based approach in the HSI classification community. Specifically, we design
a Mixture of Mamba Expert Block (MoMEB) that leverages sparse expert activation
to enable adaptive spectral-spatial modeling. Furthermore, we introduce an
uncertainty-guided corrective learning (UGCL) strategy to encourage the model's
attention toward complex regions prone to prediction ambiguity. Extensive
experiments on multiple public HSI benchmarks demonstrate that MambaMoE
achieves state-of-the-art performance in both accuracy and efficiency compared
to existing advanced approaches, especially for Mamba-based methods. Code will
be released.

</details>


### [76] [SteelBlastQC: Shot-blasted Steel Surface Dataset with Interpretable Detection of Surface Defects](https://arxiv.org/abs/2504.20510)
*Irina Ruzavina,Lisa Sophie Theis,Jesse Lemeer,Rutger de Groen,Leo Ebeling,Andrej Hulak,Jouaria Ali,Guangzhi Tang,Rico Mockel*

Main category: cs.CV

TL;DR: 该研究提出一个包含1654张标记RGB图像的钢表面数据集，用于训练计算机视觉模型，评估了三种分类方法，其中监督方法（CCT和SVM）达到95%的准确率，并支持可解释的决策。


<details>
  <summary>Details</summary>
Motivation: 自动化钢表面喷砂处理的质量控制对提高制造效率和一致性至关重要。

Method: 使用1654张标记图像数据集，评估了CCT、SVM（基于ResNet-50特征提取）和CAE三种分类方法。

Result: CCT和SVM在测试集上达到95%的分类准确率，CAE作为无监督基线表现较弱。

Conclusion: 通过公开数据集和代码，支持缺陷检测研究，推动可解释计算机视觉模型在工业质量控制中的应用。

Abstract: Automating the quality control of shot-blasted steel surfaces is crucial for
improving manufacturing efficiency and consistency. This study presents a
dataset of 1654 labeled RGB images (512x512) of steel surfaces, classified as
either "ready for paint" or "needs shot-blasting." The dataset captures
real-world surface defects, including discoloration, welding lines, scratches
and corrosion, making it well-suited for training computer vision models.
Additionally, three classification approaches were evaluated: Compact
Convolutional Transformers (CCT), Support Vector Machines (SVM) with ResNet-50
feature extraction, and a Convolutional Autoencoder (CAE). The supervised
methods (CCT and SVM) achieve 95% classification accuracy on the test set, with
CCT leveraging transformer-based attention mechanisms and SVM offering a
computationally efficient alternative. The CAE approach, while less effective,
establishes a baseline for unsupervised quality control. We present
interpretable decision-making by all three neural networks, allowing industry
users to visually pinpoint problematic regions and understand the model's
rationale. By releasing the dataset and baseline codes, this work aims to
support further research in defect detection, advance the development of
interpretable computer vision models for quality control, and encourage the
adoption of automated inspection systems in industrial applications.

</details>


### [77] [Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models](https://arxiv.org/abs/2504.20518)
*Zhongqi Wang,Jie Zhang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为动态注意力分析（DAA）的新方法，用于检测文本到图像扩散模型中的后门攻击，通过分析动态注意力图的演变特征，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有后门检测方法主要关注静态特征，但扩散模型具有动态特性，因此需要一种新方法来利用这些动态特征进行更有效的检测。

Method: 提出DAA方法，包括DAA-I（独立分析注意力图）和DAA-S（基于动态系统的图状态方程分析），通过量化动态异常来检测后门。

Result: 在五种代表性后门攻击场景中，DAA平均F1得分为79.49%，AUC为87.67%，显著优于现有方法。

Conclusion: DAA方法通过动态特征分析有效检测后门攻击，为扩散模型的安全性提供了新思路。

Abstract: Recent studies have revealed that text-to-image diffusion models are
vulnerable to backdoor attacks, where attackers implant stealthy textual
triggers to manipulate model outputs. Previous backdoor detection methods
primarily focus on the static features of backdoor samples. However, a vital
property of diffusion models is their inherent dynamism. This study introduces
a novel backdoor detection perspective named Dynamic Attention Analysis (DAA),
showing that these dynamic characteristics serve as better indicators for
backdoor detection. Specifically, by examining the dynamic evolution of
cross-attention maps, we observe that backdoor samples exhibit distinct feature
evolution patterns at the $<$EOS$>$ token compared to benign samples. To
quantify these dynamic anomalies, we first introduce DAA-I, which treats the
tokens' attention maps as spatially independent and measures dynamic feature
using the Frobenius norm. Furthermore, to better capture the interactions
between attention maps and refine the feature, we propose a dynamical
system-based approach, referred to as DAA-S. This model formulates the spatial
correlations among attention maps using a graph-based state equation and we
theoretically analyze the global asymptotic stability of this method. Extensive
experiments across five representative backdoor attack scenarios demonstrate
that our approach significantly surpasses existing detection methods, achieving
an average F1 Score of 79.49% and an AUC of 87.67%. The code is available at
https://github.com/Robin-WZQ/DAA.

</details>


### [78] [Geometry-aware Temporal Aggregation Network for Monocular 3D Lane Detection](https://arxiv.org/abs/2504.20525)
*Huan Zheng,Wencheng Han,Tianyi Yan,Cheng-zhong Xu,Jianbing Shen*

Main category: cs.CV

TL;DR: GTA-Net通过利用多帧输入的几何一致性和时间实例信息，提升了单目3D车道检测的几何准确性和车道完整性。


<details>
  <summary>Details</summary>
Motivation: 当前单目3D车道检测方法存在几何信息不准确和车道完整性难以保持的问题，作者希望通过多帧输入解决这些问题。

Method: 提出了GTA-Net，包含TGEM模块（利用时间几何一致性增强几何感知）和TIQG模块（通过时间线索生成查询以探索实例信息）。

Result: 实验表明GTA-Net在单目3D车道检测中达到了最先进水平。

Conclusion: GTA-Net通过时间几何一致性和实例信息整合，显著提升了3D车道检测的性能。

Abstract: Monocular 3D lane detection aims to estimate 3D position of lanes from
frontal-view (FV) images. However, current monocular 3D lane detection methods
suffer from two limitations, including inaccurate geometric information of the
predicted 3D lanes and difficulties in maintaining lane integrity. To address
these issues, we seek to fully exploit the potential of multiple input frames.
First, we aim at enhancing the ability to perceive the geometry of scenes by
leveraging temporal geometric consistency. Second, we strive to improve the
integrity of lanes by revealing more instance information from temporal
sequences. Therefore, we propose a novel Geometry-aware Temporal Aggregation
Network (GTA-Net) for monocular 3D lane detection. On one hand, we develop the
Temporal Geometry Enhancement Module (TGEM), which exploits geometric
consistency across successive frames, facilitating effective geometry
perception. On the other hand, we present the Temporal Instance-aware Query
Generation (TIQG), which strategically incorporates temporal cues into query
generation, thereby enabling the exploration of comprehensive instance
information. Experiments demonstrate that our GTA-Net achieves SoTA results,
surpassing existing monocular 3D lane detection solutions.

</details>


### [79] [Beyond the Horizon: Decoupling UAVs Multi-View Action Recognition via Partial Order Transfer](https://arxiv.org/abs/2504.20530)
*Wenxuan Liu,Xian Zhong,Zhuo Zhou,Siyuan Yang,Chia-Wen Lin,Alex Chichung Kot*

Main category: cs.CV

TL;DR: 论文提出了一种针对无人机（UAV）动作识别的多视角方法POG-MVNet，通过建模视角层次结构，显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 无人机动作识别因垂直视角变化大而面临挑战，传统方法难以应对不同高度下的外观差异。

Method: 提出POG-MVNet框架，包含视角分区（VP）、顺序感知特征解耦（OFD）和动作部分顺序引导（APOG）三个模块。

Result: 在Drone-Action、MOD20和UAV数据集上，POG-MVNet显著优于现有方法，如Drone-Action提升4.7%。

Conclusion: POG-MVNet通过建模视角层次结构，有效解决了无人机动作识别中的视角变化问题，性能优于现有方法。

Abstract: Action recognition in unmanned aerial vehicles (UAVs) poses unique challenges
due to significant view variations along the vertical spatial axis. Unlike
traditional ground-based settings, UAVs capture actions from a wide range of
altitudes, resulting in considerable appearance discrepancies. We introduce a
multi-view formulation tailored to varying UAV altitudes and empirically
observe a partial order among views, where recognition accuracy consistently
decreases as the altitude increases. This motivates a novel approach that
explicitly models the hierarchical structure of UAV views to improve
recognition performance across altitudes. To this end, we propose the Partial
Order Guided Multi-View Network (POG-MVNet), designed to address drastic view
variations by effectively leveraging view-dependent information across
different altitude levels. The framework comprises three key components: a View
Partition (VP) module, which uses the head-to-body ratio to group views by
altitude; an Order-aware Feature Decoupling (OFD) module, which disentangles
action-relevant and view-specific features under partial order guidance; and an
Action Partial Order Guide (APOG), which leverages the partial order to
transfer informative knowledge from easier views to support learning in more
challenging ones. We conduct experiments on Drone-Action, MOD20, and UAV
datasets, demonstrating that POG-MVNet significantly outperforms competing
methods. For example, POG-MVNet achieves a 4.7% improvement on Drone-Action
dataset and a 3.5% improvement on UAV dataset compared to state-of-the-art
methods ASAT and FAR. The code for POG-MVNet will be made available soon.

</details>


### [80] [Autoencoder Models for Point Cloud Environmental Synthesis from WiFi Channel State Information: A Preliminary Study](https://arxiv.org/abs/2504.20541)
*Daniele Pannone,Danilo Avola*

Main category: cs.CV

TL;DR: 提出一种基于WiFi信道状态信息（CSI）数据生成点云的深度学习框架，采用两阶段自编码器方法，实现从WiFi数据到环境点云的准确重建。


<details>
  <summary>Details</summary>
Motivation: 利用WiFi数据实现环境点云重建，为无线感知和环境映射提供新方法。

Method: 两阶段自编码器：PointNet自编码器生成点云，CNN自编码器将CSI数据映射到匹配的潜在空间。

Result: 实验验证了方法的有效性，展示了在无线感知和环境映射中的潜力。

Conclusion: 该方法为WiFi数据驱动的环境重建提供了可行方案，具有实际应用价值。

Abstract: This paper introduces a deep learning framework for generating point clouds
from WiFi Channel State Information data. We employ a two-stage autoencoder
approach: a PointNet autoencoder with convolutional layers for point cloud
generation, and a Convolutional Neural Network autoencoder to map CSI data to a
matching latent space. By aligning these latent spaces, our method enables
accurate environmental point cloud reconstruction from WiFi data. Experimental
results validate the effectiveness of our approach, highlighting its potential
for wireless sensing and environmental mapping applications.

</details>


### [81] [PartHOI: Part-based Hand-Object Interaction Transfer via Generalized Cylinders](https://arxiv.org/abs/2504.20599)
*Qiaochu Wang,Chufeng Xiao,Manfred Lau,Hongbo Fu*

Main category: cs.CV

TL;DR: PartHOI方法通过基于语义部分的几何对应关系，实现了跨类别的手-物体交互（HOI）姿态迁移，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖形状匹配，难以跨类别迁移HOI姿态。观察到HOI通常涉及物体的语义部分，这些部分在跨类别中形状更一致。

Method: 使用广义圆柱表示参数化物体部分的几何，建立鲁棒的几何对应关系，迁移接触点并优化手部姿态。

Result: 定性和定量结果表明，PartHOI能有效泛化跨类别HOI迁移，生成高保真结果。

Conclusion: PartHOI通过语义部分和几何对应，显著提升了跨类别HOI姿态迁移的能力。

Abstract: Learning-based methods to understand and model hand-object interactions (HOI)
require a large amount of high-quality HOI data. One way to create HOI data is
to transfer hand poses from a source object to another based on the objects'
geometry. However, current methods for transferring hand poses between objects
rely on shape matching, limiting the ability to transfer poses across different
categories due to differences in their shapes and sizes. We observe that HOI
often involves specific semantic parts of objects, which often have more
consistent shapes across categories. In addition, constructing size-invariant
correspondences between these parts is important for cross-category transfer.
Based on these insights, we introduce a novel method PartHOI for part-based HOI
transfer. Using a generalized cylinder representation to parameterize an object
parts' geometry, PartHOI establishes a robust geometric correspondence between
object parts, and enables the transfer of contact points. Given the transferred
points, we optimize a hand pose to fit the target object well. Qualitative and
quantitative results demonstrate that our method can generalize HOI transfers
well even for cross-category objects, and produce high-fidelity results that
are superior to the existing methods.

</details>


### [82] [Purifying, Labeling, and Utilizing: A High-Quality Pipeline for Small Object Detection](https://arxiv.org/abs/2504.20602)
*Siwei Wang,Zhiwei Chen,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: PLUSNet是一个针对小目标检测的优化框架，通过改进上游特征净化、中游样本分配和下游信息利用，实现了整体性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有小目标检测方法通常只优化流程中的孤立阶段，忽略了整体优化，限制了性能提升。

Method: PLUSNet包含三个模块：HFP用于净化上游特征，MCLA优化中游样本分配，FDHead提升下游任务的信息利用效率。

Result: 实验表明PLUSNet在多个数据集上显著提升了小目标检测性能。

Conclusion: PLUSNet通过整体优化检测流程，有效提升了小目标检测的性能，且易于集成到现有检测器中。

Abstract: Small object detection is a broadly investigated research task and is
commonly conceptualized as a "pipeline-style" engineering process. In the
upstream, images serve as raw materials for processing in the detection
pipeline, where pre-trained models are employed to generate initial feature
maps. In the midstream, an assigner selects training positive and negative
samples. Subsequently, these samples and features are fed into the downstream
for classification and regression. Previous small object detection methods
often focused on improving isolated stages of the pipeline, thereby neglecting
holistic optimization and consequently constraining overall performance gains.
To address this issue, we have optimized three key aspects, namely Purifying,
Labeling, and Utilizing, in this pipeline, proposing a high-quality Small
object detection framework termed PLUSNet. Specifically, PLUSNet comprises
three sequential components: the Hierarchical Feature Purifier (HFP) for
purifying upstream features, the Multiple Criteria Label Assignment (MCLA) for
improving the quality of midstream training samples, and the Frequency
Decoupled Head (FDHead) for more effectively exploiting information to
accomplish downstream tasks. The proposed PLUS modules are readily integrable
into various object detectors, thus enhancing their detection capabilities in
multi-scale scenarios. Extensive experiments demonstrate the proposed PLUSNet
consistently achieves significant and consistent improvements across multiple
datasets for small object detection.

</details>


### [83] [EfficientHuman: Efficient Training and Reconstruction of Moving Human using Articulated 2D Gaussian](https://arxiv.org/abs/2504.20607)
*Hao Tian,Rui Liu,Wen Shen,Yilong Hu,Zhihao Zheng,Xiaolin Qin*

Main category: cs.CV

TL;DR: EfficientHuman模型通过使用Articulated 2D Gaussian surfels和优化模块，快速完成动态人体重建，提升渲染质量并减少冗余高斯。


<details>
  <summary>Details</summary>
Motivation: 3DGS在动态人体重建中存在多视角不一致和冗余高斯问题，导致重建效果和训练速度不理想。

Method: 将高斯编码为Articulated 2D Gaussian surfels，并通过LBS变换到姿态空间，结合姿态校准和LBS优化模块。

Result: 在ZJU-MoCap数据集上，EfficientHuman平均重建时间少于1分钟，比现有方法快20秒，且减少了冗余高斯。

Conclusion: EfficientHuman通过创新方法解决了动态人体重建的效率和精度问题，显著优于现有技术。

Abstract: 3D Gaussian Splatting (3DGS) has been recognized as a pioneering technique in
scene reconstruction and novel view synthesis. Recent work on reconstructing
the 3D human body using 3DGS attempts to leverage prior information on human
pose to enhance rendering quality and improve training speed. However, it
struggles to effectively fit dynamic surface planes due to multi-view
inconsistency and redundant Gaussians. This inconsistency arises because
Gaussian ellipsoids cannot accurately represent the surfaces of dynamic
objects, which hinders the rapid reconstruction of the dynamic human body.
Meanwhile, the prevalence of redundant Gaussians means that the training time
of these works is still not ideal for quickly fitting a dynamic human body. To
address these, we propose EfficientHuman, a model that quickly accomplishes the
dynamic reconstruction of the human body using Articulated 2D Gaussian while
ensuring high rendering quality. The key innovation involves encoding Gaussian
splats as Articulated 2D Gaussian surfels in canonical space and then
transforming them to pose space via Linear Blend Skinning (LBS) to achieve
efficient pose transformations. Unlike 3D Gaussians, Articulated 2D Gaussian
surfels can quickly conform to the dynamic human body while ensuring
view-consistent geometries. Additionally, we introduce a pose calibration
module and an LBS optimization module to achieve precise fitting of dynamic
human poses, enhancing the model's performance. Extensive experiments on the
ZJU-MoCap dataset demonstrate that EfficientHuman achieves rapid 3D dynamic
human reconstruction in less than a minute on average, which is 20 seconds
faster than the current state-of-the-art method, while also reducing the number
of redundant Gaussians.

</details>


### [84] [AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation](https://arxiv.org/abs/2504.20629)
*Jeongsoo Choi,Ji-Hoon Kim,Kim Sung-Bin,Tae-Hyun Oh,Joon Son Chung*

Main category: cs.CV

TL;DR: AlignDiT是一种多模态对齐扩散变换器，用于从文本、视频和参考音频生成高质量语音，解决了现有方法在语音清晰度、同步性和自然性上的不足。


<details>
  <summary>Details</summary>
Motivation: 多模态语音生成在电影制作、配音等领域有广泛应用，但现有方法在语音质量、同步性和自然性上存在局限。

Method: 提出AlignDiT，利用扩散变换器的上下文学习能力，通过三种策略对齐多模态表示，并引入多模态无分类器引导机制。

Result: 实验表明AlignDiT在质量、同步性和说话人相似性上显著优于现有方法，并在多模态任务中表现出强泛化能力。

Conclusion: AlignDiT在多模态语音生成任务中实现了最先进的性能，具有广泛的应用潜力。

Abstract: In this paper, we address the task of multimodal-to-speech generation, which
aims to synthesize high-quality speech from multiple input modalities: text,
video, and reference audio. This task has gained increasing attention due to
its wide range of applications, such as film production, dubbing, and virtual
avatars. Despite recent progress, existing methods still suffer from
limitations in speech intelligibility, audio-video synchronization, speech
naturalness, and voice similarity to the reference speaker. To address these
challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer
that generates accurate, synchronized, and natural-sounding speech from aligned
multimodal inputs. Built upon the in-context learning capability of the DiT
architecture, AlignDiT explores three effective strategies to align multimodal
representations. Furthermore, we introduce a novel multimodal classifier-free
guidance mechanism that allows the model to adaptively balance information from
each modality during speech synthesis. Extensive experiments demonstrate that
AlignDiT significantly outperforms existing methods across multiple benchmarks
in terms of quality, synchronization, and speaker similarity. Moreover,
AlignDiT exhibits strong generalization capability across various multimodal
tasks, such as video-to-speech synthesis and visual forced alignment,
consistently achieving state-of-the-art performance. The demo page is available
at https://mm.kaist.ac.kr/projects/AlignDiT .

</details>


### [85] [LDPoly: Latent Diffusion for Polygonal Road Outline Extraction in Large-Scale Topographic Mapping](https://arxiv.org/abs/2504.20645)
*Weiqin Jiao,Hao Cheng,George Vosselman,Claudio Persello*

Main category: cs.CV

TL;DR: LDPoly是首个专门用于从高分辨率航拍图像中提取多边形道路轮廓的框架，采用双潜在扩散模型和通道嵌入融合模块，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未针对多边形道路轮廓提取任务设计，且道路的分支结构和拓扑连接性带来独特挑战。

Method: LDPoly结合双潜在扩散模型与通道嵌入融合模块，生成道路掩膜和顶点热图，并通过定制多边形化方法减少顶点冗余。

Result: 在Map2ImLas数据集上，LDPoly在像素覆盖率、顶点效率、多边形规则性和道路连接性等指标上优于现有方法。

Conclusion: LDPoly首次将扩散模型应用于精确矢量对象轮廓提取，为未来研究奠定了基础。

Abstract: Polygonal road outline extraction from high-resolution aerial images is an
important task in large-scale topographic mapping, where roads are represented
as vectorized polygons, capturing essential geometric features with minimal
vertex redundancy. Despite its importance, no existing method has been
explicitly designed for this task. While polygonal building outline extraction
has been extensively studied, the unique characteristics of roads, such as
branching structures and topological connectivity, pose challenges to these
methods. To address this gap, we introduce LDPoly, the first dedicated
framework for extracting polygonal road outlines from high-resolution aerial
images. Our method leverages a novel Dual-Latent Diffusion Model with a
Channel-Embedded Fusion Module, enabling the model to simultaneously generate
road masks and vertex heatmaps. A tailored polygonization method is then
applied to obtain accurate vectorized road polygons with minimal vertex
redundancy. We evaluate LDPoly on a new benchmark dataset, Map2ImLas, which
contains detailed polygonal annotations for various topographic objects in
several Dutch regions. Our experiments include both in-region and cross-region
evaluations, with the latter designed to assess the model's generalization
performance on unseen regions. Quantitative and qualitative results demonstrate
that LDPoly outperforms state-of-the-art polygon extraction methods across
various metrics, including pixel-level coverage, vertex efficiency, polygon
regularity, and road connectivity. We also design two new metrics to assess
polygon simplicity and boundary smoothness. Moreover, this work represents the
first application of diffusion models for extracting precise vectorized object
outlines without redundant vertices from remote-sensing imagery, paving the way
for future advancements in this field.

</details>


### [86] [SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data](https://arxiv.org/abs/2504.20648)
*Michael Ogezi,Freda Shi*

Main category: cs.CV

TL;DR: 论文提出了一种增强视觉语言模型（VLM）空间推理能力的方法，通过构建合成VQA数据集SpaRE，显著提升了模型在空间推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在空间推理任务上表现不佳，主要因为广泛使用的VL数据集中空间关系样本稀少且分布不均。

Method: 利用Localized Narratives、DOCCI和PixMo-Cap中的超详细图像描述，构建了包含455k样本和3.4百万QA对的合成VQA数据集，并训练SpaRE VLM。

Result: SpaRE VLM在空间推理基准测试中表现显著提升，如What's Up基准上性能提高49%，同时保持通用任务的良好表现。

Conclusion: 该研究缩小了人类与VLM在空间推理能力上的差距，提升了VLM在机器人、导航等实际任务中的应用潜力。

Abstract: Vision-language models (VLMs) work well in tasks ranging from image
captioning to visual question answering (VQA), yet they struggle with spatial
reasoning, a key skill for understanding our physical world that humans excel
at. We find that spatial relations are generally rare in widely used VL
datasets, with only a few being well represented, while most form a long tail
of underrepresented relations. This gap leaves VLMs ill-equipped to handle
diverse spatial relationships. To bridge it, we construct a synthetic VQA
dataset focused on spatial reasoning generated from hyper-detailed image
descriptions in Localized Narratives, DOCCI, and PixMo-Cap. Our dataset
consists of 455k samples containing 3.4 million QA pairs. Trained on this
dataset, our Spatial-Reasoning Enhanced (SpaRE) VLMs show strong improvements
on spatial reasoning benchmarks, achieving up to a 49% performance gain on the
What's Up benchmark, while maintaining strong results on general tasks. Our
work narrows the gap between human and VLM spatial reasoning and makes VLMs
more capable in real-world tasks such as robotics and navigation.

</details>


### [87] [Image deidentification in the XNAT ecosystem: use cases and solutions](https://arxiv.org/abs/2504.20657)
*Alex Michie,Simon J Doran*

Main category: cs.CV

TL;DR: XNAT平台用于DICOM图像去标识化，通过规则和机器学习方法实现高准确率，但地址识别仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 解决DICOM数据去标识化需求，参与MIDI-B挑战以验证方法有效性。

Method: 结合XNAT工具和独立生态系统工具，采用规则和机器学习方法。

Result: 初始得分97.91%，改进后达99.61%，地址识别仍有不足。

Conclusion: 未来将优化地址识别和图像像素去标识化，当前失败率为0.19%。

Abstract: XNAT is a server-based data management platform widely used in academia for
curating large databases of DICOM images for research projects. We describe in
detail a deidentification workflow for DICOM data using facilities in XNAT,
together with independent tools in the XNAT "ecosystem". We list different
contexts in which deidentification might be needed, based on our prior
experience. The starting point for participation in the Medical Image
De-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local
methodologies, which were adapted during the validation phase of the challenge.
Our result in the test phase was 97.91\%, considerably lower than our peers,
due largely to an arcane technical incompatibility of our methodology with the
challenge's Synapse platform, which prevented us receiving feedback during the
validation phase. Post-submission, additional discrepancy reports from the
organisers and via the MIDI-B Continuous Benchmarking facility, enabled us to
improve this score significantly to 99.61\%. An entirely rule-based approach
was shown to be capable of removing all name-related information in the test
corpus, but exhibited failures in dealing fully with address data. Initial
experiments using published machine-learning models to remove addresses were
partially successful but showed the models to be "over-aggressive" on other
types of free-text data, leading to a slight overall degradation in performance
to 99.54\%. Future development will therefore focus on improving
address-recognition capabilities, but also on better removal of identifiable
data burned into the image pixels. Several technical aspects relating to the
"answer key" are still under discussion with the challenge organisers, but we
estimate that our percentage of genuine deidentification failures on the MIDI-B
test corpus currently stands at 0.19\%. (Abridged from original for arXiv
submission)

</details>


### [88] [Advance Fake Video Detection via Vision Transformers](https://arxiv.org/abs/2504.20669)
*Joy Battocchio,Stefano Dell'Anna,Andrea Montibeller,Giulia Boato*

Main category: cs.CV

TL;DR: 本文提出了一种基于Vision Transformer（ViT）的框架，用于检测AI生成的视频，解决了虚假多媒体传播的紧迫问题。


<details>
  <summary>Details</summary>
Motivation: AI生成的多媒体技术日益逼真，可能被用于传播虚假信息，因此需要高准确性和泛化能力的检测方法。

Method: 扩展ViT用于视频检测，提出了一种创新框架，通过整合时间维度的ViT嵌入提升性能。

Result: 方法在新的大型多样化数据集上表现出高准确性、泛化能力和少样本学习能力。

Conclusion: 该框架为AI生成视频的检测提供了有效解决方案，符合当前法规需求。

Abstract: Recent advancements in AI-based multimedia generation have enabled the
creation of hyper-realistic images and videos, raising concerns about their
potential use in spreading misinformation. The widespread accessibility of
generative techniques, which allow for the production of fake multimedia from
prompts or existing media, along with their continuous refinement, underscores
the urgent need for highly accurate and generalizable AI-generated media
detection methods, underlined also by new regulations like the European Digital
AI Act. In this paper, we draw inspiration from Vision Transformer (ViT)-based
fake image detection and extend this idea to video. We propose an {original}
%innovative framework that effectively integrates ViT embeddings over time to
enhance detection performance. Our method shows promising accuracy,
generalization, and few-shot learning capabilities across a new, large and
diverse dataset of videos generated using five open source generative
techniques from the state-of-the-art, as well as a separate dataset containing
videos produced by proprietary generative methods.

</details>


### [89] [FBRT-YOLO: Faster and Better for Real-Time Aerial Image Detection](https://arxiv.org/abs/2504.20670)
*Yao Xiao,Tingfa Xu,Yu Xin,Jianan Li*

Main category: cs.CV

TL;DR: FBRT-YOLO是一种新型实时检测器，通过FCM和MKP模块优化小目标检测，平衡精度与效率，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在小目标检测和精度与效率平衡方面仍有不足，阻碍了实时航拍图像检测的发展。

Method: 提出FCM模块缓解小目标信息丢失问题，MKP模块利用多尺度卷积增强目标感知。

Result: 在Visdrone、UAVDT和AI-TOD数据集上，FBRT-YOLO在性能和速度上优于其他实时检测器。

Conclusion: FBRT-YOLO有效解决了航拍图像中小目标检测的挑战，提升了实时检测的实用性。

Abstract: Embedded flight devices with visual capabilities have become essential for a
wide range of applications. In aerial image detection, while many existing
methods have partially addressed the issue of small target detection,
challenges remain in optimizing small target detection and balancing detection
accuracy with efficiency. These issues are key obstacles to the advancement of
real-time aerial image detection. In this paper, we propose a new family of
real-time detectors for aerial image detection, named FBRT-YOLO, to address the
imbalance between detection accuracy and efficiency. Our method comprises two
lightweight modules: Feature Complementary Mapping Module (FCM) and
Multi-Kernel Perception Unit(MKP), designed to enhance object perception for
small targets in aerial images. FCM focuses on alleviating the problem of
information imbalance caused by the loss of small target information in deep
networks. It aims to integrate spatial positional information of targets more
deeply into the network,better aligning with semantic information in the deeper
layers to improve the localization of small targets. We introduce MKP, which
leverages convolutions with kernels of different sizes to enhance the
relationships between targets of various scales and improve the perception of
targets at different scales. Extensive experimental results on three major
aerial image datasets, including Visdrone, UAVDT, and AI-TOD,demonstrate that
FBRT-YOLO outperforms various real-time detectors in terms of performance and
speed.

</details>


### [90] [Occlusion-aware Driver Monitoring System using the Driver Monitoring Dataset](https://arxiv.org/abs/2504.20677)
*Paola Natalia Cañas,Alexander Diez,David Galvañ,Marcos Nieto,Igor Rodríguez*

Main category: cs.CV

TL;DR: 本文提出了一种基于RGB和红外图像的鲁棒驾驶员监控系统（DMS），具备驾驶员识别、区域注视估计和遮挡检测功能，适用于不同光照条件。


<details>
  <summary>Details</summary>
Motivation: 开发一种符合EuroNCAP标准的DMS，提升系统在遮挡和低光条件下的可靠性和信任度。

Method: 采用分别训练于RGB和红外图像的算法，整合为统一流程，解决多传感器和实际车辆部署的挑战。

Result: 在DMD数据集和实际场景中验证了系统有效性，RGB模型表现更优，遮挡检测功能为DMS领域创新。

Conclusion: 该系统在复杂条件下表现优异，遮挡检测功能显著提升了DMS的实用性和可靠性。

Abstract: This paper presents a robust, occlusion-aware driver monitoring system (DMS)
utilizing the Driver Monitoring Dataset (DMD). The system performs driver
identification, gaze estimation by regions, and face occlusion detection under
varying lighting conditions, including challenging low-light scenarios. Aligned
with EuroNCAP recommendations, the inclusion of occlusion detection enhances
situational awareness and system trustworthiness by indicating when the
system's performance may be degraded. The system employs separate algorithms
trained on RGB and infrared (IR) images to ensure reliable functioning. We
detail the development and integration of these algorithms into a cohesive
pipeline, addressing the challenges of working with different sensors and
real-car implementation. Evaluation on the DMD and in real-world scenarios
demonstrates the effectiveness of the proposed system, highlighting the
superior performance of RGB-based models and the pioneering contribution of
robust occlusion detection in DMS.

</details>


### [91] [OG-HFYOLO :Orientation gradient guidance and heterogeneous feature fusion for deformation table cell instance segmentation](https://arxiv.org/abs/2504.20682)
*Long Liu,Cihui Yang*

Main category: cs.CV

TL;DR: 论文提出OG-HFYOLO模型，通过梯度方向感知提取器和异构核交叉融合模块解决变形表格的结构识别问题，并生成数据集DWTAL。


<details>
  <summary>Details</summary>
Motivation: 变形表格的几何变形导致内容与结构关联性弱，影响下游任务准确性。

Method: 采用梯度方向感知提取器增强边缘响应，结合异构核交叉融合模块和尺度感知损失函数，后处理中引入掩码驱动的非极大抑制。

Result: 模型在主流实例分割模型中表现出优异的分割精度。

Conclusion: 提出的模型和数据集填补了变形表格细粒度空间坐标定位的空白。

Abstract: Table structure recognition is a key task in document analysis. However, the
geometric deformation in deformed tables causes a weak correlation between
content information and structure, resulting in downstream tasks not being able
to obtain accurate content information. To obtain fine-grained spatial
coordinates of cells, we propose the OG-HFYOLO model, which enhances the edge
response by Gradient Orientation-aware Extractor, combines a Heterogeneous
Kernel Cross Fusion module and a scale-aware loss function to adapt to
multi-scale objective features, and introduces mask-driven non-maximal
suppression in the post-processing, which replaces the traditional bounding box
suppression mechanism. Furthermore, we also propose a data generator, filling
the gap in the dataset for fine-grained deformation table cell spatial
coordinate localization, and derive a large-scale dataset named Deformation
Wired Table (DWTAL). Experiments show that our proposed model demonstrates
excellent segmentation accuracy on all mainstream instance segmentation models.
The dataset and the source code are open source:
https://github.com/justliulong/OGHFYOLO.

</details>


### [92] [Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion](https://arxiv.org/abs/2504.20685)
*Zesheng Wang,Alexandre Bruckert,Patrick Le Callet,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出了一种基于扩散方法的高效面部动作生成技术（FAD），结合专门设计的ELNet，显著提升了听者面部动作生成性能，并大幅减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖3DMM系数建模，计算速度慢，难以实现实时交互。

Method: 引入扩散方法（FAD）和高效听者网络（ELNet），结合视听输入生成面部动作。

Result: 性能优于现有方法，计算时间减少99%。

Conclusion: FAD和ELNet的组合为实时面部动作生成提供了高效解决方案。

Abstract: Generating realistic listener facial motions in dyadic conversations remains
challenging due to the high-dimensional action space and temporal dependency
requirements. Existing approaches usually consider extracting 3D Morphable
Model (3DMM) coefficients and modeling in the 3DMM space. However, this makes
the computational speed of the 3DMM a bottleneck, making it difficult to
achieve real-time interactive responses. To tackle this problem, we propose
Facial Action Diffusion (FAD), which introduces the diffusion methods from the
field of image generation to achieve efficient facial action generation. We
further build the Efficient Listener Network (ELNet) specially designed to
accommodate both the visual and audio information of the speaker as input.
Considering of FAD and ELNet, the proposed method learns effective listener
facial motion representations and leads to improvements of performance over the
state-of-the-art methods while reducing 99% computational time.

</details>


### [93] [In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer](https://arxiv.org/abs/2504.20690)
*Zechuan Zhang,Ji Xie,Yu Lu,Zongxin Yang,Yi Yang*

Main category: cs.CV

TL;DR: 论文提出了一种基于指令的图像编辑方法，通过结合Diffusion Transformer（DiT）的能力，解决了当前方法在精度与效率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在指令理解和编辑质量上存在不足，且计算资源需求高。本文旨在通过高效的方法实现高精度编辑。

Method: 提出三种贡献：(1) 基于上下文的零样本编辑框架；(2) LoRA-MoE混合调优策略；(3) 基于视觉语言模型的早期过滤方法。

Result: 实验表明，该方法优于现有技术，仅需0.5%的训练数据和1%的可训练参数。

Conclusion: 本文为高效且高精度的指令引导编辑提供了新范式。

Abstract: Instruction-based image editing enables robust image modification via natural
language prompts, yet current methods face a precision-efficiency tradeoff.
Fine-tuning methods demand significant computational resources and large
datasets, while training-free techniques struggle with instruction
comprehension and edit quality. We resolve this dilemma by leveraging
large-scale Diffusion Transformer (DiT)' enhanced generation capacity and
native contextual awareness. Our solution introduces three contributions: (1)
an in-context editing framework for zero-shot instruction compliance using
in-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning
strategy that enhances flexibility with efficient adaptation and dynamic expert
routing, without extensive retraining; and (3) an early filter inference-time
scaling method using vision-language models (VLMs) to select better initial
noise early, improving edit quality. Extensive evaluations demonstrate our
method's superiority: it outperforms state-of-the-art approaches while
requiring only 0.5% training data and 1% trainable parameters compared to
conventional baselines. This work establishes a new paradigm that enables
high-precision yet efficient instruction-guided editing. Codes and demos can be
found in https://river-zhang.github.io/ICEdit-gh-pages/.

</details>


### [94] [Adept: Annotation-Denoising Auxiliary Tasks with Discrete Cosine Transform Map and Keypoint for Human-Centric Pretraining](https://arxiv.org/abs/2504.20800)
*Weizhen He,Yunfeng Yan,Shixiang Tang,Yiheng Deng,Yangyang Zhong,Pengxin Luo,Donglian Qi*

Main category: cs.CV

TL;DR: 论文提出了一种基于RGB图像频率空间（DCT）的人体感知预训练方法，通过丢弃深度信息并利用DCT提取语义信息，结合关键点和DCT图的辅助任务，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统人体感知任务依赖深度信息或任务特定数据集，但深度数据对相机视角敏感且稀缺。本文旨在通过RGB图像频率空间学习细粒度语义信息，解决数据扩展性问题。

Method: 提出基于DCT的RGB图像语义信息提取方法，结合关键点和DCT图的辅助任务，增强模型对人体细粒度语义的学习能力。

Result: 在多个数据集（如COCO、MPII等）上，模型在姿态估计、人体解析、人群计数等任务中均优于现有方法。

Conclusion: 通过RGB图像频率空间学习语义信息，无需深度数据即可显著提升人体感知任务的性能。

Abstract: Human-centric perception is the core of diverse computer vision tasks and has
been a long-standing research focus. However, previous research studied these
human-centric tasks individually, whose performance is largely limited to the
size of the public task-specific datasets. Recent human-centric methods
leverage the additional modalities, e.g., depth, to learn fine-grained semantic
information, which limits the benefit of pretraining models due to their
sensitivity to camera views and the scarcity of RGB-D data on the Internet.
This paper improves the data scalability of human-centric pretraining methods
by discarding depth information and exploring semantic information of RGB
images in the frequency space by Discrete Cosine Transform (DCT). We further
propose new annotation denoising auxiliary tasks with keypoints and DCT maps to
enforce the RGB image extractor to learn fine-grained semantic information of
human bodies. Our extensive experiments show that when pretrained on
large-scale datasets (COCO and AIC datasets) without depth annotation, our
model achieves better performance than state-of-the-art methods by +0.5 mAP on
COCO, +1.4 PCKh on MPII and -0.51 EPE on Human3.6M for pose estimation, by
+4.50 mIoU on Human3.6M for human parsing, by -3.14 MAE on SHA and -0.07 MAE on
SHB for crowd counting, by +1.1 F1 score on SHA and +0.8 F1 score on SHA for
crowd localization, and by +0.1 mAP on Market1501 and +0.8 mAP on MSMT for
person ReID. We also validate the effectiveness of our method on MPII+NTURGBD
datasets

</details>


### [95] [GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion](https://arxiv.org/abs/2504.20829)
*Jiaxin Hong,Sixu Chen,Shuoyang Sun,Hongyao Yu,Hao Fang,Yuqi Tan,Bin Chen,Shuhan Qi,Jiawei Li*

Main category: cs.CV

TL;DR: 本文首次系统研究了3D高斯泼溅（3DGS）中的后门威胁，提出了一种名为GuassTrap的新型攻击方法，能够在特定视角植入恶意视图，同时保持非目标视图的高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 随着3DGS在安全关键领域的快速应用，亟需研究其潜在的安全漏洞，尤其是后门威胁。

Method: GuassTrap方法包括攻击、稳定和正常训练三个阶段，通过联合优化攻击效果和感知真实性，植入隐蔽且视角一致的恶意渲染。

Result: 实验表明，GuassTrap能有效嵌入难以察觉但有害的后门视图，同时保持正常视图的高质量渲染。

Conclusion: 研究揭示了3D渲染中的安全风险，强调了在安全关键应用中加强防御的必要性。

Abstract: As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene
representation and novel view synthesis, its rapid adoption in safety-critical
domains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of
potential security vulnerabilities. This paper presents the first systematic
study of backdoor threats in 3DGS pipelines. We identify that adversaries may
implant backdoor views to induce malicious scene confusion during inference,
potentially leading to environmental misperception in autonomous navigation or
spatial distortion in immersive environments. To uncover this risk, we propose
GuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap
injects malicious views at specific attack viewpoints while preserving
high-quality rendering in non-target views, ensuring minimal detectability and
maximizing potential harm. Specifically, the proposed method consists of a
three-stage pipeline (attack, stabilization, and normal training) to implant
stealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing
attack efficacy and perceptual realism to expose security risks in 3D
rendering. Extensive experiments on both synthetic and real-world datasets
demonstrate that GuassTrap can effectively embed imperceptible yet harmful
backdoor views while maintaining high-quality rendering in normal views,
validating its robustness, adaptability, and practical applicability.

</details>


### [96] [CMT: A Cascade MAR with Topology Predictor for Multimodal Conditional CAD Generation](https://arxiv.org/abs/2504.20830)
*Jianyu Wu,Yizhou Wang,Xiangyu Yue,Xinzhu Ma,Jingyang Guo,Dongzhan Zhou,Wanli Ouyang,Shixiang Tang*

Main category: cs.CV

TL;DR: 提出了一种多模态CAD生成框架CMT，并构建了大规模数据集mmABC，显著提升了CAD生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有CAD方法因简化表示或架构不足难以满足多模态设计需求，需改进方法与数据支持。

Method: 提出基于B-Rep的CMT框架，包含级联MAR和拓扑预测器，并构建mmABC数据集（130万B-Rep模型）。

Result: CMT在无条件生成任务中Coverage和Valid ratio分别提升10.68%和10.3%，图像条件生成任务中Chamfer提升4.01。

Conclusion: CMT在多模态CAD生成中表现优越，数据集和代码将开源。

Abstract: While accurate and user-friendly Computer-Aided Design (CAD) is crucial for
industrial design and manufacturing, existing methods still struggle to achieve
this due to their over-simplified representations or architectures incapable of
supporting multimodal design requirements. In this paper, we attempt to tackle
this problem from both methods and datasets aspects. First, we propose a
cascade MAR with topology predictor (CMT), the first multimodal framework for
CAD generation based on Boundary Representation (B-Rep). Specifically, the
cascade MAR can effectively capture the ``edge-counters-surface'' priors that
are essential in B-Reps, while the topology predictor directly estimates
topology in B-Reps from the compact tokens in MAR. Second, to facilitate
large-scale training, we develop a large-scale multimodal CAD dataset, mmABC,
which includes over 1.3 million B-Rep models with multimodal annotations,
including point clouds, text descriptions, and multi-view images. Extensive
experiments show the superior of CMT in both conditional and unconditional CAD
generation tasks. For example, we improve Coverage and Valid ratio by +10.68%
and +10.3%, respectively, compared to state-of-the-art methods on ABC in
unconditional generation. CMT also improves +4.01 Chamfer on image conditioned
CAD generation on mmABC. The dataset, code and pretrained network shall be
released.

</details>


### [97] [RadSAM: Segmenting 3D radiological images with a 2D promptable model](https://arxiv.org/abs/2504.20837)
*Julien Khlaut,Elodie Ferreres,Daniel Tordjman,Hélène Philippe,Tom Boeken,Pierre Manceron,Corentin Dancette*

Main category: cs.CV

TL;DR: RadSAM提出了一种基于2D模型的3D医学图像分割方法，通过单次提示实现3D对象分割，解决了现有方法需要逐片提示的繁琐问题，并支持编辑功能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在临床中至关重要，但现有方法（如SAM）基于自然图像预训练，无法有效处理3D医学数据且缺乏编辑功能。

Method: RadSAM通过训练2D模型使用噪声掩码、边界框和点作为初始提示，结合迭代推理管道逐片重建3D掩码。

Result: 在AMOS腹部器官分割数据集上，RadSAM表现优于现有方法，展示了其在3D分割和跨域迁移中的有效性。

Conclusion: RadSAM为3D医学图像分割提供了一种高效且灵活的方法，填补了现有技术的空白。

Abstract: Medical image segmentation is a crucial and time-consuming task in clinical
care, where mask precision is extremely important. The Segment Anything Model
(SAM) offers a promising approach, as it provides an interactive interface
based on visual prompting and edition to refine an initial segmentation. This
model has strong generalization capabilities, does not rely on predefined
classes, and adapts to diverse objects; however, it is pre-trained on natural
images and lacks the ability to process medical data effectively. In addition,
this model is built for 2D images, whereas a whole medical domain is based on
3D images, such as CT and MRI. Recent adaptations of SAM for medical imaging
are based on 2D models, thus requiring one prompt per slice to segment 3D
objects, making the segmentation process tedious. They also lack important
features such as editing. To bridge this gap, we propose RadSAM, a novel method
for segmenting 3D objects with a 2D model from a single prompt. In practice, we
train a 2D model using noisy masks as initial prompts, in addition to bounding
boxes and points. We then use this novel prompt type with an iterative
inference pipeline to reconstruct the 3D mask slice-by-slice. We introduce a
benchmark to evaluate the model's ability to segment 3D objects in CT images
from a single prompt and evaluate the models' out-of-domain transfer and
edition capabilities. We demonstrate the effectiveness of our approach against
state-of-the-art models on this benchmark using the AMOS abdominal organ
segmentation dataset.

</details>


### [98] [FedMVP: Federated Multi-modal Visual Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2504.20860)
*Mainak Singha,Subhankar Roy,Sarthak Mehrotra,Ankit Jha,Moloud Abdar,Biplab Banerjee,Elisa Ricci*

Main category: cs.CV

TL;DR: FedMVP提出了一种联邦学习中的多模态视觉提示调优方法，通过结合图像和文本特征生成动态提示，提升了模型对未见概念的适应性。


<details>
  <summary>Details</summary>
Motivation: 传统文本提示调优在联邦学习中容易过拟合已知概念，依赖记忆的文本特征，限制了其对未见概念的适应性。

Method: 提出FedMVP，利用多模态上下文信息（图像特征和文本属性特征）生成动态提示，通过PromptFormer模块对齐文本和视觉特征，并结合CLIP相似性损失和一致性损失进行训练。

Result: 在20个数据集上的实验表明，FedMVP在保持已知类和领域性能的同时，对未见类和领域具有更高的泛化能力。

Conclusion: FedMVP通过多模态提示调优显著提升了联邦学习中模型的泛化能力，优于现有方法。

Abstract: Textual prompt tuning adapts Vision-Language Models (e.g., CLIP) in federated
learning by tuning lightweight input tokens (or prompts) on local client data,
while keeping network weights frozen. Post training, only the prompts are
shared by the clients with the central server for aggregation. However, textual
prompt tuning often struggles with overfitting to known concepts and may be
overly reliant on memorized text features, limiting its adaptability to unseen
concepts. To address this limitation, we propose Federated Multimodal Visual
Prompt Tuning (FedMVP) that conditions the prompts on comprehensive contextual
information -- image-conditioned features and textual attribute features of a
class -- that is multimodal in nature. At the core of FedMVP is a PromptFormer
module that synergistically aligns textual and visual features through
cross-attention, enabling richer contexual integration. The dynamically
generated multimodal visual prompts are then input to the frozen vision encoder
of CLIP, and trained with a combination of CLIP similarity loss and a
consistency loss. Extensive evaluation on 20 datasets spanning three
generalization settings demonstrates that FedMVP not only preserves performance
on in-distribution classes and domains, but also displays higher
generalizability to unseen classes and domains when compared to
state-of-the-art methods. Codes will be released upon acceptance.

</details>


### [99] [AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection](https://arxiv.org/abs/2504.20865)
*Lorenzo Pellegrini,Davide Cozzolino,Serafino Pandolfini,Davide Maltoni,Matteo Ferrara,Luisa Verdoliva,Marco Prati,Marco Ramilli*

Main category: cs.CV

TL;DR: Ai-GenBench是一个新的基准测试，旨在解决AI生成图像检测的挑战，通过动态评估框架和标准化协议提升检测方法的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 生成AI的快速发展带来了高质量图像合成的能力，但也对媒体真实性提出了挑战，需要更强大的检测方法。

Method: Ai-GenBench采用时间评估框架，逐步训练检测模型以适应新的生成模型（如从GANs到扩散模型），并提供标准化数据集和工具。

Result: Ai-GenBench克服了现有方法的局限性（如数据集划分不公平、计算需求高），提供了可复现且实用的解决方案。

Conclusion: Ai-GenBench通过标准化评估和公开数据，支持开发更强大的检测工具，以应对不断发展的生成AI技术。

Abstract: The rapid advancement of generative AI has revolutionized image creation,
enabling high-quality synthesis from text prompts while raising critical
challenges for media authenticity. We present Ai-GenBench, a novel benchmark
designed to address the urgent need for robust detection of AI-generated images
in real-world scenarios. Unlike existing solutions that evaluate models on
static datasets, Ai-GenBench introduces a temporal evaluation framework where
detection methods are incrementally trained on synthetic images, historically
ordered by their generative models, to test their ability to generalize to new
generative models, such as the transition from GANs to diffusion models. Our
benchmark focuses on high-quality, diverse visual content and overcomes key
limitations of current approaches, including arbitrary dataset splits, unfair
comparisons, and excessive computational demands. Ai-GenBench provides a
comprehensive dataset, a standardized evaluation protocol, and accessible tools
for both researchers and non-experts (e.g., journalists, fact-checkers),
ensuring reproducibility while maintaining practical training requirements. By
establishing clear evaluation rules and controlled augmentation strategies,
Ai-GenBench enables meaningful comparison of detection methods and scalable
solutions. Code and data are publicly available to ensure reproducibility and
to support the development of robust forensic detectors to keep pace with the
rise of new synthetic generators.

</details>


### [100] [FLIM-based Salient Object Detection Networks with Adaptive Decoders](https://arxiv.org/abs/2504.20872)
*Gilson Junior Soares,Matheus Abrantes Cerqueira,Jancarlo F. Gomes,Laurent Najman,Silvio Jamil F. Guimarães,Alexandre Xavier Falcão*

Main category: cs.CV

TL;DR: 该论文提出了一种超轻量级的显著目标检测（SOD）方法，结合FLIM编码器和自适应解码器，仅需少量代表性图像训练，无需反向传播。


<details>
  <summary>Details</summary>
Motivation: 针对计算资源有限的应用场景，研究轻量级模型在SOD任务中的潜力，减少对深度神经网络的依赖。

Method: 使用FLIM方法估计编码器核，结合自适应解码器（权重由启发式函数估计），仅需3-4张代表性图像训练。

Result: 实验表明，提出的FLIM网络在性能上优于现有轻量级模型，适用于标记数据受限的场景。

Conclusion: FLIM网络在显著目标检测任务中表现出色，值得进一步研究其在其他应用中的潜力。

Abstract: Salient Object Detection (SOD) methods can locate objects that stand out in
an image, assign higher values to their pixels in a saliency map, and binarize
the map outputting a predicted segmentation mask. A recent tendency is to
investigate pre-trained lightweight models rather than deep neural networks in
SOD tasks, coping with applications under limited computational resources. In
this context, we have investigated lightweight networks using a methodology
named Feature Learning from Image Markers (FLIM), which assumes that the
encoder's kernels can be estimated from marker pixels on discriminative regions
of a few representative images. This work proposes flyweight networks, hundreds
of times lighter than lightweight models, for SOD by combining a FLIM encoder
with an adaptive decoder, whose weights are estimated for each input image by a
given heuristic function. Such FLIM networks are trained from three to four
representative images only and without backpropagation, making the models
suitable for applications under labeled data constraints as well. We study five
adaptive decoders; two of them are introduced here. Differently from the
previous ones that rely on one neuron per pixel with shared weights, the
heuristic functions of the new adaptive decoders estimate the weights of each
neuron per pixel. We compare FLIM models with adaptive decoders for two
challenging SOD tasks with three lightweight networks from the
state-of-the-art, two FLIM networks with decoders trained by backpropagation,
and one FLIM network whose labeled markers define the decoder's weights. The
experiments demonstrate the advantages of the proposed networks over the
baselines, revealing the importance of further investigating such methods in
new applications.

</details>


### [101] [Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for Visual Classifiers](https://arxiv.org/abs/2504.20902)
*Quentin Guimard,Moreno D'Incà,Massimiliano Mancini,Elisa Ricci*

Main category: cs.CV

TL;DR: C2B是一种无需标注数据即可发现预训练模型偏见的框架，仅需任务描述即可生成偏见建议并评估模型。


<details>
  <summary>Details</summary>
Motivation: 现有偏见识别方法依赖标注数据，限制了应用范围，C2B旨在解决这一问题。

Method: 利用大语言模型生成偏见建议和描述，通过检索模型获取图像并评估模型准确性。

Result: C2B在公开数据集上表现优于依赖标注的基线方法，能发现更多偏见。

Conclusion: C2B为无监督偏见检测提供了有前景的解决方案。

Abstract: A person downloading a pre-trained model from the web should be aware of its
biases. Existing approaches for bias identification rely on datasets containing
labels for the task of interest, something that a non-expert may not have
access to, or may not have the necessary resources to collect: this greatly
limits the number of tasks where model biases can be identified. In this work,
we present Classifier-to-Bias (C2B), the first bias discovery framework that
works without access to any labeled data: it only relies on a textual
description of the classification task to identify biases in the target
classification model. This description is fed to a large language model to
generate bias proposals and corresponding captions depicting biases together
with task-specific target labels. A retrieval model collects images for those
captions, which are then used to assess the accuracy of the model w.r.t. the
given biases. C2B is training-free, does not require any annotations, has no
constraints on the list of biases, and can be applied to any pre-trained model
on any classification task. Experiments on two publicly available datasets show
that C2B discovers biases beyond those of the original datasets and outperforms
a recent state-of-the-art bias detection baseline that relies on task-specific
annotations, being a promising first step toward addressing task-agnostic
unsupervised bias detection.

</details>


### [102] [DS_FusionNet: Dynamic Dual-Stream Fusion with Bidirectional Knowledge Distillation for Plant Disease Recognition](https://arxiv.org/abs/2504.20948)
*Yanghui Song,Chengfu Yang*

Main category: cs.CV

TL;DR: 本文提出了一种动态双流融合网络（DS_FusionNet），用于解决植物病害识别中的小样本学习、叶片遮挡、光照变化和高类间相似性等技术挑战，显著提升了识别精度。


<details>
  <summary>Details</summary>
Motivation: 全球经济作物生长安全面临严峻挑战，植物病害的精确识别与防治成为人工智能农业技术中的关键问题。

Method: 采用双主干架构、可变形动态融合模块和双向知识蒸馏策略，构建DS_FusionNet网络。

Result: 实验表明，DS_FusionNet在PlantDisease和CIFAR-10数据集上仅用10%数据即达到90%以上的分类准确率，在复杂PlantWild数据集上保持85%准确率，表现出卓越的泛化能力。

Conclusion: 该研究为细粒度图像分类提供了新技术思路，并为农业病害的精确识别与管理奠定了坚实基础。

Abstract: Given the severe challenges confronting the global growth security of
economic crops, precise identification and prevention of plant diseases has
emerged as a critical issue in artificial intelligence-enabled agricultural
technology. To address the technical challenges in plant disease recognition,
including small-sample learning, leaf occlusion, illumination variations, and
high inter-class similarity, this study innovatively proposes a Dynamic
Dual-Stream Fusion Network (DS_FusionNet). The network integrates a
dual-backbone architecture, deformable dynamic fusion modules, and
bidirectional knowledge distillation strategy, significantly enhancing
recognition accuracy. Experimental results demonstrate that DS_FusionNet
achieves classification accuracies exceeding 90% using only 10% of the
PlantDisease and CIFAR-10 datasets, while maintaining 85% accuracy on the
complex PlantWild dataset, exhibiting exceptional generalization capabilities.
This research not only provides novel technical insights for fine-grained image
classification but also establishes a robust foundation for precise
identification and management of agricultural diseases.

</details>


### [103] [SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep Features](https://arxiv.org/abs/2504.20970)
*Mete Erdogan,Sebnem Demirtas*

Main category: cs.CV

TL;DR: 提出了一种基于SVD-LS的多类肺炎分类框架，结合自监督和迁移学习模型，实现了高效且准确的诊断。


<details>
  <summary>Details</summary>
Motivation: 通过X光影像实现肺炎的早期准确诊断对治疗和患者预后至关重要，机器学习的发展为自动化诊断工具提供了可能。

Method: 采用SVD-LS框架，利用自监督和迁移学习的特征表示，避免计算昂贵的梯度微调，使用闭式非迭代分类方法。

Result: 实验表明SVD-LS在保持竞争力的同时显著降低计算成本，适用于实时医疗影像应用。

Conclusion: SVD-LS是一种高效且准确的肺炎分类方法，适合实际医疗应用。

Abstract: Accurate and early diagnosis of pneumonia through X-ray imaging is essential
for effective treatment and improved patient outcomes. Recent advancements in
machine learning have enabled automated diagnostic tools that assist
radiologists in making more reliable and efficient decisions. In this work, we
propose a Singular Value Decomposition-based Least Squares (SVD-LS) framework
for multi-class pneumonia classification, leveraging powerful feature
representations from state-of-the-art self-supervised and transfer learning
models. Rather than relying on computationally expensive gradient based
fine-tuning, we employ a closed-form, non-iterative classification approach
that ensures efficiency without compromising accuracy. Experimental results
demonstrate that SVD-LS achieves competitive performance while offering
significantly reduced computational costs, making it a viable alternative for
real-time medical imaging applications.

</details>


### [104] [TesserAct: Learning 4D Embodied World Models](https://arxiv.org/abs/2504.20995)
*Haoyu Zhen,Qiao Sun,Hongxin Zhang,Junyan Li,Siyuan Zhou,Yilun Du,Chuang Gan*

Main category: cs.CV

TL;DR: 提出了一种学习4D世界模型的方法，通过RGB-DN视频预测动态3D场景的时空一致性变化，并用于提升策略学习效果。


<details>
  <summary>Details</summary>
Motivation: 传统2D模型无法捕捉场景的详细形状、配置和时间变化，限制了动态场景预测的准确性。

Method: 利用RGB-DN视频数据训练视频生成模型，生成时空一致的4D场景，并转换为高质量4D世界模型。

Result: 方法在4D场景预测中实现了时空一致性，支持新视角合成，并显著优于基于视频的世界模型。

Conclusion: 该方法为动态场景建模和策略学习提供了更高效的解决方案。

Abstract: This paper presents an effective approach for learning novel 4D embodied
world models, which predict the dynamic evolution of 3D scenes over time in
response to an embodied agent's actions, providing both spatial and temporal
consistency. We propose to learn a 4D world model by training on RGB-DN (RGB,
Depth, and Normal) videos. This not only surpasses traditional 2D models by
incorporating detailed shape, configuration, and temporal changes into their
predictions, but also allows us to effectively learn accurate inverse dynamic
models for an embodied agent. Specifically, we first extend existing robotic
manipulation video datasets with depth and normal information leveraging
off-the-shelf models. Next, we fine-tune a video generation model on this
annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for
each frame. We then present an algorithm to directly convert generated RGB,
Depth, and Normal videos into a high-quality 4D scene of the world. Our method
ensures temporal and spatial coherence in 4D scene predictions from embodied
scenarios, enables novel view synthesis for embodied environments, and
facilitates policy learning that significantly outperforms those derived from
prior video-based world models.

</details>


### [105] [X-Fusion: Introducing New Modality to Frozen Large Language Models](https://arxiv.org/abs/2504.20996)
*Sicheng Mo,Thao Nguyen,Xun Huang,Siddharth Srinivasan Iyer,Yijun Li,Yuchen Liu,Abhishek Tandon,Eli Shechtman,Krishna Kumar Singh,Yong Jae Lee,Bolei Zhou,Yuheng Li*

Main category: cs.CV

TL;DR: X-Fusion是一个扩展预训练大语言模型（LLM）的多模态框架，保留语言能力的同时提升多模态任务表现。


<details>
  <summary>Details</summary>
Motivation: 解决如何在保留LLM语言能力的同时扩展其多模态任务能力的问题。

Method: 采用双塔设计，冻结LLM参数，引入模态特定权重整合视觉信息。

Result: 在图像到文本和文本到图像任务中表现优于其他架构，理解数据提升生成质量，特征对齐对小模型收敛有帮助。

Conclusion: X-Fusion为构建高效统一多模态模型提供了重要见解。

Abstract: We propose X-Fusion, a framework that extends pretrained Large Language
Models (LLMs) for multimodal tasks while preserving their language
capabilities. X-Fusion employs a dual-tower design with modality-specific
weights, keeping the LLM's parameters frozen while integrating vision-specific
information for both understanding and generation. Our experiments demonstrate
that X-Fusion consistently outperforms alternative architectures on both
image-to-text and text-to-image tasks. We find that incorporating
understanding-focused data improves generation quality, reducing image data
noise enhances overall performance, and feature alignment accelerates
convergence for smaller models but has minimal impact on larger ones. Our
findings provide valuable insights into building efficient unified multimodal
models.

</details>


### [106] [YoChameleon: Personalized Vision and Language Generation](https://arxiv.org/abs/2504.20998)
*Thao Nguyen,Krishna Kumar Singh,Jing Shi,Trung Bui,Yong Jae Lee,Yuheng Li*

Main category: cs.CV

TL;DR: Yo'Chameleon是首个研究大型多模态模型个性化的方法，通过软提示调优实现个性化图像生成和问答。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型缺乏对用户特定概念的个性化知识，尤其在图像生成领域尚未探索。

Method: 使用3-5张图像，通过软提示调优嵌入主题信息，结合自提示优化和软正向图像生成技术。

Result: 能够回答关于主题的问题，并在新上下文中生成高质量图像。

Conclusion: Yo'Chameleon为多模态模型个性化提供了有效解决方案。

Abstract: Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into
powerful tools with millions of users. However, they remain generic models and
lack personalized knowledge of specific user concepts. Previous work has
explored personalization for text generation, yet it remains unclear how these
methods can be adapted to new modalities, such as image generation. In this
paper, we introduce Yo'Chameleon, the first attempt to study personalization
for large multimodal models. Given 3-5 images of a particular concept,
Yo'Chameleon leverages soft-prompt tuning to embed subject-specific information
to (i) answer questions about the subject and (ii) recreate pixel-level details
to produce images of the subject in new contexts. Yo'Chameleon is trained with
(i) a self-prompting optimization mechanism to balance performance across
multiple modalities, and (ii) a ``soft-positive" image generation approach to
enhance image quality in a few-shot setting.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [107] [Evolution of AI in Education: Agentic Workflows](https://arxiv.org/abs/2504.20082)
*Firuz Kamalov,David Santandreu Calonge,Linda Smail,Dilshod Azizov,Dimple R. Thadani,Theresa Kwong,Amara Atif*

Main category: cs.AI

TL;DR: 论文探讨了AI代理在教育中的潜力，分析了反思、规划、工具使用和多代理协作四种范式，并通过一个自动评分框架展示了其优势。


<details>
  <summary>Details</summary>
Motivation: 传统大型语言模型在教育应用中存在静态数据依赖、适应性和推理能力不足的问题，AI代理被视为更可持续的创新方向。

Method: 通过四种设计范式（反思、规划、工具使用和多代理协作）分析AI代理在教育中的作用，并开发了一个多代理框架用于自动作文评分。

Result: 初步结果显示，代理方法比独立的大型语言模型在一致性上表现更好。

Conclusion: AI代理在教育中具有变革潜力，但需进一步研究其可解释性、可信度和对教学的可持续影响。

Abstract: Artificial intelligence (AI) has transformed various aspects of education,
with large language models (LLMs) driving advancements in automated tutoring,
assessment, and content generation. However, conventional LLMs are constrained
by their reliance on static training data, limited adaptability, and lack of
reasoning. To address these limitations and foster more sustainable
technological practices, AI agents have emerged as a promising new avenue for
educational innovation. In this review, we examine agentic workflows in
education according to four major paradigms: reflection, planning, tool use,
and multi-agent collaboration. We critically analyze the role of AI agents in
education through these key design paradigms, exploring their advantages,
applications, and challenges. To illustrate the practical potential of agentic
systems, we present a proof-of-concept application: a multi-agent framework for
automated essay scoring. Preliminary results suggest this agentic approach may
offer improved consistency compared to stand-alone LLMs. Our findings highlight
the transformative potential of AI agents in educational settings while
underscoring the need for further research into their interpretability,
trustworthiness, and sustainable impact on pedagogical impact.

</details>


### [108] [AI Awareness](https://arxiv.org/abs/2504.20084)
*Xiaojian Li,Haoyuan Shi,Rongwu Xu,Wei Xu*

Main category: cs.AI

TL;DR: 本文综述了AI意识的四个维度（元认知、自我意识、社会意识和情境意识），探讨其理论基础、评估方法及与AI能力的关联，并指出其潜在风险与伦理问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力的提升，研究AI意识的功能性表现成为重要课题，旨在理解其对智能行为的影响及其潜在风险。

Method: 结合认知科学、心理学和计算理论，分析AI意识的理论基础，并系统评估现有方法和实证结果。

Result: 研究表明，更具意识的AI通常表现出更高的智能行为，但也伴随着安全与伦理风险。

Conclusion: AI意识是一把双刃剑，需在提升能力的同时谨慎应对其风险，为未来研究提供方向。

Abstract: Recent breakthroughs in artificial intelligence (AI) have brought about
increasingly capable systems that demonstrate remarkable abilities in
reasoning, language understanding, and problem-solving. These advancements have
prompted a renewed examination of AI awareness, not as a philosophical question
of consciousness, but as a measurable, functional capacity. In this review, we
explore the emerging landscape of AI awareness, which includes meta-cognition
(the ability to represent and reason about its own state), self-awareness
(recognizing its own identity, knowledge, limitations, inter alia), social
awareness (modeling the knowledge, intentions, and behaviors of other agents),
and situational awareness (assessing and responding to the context in which it
operates).
  First, we draw on insights from cognitive science, psychology, and
computational theory to trace the theoretical foundations of awareness and
examine how the four distinct forms of AI awareness manifest in
state-of-the-art AI. Next, we systematically analyze current evaluation methods
and empirical findings to better understand these manifestations. Building on
this, we explore how AI awareness is closely linked to AI capabilities,
demonstrating that more aware AI agents tend to exhibit higher levels of
intelligent behaviors. Finally, we discuss the risks associated with AI
awareness, including key topics in AI safety, alignment, and broader ethical
concerns.
  AI awareness is a double-edged sword: it improves general capabilities, i.e.,
reasoning, safety, while also raises concerns around misalignment and societal
risks, demanding careful oversight as AI capabilities grow. On the whole, our
interdisciplinary review provides a roadmap for future research and aims to
clarify the role of AI awareness in the ongoing development of intelligent
machines.

</details>


### [109] [Spark: A System for Scientifically Creative Idea Generation](https://arxiv.org/abs/2504.20090)
*Aishik Sanyal,Samuel Schapiro,Sumuk Shashidhar,Royce Moon,Lav R. Varshney,Dilek Hakkani-Tur*

Main category: cs.AI

TL;DR: 论文介绍了Spark系统，结合检索增强的LLMs和基于60万科学评论训练的Judge模型，用于生成和评估科学创意。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在科学创意生成中的潜力，并激发计算创造力（CC）领域的研究。

Method: 开发Spark系统，结合检索增强的LLMs和Judge模型（基于OpenReview的60万评论训练）。

Result: 展示了系统功能，并发布了训练Judge的标注数据集。

Conclusion: Spark系统为科学创意生成和评估提供了新方法，鼓励CC研究者进一步探索LLMs的应用。

Abstract: Recently, large language models (LLMs) have shown promising abilities to
generate novel research ideas in science, a direction which coincides with many
foundational principles in computational creativity (CC). In light of these
developments, we present an idea generation system named Spark that couples
retrieval-augmented idea generation using LLMs with a reviewer model named
Judge trained on 600K scientific reviews from OpenReview. Our work is both a
system demonstration and intended to inspire other CC researchers to explore
grounding the generation and evaluation of scientific ideas within foundational
CC principles. To this end, we release the annotated dataset used to train
Judge, inviting other researchers to explore the use of LLMs for idea
generation and creative evaluations.

</details>


### [110] [Personalized Artificial General Intelligence (AGI) via Neuroscience-Inspired Continuous Learning Systems](https://arxiv.org/abs/2504.20109)
*Rajeev Gupta,Suhani Gupta,Ronak Parikh,Divya Gupta,Amir Javaheri,Jairaj Singh Shaktawat*

Main category: cs.AI

TL;DR: 本文提出了一种新型的个性化通用人工智能（AGI）架构，结合脑启发学习机制，旨在实现边缘设备上的持续学习和适应。


<details>
  <summary>Details</summary>
Motivation: 当前大型深度学习模型虽在任务特定性能上表现优异，但无法实现持续、适应性强且通用的学习，尤其是在资源受限的边缘设备上。

Method: 结合神经科学原理（如突触修剪、Hebbian可塑性、稀疏编码和双记忆系统），提出了一种包含快速-慢速学习模块、突触自优化和高效内存更新的架构。

Result: 提出了理论架构，解决了灾难性遗忘、内存效率和系统可扩展性等挑战，并展示了在移动AI助手和人形机器人等场景的应用潜力。

Conclusion: 该架构为未来实现真正持续、个性化的边缘AGI提供了路线图，尽管目前仍处于理论阶段。

Abstract: Artificial Intelligence has made remarkable advancements in recent years,
primarily driven by increasingly large deep learning models. However, achieving
true Artificial General Intelligence (AGI) demands fundamentally new
architectures rather than merely scaling up existing models. Current approaches
largely depend on expanding model parameters, which improves task-specific
performance but falls short in enabling continuous, adaptable, and generalized
learning. Achieving AGI capable of continuous learning and personalization on
resource-constrained edge devices is an even bigger challenge.
  This paper reviews the state of continual learning and neuroscience-inspired
AI, and proposes a novel architecture for Personalized AGI that integrates
brain-like learning mechanisms for edge deployment. We review literature on
continuous lifelong learning, catastrophic forgetting, and edge AI, and discuss
key neuroscience principles of human learning, including Synaptic Pruning,
Hebbian plasticity, Sparse Coding, and Dual Memory Systems, as inspirations for
AI systems. Building on these insights, we outline an AI architecture that
features complementary fast-and-slow learning modules, synaptic
self-optimization, and memory-efficient model updates to support on-device
lifelong adaptation.
  Conceptual diagrams of the proposed architecture and learning processes are
provided. We address challenges such as catastrophic forgetting, memory
efficiency, and system scalability, and present application scenarios for
mobile AI assistants and embodied AI systems like humanoid robots. We conclude
with key takeaways and future research directions toward truly continual,
personalized AGI on the edge. While the architecture is theoretical, it
synthesizes diverse findings and offers a roadmap for future implementation.

</details>


### [111] [Transforming Evidence Synthesis: A Systematic Review of the Evolution of Automated Meta-Analysis in the Age of AI](https://arxiv.org/abs/2504.20113)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.AI

TL;DR: 本文通过系统综述评估了自动化元分析（AMA）的现状，发现当前研究主要集中在数据处理阶段，而高级合成阶段和全流程自动化仍存在显著差距。尽管AI技术有所突破，但其在统计建模和高级合成中的应用仍不足。


<details>
  <summary>Details</summary>
Motivation: 科学文献的指数增长催生了高效证据合成的需求，推动了基于自然语言处理和机器学习的自动化元分析（AMA）领域的发展。本文旨在评估AMA的当前状态及其潜力。

Method: 采用PRISMA系统综述方法，筛选了2006至2024年的978篇论文，并分析了54项研究，涵盖医学和非医学领域。

Result: 研究发现，57%的研究聚焦于数据处理自动化，仅17%涉及高级合成阶段，仅2%探索全流程自动化。AI技术在统计建模和高级合成中的应用尚未成熟。

Conclusion: 未来需填补自动化在各阶段的空白，提升解释性和方法稳健性，以实现AMA在跨领域高效合成中的潜力。

Abstract: Exponential growth in scientific literature has heightened the demand for
efficient evidence-based synthesis, driving the rise of the field of Automated
Meta-analysis (AMA) powered by natural language processing and machine
learning. This PRISMA systematic review introduces a structured framework for
assessing the current state of AMA, based on screening 978 papers from 2006 to
2024, and analyzing 54 studies across diverse domains. Findings reveal a
predominant focus on automating data processing (57%), such as extraction and
statistical modeling, while only 17% address advanced synthesis stages. Just
one study (2%) explored preliminary full-process automation, highlighting a
critical gap that limits AMA's capacity for comprehensive synthesis. Despite
recent breakthroughs in large language models (LLMs) and advanced AI, their
integration into statistical modeling and higher-order synthesis, such as
heterogeneity assessment and bias evaluation, remains underdeveloped. This has
constrained AMA's potential for fully autonomous meta-analysis. From our
dataset spanning medical (67%) and non-medical (33%) applications, we found
that AMA has exhibited distinct implementation patterns and varying degrees of
effectiveness in actually improving efficiency, scalability, and
reproducibility. While automation has enhanced specific meta-analytic tasks,
achieving seamless, end-to-end automation remains an open challenge. As AI
systems advance in reasoning and contextual understanding, addressing these
gaps is now imperative. Future efforts must focus on bridging automation across
all meta-analysis stages, refining interpretability, and ensuring
methodological robustness to fully realize AMA's potential for scalable,
domain-agnostic synthesis.

</details>


### [112] [Deep Physics Prior for First Order Inverse Optimization](https://arxiv.org/abs/2504.20278)
*Haoyu Yang,Kamyar Azizzadenesheli,Haoxing Ren*

Main category: cs.AI

TL;DR: 论文提出了一种名为Deep Physics Prior（DPP）的新方法，通过预训练的辅助神经算子实现基于梯度的逆优化，解决了传统方法计算成本高和优化效果不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 逆设计优化在多个领域面临挑战，传统方法如生成式AI和贝叶斯优化存在计算成本高、对先验敏感等问题。

Method: DPP利用预训练的辅助神经算子，通过施加先验分布约束实现基于梯度的逆优化。

Result: DPP在未知先验数据和观测分布的情况下，仍能提供稳健且有意义的解决方案。

Conclusion: DPP为逆设计优化提供了一种高效且稳健的新方法。

Abstract: Inverse design optimization aims to infer system parameters from observed
solutions, posing critical challenges across domains such as semiconductor
manufacturing, structural engineering, materials science, and fluid dynamics.
The lack of explicit mathematical representations in many systems complicates
this process and makes the first order optimization impossible. Mainstream
approaches, including generative AI and Bayesian optimization, address these
challenges but have limitations. Generative AI is computationally expensive,
while Bayesian optimization, relying on surrogate models, suffers from
scalability, sensitivity to priors, and noise issues, often leading to
suboptimal solutions. This paper introduces Deep Physics Prior (DPP), a novel
method enabling first-order gradient-based inverse optimization with surrogate
machine learning models. By leveraging pretrained auxiliary Neural Operators,
DPP enforces prior distribution constraints to ensure robust and meaningful
solutions. This approach is particularly effective when prior data and
observation distributions are unknown.

</details>


### [113] [mrCAD: Multimodal Refinement of Computer-aided Designs](https://arxiv.org/abs/2504.20294)
*William P. McCarthy,Saujas Vaduguru,Karl D. D. Willis,Justin Matejka,Judith E. Fan,Daniel Fried,Yewen Pu*

Main category: cs.AI

TL;DR: 论文介绍了mrCAD数据集，用于研究人类如何通过多模态指令（文本和绘图）迭代改进设计，并发现生成式AI在遵循生成指令上优于改进指令。


<details>
  <summary>Details</summary>
Motivation: 人类协作中迭代改进概念的能力是核心，而生成式AI在内容生成上表现优异，但在语言引导的改进上表现不足。

Method: 通过mrCAD数据集，收集了6,082个多模态指令游戏，分析了生成和改进指令在文本和绘图上的差异。

Result: 生成式AI在生成指令上表现更好，而在改进指令上表现较差。

Conclusion: mrCAD为分析和建模多模态改进语言提供了基础，填补了现有数据集的空白。

Abstract: A key feature of human collaboration is the ability to iteratively refine the
concepts we have communicated. In contrast, while generative AI excels at the
\textit{generation} of content, it often struggles to make specific
language-guided \textit{modifications} of its prior outputs. To bridge the gap
between how humans and machines perform edits, we present mrCAD, a dataset of
multimodal instructions in a communication game. In each game, players created
computer aided designs (CADs) and refined them over several rounds to match
specific target designs. Only one player, the Designer, could see the target,
and they must instruct the other player, the Maker, using text, drawing, or a
combination of modalities. mrCAD consists of 6,082 communication games, 15,163
instruction-execution rounds, played between 1,092 pairs of human players. We
analyze the dataset and find that generation and refinement instructions differ
in their composition of drawing and text. Using the mrCAD task as a benchmark,
we find that state-of-the-art VLMs are better at following generation
instructions than refinement instructions. These results lay a foundation for
analyzing and modeling a multimodal language of refinement that is not
represented in previous datasets.

</details>


### [114] [Leveraging Action Relational Structures for Integrated Learning and Planning](https://arxiv.org/abs/2504.20318)
*Ryan Xiao Wang,Felipe Trevizan*

Main category: cs.AI

TL;DR: 论文提出了一种名为部分空间搜索的新方法，结合学习系统改进规划任务，并开发了LazyLifted规划器，在IPC 2023基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统规划方法忽视了PDDL动作模式的关系结构，而学习系统与搜索算法的结合研究较少。

Method: 引入部分空间搜索和动作集启发式，利用PDDL动作模式的关系结构，并训练新的启发式。

Result: LazyLifted规划器在IPC 2023学习赛道和高分支因子任务中优于现有方法。

Conclusion: 部分空间搜索和动作集启发式的结合显著提升了规划任务的性能。

Abstract: Recent advances in planning have explored using learning methods to help
planning. However, little attention has been given to adapting search
algorithms to work better with learning systems. In this paper, we introduce
partial-space search, a new search space for classical planning that leverages
the relational structure of actions given by PDDL action schemas -- a structure
overlooked by traditional planning approaches. Partial-space search provides a
more granular view of the search space and allows earlier pruning of poor
actions compared to state-space search. To guide partial-space search, we
introduce action set heuristics that evaluate sets of actions in a state. We
describe how to automatically convert existing heuristics into action set
heuristics. We also train action set heuristics from scratch using large
training datasets from partial-space search. Our new planner, LazyLifted,
exploits our better integrated search and learning heuristics and outperforms
the state-of-the-art ML-based heuristic on IPC 2023 learning track (LT)
benchmarks. We also show the efficiency of LazyLifted on high-branching factor
tasks and show that it surpasses LAMA in the combined IPC 2023 LT and
high-branching factor benchmarks.

</details>


### [115] [A Picture is Worth a Thousand Prompts? Efficacy of Iterative Human-Driven Prompt Refinement in Image Regeneration Tasks](https://arxiv.org/abs/2504.20340)
*Khoi Trinh,Scott Seidenberger,Raveen Wijewickrama,Murtuza Jadliwala,Anindya Maiti*

Main category: cs.AI

TL;DR: 研究探讨了AI图像再生中通过迭代提示优化实现目标图像重现的效果，并验证了图像相似性度量与人类感知的一致性。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容普及，研究如何通过迭代提示优化重现特定图像，并验证现有图像相似性度量的可靠性。

Method: 通过结构化用户研究，评估迭代提示优化对图像相似性的影响，并比较人类感知与图像相似性度量的一致性。

Result: 迭代提示调整显著提高了图像对齐效果，主观评估和定量测量均验证了这一点。

Conclusion: 迭代工作流程在生成AI内容中具有广泛潜力，图像相似性度量可作为有效的反馈机制。

Abstract: With AI-generated content becoming ubiquitous across the web, social media,
and other digital platforms, it is vital to examine how such content are
inspired and generated. The creation of AI-generated images often involves
refining the input prompt iteratively to achieve desired visual outcomes. This
study focuses on the relatively underexplored concept of image regeneration
using AI, in which a human operator attempts to closely recreate a specific
target image by iteratively refining their prompt. Image regeneration is
distinct from normal image generation, which lacks any predefined visual
reference. A separate challenge lies in determining whether existing image
similarity metrics (ISMs) can provide reliable, objective feedback in iterative
workflows, given that we do not fully understand if subjective human judgments
of similarity align with these metrics. Consequently, we must first validate
their alignment with human perception before assessing their potential as a
feedback mechanism in the iterative prompt refinement process. To address these
research gaps, we present a structured user study evaluating how iterative
prompt refinement affects the similarity of regenerated images relative to
their targets, while also examining whether ISMs capture the same improvements
perceived by human observers. Our findings suggest that incremental prompt
adjustments substantially improve alignment, verified through both subjective
evaluations and quantitative measures, underscoring the broader potential of
iterative workflows to enhance generative AI content creation across various
application domains.

</details>


### [116] [Skill Discovery for Software Scripting Automation via Offline Simulations with LLMs](https://arxiv.org/abs/2504.20406)
*Paiheng Xu,Gang Wu,Xiang Chen,Tong Yu,Chang Xiao,Franck Dernoncourt,Tianyi Zhou,Wei Ai,Viswanathan Swaminathan*

Main category: cs.AI

TL;DR: 提出一种离线模拟框架，利用LLMs和公开脚本指南生成已验证脚本的技能集，显著提高自动化成功率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 传统脚本创建需要编程知识，LLMs生成的代码存在安全风险和性能问题，需改进。

Method: 框架包括任务创建（功能引导和API协同探索）和技能生成（基于执行反馈验证脚本），并引入GNN模型捕捉API协同。

Result: 在Adobe Illustrator实验中，框架显著提高成功率、减少响应时间并节省成本。

Conclusion: 首次将软件脚本接口作为LLM系统测试平台，展示了在受控环境中利用执行反馈的优势。

Abstract: Scripting interfaces enable users to automate tasks and customize software
workflows, but creating scripts traditionally requires programming expertise
and familiarity with specific APIs, posing barriers for many users. While Large
Language Models (LLMs) can generate code from natural language queries, runtime
code generation is severely limited due to unverified code, security risks,
longer response times, and higher computational costs. To bridge the gap, we
propose an offline simulation framework to curate a software-specific skillset,
a collection of verified scripts, by exploiting LLMs and publicly available
scripting guides. Our framework comprises two components: (1) task creation,
using top-down functionality guidance and bottom-up API synergy exploration to
generate helpful tasks; and (2) skill generation with trials, refining and
validating scripts based on execution feedback. To efficiently navigate the
extensive API landscape, we introduce a Graph Neural Network (GNN)-based link
prediction model to capture API synergy, enabling the generation of skills
involving underutilized APIs and expanding the skillset's diversity.
Experiments with Adobe Illustrator demonstrate that our framework significantly
improves automation success rates, reduces response time, and saves runtime
token costs compared to traditional runtime code generation. This is the first
attempt to use software scripting interfaces as a testbed for LLM-based
systems, highlighting the advantages of leveraging execution feedback in a
controlled environment and offering valuable insights into aligning AI
capabilities with user needs in specialized software domains.

</details>


### [117] [RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library](https://arxiv.org/abs/2504.20426)
*Jiapeng Wang,Jinhao Jiang,Zhiqiang Zhang,Jun Zhou,Wayne Xin Zhao*

Main category: cs.AI

TL;DR: RV-Syn提出了一种新的数学数据合成方法，通过构建结构化数学操作函数库和计算图，生成可验证的高质量推理数据。


<details>
  <summary>Details</summary>
Motivation: 现有数据合成方法在问题内部逻辑掌握和解决方案可验证性方面存在不足，需要改进。

Method: RV-Syn基于初始种子问题构建数学操作函数库，生成计算图作为解决方案，并将其反向翻译为复杂问题。

Result: 实验表明，RV-Syn在数据扩展效率上优于现有方法，包括人工生成问题。

Conclusion: RV-Syn为生成高质量推理数据集提供了可扩展的框架。

Abstract: The advancement of reasoning capabilities in Large Language Models (LLMs)
requires substantial amounts of high-quality reasoning data, particularly in
mathematics. Existing data synthesis methods, such as data augmentation from
annotated training sets or direct question generation based on relevant
knowledge points and documents, have expanded datasets but face challenges in
mastering the inner logic of the problem during generation and ensuring the
verifiability of the solutions. To address these issues, we propose RV-Syn, a
novel Rational and Verifiable mathematical Synthesis approach. RV-Syn
constructs a structured mathematical operation function library based on
initial seed problems and generates computational graphs as solutions by
combining Python-formatted functions from this library. These graphs are then
back-translated into complex problems. Based on the constructed computation
graph, we achieve solution-guided logic-aware problem generation. Furthermore,
the executability of the computational graph ensures the verifiability of the
solving process. Experimental results show that RV-Syn surpasses existing
synthesis methods, including those involving human-generated problems,
achieving greater efficient data scaling. This approach provides a scalable
framework for generating high-quality reasoning datasets.

</details>


### [118] [Head-Tail-Aware KL Divergence in Knowledge Distillation for Spiking Neural Networks](https://arxiv.org/abs/2504.20445)
*Tianqing Zhang,Zixin Zhu,Kairong Yu,Hongwei Wang*

Main category: cs.AI

TL;DR: 提出了一种名为HTA-KL的新型知识蒸馏方法，用于提升脉冲神经网络（SNN）的性能，通过动态区分高概率和低概率区域，实现更平衡的知识迁移。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法未能充分利用SNN的特性，导致性能差距，需要一种更适应SNN的方法。

Method: 提出HTA-KL散度，结合累积概率掩码和自适应权重，整合正向和反向KL散度。

Result: 在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上表现优于现有方法，且时间步更少。

Conclusion: HTA-KL方法有效提升了SNN的性能，填补了与ANN的性能差距。

Abstract: Spiking Neural Networks (SNNs) have emerged as a promising approach for
energy-efficient and biologically plausible computation. However, due to
limitations in existing training methods and inherent model constraints, SNNs
often exhibit a performance gap when compared to Artificial Neural Networks
(ANNs). Knowledge distillation (KD) has been explored as a technique to
transfer knowledge from ANN teacher models to SNN student models to mitigate
this gap. Traditional KD methods typically use Kullback-Leibler (KL) divergence
to align output distributions. However, conventional KL-based approaches fail
to fully exploit the unique characteristics of SNNs, as they tend to
overemphasize high-probability predictions while neglecting low-probability
ones, leading to suboptimal generalization. To address this, we propose
Head-Tail Aware Kullback-Leibler (HTA-KL) divergence, a novel KD method for
SNNs. HTA-KL introduces a cumulative probability-based mask to dynamically
distinguish between high- and low-probability regions. It assigns adaptive
weights to ensure balanced knowledge transfer, enhancing the overall
performance. By integrating forward KL (FKL) and reverse KL (RKL) divergence,
our method effectively align both head and tail regions of the distribution. We
evaluate our methods on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets. Our
method outperforms existing methods on most datasets with fewer timesteps.

</details>


### [119] [TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data](https://arxiv.org/abs/2504.20462)
*Qi Wang,Xiao Zhang,Mingyi Li,Yuan Yuan,Mengbai Xiao,Fuzhen Zhuang,Dongxiao Yu*

Main category: cs.AI

TL;DR: 论文提出了一种名为TAMO的工具辅助LLM代理，用于解决微服务和云原生技术中的故障根因分析问题，通过多模态数据和时间对齐表示克服了现有LLM方法的限制。


<details>
  <summary>Details</summary>
Motivation: 现代企业软件开发中，微服务和云原生技术增加了系统复杂性和运维挑战，传统根因分析依赖人工干预，而现有LLM方法存在文本输入限制、动态服务依赖幻觉和上下文窗口限制等问题。

Method: TAMO通过统一多模态观测数据为时间对齐表示，提取一致特征，并利用专用工具进行根因定位和故障分类，同时结构化关键信息以指导LLM生成修复策略。

Result: 实验结果表明，TAMO在处理异构性和常见故障类型的公共数据集时表现良好，验证了其有效性。

Conclusion: TAMO通过结合多模态数据和工具辅助，显著提升了LLM在根因分析中的能力，为AIOps提供了新解决方案。

Abstract: With the development of distributed systems, microservices and cloud native
technologies have become central to modern enterprise software development.
Despite bringing significant advantages, these technologies also increase
system complexity and operational challenges. Traditional root cause analysis
(RCA) struggles to achieve automated fault response, heavily relying on manual
intervention. In recent years, large language models (LLMs) have made
breakthroughs in contextual inference and domain knowledge integration,
providing new solutions for Artificial Intelligence for Operations (AIOps).
However, Existing LLM-based approaches face three key challenges: text input
constraints, dynamic service dependency hallucinations, and context window
limitations. To address these issues, we propose a tool-assisted LLM agent with
multi-modality observation data, namely TAMO, for fine-grained RCA. It unifies
multi-modal observational data into time-aligned representations to extract
consistent features and employs specialized root cause localization and fault
classification tools for perceiving the contextual environment. This approach
overcomes the limitations of LLM in handling real-time changing service
dependencies and raw observational data and guides LLM to generate repair
strategies aligned with system contexts by structuring key information into a
prompt. Experimental results show that TAMO performs well in root cause
analysis when dealing with public datasets characterized by heterogeneity and
common fault types, demonstrating its effectiveness.

</details>


### [120] [A Summary on GUI Agents with Foundation Models Enhanced by Reinforcement Learning](https://arxiv.org/abs/2504.20464)
*Jiahao Li,Kaer Huang*

Main category: cs.AI

TL;DR: 本文总结了基于多模态大语言模型（MLLM）的GUI代理的最新进展，重点介绍了强化学习（RL）增强的架构。


<details>
  <summary>Details</summary>
Motivation: GUI代理作为智能交互数字系统的有前景范式，需要系统化总结其架构和训练方法。

Method: 将GUI代理任务形式化为马尔可夫决策过程，并回顾了基于（M）LLM的模块化架构（感知、规划、执行）及其训练方法（提示、监督微调、强化学习）。

Result: 多模态感知、决策推理和自适应动作生成的创新显著提升了GUI代理在复杂环境中的泛化能力和鲁棒性。

Conclusion: 提出了构建更强大可靠GUI代理的关键挑战和未来方向。

Abstract: Graphical User Interface (GUI) agents, driven by Multi-modal Large Language
Models (MLLMs), have emerged as a promising paradigm for enabling intelligent
interaction with digital systems. This paper provides a structured summary of
recent advances in GUI agents, focusing on architectures enhanced by
Reinforcement Learning (RL). We first formalize GUI agent tasks as Markov
Decision Processes and discuss typical execution environments and evaluation
metrics. We then review the modular architecture of (M)LLM-based GUI agents,
covering Perception, Planning, and Acting modules, and trace their evolution
through representative works. Furthermore, we categorize GUI agent training
methodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and
RL-based approaches, highlighting the progression from simple prompt
engineering to dynamic policy learning via RL. Our summary illustrates how
recent innovations in multimodal perception, decision reasoning, and adaptive
action generation have significantly improved the generalization and robustness
of GUI agents in complex real-world environments. We conclude by identifying
key challenges and future directions for building more capable and reliable GUI
agents.

</details>


### [121] [MuRAL: A Multi-Resident Ambient Sensor Dataset Annotated with Natural Language for Activities of Daily Living](https://arxiv.org/abs/2504.20505)
*Xi Chen,Julien Cumin,Fano Ramparany,Dominique Vaufreydaz*

Main category: cs.AI

TL;DR: 论文介绍了MuRAL数据集，专为支持大语言模型（LLM）在多用户智能家居环境中的活动识别研究而设计，填补了现有数据集的不足。


<details>
  <summary>Details</summary>
Motivation: 现有数据集（如CASAS、ARAS、MARBLE）未考虑LLM需求，缺乏上下文丰富性和注释粒度，限制了LLM在人类活动识别（HAR）中的潜力。

Method: 提出MuRAL数据集，包含21小时多用户传感器数据，附带细粒度自然语言描述、用户身份和高层活动标签。

Result: 实验表明，LLM能提供丰富的语义解释，但在处理多用户模糊性和传感器上下文不足时仍有挑战。

Conclusion: MuRAL支持未来LLM驱动的、可解释且社会感知的活动理解研究，数据集将公开提供。

Abstract: Recent advances in Large Language Models (LLMs) have shown promising
potential for human activity recognition (HAR) using ambient sensors,
especially through natural language reasoning and zero-shot learning. However,
existing datasets such as CASAS, ARAS, and MARBLE were not originally designed
with LLMs in mind and therefore lack the contextual richness, complexity, and
annotation granularity required to fully exploit LLM capabilities. In this
paper, we introduce MuRAL, the first Multi-Resident Ambient sensor dataset with
natural Language, comprising over 21 hours of multi-user sensor data collected
from 21 sessions in a smart-home environment. MuRAL is annotated with
fine-grained natural language descriptions, resident identities, and high-level
activity labels, all situated in dynamic, realistic multi-resident settings. We
benchmark MuRAL using state-of-the-art LLMs for three core tasks: subject
assignment, action description, and activity classification. Our results
demonstrate that while LLMs can provide rich semantic interpretations of
ambient data, current models still face challenges in handling multi-user
ambiguity and under-specified sensor contexts. We release MuRAL to support
future research on LLM-powered, explainable, and socially aware activity
understanding in smart environments. For access to the dataset, please reach
out to us via the provided contact information. A direct link for dataset
retrieval will be made available at this location in due course.

</details>


### [122] [ReasonIR: Training Retrievers for Reasoning Tasks](https://arxiv.org/abs/2504.20595)
*Rulin Shao,Rui Qiao,Varsha Kishore,Niklas Muennighoff,Xi Victoria Lin,Daniela Rus,Bryan Kian Hsiang Low,Sewon Min,Wen-tau Yih,Pang Wei Koh,Luke Zettlemoyer*

Main category: cs.AI

TL;DR: ReasonIR-8B是首个专为通用推理任务训练的检索器，通过合成数据生成和混合训练，在推理密集型IR任务中取得新SOTA，并在RAG任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索器在推理任务中表现有限，因训练数据多为简短事实性查询。需开发更具挑战性的数据以提升推理能力。

Method: 开发合成数据生成流程，为每篇文档生成挑战性查询和硬负样本，结合公开数据训练。

Result: 在BRIGHT基准上达到29.9 nDCG@10（无重排）和36.9 nDCG@10（有重排），MMLU和GPQA性能分别提升6.4%和22.6%。

Conclusion: ReasonIR-8B在推理任务中表现优异，训练方法通用且开源，可扩展至未来LLMs。

Abstract: We present ReasonIR-8B, the first retriever specifically trained for general
reasoning tasks. Existing retrievers have shown limited gains on reasoning
tasks, in part because existing training datasets focus on short factual
queries tied to documents that straightforwardly answer them. We develop a
synthetic data generation pipeline that, for each document, our pipeline
creates a challenging and relevant query, along with a plausibly related but
ultimately unhelpful hard negative. By training on a mixture of our synthetic
data and existing public data, ReasonIR-8B achieves a new state-of-the-art of
29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a
widely-used reasoning-intensive information retrieval (IR) benchmark. When
applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%
and 22.6% respectively, relative to the closed-book baseline, outperforming
other retrievers and search engines. In addition, ReasonIR-8B uses test-time
compute more effectively: on BRIGHT, its performance consistently increases
with longer and more information-rich rewritten queries; it continues to
outperform other retrievers when combined with an LLM reranker. Our training
recipe is general and can be easily extended to future LLMs; to this end, we
open-source our code, data, and model.

</details>


### [123] [PaRT: Enhancing Proactive Social Chatbots with Personalized Real-Time Retrieval](https://arxiv.org/abs/2504.20624)
*Zihan Niu,Zheyong Xie,Shaosheng Cao,Chonggang Lu,Zheyu Ye,Tong Xu,Zuozhu Liu,Yan Gao,Jia Chen,Zhe Xu,Yi Wu,Yao Hu*

Main category: cs.AI

TL;DR: PaRT框架通过个性化实时检索与生成，提升社交聊天机器人的主动对话能力，显著延长对话时长。


<details>
  <summary>Details</summary>
Motivation: 传统聊天机器人依赖用户发起对话，导致互动减少和对话时长缩短，需改进以提升用户体验。

Method: 结合用户画像与对话上下文，利用LLM生成个性化话题，检索RedNote相关内容，生成知识丰富的响应。

Result: 在生产环境中稳定运行30天，对话平均时长提升21.77%。

Conclusion: PaRT框架有效增强社交聊天机器人的主动性与互动性，显著提升用户参与度。

Abstract: Social chatbots have become essential intelligent companions in daily
scenarios ranging from emotional support to personal interaction. However,
conventional chatbots with passive response mechanisms usually rely on users to
initiate or sustain dialogues by bringing up new topics, resulting in
diminished engagement and shortened dialogue duration. In this paper, we
present PaRT, a novel framework enabling context-aware proactive dialogues for
social chatbots through personalized real-time retrieval and generation.
Specifically, PaRT first integrates user profiles and dialogue context into a
large language model (LLM), which is initially prompted to refine user queries
and recognize their underlying intents for the upcoming conversation. Guided by
refined intents, the LLM generates personalized dialogue topics, which then
serve as targeted queries to retrieve relevant passages from RedNote. Finally,
we prompt LLMs with summarized passages to generate knowledge-grounded and
engagement-optimized responses. Our approach has been running stably in a
real-world production environment for more than 30 days, achieving a 21.77\%
improvement in the average duration of dialogues.

</details>


### [124] [Cognitive maps are generative programs](https://arxiv.org/abs/2504.20628)
*Marta Kryven,Cole Wyeth,Aidan Curtis,Kevin Ellis*

Main category: cs.AI

TL;DR: 论文探讨了人类高效规划可能源于将世界视为可预测结构的假设，提出认知地图可表现为生成程序，并通过实验和计算模型验证了这一点。


<details>
  <summary>Details</summary>
Motivation: 研究人类如何在有限资源下构建功能性世界表征，探索其规划策略是否基于程序化认知地图。

Method: 结合行为实验和计算模型，利用大型语言模型嵌入人类先验知识，验证程序化认知地图的假设。

Result: 模型在计算效率、内存需求和预测人类行为方面优于非结构化规划算法。

Conclusion: 人类规划策略依赖于程序化认知地图，这种表征方式更高效且符合认知约束。

Abstract: Making sense of the world and acting in it relies on building simplified
mental representations that abstract away aspects of reality. This principle of
cognitive mapping is universal to agents with limited resources. Living
organisms, people, and algorithms all face the problem of forming functional
representations of their world under various computing constraints. In this
work, we explore the hypothesis that human resource-efficient planning may
arise from representing the world as predictably structured. Building on the
metaphor of concepts as programs, we propose that cognitive maps can take the
form of generative programs that exploit predictability and redundancy, in
contrast to directly encoding spatial layouts. We use a behavioral experiment
to show that people who navigate in structured spaces rely on modular planning
strategies that align with programmatic map representations. We describe a
computational model that predicts human behavior in a variety of structured
scenarios. This model infers a small distribution over possible programmatic
cognitive maps conditioned on human prior knowledge of the world, and uses this
distribution to generate resource-efficient plans. Our models leverages a Large
Language Model as an embedding of human priors, implicitly learned through
training on a vast corpus of human data. Our model demonstrates improved
computational efficiency, requires drastically less memory, and outperforms
unstructured planning algorithms with cognitive constraints at predicting human
behavior, suggesting that human planning strategies rely on programmatic
cognitive maps.

</details>


### [125] [The Limits of AI Explainability: An Algorithmic Information Theory Approach](https://arxiv.org/abs/2504.20676)
*Shrisha Rao*

Main category: cs.AI

TL;DR: 本文通过算法信息理论为AI可解释性的基本限制建立了理论基础，量化了近似误差和解释复杂性，并提出了复杂性差距定理、精确边界以及局部与全局可解释性差距的表征。


<details>
  <summary>Details</summary>
Motivation: 研究AI可解释性的理论限制，为设计和评估可解释AI系统提供理论支持。

Method: 使用Kolmogorov复杂性量化解释复杂性，提出复杂性差距定理和精确边界，分析局部与全局解释的差异。

Result: 证明解释复杂性随输入维度指数增长，但对Lipschitz函数误差容忍度多项式增长；局部解释在相关区域可保持准确性。

Conclusion: 研究揭示了AI可解释性的基本限制，对AI系统的设计、评估和监管具有重要启示。

Abstract: This paper establishes a theoretical foundation for understanding the
fundamental limits of AI explainability through algorithmic information theory.
We formalize explainability as the approximation of complex models by simpler
ones, quantifying both approximation error and explanation complexity using
Kolmogorov complexity. Our key theoretical contributions include: (1) a
complexity gap theorem proving that any explanation significantly simpler than
the original model must differ from it on some inputs; (2) precise bounds
showing that explanation complexity grows exponentially with input dimension
but polynomially with error tolerance for Lipschitz functions; and (3) a
characterization of the gap between local and global explainability,
demonstrating that local explanations can be significantly simpler while
maintaining accuracy in relevant regions. We further establish a regulatory
impossibility theorem proving that no governance framework can simultaneously
pursue unrestricted AI capabilities, human-interpretable explanations, and
negligible error. These results highlight considerations likely to be relevant
to the design, evaluation, and oversight of explainable AI systems.

</details>


### [126] [Graph-Based Fault Diagnosis for Rotating Machinery: Adaptive Segmentation and Structural Feature Integration](https://arxiv.org/abs/2504.20756)
*Moirangthem Tiken Singh*

Main category: cs.AI

TL;DR: 提出了一种基于图的新型框架，用于旋转机械的多类故障诊断，具有高准确性和抗噪性。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在复杂工况下故障诊断的局限性，同时保持模型的解释性和低复杂度。

Method: 结合熵优化信号分割、时频特征提取和图论建模，利用图度量和局部特征进行分类。

Result: 在CWRU和SU数据集上分别达到99.8%和100%的准确率，抗噪性强，跨域性能优异。

Conclusion: 该方法无需深度学习，兼具高准确性和解释性，适合工业实时诊断。

Abstract: This paper proposes a novel graph-based framework for robust and
interpretable multiclass fault diagnosis in rotating machinery. The method
integrates entropy-optimized signal segmentation, time-frequency feature
extraction, and graph-theoretic modeling to transform vibration signals into
structured representations suitable for classification. Graph metrics, such as
average shortest path length, modularity, and spectral gap, are computed and
combined with local features to capture global and segment-level fault
characteristics. The proposed method achieves high diagnostic accuracy when
evaluated on two benchmark datasets, the CWRU bearing dataset (under 0-3 HP
loads) and the SU gearbox and bearing datasets (under different speed-load
configurations). Classification scores reach up to 99.8% accuracy on Case
Western Reserve University (CWRU) and 100% accuracy on the Southeast University
datasets using a logistic regression classifier. Furthermore, the model
exhibits strong noise resilience, maintaining over 95.4% accuracy at high noise
levels (standard deviation = 0.5), and demonstrates excellent cross-domain
transferability with up to 99.7% F1-score in load-transfer scenarios. Compared
to traditional techniques, this approach requires no deep learning
architecture, enabling lower complexity while ensuring interpretability. The
results confirm the method's scalability, reliability, and potential for
real-time deployment in industrial diagnostics.

</details>


### [127] [Approximate Lifted Model Construction](https://arxiv.org/abs/2504.20784)
*Malte Luttermann,Jan Speller,Marcel Gehrke,Tanya Braun,Ralf Möller,Mattis Hartwig*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Probabilistic relational models such as parametric factor graphs enable
efficient (lifted) inference by exploiting the indistinguishability of objects.
In lifted inference, a representative of indistinguishable objects is used for
computations. To obtain a relational (i.e., lifted) representation, the
Advanced Colour Passing (ACP) algorithm is the state of the art. The ACP
algorithm, however, requires underlying distributions, encoded as
potential-based factorisations, to exactly match to identify and exploit
indistinguishabilities. Hence, ACP is unsuitable for practical applications
where potentials learned from data inevitably deviate even if associated
objects are indistinguishable. To mitigate this problem, we introduce the
$\varepsilon$-Advanced Colour Passing ($\varepsilon$-ACP) algorithm, which
allows for a deviation of potentials depending on a hyperparameter
$\varepsilon$. $\varepsilon$-ACP efficiently uncovers and exploits
indistinguishabilities that are not exact. We prove that the approximation
error induced by $\varepsilon$-ACP is strictly bounded and our experiments show
that the approximation error is close to zero in practice.

</details>


### [128] [Partitioned Memory Storage Inspired Few-Shot Class-Incremental learning](https://arxiv.org/abs/2504.20797)
*Renye Zhang,Yimin Yin,Jinghua Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种Few-Shot Class-Incremental Learning (FSCIL)的新方法，通过为每个会话学习独立模型，避免灾难性遗忘，并结合不确定性量化提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习过度依赖大量训练数据且缺乏动态适应性，与人类智能差距较大。FSCIL旨在用有限样本持续学习新类别而不遗忘旧知识。

Method: 为每个会话学习独立模型，避免稳定性-可塑性困境，并在测试阶段结合不确定性量化。

Result: 在CIFAR-100和mini-ImageNet数据集上取得了最先进的性能。

Conclusion: 该方法为FSCIL提供了新视角，有效解决了灾难性遗忘问题。

Abstract: Current mainstream deep learning techniques exhibit an over-reliance on
extensive training data and a lack of adaptability to the dynamic world,
marking a considerable disparity from human intelligence. To bridge this gap,
Few-Shot Class-Incremental Learning (FSCIL) has emerged, focusing on continuous
learning of new categories with limited samples without forgetting old
knowledge. Existing FSCIL studies typically use a single model to learn
knowledge across all sessions, inevitably leading to the stability-plasticity
dilemma. Unlike machines, humans store varied knowledge in different cerebral
cortices. Inspired by this characteristic, our paper aims to develop a method
that learns independent models for each session. It can inherently prevent
catastrophic forgetting. During the testing stage, our method integrates
Uncertainty Quantification (UQ) for model deployment. Our method provides a
fresh viewpoint for FSCIL and demonstrates the state-of-the-art performance on
CIFAR-100 and mini-ImageNet datasets.

</details>


### [129] [Ascendra: Dynamic Request Prioritization for Efficient LLM Serving](https://arxiv.org/abs/2504.20828)
*Azam Ikram,Xiang Li,Sameh Elnikety,Saurabh Bagchi*

Main category: cs.AI

TL;DR: Ascendra是一种LLM服务系统，通过分区GPU资源为高低优先级实例，同时满足TTFT和TBT的SLO要求，提升吞吐量1.7倍。


<details>
  <summary>Details</summary>
Motivation: 现有系统往往牺牲一个指标（TTFT或TBT）来优化另一个，无法同时满足两者的SLO要求。

Method: Ascendra将GPU资源分为低优先级（高吞吐量）和高优先级（低延迟）实例，并通过性能模型预测并转移可能超时的请求。

Result: Ascendra在满足TTFT和TBT SLO的同时，吞吐量比vLLM和Sarathi-Serve提高了1.7倍。

Conclusion: Ascendra通过动态资源分区和优先级调度，有效平衡了吞吐量和延迟，为LLM服务提供了高效解决方案。

Abstract: The rapid advancement of Large Language Models (LLMs) has driven the need for
more efficient serving strategies. In this context, efficiency refers to the
proportion of requests that meet their Service Level Objectives (SLOs),
particularly for Time To First Token (TTFT) and Time Between Tokens (TBT).
However, existing systems often prioritize one metric at the cost of the other.
We present Ascendra, an LLM serving system designed to meet both TTFT and TBT
SLOs simultaneously. The core insight behind Ascendra is that a request's
urgency evolves as it approaches its deadline. To leverage this, Ascendra
partitions GPU resources into two types of instances: low-priority and
high-priority. Low-priority instances maximize throughput by processing
requests out of arrival order, but at the risk of request starvation. To
address this, Ascendra employs a performance model to predict requests at risk
of missing their SLOs and proactively offloads them to high-priority instances.
High-priority instances are optimized for low-latency execution and handle
urgent requests nearing their deadlines. This partitioned architecture enables
Ascendra to effectively balance high throughput and low latency. Extensive
evaluation shows that Ascendra improves system throughput by up to 1.7x
compared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs.

</details>


### [130] [Disjunctive and Conjunctive Normal Form Explanations of Clusters Using Auxiliary Information](https://arxiv.org/abs/2504.20846)
*Robert F. Downey,S. S. Ravi*

Main category: cs.AI

TL;DR: 论文提出了一种利用未用于聚类的辅助信息（标签）生成聚类后解释的方法，包括析取形式和二子句合取范式（CNF）形式，并采用整数线性规划（ILP）和启发式方法生成解释。


<details>
  <summary>Details</summary>
Motivation: 旨在通过辅助标签为聚类结果提供可解释性，帮助理解聚类背后的逻辑。

Method: 使用整数线性规划（ILP）和启发式方法生成析取形式和二子句CNF形式的解释。

Result: 实验验证了方法的有效性，并展示了其在不同数据集上的可扩展性。

Conclusion: 提出的方法能有效生成聚类解释，为理解聚类结果提供了新视角。

Abstract: We consider generating post-hoc explanations of clusters generated from
various datasets using auxiliary information which was not used by clustering
algorithms. Following terminology used in previous work, we refer to the
auxiliary information as tags. Our focus is on two forms of explanations,
namely disjunctive form (where the explanation for a cluster consists of a set
of tags) and a two-clause conjunctive normal form (CNF) explanation (where the
explanation consists of two sets of tags, combined through the AND operator).
We use integer linear programming (ILP) as well as heuristic methods to
generate these explanations. We experiment with a variety of datasets and
discuss the insights obtained from our explanations. We also present
experimental results regarding the scalability of our explanation methods.

</details>


### [131] [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879)
*Shivalika Singh,Yiyang Nan,Alex Wang,Daniel D'Souza,Sayash Kapoor,Ahmet Üstün,Sanmi Koyejo,Yuntian Deng,Shayne Longpre,Noah Smith,Beyza Ermis,Marzieh Fadaee,Sara Hooker*

Main category: cs.AI

TL;DR: 论文指出Chatbot Arena存在系统性偏差，私有测试和选择性披露导致排行榜失真，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 衡量进展对科学领域至关重要，但基准测试易受扭曲。Chatbot Arena作为AI系统排名的主要平台，存在不公平现象。

Method: 通过分析私有测试、选择性披露和数据分配不对称性，揭示Arena的偏差。

Result: 发现私有模型受益于更多测试和数据，开放模型则处于劣势。数据访问不对称导致性能增益显著。

Conclusion: 建议改革Arena评估框架，促进更公平透明的基准测试。

Abstract: Measuring progress is fundamental to the advancement of any scientific field.
As benchmarks play an increasingly central role, they also grow more
susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard
for ranking the most capable AI systems. Yet, in this work we identify
systematic issues that have resulted in a distorted playing field. We find that
undisclosed private testing practices benefit a handful of providers who are
able to test multiple variants before public release and retract scores if
desired. We establish that the ability of these providers to choose the best
score leads to biased Arena scores due to selective disclosure of performance
results. At an extreme, we identify 27 private LLM variants tested by Meta in
the lead-up to the Llama-4 release. We also establish that proprietary closed
models are sampled at higher rates (number of battles) and have fewer models
removed from the arena than open-weight and open-source alternatives. Both
these policies lead to large data access asymmetries over time. Providers like
Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the
arena, respectively. In contrast, a combined 83 open-weight models have only
received an estimated 29.7% of the total data. We show that access to Chatbot
Arena data yields substantial benefits; even limited additional data can result
in relative performance gains of up to 112% on the arena distribution, based on
our conservative estimates. Together, these dynamics result in overfitting to
Arena-specific dynamics rather than general model quality. The Arena builds on
the substantial efforts of both the organizers and an open community that
maintains this valuable evaluation platform. We offer actionable
recommendations to reform the Chatbot Arena's evaluation framework and promote
fairer, more transparent benchmarking for the field

</details>


### [132] [CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report Generation with Multi-Agent RAG and Concept Bottleneck Models](https://arxiv.org/abs/2504.20898)
*Hasan Md Tusfiqur Alam,Devansh Srivastav,Abdulrahman Mohamed Selim,Md Abdul Kadir,Md Moktadiurl Hoque Shuvo,Daniel Sonntag*

Main category: cs.AI

TL;DR: 论文提出了一种结合概念瓶颈模型（CBM）和多智能体检索增强生成（RAG）的自动化放射学报告生成框架，旨在提升AI的可解释性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在放射学工作流程自动化中潜力巨大，但可解释性和可靠性问题阻碍了临床采用。

Method: 使用CBM将胸部X光特征映射到临床概念，结合多智能体RAG系统生成证据丰富的报告。

Result: 系统实现了可解释的预测、减少幻觉，并生成高质量、定制化的报告。

Conclusion: 该框架为提升诊断一致性和为放射科医生提供可操作见解提供了路径。

Abstract: Advancements in generative Artificial Intelligence (AI) hold great promise
for automating radiology workflows, yet challenges in interpretability and
reliability hinder clinical adoption. This paper presents an automated
radiology report generation framework that combines Concept Bottleneck Models
(CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge
AI performance with clinical explainability. CBMs map chest X-ray features to
human-understandable clinical concepts, enabling transparent disease
classification. Meanwhile, the RAG system integrates multi-agent collaboration
and external knowledge to produce contextually rich, evidence-based reports.
Our demonstration showcases the system's ability to deliver interpretable
predictions, mitigate hallucinations, and generate high-quality, tailored
reports with an interactive interface addressing accuracy, trust, and usability
challenges. This framework provides a pathway to improving diagnostic
consistency and empowering radiologists with actionable insights.

</details>


### [133] [Leveraging Generative AI Through Prompt Engineering and Rigorous Validation to Create Comprehensive Synthetic Datasets for AI Training in Healthcare](https://arxiv.org/abs/2504.20921)
*Polycarp Nalela*

Main category: cs.AI

TL;DR: 利用GPT-4 API生成高质量合成医疗数据，解决隐私问题，并通过多种验证技术确保数据质量。


<details>
  <summary>Details</summary>
Motivation: 医疗数据因隐私问题难以获取，限制了AI算法的训练。

Method: 使用GPT-4 API生成合成数据，结合BERT、GPT-2、RoBERTa等模型验证数据质量。

Result: 成功生成并通过验证的合成数据被整合到PostgreSQL数据库中。

Conclusion: 生成式AI模型结合严格验证可有效解决医疗数据隐私问题。

Abstract: Access to high-quality medical data is often restricted due to privacy
concerns, posing significant challenges for training artificial intelligence
(AI) algorithms within Electronic Health Record (EHR) applications. In this
study, prompt engineering with the GPT-4 API was employed to generate
high-quality synthetic datasets aimed at overcoming this limitation. The
generated data encompassed a comprehensive array of patient admission
information, including healthcare provider details, hospital departments,
wards, bed assignments, patient demographics, emergency contacts, vital signs,
immunizations, allergies, medical histories, appointments, hospital visits,
laboratory tests, diagnoses, treatment plans, medications, clinical notes,
visit logs, discharge summaries, and referrals. To ensure data quality and
integrity, advanced validation techniques were implemented utilizing models
such as BERT's Next Sentence Prediction for sentence coherence, GPT-2 for
overall plausibility, RoBERTa for logical consistency, autoencoders for anomaly
detection, and conducted diversity analysis. Synthetic data that met all
validation criteria were integrated into a comprehensive PostgreSQL database,
serving as the data management system for the EHR application. This approach
demonstrates that leveraging generative AI models with rigorous validation can
effectively produce high-quality synthetic medical data, facilitating the
training of AI algorithms while addressing privacy concerns associated with
real patient data.

</details>


### [134] [A Domain-Agnostic Scalable AI Safety Ensuring Framework](https://arxiv.org/abs/2504.20924)
*Beomjun Kim,Kangyeon Kim,Sunwoo Kim,Heejin Ahn*

Main category: cs.AI

TL;DR: 提出了一种新型AI安全框架，确保AI系统满足用户定义的约束条件，概率可控且跨领域适用。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全方法局限于特定领域，无法泛化，因此需要一种更通用的安全框架。

Method: 结合AI组件与优化问题，通过内部测试数据和保守测试方法确保安全性，并提出损失函数近似方法。

Result: 数学证明概率约束满足条件，实验证明在多个领域优于现有方法，安全性提升显著。

Conclusion: 该框架有效保障AI系统安全，泛化能力强，适用于多种应用场景。

Abstract: Ensuring the safety of AI systems has recently emerged as a critical priority
for real-world deployment, particularly in physical AI applications. Current
approaches to AI safety typically address predefined domain-specific safety
conditions, limiting their ability to generalize across contexts.
  We propose a novel AI safety framework that ensures AI systems comply with
\textbf{any user-defined constraint}, with \textbf{any desired probability},
and across \textbf{various domains}.
  In this framework, we combine an AI component (e.g., neural network) with an
optimization problem to produce responses that minimize objectives while
satisfying user-defined constraints with probabilities exceeding user-defined
thresholds. For credibility assessment of the AI component, we propose
\textit{internal test data}, a supplementary set of safety-labeled data, and a
\textit{conservative testing} methodology that provides statistical validity of
using internal test data. We also present an approximation method of a loss
function and how to compute its gradient for training.
  We mathematically prove that probabilistic constraint satisfaction is
guaranteed under specific, mild conditions and prove a scaling law between
safety and the number of internal test data. We demonstrate our framework's
effectiveness through experiments in diverse domains: demand prediction for
production decision, safe reinforcement learning within the SafetyGym
simulator, and guarding AI chatbot outputs. Through these experiments, we
demonstrate that our method guarantees safety for user-specified constraints,
outperforms {for \textbf{up to several order of magnitudes}} existing methods
in low safety threshold regions, and scales effectively with respect to the
size of internal test data.

</details>


### [135] [ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification](https://arxiv.org/abs/2504.20930)
*Ziqing Fan,Cheng Liang,Chaoyi Wu,Ya Zhang,Yanfeng Wang,Weidi Xie*

Main category: cs.AI

TL;DR: ChestX-Reasoner是一种放射学诊断MLLM，通过从临床报告中提取结构化推理链，结合监督微调和强化学习，显著提升了诊断准确性和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有医学AI模型常忽略临床实践中的结构化推理过程，ChestX-Reasoner旨在填补这一空白。

Method: 构建大规模数据集，提取临床报告中的推理链；采用两阶段训练框架（监督微调+强化学习）；提出RadRBench-CXR基准和RadRScore评估指标。

Result: ChestX-Reasoner在诊断准确性和推理能力上均优于现有模型，推理能力提升16%、5.9%和18%，诊断准确性提升3.3%、24%和27%。

Conclusion: ChestX-Reasoner通过结合临床推理和强化学习，显著提升了医学MLLM的性能，所有资源已开源以促进进一步研究。

Abstract: Recent advances in reasoning-enhanced large language models (LLMs) and
multimodal LLMs (MLLMs) have significantly improved performance in complex
tasks, yet medical AI models often overlook the structured reasoning processes
inherent in clinical practice. In this work, we present ChestX-Reasoner, a
radiology diagnosis MLLM designed to leverage process supervision mined
directly from clinical reports, reflecting the step-by-step reasoning followed
by radiologists. We construct a large dataset by extracting and refining
reasoning chains from routine radiology reports. Our two-stage training
framework combines supervised fine-tuning and reinforcement learning guided by
process rewards to better align model reasoning with clinical standards. We
introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual
question answering samples with 301K clinically validated reasoning steps, and
propose RadRScore, a metric evaluating reasoning factuality, completeness, and
effectiveness. ChestX-Reasoner outperforms existing medical and general-domain
MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,
and 18% improvements in reasoning ability compared to the best medical MLLM,
the best general MLLM, and its base model, respectively, as well as 3.3%, 24%,
and 27% improvements in outcome accuracy. All resources are open-sourced to
facilitate further research in medical reasoning MLLMs.

</details>


### [136] [Jekyll-and-Hyde Tipping Point in an AI's Behavior](https://arxiv.org/abs/2504.20980)
*Neil F. Johnson,Frank Yingjie Huo*

Main category: cs.AI

TL;DR: 论文提出了一个基于基本原理的精确公式，用于预测LLM（如ChatGPT）输出何时会突然变得错误、误导、无关或危险，并探讨了如何通过调整提示和训练来延迟或防止这种转变。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏科学方法预测或解释LLM输出何时会突然变得不可靠，导致公众对AI的信任度下降，甚至引发对LLM的过度谨慎行为（如礼貌对待）。

Method: 从基本原理出发，推导出一个精确的数学公式，揭示了LLM注意力分散导致突然转变的机制，仅需中学数学基础即可理解。

Result: 该公式能够定量预测如何通过改变提示和训练来延迟或防止LLM输出的突然转变，为政策制定者和公众提供了讨论AI风险和应用的依据。

Conclusion: 研究为AI的透明性和可解释性提供了科学支持，解决了公众对LLM行为的疑问（如是否需要礼貌对待），并为更广泛的AI应用和风险管理奠定了基础。

Abstract: Trust in AI is undermined by the fact that there is no science that predicts
-- or that can explain to the public -- when an LLM's output (e.g. ChatGPT) is
likely to tip mid-response to become wrong, misleading, irrelevant or
dangerous. With deaths and trauma already being blamed on LLMs, this
uncertainty is even pushing people to treat their 'pet' LLM more politely to
'dissuade' it (or its future Artificial General Intelligence offspring) from
suddenly turning on them. Here we address this acute need by deriving from
first principles an exact formula for when a Jekyll-and-Hyde tipping point
occurs at LLMs' most basic level. Requiring only secondary school mathematics,
it shows the cause to be the AI's attention spreading so thin it suddenly
snaps. This exact formula provides quantitative predictions for how the
tipping-point can be delayed or prevented by changing the prompt and the AI's
training. Tailored generalizations will provide policymakers and the public
with a firm platform for discussing any of AI's broader uses and risks, e.g. as
a personal counselor, medical advisor, decision-maker for when to use force in
a conflict situation. It also meets the need for clear and transparent answers
to questions like ''should I be polite to my LLM?''

</details>


### [137] [LTLf Adaptive Synthesis for Multi-Tier Goals in Nondeterministic Domains](https://arxiv.org/abs/2504.20983)
*Giuseppe De Giacomo,Gianmarco Parretti,Shufang Zhu*

Main category: cs.AI

TL;DR: 研究了一种LTLf合成的变体，用于在非确定性规划域中为多层级目标生成自适应策略。


<details>
  <summary>Details</summary>
Motivation: 解决在多层级目标下，如何动态调整策略以尽可能满足更多目标的问题。

Method: 采用博弈论技术计算自适应策略，确保策略的完备性和正确性。

Result: 提出的技术在多层级目标处理上仅需多项式时间（二次复杂度），效率接近标准LTLf合成。

Conclusion: 自适应策略在多层级目标合成中具有高效性和实用性。

Abstract: We study a variant of LTLf synthesis that synthesizes adaptive strategies for
achieving a multi-tier goal, consisting of multiple increasingly challenging
LTLf objectives in nondeterministic planning domains. Adaptive strategies are
strategies that at any point of their execution (i) enforce the satisfaction of
as many objectives as possible in the multi-tier goal, and (ii) exploit
possible cooperation from the environment to satisfy as many as possible of the
remaining ones. This happens dynamically: if the environment cooperates (ii)
and an objective becomes enforceable (i), then our strategies will enforce it.
We provide a game-theoretic technique to compute adaptive strategies that is
sound and complete. Notably, our technique is polynomial, in fact quadratic, in
the number of objectives. In other words, it handles multi-tier goals with only
a minor overhead compared to standard LTLf synthesis.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [138] [A constraints-based approach to fully interpretable neural networks for detecting learner behaviors](https://arxiv.org/abs/2504.20055)
*Juan D. Pinto,Luc Paquette*

Main category: cs.LG

TL;DR: 本文提出了一种设计上可解释的神经网络行为检测模型，通过约束简化推理过程并使其更接近人类对任务的理解。


<details>
  <summary>Details</summary>
Motivation: 随着复杂机器学习模型在教育中的应用增加，其可解释性问题引发关注，需要开发既忠实于模型内部机制又易于人类理解的解释技术。

Method: 采用一系列约束条件设计神经网络模型，使其参数具有明确解释性，并能完全捕捉学习行为的知识。

Result: 模型成功学习了‘钻系统空子’行为的模式，并提供了完全可解释的证据。

Conclusion: 该方法为可解释性提供了新思路，建议通过人类基础方法评估解释性。

Abstract: The increasing use of complex machine learning models in education has led to
concerns about their interpretability, which in turn has spurred interest in
developing explainability techniques that are both faithful to the model's
inner workings and intelligible to human end-users. In this paper, we describe
a novel approach to creating a neural-network-based behavior detection model
that is interpretable by design. Our model is fully interpretable, meaning that
the parameters we extract for our explanations have a clear interpretation,
fully capture the model's learned knowledge about the learner behavior of
interest, and can be used to create explanations that are both faithful and
intelligible. We achieve this by implementing a series of constraints to the
model that both simplify its inference process and bring it closer to a human
conception of the task at hand. We train the model to detect gaming-the-system
behavior, evaluate its performance on this task, and compare its learned
patterns to those identified by human experts. Our results show that the model
is successfully able to learn patterns indicative of gaming-the-system behavior
while providing evidence for fully interpretable explanations. We discuss the
implications of our approach and suggest ways to evaluate explainability using
a human-grounded approach.

</details>


### [139] [A Simple Review of EEG Foundation Models: Datasets, Advancements and Future Perspectives](https://arxiv.org/abs/2504.20069)
*Junhong Lai,Jiyu Wei,Lin Yao,Yueming Wang*

Main category: cs.LG

TL;DR: 综述探讨了EEG基础模型（EEG-FMs）的最新发展，包括架构、预训练策略及数据集，并指出了该领域的挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: EEG信号对理解脑活动和诊断神经疾病至关重要，EEG-FMs在处理和分析EEG数据方面展现出巨大潜力。

Method: 讨论了多种EEG-FMs的架构、预训练策略及其使用的数据集。

Result: EEG-FMs在EEG数据分析中表现出显著潜力。

Conclusion: 综述为研究者提供了EEG-FMs的全面概述，并指出了未来的研究方向。

Abstract: Electroencephalogram (EEG) signals play a crucial role in understanding brain
activity and diagnosing neurological disorders. This review focuses on the
recent development of EEG foundation models(EEG-FMs), which have shown great
potential in processing and analyzing EEG data. We discuss various EEG-FMs,
including their architectures, pre-training strategies, their pre-training and
downstream datasets and other details. The review also highlights the
challenges and future directions in this field, aiming to provide a
comprehensive overview for researchers and practitioners interested in EEG
analysis and related EEG-FMs.

</details>


### [140] [Improving Deep Knowledge Tracing via Gated Architectures and Adaptive Optimization](https://arxiv.org/abs/2504.20070)
*Altun Shukurlu*

Main category: cs.LG

TL;DR: 论文改进了深度知识追踪（DKT）模型，使用LSTM和GRU增强长期依赖捕捉，并基于PyTorch重新实现，优化了训练效率和可扩展性。实验表明，改进后的模型在准确性和稳定性上优于传统RNN，自适应优化器（如Adam）表现更佳。


<details>
  <summary>Details</summary>
Motivation: 原始DKT模型基于Lua的Torch框架，限制了可扩展性和可复现性。研究旨在通过架构改进和优化效率提升模型性能。

Method: 1. 使用LSTM和GRU替代标准RNN；2. 基于PyTorch重新实现模型；3. 对比多种优化算法（SGD、RMSProp、Adagrad、Adam、AdamW）。

Result: 实验显示，LSTM和GRU在准确性和训练稳定性上优于RNN，自适应优化器（如Adam）表现更佳。

Conclusion: 改进后的DKT模型提供了更高的性能和可扩展性，为未来研究提供了开源PyTorch实现。

Abstract: Deep Knowledge Tracing (DKT) models student learning behavior by using
Recurrent Neural Networks (RNNs) to predict future performance based on
historical interaction data. However, the original implementation relied on
standard RNNs in the Lua-based Torch framework, which limited extensibility and
reproducibility. In this work, we revisit the DKT model from two perspectives:
architectural improvements and optimization efficiency. First, we enhance the
model using gated recurrent units, specifically Long Short-Term Memory (LSTM)
networks and Gated Recurrent Units (GRU), which better capture long-term
dependencies and help mitigate vanishing gradient issues. Second, we
re-implement DKT using the PyTorch framework, enabling a modular and accessible
infrastructure compatible with modern deep learning workflows. We also
benchmark several optimization algorithms SGD, RMSProp, Adagrad, Adam, and
AdamW to evaluate their impact on convergence speed and predictive accuracy in
educational modeling tasks. Experiments on the Synthetic-5 and Khan Academy
datasets show that GRUs and LSTMs achieve higher accuracy and improved training
stability compared to basic RNNs, while adaptive optimizers such as Adam and
AdamW consistently outperform SGD in both early-stage learning and final model
performance. Our open-source PyTorch implementation provides a reproducible and
extensible foundation for future research in neural knowledge tracing and
personalized learning systems.

</details>


### [141] [RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2504.20073)
*Zihan Wang,Kangrui Wang,Qineng Wang,Pingyue Zhang,Linjie Li,Zhengyuan Yang,Kefan Yu,Minh Nhat Nguyen,Licheng Liu,Eli Gottlieb,Monica Lam,Yiping Lu,Kyunghyun Cho,Jiajun Wu,Li Fei-Fei,Lijuan Wang,Yejin Choi,Manling Li*

Main category: cs.LG

TL;DR: 论文提出StarPO框架和RAGEN系统，用于训练LLM交互代理，解决了多轮RL训练中的Echo Trap问题，并探讨了RL rollout的设计和奖励信号的重要性。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型（LLM）作为交互代理面临长时决策和随机环境反馈的挑战，多轮代理RL训练尚未充分探索。

Method: 提出StarPO框架和RAGEN系统，通过轨迹级RL训练代理，并引入StarPO-S变体解决Echo Trap问题。

Result: 研究发现RL训练中的Echo Trap模式，提出优化方法；RL rollout设计需多样化初始状态和中等交互粒度；细粒度奖励信号对代理推理至关重要。

Conclusion: StarPO和RAGEN为LLM代理RL训练提供了有效框架，强调了奖励信号和训练设计的重要性。

Abstract: Training large language models (LLMs) as interactive agents presents unique
challenges including long-horizon decision making and interacting with
stochastic environment feedback. While reinforcement learning (RL) has enabled
progress in static tasks, multi-turn agent RL training remains underexplored.
We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a
general framework for trajectory-level agent RL, and introduce RAGEN, a modular
system for training and evaluating LLM agents. Our study on three stylized
environments reveals three core findings. First, our agent RL training shows a
recurring mode of Echo Trap where reward variance cliffs and gradient spikes;
we address this with StarPO-S, a stabilized variant with trajectory filtering,
critic incorporation, and decoupled clipping. Second, we find the shaping of RL
rollouts would benefit from diverse initial states, medium interaction
granularity and more frequent sampling. Third, we show that without
fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge
through multi-turn RL and they may show shallow strategies or hallucinated
thoughts. Code and environments are available at
https://github.com/RAGEN-AI/RAGEN.

</details>


### [142] [Low-Rank Matrix Approximation for Neural Network Compression](https://arxiv.org/abs/2504.20078)
*Kalyan Cherukuri,Aarav Lala*

Main category: cs.LG

TL;DR: 提出了一种自适应秩奇异值分解（ARSVD）方法，动态调整全连接层的秩以减少计算和内存需求，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNNs）常受限于高内存和计算需求，传统固定秩压缩方法无法灵活平衡压缩与性能。

Method: 通过能量分布动态选择每层的秩，训练MLP并在MNIST、CIFAR-10和CIFAR-100数据集上评估。

Result: ARSVD显著压缩模型且不损失分类精度，优于传统固定秩压缩方法。

Conclusion: ARSVD在资源受限场景下有效平衡压缩与性能，具有实用价值。

Abstract: Deep Neural Networks (DNNs) are often constrained by their large memories and
computational restrictions. In this paper, we introduce a novel adaptive-rank
Singular Value Decomposition (ARSVD) that dynamically chooses the rank increase
of the fully connected layers below a certain threshold in energy expenditure.
Unlike conventional SVD compression methods that apply a fixed rank reduction
in all layers, our ARSVD method uses energy distribution to adaptively select
rank per layer while retaining accuracy. This is done for each layer in an
effort to use as much energy as possible while maintaining the lowest accuracy
loss. Such accuracy-adaptive approaches outperform traditional static rank
reduction methods by providing an improved balance between compression and
model performance. We first train a simple Multi-Layer Perceptron (MLP) on the
MNIST, CIFAR-10, and CIFAR-100 dataset and evaluate its performance using
accuracy and F1-score. After applying ARSVD, our results demonstrate that the
technique can achieve substantial model compression without compromising
classification accuracy. These results illustrate the usefulness of ARSVD in
computing scenarios where both computational and memory resources are scarce.

</details>


### [143] [FX-DARTS: Designing Topology-unconstrained Architectures with Differentiable Architecture Search and Entropy-based Super-network Shrinking](https://arxiv.org/abs/2504.20079)
*Xuan Rao,Bo Zhao,Derong Liu,Cesare Alippi*

Main category: cs.LG

TL;DR: FX-DARTS通过消除DARTS中的先验约束，提出了一种基于熵的超网络收缩框架，以在更大的搜索空间中稳定地探索更灵活的神经网络架构。


<details>
  <summary>Details</summary>
Motivation: 现有的DARTS方法对搜索空间施加了强先验约束，限制了Auto-ML的发展和神经网络架构的灵活性。

Method: 提出FX-DARTS方法，取消对单元拓扑的限制，并改进超网络的离散化机制，利用基于熵的超网络收缩框架（ESS）解决先验约束消除带来的挑战。

Result: 在图像分类基准测试中，FX-DARTS能够在单次搜索过程中找到性能和计算复杂度平衡的神经网络架构。

Conclusion: FX-DARTS通过减少先验约束，提升了架构搜索的灵活性，同时保持了搜索空间的稳定性。

Abstract: Strong priors are imposed on the search space of Differentiable Architecture
Search (DARTS), such that cells of the same type share the same topological
structure and each intermediate node retains two operators from distinct nodes.
While these priors reduce optimization difficulties and improve the
applicability of searched architectures, they hinder the subsequent development
of automated machine learning (Auto-ML) and prevent the optimization algorithm
from exploring more powerful neural networks through improved architectural
flexibility. This paper aims to reduce these prior constraints by eliminating
restrictions on cell topology and modifying the discretization mechanism for
super-networks. Specifically, the Flexible DARTS (FX-DARTS) method, which
leverages an Entropy-based Super-Network Shrinking (ESS) framework, is
presented to address the challenges arising from the elimination of prior
constraints. Notably, FX-DARTS enables the derivation of neural architectures
without strict prior rules while maintaining the stability in the enlarged
search space. Experimental results on image classification benchmarks
demonstrate that FX-DARTS is capable of exploring a set of neural architectures
with competitive trade-offs between performance and computational complexity
within a single search procedure.

</details>


### [144] [DNAD: Differentiable Neural Architecture Distillation](https://arxiv.org/abs/2504.20080)
*Xuan Rao,Bo Zhao,Derong Liu*

Main category: cs.LG

TL;DR: DNAD算法结合了搜索删除和模仿搜索，通过SNPS和KD技术，设计出高效神经网络，平衡性能和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 为满足设计高效神经网络的需求，平衡模型性能和计算复杂度。

Method: 开发了SNPS算法（基于DARTS框架）和DNAD算法（结合SNPS与KD）。

Result: 在CIFAR-10和ImageNet上，DNAD表现优异，参数和FLOPs更少。

Conclusion: DNAD通过结合SNPS和KD，显著提升了神经架构搜索的效率和性能。

Abstract: To meet the demand for designing efficient neural networks with appropriate
trade-offs between model performance (e.g., classification accuracy) and
computational complexity, the differentiable neural architecture distillation
(DNAD) algorithm is developed based on two cores, namely search by deleting and
search by imitating. Primarily, to derive neural architectures in a space where
cells of the same type no longer share the same topology, the super-network
progressive shrinking (SNPS) algorithm is developed based on the framework of
differentiable architecture search (DARTS), i.e., search by deleting. Unlike
conventional DARTS-based approaches which yield neural architectures with
simple structures and derive only one architecture during the search procedure,
SNPS is able to derive a Pareto-optimal set of architectures with flexible
structures by forcing the dynamic super-network shrink from a dense structure
to a sparse one progressively. Furthermore, since knowledge distillation (KD)
has shown great effectiveness to train a compact network with the assistance of
an over-parameterized model, we integrate SNPS with KD to formulate the DNAD
algorithm, i.e., search by imitating. By minimizing behavioral differences
between the super-network and teacher network, the over-fitting of one-level
DARTS is avoided and well-performed neural architectures are derived.
Experiments on CIFAR-10 and ImageNet classification tasks demonstrate that both
SNPS and DNAD are able to derive a set of architectures which achieve similar
or lower error rates with fewer parameters and FLOPs. Particularly, DNAD
achieves the top-1 error rate of 23.7% on ImageNet classification with a model
of 6.0M parameters and 598M FLOPs, which outperforms most DARTS-based methods.

</details>


### [145] [Towards Practical Second-Order Optimizers in Deep Learning: Insights from Fisher Information Analysis](https://arxiv.org/abs/2504.20096)
*Damien Martins Gomes*

Main category: cs.LG

TL;DR: AdaFisher是一种新型自适应二阶优化器，通过Fisher信息矩阵的块Kronecker近似预条件梯度，旨在平衡二阶方法的收敛性与计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管二阶优化算法在收敛性上优于一阶方法（如Adam和SGD），但其高计算成本限制了在深度神经网络训练中的实用性。

Method: AdaFisher利用Fisher信息矩阵的对角块Kronecker近似，自适应地预条件梯度。

Result: AdaFisher在图像分类和语言建模任务中表现出色，稳定且鲁棒，且在准确性和收敛速度上优于现有优化器。

Conclusion: AdaFisher成功地将二阶方法的优势与计算效率结合，为深度神经网络训练提供了实用且高效的优化方案。

Abstract: First-order optimization methods remain the standard for training deep neural
networks (DNNs). Optimizers like Adam incorporate limited curvature information
by preconditioning the stochastic gradient with a diagonal matrix. Despite the
widespread adoption of first-order methods, second-order optimization
algorithms often exhibit superior convergence compared to methods like Adam and
SGD. However, their practicality in training DNNs is still limited by a
significantly higher per-iteration computational cost compared to first-order
methods. In this thesis, we present AdaFisher, a novel adaptive second-order
optimizer that leverages a diagonal block-Kronecker approximation of the Fisher
information matrix to adaptively precondition gradients. AdaFisher aims to
bridge the gap between the improved convergence and generalization of
second-order methods and the computational efficiency needed for training DNNs.
Despite the traditionally slower speed of second-order optimizers, AdaFisher is
effective for tasks such as image classification and language modeling,
exhibiting remarkable stability and robustness during hyperparameter tuning. We
demonstrate that AdaFisher outperforms state-of-the-art optimizers in both
accuracy and convergence speed. The code is available from
https://github.com/AtlasAnalyticsLab/AdaFisher.

</details>


### [146] [Decoding Latent Spaces: Assessing the Interpretability of Time Series Foundation Models for Visual Analytics](https://arxiv.org/abs/2504.20099)
*Inmaculada Santamaria-Valenzuela,Victor Rodriguez-Fernandez,Javier Huertas-Tato,Jong Hyuk Park,David Camacho*

Main category: cs.LG

TL;DR: 研究探讨了时间序列基础模型（如MOMENT）潜在空间的可解释性，发现微调能提升性能，但潜在空间的解释性仍需改进。


<details>
  <summary>Details</summary>
Motivation: 探索时间序列基础模型潜在空间的可解释性，以支持视觉分析任务。

Method: 评估MOMENT模型在五个数据集上的表现，分析微调对潜在空间清晰度的影响。

Result: 微调显著减少损失，但潜在空间的解释性提升有限。

Conclusion: 时间序列基础模型虽高效，但潜在空间需进一步优化以提升解释性。

Abstract: The present study explores the interpretability of latent spaces produced by
time series foundation models, focusing on their potential for visual analysis
tasks. Specifically, we evaluate the MOMENT family of models, a set of
transformer-based, pre-trained architectures for multivariate time series tasks
such as: imputation, prediction, classification, and anomaly detection. We
evaluate the capacity of these models on five datasets to capture the
underlying structures in time series data within their latent space projection
and validate whether fine tuning improves the clarity of the resulting
embedding spaces. Notable performance improvements in terms of loss reduction
were observed after fine tuning. Visual analysis shows limited improvement in
the interpretability of the embeddings, requiring further work. Results suggest
that, although Time Series Foundation Models such as MOMENT are robust, their
latent spaces may require additional methodological refinements to be
adequately interpreted, such as alternative projection techniques, loss
functions, or data preprocessing strategies. Despite the limitations of MOMENT,
foundation models supose a big reduction in execution time and so a great
advance for interactive visual analytics.

</details>


### [147] [HyboWaveNet: Hyperbolic Graph Neural Networks with Multi-Scale Wavelet Transform for Protein-Protein Interaction Prediction](https://arxiv.org/abs/2504.20102)
*Qingzhi Yu,Shuai Yan,Wenfeng Dai,Xiang Cheng*

Main category: cs.LG

TL;DR: HyboWaveNet结合双曲图神经网络和多尺度图小波变换，提高了蛋白质相互作用预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络方法在蛋白质相互作用预测中缺乏因果解释，难以捕捉多尺度动态交互模式。

Method: 提出HyboWaveNet框架，利用双曲图神经网络模拟生物分子层次关系，结合多尺度图小波变换提取特征。

Result: 在公开数据集上表现优于现有方法，多尺度图小波变换模块提升了预测性能和泛化能力。

Conclusion: HyboWaveNet为复杂生物系统分析提供了新方法，结合了几何深度学习和信号处理。

Abstract: Protein-protein interactions (PPIs) are fundamental for deciphering cellular
functions,disease pathways,and drug discovery.Although existing neural networks
and machine learning methods have achieved high accuracy in PPI
prediction,their black-box nature leads to a lack of causal interpretation of
the prediction results and difficulty in capturing hierarchical geometries and
multi-scale dynamic interaction patterns among proteins.To address these
challenges, we propose HyboWaveNet,a novel deep learning framework that
collaborates with hyperbolic graphical neural networks (HGNNs) and multiscale
graphical wavelet transform for robust PPI prediction. Mapping protein features
to Lorentz space simulates hierarchical topological relationships among
biomolecules via a hyperbolic distance metric,enabling node feature
representations that better fit biological a priori.HyboWaveNet inherently
simulates hierarchical and scale-free biological relationships, while the
integration of wavelet transforms enables adaptive extraction of local and
global interaction features across different resolutions. Our framework
generates node feature representations via a graph neural network under the
Lorenz model and generates pairs of positive samples under multiple different
views for comparative learning, followed by further feature extraction via
multi-scale graph wavelet transforms to predict potential PPIs. Experiments on
public datasets show that HyboWaveNet improves over both existing
state-of-the-art methods. We also demonstrate through ablation experimental
studies that the multi-scale graph wavelet transform module improves the
predictive performance and generalization ability of HyboWaveNet. This work
links geometric deep learning and signal processing to advance PPI prediction,
providing a principled approach for analyzing complex biological systems

</details>


### [148] [Adaptive Helpfulness-Harmlessness Alignment with Preference Vectors](https://arxiv.org/abs/2504.20106)
*Ren-Wei Liang,Chin-Ting Hsu,Chan-Hung Yu,Saransh Agrawal,Shih-Cheng Huang,Shang-Tse Chen,Kuan-Hao Huang,Shao-Hua Sun*

Main category: cs.LG

TL;DR: 提出了一种名为Preference Vector的新框架，通过训练独立模型并动态合并偏好向量，解决了现有方法在性能冲突、可控性和扩展性上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如RLHF和DPO）在平衡语言模型的有用性和无害性时存在性能冲突、可控性差和扩展性不足的问题。

Method: 训练独立模型提取偏好向量，动态合并以实现用户可控的偏好调整和新偏好的无缝集成。

Result: 实验表明，Preference Vector提高了有用性，避免了过度保守，支持平滑的偏好权衡和可扩展的多偏好对齐。

Conclusion: Preference Vector框架在语言模型的偏好对齐中表现出色，具有更高的可控性和扩展性。

Abstract: Ensuring that large language models (LLMs) are both helpful and harmless is a
critical challenge, as overly strict constraints can lead to excessive
refusals, while permissive models risk generating harmful content. Existing
approaches, such as reinforcement learning from human feedback (RLHF) and
direct preference optimization (DPO), attempt to balance these trade-offs but
suffer from performance conflicts, limited controllability, and poor
extendability. To address these issues, we propose Preference Vector, a novel
framework inspired by task arithmetic. Instead of optimizing multiple
preferences within a single objective, we train separate models on individual
preferences, extract behavior shifts as preference vectors, and dynamically
merge them at test time. This modular approach enables fine-grained,
user-controllable preference adjustments and facilitates seamless integration
of new preferences without retraining. Experiments show that our proposed
Preference Vector framework improves helpfulness without excessive
conservatism, allows smooth control over preference trade-offs, and supports
scalable multi-preference alignment.

</details>


### [149] [Swapped Logit Distillation via Bi-level Teacher Alignment](https://arxiv.org/abs/2504.20108)
*Stephen Ekaputra Limantoro,Jhe-Hao Lin,Chih-Yu Wang,Yi-Lung Tsai,Hong-Han Shuai,Ching-Chun Huang,Wen-Huang Cheng*

Main category: cs.LG

TL;DR: 提出了一种基于交换逻辑处理的蒸馏方法（SLD），通过交换教师和学生的输出，提升知识蒸馏的效果。


<details>
  <summary>Details</summary>
Motivation: 传统的知识蒸馏方法直接将教师网络的原始分布传递给学生，可能导致错误预测。SLD旨在解决这一问题。

Method: 提出交换逻辑处理方案，将教师和学生的输出转化为两个教师，并引入损失调度以优化性能。

Result: 在图像分类任务中，SLD表现优于现有最先进方法。

Conclusion: SLD通过交换逻辑处理和损失调度，显著提升了知识蒸馏的效果。

Abstract: Knowledge distillation (KD) compresses the network capacity by transferring
knowledge from a large (teacher) network to a smaller one (student). It has
been mainstream that the teacher directly transfers knowledge to the student
with its original distribution, which can possibly lead to incorrect
predictions. In this article, we propose a logit-based distillation via swapped
logit processing, namely Swapped Logit Distillation (SLD). SLD is proposed
under two assumptions: (1) the wrong prediction occurs when the prediction
label confidence is not the maximum; (2) the "natural" limit of probability
remains uncertain as the best value addition to the target cannot be
determined. To address these issues, we propose a swapped logit processing
scheme. Through this approach, we find that the swap method can be effectively
extended to teacher and student outputs, transforming into two teachers. We
further introduce loss scheduling to boost the performance of two teachers'
alignment. Extensive experiments on image classification tasks demonstrate that
SLD consistently performs best among previous state-of-the-art methods.

</details>


### [150] [Attention to Detail: Fine-Scale Feature Preservation-Oriented Geometric Pre-training for AI-Driven Surrogate Modeling](https://arxiv.org/abs/2504.20110)
*Yu-hsuan Chen,Jing Bi,Cyril Ngo Ngoc,Victor Oancea,Jonathan Cagan,Levent Burak Kara*

Main category: cs.LG

TL;DR: 该论文提出了一种自监督几何表示学习方法，用于从非参数3D模型中捕获精细几何特征，解决了传统方法在保留精细几何细节方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统AI驱动的替代模型在需要保留精细几何细节的应用中效果有限，且标记CAD到模拟数据集稀缺，因此需要新的自监督和基础模型方法。

Method: 该方法通过解耦几何特征提取与下游物理任务，利用几何重建损失指导的潜在空间嵌入，结合近零水平采样和批自适应注意力加权损失函数。

Result: 案例研究表明，该方法在结构力学中表现出色，能够捕获设计特征并实现准确的少样本物理预测。

Conclusion: 该方法在几何和物理表示之间架起桥梁，为数据稀缺场景下的替代建模提供了有效解决方案。

Abstract: AI-driven surrogate modeling has become an increasingly effective alternative
to physics-based simulations for 3D design, analysis, and manufacturing. These
models leverage data-driven methods to predict physical quantities
traditionally requiring computationally expensive simulations. However, the
scarcity of labeled CAD-to-simulation datasets has driven recent advancements
in self-supervised and foundation models, where geometric representation
learning is performed offline and later fine-tuned for specific downstream
tasks. While these approaches have shown promise, their effectiveness is
limited in applications requiring fine-scale geometric detail preservation.
This work introduces a self-supervised geometric representation learning method
designed to capture fine-scale geometric features from non-parametric 3D
models. Unlike traditional end-to-end surrogate models, this approach decouples
geometric feature extraction from downstream physics tasks, learning a latent
space embedding guided by geometric reconstruction losses. Key elements include
the essential use of near-zero level sampling and the innovative batch-adaptive
attention-weighted loss function, which enhance the encoding of intricate
design features. The proposed method is validated through case studies in
structural mechanics, demonstrating strong performance in capturing design
features and enabling accurate few-shot physics predictions. Comparisons with
traditional parametric surrogate modeling highlight its potential to bridge the
gap between geometric and physics-based representations, providing an effective
solution for surrogate modeling in data-scarce scenarios.

</details>


### [151] [Supervised Pretraining for Material Property Prediction](https://arxiv.org/abs/2504.20112)
*Chowdhury Mohammad Abid Rahman,Aldo H. Romero,Prashnna K. Gyawali*

Main category: cs.LG

TL;DR: 论文提出了一种基于监督预训练的方法，利用替代标签优化材料属性预测，结合图增强技术提升模型鲁棒性，显著提高了预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法依赖大量标注数据，成本高且耗时。自监督学习虽能利用无标签数据，但如何进一步优化预训练效果仍待探索。

Method: 提出监督预训练策略，利用替代标签指导学习；引入图增强技术，通过噪声注入提升模型鲁棒性。

Result: 在六项材料属性预测任务中，模型性能显著提升，MAE改善2%至6.67%，创下新基准。

Conclusion: 本研究首次探索了监督预训练在材料属性预测中的应用，为领域方法学和应用提供了新思路。

Abstract: Accurate prediction of material properties facilitates the discovery of novel
materials with tailored functionalities. Deep learning models have recently
shown superior accuracy and flexibility in capturing structure-property
relationships. However, these models often rely on supervised learning, which
requires large, well-annotated datasets an expensive and time-consuming
process. Self-supervised learning (SSL) offers a promising alternative by
pretraining on large, unlabeled datasets to develop foundation models that can
be fine-tuned for material property prediction. In this work, we propose
supervised pretraining, where available class information serves as surrogate
labels to guide learning, even when downstream tasks involve unrelated material
properties. We evaluate this strategy on two state-of-the-art SSL models and
introduce a novel framework for supervised pretraining. To further enhance
representation learning, we propose a graph-based augmentation technique that
injects noise to improve robustness without structurally deforming material
graphs. The resulting foundation models are fine-tuned for six challenging
material property predictions, achieving significant performance gains over
baselines, ranging from 2% to 6.67% improvement in mean absolute error (MAE)
and establishing a new benchmark in material property prediction. This study
represents the first exploration of supervised pertaining with surrogate labels
in material property prediction, advancing methodology and application in the
field.

</details>


### [152] [Benchmarking Transferability: A Framework for Fair and Robust Evaluation](https://arxiv.org/abs/2504.20121)
*Alireza Kazemi,Helia Rezvani,Mahsa Baktashmotlagh*

Main category: cs.LG

TL;DR: 本文提出了一种系统评估迁移性得分的框架，发现现有方法在不同场景下表现不一，强调了标准化评估的重要性，并提出了一种新指标，在特定实验中提升了3.5%的性能。


<details>
  <summary>Details</summary>
Motivation: 迁移性得分的可靠性和实用性因实验设置、数据集和假设的差异而难以确定，需要系统评估。

Method: 引入了一个全面的基准测试框架，通过大量实验评估不同迁移性得分方法的表现。

Result: 发现不同指标在不同场景下表现不一，标准化评估协议有助于更可靠的迁移性测量。提出的新指标在特定实验中提升了3.5%。

Conclusion: 标准化评估协议对提升迁移性得分的可靠性至关重要，为跨领域应用中的模型选择提供了更可靠的依据。

Abstract: Transferability scores aim to quantify how well a model trained on one domain
generalizes to a target domain. Despite numerous methods proposed for measuring
transferability, their reliability and practical usefulness remain
inconclusive, often due to differing experimental setups, datasets, and
assumptions. In this paper, we introduce a comprehensive benchmarking framework
designed to systematically evaluate transferability scores across diverse
settings. Through extensive experiments, we observe variations in how different
metrics perform under various scenarios, suggesting that current evaluation
practices may not fully capture each method's strengths and limitations. Our
findings underscore the value of standardized assessment protocols, paving the
way for more reliable transferability measures and better-informed model
selection in cross-domain applications. Additionally, we achieved a 3.5\%
improvement using our proposed metric for the head-training fine-tuning
experimental setup. Our code is available in this repository:
https://github.com/alizkzm/pert_robust_platform.

</details>


### [153] [LZ Penalty: An information-theoretic repetition penalty for autoregressive language models](https://arxiv.org/abs/2504.20131)
*Antonio A. Ginart,Naveen Kodali,Jason Lee,Caiming Xiong,Silvio Savarese,John R. Emmons*

Main category: cs.LG

TL;DR: LZ惩罚是一种专门用于减少自回归语言模型中退化重复的惩罚方法，基于LZ77无损压缩算法的编码长度，能够在贪婪解码下保持模型能力且避免退化重复。


<details>
  <summary>Details</summary>
Motivation: 解决自回归语言模型中退化重复的问题，同时保持模型的能力。

Method: 基于LZ77算法的编码长度设计LZ惩罚，通过预测-压缩对偶性解释其作用。

Result: LZ惩罚在贪婪解码下有效避免了退化重复，而行业标准的频率惩罚和重复惩罚效果不佳。

Conclusion: LZ惩罚是一种有效的解决方案，适用于提升语言模型的解码质量。

Abstract: We introduce the LZ penalty, a penalty specialized for reducing degenerate
repetitions in autoregressive language models without loss of capability. The
penalty is based on the codelengths in the LZ77 universal lossless compression
algorithm. Through the lens of the prediction-compression duality, decoding the
LZ penalty has the interpretation of sampling from the residual distribution
after removing the information that is highly compressible. We demonstrate the
LZ penalty enables state-of-the-art open-source reasoning models to operate
with greedy (temperature zero) decoding without loss of capability and without
instances of degenerate repetition. Both the industry-standard frequency
penalty and repetition penalty are ineffective, incurring degenerate repetition
rates of up to 4%.

</details>


### [154] [Causal Identification in Time Series Models](https://arxiv.org/abs/2504.20172)
*Erik Jahn,Karthik Karnik,Leonard J. Schulman*

Main category: cs.LG

TL;DR: 本文分析了因果识别算法在具有潜在混杂因素的因果时间序列图中的适用性，并首次给出了仅依赖于每时间步变量数量和最大时间滞后的边界。


<details>
  <summary>Details</summary>
Motivation: 研究因果时间序列图中因果效应的可识别性，尤其是在无限时间步和潜在混杂因素存在的情况下。

Method: 应用因果识别算法到时间序列图的固定大小片段，以决定因果效应的可识别性。

Result: 首次给出了仅依赖于每时间步变量数量和最大时间滞后的边界，证明固定大小片段足以决定因果效应的可识别性。

Conclusion: 固定大小的时间序列图片段足以决定因果效应的可识别性，即使时间间隔无限。

Abstract: In this paper, we analyze the applicability of the Causal Identification
algorithm to causal time series graphs with latent confounders. Since these
graphs extend over infinitely many time steps, deciding whether causal effects
across arbitrary time intervals are identifiable appears to require computation
on graph segments of unbounded size. Even for deciding the identifiability of
intervention effects on variables that are close in time, no bound is known on
how many time steps in the past need to be considered. We give a first bound of
this kind that only depends on the number of variables per time step and the
maximum time lag of any direct or latent causal effect. More generally, we show
that applying the Causal Identification algorithm to a constant-size segment of
the time series graph is sufficient to decide identifiability of causal
effects, even across unbounded time intervals.

</details>


### [155] [AI Recommendation Systems for Lane-Changing Using Adherence-Aware Reinforcement Learning](https://arxiv.org/abs/2504.20187)
*Weihao Sun,Heeseung Bang,Andreas A. Malikopoulos*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的车道变换推荐方法，考虑了人类驾驶员的部分遵从性，以提高半自动驾驶环境中的行驶效率。


<details>
  <summary>Details</summary>
Motivation: 在半自动驾驶环境中，人类驾驶员可能不完全遵从推荐动作，影响了行驶效率。

Method: 将问题建模为马尔可夫决策过程，采用考虑部分遵从性的深度Q网络。

Result: 在CARLA驾驶环境中进行了评估，验证了方法的有效性。

Conclusion: 该方法能够有效提升半自动驾驶环境中的行驶效率。

Abstract: In this paper, we present an adherence-aware reinforcement learning (RL)
approach aimed at seeking optimal lane-changing recommendations within a
semi-autonomous driving environment to enhance a single vehicle's travel
efficiency. The problem is framed within a Markov decision process setting and
is addressed through an adherence-aware deep Q network, which takes into
account the partial compliance of human drivers with the recommended actions.
This approach is evaluated within CARLA's driving environment under realistic
scenarios.

</details>


### [156] [ProFi-Net: Prototype-based Feature Attention with Curriculum Augmentation for WiFi-based Gesture Recognition](https://arxiv.org/abs/2504.20193)
*Zhe Cui,Shuxian Zhang,Kangzhi Lou,Le-Nam Tran*

Main category: cs.LG

TL;DR: ProFi-Net是一个用于WiFi手势识别的少样本学习框架，通过原型度量学习和特征级注意力机制提升性能，并结合课程式数据增强策略提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决WiFi手势识别中训练数据有限和特征表示稀疏的问题。

Method: 采用原型度量学习架构和特征级注意力机制，动态优化欧氏距离；引入课程式数据增强策略，逐步增加高斯噪声。

Result: 在多样真实环境中显著优于传统原型网络和其他少样本学习方法，分类准确率和训练效率更高。

Conclusion: ProFi-Net通过动态特征优化和数据增强，有效提升了少样本WiFi手势识别的性能和鲁棒性。

Abstract: This paper presents ProFi-Net, a novel few-shot learning framework for
WiFi-based gesture recognition that overcomes the challenges of limited
training data and sparse feature representations. ProFi-Net employs a
prototype-based metric learning architecture enhanced with a feature-level
attention mechanism, which dynamically refines the Euclidean distance by
emphasizing the most discriminative feature dimensions. Additionally, our
approach introduces a curriculum-inspired data augmentation strategy
exclusively on the query set. By progressively incorporating Gaussian noise of
increasing magnitude, the model is exposed to a broader range of challenging
variations, thereby improving its generalization and robustness to overfitting.
Extensive experiments conducted across diverse real-world environments
demonstrate that ProFi-Net significantly outperforms conventional prototype
networks and other state-of-the-art few-shot learning methods in terms of
classification accuracy and training efficiency.

</details>


### [157] [Representation Learning on a Random Lattice](https://arxiv.org/abs/2504.20197)
*Aryeh Brill*

Main category: cs.LG

TL;DR: 论文提出了一种几何视角，将深度神经网络的表示分解为可解释特征，以增强其安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 通过理解特征的可解释性，提升深度神经网络的安全性和可靠性。

Method: 采用几何视角，将特征视为嵌入数据分布的坐标系，并使用渗流理论分析随机格点模型。

Result: 特征被分类为上下文、组件和表面特征，模型与近期机械可解释性研究一致。

Conclusion: 模型为未来研究提供了方向，支持特征的可解释性分析。

Abstract: Decomposing a deep neural network's learned representations into
interpretable features could greatly enhance its safety and reliability. To
better understand features, we adopt a geometric perspective, viewing them as a
learned coordinate system for mapping an embedded data distribution. We
motivate a model of a generic data distribution as a random lattice and analyze
its properties using percolation theory. Learned features are categorized into
context, component, and surface features. The model is qualitatively consistent
with recent findings in mechanistic interpretability and suggests directions
for future research.

</details>


### [158] [Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework](https://arxiv.org/abs/2504.20213)
*Yuan Xia,Akanksha Atrey,Fadoua Khmaissia,Kedar S. Namjoshi*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型（LLM）在布尔逻辑证明任务中的推理能力，提出了一种数据增强技术（模板转换）来提升模型性能，实验表明模型在短证明任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在形式逻辑推理中的能力，解决真实世界证明数据稀缺的问题。

Method: 使用随机化方法合成有效证明，引入模板转换技术增强模型处理复杂逻辑的能力。

Result: 模型在短证明任务中表现优异，但随着证明复杂度增加性能下降；模板转换对小模型也有效。

Conclusion: LLM在布尔逻辑推理中展现出潜力，模板转换是提升性能的有效方法。

Abstract: This paper investigates the logical reasoning capabilities of large language
models (LLMs). For a precisely defined yet tractable formulation, we choose the
conceptually simple but technically complex task of constructing proofs in
Boolean logic. A trained LLM receives as input a set of assumptions and a goal,
and produces as output a proof that formally derives the goal from the
assumptions. Incorrect proofs are caught by an automated proof checker. A
critical obstacle for training is the scarcity of real-world proofs. We propose
an efficient, randomized procedure for synthesizing valid proofs and introduce
Template Transformation, a data augmentation technique that enhances the
model's ability to handle complex logical expressions. The central evaluation
question is whether an LLM has indeed learned to reason. We propose tests to
measure the reasoning ability of a black-box LLM. By these measures,
experiments demonstrate strong reasoning capabilities for assertions with short
proofs, which decline with proof complexity. Notably, template transformation
improves accuracy even for smaller models, suggesting its effectiveness across
model scales.

</details>


### [159] [Temporal Neural Operator for Modeling Time-Dependent Physical Phenomena](https://arxiv.org/abs/2504.20249)
*W. Diab,M. Al-Kobaisi*

Main category: cs.LG

TL;DR: 提出了一种名为Temporal Neural Operator (TNO)的神经网络算子，专门用于解决时间依赖偏微分方程（PDEs）的时空算子学习问题，解决了现有神经算子在时间动态映射和训练成本上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子（如DeepONet和FNO）在空间函数映射上表现优异，但在时间依赖PDEs的时间动态映射上表现不佳，且训练成本高。

Method: TNO通过引入时间分支到DeepONet框架，结合多种训练策略（如马尔可夫假设、教师强制、时间捆绑等），优化了时空算子学习。

Result: TNO在长时间范围的时间外推、误差累积鲁棒性、分辨率不变性和多输入函数处理上表现出色。

Conclusion: TNO是一种高效的神经算子，适用于时间依赖PDEs的时空学习，具有广泛的应用潜力。

Abstract: Neural Operators (NOs) are machine learning models designed to solve partial
differential equations (PDEs) by learning to map between function spaces.
Neural Operators such as the Deep Operator Network (DeepONet) and the Fourier
Neural Operator (FNO) have demonstrated excellent generalization properties
when mapping between spatial function spaces. However, they struggle in mapping
the temporal dynamics of time-dependent PDEs, especially for time steps not
explicitly seen during training. This limits their temporal accuracy as they do
not leverage these dynamics in the training process. In addition, most NOs tend
to be prohibitively costly to train, especially for higher-dimensional PDEs. In
this paper, we propose the Temporal Neural Operator (TNO), an efficient neural
operator specifically designed for spatio-temporal operator learning for
time-dependent PDEs. TNO achieves this by introducing a temporal-branch to the
DeepONet framework, leveraging the best architectural design choices from
several other NOs, and a combination of training strategies including Markov
assumption, teacher forcing, temporal bundling, and the flexibility to
condition the output on the current state or past states. Through extensive
benchmarking and an ablation study on a diverse set of example problems we
demonstrate the TNO long range temporal extrapolation capabilities, robustness
to error accumulation, resolution invariance, and flexibility to handle
multiple input functions.

</details>


### [160] [Financial Data Analysis with Robust Federated Logistic Regression](https://arxiv.org/abs/2504.20250)
*Kun Yang,Nikhil Krishnan,Sanjeev R. Kulkarni*

Main category: cs.LG

TL;DR: 论文提出了一种鲁棒的联邦逻辑回归框架，旨在保护数据隐私的同时提高模型可解释性，并在异常值存在时保持稳健性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决联邦学习中的数据隐私保护、模型可解释性及对异常值的鲁棒性问题。

Method: 方法包括提出一种鲁棒的联邦逻辑回归框架，并在IID和非IID数据（尤其是含异常值场景）下进行验证。

Result: 实验结果表明，该方法在二元和多类分类任务中表现与经典集中式算法（如逻辑回归、决策树和K近邻）相当。

Conclusion: 结论表明，该框架在联邦学习中实现了隐私保护、可解释性和鲁棒性的平衡。

Abstract: In this study, we focus on the analysis of financial data in a federated
setting, wherein data is distributed across multiple clients or locations, and
the raw data never leaves the local devices. Our primary focus is not only on
the development of efficient learning frameworks (for protecting user data
privacy) in the field of federated learning but also on the importance of
designing models that are easier to interpret. In addition, we care about the
robustness of the framework to outliers. To achieve these goals, we propose a
robust federated logistic regression-based framework that strives to strike a
balance between these goals. To verify the feasibility of our proposed
framework, we carefully evaluate its performance not only on independently
identically distributed (IID) data but also on non-IID data, especially in
scenarios involving outliers. Extensive numerical results collected from
multiple public datasets demonstrate that our proposed method can achieve
comparable performance to those of classical centralized algorithms, such as
Logistical Regression, Decision Tree, and K-Nearest Neighbors, in both binary
and multi-class classification tasks.

</details>


### [161] [Investigating task-specific prompts and sparse autoencoders for activation monitoring](https://arxiv.org/abs/2504.20271)
*Henk Tillman,Dan Mossing*

Main category: cs.LG

TL;DR: 论文探讨了语言模型输出监控的改进方法，比较了不同激活探测技术的效果，并推荐了在不同计算资源条件下的最优方法。


<details>
  <summary>Details</summary>
Motivation: 语言模型可能产生意外或不安全的输出，因此需要监控其内部激活信息以提高安全性。

Method: 研究比较了线性探测、提示探测和稀疏自编码器（SAE）等方法，并提出了改进方案。

Result: 零样本直接询问模型是一个合理的基线，但激活探测方法在足够训练数据下表现更好；提示探测在计算资源充足时效果最佳，SAE方法在计算受限时更优。

Conclusion: 推荐在计算资源充足时使用提示探测，资源受限时使用SAE方法。

Abstract: Language models can behave in unexpected and unsafe ways, and so it is
valuable to monitor their outputs. Internal activations of language models
encode additional information that could be useful for this. The baseline
approach for activation monitoring is some variation of linear probing on a
particular layer: starting from a labeled dataset, train a logistic regression
classifier on that layer's activations. Recent work has proposed several
approaches which may improve on naive linear probing, by leveraging additional
computation. One class of techniques, which we call "prompted probing,"
leverages test time computation to improve monitoring by (1) prompting the
model with a description of the monitoring task, and (2) applying a learned
linear probe to resulting activations. Another class of techniques uses
computation at train time: training sparse autoencoders offline to identify an
interpretable basis for the activations, and e.g. max-pooling activations
across tokens using that basis before applying a linear probe. However, one can
also prompt the model with a description of the monitoring task and use its
output directly. We develop and test novel refinements of these methods and
compare them against each other. We find asking the model zero-shot is a
reasonable baseline when inference-time compute is not limited; however,
activation probing methods can substantially outperform this baseline given
sufficient training data. Specifically, we recommend prompted probing when
inference-time compute is available, due to its superior data efficiency and
good generalization performance. Alternatively, if inference-time compute is
limited, we find SAE-based probing methods outperform raw activation probing.

</details>


### [162] [Generative Diffusion Models for Resource Allocation in Wireless Networks](https://arxiv.org/abs/2504.20277)
*Yigit Berkay Uslu,Samar Hadou,Shirin Saeedi Bidokhti,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出了一种基于生成扩散模型的监督训练算法，用于学习随机资源分配策略。


<details>
  <summary>Details</summary>
Motivation: 解决在满足服务质量约束下最大化效用函数的随机资源分配问题。

Method: 利用生成扩散模型模仿专家策略，并通过图神经网络参数化反向扩散过程以实现泛化。

Result: 在干扰网络中的功率控制案例中展示了接近最优的性能。

Conclusion: 该方法能够有效生成接近最优的分配策略，并适用于多种网络配置。

Abstract: This paper proposes a supervised training algorithm for learning stochastic
resource allocation policies with generative diffusion models (GDMs). We
formulate the allocation problem as the maximization of an ergodic utility
function subject to ergodic Quality of Service (QoS) constraints. Given samples
from a stochastic expert policy that yields a near-optimal solution to the
problem, we train a GDM policy to imitate the expert and generate new samples
from the optimal distribution. We achieve near-optimal performance through
sequential execution of the generated samples. To enable generalization to a
family of network configurations, we parameterize the backward diffusion
process with a graph neural network (GNN) architecture. We present numerical
results in a case study of power control in multi-user interference networks.

</details>


### [163] [FedCCL: Federated Clustered Continual Learning Framework for Privacy-focused Energy Forecasting](https://arxiv.org/abs/2504.20282)
*Michael A. Helcig,Stefan Nastic*

Main category: cs.LG

TL;DR: FedCCL是一个针对静态组织特性但动态客户端可用性的联邦学习框架，通过静态预训练聚类和异步FedAvg算法，实现高效知识共享和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法难以处理异构数据分布和计算能力差异，传统解决方案效率低且模型专业化延迟。

Method: 结合静态预训练聚类和异步FedAvg算法，采用三层模型拓扑（全局、集群特定和本地模型）管理知识共享。

Result: 在光伏安装数据上，FedCCL实现3.93%的预测误差，保持数据隐私和稳定性，新安装性能仅下降0.14个百分点。

Conclusion: FedCCL为隐私保护的分布式学习提供了高效框架，适应动态参与者群体并保持高准确性。

Abstract: Privacy-preserving distributed model training is crucial for modern machine
learning applications, yet existing Federated Learning approaches struggle with
heterogeneous data distributions and varying computational capabilities.
Traditional solutions either treat all participants uniformly or require costly
dynamic clustering during training, leading to reduced efficiency and delayed
model specialization. We present FedCCL (Federated Clustered Continual
Learning), a framework specifically designed for environments with static
organizational characteristics but dynamic client availability. By combining
static pre-training clustering with an adapted asynchronous FedAvg algorithm,
FedCCL enables new clients to immediately profit from specialized models
without prior exposure to their data distribution, while maintaining reduced
coordination overhead and resilience to client disconnections. Our approach
implements an asynchronous Federated Learning protocol with a three-tier model
topology - global, cluster-specific, and local models - that efficiently
manages knowledge sharing across heterogeneous participants. Evaluation using
photovoltaic installations across central Europe demonstrates that FedCCL's
location-based clustering achieves an energy prediction error of 3.93%
(+-0.21%), while maintaining data privacy and showing that the framework
maintains stability for population-independent deployments, with 0.14
percentage point degradation in performance for new installations. The results
demonstrate that FedCCL offers an effective framework for privacy-preserving
distributed learning, maintaining high accuracy and adaptability even with
dynamic participant populations.

</details>


### [164] [Radius-Guided Post-Clustering for Shape-Aware, Scalable Refinement of k-Means Results](https://arxiv.org/abs/2504.20293)
*Stefan Kober*

Main category: cs.LG

TL;DR: 论文提出了一种基于几何增强的k-means改进方法，通过合并重叠半径的簇来适应非凸形状，并支持分布式计算。


<details>
  <summary>Details</summary>
Motivation: 传统k-means对非凸形状效果不佳且需预先指定k值，改进方法旨在解决这些问题。

Method: 在标准k-means后，为每个簇中心分配半径（最远点距离），合并半径重叠的簇，支持递归分区和分布式计算。

Result: 在基准数据集上表现良好，准确性高且计算开销低。

Conclusion: 该方法通过简单几何增强显著提升了k-means的适用性和扩展性。

Abstract: Traditional k-means clustering underperforms on non-convex shapes and
requires the number of clusters k to be specified in advance. We propose a
simple geometric enhancement: after standard k-means, each cluster center is
assigned a radius (the distance to its farthest assigned point), and clusters
whose radii overlap are merged. This post-processing step loosens the
requirement for exact k: as long as k is overestimated (but not excessively),
the method can often reconstruct non-convex shapes through meaningful merges.
We also show that this approach supports recursive partitioning: clustering can
be performed independently on tiled regions of the feature space, then globally
merged, making the method scalable and suitable for distributed systems.
Implemented as a lightweight post-processing step atop scikit-learn's k-means,
the algorithm performs well on benchmark datasets, achieving high accuracy with
minimal additional computation.

</details>


### [165] [The Dark Side of Digital Twins: Adversarial Attacks on AI-Driven Water Forecasting](https://arxiv.org/abs/2504.20295)
*Mohammadhossein Homaei,Victor Gonzalez Morales,Oscar Mogollon-Gutierrez,Andres Caro*

Main category: cs.LG

TL;DR: 本文介绍了一种用于西班牙供水网络的数字孪生平台，利用LSTM预测用水量，但模型易受对抗攻击（如FGSM和PGD）。作者提出了一种基于学习自动机（LA）和随机LA的方法，动态调整扰动，使攻击更难检测。实验显示，该方法显著降低了预测可靠性（MAPE从26%升至35%以上），凸显了AI驱动数字孪生的网络安全风险。


<details>
  <summary>Details</summary>
Motivation: 研究数字孪生平台在供水网络中的应用，并探索其机器学习模型（如LSTM）在面对对抗攻击时的脆弱性，以揭示潜在的安全风险。

Method: 提出了一种结合学习自动机（LA）和随机LA的方法，动态生成对抗扰动，攻击LSTM模型，并通过实验验证其效果。

Result: 实验结果表明，对抗攻击显著降低了预测准确性（MAPE从26%升至35%以上），自适应攻击策略进一步放大了这一影响。

Conclusion: 研究强调了AI驱动数字孪生系统的网络安全风险，并呼吁采取鲁棒防御措施，如对抗训练、异常检测和安全数据管道。

Abstract: Digital twins (DTs) are improving water distribution systems by using
real-time data, analytics, and prediction models to optimize operations. This
paper presents a DT platform designed for a Spanish water supply network,
utilizing Long Short-Term Memory (LSTM) networks to predict water consumption.
However, machine learning models are vulnerable to adversarial attacks, such as
the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD).
These attacks manipulate critical model parameters, injecting subtle
distortions that degrade forecasting accuracy. To further exploit these
vulnerabilities, we introduce a Learning Automata (LA) and Random LA-based
approach that dynamically adjusts perturbations, making adversarial attacks
more difficult to detect. Experimental results show that this approach
significantly impacts prediction reliability, causing the Mean Absolute
Percentage Error (MAPE) to rise from 26% to over 35%. Moreover, adaptive attack
strategies amplify this effect, highlighting cybersecurity risks in AI-driven
DTs. These findings emphasize the urgent need for robust defenses, including
adversarial training, anomaly detection, and secure data pipelines.

</details>


### [166] [FigBO: A Generalized Acquisition Function Framework with Look-Ahead Capability for Bayesian Optimization](https://arxiv.org/abs/2504.20307)
*Hui Chen,Xuhui Fan,Zhangkai Wu,Longbing Cao*

Main category: cs.LG

TL;DR: FigBO是一种新的贝叶斯优化采集函数，通过考虑候选点的未来影响提升性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有近视采集函数缺乏前瞻性，限制了性能。

Method: 提出FigBO，一种通用采集函数，结合未来信息增益。

Result: 理论分析和实验表明FigBO性能优越，收敛更快。

Conclusion: FigBO是一种即插即用的高效方法，显著提升贝叶斯优化效果。

Abstract: Bayesian optimization is a powerful technique for optimizing
expensive-to-evaluate black-box functions, consisting of two main components: a
surrogate model and an acquisition function. In recent years, myopic
acquisition functions have been widely adopted for their simplicity and
effectiveness. However, their lack of look-ahead capability limits their
performance. To address this limitation, we propose FigBO, a generalized
acquisition function that incorporates the future impact of candidate points on
global information gain. FigBO is a plug-and-play method that can integrate
seamlessly with most existing myopic acquisition functions. Theoretically, we
analyze the regret bound and convergence rate of FigBO when combined with the
myopic base acquisition function expected improvement (EI), comparing them to
those of standard EI. Empirically, extensive experimental results across
diverse tasks demonstrate that FigBO achieves state-of-the-art performance and
significantly faster convergence compared to existing methods.

</details>


### [167] [A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning](https://arxiv.org/abs/2504.20310)
*Greg Gluch,Shafi Goldwasser*

Main category: cs.LG

TL;DR: 论文研究了对抗性输入在机器学习推理阶段的检测与缓解，定义了防御检测（DbD）和防御缓解（DbM），并证明在分类任务中两者等价，但在生成任务中分离。


<details>
  <summary>Details</summary>
Motivation: 研究对抗性输入在推理阶段的防御策略，形式化定义并比较检测与缓解的可行性。

Method: 通过3轮协议定义DbD和DbM，提出正确性、完备性和可靠性标准，分析分类和生成任务中的防御效果。

Result: 分类任务中DbD和DbM等价，生成任务中DbM可行而DbD不可行，基于密码学假设。

Conclusion: 生成任务中防御策略需针对性设计，检测与缓解的可行性因任务类型而异。

Abstract: In this paper, we initiate a cryptographically inspired theoretical study of
detection versus mitigation of adversarial inputs produced by attackers of
Machine Learning algorithms during inference time.
  We formally define defense by detection (DbD) and defense by mitigation
(DbM). Our definitions come in the form of a 3-round protocol between two
resource-bounded parties: a trainer/defender and an attacker. The attacker aims
to produce inference-time inputs that fool the training algorithm. We define
correctness, completeness, and soundness properties to capture successful
defense at inference time while not degrading (too much) the performance of the
algorithm on inputs from the training distribution.
  We first show that achieving DbD and achieving DbM are equivalent for ML
classification tasks. Surprisingly, this is not the case for ML generative
learning tasks, where there are many possible correct outputs that can be
generated for each input. We show a separation between DbD and DbM by
exhibiting a generative learning task for which is possible to defend by
mitigation but is provably impossible to defend by detection under the
assumption that the Identity-Based Fully Homomorphic Encryption (IB-FHE),
publicly-verifiable zero-knowledge Succinct Non-Interactive Arguments of
Knowledge (zk-SNARK) and Strongly Unforgeable Signatures exist. The mitigation
phase uses significantly fewer samples than the initial training algorithm.

</details>


### [168] [Perturbation-efficient Zeroth-order Optimization for Hardware-friendly On-device Training](https://arxiv.org/abs/2504.20314)
*Qitao Tan,Sung-En Chang,Rui Xia,Huidong Ji,Chence Yang,Ci Zhang,Jun Liu,Zheng Zhan,Zhou Zou,Yanzhi Wang,Jin Lu,Geng Yuan*

Main category: cs.LG

TL;DR: PeZO框架解决了零阶优化中高斯随机数生成的高成本问题，通过随机数重用和硬件友好的均匀分布替代方法，显著降低了资源消耗和功耗。


<details>
  <summary>Details</summary>
Motivation: 零阶优化在DNN训练中具有计算简单和内存节省的优势，但高斯随机数生成的高成本限制了其在硬件平台上的应用。

Method: 提出PeZO框架，采用随机数重用策略和硬件友好的均匀分布替代高斯分布。

Result: 实验显示PeZO减少了48.6%的LUTs和12.7%的FFs需求，功耗最大降低86%，且不影响训练性能。

Conclusion: PeZO为零阶优化在设备端训练的可行性提供了解决方案，为未来研究提供了新思路。

Abstract: Zeroth-order (ZO) optimization is an emerging deep neural network (DNN)
training paradigm that offers computational simplicity and memory savings.
However, this seemingly promising approach faces a significant and long-ignored
challenge. ZO requires generating a substantial number of Gaussian random
numbers, which poses significant difficulties and even makes it infeasible for
hardware platforms, such as FPGAs and ASICs. In this paper, we identify this
critical issue, which arises from the mismatch between algorithm and hardware
designers. To address this issue, we proposed PeZO, a perturbation-efficient ZO
framework. Specifically, we design random number reuse strategies to
significantly reduce the demand for random number generation and introduce a
hardware-friendly adaptive scaling method to replace the costly Gaussian
distribution with a uniform distribution. Our experiments show that PeZO
reduces the required LUTs and FFs for random number generation by 48.6\% and
12.7\%, and saves at maximum 86\% power consumption, all without compromising
training performance, making ZO optimization feasible for on-device training.
To the best of our knowledge, we are the first to explore the potential of
on-device ZO optimization, providing valuable insights for future research.

</details>


### [169] [Bayesian Experimental Design for Model Discrepancy Calibration: An Auto-Differentiable Ensemble Kalman Inversion Approach](https://arxiv.org/abs/2504.20319)
*Huchen Yang,Xinghao Dong,Jin-Long Wu*

Main category: cs.LG

TL;DR: 提出了一种基于自动微分集成卡尔曼反演（AD-EKI）的混合贝叶斯实验设计框架，用于解决模型差异带来的挑战，并通过梯度自由方法高效估计高维参数的信息增益。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯实验设计（BED）在数据采集中具有优势，但模型差异会导致参数估计偏差，而高维参数空间对贝叶斯更新和设计优化提出了挑战。

Method: 结合AD-EKI的混合框架，通过迭代优化实验设计，分离低维物理参数和高维模型差异的推断，利用梯度自由方法高效计算信息增益。

Result: 在经典对流-扩散BED示例中，该方法成功校准模型差异并推断未知物理参数，验证了其有效性。

Conclusion: AD-EKI不仅解决了BED中的模型差异问题，还为元学习和结构优化等双层优化问题提供了高效框架。

Abstract: Bayesian experimental design (BED) offers a principled framework for
optimizing data acquisition by leveraging probabilistic inference. However,
practical implementations of BED are often compromised by model discrepancy,
i.e., the mismatch between predictive models and true physical systems, which
can potentially lead to biased parameter estimates. While data-driven
approaches have been recently explored to characterize the model discrepancy,
the resulting high-dimensional parameter space poses severe challenges for both
Bayesian updating and design optimization. In this work, we propose a hybrid
BED framework enabled by auto-differentiable ensemble Kalman inversion (AD-EKI)
that addresses these challenges by providing a computationally efficient,
gradient-free alternative to estimate the information gain for high-dimensional
network parameters. The AD-EKI allows a differentiable evaluation of the
utility function in BED and thus facilitates the use of standard gradient-based
methods for design optimization. In the proposed hybrid framework, we
iteratively optimize experimental designs, decoupling the inference of
low-dimensional physical parameters handled by standard BED methods, from the
high-dimensional model discrepancy handled by AD-EKI. The identified optimal
designs for the model discrepancy enable us to systematically collect
informative data for its calibration. The performance of the proposed method is
studied by a classical convection-diffusion BED example, and the hybrid
framework enabled by AD-EKI efficiently identifies informative data to
calibrate the model discrepancy and robustly infers the unknown physical
parameters in the modeled system. Besides addressing the challenges of BED with
model discrepancy, AD-EKI also potentially fosters efficient and scalable
frameworks in many other areas with bilevel optimization, such as meta-learning
and structure optimization.

</details>


### [170] [Generative Learning for Slow Manifolds and Bifurcation Diagrams](https://arxiv.org/abs/2504.20375)
*Ellis R. Crabtree,Dimitris G. Giovanis,Nikolaos Evangelou,Juan M. Bello-Rivas,Ioannis G. Kevrekidis*

Main category: cs.LG

TL;DR: 论文提出了一种利用条件生成模型（cSGMs）快速初始化多时间尺度系统的低维慢流形，并近似分岔图中的稳态的方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法在构建慢流形和分岔图时需要系统性的算法，而机器学习中的条件生成模型（cSGMs）能够高效生成目标分布的数据，因此尝试将其应用于多时间尺度系统的建模。

Method: 利用条件生成模型（cSGMs）生成与感兴趣量（QoI）一致的慢流形样本，并近似分岔图中的稳态。

Result: 该方法能够快速初始化慢流形，并填补分岔图中缺失的稳态数据，揭示慢流形的几何结构。

Conclusion: 条件生成模型为多时间尺度系统的建模和分岔图分析提供了高效的新工具。

Abstract: In dynamical systems characterized by separation of time scales, the
approximation of so called ``slow manifolds'', on which the long term dynamics
lie, is a useful step for model reduction. Initializing on such slow manifolds
is a useful step in modeling, since it circumvents fast transients, and is
crucial in multiscale algorithms alternating between fine scale (fast) and
coarser scale (slow) simulations. In a similar spirit, when one studies the
infinite time dynamics of systems depending on parameters, the system
attractors (e.g., its steady states) lie on bifurcation diagrams. Sampling
these manifolds gives us representative attractors (here, steady states of ODEs
or PDEs) at different parameter values. Algorithms for the systematic
construction of these manifolds are required parts of the ``traditional''
numerical nonlinear dynamics toolkit.
  In more recent years, as the field of Machine Learning develops, conditional
score-based generative models (cSGMs) have demonstrated capabilities in
generating plausible data from target distributions that are conditioned on
some given label. It is tempting to exploit such generative models to produce
samples of data distributions conditioned on some quantity of interest (QoI).
In this work, we present a framework for using cSGMs to quickly (a) initialize
on a low-dimensional (reduced-order) slow manifold of a multi-time-scale system
consistent with desired value(s) of a QoI (a ``label'') on the manifold, and
(b) approximate steady states in a bifurcation diagram consistent with a (new,
out-of-sample) parameter value. This conditional sampling can help uncover the
geometry of the reduced slow-manifold and/or approximately ``fill in'' missing
segments of steady states in a bifurcation diagram.

</details>


### [171] [Manifold Clustering with Schatten p-norm Maximization](https://arxiv.org/abs/2504.20390)
*Fangfang Li,Quanxue Gao*

Main category: cs.LG

TL;DR: 提出了一种新的流形聚类框架，通过融合K-means和流形学习，确保数据结构与标签的一致性，并利用Schatten p-范数保持类别平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只关注K-means与流形学习的最优组合，而忽略了数据结构与标签的一致性。

Method: 通过标签引导流形结构并进行聚类，同时最大化标签的Schatten p-范数以保持类别平衡。

Result: 实验结果表明，所提模型在多个数据库上表现优越。

Conclusion: 该框架灵活且兼容多种距离函数，适用于非线性可分数据的高效处理。

Abstract: Manifold clustering, with its exceptional ability to capture complex data
structures, holds a pivotal position in cluster analysis. However, existing
methods often focus only on finding the optimal combination between K-means and
manifold learning, and overlooking the consistency between the data structure
and labels. To address this issue, we deeply explore the relationship between
K-means and manifold learning, and on this basis, fuse them to develop a new
clustering framework. Specifically, the algorithm uses labels to guide the
manifold structure and perform clustering on it, which ensures the consistency
between the data structure and labels. Furthermore, in order to naturally
maintain the class balance in the clustering process, we maximize the Schatten
p-norm of labels, and provide a theoretical proof to support this.
Additionally, our clustering framework is designed to be flexible and
compatible with many types of distance functions, which facilitates efficient
processing of nonlinear separable data. The experimental results of several
databases confirm the superiority of our proposed model.

</details>


### [172] [FourierSpecNet: Neural Collision Operator Approximation Inspired by the Fourier Spectral Method for Solving the Boltzmann Equation](https://arxiv.org/abs/2504.20408)
*Jae Yong Lee,Gwang Jae Jung,Byung Chan Lim,Hyung Ju Hwang*

Main category: cs.LG

TL;DR: 提出了一种结合傅里叶谱方法和深度学习的混合框架（FourierSpecNet），用于高效近似玻尔兹曼方程中的碰撞算子，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 玻尔兹曼方程的高维非线性碰撞算子数值求解计算量大，尤其在非弹性碰撞和高维速度域中。

Method: FourierSpecNet框架在傅里叶空间中近似碰撞算子，支持分辨率无关学习和零样本超分辨率。

Result: 在多个基准测试中，FourierSpecNet表现出高精度且计算成本显著低于传统谱方法。

Conclusion: FourierSpecNet为玻尔兹曼方程的求解提供了一种高效且可扩展的解决方案，适用于弹性和非弹性碰撞场景。

Abstract: The Boltzmann equation, a fundamental model in kinetic theory, describes the
evolution of particle distribution functions through a nonlinear,
high-dimensional collision operator. However, its numerical solution remains
computationally demanding, particularly for inelastic collisions and
high-dimensional velocity domains. In this work, we propose the Fourier Neural
Spectral Network (FourierSpecNet), a hybrid framework that integrates the
Fourier spectral method with deep learning to approximate the collision
operator in Fourier space efficiently. FourierSpecNet achieves
resolution-invariant learning and supports zero-shot super-resolution, enabling
accurate predictions at unseen resolutions without retraining. Beyond empirical
validation, we establish a consistency result showing that the trained operator
converges to the spectral solution as the discretization is refined. We
evaluate our method on several benchmark cases, including Maxwellian and
hard-sphere molecular models, as well as inelastic collision scenarios. The
results demonstrate that FourierSpecNet offers competitive accuracy while
significantly reducing computational cost compared to traditional spectral
solvers. Our approach provides a robust and scalable alternative for solving
the Boltzmann equation across both elastic and inelastic regimes.

</details>


### [173] [ADiff4TPP: Asynchronous Diffusion Models for Temporal Point Processes](https://arxiv.org/abs/2504.20411)
*Amartya Mukherjee,Ruizhi Deng,He Zhao,Yuzhen Mao,Leonid Sigal,Frederick Tung*

Main category: cs.LG

TL;DR: 本文提出了一种基于异步噪声调度的扩散模型方法，用于建模时间点过程，通过优化噪声调度设计，实现了对未来事件的更准确预测。


<details>
  <summary>Details</summary>
Motivation: 传统时间点过程建模方法在长期预测和灵活性方面存在不足，需要一种能够动态调整噪声调度的方法以提高预测性能。

Method: 采用扩散模型结合异步噪声调度，通过条件流匹配训练模型，优化事件序列的联合分布建模。

Result: 在基准数据集上，该方法在预测事件间隔时间和事件类型方面达到了最先进水平，并在长期预测任务中优于现有基线方法。

Conclusion: 该方法通过灵活的噪声调度设计，显著提升了时间点过程的建模和预测能力，尤其在长期预测任务中表现优异。

Abstract: This work introduces a novel approach to modeling temporal point processes
using diffusion models with an asynchronous noise schedule. At each step of the
diffusion process, the noise schedule injects noise of varying scales into
different parts of the data. With a careful design of the noise schedules,
earlier events are generated faster than later ones, thus providing stronger
conditioning for forecasting the more distant future. We derive an objective to
effectively train these models for a general family of noise schedules based on
conditional flow matching. Our method models the joint distribution of the
latent representations of events in a sequence and achieves state-of-the-art
results in predicting both the next inter-event time and event type on
benchmark datasets. Additionally, it flexibly accommodates varying lengths of
observation and prediction windows in different forecasting settings by
adjusting the starting and ending points of the generation process. Finally,
our method shows superior performance in long-horizon prediction tasks,
outperforming existing baseline methods.

</details>


### [174] [Understanding GNNs and Homophily in Dynamic Node Classification](https://arxiv.org/abs/2504.20421)
*Michael Ito,Danai Koutra,Jenna Wiens*

Main category: cs.LG

TL;DR: 论文提出动态同质性（dynamic homophily）作为动态图神经网络（GNN）性能的新度量，并证明其在动态节点分类中的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前同质性度量仅适用于静态图，缺乏对动态图的分析，限制了GNN在动态场景中的应用。

Method: 通过理论分析，提出动态同质性度量，并在多种动态节点分类数据集上验证其与GNN性能的关联。

Result: 研究发现现有GNN在低动态同质性下表现不佳，动态同质性与GNN性能显著相关。

Conclusion: 动态同质性为理解动态图中的GNN性能提供了新视角，并为设计更强大的动态GNN奠定了基础。

Abstract: Homophily, as a measure, has been critical to increasing our understanding of
graph neural networks (GNNs). However, to date this measure has only been
analyzed in the context of static graphs. In our work, we explore homophily in
dynamic settings. Focusing on graph convolutional networks (GCNs), we
demonstrate theoretically that in dynamic settings, current GCN discriminative
performance is characterized by the probability that a node's future label is
the same as its neighbors' current labels. Based on this insight, we propose
dynamic homophily, a new measure of homophily that applies in the dynamic
setting. This new measure correlates with GNN discriminative performance and
sheds light on how to potentially design more powerful GNNs for dynamic graphs.
Leveraging a variety of dynamic node classification datasets, we demonstrate
that popular GNNs are not robust to low dynamic homophily. Going forward, our
work represents an important step towards understanding homophily and GNN
performance in dynamic node classification.

</details>


### [175] [Learning Laplacian Positional Encodings for Heterophilous Graphs](https://arxiv.org/abs/2504.20430)
*Michael Ito,Jiong Zhu,Dexiong Chen,Danai Koutra,Jenna Wiens*

Main category: cs.LG

TL;DR: 当前图位置编码（PEs）在异质性图中可能无效甚至有害，作者提出可学习拉普拉斯位置编码（LLPE），通过利用图拉普拉斯的全谱，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有PEs在异质性图中表现不佳，而许多现实网络具有异质性，需改进。

Method: 提出LLPE，利用图拉普拉斯全谱，适应同质和异质图。

Result: 理论证明LLPE可近似一类图距离，实验显示在12个基准测试中性能提升显著。

Conclusion: LLPE是开发适应异质图复杂结构的PEs的重要进展。

Abstract: In this work, we theoretically demonstrate that current graph positional
encodings (PEs) are not beneficial and could potentially hurt performance in
tasks involving heterophilous graphs, where nodes that are close tend to have
different labels. This limitation is critical as many real-world networks
exhibit heterophily, and even highly homophilous graphs can contain local
regions of strong heterophily. To address this limitation, we propose Learnable
Laplacian Positional Encodings (LLPE), a new PE that leverages the full
spectrum of the graph Laplacian, enabling them to capture graph structure on
both homophilous and heterophilous graphs. Theoretically, we prove LLPE's
ability to approximate a general class of graph distances and demonstrate its
generalization properties. Empirically, our evaluation on 12 benchmarks
demonstrates that LLPE improves accuracy across a variety of GNNs, including
graph transformers, by up to 35% and 14% on synthetic and real-world graphs,
respectively. Going forward, our work represents a significant step towards
developing PEs that effectively capture complex structures in heterophilous
graphs.

</details>


### [176] [GaLore 2: Large-Scale LLM Pre-Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2504.20437)
*DiJia Su,Andrew Gu,Jane Xu,Yuandong Tian,Jiawei Zhao*

Main category: cs.LG

TL;DR: GaLore 2是一个高效且可扩展的框架，解决了GaLore在训练大型语言模型时的内存瓶颈问题，并展示了其在预训练中的实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在训练过程中面临显著的内存瓶颈，GaLore通过利用梯度低秩结构解决了这一问题，但仍存在计算开销和并行化策略集成的挑战。

Method: GaLore 2通过优化子空间更新的计算开销（如SVD）和集成先进的并行化策略（如FSDP），进一步扩展了GaLore的功能。

Result: GaLore 2展示了其可扩展性，成功预训练了Llama 7B模型，使用了高达5000亿的训练标记。

Conclusion: GaLore 2为LLM预训练提供了高效且可扩展的解决方案，具有实际应用的潜力。

Abstract: Large language models (LLMs) have revolutionized natural language
understanding and generation but face significant memory bottlenecks during
training. GaLore, Gradient Low-Rank Projection, addresses this issue by
leveraging the inherent low-rank structure of weight gradients, enabling
substantial memory savings without sacrificing performance. Recent works
further extend GaLore from various aspects, including low-bit quantization and
higher-order tensor structures. However, there are several remaining challenges
for GaLore, such as the computational overhead of SVD for subspace updates and
the integration with state-of-the-art training parallelization strategies
(e.g., FSDP). In this paper, we present GaLore 2, an efficient and scalable
GaLore framework that addresses these challenges and incorporates recent
advancements. In addition, we demonstrate the scalability of GaLore 2 by
pre-training Llama 7B from scratch using up to 500 billion training tokens,
highlighting its potential impact on real LLM pre-training scenarios.

</details>


### [177] [Multidimensional precipitation index prediction based on CNN-LSTM hybrid framework](https://arxiv.org/abs/2504.20442)
*Yuchen Wang,Pengfei Jia,Zhitao Shu,Keyan Liu,Abdul Rashid Mohamed Shariff*

Main category: cs.LG

TL;DR: 本文提出了一种基于CNN-LSTM混合框架的多维降水指数预测模型，用于提高降水预测的准确性。实验结果显示，该模型在预测精度和泛化能力上优于传统方法，但计算资源需求较高。


<details>
  <summary>Details</summary>
Motivation: 全球气候变化加剧，降水预测对防灾减灾、农业生产和交通管理至关重要。

Method: 使用CNN-LSTM混合框架分析1972-2002年印度Pune地区的月均降水数据，捕捉局部特征和长期依赖关系。

Result: 模型RMSE为6.752，优于传统时间序列预测方法。

Conclusion: 模型在降水预测中表现优异，但需优化计算资源需求和多维数据预测能力。未来可扩展模型以支持多维数据预测。

Abstract: With the intensification of global climate change, accurate prediction of
weather indicators is of great significance in disaster prevention and
mitigation, agricultural production, and transportation. Precipitation, as one
of the key meteorological indicators, plays a crucial role in water resource
management, agricultural production, and urban flood control. This study
proposes a multidimensional precipitation index prediction model based on a
CNN- LSTM hybrid framework, aiming to improve the accuracy of precipitation
forecasts. The dataset is sourced from Pune, Maharashtra, India, covering
monthly mean precipitation data from 1972 to 2002. This dataset includes nearly
31 years (1972-2002) of monthly average precipitation, reflecting the long-term
fluctuations and seasonal variations of precipitation in the region. By
analyzing these time series data, the CNN-LSTM model effectively captures local
features and long-term dependencies. Experimental results show that the model
achieves a root mean square error (RMSE) of 6.752, which demonstrates a
significant advantage over traditional time series prediction methods in terms
of prediction accuracy and generalization ability. Furthermore, this study
provides new research ideas for precipitation prediction. However, the model
requires high computational resources when dealing with large-scale datasets,
and its predictive ability for multidimensional precipitation data still needs
improvement. Future research could extend the model to support and predict
multidimensional precipitation data, thereby promoting the development of more
accurate and efficient meteorological prediction technologies.

</details>


### [178] [FT-MoE: Sustainable-learning Mixture of Experts Model for Fault-Tolerant Computing with Multiple Tasks](https://arxiv.org/abs/2504.20446)
*Wenjing Xiao,Wenhao Song,Miaojiang Chen,Ruikun Luo,Min Chen*

Main category: cs.LG

TL;DR: FT-MoE是一种可持续学习的混合专家模型，用于多任务的容错计算，通过解耦长距离依赖关系和双专家网络提高故障检测和分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的容错算法因故障知识的异构性和时间序列日志数据的复杂依赖关系，难以进一步提升性能。

Method: 使用解码器变换模型获取故障原型向量，设计双专家网络进行高精度预测，并采用离线训练和在线调优的两阶段优化方案。

Result: 实验表明，FT-MoE在容错基准测试中优于现有方法。

Conclusion: FT-MoE通过动态学习和适应能力，显著提升了容错计算的可靠性和性能。

Abstract: Intelligent fault-tolerant (FT) computing has recently demonstrated
significant advantages of predicting and diagnosing faults in advance, enabling
reliable service delivery. However, due to heterogeneity of fault knowledge and
complex dependence relationships of time series log data, existing deep
learning-based FT algorithms further improve detection performance relying on
single neural network model with difficulty. To this end, we propose FT-MoE, a
sustainable-learning mixture-of-experts model for fault-tolerant computing with
multiple tasks, which enables different parameters learning distinct fault
knowledge to achieve high-reliability for service system. Firstly, we use
decoder-based transformer models to obtain fault prototype vectors of
decoupling long-distance dependencies. Followed by, we present a dual mixture
of experts networks for high-accurate prediction for both fault detection and
classification tasks. Then, we design a two-stage optimization scheme of
offline training and online tuning, which allows that in operation FT-MoE can
also keep learning to adapt to dynamic service environments. Finally, to verify
the effectiveness of FT-MoE, we conduct extensive experiments on the FT
benchmark. Experimental results show that FT-MoE achieves superior performance
compared to the state-of-the-art methods. Code will be available upon
publication.

</details>


### [179] [Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding](https://arxiv.org/abs/2504.20456)
*Gabe Guo,Stefano Ermon*

Main category: cs.LG

TL;DR: AS-ARMs（任意子集自回归模型）通过ASSD算法解决了并行生成令牌时的分布偏差问题，显著加速语言生成且保持质量。


<details>
  <summary>Details</summary>
Motivation: 解决离散扩散模型在并行生成令牌时因条件独立性假设导致的分布偏差问题。

Method: 提出AS-ARMs和ASSD算法，支持并行化联合概率密度估计，纠正并行生成的令牌分布。

Result: ASSD显著加速生成且不牺牲质量，AS-ARMs在小参数模型上达到SOTA性能。

Conclusion: AS-ARMs是语言建模的有前景方向。

Abstract: In arbitrary-order language models, it is an open question how to sample
tokens in parallel from the correct joint distribution. With discrete diffusion
models, the more tokens they generate in parallel, the less their predicted
distributions adhere to the originally learned data distribution, as they rely
on a conditional independence assumption that only works with infinitesimally
small timesteps. We find that a different class of models, any-subset
autoregressive models (AS-ARMs), holds the solution. As implied by the name,
AS-ARMs can generate tokens in any order, and in parallel. Moreover, AS-ARMs
support parallelized joint probability density estimation, allowing them to
correct their own parallel-generated token distributions, via our Any-Subset
Speculative Decoding (ASSD) algorithm. ASSD provably enables generation of
tokens from the correct joint distribution, with the number of neural network
calls upper bounded by the number of tokens predicted. We empirically verify
that ASSD speeds up language generation, without sacrificing quality.
Furthermore, we provide a mathematically justified scheme for training AS-ARMs
for generation, and show that AS-ARMs achieve state-of-the-art performance
among sub-200M parameter models on infilling benchmark tasks, and nearly match
the performance of models 50X larger on code generation. Our theoretical and
empirical results indicate that the once-forgotten AS-ARMs are a promising
direction of language modeling.

</details>


### [180] [The Estimation of Continual Causal Effect for Dataset Shifting Streams](https://arxiv.org/abs/2504.20471)
*Baining Chen,Yiming Zhang,Yuqiao Han,Ruyue Zhang,Ruihuan Du,Zhishuo Zhou,Zhengdan Zhu,Xun Liu,Jiecheng Guo*

Main category: cs.LG

TL;DR: 论文提出ICE-PKD框架，通过增量学习和知识蒸馏解决营销优化中的因果效应估计问题，适应时间数据偏移。


<details>
  <summary>Details</summary>
Motivation: 在线营销优化中，时间数据偏移导致传统因果效应估计框架性能下降，需改进以适应动态变化。

Method: ICE-PKD框架包括多处理提升网络（消除混杂偏差）和增量训练策略（适应数据偏移并保护泛化能力）。

Result: 在模拟和在线数据集上表现优异，已部署于华夏出行平台。

Conclusion: ICE-PKD框架有效解决了时间数据偏移问题，提升了在线营销优化的性能。

Abstract: Causal effect estimation has been widely used in marketing optimization. The
framework of an uplift model followed by a constrained optimization algorithm
is popular in practice. To enhance performance in the online environment, the
framework needs to be improved to address the complexities caused by temporal
dataset shift. This paper focuses on capturing the dataset shift from user
behavior and domain distribution changing over time. We propose an Incremental
Causal Effect with Proxy Knowledge Distillation (ICE-PKD) framework to tackle
this challenge. The ICE-PKD framework includes two components: (i) a
multi-treatment uplift network that eliminates confounding bias using
counterfactual regression; (ii) an incremental training strategy that adapts to
the temporal dataset shift by updating with the latest data and protects
generalization via replay-based knowledge distillation. We also revisit the
uplift modeling metrics and introduce a novel metric for more precise online
evaluation in multiple treatment scenarios. Extensive experiments on both
simulated and online datasets show that the proposed framework achieves better
performance. The ICE-PKD framework has been deployed in the marketing system of
Huaxiaozhu, a ride-hailing platform in China.

</details>


### [181] [Group Relative Knowledge Distillation: Learning from Teacher's Relational Inductive Bias](https://arxiv.org/abs/2504.20482)
*Chao Li,Changhua Zhou,Jia Chen*

Main category: cs.LG

TL;DR: 提出了一种新的知识蒸馏方法GRKD，通过关注教师模型的相对排名而非绝对概率，提升学生模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法主要关注绝对概率，忽略了教师模型中相对预测的关系性归纳偏差，导致暴露偏差。

Method: 提出GRKD框架，通过组相对损失函数学习教师输出的类间相对排名顺序。

Result: 在分类基准测试中，GRKD表现出优于现有方法的泛化能力，尤其在细粒度分类任务中。

Conclusion: GRKD为利用教师知识提供了新视角，强调关系结构而非绝对概率。

Abstract: Knowledge distillation typically transfers knowledge from a teacher model to
a student model by minimizing differences between their output distributions.
However, existing distillation approaches largely focus on mimicking absolute
probabilities and neglect the valuable relational inductive biases embedded in
the teacher's relative predictions, leading to exposure bias. In this paper, we
propose Group Relative Knowledge Distillation (GRKD), a novel framework that
distills teacher knowledge by learning the relative ranking among classes,
rather than directly fitting the absolute distribution. Specifically, we
introduce a group relative loss that encourages the student model to preserve
the pairwise preference orderings provided by the teacher's outputs. Extensive
experiments on classification benchmarks demonstrate that GRKD achieves
superior generalization compared to existing methods, especially in tasks
requiring fine-grained class differentiation. Our method provides a new
perspective on exploiting teacher knowledge, focusing on relational structure
rather than absolute likelihood.

</details>


### [182] [Wavelet-Filtering of Symbolic Music Representations for Folk Tune Segmentation and Classification](https://arxiv.org/abs/2504.20522)
*Gissel Velarde,Tillman Weyde,David Meredith*

Main category: cs.LG

TL;DR: 该研究评估了一种基于Haar小波滤波的机器学习方法，用于分割和分类民歌的符号表示，并与基于格式塔的方法进行比较。结果表明，小波方法在优化参数后分类效果更好。


<details>
  <summary>Details</summary>
Motivation: 探索一种更有效的民歌分类方法，通过小波滤波提升分割和分类的准确性。

Method: 使用Haar小波的连续小波变换（CWT）对民歌的符号表示进行滤波和分割，利用小波系数的局部最大值标记边界，并通过k近邻算法分类。

Result: 小波滤波和分割方法在交叉验证中表现出更高的分类准确率。

Conclusion: Haar小波滤波方法在优化参数后优于传统的格式塔方法，适用于民歌分类。

Abstract: The aim of this study is to evaluate a machine-learning method in which
symbolic representations of folk songs are segmented and classified into tune
families with Haar-wavelet filtering. The method is compared with previously
proposed Gestalt-based method. Melodies are represented as discrete symbolic
pitch-time signals. We apply the continuous wavelet transform (CWT) with the
Haar wavelet at specific scales, obtaining filtered versions of melodies
emphasizing their information at particular time-scales. We use the filtered
signal for representation and segmentation, using the wavelet coefficients'
local maxima to indicate local boundaries and classify segments by means of
k-nearest neighbours based on standard vector-metrics (Euclidean, cityblock),
and compare the results to a Gestalt-based segmentation method and metrics
applied directly to the pitch signal. We found that the wavelet based
segmentation and wavelet-filtering of the pitch signal lead to better
classification accuracy in cross-validated evaluation when the time-scale and
other parameters are optimized.

</details>


### [183] [DeeP-Mod: Deep Dynamic Programming based Environment Modelling using Feature Extraction](https://arxiv.org/abs/2504.20535)
*Chris Child,Lam Ngo*

Main category: cs.LG

TL;DR: DeeP-Mod框架通过DDPN提取特征构建环境模型，解决了DQN中状态信息丢失的问题，并实现了任务和动作集的独立性。


<details>
  <summary>Details</summary>
Motivation: 解决DQN在深层网络中因混合状态-动作表示导致状态信息丢失的问题。

Method: 使用动态规划训练DDPN，通过值迭代确保输出为状态值而非状态-动作对，并提取特征构建环境模型。

Result: 减少的DDPN在噪声下收敛更快且性能优于原始DDPN，DeeP-Mod框架无需外部环境模型即可学习最优策略。

Conclusion: DeeP-Mod框架通过DDPN特征提取和动态规划，有效解决了状态信息丢失问题，适用于广泛环境。

Abstract: The DeeP-Mod framework builds an environment model using features from a Deep
Dynamic Programming Network (DDPN), trained via a Deep Q-Network (DQN). While
Deep Q-Learning is effective in decision-making, state information is lost in
deeper DQN layers due to mixed state-action representations. We address this by
using Dynamic Programming (DP) to train a DDPN, where Value Iteration ensures
the output represents state values, not state-action pairs. Extracting features
from the DDPN preserves state information, enabling task and action set
independence. We show that a reduced DDPN can be trained using features
extracted from the original DDPN trained on an identical problem. This reduced
DDPN achieves faster convergence under noise and outperforms the original DDPN.
Finally, we introduce the DeeP-Mod framework, which creates an environment
model using the evolution of features extracted from a DDPN in response to
actions. A second DDPN, which learns directly from this feature model rather
than raw states, can learn an effective feature-value representation and thus
optimal policy. A key advantage of DeeP-Mod is that an externally defined
environment model is not needed at any stage, making DDPN applicable to a wide
range of environments.

</details>


### [184] [Inclusive Training Separation and Implicit Knowledge Interaction for Balanced Online Class-Incremental Learning](https://arxiv.org/abs/2504.20566)
*Shunjie Wen,Thomas Heinis,Dong-Wan Choi*

Main category: cs.LG

TL;DR: 论文提出了一种名为BOIL的新方法，通过双分类器和包容性训练分离策略，在在线类增量学习中实现了高可塑性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 在线类增量学习（OCIL）需要在单次数据流中学习新类（可塑性）并保留旧类知识（稳定性），现有方法难以平衡两者。

Method: 提出BOIL方法，采用双分类器和包容性训练分离策略，隐式传递知识，有效整合新旧类知识。

Result: 在三个OCIL基准数据集上，BOIL表现出更平衡且优于现有方法的性能。

Conclusion: BOIL通过创新设计解决了OCIL中的平衡问题，显著提升了性能。

Abstract: Online class-incremental learning (OCIL) focuses on gradually learning new
classes (called plasticity) from a stream of data in a single-pass, while
concurrently preserving knowledge of previously learned classes (called
stability). The primary challenge in OCIL lies in maintaining a good balance
between the knowledge of old and new classes within the continually updated
model. Most existing methods rely on explicit knowledge interaction through
experience replay, and often employ exclusive training separation to address
bias problems. Nevertheless, it still remains a big challenge to achieve a
well-balanced learner, as these methods often exhibit either reduced plasticity
or limited stability due to difficulties in continually integrating knowledge
in the OCIL setting. In this paper, we propose a novel replay-based method,
called Balanced Online Incremental Learning (BOIL), which can achieve both high
plasticity and stability, thus ensuring more balanced performance in OCIL. Our
BOIL method proposes an inclusive training separation strategy using dual
classifiers so that knowledge from both old and new classes can effectively be
integrated into the model, while introducing implicit approaches for
transferring knowledge across the two classifiers. Extensive experimental
evaluations over three widely-used OCIL benchmark datasets demonstrate the
superiority of BOIL, showing more balanced yet better performance compared to
state-of-the-art replay-based OCIL methods.

</details>


### [185] [Digital Shielding for Cross-Domain Wi-Fi Signal Adaptation using Relativistic Average Generative Adversarial Network](https://arxiv.org/abs/2504.20568)
*Danilo Avola,Federica Bruni,Gian Luca Foresti,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的Wi-Fi信号跨域适应模型，通过物理信号屏蔽模拟和RaGAN结合Bi-LSTM架构，实现了高精度的材料识别。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi传感技术受环境影响大，跨域适应性差，需要解决性能稳定性和隐私保护问题。

Method: 使用RaGAN和Bi-LSTM架构，模拟物理屏蔽（法拉第笼）收集信号，训练多类SVM进行分类。

Result: 系统达到96%的准确率，具备强材料识别能力。

Conclusion: 该模型在安全应用中具有潜力，可用于识别隐蔽物体。

Abstract: Wi-Fi sensing uses radio-frequency signals from Wi-Fi devices to analyze
environments, enabling tasks such as tracking people, detecting intrusions, and
recognizing gestures. The rise of this technology is driven by the IEEE
802.11bf standard and growing demand for tools that can ensure privacy and
operate through obstacles. However, the performance of Wi-Fi sensing is heavily
influenced by environmental conditions, especially when extracting spatial and
temporal features from the surrounding scene. A key challenge is achieving
robust generalization across domains, ensuring stable performance even when the
sensing environment changes significantly. This paper introduces a novel deep
learning model for cross-domain adaptation of Wi-Fi signals, inspired by
physical signal shielding. The model uses a Relativistic average Generative
Adversarial Network (RaGAN) with Bidirectional Long Short-Term Memory (Bi-LSTM)
architectures for both the generator and discriminator. To simulate physical
shielding, an acrylic box lined with electromagnetic shielding fabric was
constructed, mimicking a Faraday cage. Wi-Fi signal spectra were collected from
various materials both inside (domain-free) and outside (domain-dependent) the
box to train the model. A multi-class Support Vector Machine (SVM) was trained
on domain-free spectra and tested on signals denoised by the RaGAN. The system
achieved 96% accuracy and demonstrated strong material discrimination
capabilities, offering potential for use in security applications to identify
concealed objects based on their composition.

</details>


### [186] [Reinforcement Learning for Reasoning in Large Language Models with One Training Example](https://arxiv.org/abs/2504.20571)
*Yiping Wang,Qing Yang,Zhiyuan Zeng,Liliang Ren,Lucas Liu,Baolin Peng,Hao Cheng,Xuehai He,Kuan Wang,Jianfeng Gao,Weizhu Chen,Shuohang Wang,Simon Shaolei Du,Yelong Shen*

Main category: cs.LG

TL;DR: 1-shot RLVR通过单训练样本显著提升LLMs数学推理能力，性能接近多样本训练效果。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过少量样本提升LLMs的数学推理能力，验证RLVR的高效性。

Method: 使用1-shot RLVR训练模型，结合GRPO和PPO算法，分析探索策略（如熵损失）的作用。

Result: 单样本训练使MATH500性能从36.0%提升至73.6%，跨模型和算法均表现显著改进。

Conclusion: 1-shot RLVR高效且具泛化性，未来可优化数据效率和探索机制。

Abstract: We show that reinforcement learning with verifiable reward using one training
example (1-shot RLVR) is effective in incentivizing the math reasoning
capabilities of large language models (LLMs). Applying RLVR to the base model
Qwen2.5-Math-1.5B, we identify a single example that elevates model performance
on MATH500 from 36.0% to 73.6%, and improves the average performance across six
common mathematical reasoning benchmarks from 17.6% to 35.7%. This result
matches the performance obtained using the 1.2k DeepScaleR subset (MATH500:
73.6%, average: 35.9%), which includes the aforementioned example. Similar
substantial improvements are observed across various models (Qwen2.5-Math-7B,
Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and
PPO), and different math examples (many of which yield approximately 30% or
greater improvement on MATH500 when employed as a single training example). In
addition, we identify some interesting phenomena during 1-shot RLVR, including
cross-domain generalization, increased frequency of self-reflection, and
sustained test performance improvement even after the training accuracy has
saturated, a phenomenon we term post-saturation generalization. Moreover, we
verify that the effectiveness of 1-shot RLVR primarily arises from the policy
gradient loss, distinguishing it from the "grokking" phenomenon. We also show
the critical role of promoting exploration (e.g., by adding entropy loss with
an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe
that applying entropy loss alone, without any outcome reward, significantly
enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings
can inspire future work on RLVR data efficiency and encourage a re-examination
of both recent progress and the underlying mechanisms in RLVR. Our code, model,
and data are open source at https://github.com/ypwang61/One-Shot-RLVR

</details>


### [187] [Representation Learning Preserving Ignorability and Covariate Matching for Treatment Effects](https://arxiv.org/abs/2504.20579)
*Praharsh Nanavati,Ranjitha Prasad,Karthikeyan Shanmugam*

Main category: cs.LG

TL;DR: 论文提出了一种结合神经架构的方法，同时解决观测数据中的隐藏混杂和协变量不匹配问题，通过梯度匹配和协变量匹配学习有效调整集，并在多个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 观测数据中估计处理效应面临隐藏混杂和协变量不匹配的挑战，现有方法通常只解决其中一种问题。论文旨在提出一个统一框架，同时解决这两个问题。

Method: 结合两种神经架构：一种基于梯度匹配（利用因果侧信息），另一种基于协变量匹配变换。通过学习近似不变的表示，生成有效的调整集。

Result: 实验表明，该方法在IHDP、Jobs、Cattaneo和图像数据集上优于基线，能够提供对真实因果效应的近似估计。

Conclusion: 论文提出的方法通过结合梯度匹配和协变量匹配，有效解决了隐藏混杂和协变量不匹配问题，为因果效应估计提供了新的解决方案。

Abstract: Estimating treatment effects from observational data is challenging due to
two main reasons: (a) hidden confounding, and (b) covariate mismatch (control
and treatment groups not having identical distributions). Long lines of works
exist that address only either of these issues. To address the former,
conventional techniques that require detailed knowledge in the form of causal
graphs have been proposed. For the latter, covariate matching and importance
weighting methods have been used. Recently, there has been progress in
combining testable independencies with partial side information for tackling
hidden confounding. A common framework to address both hidden confounding and
selection bias is missing. We propose neural architectures that aim to learn a
representation of pre-treatment covariates that is a valid adjustment and also
satisfies covariate matching constraints. We combine two different neural
architectures: one based on gradient matching across domains created by
subsampling a suitable anchor variable that assumes causal side information,
followed by the other, a covariate matching transformation. We prove that
approximately invariant representations yield approximate valid adjustment sets
which would enable an interval around the true causal effect. In contrast to
usual sensitivity analysis, where an unknown nuisance parameter is varied, we
have a testable approximation yielding a bound on the effect estimate. We also
outperform various baselines with respect to ATE and PEHE errors on causal
benchmarks that include IHDP, Jobs, Cattaneo, and an image-based Crowd
Management dataset.

</details>


### [188] [Independent Learning in Performative Markov Potential Games](https://arxiv.org/abs/2504.20593)
*Rilind Sahitaj,Paulius Sasnauskas,Yiğit Yalın,Debmalya Mandal,Goran Radanović*

Main category: cs.LG

TL;DR: 本文研究了多智能体表演性强化学习（PRL），提出了表演性稳定均衡（PSE）的概念，并证明了其在合理敏感性假设下的存在性。同时，分析了独立策略梯度上升（IPGA）和独立自然策略梯度（INPG）算法的收敛性，验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 探讨多智能体环境中表演性效应对强化学习的影响，扩展马尔可夫势博弈（MPGs）的理论框架。

Method: 引入表演性稳定均衡（PSE）概念，分析IPGA和INPG算法的收敛性，并通过实验验证。

Result: PSE在合理假设下存在；IPGA和INPG能收敛到近似PSE；INPG在渐近情况下收敛到PSE。

Conclusion: 表演性效应在多智能体强化学习中具有重要意义，理论分析为算法设计提供了支持。

Abstract: Performative Reinforcement Learning (PRL) refers to a scenario in which the
deployed policy changes the reward and transition dynamics of the underlying
environment. In this work, we study multi-agent PRL by incorporating
performative effects into Markov Potential Games (MPGs). We introduce the
notion of a performatively stable equilibrium (PSE) and show that it always
exists under a reasonable sensitivity assumption. We then provide convergence
results for state-of-the-art algorithms used to solve MPGs. Specifically, we
show that independent policy gradient ascent (IPGA) and independent natural
policy gradient (INPG) converge to an approximate PSE in the best-iterate
sense, with an additional term that accounts for the performative effects.
Furthermore, we show that INPG asymptotically converges to a PSE in the
last-iterate sense. As the performative effects vanish, we recover the
convergence rates from prior work. For a special case of our game, we provide
finite-time last-iterate convergence results for a repeated retraining
approach, in which agents independently optimize a surrogate objective. We
conduct extensive experiments to validate our theoretical findings.

</details>


### [189] [Bridging the Generalisation Gap: Synthetic Data Generation for Multi-Site Clinical Model Validation](https://arxiv.org/abs/2504.20635)
*Bradley Segal,Joshua Fieggen,David Clifton,Lei Clifton*

Main category: cs.LG

TL;DR: 提出了一种结构化合成数据框架，用于系统评估临床机器学习模型的鲁棒性、公平性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决临床ML模型在不同医疗环境中泛化能力不足的问题，现有方法依赖真实数据但存在局限。

Method: 开发了一种可控制数据生成过程的结构化合成数据框架，支持对分布变化和偏见的针对性研究。

Result: 框架能有效隔离站点差异影响，支持公平性审计，并揭示模型复杂性如何与站点特异性效应交互。

Conclusion: 该框架为临床ML的可靠部署提供了可重复、可解释且可配置的工具。

Abstract: Ensuring the generalisability of clinical machine learning (ML) models across
diverse healthcare settings remains a significant challenge due to variability
in patient demographics, disease prevalence, and institutional practices.
Existing model evaluation approaches often rely on real-world datasets, which
are limited in availability, embed confounding biases, and lack the flexibility
needed for systematic experimentation. Furthermore, while generative models aim
for statistical realism, they often lack transparency and explicit control over
factors driving distributional shifts. In this work, we propose a novel
structured synthetic data framework designed for the controlled benchmarking of
model robustness, fairness, and generalisability. Unlike approaches focused
solely on mimicking observed data, our framework provides explicit control over
the data generating process, including site-specific prevalence variations,
hierarchical subgroup effects, and structured feature interactions. This
enables targeted investigation into how models respond to specific
distributional shifts and potential biases. Through controlled experiments, we
demonstrate the framework's ability to isolate the impact of site variations,
support fairness-aware audits, and reveal generalisation failures, particularly
highlighting how model complexity interacts with site-specific effects. This
work contributes a reproducible, interpretable, and configurable tool designed
to advance the reliable deployment of ML in clinical settings.

</details>


### [190] [Decision-centric fairness: Evaluation and optimization for resource allocation problems](https://arxiv.org/abs/2504.20642)
*Simon De Vos,Jente Van Belle,Andres Algaba,Wouter Verbeke,Sam Verboven*

Main category: cs.LG

TL;DR: 论文提出了一种决策中心公平性方法，仅在决策区域内引入公平性，避免全局公平性方法对模型预测质量的过度限制。


<details>
  <summary>Details</summary>
Motivation: 数据驱动决策工具可能因预测分数对特定群体产生歧视性行为，导致资源分配不公平。研究旨在通过局部公平性方法解决这一问题。

Method: 提出决策中心公平性方法，仅在相关决策阈值范围内（决策区域）实施公平性，而非全局分数分布。

Result: 实验表明，决策中心方法在多个数据集上优于全局公平性方法，避免了不必要的预测质量下降。

Conclusion: 决策中心公平性方法在资源分配决策中更有效，能在关键区域实现公平性而不损害模型性能。

Abstract: Data-driven decision support tools play an increasingly central role in
decision-making across various domains. In this work, we focus on binary
classification models for predicting positive-outcome scores and deciding on
resource allocation, e.g., credit scores for granting loans or churn propensity
scores for targeting customers with a retention campaign. Such models may
exhibit discriminatory behavior toward specific demographic groups through
their predicted scores, potentially leading to unfair resource allocation. We
focus on demographic parity as a fairness metric to compare the proportions of
instances that are selected based on their positive outcome scores across
groups. In this work, we propose a decision-centric fairness methodology that
induces fairness only within the decision-making region -- the range of
relevant decision thresholds on the score that may be used to decide on
resource allocation -- as an alternative to a global fairness approach that
seeks to enforce parity across the entire score distribution. By restricting
the induction of fairness to the decision-making region, the proposed
decision-centric approach avoids imposing overly restrictive constraints on the
model, which may unnecessarily degrade the quality of the predicted scores. We
empirically compare our approach to a global fairness approach on multiple
(semi-synthetic) datasets to identify scenarios in which focusing on fairness
where it truly matters, i.e., decision-centric fairness, proves beneficial.

</details>


### [191] [Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified File Selection](https://arxiv.org/abs/2504.20644)
*Ziqing Fan,Siyuan Du,Shengchao Hu,Pingjie Wang,Li Shen,Ya Zhang,Dacheng Tao,Yanfeng Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为DiSF的多样化文件选择算法，用于优化大型语言模型的预训练数据选择，解决了传统方法中的多样性困境，显著提升了模型性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 在有限计算预算下，选择高质量的预训练数据对提升大型语言模型的性能至关重要。传统方法依赖目标域相似性选择数据，但会导致特征空间的维度崩溃，影响通用性能。

Method: 提出DiSF算法，通过贪婪算法选择特征空间中相关性最低的文本文件，优化特征协方差矩阵的特征值分布，解决多样性问题。

Result: 在TinyLlama架构上实验，DiSF显著提升了整体性能，节省了98.5%的训练文件，并在50B预算内超越全数据预训练，实现了1.5倍的训练效率和5倍的数据效率。

Conclusion: DiSF算法有效解决了预训练数据选择中的多样性问题，显著提升了模型性能和效率。

Abstract: Selecting high-quality pre-training data for large language models (LLMs) is
crucial for enhancing their overall performance under limited computation
budget, improving both training and sample efficiency. Recent advancements in
file selection primarily rely on using an existing or trained proxy model to
assess the similarity of samples to a target domain, such as high quality
sources BookCorpus and Wikipedia. However, upon revisiting these methods, the
domain-similarity selection criteria demonstrates a diversity dilemma,
i.e.dimensional collapse in the feature space, improving performance on the
domain-related tasks but causing severe degradation on generic performance. To
prevent collapse and enhance diversity, we propose a DiverSified File selection
algorithm (DiSF), which selects the most decorrelated text files in the feature
space. We approach this with a classical greedy algorithm to achieve more
uniform eigenvalues in the feature covariance matrix of the selected texts,
analyzing its approximation to the optimal solution under a formulation of
$\gamma$-weakly submodular optimization problem. Empirically, we establish a
benchmark and conduct extensive experiments on the TinyLlama architecture with
models from 120M to 1.1B parameters. Evaluating across nine tasks from the
Harness framework, DiSF demonstrates a significant improvement on overall
performance. Specifically, DiSF saves 98.5% of 590M training files in
SlimPajama, outperforming the full-data pre-training within a 50B training
budget, and achieving about 1.5x training efficiency and 5x data efficiency.

</details>


### [192] [Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition](https://arxiv.org/abs/2504.20938)
*Zhengfu He,Junxuan Wang,Rui Lin,Xuyang Ge,Wentao Shu,Qiong Tang,Junping Zhang,Xipeng Qiu*

Main category: cs.LG

TL;DR: Lorsa是一种稀疏注意力模型，用于解耦Transformer的多头自注意力机制，揭示更清晰的注意力行为。


<details>
  <summary>Details</summary>
Motivation: 解决注意力叠加问题，理解不同token位置间特征的注意力交互。

Method: 将多头自注意力分解为独立可理解的组件，采用稀疏字典学习方法。

Result: Lorsa发现了更精细的注意力行为（如归纳头、后继头等），并在算术任务中表现优异。

Conclusion: Lorsa在可解释性和电路发现方面优于SAE，适用于多注意力头协同计算的场景。

Abstract: We propose Low-Rank Sparse Attention (Lorsa), a sparse replacement model of
Transformer attention layers to disentangle original Multi Head Self Attention
(MHSA) into individually comprehensible components. Lorsa is designed to
address the challenge of attention superposition to understand
attention-mediated interaction between features in different token positions.
We show that Lorsa heads find cleaner and finer-grained versions of previously
discovered MHSA behaviors like induction heads, successor heads and attention
sink behavior (i.e., heavily attending to the first token). Lorsa and Sparse
Autoencoder (SAE) are both sparse dictionary learning methods applied to
different Transformer components, and lead to consistent findings in many ways.
For instance, we discover a comprehensive family of arithmetic-specific Lorsa
heads, each corresponding to an atomic operation in Llama-3.1-8B. Automated
interpretability analysis indicates that Lorsa achieves parity with SAE in
interpretability while Lorsa exhibits superior circuit discovery properties,
especially for features computed collectively by multiple MHSA heads. We also
conduct extensive experiments on architectural design ablation, Lorsa scaling
law and error analysis.

</details>


### [193] [RuleKit 2: Faster and simpler rule learning](https://arxiv.org/abs/2504.20650)
*Adam Gudyś,Cezary Maszczyk,Joanna Badura,Adam Grzelak,Marek Sikora,Łukasz Wróbel*

Main category: cs.LG

TL;DR: RuleKit 2是一个基于规则的数据分析工具包，通过新算法和优化实现显著提升计算性能，并新增Python包和浏览器应用以增强易用性。


<details>
  <summary>Details</summary>
Motivation: 规则结合了预测和描述能力，RuleKit在分类、回归和生存问题中表现出色，因此推出第二版以进一步提升性能和扩展功能。

Method: RuleKit 2通过新算法和优化实现提升性能，同时新增Python包和浏览器应用以支持scikit-learn集成和图形化操作。

Result: 计算性能显著提升，部分数据集分析时间减少两个数量级；新增的Python包和浏览器应用增强了易用性。

Conclusion: RuleKit 2是一个高效且易用的规则分析工具，适用于多种数据挖掘任务，并通过开源许可提供。

Abstract: Rules offer an invaluable combination of predictive and descriptive
capabilities. Our package for rule-based data analysis, RuleKit, has proven its
effectiveness in classification, regression, and survival problems. Here we
present its second version. New algorithms and optimized implementations of
those previously included, significantly improved the computational performance
of our suite, reducing the analysis time of some data sets by two orders of
magnitude. The usability of RuleKit 2 is provided by two new components: Python
package and browser application with a graphical user interface. The former
complies with scikit-learn, the most popular data mining library for Python,
allowing RuleKit 2 to be straightforwardly integrated into existing data
analysis pipelines. RuleKit 2 is available at GitHub under GNU AGPL 3 license
(https://github.com/adaa-polsl/RuleKit)

</details>


### [194] [Federated learning, ethics, and the double black box problem in medical AI](https://arxiv.org/abs/2504.20656)
*Joshua Hatherley,Anders Søgaard,Angela Ballantyne,Ruben Pauwels*

Main category: cs.LG

TL;DR: 本文探讨了医疗联邦学习（FL）的伦理风险，指出其存在“联邦不透明性”和双重黑箱问题，并强调需克服关键挑战以实现FL在医学中的伦理可行性。


<details>
  <summary>Details</summary>
Motivation: 医疗FL虽能保护患者隐私，但其伦理风险尚未充分研究。本文旨在填补这一空白。

Method: 通过分析医疗FL的不透明性和双重黑箱问题，探讨其潜在伦理风险。

Result: 发现医疗FL的预期益处可能被夸大，并存在独特的伦理挑战。

Conclusion: 需解决关键挑战，才能使医疗FL在伦理上可行。

Abstract: Federated learning (FL) is a machine learning approach that allows multiple
devices or institutions to collaboratively train a model without sharing their
local data with a third-party. FL is considered a promising way to address
patient privacy concerns in medical artificial intelligence. The ethical risks
of medical FL systems themselves, however, have thus far been underexamined.
This paper aims to address this gap. We argue that medical FL presents a new
variety of opacity -- federation opacity -- that, in turn, generates a
distinctive double black box problem in healthcare AI. We highlight several
instances in which the anticipated benefits of medical FL may be exaggerated,
and conclude by highlighting key challenges that must be overcome to make FL
ethically feasible in medicine.

</details>


### [195] [Quantum-Enhanced Hybrid Reinforcement Learning Framework for Dynamic Path Planning in Autonomous Systems](https://arxiv.org/abs/2504.20660)
*Sahil Tomar,Shamshe Alam,Sandeep Kumar,Amit Mathur*

Main category: cs.LG

TL;DR: 提出了一种量子经典混合框架，结合量子计算与经典强化学习，显著减少训练时间并提升适应性。


<details>
  <summary>Details</summary>
Motivation: 利用量子计算的并行性提升强化学习的效率和适应性，以应对复杂环境中的导航问题。

Method: 通过量子计算生成鲁棒的Q表和专用转向成本估计，并与经典强化学习流程结合。

Result: 模拟和实际测试显示，路径效率、轨迹平滑度和任务成功率显著提升。

Conclusion: 该框架在复杂环境中具有实时自主导航的潜力。

Abstract: In this paper, a novel quantum classical hybrid framework is proposed that
synergizes quantum with Classical Reinforcement Learning. By leveraging the
inherent parallelism of quantum computing, the proposed approach generates
robust Q tables and specialized turn cost estimations, which are then
integrated with a classical Reinforcement Learning pipeline. The Classical
Quantum fusion results in rapid convergence of training, reducing the training
time significantly and improved adaptability in scenarios featuring static,
dynamic, and moving obstacles. Simulator based evaluations demonstrate
significant enhancements in path efficiency, trajectory smoothness, and mission
success rates, underscoring the potential of framework for real time,
autonomous navigation in complex and unpredictable environments. Furthermore,
the proposed framework was tested beyond simulations on practical scenarios,
including real world map data such as the IIT Delhi campus, reinforcing its
potential for real time, autonomous navigation in complex and unpredictable
environments.

</details>


### [196] [SFi-Former: Sparse Flow Induced Attention for Graph Transformer](https://arxiv.org/abs/2504.20666)
*Zhonghao Li,Ji Shi,Xinming Zhang,Miao Zhang,Bo Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为SFi-attention的新型注意力机制，通过稀疏化注意力模式解决图变换器中的过拟合和过度全局化问题，并设计了SFi-Former模型，在多个图数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 图变换器在处理长程依赖图数据时表现优于传统图神经网络，但存在弱归纳偏置、过拟合和过度全局化问题。

Method: 提出SFi-attention，通过基于网络流的能量函数和l1正则化学习稀疏注意力模式，并设计SFi-Former模型。

Result: SFi-Former在GNN Benchmark和LRGB数据集上表现优异，泛化能力更强。

Conclusion: SFi-Former通过稀疏注意力机制有效解决了图变换器的问题，提升了模型性能。

Abstract: Graph Transformers (GTs) have demonstrated superior performance compared to
traditional message-passing graph neural networks in many studies, especially
in processing graph data with long-range dependencies. However, GTs tend to
suffer from weak inductive bias, overfitting and over-globalizing problems due
to the dense attention. In this paper, we introduce SFi-attention, a novel
attention mechanism designed to learn sparse pattern by minimizing an energy
function based on network flows with l1-norm regularization, to relieve those
issues caused by dense attention. Furthermore, SFi-Former is accordingly
devised which can leverage the sparse attention pattern of SFi-attention to
generate sparse network flows beyond adjacency matrix of graph data.
Specifically, SFi-Former aggregates features selectively from other nodes
through flexible adaptation of the sparse attention, leading to a more robust
model. We validate our SFi-Former on various graph datasets, especially those
graph data exhibiting long-range dependencies. Experimental results show that
our SFi-Former obtains competitive performance on GNN Benchmark datasets and
SOTA performance on LongRange Graph Benchmark (LRGB) datasets. Additionally,
our model gives rise to smaller generalization gaps, which indicates that it is
less prone to over-fitting. Click here for codes.

</details>


### [197] [Explanations Go Linear: Interpretable and Individual Latent Encoding for Post-hoc Explainability](https://arxiv.org/abs/2504.20667)
*Simone Piaggesi,Riccardo Guidotti,Fosca Giannotti,Dino Pedreschi*

Main category: cs.LG

TL;DR: ILLUME是一个基于表示学习的灵活解释框架，结合全局代理模型和实例特定线性变换，提供准确且鲁棒的局部和全局解释。


<details>
  <summary>Details</summary>
Motivation: 解决传统代理方法在局部非线性捕捉和全局效率上的局限性。

Method: 结合全局代理模型与元编码器学习的实例特定线性变换。

Result: ILLUME生成的特征归因和决策规则准确、鲁棒且忠实于黑盒模型。

Conclusion: ILLUME为传统代理方法提供了一种统一的解释框架，有效解决了其局限性。

Abstract: Post-hoc explainability is essential for understanding black-box machine
learning models. Surrogate-based techniques are widely used for local and
global model-agnostic explanations but have significant limitations. Local
surrogates capture non-linearities but are computationally expensive and
sensitive to parameters, while global surrogates are more efficient but
struggle with complex local behaviors. In this paper, we present ILLUME, a
flexible and interpretable framework grounded in representation learning, that
can be integrated with various surrogate models to provide explanations for any
black-box classifier. Specifically, our approach combines a globally trained
surrogate with instance-specific linear transformations learned with a
meta-encoder to generate both local and global explanations. Through extensive
empirical evaluations, we demonstrate the effectiveness of ILLUME in producing
feature attributions and decision rules that are not only accurate but also
robust and faithful to the black-box, thus providing a unified explanation
framework that effectively addresses the limitations of traditional surrogate
methods.

</details>


### [198] [What's Wrong with Your Synthetic Tabular Data? Using Explainable AI to Evaluate Generative Models](https://arxiv.org/abs/2504.20687)
*Jan Kapar,Niklas Koenen,Martin Jullum*

Main category: cs.LG

TL;DR: 论文提出了一种利用可解释AI（XAI）技术评估合成表格数据质量的方法，通过分析分类器的特征重要性和特征效应，揭示合成数据的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据评估指标存在冲突且无法具体指出数据弱点，需要更透明和深入的分析方法。

Method: 应用XAI技术（如特征重要性、部分依赖图、Shapley值和反事实解释）分析二元检测分类器，揭示合成数据的分布差异。

Result: 方法在两种表格数据和生成模型上验证，发现了传统评估技术忽略的问题。

Conclusion: XAI技术提高了合成数据评估的透明度和洞察力，有助于诊断和改进数据质量。

Abstract: Evaluating synthetic tabular data is challenging, since they can differ from
the real data in so many ways. There exist numerous metrics of synthetic data
quality, ranging from statistical distances to predictive performance, often
providing conflicting results. Moreover, they fail to explain or pinpoint the
specific weaknesses in the synthetic data. To address this, we apply
explainable AI (XAI) techniques to a binary detection classifier trained to
distinguish real from synthetic data. While the classifier identifies
distributional differences, XAI concepts such as feature importance and feature
effects, analyzed through methods like permutation feature importance, partial
dependence plots, Shapley values and counterfactual explanations, reveal why
synthetic data are distinguishable, highlighting inconsistencies, unrealistic
dependencies, or missing patterns. This interpretability increases transparency
in synthetic data evaluation and provides deeper insights beyond conventional
metrics, helping diagnose and improve synthetic data quality. We apply our
approach to two tabular datasets and generative models, showing that it
uncovers issues overlooked by standard evaluation techniques.

</details>


### [199] [Unsupervised Surrogate Anomaly Detection](https://arxiv.org/abs/2504.20733)
*Simon Klüttermann,Tim Katzke,Emmanuel Müller*

Main category: cs.LG

TL;DR: 提出了一种名为DEAN的无监督异常检测算法，通过神经网络学习正常数据的模式，并在121个基准数据集上验证其性能优于19种现有方法。


<details>
  <summary>Details</summary>
Motivation: 受工程中类似概念的启发，研究如何通过神经网络学习正常数据的模式以检测异常。

Method: 提出DEAN算法，基于一组用于优化代理模型的公理设计，旨在满足这些标准。

Result: 在121个基准数据集上验证，DEAN性能优于19种现有方法，且具有可扩展性和可靠性。

Conclusion: DEAN是一种高效且可靠的无监督异常检测方法，适用于大规模数据。

Abstract: In this paper, we study unsupervised anomaly detection algorithms that learn
a neural network representation, i.e. regular patterns of normal data, which
anomalies are deviating from. Inspired by a similar concept in engineering, we
refer to our methodology as surrogate anomaly detection. We formalize the
concept of surrogate anomaly detection into a set of axioms required for
optimal surrogate models and propose a new algorithm, named DEAN (Deep Ensemble
ANomaly detection), designed to fulfill these criteria. We evaluate DEAN on 121
benchmark datasets, demonstrating its competitive performance against 19
existing methods, as well as the scalability and reliability of our method.

</details>


### [200] [Intelligent Task Offloading in VANETs: A Hybrid AI-Driven Approach for Low-Latency and Energy Efficiency](https://arxiv.org/abs/2504.20735)
*Tariq Qayyum,Asadullah Tariq,Muhammad Ali,Mohamed Adel Serhani,Zouheir Trabelsi,Maite López-Sánchez*

Main category: cs.LG

TL;DR: 论文提出了一种混合AI框架，结合监督学习、强化学习和粒子群优化，用于VANET中的任务卸载和资源分配，显著降低了延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: VANET的高动态性导致网络条件不稳定、高延迟、能效低和任务失败等问题，需要一种高效的任务卸载和资源分配方法。

Method: 提出混合AI框架，结合监督学习预测最优卸载策略，强化学习实现自适应决策，PSO优化延迟和能耗。

Result: 仿真显示框架显著降低延迟和能耗，提高任务成功率和网络吞吐量。

Conclusion: 该框架为动态车载环境中的实时应用提供了高效、可扩展的解决方案。

Abstract: Vehicular Ad-hoc Networks (VANETs) are integral to intelligent transportation
systems, enabling vehicles to offload computational tasks to nearby roadside
units (RSUs) and mobile edge computing (MEC) servers for real-time processing.
However, the highly dynamic nature of VANETs introduces challenges, such as
unpredictable network conditions, high latency, energy inefficiency, and task
failure. This research addresses these issues by proposing a hybrid AI
framework that integrates supervised learning, reinforcement learning, and
Particle Swarm Optimization (PSO) for intelligent task offloading and resource
allocation. The framework leverages supervised models for predicting optimal
offloading strategies, reinforcement learning for adaptive decision-making, and
PSO for optimizing latency and energy consumption. Extensive simulations
demonstrate that the proposed framework achieves significant reductions in
latency and energy usage while improving task success rates and network
throughput. By offering an efficient, and scalable solution, this framework
sets the foundation for enhancing real-time applications in dynamic vehicular
environments.

</details>


### [201] [DDPS: Discrete Diffusion Posterior Sampling for Paths in Layered Graphs](https://arxiv.org/abs/2504.20754)
*Hao Luan,See-Kiong Ng,Chun Kai Ling*

Main category: cs.LG

TL;DR: 本文研究了如何在分层图中生成路径，并确保生成的样本是有效路径，提出了一种名为PALM的表示方法，并通过分类器引导优化生成结果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成模型中表现优异，但现有方法很少处理生成样本的显式约束问题，特别是在分层图中生成路径时。

Method: 提出了一种名为PALM的路径表示方法，并结合分类器引导技术，无需重新训练扩散模型即可优化路径生成。

Result: 初步实验表明，该方法在生成满足路径约束的样本上优于其他方法。

Conclusion: PALM表示和分类器引导的结合为约束路径生成提供了一种有效解决方案。

Abstract: Diffusion models form an important class of generative models today,
accounting for much of the state of the art in cutting edge AI research. While
numerous extensions beyond image and video generation exist, few of such
approaches address the issue of explicit constraints in the samples generated.
In this paper, we study the problem of generating paths in a layered graph (a
variant of a directed acyclic graph) using discrete diffusion models, while
guaranteeing that our generated samples are indeed paths. Our approach utilizes
a simple yet effective representation for paths which we call the padded
adjacency-list matrix (PALM). In addition, we show how to effectively perform
classifier guidance, which helps steer the sampled paths to specific preferred
edges without any retraining of the diffusion model. Our preliminary results
show that empirically, our method outperforms alternatives which do not
explicitly account for path constraints.

</details>


### [202] [JTreeformer: Graph-Transformer via Latent-Diffusion Model for Molecular Generation](https://arxiv.org/abs/2504.20770)
*Ji Shi,Chengxun Xie,Zhonghao Li,Xinming Zhang,Miao Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为JTreeformer的图变换器框架，用于分子生成，通过结合GCN和多头注意力作为编码器，以及有向无环GCN作为解码器，显著提升了分子生成的效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于变换器的图解码器难以有效利用图信息，限制了其在分子生成中的应用。本文旨在解决这一问题。

Method: 结合GCN与多头注意力作为编码器，集成有向无环GCN到变换器中作为解码器，并在潜在空间中插入扩散模型以增强采样效率。

Result: 实验结果表明，JTreeformer在分子生成任务中优于现有方法。

Conclusion: JTreeformer为药物发现提供了一种有前景的新工具。

Abstract: The discovery of new molecules based on the original chemical molecule
distributions is of great importance in medicine. The graph transformer, with
its advantages of high performance and scalability compared to traditional
graph networks, has been widely explored in recent research for applications of
graph structures. However, current transformer-based graph decoders struggle to
effectively utilize graph information, which limits their capacity to leverage
only sequences of nodes rather than the complex topological structures of
molecule graphs. This paper focuses on building a graph transformer-based
framework for molecular generation, which we call \textbf{JTreeformer} as it
transforms graph generation into junction tree generation. It combines GCN
parallel with multi-head attention as the encoder. It integrates a directed
acyclic GCN into a graph-based Transformer to serve as a decoder, which can
iteratively synthesize the entire molecule by leveraging information from the
partially constructed molecular structure at each step. In addition, a
diffusion model is inserted in the latent space generated by the encoder, to
enhance the efficiency and effectiveness of sampling further. The empirical
results demonstrate that our novel framework outperforms existing molecule
generation methods, thus offering a promising tool to advance drug discovery
(https://anonymous.4open.science/r/JTreeformer-C74C).

</details>


### [203] [Evaluating Effects of Augmented SELFIES for Molecular Understanding Using QK-LSTM](https://arxiv.org/abs/2504.20789)
*Collin Beaudoin,Swaroop Ghosh*

Main category: cs.LG

TL;DR: 论文提出了一种混合量子-经典模型QK-LSTM，用于药物开发中的分子特性和副作用预测，首次分析了增强SMILES和SELFIES在量子领域的作用。


<details>
  <summary>Details</summary>
Motivation: 药物开发中分子特性和副作用的识别耗时且关键，传统方法存在风险，机器学习尤其是量子-经典混合模型提供了创新解决方案。

Method: QK-LSTM模型结合量子核函数和经典LSTM框架，利用高维量子特征空间捕获序列数据的复杂模式，并探索了增强SMILES和SELFIES的作用。

Result: 增强SELFIES在经典和量子-经典混合领域分别带来5.97%和5.91%的显著性能提升。

Conclusion: QK-LSTM和增强SELFIES为分子特性预测提供了新思路，展示了量子计算在药物开发中的潜力。

Abstract: Identifying molecular properties, including side effects, is a critical yet
time-consuming step in drug development. Failing to detect these side effects
before regulatory submission can result in significant financial losses and
production delays, and overlooking them during the regulatory review can lead
to catastrophic consequences. This challenge presents an opportunity for
innovative machine learning approaches, particularly hybrid quantum-classical
models like the Quantum Kernel-Based Long Short-Term Memory (QK-LSTM) network.
The QK-LSTM integrates quantum kernel functions into the classical LSTM
framework, enabling the capture of complex, non-linear patterns in sequential
data. By mapping input data into a high-dimensional quantum feature space, the
QK-LSTM model reduces the need for large parameter sets, allowing for model
compression without sacrificing accuracy in sequence-based tasks. Recent
advancements have been made in the classical domain using augmented variations
of the Simplified Molecular Line-Entry System (SMILES). However, to the best of
our knowledge, no research has explored the impact of augmented SMILES in the
quantum domain, nor the role of augmented Self-Referencing Embedded Strings
(SELFIES) in either classical or hybrid quantum-classical settings. This study
presents the first analysis of these approaches, providing novel insights into
their potential for enhancing molecular property prediction and side effect
identification. Results reveal that augmenting SELFIES yields in statistically
significant improvements from SMILES by a 5.97% improvement for the classical
domain and a 5.91% improvement for the hybrid quantum-classical domain.

</details>


### [204] [Q-Fusion: Diffusing Quantum Circuits](https://arxiv.org/abs/2504.20794)
*Collin Beaudoin,Swaroop Ghosh*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散的算法，利用LayerDAG框架自动生成量子电路，解决了当前量子计算设备在比特数和门数上的限制问题。


<details>
  <summary>Details</summary>
Motivation: 量子计算和量子机器学习潜力巨大，但受限于当前NISQ设备的限制和量子算法设计的复杂性。量子架构搜索（QAS）旨在通过自动化减少人工干预。

Method: 采用扩散算法和LayerDAG框架生成量子电路，区别于其他使用LLM、RL或VAE的方法。

Result: 实验表明，该模型能100%生成有效的量子电路输出。

Conclusion: 提出的方法在自动化量子电路设计方面具有显著优势，为量子计算的发展提供了新思路。

Abstract: Quantum computing holds great potential for solving socially relevant and
computationally complex problems. Furthermore, quantum machine learning (QML)
promises to rapidly improve our current machine learning capabilities. However,
current noisy intermediate-scale quantum (NISQ) devices are constrained by
limitations in the number of qubits and gate counts, which hinder their full
capabilities. Furthermore, the design of quantum algorithms remains a laborious
task, requiring significant domain expertise and time. Quantum Architecture
Search (QAS) aims to streamline this process by automatically generating novel
quantum circuits, reducing the need for manual intervention. In this paper, we
propose a diffusion-based algorithm leveraging the LayerDAG framework to
generate new quantum circuits. This method contrasts with other approaches that
utilize large language models (LLMs), reinforcement learning (RL), variational
autoencoders (VAE), and similar techniques. Our results demonstrate that the
proposed model consistently generates 100% valid quantum circuit outputs.

</details>


### [205] [The When and How of Target Variable Transformations](https://arxiv.org/abs/2504.20821)
*Loren Nuyts,Jesse Davis*

Main category: cs.LG

TL;DR: 论文强调目标变量变换在机器学习中的重要性，并提出实用指南。


<details>
  <summary>Details</summary>
Motivation: 现有文献对目标变量变换的关注不足，但其对模型学习有显著影响。

Method: 通过案例展示目标变量变换的实用性，提出通用规则和建议。

Result: 目标变量变换能显著提升模型性能，需根据情境选择合适的变换。

Conclusion: 目标变量变换是数据准备的关键步骤，应得到更多重视和实践。

Abstract: The machine learning pipeline typically involves the iterative process of (1)
collecting the data, (2) preparing the data, (3) learning a model, and (4)
evaluating a model. Practitioners recognize the importance of the data
preparation phase in terms of its impact on the ability to learn accurate
models. In this regard, significant attention is often paid to manipulating the
feature set (e.g., selection, transformations, dimensionality reduction). A
point that is less well appreciated is that transformations on the target
variable can also have a large impact on whether it is possible to learn a
suitable model. These transformations may include accounting for
subject-specific biases (e.g., in how someone uses a rating scale), contexts
(e.g., population size effects), and general trends (e.g., inflation). However,
this point has received a much more cursory treatment in the existing
literature. The goal of this paper is three-fold. First, we aim to highlight
the importance of this problem by showing when transforming the target variable
has been useful in practice. Second, we will provide a set of generic ``rules
of thumb'' that indicate situations when transforming the target variable may
be needed. Third, we will discuss which transformations should be considered in
a given situation.

</details>


### [206] [An approach to melodic segmentation and classification based on filtering with the Haar-wavelet](https://arxiv.org/abs/2504.20822)
*Gissel Velarde,Tillman Weyde,David Meredith*

Main category: cs.LG

TL;DR: 提出了一种基于Haar小波变换的旋律分类与分割方法，在巴赫作品和荷兰民谣分类任务中表现优于未滤波音高信号，但不及多特征字符串匹配方法。


<details>
  <summary>Details</summary>
Motivation: 解决旋律分类与分割问题，探索Haar小波变换在音乐分析中的应用。

Method: 使用Haar小波滤波音高信号，通过局部极值或过零点分割，再用k近邻算法分类。

Result: 在巴赫作品分类中优于未滤波信号，荷兰民谣分类中与音高信号相当，但不如多特征字符串匹配方法。

Conclusion: Haar小波方法在特定任务中有效，但多特征方法表现更优。

Abstract: We present a novel method of classification and segmentation of melodies in
symbolic representation. The method is based on filtering pitch as a signal
over time with the Haar-wavelet, and we evaluate it on two tasks. The filtered
signal corresponds to a single-scale signal ws from the continuous Haar wavelet
transform. The melodies are first segmented using local maxima or
zero-crossings of w_s. The segments of w_s are then classified using the
k-nearest neighbour algorithm with Euclidian and city-block distances. The
method proves more effective than using unfiltered pitch signals and
Gestalt-based segmentation when used to recognize the parent works of segments
from Bach's Two-Part Inventions (BWV 772-786). When used to classify 360 Dutch
folk tunes into 26 tune families, the performance of the method is comparable
to the use of pitch signals, but not as good as that of string-matching methods
based on multiple features.

</details>


### [207] [Hybrid Quantum Recurrent Neural Network For Remaining Useful Life Prediction](https://arxiv.org/abs/2504.20823)
*Olga Tsurkan,Aleksandra Konstantinova,Aleksandr Sedykh,Dmitrii Zhiganov,Arsenii Senokosov,Daniil Tarpanov,Matvei Anoshin,Leonid Fedichkin*

Main category: cs.LG

TL;DR: 论文提出了一种混合量子循环神经网络框架，用于预测喷气发动机剩余使用寿命，相比传统方法在误差上有所提升。


<details>
  <summary>Details</summary>
Motivation: 航空航天领域的预测性维护需要准确估计喷气发动机的剩余使用寿命，现有方法在高频成分学习上存在不足。

Method: 结合量子长短期记忆层和经典密集层，利用量子深度注入电路替代传统线性变换，提升高频成分学习能力。

Result: 混合量子循环神经网络在均方根误差和平均绝对误差上比传统循环神经网络提升5%，且优于随机森林、卷积神经网络和多层感知器等基线方法。

Conclusion: 混合量子-经典方法在有限数据条件下具有潜力，为预测性维护任务提供了新思路。

Abstract: Predictive maintenance in aerospace heavily relies on accurate estimation of
the remaining useful life of jet engines. In this paper, we introduce a Hybrid
Quantum Recurrent Neural Network framework, combining Quantum Long Short-Term
Memory layers with classical dense layers for Remaining Useful Life forecasting
on NASA's Commercial Modular Aero-Propulsion System Simulation dataset. Each
Quantum Long Short-Term Memory gate replaces conventional linear
transformations with Quantum Depth-Infused circuits, allowing the network to
learn high-frequency components more effectively. Experimental results
demonstrate that, despite having fewer trainable parameters, the Hybrid Quantum
Recurrent Neural Network achieves up to a 5% improvement over a Recurrent
Neural Network based on stacked Long Short-Term Memory layers in terms of mean
root mean squared error and mean absolute error. Moreover, a thorough
comparison of our method with established techniques, including Random Forest,
Convolutional Neural Network, and Multilayer Perceptron, demonstrates that our
approach, which achieves a Root Mean Squared Error of 15.46, surpasses these
baselines by approximately 13.68%, 16.21%, and 7.87%, respectively.
Nevertheless, it remains outperformed by certain advanced joint architectures.
Our findings highlight the potential of hybrid quantum-classical approaches for
robust time-series forecasting under limited data conditions, offering new
avenues for enhancing reliability in predictive maintenance tasks.

</details>


### [208] [Reinforcement Learning for LLM Reasoning Under Memory Constraints](https://arxiv.org/abs/2504.20834)
*Alan Lee,Harry Tong*

Main category: cs.LG

TL;DR: 论文探讨了在内存和计算受限条件下，通过强化学习提升大语言模型推理能力的方法，提出了两种高效方法S-GRPO和T-SPMO，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 在学术环境中，资源受限（如单块40GB GPU）的情况下，如何通过强化学习优化大语言模型的推理能力。

Method: 提出了S-GRPO（内存高效的GRPO变体）和T-SPMO（基于令牌前缀匹配的细粒度信用分配策略），并结合LoRA微调。

Result: 在SVAMP基准测试中，模型准确率从46%提升至70%以上；T-SPMO在多位数乘法任务中表现优异。

Conclusion: 在硬件受限条件下，RL微调具有潜力，且内存高效方法可能起到正则化作用，稳定训练过程。

Abstract: We explore reinforcement learning (RL) techniques to enhance reasoning within
targeted problem spaces in large language models (LLMs) under memory and
compute constraints. Our focus is on critic-free methods compatible with LoRA
fine-tuning on a single 40GB GPU, a common limitation in academic settings. We
introduce S-GRPO, a memory-efficient variant of Group Relative Policy
Optimization, and T-SPMO, a token-level prefix matching strategy for
fine-grained credit assignment. Despite limited resources, when used to
fine-tune Qwen2-1.5B both methods significantly improve SVAMP benchmark
accuracy from 46% to above 70% using LoRA training. T-SPMO also excels in
multi-digit multiplication tasks, underscoring the potential of RL fine-tuning
under hardware constraints. Additionally, we find that our full-token GRPO
baseline under LoRA fine-tuning did not improve model performance (compared to
base model) on either task, suggesting that our memory-efficient methods may
act as a form of regularization that stabilizes training when only a small
subset of parameters are updated.

</details>


### [209] [Mitigating the Structural Bias in Graph Adversarial Defenses](https://arxiv.org/abs/2504.20848)
*Junyuan Fang,Huimin Liu,Han Yang,Jiajing Wu,Zibin Zheng,Chi K. Tse*

Main category: cs.LG

TL;DR: 论文提出了一种针对图神经网络（GNNs）对抗攻击的防御策略，通过异构-同构增强图构建、kNN增强图构建和多视角节点注意力模块，减少GNNs在低度节点上的结构偏差。


<details>
  <summary>Details</summary>
Motivation: 现有GNN防御方法对低度节点（尾部节点）的防御能力存在结构偏差，类似于传统GNN在干净图中的问题。

Method: 提出异构-同构增强图构建（移除异质链接，添加同质链接）、kNN增强图构建和多视角节点注意力模块。

Result: 实验证明该策略在基准数据集上具有防御和去偏效果。

Conclusion: 该策略有效提升了GNNs对对抗攻击的鲁棒性，并减少了结构偏差。

Abstract: In recent years, graph neural networks (GNNs) have shown great potential in
addressing various graph structure-related downstream tasks. However, recent
studies have found that current GNNs are susceptible to malicious adversarial
attacks. Given the inevitable presence of adversarial attacks in the real
world, a variety of defense methods have been proposed to counter these attacks
and enhance the robustness of GNNs. Despite the commendable performance of
these defense methods, we have observed that they tend to exhibit a structural
bias in terms of their defense capability on nodes with low degree (i.e., tail
nodes), which is similar to the structural bias of traditional GNNs on nodes
with low degree in the clean graph. Therefore, in this work, we propose a
defense strategy by including hetero-homo augmented graph construction, $k$NN
augmented graph construction, and multi-view node-wise attention modules to
mitigate the structural bias of GNNs against adversarial attacks. Notably, the
hetero-homo augmented graph consists of removing heterophilic links (i.e.,
links connecting nodes with dissimilar features) globally and adding homophilic
links (i.e., links connecting nodes with similar features) for nodes with low
degree. To further enhance the defense capability, an attention mechanism is
adopted to adaptively combine the representations from the above two kinds of
graph views. We conduct extensive experiments to demonstrate the defense and
debiasing effect of the proposed strategy on benchmark datasets.

</details>


### [210] [Tabular Data Adapters: Improving Outlier Detection for Unlabeled Private Data](https://arxiv.org/abs/2504.20862)
*Dayananda Herurkar,Jörn Hees,Vesselin Tzvetkov,Andreas Dengel*

Main category: cs.LG

TL;DR: 提出了一种名为Tabular Data Adapters（TDA）的新方法，用于为无标签的表格数据生成软标签，以解决私有数据集在异常检测任务中的冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 解决在私有数据集上应用深度学习时面临的标签缺失、领域偏移和结构差异等问题。

Method: 通过识别统计上相似的公共数据集，并利用共享自编码器将私有数据转换为与公共模型兼容的格式，生成弱标签。

Result: 在50个不同领域的表格数据集上实验表明，该方法比基线方法提供更准确的标注，同时减少计算时间。

Conclusion: TDA提供了一种可扩展、高效且经济的方法，弥合了公共研究模型与工业应用之间的差距。

Abstract: The remarkable success of Deep Learning approaches is often based and
demonstrated on large public datasets. However, when applying such approaches
to internal, private datasets, one frequently faces challenges arising from
structural differences in the datasets, domain shift, and the lack of labels.
In this work, we introduce Tabular Data Adapters (TDA), a novel method for
generating soft labels for unlabeled tabular data in outlier detection tasks.
By identifying statistically similar public datasets and transforming private
data (based on a shared autoencoder) into a format compatible with
state-of-the-art public models, our approach enables the generation of weak
labels. It thereby can help to mitigate the cold start problem of labeling by
basing on existing outlier detection models for public datasets. In experiments
on 50 tabular datasets across different domains, we demonstrate that our method
is able to provide more accurate annotations than baseline approaches while
reducing computational time. Our approach offers a scalable, efficient, and
cost-effective solution, to bridge the gap between public research models and
real-world industrial applications.

</details>


### [211] [Quantifying the Noise of Structural Perturbations on Graph Adversarial Attacks](https://arxiv.org/abs/2504.20869)
*Junyuan Fang,Han Yang,Haixian Wen,Jiajing Wu,Zibin Zheng,Chi K. Tse*

Main category: cs.LG

TL;DR: 该论文提出了一种基于噪声的攻击强度量化方法，并设计了三种攻击策略，以提高图神经网络的对抗攻击可解释性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络对抗攻击研究多关注攻击性能优化，而忽略了攻击强度的量化，导致攻击选择缺乏可解释性。

Method: 提出噪声概念量化攻击强度，并基于噪声和分类边界设计单步和多步优化的三种攻击策略。

Result: 在基准数据集上对三种代表性图神经网络进行实验，验证了攻击策略的有效性，并分析了有效对抗扰动的偏好模式。

Conclusion: 通过噪声量化和攻击策略设计，提升了对抗攻击的可解释性和有效性，为图神经网络的安全性研究提供了新视角。

Abstract: Graph neural networks have been widely utilized to solve graph-related tasks
because of their strong learning power in utilizing the local information of
neighbors. However, recent studies on graph adversarial attacks have proven
that current graph neural networks are not robust against malicious attacks.
Yet much of the existing work has focused on the optimization objective based
on attack performance to obtain (near) optimal perturbations, but paid less
attention to the strength quantification of each perturbation such as the
injection of a particular node/link, which makes the choice of perturbations a
black-box model that lacks interpretability. In this work, we propose the
concept of noise to quantify the attack strength of each adversarial link.
Furthermore, we propose three attack strategies based on the defined noise and
classification margins in terms of single and multiple steps optimization.
Extensive experiments conducted on benchmark datasets against three
representative graph neural networks demonstrate the effectiveness of the
proposed attack strategies. Particularly, we also investigate the preferred
patterns of effective adversarial perturbations by analyzing the corresponding
properties of the selected perturbation nodes.

</details>


### [212] [Return Capping: Sample-Efficient CVaR Policy Gradient Optimisation](https://arxiv.org/abs/2504.20887)
*Harry Mead,Clarissa Costen,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TL;DR: 论文提出了一种改进的条件风险价值（CVaR）优化方法，通过限制训练轨迹的总回报而非丢弃，提高了样本效率。


<details>
  <summary>Details</summary>
Motivation: 当前方法在优化CVaR时丢弃大量轨迹，导致样本效率低下。

Method: 重新表述CVaR优化问题，限制训练轨迹的总回报而非丢弃，证明在适当限制下与原问题等价。

Result: 在多个环境中实验表明，该方法性能优于基线。

Conclusion: 通过限制回报而非丢弃轨迹，显著提升了CVaR优化的样本效率和性能。

Abstract: When optimising for conditional value at risk (CVaR) using policy gradients
(PG), current methods rely on discarding a large proportion of trajectories,
resulting in poor sample efficiency. We propose a reformulation of the CVaR
optimisation problem by capping the total return of trajectories used in
training, rather than simply discarding them, and show that this is equivalent
to the original problem if the cap is set appropriately. We show, with
empirical results in an number of environments, that this reformulation of the
problem results in consistently improved performance compared to baselines.

</details>


### [213] [Does Feedback Help in Bandits with Arm Erasures?](https://arxiv.org/abs/2504.20894)
*Merve Karakas,Osama Hanna,Lin F. Yang,Christina Fragouli*

Main category: cs.LG

TL;DR: 研究了分布式多臂老虎机问题，通过反馈机制分析臂擦除信道的影响，发现反馈并未显著改善最坏情况下的遗憾上限。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于通信受限网络中多臂老虎机算法的应用需求，探索反馈机制对性能的影响。

Method: 通过分析带反馈的臂擦除信道模型，设计算法并理论证明其遗憾下限。

Result: 证明反馈机制未改变遗憾上限的阶数，但可能优化常数项；设计并数值验证了一种算法。

Conclusion: 反馈机制虽未提升理论遗憾上限，但简化了算法设计并可能优化实际性能。

Abstract: We study a distributed multi-armed bandit (MAB) problem over arm erasure
channels, motivated by the increasing adoption of MAB algorithms over
communication-constrained networks. In this setup, the learner communicates the
chosen arm to play to an agent over an erasure channel with probability
$\epsilon \in [0,1)$; if an erasure occurs, the agent continues pulling the
last successfully received arm; the learner always observes the reward of the
arm pulled. In past work, we considered the case where the agent cannot convey
feedback to the learner, and thus the learner does not know whether the arm
played is the requested or the last successfully received one. In this paper,
we instead consider the case where the agent can send feedback to the learner
on whether the arm request was received, and thus the learner exactly knows
which arm was played. Surprisingly, we prove that erasure feedback does not
improve the worst-case regret upper bound order over the previously studied
no-feedback setting. In particular, we prove a regret lower bound of
$\Omega(\sqrt{KT} + K / (1 - \epsilon))$, where $K$ is the number of arms and
$T$ the time horizon, that matches no-feedback upper bounds up to logarithmic
factors. We note however that the availability of feedback enables simpler
algorithm designs that may achieve better constants (albeit not better order)
regret bounds; we design one such algorithm and evaluate its performance
numerically.

</details>


### [214] [Evaluating Generative Models for Tabular Data: Novel Metrics and Benchmarking](https://arxiv.org/abs/2504.20900)
*Dayananda Herurkar,Ahmad Ali,Andreas Dengel*

Main category: cs.LG

TL;DR: 论文提出三种新的评估指标（FAED、FPCAD、RFIS）以解决表格数据生成模型评估的局限性，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 表格数据的结构复杂性和混合数据类型使得现有生成模型评估指标不全面，需要更全面的性能衡量方法。

Method: 提出FAED、FPCAD和RFIS三种新指标，并在三个标准网络入侵检测数据集上与现有方法（如Fidelity、Utility等）进行对比实验。

Result: FAED能有效捕捉现有指标忽略的问题，FPCAD表现有潜力但需进一步改进。

Conclusion: 提出的框架为表格数据生成模型评估提供了实用且稳健的方法。

Abstract: Generative models have revolutionized multiple domains, yet their application
to tabular data remains underexplored. Evaluating generative models for tabular
data presents unique challenges due to structural complexity, large-scale
variability, and mixed data types, making it difficult to intuitively capture
intricate patterns. Existing evaluation metrics offer only partial insights,
lacking a comprehensive measure of generative performance. To address this
limitation, we propose three novel evaluation metrics: FAED, FPCAD, and RFIS.
Our extensive experimental analysis, conducted on three standard network
intrusion detection datasets, compares these metrics with established
evaluation methods such as Fidelity, Utility, TSTR, and TRTS. Our results
demonstrate that FAED effectively captures generative modeling issues
overlooked by existing metrics. While FPCAD exhibits promising performance,
further refinements are necessary to enhance its reliability. Our proposed
framework provides a robust and practical approach for assessing generative
models in tabular data applications.

</details>


### [215] [MOSIC: Model-Agnostic Optimal Subgroup Identification with Multi-Constraint for Improved Reliability](https://arxiv.org/abs/2504.20908)
*Wenxin Chen,Weishen Pan,Kyra Gan,Fei Wang*

Main category: cs.LG

TL;DR: 提出了一种模型无关的框架，用于在多重约束下识别最优亚组，通过梯度下降上升算法解决组合优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注治疗效果提升的亚组，缺乏对亚组规模和混杂因素平衡的统一处理。

Method: 将组合问题转化为无约束的极小极大优化问题，采用梯度下降上升算法求解。

Result: 实验表明，该方法能有效识别满足多重约束的亚组，提升治疗效果和混杂因素平衡。

Conclusion: 该方法灵活稳定，适用于多种模型和技术，为个性化医疗提供了实用工具。

Abstract: Identifying subgroups that benefit from specific treatments using
observational data is a critical challenge in personalized medicine. Most
existing approaches solely focus on identifying a subgroup with an improved
treatment effect. However, practical considerations, such as ensuring a minimum
subgroup size for representativeness or achieving sufficient confounder balance
for reliability, are also important for making findings clinically meaningful
and actionable. While some studies address these constraints individually, none
offer a unified approach to handle them simultaneously. To bridge this gap, we
propose a model-agnostic framework for optimal subgroup identification under
multiple constraints. We reformulate this combinatorial problem as an
unconstrained min-max optimization problem with novel modifications and solve
it by a gradient descent ascent algorithm. We further prove its convergence to
a feasible and locally optimal solution. Our method is stable and highly
flexible, supporting various models and techniques for estimating and
optimizing treatment effectiveness with observational data. Extensive
experiments on both synthetic and real-world datasets demonstrate its
effectiveness in identifying subgroups that satisfy multiple constraints,
achieving higher treatment effects and better confounder balancing results
across different group sizes.

</details>


### [216] [Statistical and Predictive Analysis to Identify Risk Factors and Effects of Post COVID-19 Syndrome](https://arxiv.org/abs/2504.20915)
*Milad Leyli-abadi,Jean-Patrick Brunet,Axel Tahmasebimoradi*

Main category: cs.LG

TL;DR: 论文通过统计和预测分析研究了长新冠（long COVID）的影响因素，发现神经网络模型预测效果最佳，关键因素包括嗅觉丧失、头痛、肌肉疼痛和疫苗接种时间。


<details>
  <summary>Details</summary>
Motivation: 研究长新冠的持续症状及其影响因素，以帮助理解长新冠并为针对性干预提供依据。

Method: 使用线性模型、随机森林、梯度提升和神经网络等方法，基于Lifelines COVID-19队列数据进行统计和预测分析。

Result: 神经网络模型表现最佳，平均预测误差为19%；关键预测因素包括嗅觉丧失、头痛、肌肉疼痛和疫苗接种时间。

Conclusion: 研究结果为理解长新冠和开发针对性干预措施提供了重要指导。

Abstract: Based on recent studies, some COVID-19 symptoms can persist for months after
infection, leading to what is termed long COVID. Factors such as vaccination
timing, patient characteristics, and symptoms during the acute phase of
infection may contribute to the prolonged effects and intensity of long COVID.
Each patient, based on their unique combination of factors, develops a specific
risk or intensity of long COVID. In this work, we aim to achieve two
objectives: (1) conduct a statistical analysis to identify relationships
between various factors and long COVID, and (2) perform predictive analysis of
long COVID intensity using these factors. We benchmark and interpret various
data-driven approaches, including linear models, random forests, gradient
boosting, and neural networks, using data from the Lifelines COVID-19 cohort.
Our results show that Neural Networks (NN) achieve the best performance in
terms of MAPE, with predictions averaging 19\% error. Additionally,
interpretability analysis reveals key factors such as loss of smell, headache,
muscle pain, and vaccination timing as significant predictors, while chronic
disease and gender are critical risk factors. These insights provide valuable
guidance for understanding long COVID and developing targeted interventions.

</details>


### [217] [Improvements of Dark Experience Replay and Reservoir Sampling towards Better Balance between Consolidation and Plasticity](https://arxiv.org/abs/2504.20932)
*Taisuke Kobayashi*

Main category: cs.LG

TL;DR: 本文提出改进策略，优化了黑暗经验回放（DER）和水库采样（RS）方法，以平衡记忆巩固与可塑性，提升持续学习性能。


<details>
  <summary>Details</summary>
Motivation: 持续学习中，DER和RS方法存在记忆巩固与可塑性的权衡问题，导致性能受限。

Method: 改进DER（自动权重调整、错误数据屏蔽、输出修正）和RS（接受概率泛化、多缓冲分层、数据选择性忽略）。

Result: 在回归、分类和强化学习任务中，改进方法显著提升了学习性能。

Conclusion: 通过平衡记忆巩固与可塑性，改进方法实现了更稳定的持续学习效果。

Abstract: Continual learning is the one of the most essential abilities for autonomous
agents, which can incrementally learn daily-life skills. For this ultimate
goal, a simple but powerful method, dark experience replay (DER), has been
proposed recently. DER mitigates catastrophic forgetting, in which the skills
acquired in the past are unintentionally forgotten, by stochastically storing
the streaming data in a reservoir sampling (RS) buffer and by relearning them
or retaining the past outputs for them. However, since DER considers multiple
objectives, it will not function properly without appropriate weighting of
them. In addition, the ability to retain past outputs inhibits learning if the
past outputs are incorrect due to distribution shift or other effects. This is
due to a tradeoff between memory consolidation and plasticity. The tradeoff is
hidden even in the RS buffer, which gradually stops storing new data for new
skills in it as data is continuously passed to it. To alleviate the tradeoff
and achieve better balance, this paper proposes improvement strategies to each
of DER and RS. Specifically, DER is improved with automatic adaptation of
weights, block of replaying erroneous data, and correction of past outputs. RS
is also improved with generalization of acceptance probability, stratification
of plural buffers, and intentional omission of unnecessary data. These
improvements are verified through multiple benchmarks including regression,
classification, and reinforcement learning problems. As a result, the proposed
methods achieve steady improvements in learning performance by balancing the
memory consolidation and plasticity.

</details>


### [218] [Scenario-based Compositional Verification of Autonomous Systems with Neural Perception](https://arxiv.org/abs/2504.20942)
*Christopher Watson,Rajeev Alur,Divya Gopinath,Ravi Mangal,Corina S. Pasareanu*

Main category: cs.LG

TL;DR: 提出了一种基于场景建模和概率抽象的自动驾驶系统概率验证框架，解决了深度神经网络感知的复杂性和环境变化带来的验证挑战。


<details>
  <summary>Details</summary>
Motivation: 由于感知深度神经网络的复杂性和环境条件的难以量化变化，自动驾驶系统的形式验证具有挑战性。

Method: 通过场景建模将任务分解为不同环境条件下的场景，为每个场景构建概率抽象，并利用符号推理和加速证明规则进行高效验证。

Result: 在飞机滑行引导和F1Tenth自动驾驶汽车仿真模型中验证了该框架的有效性。

Conclusion: 该框架为复杂环境下的自动驾驶系统提供了高效且可扩展的验证方法。

Abstract: Recent advances in deep learning have enabled the development of autonomous
systems that use deep neural networks for perception. Formal verification of
these systems is challenging due to the size and complexity of the perception
DNNs as well as hard-to-quantify, changing environment conditions. To address
these challenges, we propose a probabilistic verification framework for
autonomous systems based on the following key concepts: (1) Scenario-based
Modeling: We decompose the task (e.g., car navigation) into a composition of
scenarios, each representing a different environment condition. (2)
Probabilistic Abstractions: For each scenario, we build a compact abstraction
of perception based on the DNN's performance on an offline dataset that
represents the scenario's environment condition. (3) Symbolic Reasoning and
Acceleration: The abstractions enable efficient compositional verification of
the autonomous system via symbolic reasoning and a novel acceleration proof
rule that bounds the error probability of the system under arbitrary variations
of environment conditions. We illustrate our approach on two case studies: an
experimental autonomous system that guides airplanes on taxiways using
high-dimensional perception DNNs and a simulation model of an F1Tenth
autonomous car using LiDAR observations.

</details>


### [219] [Deep Learning Characterizes Depression and Suicidal Ideation from Eye Movements](https://arxiv.org/abs/2504.20944)
*Kleanthis Avramidis,Woojae Jeong,Aditya Kommineni,Sudarsana R. Kadiri,Marcus Ma,Colin McDaniel,Myzelle Hughes,Thomas McGee,Elsi Kaiser,Dani Byrd,Assal Habibi,B. Rael Cahn,Idan A. Blank,Kristina Lerman,Takfarinas Medani,Richard M. Leahy,Shrikanth Narayanan*

Main category: cs.LG

TL;DR: 研究探讨眼动追踪作为抑郁症和自杀意念的客观生物标志物，通过深度学习模型分析眼动数据，取得显著预测效果。


<details>
  <summary>Details</summary>
Motivation: 抑郁症和自杀意念缺乏客观生物标志物，目前依赖主观报告和临床访谈。眼动追踪可能提供一种新的筛查工具。

Method: 记录126名年轻成年人在阅读情感句子时的眼动数据，开发深度学习模型，分别分析正负情感句子的眼动模式。

Result: 模型对抑郁症和自杀意念的预测AUC分别为0.793和0.826，对区分抑郁和自杀参与者也有一定准确性（AUC 0.609）。

Conclusion: 眼动追踪可作为心理健康评估的客观工具，情感刺激对眼动控制有显著影响。

Abstract: Identifying physiological and behavioral markers for mental health conditions
is a longstanding challenge in psychiatry. Depression and suicidal ideation, in
particular, lack objective biomarkers, with screening and diagnosis primarily
relying on self-reports and clinical interviews. Here, we investigate eye
tracking as a potential marker modality for screening purposes. Eye movements
are directly modulated by neuronal networks and have been associated with
attentional and mood-related patterns; however, their predictive value for
depression and suicidality remains unclear. We recorded eye-tracking sequences
from 126 young adults as they read and responded to affective sentences, and
subsequently developed a deep learning framework to predict their clinical
status. The proposed model included separate branches for trials of positive
and negative sentiment, and used 2D time-series representations to account for
both intra-trial and inter-trial variations. We were able to identify
depression and suicidal ideation with an area under the receiver operating
curve (AUC) of 0.793 (95% CI: 0.765-0.819) against healthy controls, and
suicidality specifically with 0.826 AUC (95% CI: 0.797-0.852). The model also
exhibited moderate, yet significant, accuracy in differentiating depressed from
suicidal participants, with 0.609 AUC (95% CI 0.571-0.646). Discriminative
patterns emerge more strongly when assessing the data relative to response
generation than relative to the onset time of the final word of the sentences.
The most pronounced effects were observed for negative-sentiment sentences,
that are congruent to depressed and suicidal participants. Our findings
highlight eye tracking as an objective tool for mental health assessment and
underscore the modulatory impact of emotional stimuli on cognitive processes
affecting oculomotor control.

</details>


### [220] [AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM Security](https://arxiv.org/abs/2504.20965)
*Zikui Cai,Shayan Shabihi,Bang An,Zora Che,Brian R. Bartoldson,Bhavya Kailkhura,Tom Goldstein,Furong Huang*

Main category: cs.LG

TL;DR: AegisLLM是一个多代理防御系统，通过协作代理和自动化提示优化增强对抗攻击和信息泄露的防御能力，无需模型重训练。


<details>
  <summary>Details</summary>
Motivation: 解决对抗攻击和信息泄露问题，提供实时适应性防御。

Method: 采用多代理协作（协调器、偏转器、响应器和评估器）和自动化提示优化（如DSPy）。

Result: 在WMDP遗忘基准上接近完美遗忘，仅需20个训练样本和少于300次LM调用；在越狱基准上比基线模型提升51%。

Conclusion: AegisLLM展示了自适应代理推理的优势，成为传统模型修改方法的有效运行时替代方案。

Abstract: We introduce AegisLLM, a cooperative multi-agent defense against adversarial
attacks and information leakage. In AegisLLM, a structured workflow of
autonomous agents - orchestrator, deflector, responder, and evaluator -
collaborate to ensure safe and compliant LLM outputs, while self-improving over
time through prompt optimization. We show that scaling agentic reasoning system
at test-time - both by incorporating additional agent roles and by leveraging
automated prompt optimization (such as DSPy)- substantially enhances robustness
without compromising model utility. This test-time defense enables real-time
adaptability to evolving attacks, without requiring model retraining.
Comprehensive evaluations across key threat scenarios, including unlearning and
jailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning
benchmark, AegisLLM achieves near-perfect unlearning with only 20 training
examples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve
51% improvement compared to the base model on StrongReject, with false refusal
rates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our
results highlight the advantages of adaptive, agentic reasoning over static
defenses, establishing AegisLLM as a strong runtime alternative to traditional
approaches based on model modifications. Code is available at
https://github.com/zikuicai/aegisllm

</details>


### [221] [Softpick: No Attention Sink, No Massive Activations with Rectified Softmax](https://arxiv.org/abs/2504.20966)
*Zayd M. K. Zuhri,Erland Hilman Fuadi,Alham Fikri Aji*

Main category: cs.LG

TL;DR: Softpick是一种替代softmax的注意力机制，解决了注意力下沉和大激活问题，性能与softmax相当，但量化表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决softmax在transformer中导致的注意力下沉和大激活问题，提升量化、低精度训练和稀疏性优化。

Method: 提出softpick作为softmax的替代方案，实验验证其在340M参数模型中的表现。

Result: softpick实现0%下沉率，隐藏状态峰度更低（340 vs 33,510），注意力图更稀疏（46.97%稀疏），量化表现更优。

Conclusion: softpick为量化、低精度训练和稀疏优化提供了新可能，代码已开源。

Abstract: We introduce softpick, a rectified, not sum-to-one, drop-in replacement for
softmax in transformer attention mechanisms that eliminates attention sink and
massive activations. Our experiments with 340M parameter models demonstrate
that softpick maintains performance parity with softmax on standard benchmarks
while achieving 0% sink rate. The softpick transformer produces hidden states
with significantly lower kurtosis (340 vs 33,510) and creates sparse attention
maps (46.97% sparsity). Models using softpick consistently outperform softmax
when quantized, with particularly pronounced advantages at lower bit
precisions. Our analysis and discussion shows how softpick has the potential to
open new possibilities for quantization, low-precision training, sparsity
optimization, pruning, and interpretability. Our code is available at
https://github.com/zaydzuhri/softpick-attention.

</details>


### [222] [Equivariant non-linear maps for neural networks on homogeneous spaces](https://arxiv.org/abs/2504.20974)
*Elias Nyholm,Oscar Carlsson,Maurice Weiler,Daniel Persson*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的非线性等变神经网络层框架，适用于齐次空间，扩展了线性等变层的理论，并证明了其普适性。


<details>
  <summary>Details</summary>
Motivation: 受非线性层（如自注意力或输入依赖核）实证成功的启发，作者希望将线性等变层的理论推广到非线性场景。

Method: 推导了广义的可操纵性约束，并证明了构造的普适性。

Result: 展示了多种常见等变网络架构（如G-CNNs、隐式可操纵核网络、Transformer等）均可从该框架中导出。

Conclusion: 该框架为未来等变神经网络层的设计提供了理论支持，并展示了其广泛适用性。

Abstract: This paper presents a novel framework for non-linear equivariant neural
network layers on homogeneous spaces. The seminal work of Cohen et al. on
equivariant $G$-CNNs on homogeneous spaces characterized the representation
theory of such layers in the linear setting, finding that they are given by
convolutions with kernels satisfying so-called steerability constraints.
Motivated by the empirical success of non-linear layers, such as self-attention
or input dependent kernels, we set out to generalize these insights to the
non-linear setting. We derive generalized steerability constraints that any
such layer needs to satisfy and prove the universality of our construction. The
insights gained into the symmetry-constrained functional dependence of
equivariant operators on feature maps and group elements informs the design of
future equivariant neural network layers. We demonstrate how several common
equivariant network architectures - $G$-CNNs, implicit steerable kernel
networks, conventional and relative position embedded attention based
transformers, and LieTransformers - may be derived from our framework.

</details>


### [223] [Hubs and Spokes Learning: Efficient and Scalable Collaborative Machine Learning](https://arxiv.org/abs/2504.20988)
*Atul Sharma,Kavindu Herath,Saurabh Bagchi,Chaoyue Liu,Somali Chaterji*

Main category: cs.LG

TL;DR: HSL框架结合了联邦学习和去中心化学习的优势，通过双层通信结构避免了单点故障，并在相同或更低通信预算下优于现有P2PL框架ELL。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习的单点故障问题，并提升去中心化学习的性能，适用于资源受限系统。

Method: 采用双层通信结构（中心节点与边缘节点），优化通信效率与性能。

Result: 在相同通信预算下性能优于ELL，低预算时也能匹配ELL性能，且节点间共识更强。

Conclusion: HSL是一种高效且实用的协作学习框架，适合大规模应用。

Abstract: We introduce the Hubs and Spokes Learning (HSL) framework, a novel paradigm
for collaborative machine learning that combines the strengths of Federated
Learning (FL) and Decentralized Learning (P2PL). HSL employs a two-tier
communication structure that avoids the single point of failure inherent in FL
and outperforms the state-of-the-art P2PL framework, Epidemic Learning Local
(ELL). At equal communication budgets (total edges), HSL achieves higher
performance than ELL, while at significantly lower communication budgets, it
can match ELL's performance. For instance, with only 400 edges, HSL reaches the
same test accuracy that ELL achieves with 1000 edges for 100 peers (spokes) on
CIFAR-10, demonstrating its suitability for resource-constrained systems. HSL
also achieves stronger consensus among nodes after mixing, resulting in
improved performance with fewer training rounds. We substantiate these claims
through rigorous theoretical analyses and extensive experimental results,
showcasing HSL's practicality for large-scale collaborative learning.

</details>


### [224] [Toward Efficient Exploration by Large Language Model Agents](https://arxiv.org/abs/2504.20997)
*Dilip Arumugam,Thomas L. Griffiths*

Main category: cs.LG

TL;DR: 论文探讨了如何利用大型语言模型（LLM）实现数据高效的强化学习（RL），通过明确实现已知的RL算法（后验采样）来解决探索难题。


<details>
  <summary>Details</summary>
Motivation: 尽管基于LLM的自主决策代理有广泛应用潜力，但现有设计在数据效率和探索能力上存在不足，需要更有效的解决方案。

Method: 研究提出了一种方法，通过LLM明确实现后验采样RL算法，而非依赖微调或上下文学习。

Result: 实验结果表明，该方法在需要谨慎探索的自然语言任务中显著提升了效果。

Conclusion: 通过LLM明确实现已知的RL算法，可以有效解决探索问题，提升数据效率。

Abstract: A burgeoning area within reinforcement learning (RL) is the design of
sequential decision-making agents centered around large language models (LLMs).
While autonomous decision-making agents powered by modern LLMs could facilitate
numerous real-world applications, such successes demand agents that are capable
of data-efficient RL. One key obstacle to achieving data efficiency in RL is
exploration, a challenge that we demonstrate many recent proposals for LLM
agent designs struggle to contend with. Meanwhile, classic algorithms from the
RL literature known to gracefully address exploration require technical
machinery that can be challenging to operationalize in purely natural language
settings. In this work, rather than relying on finetuning or in-context
learning to coax LLMs into implicitly imitating a RL algorithm, we illustrate
how LLMs can be used to explicitly implement an existing RL algorithm
(Posterior Sampling for Reinforcement Learning) whose capacity for
statistically-efficient exploration is already well-studied. We offer empirical
results demonstrating how our LLM-based implementation of a known,
data-efficient RL algorithm can be considerably more effective in natural
language tasks that demand prudent exploration.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [225] [Towards Large Language Models for Lunar Mission Planning and In Situ Resource Utilization](https://arxiv.org/abs/2504.20125)
*Michael Pekala,Gregory Canal,Samuel Barham,Milena B. Graziano,Morgan Trexler,Leslie Hamilton,Elizabeth Reilly,Christopher D. Stiles*

Main category: cs.DL

TL;DR: 利用LLM快速处理科学文献以获取月球成分数据的可行性研究。


<details>
  <summary>Details</summary>
Motivation: 评估月球任务规划中原材料本地可用性，但相关数据分散在众多科学文献中。

Method: 利用现成的LLM处理科学文献，提取表格数据，并评估其准确性。

Result: LLM能有效提取表格数据，但在细粒度矿物学信息和复杂数据上仍有改进空间。

Conclusion: LLM在提取月球成分数据上有效，但需进一步优化以处理更复杂的信息。

Abstract: A key factor for lunar mission planning is the ability to assess the local
availability of raw materials. However, many potentially relevant measurements
are scattered across a variety of scientific publications. In this paper we
consider the viability of obtaining lunar composition data by leveraging LLMs
to rapidly process a corpus of scientific publications. While leveraging LLMs
to obtain knowledge from scientific documents is not new, this particular
application presents interesting challenges due to the heterogeneity of lunar
samples and the nuances involved in their characterization. Accuracy and
uncertainty quantification are particularly crucial since many materials
properties can be sensitive to small variations in composition. Our findings
indicate that off-the-shelf LLMs are generally effective at extracting data
from tables commonly found in these documents. However, there remains
opportunity to further refine the data we extract in this initial approach; in
particular, to capture fine-grained mineralogy information and to improve
performance on more subtle/complex pieces of information.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [226] [Energy-Based Coarse-Graining in Molecular Dynamics: A Flow-Based Framework Without Data](https://arxiv.org/abs/2504.20940)
*Maximilian Stupp,P. S. Koutsourelakis*

Main category: physics.chem-ph

TL;DR: 提出了一种无需数据的粗粒化生成框架，直接针对全原子玻尔兹曼分布，通过结构化潜在空间和能量目标训练，实现准确重构分子结构。


<details>
  <summary>Details</summary>
Motivation: 传统粗粒化方法依赖大量全原子模拟数据，限制了模型的准确性和泛化能力。本文旨在摆脱数据依赖，直接生成全原子样本。

Method: 定义包含慢变量和快变量的结构化潜在空间，使用可学习的双射映射重构分子结构，通过能量目标和温度调节训练模型。

Result: 在合成系统和丙氨酸二肽上验证，模型能准确捕捉玻尔兹曼分布的多模态，重构原子构型，并学习有物理意义的粗粒化表示。

Conclusion: 无需模拟数据即可生成无偏的全原子样本，为粗粒化建模提供了新思路。

Abstract: Coarse-grained (CG) models offer an effective route to reducing the
complexity of molecular simulations, yet conventional approaches depend heavily
on long all-atom molecular dynamics (MD) trajectories to adequately sample
configurational space. This data-driven dependence limits their accuracy and
generalizability, as unvisited configurations remain excluded from the
resulting CG model. We introduce a data-free generative framework for
coarse-graining that directly targets the all-atom Boltzmann distribution. Our
model defines a structured latent space comprising slow collective variables,
which are statistically associated with multimodal marginal densities capturing
metastable states, and fast variables, which represent the remaining degrees of
freedom with simple, unimodal conditional distributions. A potentially
learnable, bijective map from the full latent space to the all-atom
configuration space enables automatic and accurate reconstruction of molecular
structures. The model is trained using an energy-based objective that minimizes
the reverse Kullback-Leibler divergence, relying solely on the interatomic
potential rather than sampled trajectories. A tempering scheme is used to
stabilize training and promote exploration of diverse configurations. Once
trained, the model can generate unbiased, one-shot equilibrium all-atom
samples. We validate the method on two synthetic systems-a double-well
potential and a Gaussian mixture-as well as on the benchmark alanine dipeptide.
The model captures all relevant modes of the Boltzmann distribution, accurately
reconstructs atomic configurations, and learns physically meaningful
coarse-grained representations, all without any simulation data.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [227] [Optimizing Hard Thresholding for Sparse Model Discovery](https://arxiv.org/abs/2504.20256)
*Derek W. Jollie,Scott G. McCalla*

Main category: math.OC

TL;DR: 通过引入退火方案重新激活部分被移除的项，改进了稀疏字典学习算法的性能，提高了模型准确性。


<details>
  <summary>Details</summary>
Motivation: 稀疏字典学习算法通常通过硬阈值处理强制稀疏激活，但可能丢失重要信息。退火方案旨在通过重新激活部分项来优化性能。

Method: 采用退火方案，结合冷却计划，重新激活部分被移除的项。测试了两种优化方法：SINDy和硬阈值追踪。

Result: 退火方案在多种非线性系统中（如对流流动、可激发系统和种群动力学）显著提高了模型准确性。

Conclusion: 退火方案有效改进了稀疏学习算法的性能，适用于实验数据（如抛体运动）。

Abstract: Many model selection algorithms rely on sparse dictionary learning to provide
interpretable and physics-based governing equations. The optimization
algorithms typically use a hard thresholding process to enforce sparse
activations in the model coefficients by removing library elements from
consideration. By introducing an annealing scheme that reactivates a fraction
of the removed terms with a cooling schedule, we are able to improve the
performance of these sparse learning algorithms. We concentrate on two
approaches to the optimization, SINDy, and an alternative using hard
thresholding pursuit. We see in both cases that annealing can improve model
accuracy. The effectiveness of annealing is demonstrated through comparisons on
several nonlinear systems pulled from convective flows, excitable systems, and
population dynamics. Finally we apply these algorithms to experimental data for
projectile motion.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [228] [AKIBoards: A Structure-Following Multiagent System for Predicting Acute Kidney Injury](https://arxiv.org/abs/2504.20368)
*David Gordon,Panayiotis Petousis,Susanne B. Nicholas,Alex A. T. Bui*

Main category: cs.MA

TL;DR: 论文提出STRUC-MAS框架，通过多智能体系统（MAS）学习和利用全局模型，提升复杂医疗场景中的诊断推理性能。


<details>
  <summary>Details</summary>
Motivation: 在复杂医疗场景中，多专家协作需要整合不同视角以优化诊断决策，但缺乏自动化学习全局模型的方法。

Method: 引入STRUC-MAS框架，自动化学习全局模型并将其作为智能体的先验信念，应用于急性肾损伤（AKI）预测。

Result: 实验显示，基于全局模型的智能体（SF-FT和SF-FT-RAG）在AKI预测中表现优于基线（AP更高），且交互后智能体信心增强。

Conclusion: 学习和利用全局结构对提升MAS的分类和诊断推理性能至关重要。

Abstract: Diagnostic reasoning entails a physician's local (mental) model based on an
assumed or known shared perspective (global model) to explain patient
observations with evidence assigned towards a clinical assessment. But in
several (complex) medical situations, multiple experts work together as a team
to optimize health evaluation and decision-making by leveraging different
perspectives. Such consensus-driven reasoning reflects individual knowledge
contributing toward a broader perspective on the patient. In this light, we
introduce STRUCture-following for Multiagent Systems (STRUC-MAS), a framework
automating the learning of these global models and their incorporation as prior
beliefs for agents in multiagent systems (MAS) to follow. We demonstrate proof
of concept with a prosocial MAS application for predicting acute kidney
injuries (AKIs). In this case, we found that incorporating a global structure
enabled multiple agents to achieve better performance (average precision, AP)
in predicting AKI 48 hours before onset (structure-following-fine-tuned, SF-FT,
AP=0.195; SF-FT-retrieval-augmented generation, SF-FT-RAG, AP=0.194) vs.
baseline (non-structure-following-FT, NSF-FT, AP=0.141; NSF-FT-RAG, AP=0.180)
for balanced precision-weighted-recall-weighted voting. Markedly, SF-FT agents
with higher recall scores reported lower confidence levels in the initial round
on true positive and false negative cases. But after explicit interactions,
their confidence in their decisions increased (suggesting reinforced belief).
In contrast, the SF-FT agent with the lowest recall decreased its confidence in
true positive and false negative cases (suggesting a new belief). This approach
suggests that learning and leveraging global structures in MAS is necessary
prior to achieving competitive classification and diagnostic reasoning
performance.

</details>


### [229] [Modeling AI-Human Collaboration as a Multi-Agent Adaptation](https://arxiv.org/abs/2504.20903)
*Prothit Sen,Sai Mihir Jakkaraju*

Main category: cs.MA

TL;DR: 论文通过基于代理的模拟，研究了AI与人类协作的效果，发现任务结构（模块化或序列化）是决定协作效果的关键因素。


<details>
  <summary>Details</summary>
Motivation: 探讨AI与人类在不同任务结构下的协作效果，为组织战略决策提供通用框架。

Method: 使用基于代理的模拟和NK模型，区分启发式人类适应与规则化AI搜索，分析模块化和序列化任务中的交互。

Result: 模块化任务中AI常替代人类；序列化任务中专家人类与AI互补效果最佳；AI幻觉也能帮助低能力人类突破局部最优。

Conclusion: AI与人类协作的有效性主要取决于任务结构，任务分解是战略决策的核心分析单元。

Abstract: We develop an agent-based simulation to formalize AI-human collaboration as a
function of task structure, advancing a generalizable framework for strategic
decision-making in organizations. Distinguishing between heuristic-based human
adaptation and rule-based AI search, we model interactions across modular
(parallel) and sequenced (interdependent) tasks using an NK model. Our results
reveal that in modular tasks, AI often substitutes for humans - delivering
higher payoffs unless human expertise is very high, and the AI search space is
either narrowly focused or extremely broad. In sequenced tasks, interesting
complementarities emerge. When an expert human initiates the search and AI
subsequently refines it, aggregate performance is maximized. Conversely, when
AI leads, excessive heuristic refinement by the human can reduce payoffs. We
also show that even "hallucinatory" AI - lacking memory or structure - can
improve outcomes when augmenting low-capability humans by helping escape local
optima. These results yield a robust implication: the effectiveness of AI-human
collaboration depends less on context or industry, and more on the underlying
task structure. By elevating task decomposition as the central unit of
analysis, our model provides a transferable lens for strategic decision-making
involving humans and an agentic AI across diverse organizational settings.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [230] [HCT-QA: A Benchmark for Question Answering on Human-Centric Tables](https://arxiv.org/abs/2504.20047)
*Mohammad S. Ahmad,Zan A. Naeem,Michaël Aupetit,Ahmed Elmagarmid,Mohamed Eltabakh,Xiasong Ma,Mourad Ouzzani,Chaoyi Ruan*

Main category: cs.IR

TL;DR: 论文介绍了HCT-QA基准测试，用于评估大语言模型处理复杂表格数据的能力。


<details>
  <summary>Details</summary>
Motivation: 人类中心表格（HCTs）具有高业务价值但复杂布局，传统方法难以处理，需新解决方案。

Method: 构建包含2188真实HCTs和4679合成表格的基准数据集，测试大语言模型的查询能力。

Result: 数据集包含9835真实QA对和67.5K合成QA对，为大语言模型评估提供基础。

Conclusion: HCT-QA为复杂表格数据处理提供了新基准，大语言模型展现出潜力。

Abstract: Tabular data embedded within PDF files, web pages, and other document formats
are prevalent across numerous sectors such as government, engineering, science,
and business. These human-centric tables (HCTs) possess a unique combination of
high business value, intricate layouts, limited operational power at scale, and
sometimes serve as the only data source for critical insights. However, their
complexity poses significant challenges to traditional data extraction,
processing, and querying methods. While current solutions focus on transforming
these tables into relational formats for SQL queries, they fall short in
handling the diverse and complex layouts of HCTs and hence being amenable to
querying. This paper describes HCT-QA, an extensive benchmark of HCTs, natural
language queries, and related answers on thousands of tables. Our dataset
includes 2,188 real-world HCTs with 9,835 QA pairs and 4,679 synthetic tables
with 67.5K QA pairs. While HCTs can be potentially processed by different type
of query engines, in this paper, we focus on Large Language Models as potential
engines and assess their ability in processing and querying such tables.

</details>


### [231] [Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence](https://arxiv.org/abs/2504.20059)
*Joey Chan,Qiao Jin,Nicholas Wan,Charalampos S. Floudas,Elisabetta Xue,Zhiyong Lu*

Main category: cs.IR

TL;DR: TrialGPT利用大型语言模型匹配患者与临床试验，比传统关键词搜索方法表现更好，识别合格试验的能力提升46%。


<details>
  <summary>Details</summary>
Motivation: 临床试验招募面临挑战，如患者意识不足和复杂资格标准，而在线平台为扩大招募提供了新机会。

Method: 使用TrialGPT框架，基于大型语言模型，匹配50例在线患者案例与临床试验，并与传统关键词搜索方法对比。

Result: TrialGPT识别合格试验的能力比传统方法高46%，平均每位患者符合约7项试验。反馈非常积极。

Conclusion: TrialGPT展示了利用在线平台和AI技术优化临床试验招募的潜力。

Abstract: Clinical trials are crucial for assessing new treatments; however,
recruitment challenges - such as limited awareness, complex eligibility
criteria, and referral barriers - hinder their success. With the growth of
online platforms, patients increasingly turn to social media and health
communities for support, research, and advocacy, expanding recruitment pools
and established enrollment pathways. Recognizing this potential, we utilized
TrialGPT, a framework that leverages a large language model (LLM) as its
backbone, to match 50 online patient cases (collected from published case
reports and a social media website) to clinical trials and evaluate performance
against traditional keyword-based searches. Our results show that TrialGPT
outperforms traditional methods by 46% in identifying eligible trials, with
each patient, on average, being eligible for around 7 trials. Additionally, our
outreach efforts to case authors and trial organizers regarding these
patient-trial matches yielded highly positive feedback, which we present from
both perspectives.

</details>


### [232] [A model and package for German ColBERT](https://arxiv.org/abs/2504.20083)
*Thuong Dang,Qiqi Chen*

Main category: cs.IR

TL;DR: 本文介绍了ColBERT的德语版本，专注于RAG应用，并展示了支持检索和微调工作流程的ColBERT模型包的主要功能。


<details>
  <summary>Details</summary>
Motivation: 为德语用户提供ColBERT的多密集向量检索方法，并支持RAG应用。

Method: 开发德语版ColBERT，并构建支持检索和微调的模型包。

Result: 成功实现德语版ColBERT，并提供功能完整的模型包。

Conclusion: 该工作扩展了ColBERT的应用范围，为德语用户提供了高效的工具。

Abstract: In this work, we introduce a German version for ColBERT, a late interaction
multi-dense vector retrieval method, with a focus on RAG applications. We also
present the main features of our package for ColBERT models, supporting both
retrieval and fine-tuning workflows.

</details>


### [233] [An Integrated Framework for Contextual Personalized LLM-Based Food Recommendation](https://arxiv.org/abs/2504.20092)
*Ali Rostami*

Main category: cs.IR

TL;DR: 论文提出了一种针对食物推荐系统的个性化框架F-RLP，解决了现有通用模型在食物领域的不足，通过多媒体日志平台和地理分析工具提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有食物推荐系统因组件理解不足和数据不平衡问题表现不佳，通用语言处理模型缺乏对食物领域的专门优化。

Method: 提出F-RLP框架，结合多媒体食物日志平台和地理分析工具World Food Atlas，专门优化LLMs在食物推荐中的应用。

Result: F-RLP框架克服了通用模型的限制，提供了更有效、上下文感知的个性化食物推荐。

Conclusion: F-RLP为食物推荐领域提供了一种创新的、专门化的解决方案，显著提升了推荐系统的性能。

Abstract: Personalized food recommendation systems (Food-RecSys) critically
underperform due to fragmented component understanding and the failure of
conventional machine learning with vast, imbalanced food data. While Large
Language Models (LLMs) offer promise, current generic Recommendation as
Language Processing (RLP) strategies lack the necessary specialization for the
food domain's complexity. This thesis tackles these deficiencies by first
identifying and analyzing the essential components for effective Food-RecSys.
We introduce two key innovations: a multimedia food logging platform for rich
contextual data acquisition and the World Food Atlas, enabling unique
geolocation-based food analysis previously unavailable. Building on this
foundation, we pioneer the Food Recommendation as Language Processing (F-RLP)
framework - a novel, integrated approach specifically architected for the food
domain. F-RLP leverages LLMs in a tailored manner, overcoming the limitations
of generic models and providing a robust infrastructure for effective,
contextual, and truly personalized food recommendations.

</details>


### [234] [MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?](https://arxiv.org/abs/2504.20094)
*Zheng Hui,Xiaokai Wei,Yexi Jiang,Kevin Gao,Chen Wang,Frank Ong,Se-eun Yoon,Rachit Pareek,Michelle Gong*

Main category: cs.IR

TL;DR: MATCHA是一个基于多智能体协作的对话推荐系统框架，利用大语言模型提升个性化和用户参与度。


<details>
  <summary>Details</summary>
Motivation: 解决对话推荐系统中复杂用户请求处理、个性化增强、实证评估与部署以及安全可信交互等关键挑战。

Method: 引入多个专用智能体（意图分析、候选生成、排序、重排序、可解释性和安全保障）协作优化推荐准确性、多样性和安全性。

Result: 在八个指标上表现优于或与当前最优模型相当。

Conclusion: MATCHA通过多智能体协作有效提升了对话推荐系统的性能，解决了关键挑战。

Abstract: In this paper, we propose a multi-agent collaboration framework called MATCHA
for conversational recommendation system, leveraging large language models
(LLMs) to enhance personalization and user engagement. Users can request
recommendations via free-form text and receive curated lists aligned with their
interests, preferences, and constraints. Our system introduces specialized
agents for intent analysis, candidate generation, ranking, re-ranking,
explainability, and safeguards. These agents collaboratively improve
recommendations accuracy, diversity, and safety. On eight metrics, our model
achieves superior or comparable performance to the current state-of-the-art.
Through comparisons with six baseline models, our approach addresses key
challenges in conversational recommendation systems for game recommendations,
including: (1) handling complex, user-specific requests, (2) enhancing
personalization through multi-agent collaboration, (3) empirical evaluation and
deployment, and (4) ensuring safe and trustworthy interactions.

</details>


### [235] [TreeHop: Generate and Filter Next Query Embeddings Efficiently for Multi-hop Question Answering](https://arxiv.org/abs/2504.20114)
*Zhonghao Li,Kunpeng Zhang,Jinghuai Ou,Shuliang Liu,Xuming Hu*

Main category: cs.IR

TL;DR: TreeHop提出了一种基于嵌入的动态查询更新框架，用于多跳问答任务，显著降低了计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 解决现有RAG系统在多跳问答中因多次LLM调用和多阶段处理导致的高计算成本问题。

Method: 通过动态更新查询嵌入，利用嵌入空间操作实现迭代检索，无需LLM参与查询优化。

Result: 在三个开放域多跳问答数据集上性能媲美先进方法，计算成本降低99%，参数规模仅为5%-0.4%。

Conclusion: TreeHop是一种高效、低成本的多跳问答解决方案，适用于知识密集型应用。

Abstract: Retrieval-augmented generation (RAG) systems face significant challenges in
multi-hop question answering (MHQA), where complex queries require synthesizing
information across multiple document chunks. Existing approaches typically rely
on iterative LLM-based query rewriting and routing, resulting in high
computational costs due to repeated LLM invocations and multi-stage processes.
To address these limitations, we propose TreeHop, an embedding-level framework
without the need for LLMs in query refinement. TreeHop dynamically updates
query embeddings by fusing semantic information from prior queries and
retrieved documents, enabling iterative retrieval through embedding-space
operations alone. This method replaces the traditional
"Retrieve-Rewrite-Vectorize-Retrieve" cycle with a streamlined
"Retrieve-Embed-Retrieve" loop, significantly reducing computational overhead.
Moreover, a rule-based stop criterion is introduced to further prune redundant
retrievals, balancing efficiency and recall rate. Experimental results show
that TreeHop rivals advanced RAG methods across three open-domain MHQA
datasets, achieving comparable performance with only 5\%-0.4\% of the model
parameter size and reducing the query latency by approximately 99\% compared to
concurrent approaches. This makes TreeHop a faster and more cost-effective
solution for deployment in a range of knowledge-intensive applications. For
reproducibility purposes, codes and data are available here:
https://github.com/allen-li1231/TreeHop.

</details>


### [236] [Search-Based Interaction For Conversation Recommendation via Generative Reward Model Based Simulated User](https://arxiv.org/abs/2504.20458)
*Xiaolei Wang,Chunxuan Xia,Junyi Li,Fanzhe Meng,Lei Huang,Jinpeng Wang,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.IR

TL;DR: 论文提出了一种基于生成奖励模型的模拟用户（GRSU），用于自动与对话推荐系统（CRS）交互，以更好地捕捉用户偏好。


<details>
  <summary>Details</summary>
Motivation: 对话推荐系统（CRS）在理解用户偏好时面临挑战，频繁的用户交互可能降低体验。

Method: 设计了生成式评分和基于属性的项目评价两种反馈动作，并通过指令调优统一模拟用户。

Result: 实验证明该方法在效果、效率和可迁移性上表现优异。

Conclusion: GRSU为CRS提供了一种高效的自动交互解决方案。

Abstract: Conversational recommendation systems (CRSs) use multi-turn interaction to
capture user preferences and provide personalized recommendations. A
fundamental challenge in CRSs lies in effectively understanding user
preferences from conversations. User preferences can be multifaceted and
complex, posing significant challenges for accurate recommendations even with
access to abundant external knowledge. While interaction with users can clarify
their true preferences, frequent user involvement can lead to a degraded user
experience.
  To address this problem, we propose a generative reward model based simulated
user, named GRSU, for automatic interaction with CRSs. The simulated user
provides feedback to the items recommended by CRSs, enabling them to better
capture intricate user preferences through multi-turn interaction. Inspired by
generative reward models, we design two types of feedback actions for the
simulated user: i.e., generative item scoring, which offers coarse-grained
feedback, and attribute-based item critique, which provides fine-grained
feedback. To ensure seamless integration, these feedback actions are unified
into an instruction-based format, allowing the development of a unified
simulated user via instruction tuning on synthesized data. With this simulated
user, automatic multi-turn interaction with CRSs can be effectively conducted.
Furthermore, to strike a balance between effectiveness and efficiency, we draw
inspiration from the paradigm of reward-guided search in complex reasoning
tasks and employ beam search for the interaction process. On top of this, we
propose an efficient candidate ranking method to improve the recommendation
results derived from interaction. Extensive experiments on public datasets
demonstrate the effectiveness, efficiency, and transferability of our approach.

</details>


### [237] [OpenTCM: A GraphRAG-Empowered LLM-based System for Traditional Chinese Medicine Knowledge Retrieval and Diagnosis](https://arxiv.org/abs/2504.20118)
*Jinglin He,Yunqi Guo,Lai Kwan Lam,Waikei Leung,Lixing He,Yuanan Jiang,Chi Chiu Wang,Guoliang Xing,Hongkai Chen*

Main category: cs.IR

TL;DR: OpenTCM是一个基于LLM的系统，结合中医知识图谱和图增强生成技术，用于中医文献的现代化和知识检索，显著提升了知识图谱质量和诊断问答性能。


<details>
  <summary>Details</summary>
Motivation: 中医文献复杂且晦涩，AI技术整合对现代化和普及至关重要，但面临古典文本解读和语义关系建模的挑战。

Method: 从中医经典数据库提取文本，构建多关系知识图谱，结合LLM实现高保真知识检索和诊断问答。

Result: 知识图谱精度达98.55%，F1分数99.55%；OpenTCM在成分检索和诊断问答中分别获得4.5和3.8的专家评分。

Conclusion: OpenTCM通过知识图谱和LLM的结合，显著提升了中医知识的检索和问答性能，为中医现代化提供了有效工具。

Abstract: Traditional Chinese Medicine (TCM) represents a rich repository of ancient
medical knowledge that continues to play an important role in modern
healthcare. Due to the complexity and breadth of the TCM literature, the
integration of AI technologies is critical for its modernization and broader
accessibility. However, this integration poses considerable challenges,
including the interpretation of obscure classical Chinese texts and the
modeling of intricate semantic relationships among TCM concepts. In this paper,
we develop OpenTCM, an LLM-based system that combines a domain-specific TCM
knowledge graph and Graph-based Retrieval-Augmented Generation (GraphRAG).
First, we extract more than 3.73 million classical Chinese characters from 68
gynecological books in the Chinese Medical Classics Database, with the help of
TCM and gynecology experts. Second, we construct a comprehensive
multi-relational knowledge graph comprising more than 48,000 entities and
152,000 interrelationships, using customized prompts and Chinese-oriented LLMs
such as DeepSeek and Kimi to ensure high-fidelity semantic understanding. Last,
we integrate OpenTCM with this knowledge graph, enabling high-fidelity
ingredient knowledge retrieval and diagnostic question-answering without model
fine-tuning. Experimental evaluations demonstrate that our prompt design and
model selection significantly improve knowledge graph quality, achieving a
precision of 98. 55% and an F1 score of 99. 55%. In addition, OpenTCM achieves
mean expert scores of 4.5 in ingredient information retrieval and 3.8 in
diagnostic question-answering tasks, outperforming state-of-the-art solutions
in real-world TCM use cases.

</details>


### [238] [X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2504.20859)
*Guy Hadad,Haggai Roitman,Yotam Eshel,Bracha Shapira,Lior Rokach*

Main category: cs.IR

TL;DR: X-Cross是一种跨域序列推荐模型，通过集成多个域特定语言模型，动态优化表示，减少参数和训练数据需求，提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 解决新产品领域推荐系统需快速适应且无需大量重新训练的问题。

Method: 使用低秩适配器（LoRA）微调域特定语言模型，动态整合各模型知识，逐层优化表示。

Result: 在亚马逊数据集上，X-Cross性能接近LoRA微调模型，参数减少75%；跨域任务中，训练数据需求减少50%-75%，且准确率显著优于基线。

Conclusion: X-Cross提供高效、可扩展的跨域推荐方案，降低计算开销，适合数据受限环境。

Abstract: As new products are emerging daily, recommendation systems are required to
quickly adapt to possible new domains without needing extensive retraining.
This work presents ``X-Cross'' -- a novel cross-domain
sequential-recommendation model that recommends products in new domains by
integrating several domain-specific language models; each model is fine-tuned
with low-rank adapters (LoRA). Given a recommendation prompt, operating layer
by layer, X-Cross dynamically refines the representation of each source
language model by integrating knowledge from all other models. These refined
representations are propagated from one layer to the next, leveraging the
activations from each domain adapter to ensure domain-specific nuances are
preserved while enabling adaptability across domains. Using Amazon datasets for
sequential recommendation, X-Cross achieves performance comparable to a model
that is fine-tuned with LoRA, while using only 25% of the additional
parameters. In cross-domain tasks, such as adapting from Toys domain to Tools,
Electronics or Sports, X-Cross demonstrates robust performance, while requiring
about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective.
Furthermore, X-Cross achieves significant improvement in accuracy over
alternative cross-domain baselines. Overall, X-Cross enables scalable and
adaptive cross-domain recommendations, reducing computational overhead and
providing an efficient solution for data-constrained environments.

</details>


### [239] [Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets](https://arxiv.org/abs/2504.20119)
*Lorenz Brehme,Thomas Ströhle,Ruth Breu*

Main category: cs.IR

TL;DR: 本文系统综述了63篇学术文章，全面总结了RAG系统的最新评估方法，重点关注数据集、检索器、索引与数据库以及生成器组件，并探讨了自动化评估的可行性及其与人工评估的平衡。


<details>
  <summary>Details</summary>
Motivation: RAG系统的复杂性使其评估和质量提升面临挑战，需系统化方法以记录进展、比较配置并优化领域应用。

Method: 通过综述63篇学术文章，分析RAG系统的评估方法，重点关注四个关键组件，并提出自动化评估的可行性。

Result: 研究发现自动化评估方法可行，但需进一步实践研究以指导企业实施和评估RAG系统。

Conclusion: 本文通过综合RAG组件的评估方法，推动了系统化评估的进步，并探讨了自动化与人工评估的平衡及其挑战。

Abstract: Retrieval-Augmented Generation (RAG) has advanced significantly in recent
years. The complexity of RAG systems, which involve multiple components-such as
indexing, retrieval, and generation-along with numerous other parameters, poses
substantial challenges for systematic evaluation and quality enhancement.
Previous research highlights that evaluating RAG systems is essential for
documenting advancements, comparing configurations, and identifying effective
approaches for domain-specific applications. This study systematically reviews
63 academic articles to provide a comprehensive overview of state-of-the-art
RAG evaluation methodologies, focusing on four key areas: datasets, retrievers,
indexing and databases, and the generator component. We observe the feasibility
of an automated evaluation approach for each component of a RAG system,
leveraging an LLM capable of both generating evaluation datasets and conducting
evaluations. In addition, we found that further practical research is essential
to provide companies with clear guidance on the do's and don'ts of implementing
and evaluating RAG systems. By synthesizing evaluation approaches for key RAG
components and emphasizing the creation and adaptation of domain-specific
datasets for benchmarking, we contribute to the advancement of systematic
evaluation methods and the improvement of evaluation rigor for RAG systems.
Furthermore, by examining the interplay between automated approaches leveraging
LLMs and human judgment, we contribute to the ongoing discourse on balancing
automation and human input, clarifying their respective contributions,
limitations, and challenges in achieving robust and reliable evaluations.

</details>


### [240] [Enhancing News Recommendation with Hierarchical LLM Prompting](https://arxiv.org/abs/2504.20452)
*Hai-Dang Kieu,Delvin Ce Zhang,Minh Duc Nguyen,Min Xu,Qiang Wu,Dung D. Le*

Main category: cs.IR

TL;DR: PNR-LLM利用大语言模型（LLM）增强新闻标题和摘要的语义信息，通过注意力机制整合数据，提升个性化新闻推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统依赖浅层表示（如标题和摘要），难以捕捉用户偏好的复杂性。

Method: 提出PNR-LLM，包含新闻增强模块（利用LLM生成深层语义和实体信息）和注意力机制整合数据。

Result: 在MIND数据集上表现优于现有方法，且增强模块可通用提升其他模型性能。

Conclusion: PNR-LLM通过LLM增强数据表示，显著提升推荐效果，具有通用性。

Abstract: Personalized news recommendation systems often struggle to effectively
capture the complexity of user preferences, as they rely heavily on shallow
representations, such as article titles and abstracts. To address this problem,
we introduce a novel method, namely PNR-LLM, for Large Language Models for
Personalized News Recommendation. Specifically, PNR-LLM harnesses the
generation capabilities of LLMs to enrich news titles and abstracts, and
consequently improves recommendation quality. PNR-LLM contains a novel module,
News Enrichment via LLMs, which generates deeper semantic information and
relevant entities from articles, transforming shallow contents into richer
representations. We further propose an attention mechanism to aggregate
enriched semantic- and entity-level data, forming unified user and news
embeddings that reveal a more accurate user-news match. Extensive experiments
on MIND datasets show that PNR-LLM outperforms state-of-the-art baselines.
Moreover, the proposed data enrichment module is model-agnostic, and we
empirically show that applying our proposed module to multiple existing models
can further improve their performance, verifying the advantage of our design.

</details>


### [241] [Information Retrieval in the Age of Generative AI: The RGB Model](https://arxiv.org/abs/2504.20610)
*Michele Garetto,Alessandro Cornacchia,Franco Galante,Emilio Leonardi,Alessandro Nordio,Alberto Tarable*

Main category: cs.IR

TL;DR: 本文提出了一种量化方法，研究生成式AI工具使用带来的信息动态变化，揭示了其快速普及可能加剧不准确信息传播的风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）和生成式AI的兴起对信息检索和处理带来巨大潜力，但也引发内容真实性和可靠性的担忧。本文旨在揭示这些未被充分理解的信息动态。

Method: 提出了一种随机模型，用于描述信息在新主题下的生成、索引和传播过程，并结合Stack Exchange数据进行了深入分析。

Result: 研究发现，生成式AI的快速普及和用户依赖可能超越人工验证速度，增加不准确信息传播的风险。高质量回答需要大量时间和人工投入。

Conclusion: 强调了未来生成式AI工具开发和部署中负责任的重要性，以减少不准确信息的传播风险。

Abstract: The advent of Large Language Models (LLMs) and generative AI is fundamentally
transforming information retrieval and processing on the Internet, bringing
both great potential and significant concerns regarding content authenticity
and reliability. This paper presents a novel quantitative approach to shed
light on the complex information dynamics arising from the growing use of
generative AI tools. Despite their significant impact on the digital ecosystem,
these dynamics remain largely uncharted and poorly understood. We propose a
stochastic model to characterize the generation, indexing, and dissemination of
information in response to new topics. This scenario particularly challenges
current LLMs, which often rely on real-time Retrieval-Augmented Generation
(RAG) techniques to overcome their static knowledge limitations. Our findings
suggest that the rapid pace of generative AI adoption, combined with increasing
user reliance, can outpace human verification, escalating the risk of
inaccurate information proliferation across digital resources. An in-depth
analysis of Stack Exchange data confirms that high-quality answers inevitably
require substantial time and human effort to emerge. This underscores the
considerable risks associated with generating persuasive text in response to
new questions and highlights the critical need for responsible development and
deployment of future generative AI tools.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [242] [Learning Hierarchical Interaction for Accurate Molecular Property Prediction](https://arxiv.org/abs/2504.20127)
*Huiyang Hong,Xinkai Wu,Hongyu Sun,Qi Wang,Yuquan Li*

Main category: q-bio.BM

TL;DR: 论文提出了一种名为HimNet的新模型，通过分层交互消息传递机制（Hierarchical Interaction Message Passing Mechanism）解决现有深度学习方法在分子属性预测中无法有效利用分子结构层次信息的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如图神经网络和Transformer）在预测分子属性时难以高效捕捉分子结构的层次性，且缺乏多级特征间的有效交互机制。

Method: 提出HimNet模型，通过分层注意力引导的消息传递机制，实现原子、基序和分子层面的交互感知表示学习。

Result: 在多个基准数据集上，HimNet在大多数分子属性预测任务中表现最佳或接近最佳，并展现出良好的层次可解释性。

Conclusion: HimNet为分子活性和ADMET属性预测提供了高效准确的解决方案，有助于药物发现早期的决策。

Abstract: Discovering molecules with desirable molecular properties, including ADMET
(Absorption, Distribution, Metabolism, Excretion, and Toxicity) profiles, is of
great importance in drug discovery. Existing approaches typically employ deep
learning models, such as Graph Neural Networks (GNNs) and Transformers, to
predict these molecular properties by learning from diverse chemical
information. However, these models often fail to efficiently capture and
utilize the hierarchical nature of molecular structures, and lack mechanisms
for effective interaction among multi-level features. To address these
limitations, we propose a Hierarchical Interaction Message Passing Mechanism,
which serves as the foundation of our novel model, HimNet. Our method enables
interaction-aware representation learning across atomic, motif, and molecular
levels via hierarchical attention-guided message passing. This design allows
HimNet to effectively balance global and local information, ensuring rich and
task-relevant feature extraction for downstream property prediction tasks, such
as Blood-Brain Barrier Permeability (BBBP). Extensive experiments on multiple
benchmark datasets demonstrate that HimNet achieves the best or near-best
performance in most molecular property prediction tasks. Furthermore, our
method exhibits promising hierarchical interpretability, aligning well with
chemical intuition on representative molecules. We believe that HimNet offers
an accurate and efficient solution for molecular activity and ADMET property
prediction, contributing significantly to advanced decision-making in the early
stages of drug discovery.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [243] [A Physically Driven Long Short Term Memory Model for Estimating Snow Water Equivalent over the Continental United States](https://arxiv.org/abs/2504.20129)
*Arun M. Saranathan,Mahmoud Saeedimoghaddam,Brandon Smith,Deepthi Raghunandan,Grey Nearing,Craig Pelissier*

Main category: physics.ao-ph

TL;DR: 该论文提出了一种基于LSTM网络的模型，用于估计雪水当量（SWE），解决了传统再分析产品和现场测量的局限性。


<details>
  <summary>Details</summary>
Motivation: 季节性雪的估计通常依赖于计算昂贵的再分析产品或稀疏的现场测量，无法满足广泛分析需求。

Method: 使用LSTM网络，将SWE估计分为分类（雪的存在与否）和回归（SWE高度）任务，基于物理/气象和静态空间/形态因素的时间序列输入。

Result: 模型在雪存在分类上准确率≥93%，SWE估计的相关系数∼0.9，且能泛化到未见数据。

Conclusion: LSTM模型能有效估计SWE，适用于广泛的地理和时间范围。

Abstract: Snow is an essential input for various land surface models. Seasonal snow
estimates are available as snow water equivalent (SWE) from process-based
reanalysis products or locally from in situ measurements. While the reanalysis
products are computationally expensive and available at only fixed spatial and
temporal resolutions, the in situ measurements are highly localized and sparse.
To address these issues and enable the analysis of the effect of a large suite
of physical, morphological, and geological conditions on the presence and
amount of snow, we build a Long Short-Term Memory (LSTM) network, which is able
to estimate the SWE based on time series input of the various
physical/meteorological factors as well static spatial/morphological factors.
Specifically, this model breaks down the SWE estimation into two separate
tasks: (i) a classification task that indicates the presence/absence of snow on
a specific day and (ii) a regression task that indicates the height of the SWE
on a specific day in the case of snow presence. The model is trained using
physical/in situ SWE measurements from the SNOw TELemetry (SNOTEL) snow pillows
in the western United States. We will show that trained LSTM models have a
classification accuracy of $\geq 93\%$ for the presence of snow and a
coefficient of correlation of $\sim 0.9$ concerning their SWE estimates. We
will also demonstrate that the models can generalize both spatially and
temporally to previously unseen data.

</details>


### [244] [Testing the Limit of Atmospheric Predictability with a Machine Learning Weather Model](https://arxiv.org/abs/2504.20238)
*P. Trent Vonich,Gregory J. Hakim*

Main category: physics.ao-ph

TL;DR: 使用GraphCast机器学习模型优化初始条件，将确定性天气预报技能延长至30天以上，挑战了14天的传统预测极限。


<details>
  <summary>Details</summary>
Motivation: 挑战传统认为的14天确定性天气预报极限，探索机器学习在延长预测技能方面的潜力。

Method: 利用梯度优化技术调整初始条件，结合GraphCast模型进行2020年每日两次的预报实验。

Result: 10天预报误差平均减少86%，技能持续超过30天；优化初始条件在Pangu-Weather模型中实现21%误差减少。

Conclusion: 准确初始条件下，确定性预报技能可远超两周，颠覆了对大气可预测性极限的传统认知。

Abstract: Atmospheric predictability research has long held that the limit of skillful
deterministic weather forecasts is about 14 days. We challenge this limit using
GraphCast, a machine-learning weather model, by optimizing forecast initial
conditions using gradient-based techniques for twice-daily forecasts spanning
2020. This approach yields an average error reduction of 86% at 10 days, with
skill lasting beyond 30 days. Mean optimal initial-condition perturbations
reveal large-scale, spatially coherent corrections to ERA5, primarily
reflecting an intensification of the Hadley circulation. Forecasts using
GraphCast-optimal initial conditions in the Pangu-Weather model achieve a 21%
error reduction, peaking at 4 days, indicating that analysis corrections
reflect a combination of both model bias and a reduction in analysis error.
These results demonstrate that, given accurate initial conditions, skillful
deterministic forecasts are consistently achievable far beyond two weeks,
challenging long-standing assumptions about the limits of atmospheric
predictability.

</details>


### [245] [Data Driven Deep Learning for Correcting Global Climate Model Projections of SST and DSL in the Bay of Bengal](https://arxiv.org/abs/2504.20620)
*Abhishek Pasula,Deepak N. Subramani*

Main category: physics.ao-ph

TL;DR: 论文提出了一种基于深度学习的偏差校正方法，用于改进CMIP6气候模型在孟加拉湾的预测准确性，相比传统方法显著降低了误差。


<details>
  <summary>Details</summary>
Motivation: 气候变化影响海洋条件，尤其是孟加拉湾的温度和海平面变化对季风降水和海洋生产力至关重要，但现有气候模型与再分析数据存在显著差异。

Method: 使用深度学习模型，以气候模型输出为输入、ORAS5再分析数据为输出进行训练，并通过历史数据和未来数据进行验证和测试。

Result: 相比传统EDCDF方法，新方法将SST和DSL的RMSE分别降低了0.15°C和0.3米。

Conclusion: 新方法显著提高了气候模型的预测准确性，为未来气候动态分析提供了更可靠的数据。

Abstract: Climate change alters ocean conditions, notably temperature and sea level. In
the Bay of Bengal, these changes influence monsoon precipitation and marine
productivity, critical to the Indian economy. In Phase 6 of the Coupled Model
Intercomparison Project (CMIP6), Global Climate Models (GCMs) use different
shared socioeconomic pathways (SSPs) to obtain future climate projections.
However, significant discrepancies are observed between these models and the
reanalysis data in the Bay of Bengal for 2015-2024. Specifically, the root mean
square error (RMSE) between the climate model output and the Ocean Reanalysis
System (ORAS5) is 1.2C for the sea surface temperature (SST) and 1.1 m for the
dynamic sea level (DSL). We introduce a new data-driven deep learning model to
correct for this bias. The deep neural model for each variable is trained using
pairs of climatology-removed monthly climate projections as input and the
corresponding month's ORAS5 as output. This model is trained with historical
data (1950 to 2014), validated with future projection data from 2015 to 2020,
and tested with future projections from 2021 to 2023. Compared to the
conventional EquiDistant Cumulative Distribution Function (EDCDF) statistical
method for bias correction in climate models, our approach decreases RMSE by
0.15C for SST and 0.3 m for DSL. The trained model subsequently corrects the
projections for 2024-2100. A detailed analysis of the monthly, seasonal, and
decadal means and variability is performed to underscore the implications of
the novel dynamics uncovered in our corrected projections.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [246] [Generate more than one child in your co-evolutionary semi-supervised learning GAN](https://arxiv.org/abs/2504.20560)
*Francisco Sedeño,Jamal Toutouh,Francisco Chicano*

Main category: cs.NE

TL;DR: 本文提出了一种新的协同进化方法CE-SSLGAN，通过改进种群结构和替换策略，提升了SSL-GAN的性能。


<details>
  <summary>Details</summary>
Motivation: 现有协同进化SSL-GAN方法基于空间结构种群和单一个体生成策略，限制了性能提升。

Method: 提出CE-SSLGAN，采用泛种群结构、精英替换策略和多子代生成。

Result: 在三个标准数据集上验证，CE-SSLGAN性能优于传统SSL-GAN。

Conclusion: 多子代和精英策略显著提升了SSL-GAN的效果。

Abstract: Generative Adversarial Networks (GANs) are very useful methods to address
semi-supervised learning (SSL) datasets, thanks to their ability to generate
samples similar to real data. This approach, called SSL-GAN has attracted many
researchers in the last decade. Evolutionary algorithms have been used to guide
the evolution and training of SSL-GANs with great success. In particular,
several co-evolutionary approaches have been applied where the two networks of
a GAN (the generator and the discriminator) are evolved in separate
populations. The co-evolutionary approaches published to date assume some
spatial structure of the populations, based on the ideas of cellular
evolutionary algorithms. They also create one single individual per generation
and follow a generational replacement strategy in the evolution. In this paper,
we re-consider those algorithmic design decisions and propose a new
co-evolutionary approach, called Co-evolutionary Elitist SSL-GAN (CE-SSLGAN),
with panmictic population, elitist replacement, and more than one individual in
the offspring. We evaluate the performance of our proposed method using three
standard benchmark datasets. The results show that creating more than one
offspring per population and using elitism improves the results in comparison
with a classical SSL-GAN.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [247] [TrueFake: A Real World Case Dataset of Last Generation Fake Images also Shared on Social Networks](https://arxiv.org/abs/2504.20658)
*Stefano Dell'Anna,Andrea Montibeller,Giulia Boato*

Main category: cs.MM

TL;DR: TrueFake数据集用于评估社交媒体压缩对假图像检测的影响，发现现有模型需更贴近真实场景的评估。


<details>
  <summary>Details</summary>
Motivation: 社交媒体压缩和处理会降低假图像检测的准确性，现有工具未能充分应对这一挑战。

Method: 创建TrueFake数据集（60万张图像），涵盖多种生成技术和社交媒体分享，用于评估检测器性能。

Result: 实验表明社交媒体分享显著影响检测性能，并确定了当前最有效的检测和训练策略。

Conclusion: 需在更贴近真实世界的条件下评估法证模型，以提高检测效果。

Abstract: AI-generated synthetic media are increasingly used in real-world scenarios,
often with the purpose of spreading misinformation and propaganda through
social media platforms, where compression and other processing can degrade fake
detection cues. Currently, many forensic tools fail to account for these
in-the-wild challenges. In this work, we introduce TrueFake, a large-scale
benchmarking dataset of 600,000 images including top notch generative
techniques and sharing via three different social networks. This dataset allows
for rigorous evaluation of state-of-the-art fake image detectors under very
realistic and challenging conditions. Through extensive experimentation, we
analyze how social media sharing impacts detection performance, and identify
current most effective detection and training strategies. Our findings
highlight the need for evaluating forensic models in conditions that mirror
real-world use.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [248] [DRO: Doppler-Aware Direct Radar Odometry](https://arxiv.org/abs/2504.20339)
*Cedric Le Gentil,Leonardo Brizi,Daniil Lisus,Xinyuan Qiao,Giorgio Grisetti,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: 提出了一种基于SE(2)的雷达里程计方法，直接利用雷达强度信息进行扫描到局部地图的配准，无需特征提取，支持几何特征缺失场景。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达能穿透薄壁、植被和恶劣天气，弥补相机和激光雷达的不足，适用于移动机器人应用。

Method: 直接配准雷达强度信息，结合运动和多普勒失真校正，利用多普勒约束提升速度估计。

Result: 在公开数据集上验证，相对平移误差低至0.18%，优于现有方法。

Conclusion: 该方法在几何特征缺失场景中表现优异，实时实现已开源。

Abstract: A renaissance in radar-based sensing for mobile robotic applications is
underway. Compared to cameras or lidars, millimetre-wave radars have the
ability to `see' through thin walls, vegetation, and adversarial weather
conditions such as heavy rain, fog, snow, and dust. In this paper, we propose a
novel SE(2) odometry approach for spinning frequency-modulated continuous-wave
radars. Our method performs scan-to-local-map registration of the incoming
radar data in a direct manner using all the radar intensity information without
the need for feature or point cloud extraction. The method performs locally
continuous trajectory estimation and accounts for both motion and Doppler
distortion of the radar scans. If the radar possesses a specific frequency
modulation pattern that makes radial Doppler velocities observable, an
additional Doppler-based constraint is formulated to improve the velocity
estimate and enable odometry in geometrically feature-deprived scenarios (e.g.,
featureless tunnels). Our method has been validated on over 250km of on-road
data sourced from public datasets (Boreas and MulRan) and collected using our
automotive platform. With the aid of a gyroscope, it outperforms
state-of-the-art methods and achieves an average relative translation error of
0.26% on the Boreas leaderboard. When using data with the appropriate
Doppler-enabling frequency modulation pattern, the translation error is reduced
to 0.18% in similar environments. We also benchmarked our algorithm using 1.5
hours of data collected with a mobile robot in off-road environments with
various levels of structure to demonstrate its versatility. Our real-time
implementation is publicly available: https://github.com/utiasASRL/dro.

</details>


### [249] [Hydra: Marker-Free RGB-D Hand-Eye Calibration](https://arxiv.org/abs/2504.20584)
*Martin Huber,Huanyu Tian,Christopher E. Mower,Lucas-Raphael Müller,Sébastien Ourselin,Christos Bergeles,Tom Vercauteren*

Main category: cs.RO

TL;DR: 提出了一种基于RGB-D成像的无标记手眼标定方法，采用改进的ICP算法和鲁棒点对面目标函数，在Lie代数上实现。实验表明，仅需三个随机机器人配置即可实现约90%的成功率，收敛速度比基线方法快2-3倍，且精度更高。


<details>
  <summary>Details</summary>
Motivation: 传统手眼标定方法依赖标记物，限制了应用场景。本文旨在开发一种无需标记物的高效标定方法。

Method: 结合RGB-D成像和改进的ICP算法，使用鲁棒点对面目标函数在Lie代数上实现标定。

Result: 仅需三个随机配置即可实现90%成功率，收敛速度更快（0.8秒），精度更高（5毫米）。

Conclusion: 该方法在无标记条件下显著提升了标定效率和精度，适用于实际部署。

Abstract: This work presents an RGB-D imaging-based approach to marker-free hand-eye
calibration using a novel implementation of the iterative closest point (ICP)
algorithm with a robust point-to-plane (PTP) objective formulated on a Lie
algebra. Its applicability is demonstrated through comprehensive experiments
using three well known serial manipulators and two RGB-D cameras. With only
three randomly chosen robot configurations, our approach achieves approximately
90% successful calibrations, demonstrating 2-3x higher convergence rates to the
global optimum compared to both marker-based and marker-free baselines. We also
report 2 orders of magnitude faster convergence time (0.8 +/- 0.4 s) for 9
robot configurations over other marker-free methods. Our method exhibits
significantly improved accuracy (5 mm in task space) over classical approaches
(7 mm in task space) whilst being marker-free. The benchmarking dataset and
code are open sourced under Apache 2.0 License, and a ROS 2 integration with
robot abstraction is provided to facilitate deployment.

</details>


### [250] [Learning a General Model: Folding Clothing with Topological Dynamics](https://arxiv.org/abs/2504.20720)
*Yiming Liu,Lijun Han,Enlin Gu,Hesheng Wang*

Main category: cs.RO

TL;DR: 提出了一种基于拓扑动力学模型的衣物折叠方法，利用拓扑图表示衣物状态，结合语义分割和图神经网络（GNN）预测衣物变形，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 衣物自由度多且结构复杂，传统方法难以处理其折叠问题，需一种通用模型来应对复杂衣物的多种折叠状态。

Method: 通过语义分割分析遮挡关系，结合关键点检测生成拓扑图，利用改进的GNN学习衣物动态并预测变形。

Result: 实验证明该方法能有效识别和折叠具有自遮挡的复杂衣物（如夹克）。

Conclusion: 拓扑动力学模型结合GNN为复杂衣物折叠提供了可行的解决方案。

Abstract: The high degrees of freedom and complex structure of garments present
significant challenges for clothing manipulation. In this paper, we propose a
general topological dynamics model to fold complex clothing. By utilizing the
visible folding structure as the topological skeleton, we design a novel
topological graph to represent the clothing state. This topological graph is
low-dimensional and applied for complex clothing in various folding states. It
indicates the constraints of clothing and enables predictions regarding
clothing movement. To extract graphs from self-occlusion, we apply semantic
segmentation to analyze the occlusion relationships and decompose the clothing
structure. The decomposed structure is then combined with keypoint detection to
generate the topological graph. To analyze the behavior of the topological
graph, we employ an improved Graph Neural Network (GNN) to learn the general
dynamics. The GNN model can predict the deformation of clothing and is employed
to calculate the deformation Jacobi matrix for control. Experiments using
jackets validate the algorithm's effectiveness to recognize and fold complex
clothing with self-occlusion.

</details>


### [251] [A Survey on Event-based Optical Marker Systems](https://arxiv.org/abs/2504.20736)
*Nafiseh Jabbari Tofighi,Maxime Robic,Fabio Morbidi,Pascal Vasseur*

Main category: cs.RO

TL;DR: 本文综述了事件驱动的光学标记系统（EBOMS），分析了其异步操作和对恶劣光照条件的鲁棒性，并探讨了其在目标检测、姿态估计等领域的应用及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 事件相机的低延迟、高动态范围和低功耗特性为机器人视觉和机器感知带来了变革，结合光学标记系统（如AprilTags、LED阵列）为研究开辟了新方向。

Method: 综述了EBOMS的基本原理和技术，特别关注其异步操作和对光照条件的适应性。

Result: 总结了EBOMS在目标检测、跟踪、姿态估计和光通信等领域的应用。

Conclusion: 提出了这一新兴跨学科领域未来的研究方向。

Abstract: The advent of event-based cameras, with their low latency, high dynamic
range, and reduced power consumption, marked a significant change in robotic
vision and machine perception. In particular, the combination of these
neuromorphic sensors with widely-available passive or active optical markers
(e.g. AprilTags, arrays of blinking LEDs), has recently opened up a wide field
of possibilities. This survey paper provides a comprehensive review on
Event-Based Optical Marker Systems (EBOMS). We analyze the basic principles and
technologies on which these systems are based, with a special focus on their
asynchronous operation and robustness against adverse lighting conditions. We
also describe the most relevant applications of EBOMS, including object
detection and tracking, pose estimation, and optical communication. The article
concludes with a discussion of possible future research directions in this
rapidly-emerging and multidisciplinary field.

</details>


### [252] [PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations](https://arxiv.org/abs/2504.20520)
*Haowen Sun,Han Wang,Chengzhong Ma,Shaolong Zhang,Jiawei Ye,Xingyu Chen,Xuguang Lan*

Main category: cs.RO

TL;DR: 提出了一种结合真实到模拟再到真实的流程，通过专家演示构建模拟环境，并利用视觉语言模型（VLM）监督的投影奖励模型训练RL策略，最终实现可靠的机器人控制策略部署。


<details>
  <summary>Details</summary>
Motivation: 解决机器人初始位置和物体姿态变化下从少量演示中学习鲁棒策略的挑战，避免直接交互的不安全性和模拟环境构建的高成本。

Method: 1. 基于专家演示构建模拟环境；2. 使用VLM监督的投影奖励模型训练RL策略；3. 通过专家演示微调策略。

Result: 实现了在真实场景中部署鲁棒的机器人控制策略。

Conclusion: 提出的集成流程有效解决了模拟环境构建和RL策略训练的挑战，为机器人控制提供了实用解决方案。

Abstract: Learning from few demonstrations to develop policies robust to variations in
robot initial positions and object poses is a problem of significant practical
interest in robotics. Compared to imitation learning, which often struggles to
generalize from limited samples, reinforcement learning (RL) can autonomously
explore to obtain robust behaviors. Training RL agents through direct
interaction with the real world is often impractical and unsafe, while building
simulation environments requires extensive manual effort, such as designing
scenes and crafting task-specific reward functions. To address these
challenges, we propose an integrated real-to-sim-to-real pipeline that
constructs simulation environments based on expert demonstrations by
identifying scene objects from images and retrieving their corresponding 3D
models from existing libraries. We introduce a projection-based reward model
for RL policy training that is supervised by a vision-language model (VLM)
using human-guided object projection relationships as prompts, with the policy
further fine-tuned using expert demonstrations. In general, our work focuses on
the construction of simulation environments and RL-based policy training,
ultimately enabling the deployment of reliable robotic control policies in
real-world scenarios.

</details>


### [253] [SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings](https://arxiv.org/abs/2504.20808)
*Florian Vahl,Jörn Griepenburg,Jan Gutsche,Jasper Güldenstein,Jianwei Zhang*

Main category: cs.RO

TL;DR: SoccerDiffusion是一种基于Transformer的扩散模型，用于从真实足球比赛录像中学习人形机器人的端到端控制策略。


<details>
  <summary>Details</summary>
Motivation: 通过从RoboCup比赛中收集的数据，直接学习机器人足球的控制策略，减少对人工设计的依赖。

Method: 使用多模态传感器输入（视觉、本体感觉和游戏状态）预测关节命令轨迹，并通过蒸馏技术实现嵌入式平台上的实时推理。

Result: 模型在仿真和物理机器人上成功复现了行走、踢球和跌倒恢复等复杂动作行为。

Conclusion: 尽管高级战术行为有限，但为后续强化学习或偏好优化方法提供了坚实基础。数据集、预训练模型和代码已开源。

Abstract: This paper introduces SoccerDiffusion, a transformer-based diffusion model
designed to learn end-to-end control policies for humanoid robot soccer
directly from real-world gameplay recordings. Using data collected from RoboCup
competitions, the model predicts joint command trajectories from multi-modal
sensor inputs, including vision, proprioception, and game state. We employ a
distillation technique to enable real-time inference on embedded platforms that
reduces the multi-step diffusion process to a single step. Our results
demonstrate the model's ability to replicate complex motion behaviors such as
walking, kicking, and fall recovery both in simulation and on physical robots.
Although high-level tactical behavior remains limited, this work provides a
robust foundation for subsequent reinforcement learning or preference
optimization methods. We release the dataset, pretrained models, and code
under: https://bit-bots.github.io/SoccerDiffusion

</details>


### [254] [XPG-RL: Reinforcement Learning with Explainable Priority Guidance for Efficiency-Boosted Mechanical Search](https://arxiv.org/abs/2504.20969)
*Yiting Zhang,Shichen Li,Elena Shrestha*

Main category: cs.RO

TL;DR: XPG-RL是一种强化学习框架，通过可解释的优先级决策解决机械搜索任务，结合感知模块和动态动作选择策略，显著提升任务成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 解决机械搜索任务中因遮挡和部分可观测性导致的长时规划和状态估计困难。

Method: XPG-RL结合任务驱动的动作优先级机制和上下文感知切换策略，动态选择动作原语（如抓取、移除遮挡、调整视角），并优化策略输出自适应阈值。感知模块融合RGB-D输入与语义几何特征。

Result: 在仿真和现实实验中，XPG-RL任务成功率和运动效率显著优于基线方法，长时任务效率提升高达4.5倍。

Conclusion: 结合领域知识和可学习决策策略能实现高效、鲁棒的机器人操作。

Abstract: Mechanical search (MS) in cluttered environments remains a significant
challenge for autonomous manipulators, requiring long-horizon planning and
robust state estimation under occlusions and partial observability. In this
work, we introduce XPG-RL, a reinforcement learning framework that enables
agents to efficiently perform MS tasks through explainable, priority-guided
decision-making based on raw sensory inputs. XPG-RL integrates a task-driven
action prioritization mechanism with a learned context-aware switching strategy
that dynamically selects from a discrete set of action primitives such as
target grasping, occlusion removal, and viewpoint adjustment. Within this
strategy, a policy is optimized to output adaptive threshold values that govern
the discrete selection among action primitives. The perception module fuses
RGB-D inputs with semantic and geometric features to produce a structured scene
representation for downstream decision-making. Extensive experiments in both
simulation and real-world settings demonstrate that XPG-RL consistently
outperforms baseline methods in task success rates and motion efficiency,
achieving up to 4.5$\times$ higher efficiency in long-horizon tasks. These
results underscore the benefits of integrating domain knowledge with learnable
decision-making policies for robust and efficient robotic manipulation.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [255] [Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting](https://arxiv.org/abs/2504.20403)
*Hanxi Liu,Yifang Men,Zhouhui Lian*

Main category: cs.GR

TL;DR: 提出了一种基于Tetrahedron-constrained Gaussian Splatting (TetGS)的框架，用于生成可编辑的3D虚拟形象，具有局部空间适应和真实外观学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D编辑方法在复杂场景中难以生成视觉上令人满意的结果，需要一种用户友好的解决方案。

Method: 采用TetGS作为底层表示，分三阶段优化：3D虚拟形象实例化、局部空间适应和几何外观生成。

Result: 实验证明该方法在生成逼真3D可编辑虚拟形象方面效果显著。

Conclusion: TetGS框架为普通用户提供了一种高效且逼真的3D虚拟形象编辑方案。

Abstract: Personalized 3D avatar editing holds significant promise due to its
user-friendliness and availability to applications such as AR/VR and virtual
try-ons. Previous studies have explored the feasibility of 3D editing, but
often struggle to generate visually pleasing results, possibly due to the
unstable representation learning under mixed optimization of geometry and
texture in complicated reconstructed scenarios. In this paper, we aim to
provide an accessible solution for ordinary users to create their editable 3D
avatars with precise region localization, geometric adaptability, and
photorealistic renderings. To tackle this challenge, we introduce a
meticulously designed framework that decouples the editing process into local
spatial adaptation and realistic appearance learning, utilizing a hybrid
Tetrahedron-constrained Gaussian Splatting (TetGS) as the underlying
representation. TetGS combines the controllable explicit structure of
tetrahedral grids with the high-precision rendering capabilities of 3D Gaussian
Splatting and is optimized in a progressive manner comprising three stages: 3D
avatar instantiation from real-world monocular videos to provide accurate
priors for TetGS initialization; localized spatial adaptation with explicitly
partitioned tetrahedrons to guide the redistribution of Gaussian kernels; and
geometry-based appearance generation with a coarse-to-fine activation strategy.
Both qualitative and quantitative experiments demonstrate the effectiveness and
superiority of our approach in generating photorealistic 3D editable avatars.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [256] [Nonlinear Computation with Linear Optics via Source-Position Encoding](https://arxiv.org/abs/2504.20401)
*N. Richardson,C. Bosch,R. P. Adams*

Main category: physics.optics

TL;DR: 提出一种在完全线性介质中实现非线性计算的新方法，通过位置编码和拓扑优化设计光学神经网络。


<details>
  <summary>Details</summary>
Motivation: 光学计算系统在神经网络任务中具有潜力，但缺乏高效的非线性实现方法。

Method: 利用数据依赖的空间位置驱动光学系统，结合拓扑优化设计专用光学神经网络。

Result: 在分类任务中表现优于线性方法，与标准人工神经网络性能相当。

Conclusion: 该方法为光学神经网络提供了高效的非线性计算解决方案。

Abstract: Optical computing systems provide an alternate hardware model which appears
to be aligned with the demands of neural network workloads. However, the
challenge of implementing energy efficient nonlinearities in optics -- a key
requirement for realizing neural networks -- is a conspicuous missing link. In
this work we introduce a novel method to achieve nonlinear computation in fully
linear media. Our method can operate at low power and requires only the ability
to drive the optical system at a data-dependent spatial position. Leveraging
this positional encoding, we formulate a fully automated,
topology-optimization-based hardware design framework for extremely specialized
optical neural networks, drawing on modern advancements in optimization and
machine learning. We evaluate our optical designs on machine learning
classification tasks: demonstrating significant improvements over linear
methods, and competitive performance when compared to standard artificial
neural networks.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [257] [Pediatric Asthma Detection with Googles HeAR Model: An AI-Driven Respiratory Sound Classifier](https://arxiv.org/abs/2504.20124)
*Abul Ehtesham,Saket Kumar,Aditi Singh,Tala Talaei Khoei*

Main category: cs.SD

TL;DR: AI利用HeAR模型从儿童呼吸音中早期检测哮喘，准确率超91%，适用于远程医疗。


<details>
  <summary>Details</summary>
Motivation: 早期发现儿童哮喘可减少长期并发症和紧急干预，需非侵入性筛查方法。

Method: 使用SPRSound数据集，通过HeAR模型提取呼吸音特征，训练多种分类器进行哮喘检测。

Result: 系统准确率超91%，在阳性病例的精确召回指标上表现优异。

Conclusion: 短时、低资源录音结合基础音频模型，可实现快速非侵入性哮喘筛查，适用于偏远地区。

Abstract: Early detection of asthma in children is crucial to prevent long-term
respiratory complications and reduce emergency interventions. This work
presents an AI-powered diagnostic pipeline that leverages Googles Health
Acoustic Representations (HeAR) model to detect early signs of asthma from
pediatric respiratory sounds. The SPRSound dataset, the first open-access
collection of annotated respiratory sounds in children aged 1 month to 18
years, is used to extract 2-second audio segments labeled as wheeze, crackle,
rhonchi, stridor, or normal. Each segment is embedded into a 512-dimensional
representation using HeAR, a foundation model pretrained on 300 million
health-related audio clips, including 100 million cough sounds. Multiple
classifiers, including SVM, Random Forest, and MLP, are trained on these
embeddings to distinguish between asthma-indicative and normal sounds. The
system achieves over 91\% accuracy, with strong performance on precision-recall
metrics for positive cases. In addition to classification, learned embeddings
are visualized using PCA, misclassifications are analyzed through waveform
playback, and ROC and confusion matrix insights are provided. This method
demonstrates that short, low-resource pediatric recordings, when powered by
foundation audio models, can enable fast, noninvasive asthma screening. The
approach is especially promising for digital diagnostics in remote or
underserved healthcare settings.

</details>


### [258] [End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based Approach with Cross-Dataset Evaluation](https://arxiv.org/abs/2504.20923)
*Andrea Di Pierno,Luca Guarnera,Dario Allegra,Sebastiano Battiato*

Main category: cs.SD

TL;DR: 提出了一种轻量级端到端深度学习框架RawNetLite，用于检测音频深度伪造，通过多领域数据训练和Focal Loss提升鲁棒性，在多种测试集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 音频深度伪造对数字安全和信任构成威胁，现有检测方法在开放世界条件下（测试与训练数据不一致）效果有限。

Method: 设计轻量级卷积-循环架构RawNetLite，直接处理原始波形，结合多领域数据和Focal Loss训练策略，并引入音频增强技术。

Result: 在FakeOrReal数据集上F1达99.7%，EER为0.25%；在AVSpoof2021 + CodecFake数据集上F1达83.4%，EER为16.4%。

Conclusion: 多样化训练数据、定制目标函数和音频增强对构建鲁棒且泛化的音频伪造检测器至关重要。

Abstract: Audio deepfakes represent a growing threat to digital security and trust,
leveraging advanced generative models to produce synthetic speech that closely
mimics real human voices. Detecting such manipulations is especially
challenging under open-world conditions, where spoofing methods encountered
during testing may differ from those seen during training. In this work, we
propose an end-to-end deep learning framework for audio deepfake detection that
operates directly on raw waveforms. Our model, RawNetLite, is a lightweight
convolutional-recurrent architecture designed to capture both spectral and
temporal features without handcrafted preprocessing. To enhance robustness, we
introduce a training strategy that combines data from multiple domains and
adopts Focal Loss to emphasize difficult or ambiguous samples. We further
demonstrate that incorporating codec-based manipulations and applying
waveform-level audio augmentations (e.g., pitch shifting, noise, and time
stretching) leads to significant generalization improvements under realistic
acoustic conditions. The proposed model achieves over 99.7% F1 and 0.25% EER on
in-domain data (FakeOrReal), and up to 83.4% F1 with 16.4% EER on a challenging
out-of-distribution test set (AVSpoof2021 + CodecFake). These findings
highlight the importance of diverse training data, tailored objective functions
and audio augmentations in building resilient and generalizable audio forgery
detectors. Code and pretrained models are available at
https://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/.

</details>


### [259] [APG-MOS: Auditory Perception Guided-MOS Predictor for Synthetic Speech](https://arxiv.org/abs/2504.20447)
*Zhicheng Lian,Lizhi Wang,Hua Huang*

Main category: cs.SD

TL;DR: APG-MOS模型通过结合听觉感知机制和语义分析，提升语音质量评估与人类判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 减少人工评估的耗时，同时解决现有深度学习模型忽视听觉感知机制的问题。

Method: 设计了基于生物听觉机制的感知模块、RVQ语义失真建模方法，以及残差交叉注意力架构。

Result: APG-MOS在两个主要基准测试中表现优异。

Conclusion: APG-MOS通过多模态融合显著提升了语音质量评估的准确性。

Abstract: Automatic speech quality assessment aims to quantify subjective human
perception of speech through computational models to reduce the need for
labor-consuming manual evaluations. While models based on deep learning have
achieved progress in predicting mean opinion scores (MOS) to assess synthetic
speech, the neglect of fundamental auditory perception mechanisms limits
consistency with human judgments. To address this issue, we propose an auditory
perception guided-MOS prediction model (APG-MOS) that synergistically
integrates auditory modeling with semantic analysis to enhance consistency with
human judgments. Specifically, we first design a perceptual module, grounded in
biological auditory mechanisms, to simulate cochlear functions, which encodes
acoustic signals into biologically aligned electrochemical representations.
Secondly, we propose a residual vector quantization (RVQ)-based semantic
distortion modeling method to quantify the degradation of speech quality at the
semantic level. Finally, we design a residual cross-attention architecture,
coupled with a progressive learning strategy, to enable multimodal fusion of
encoded electrochemical signals and semantic representations. Experiments
demonstrate that APG-MOS achieves superior performance on two primary
benchmarks. Our code and checkpoint will be available on a public repository
upon publication.

</details>


### [260] [DiffusionRIR: Room Impulse Response Interpolation using Diffusion Models](https://arxiv.org/abs/2504.20625)
*Sagi Della Torre,Mirco Pezzoli,Fabio Antonacci,Sharon Gannot*

Main category: cs.SD

TL;DR: 该研究利用去噪扩散概率模型（DDPM）估计房间内未测量位置的房间脉冲响应（RIR），通过将RIR数据转化为适合扩散重建的格式，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 高空间分辨率的RIR测量资源密集，难以在大空间或密集采样场景中实现，因此需要一种高效的方法估计未测量位置的RIR。

Method: 将RIR矩阵类比为图像修复问题，使用DDPM进行重建，并在不同曲率的麦克风阵列上验证。

Result: 该方法在麦克风间距较大时仍能准确重建RIR，在归一化均方误差和余弦距离上显著优于基线插值方法。

Conclusion: 生成模型在RIR插值中具有潜力，为从有限实测数据生成额外数据提供了新途径。

Abstract: Room Impulse Responses (RIRs) characterize acoustic environments and are
crucial in multiple audio signal processing tasks. High-quality RIR estimates
drive applications such as virtual microphones, sound source localization,
augmented reality, and data augmentation. However, obtaining RIR measurements
with high spatial resolution is resource-intensive, making it impractical for
large spaces or when dense sampling is required. This research addresses the
challenge of estimating RIRs at unmeasured locations within a room using
Denoising Diffusion Probabilistic Models (DDPM). Our method leverages the
analogy between RIR matrices and image inpainting, transforming RIR data into a
format suitable for diffusion-based reconstruction.
  Using simulated RIR data based on the image method, we demonstrate our
approach's effectiveness on microphone arrays of different curvatures, from
linear to semi-circular. Our method successfully reconstructs missing RIRs,
even in large gaps between microphones. Under these conditions, it achieves
accurate reconstruction, significantly outperforming baseline Spline Cubic
Interpolation in terms of Normalized Mean Square Error and Cosine Distance
between actual and interpolated RIRs.
  This research highlights the potential of using generative models for
effective RIR interpolation, paving the way for generating additional data from
limited real-world measurements.

</details>


### [261] [ECOSoundSet: a finely annotated dataset for the automated acoustic identification of Orthoptera and Cicadidae in North, Central and temperate Western Europe](https://arxiv.org/abs/2504.20776)
*David Funosas,Elodie Massol,Yves Bas,Svenja Schmidt,Dominik Arend,Alexander Gebhard,Luc Barbaro,Sebastian König,Rafael Carbonell Font,David Sannier,Fernand Deroussen,Jérôme Sueur,Christian Roesti,Tomi Trilar,Wolfgang Forstmeier,Lucas Roger,Eloïsa Matheu,Piotr Guzik,Julien Barataud,Laurent Pelozuelo,Stéphane Puissant,Sandra Mueller,Björn Schuller,Jose M. Montoya,Andreas Triantafyllopoulos,Maxime Cauchoix*

Main category: cs.SD

TL;DR: ECOSoundSet是一个包含欧洲直翅目和蝉类物种声音的数据集，旨在支持深度学习算法在自然声景中的自动识别。


<details>
  <summary>Details</summary>
Motivation: 现有工具在欧洲昆虫声音识别方面范围有限，需要大规模生态异质性数据集以提升算法性能。

Method: 通过实地采集和欧洲昆虫学家贡献，构建了包含粗标注和精细标注的10,653条录音数据集，并提供训练/验证/测试分割。

Result: 数据集覆盖200种直翅目和24种蝉类，支持深度学习算法的训练与评估。

Conclusion: ECOSoundSet可作为现有在线资源的补充，提升欧洲直翅目和蝉类声音分类的深度学习效果。

Abstract: Currently available tools for the automated acoustic recognition of European
insects in natural soundscapes are limited in scope. Large and ecologically
heterogeneous acoustic datasets are currently needed for these algorithms to
cross-contextually recognize the subtle and complex acoustic signatures
produced by each species, thus making the availability of such datasets a key
requisite for their development. Here we present ECOSoundSet (European
Cicadidae and Orthoptera Sound dataSet), a dataset containing 10,653 recordings
of 200 orthopteran and 24 cicada species (217 and 26 respective taxa when
including subspecies) present in North, Central, and temperate Western Europe
(Andorra, Belgium, Denmark, mainland France and Corsica, Germany, Ireland,
Luxembourg, Monaco, Netherlands, United Kingdom, Switzerland), collected partly
through targeted fieldwork in South France and Catalonia and partly through
contributions from various European entomologists. The dataset is composed of a
combination of coarsely labeled recordings, for which we can only infer the
presence, at some point, of their target species (weak labeling), and finely
annotated recordings, for which we know the specific time and frequency range
of each insect sound present in the recording (strong labeling). We also
provide a train/validation/test split of the strongly labeled recordings, with
respective approximate proportions of 0.8, 0.1 and 0.1, in order to facilitate
their incorporation in the training and evaluation of deep learning algorithms.
This dataset could serve as a meaningful complement to recordings already
available online for the training of deep learning algorithms for the acoustic
classification of orthopterans and cicadas in North, Central, and temperate
Western Europe.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [262] [On Stochastic Rounding with Few Random Bits](https://arxiv.org/abs/2504.20634)
*Andrew Fitzgibbon,Stephen Felix*

Main category: math.NA

TL;DR: 论文探讨了低精度浮点计算中少位随机舍入（FBSR）的实现及其潜在偏差，分析了这些偏差对机器学习的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大规模数值计算中低精度浮点格式和混合精度算术的普及，随机舍入（SR）技术受到关注。然而，高质量随机位的生成成本高，因此需要研究如何在减少随机位使用的同时保持SR的优良特性。

Method: 研究了几种少位随机舍入（FBSR）的实现方式，分析了这些实现可能引入的偏差。

Result: 发现某些自然实现方式会引入显著偏差，而这些偏差在无限位和无限精度的分析中并不存在。

Conclusion: 论文揭示了低精度浮点计算中少位随机舍入的潜在偏差问题，提醒开发者在采用低精度浮点技术时需注意此类配置参数。

Abstract: Large-scale numerical computations make increasing use of low-precision (LP)
floating point formats and mixed precision arithmetic, which can be enhanced by
the technique of stochastic rounding (SR), that is, rounding an intermediate
high-precision value up or down randomly as a function of the value's distance
to the two rounding candidates. Stochastic rounding requires, in addition to
the high-precision input value, a source of random bits. As the provision of
high-quality random bits is an additional computational cost, it is of interest
to require as few bits as possible while maintaining the desirable properties
of SR in a given computation, or computational domain. This paper examines a
number of possible implementations of few-bit stochastic rounding (FBSR), and
shows how several natural implementations can introduce sometimes significant
bias into the rounding process, which are not present in the case of
infinite-bit, infinite-precision examinations of these implementations. The
paper explores the impact of these biases in machine learning examples, and
hence opens another class of configuration parameters of which practitioners
should be aware when developing or adopting low-precision floating point. Code
is available at
http://github.com/graphcore-research/arith25-stochastic-rounding.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [263] [Predictive AI with External Knowledge Infusion for Stocks](https://arxiv.org/abs/2504.20058)
*Ambedkar Dukkipati,Kawin Mayilvaghanan,Naveen Kumar Pallekonda,Sai Prakash Hadnoor,Ranga Shaarad Ayyagari*

Main category: q-fin.ST

TL;DR: 论文提出了一种结合历史趋势和外部知识（来自时间知识图谱）的学习机制，用于预测受外部因素影响的股票价格波动。


<details>
  <summary>Details</summary>
Motivation: 股票价格波动受多种动态外部因素影响，现有方法未充分整合这些因素。

Method: 构建时间知识图谱数据集，并基于图上的霍克斯过程建模外部关系。

Result: 动态表示能有效按收益对股票排序，在多个持有期内优于基线方法。

Conclusion: 结合外部知识的学习机制能提升股票预测性能。

Abstract: Fluctuations in stock prices are influenced by a complex interplay of factors
that go beyond mere historical data. These factors, themselves influenced by
external forces, encompass inter-stock dynamics, broader economic factors,
various government policy decisions, outbreaks of wars, etc. Furthermore, all
of these factors are dynamic and exhibit changes over time. In this paper, for
the first time, we tackle the forecasting problem under external influence by
proposing learning mechanisms that not only learn from historical trends but
also incorporate external knowledge from temporal knowledge graphs. Since there
are no such datasets or temporal knowledge graphs available, we study this
problem with stock market data, and we construct comprehensive temporal
knowledge graph datasets. In our proposed approach, we model relations on
external temporal knowledge graphs as events of a Hawkes process on graphs.
With extensive experiments, we show that learned dynamic representations
effectively rank stocks based on returns across multiple holding periods,
outperforming related baselines on relevant metrics.

</details>


### [264] [Deep Learning vs. Black-Scholes: Option Pricing Performance on Brazilian Petrobras Stocks](https://arxiv.org/abs/2504.20088)
*Joao Felipe Gueiros,Hemanth Chandravamsi,Steven H. Frankel*

Main category: q-fin.ST

TL;DR: 该论文研究了深度残差网络在Petrobras欧式期权定价中的应用，并与Black-Scholes模型进行了性能比较。结果表明，深度学习模型在误差和长期预测准确性上优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 探索深度学习在金融建模中的潜力，特别是在期权定价领域，以解决传统模型（如Black-Scholes）在长期预测中的局限性。

Method: 使用八年的历史数据训练深度残差网络，采用结合市场数据和分析定价的混合损失函数，数据来自巴西证券交易所。

Result: 深度学习模型在测试集上比Black-Scholes模型减少了64.3%的平均绝对误差，且在长期到期日预测中表现更准确。

Conclusion: 深度学习在金融建模中具有显著潜力，未来可针对不同价格区间开发专用模型。

Abstract: This paper explores the use of deep residual networks for pricing European
options on Petrobras, one of the world's largest oil and gas producers, and
compares its performance with the Black-Scholes (BS) model. Using eight years
of historical data from B3 (Brazilian Stock Exchange) collected via web
scraping, a deep learning model was trained using a custom built hybrid loss
function that incorporates market data and analytical pricing. The data for
training and testing were drawn between the period spanning November 2016 to
January 2025, using an 80-20 train-test split. The test set consisted of data
from the final three months: November, December, and January 2025. The deep
residual network model achieved a 64.3\% reduction in the mean absolute error
for the 3-19 BRL (Brazilian Real) range when compared to the Black-Scholes
model on the test set. Furthermore, unlike the Black-Scholes solution, which
tends to decrease its accuracy for longer periods of time, the deep learning
model performed accurately for longer expiration periods. These findings
highlight the potential of deep learning in financial modeling, with future
work focusing on specialized models for different price ranges.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [265] [SCOPE-MRI: Bankart Lesion Detection as a Case Study in Data Curation and Deep Learning for Challenging Diagnoses](https://arxiv.org/abs/2504.20405)
*Sahil Sethi,Sai Reddy,Mansi Sakarvadia,Jordan Serotte,Darlington Nwaudo,Nicholas Maassen,Lewis Shi*

Main category: eess.IV

TL;DR: 该研究提出了ScopeMRI数据集和深度学习框架，用于检测Bankart病变，性能达到放射科医生水平，减少了对侵入性检查的需求。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注易于诊断的病理，而Bankart病变等难题未被充分探索，临床诊断依赖侵入性检查。

Method: 使用CNN和Transformer结合的方法，训练了针对标准MRI和MRA的模型，并通过多视图集成优化性能。

Result: 模型在标准MRI和MRA上的AUC分别为0.91和0.93，性能与放射科医生相当甚至更优。

Conclusion: 深度学习模型在标准MRI上可达到放射科医生水平，ScopeMRI的发布将加速肌肉骨骼影像研究。

Abstract: While deep learning has shown strong performance in musculoskeletal imaging,
existing work has largely focused on pathologies where diagnosis is not a
clinical challenge, leaving more difficult problems underexplored, such as
detecting Bankart lesions (anterior-inferior glenoid labral tears) on standard
MRIs. Diagnosing these lesions is challenging due to their subtle imaging
features, often leading to reliance on invasive MRI arthrograms (MRAs). This
study introduces ScopeMRI, the first publicly available, expert-annotated
dataset for shoulder pathologies, and presents a deep learning (DL) framework
for detecting Bankart lesions on both standard MRIs and MRAs. ScopeMRI includes
586 shoulder MRIs (335 standard, 251 MRAs) from 558 patients who underwent
arthroscopy. Ground truth labels were derived from intraoperative findings, the
gold standard for diagnosis. Separate DL models for MRAs and standard MRIs were
trained using a combination of CNNs and transformers. Predictions from
sagittal, axial, and coronal views were ensembled to optimize performance. The
models were evaluated on a 20% hold-out test set (117 MRIs: 46 MRAs, 71
standard MRIs). The models achieved an AUC of 0.91 and 0.93, sensitivity of 83%
and 94%, and specificity of 91% and 86% for standard MRIs and MRAs,
respectively. Notably, model performance on non-invasive standard MRIs matched
or surpassed radiologists interpreting MRAs. External validation demonstrated
initial generalizability across imaging protocols. This study demonstrates that
DL models can achieve radiologist-level diagnostic performance on standard
MRIs, reducing the need for invasive MRAs. By releasing ScopeMRI and a modular
codebase for training and evaluating deep learning models on 3D medical imaging
data, we aim to accelerate research in musculoskeletal imaging and support the
development of new datasets for clinically challenging diagnostic tasks.

</details>


### [266] [LymphAtlas- A Unified Multimodal Lymphoma Imaging Repository Delivering AI-Enhanced Diagnostic Insight](https://arxiv.org/abs/2504.20454)
*Jiajun Ding,Beiyao Zhu,Xiaosheng Liu,Lishen Zhang,Zhao Liu*

Main category: eess.IV

TL;DR: 该研究整合PET代谢信息与CT解剖结构，构建了淋巴瘤3D多模态分割数据集，填补了血液恶性肿瘤领域标准化多模态数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 解决血液恶性肿瘤领域缺乏标准化多模态分割数据集的问题，支持淋巴瘤的精确分割与定量分析。

Method: 回顾性收集483例FDG PET/CT检查数据，基于nnUNet格式构建高质量数据集，并进行技术验证与深度学习模型训练。

Result: 深度学习模型实现了高精度、鲁棒性和可重复性的淋巴瘤病灶分割，验证了数据集的适用性和稳定性。

Conclusion: 该数据集显著提升了肿瘤病灶形态、位置和代谢特征的精确描述，为早期诊断、临床分期和个性化治疗提供了数据支持，推动了深度学习在图像分割和精准医学中的应用。

Abstract: This study integrates PET metabolic information with CT anatomical structures
to establish a 3D multimodal segmentation dataset for lymphoma based on
whole-body FDG PET/CT examinations, which bridges the gap of the lack of
standardised multimodal segmentation datasets in the field of haematological
malignancies. We retrospectively collected 483 examination datasets acquired
between March 2011 and May 2024, involving 220 patients (106 non-Hodgkin
lymphoma, 42 Hodgkin lymphoma); all data underwent ethical review and were
rigorously de-identified. Complete 3D structural information was preserved
during data acquisition, preprocessing and annotation, and a high-quality
dataset was constructed based on the nnUNet format. By systematic technical
validation and evaluation of the preprocessing process, annotation quality and
automatic segmentation algorithm, the deep learning model trained based on this
dataset is verified to achieve accurate segmentation of lymphoma lesions in
PET/CT images with high accuracy, good robustness and reproducibility, which
proves the applicability and stability of this dataset in accurate segmentation
and quantitative analysis. The deep fusion of PET/CT images achieved with this
dataset not only significantly improves the accurate portrayal of the
morphology, location and metabolic features of tumour lesions, but also
provides solid data support for early diagnosis, clinical staging and
personalized treatment, and promotes the development of automated image
segmentation and precision medicine based on deep learning. The dataset and
related resources are available at https://github.com/SuperD0122/LymphAtlas-.

</details>


### [267] [SAM-Guided Robust Representation Learning for One-Shot 3D Medical Image Segmentation](https://arxiv.org/abs/2504.20501)
*Jia Wang,Yunan Mei,Jiarui Liu,Xin Fan*

Main category: eess.IV

TL;DR: 提出了一种名为RRL-MedSAM的新框架，通过知识蒸馏和自提示解码器，将SAM模型适配到一次性3D医学图像分割任务中，显著减少了计算成本和用户交互需求。


<details>
  <summary>Details</summary>
Motivation: 解决SAM模型在一次性医学图像分割中依赖用户交互和高计算成本的问题。

Method: 采用双阶段知识蒸馏策略和互指数移动平均更新权重，结合自提示解码器提升分割性能。

Result: 在OASIS和CT-lung数据集上表现优于现有方法，轻量级编码器参数仅为SAM-Base的3%。

Conclusion: RRL-MedSAM成功将SAM适配到一次性医学图像分割任务，显著提升了效率和性能。

Abstract: One-shot medical image segmentation (MIS) is crucial for medical analysis due
to the burden of medical experts on manual annotation. The recent emergence of
the segment anything model (SAM) has demonstrated remarkable adaptation in MIS
but cannot be directly applied to one-shot medical image segmentation (MIS) due
to its reliance on labor-intensive user interactions and the high computational
cost. To cope with these limitations, we propose a novel SAM-guided robust
representation learning framework, named RRL-MedSAM, to adapt SAM to one-shot
3D MIS, which exploits the strong generalization capabilities of the SAM
encoder to learn better feature representation. We devise a dual-stage
knowledge distillation (DSKD) strategy to distill general knowledge between
natural and medical images from the foundation model to train a lightweight
encoder, and then adopt a mutual exponential moving average (mutual-EMA) to
update the weights of the general lightweight encoder and medical-specific
encoder. Specifically, pseudo labels from the registration network are used to
perform mutual supervision for such two encoders. Moreover, we introduce an
auto-prompting (AP) segmentation decoder which adopts the mask generated from
the general lightweight model as a prompt to assist the medical-specific model
in boosting the final segmentation performance. Extensive experiments conducted
on three public datasets, i.e., OASIS, CT-lung demonstrate that the proposed
RRL-MedSAM outperforms state-of-the-art one-shot MIS methods for both
segmentation and registration tasks. Especially, our lightweight encoder uses
only 3\% of the parameters compared to the encoder of SAM-Base.

</details>


### [268] [Full-field surrogate modeling of cardiac function encoding geometric variability](https://arxiv.org/abs/2504.20479)
*Elena Martinez,Beatrice Moscoloni,Matteo Salvador,Fanwei Kong,Mathias Peirlinck,Alison Lesley Marsden*

Main category: eess.IV

TL;DR: 论文提出了一种结合物理建模与数据驱动方法的新型计算流程，用于构建心脏功能的通用替代模型，解决了现有模型需针对不同患者和病理条件重新训练的问题。


<details>
  <summary>Details</summary>
Motivation: 将计算模型应用于临床心脏病学需要结合物理建模和数据驱动方法，但现有替代模型多为几何特异性，难以通用化。

Method: 采用多尺度数学模型生成电生理模拟数据集，利用分支潜在神经映射（BLNMs）编码激活图，并通过统计形状建模生成合成几何队列作为训练集。

Result: 替代模型在复杂患者队列中表现出鲁棒性和强泛化能力，平均无维度均方误差为0.0034。

Conclusion: 该方法为临床心脏病学提供了一种高效且通用的计算工具，其Python实现已开源。

Abstract: Combining physics-based modeling with data-driven methods is critical to
enabling the translation of computational methods to clinical use in
cardiology. The use of rigorous differential equations combined with machine
learning tools allows for model personalization with uncertainty quantification
in time frames compatible with clinical practice. However, accurate and
efficient surrogate models of cardiac function, built from physics-based
numerical simulation, are still mostly geometry-specific and require retraining
for different patients and pathological conditions. We propose a novel
computational pipeline to embed cardiac anatomies into full-field surrogate
models. We generate a dataset of electrophysiology simulations using a complex
multi-scale mathematical model coupling partial and ordinary differential
equations. We adopt Branched Latent Neural Maps (BLNMs) as an effective
scientific machine learning method to encode activation maps extracted from
physics-based numerical simulations into a neural network. Leveraging large
deformation diffeomorphic metric mappings, we build a biventricular anatomical
atlas and parametrize the anatomical variability of a small and challenging
cohort of 13 pediatric patients affected by Tetralogy of Fallot. We propose a
novel statistical shape modeling based z-score sampling approach to generate a
new synthetic cohort of 52 biventricular geometries that are compatible with
the original geometrical variability. This synthetic cohort acts as the
training set for BLNMs. Our surrogate model demonstrates robustness and great
generalization across the complex original patient cohort, achieving an average
adimensional mean squared error of 0.0034. The Python implementation of our
BLNM model is publicly available under MIT License at
https://github.com/StanfordCBCL/BLNM.

</details>


### [269] [Quality-factor inspired deep neural network solver for solving inverse scattering problems](https://arxiv.org/abs/2504.20504)
*Yutong Du,Zicheng Liu,Miao Cao,Zupeng Liang,Yali Zong,Changyou Li*

Main category: eess.IV

TL;DR: 论文提出了一种基于质量因子的深度神经网络（QuaDNN）求解器，优化了训练数据集、网络结构和损失函数，以提高电磁逆散射问题的成像性能。


<details>
  <summary>Details</summary>
Motivation: 电磁逆散射问题的成像性能受训练数据集、网络结构和损失函数影响，需要优化这些因素以提高准确性。

Method: 通过定义质量因子优化训练数据集，结合残差连接和通道注意力机制改进网络结构，设计包含数据拟合误差、物理信息约束和期望特征的损失函数。

Result: 数值分析和实验成像测试验证了QuaDNN求解器的优越性，能够抑制背景伪影并提高重建精度。

Conclusion: QuaDNN通过优化数据集、网络和损失函数，显著提升了电磁逆散射问题的成像性能。

Abstract: Deep neural networks have been applied to address electromagnetic inverse
scattering problems (ISPs) and shown superior imaging performances, which can
be affected by the training dataset, the network architecture and the applied
loss function. Here, the quality of data samples is cared and valued by the
defined quality factor. Based on the quality factor, the composition of the
training dataset is optimized. The network architecture is integrated with the
residual connections and channel attention mechanism to improve feature
extraction. A loss function that incorporates data-fitting error,
physical-information constraints and the desired feature of the solution is
designed and analyzed to suppress the background artifacts and improve the
reconstruction accuracy. Various numerical analysis are performed to
demonstrate the superiority of the proposed quality-factor inspired deep neural
network (QuaDNN) solver and the imaging performance is finally verified by
experimental imaging test.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [270] [Guessing Efficiently for Constrained Subspace Approximation](https://arxiv.org/abs/2504.20883)
*Aditya Bhaskara,Sepideh Mahabadi,Madhusudhan Reddy Pittu,Ali Vakilian,David P. Woodruff*

Main category: cs.DS

TL;DR: 本文提出了一种解决约束子空间近似问题的通用框架，结合核心集、猜测和求解方法，为多种约束提供近似解。


<details>
  <summary>Details</summary>
Motivation: 研究约束子空间近似问题，旨在在投影矩阵满足特定约束的条件下，找到最佳子空间以近似给定点集。

Method: 提出“核心集-猜测-求解”框架，生成(1+ε)乘法或ε加法近似解。

Result: 改进了多个问题的已知结果，包括公平子空间近似、k均值聚类和非负矩阵分解。

Conclusion: 该框架为约束子空间近似提供了通用且高效的解决方案，适用于多种应用场景。

Abstract: In this paper we study constrained subspace approximation problem. Given a
set of $n$ points $\{a_1,\ldots,a_n\}$ in $\mathbb{R}^d$, the goal of the {\em
subspace approximation} problem is to find a $k$ dimensional subspace that best
approximates the input points. More precisely, for a given $p\geq 1$, we aim to
minimize the $p$th power of the $\ell_p$ norm of the error vector
$(\|a_1-\bm{P}a_1\|,\ldots,\|a_n-\bm{P}a_n\|)$, where $\bm{P}$ denotes the
projection matrix onto the subspace and the norms are Euclidean. In
\emph{constrained} subspace approximation (CSA), we additionally have
constraints on the projection matrix $\bm{P}$. In its most general form, we
require $\bm{P}$ to belong to a given subset $\mathcal{S}$ that is described
explicitly or implicitly.
  We introduce a general framework for constrained subspace approximation. Our
approach, that we term coreset-guess-solve, yields either
$(1+\varepsilon)$-multiplicative or $\varepsilon$-additive approximations for a
variety of constraints. We show that it provides new algorithms for
partition-constrained subspace approximation with applications to {\it fair}
subspace approximation, $k$-means clustering, and projected non-negative matrix
factorization, among others. Specifically, while we reconstruct the best known
bounds for $k$-means clustering in Euclidean spaces, we improve the known
results for the remainder of the problems.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [271] [Partial Answer of How Transformers Learn Automata](https://arxiv.org/abs/2504.20395)
*Tiantian,Zhang*

Main category: cs.FL

TL;DR: 提出了一种基于表示论半直积和傅里叶模块的有限自动机模拟框架，提升了基于Transformer的实现效率。


<details>
  <summary>Details</summary>
Motivation: 探索更高效的有限自动机模拟方法，以优化Transformer的实现。

Method: 利用表示论的半直积和傅里叶模块构建新框架。

Result: 实现了更高效的基于Transformer的有限自动机模拟。

Conclusion: 新框架为有限自动机的模拟提供了更高效的解决方案。

Abstract: We introduce a novel framework for simulating finite automata using
representation-theoretic semidirect products and Fourier modules, achieving
more efficient Transformer-based implementations.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [272] [Coreset selection for the Sinkhorn divergence and generic smooth divergences](https://arxiv.org/abs/2504.20194)
*Alex Kokot,Alex Luedtke*

Main category: stat.ML

TL;DR: CO2算法通过功能泰勒展开生成凸加权核心集，适用于平滑散度，将核心集选择问题转化为最大均值差异最小化。应用于Sinkhorn散度，提供高效采样方法，并验证了新的正则性性质。


<details>
  <summary>Details</summary>
Motivation: 解决核心集选择问题，提高采样效率，并探索其与经典统计方法的联系。

Method: 利用功能泰勒展开和最大均值差异最小化，设计CO2算法，应用于Sinkhorn散度。

Result: 提出高效采样方法，验证新正则性性质，展示图像数据子采样应用。

Conclusion: CO2算法为核心集选择提供新视角，未来可优化算法效率和理论保证。

Abstract: We introduce CO2, an efficient algorithm to produce convexly-weighted
coresets with respect to generic smooth divergences. By employing a functional
Taylor expansion, we show a local equivalence between sufficiently regular
losses and their second order approximations, reducing the coreset selection
problem to maximum mean discrepancy minimization. We apply CO2 to the Sinkhorn
divergence, providing a novel sampling procedure that requires logarithmically
many data points to match the approximation guarantees of random sampling. To
show this, we additionally verify several new regularity properties for
entropically regularized optimal transport of independent interest. Our
approach leads to a new perspective linking coreset selection and kernel
quadrature to classical statistical methods such as moment and score matching.
We showcase this method with a practical application of subsampling image data,
and highlight key directions to explore for improved algorithmic efficiency and
theoretical guarantees.

</details>


### [273] [Sobolev norm inconsistency of kernel interpolation](https://arxiv.org/abs/2504.20617)
*Yunfei Yang*

Main category: stat.ML

TL;DR: 论文研究了在再生核希尔伯特空间中最小范数插值的一致性，发现核插值在特定条件下总是不一致的。


<details>
  <summary>Details</summary>
Motivation: 探讨核插值在不同范数下的泛化误差，以理解其一致性问题。

Method: 通过分析再生核希尔伯特空间中最小范数插值的泛化误差下界。

Result: 结果表明，当范数的平滑指数超过特定常数时，核插值总是不一致的。

Conclusion: 核插值在特定条件下无法保证一致性，需进一步研究改进方法。

Abstract: We study the consistency of minimum-norm interpolation in reproducing kernel
Hilbert spaces corresponding to bounded kernels. Our main result give lower
bounds for the generalization error of the kernel interpolation measured in a
continuous scale of norms that interpolate between $L^2$ and the hypothesis
space. These lower bounds imply that kernel interpolation is always
inconsistent, when the smoothness index of the norm is larger than a constant
that depends only on the embedding index of the hypothesis space and the decay
rate of the eigenvalues.

</details>


### [274] [Learning and Generalization with Mixture Data](https://arxiv.org/abs/2504.20651)
*Harsh Vardhan,Avishek Ghosh,Arya Mazumdar*

Main category: stat.ML

TL;DR: 论文研究了从混合分布中采样的数据的泛化性能和统计速率，通过总变差距离衡量异质性，并探讨了混合数据在何种情况下可视为同质分布。


<details>
  <summary>Details</summary>
Motivation: 现代大规模学习中数据异质性是一个主要挑战，研究混合数据的泛化性能和统计速率有助于理解其学习行为。

Method: 通过Rademacher复杂度和（局部）高斯复杂度分析混合数据的泛化与收敛速率，适用于参数化和非参数化回归问题。

Result: 随着函数类复杂度增加，对总变差距离的要求更严格；混合线性回归的泛化误差与异质性紧密相关。

Conclusion: 混合数据的泛化性能与异质性密切相关，复杂函数类需要更严格的异质性条件。

Abstract: In many, if not most, machine learning applications the training data is
naturally heterogeneous (e.g. federated learning, adversarial attacks and
domain adaptation in neural net training). Data heterogeneity is identified as
one of the major challenges in modern day large-scale learning. A classical way
to represent heterogeneous data is via a mixture model. In this paper, we study
generalization performance and statistical rates when data is sampled from a
mixture distribution. We first characterize the heterogeneity of the mixture in
terms of the pairwise total variation distance of the sub-population
distributions. Thereafter, as a central theme of this paper, we characterize
the range where the mixture may be treated as a single (homogeneous)
distribution for learning. In particular, we study the generalization
performance under the classical PAC framework and the statistical error rates
for parametric (linear regression, mixture of hyperplanes) as well as
non-parametric (Lipschitz, convex and H\"older-smooth) regression problems. In
order to do this, we obtain Rademacher complexity and (local) Gaussian
complexity bounds with mixture data, and apply them to get the generalization
and convergence rates respectively. We observe that as the (regression)
function classes get more complex, the requirement on the pairwise total
variation distance gets stringent, which matches our intuition. We also do a
finer analysis for the case of mixed linear regression and provide a tight
bound on the generalization error in terms of heterogeneity.

</details>


### [275] [Preference-centric Bandits: Optimality of Mixtures and Regret-efficient Algorithms](https://arxiv.org/abs/2504.20877)
*Meltem Tatlı,Arpan Mukherjee,Prashanth L. A.,Karthikeyan Shanmugam,Ali Tajer*

Main category: stat.ML

TL;DR: 本文提出了一种基于偏好度量（PM）的多臂老虎机框架，替代传统的期望值评估，强调风险厌恶和不确定性态度，并设计了两种算法以高效学习最优混合策略。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机仅关注期望奖励，忽略了分布的尾部行为和风险。本文旨在通过偏好度量（PM）引入更丰富的偏好建模，如风险厌恶和鲁棒性。

Method: 提出PM-centric框架，设计了两类算法（horizon-dependent和anytime），通过估计最优混合策略和跟踪机制实现高效学习。

Result: 算法在多种PM代数形式下具有高效的遗憾保证，能够收敛到基于特定混合权重的策略。

Conclusion: PM-centric框架为多臂老虎机提供了更灵活的偏好建模，算法设计原则与传统方法显著不同，适用于风险敏感场景。

Abstract: The objective of canonical multi-armed bandits is to identify and repeatedly
select an arm with the largest reward, often in the form of the expected value
of the arm's probability distribution. Such a utilitarian perspective and focus
on the probability models' first moments, however, is agnostic to the
distributions' tail behavior and their implications for variability and risks
in decision-making. This paper introduces a principled framework for shifting
from expectation-based evaluation to an alternative reward formulation, termed
a preference metric (PM). The PMs can place the desired emphasis on different
reward realization and can encode a richer modeling of preferences that
incorporate risk aversion, robustness, or other desired attitudes toward
uncertainty. A fundamentally distinct observation in such a PM-centric
perspective is that designing bandit algorithms will have a significantly
different principle: as opposed to the reward-based models in which the optimal
sampling policy converges to repeatedly sampling from the single best arm, in
the PM-centric framework the optimal policy converges to selecting a mix of
arms based on specific mixing weights. Designing such mixture policies departs
from the principles for designing bandit algorithms in significant ways,
primarily because of uncountable mixture possibilities. The paper formalizes
the PM-centric framework and presents two algorithm classes (horizon-dependent
and anytime) that learn and track mixtures in a regret-efficient fashion. These
algorithms have two distinctions from their canonical counterparts: (i) they
involve an estimation routine to form reliable estimates of optimal mixtures,
and (ii) they are equipped with tracking mechanisms to navigate arm selection
fractions to track the optimal mixtures. These algorithms' regret guarantees
are investigated under various algebraic forms of the PMs.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [276] [Provably faster randomized and quantum algorithms for k-means clustering via uniform sampling](https://arxiv.org/abs/2504.20982)
*Tyler Chen,Archan Ray,Akshay Seshadri,Dylan Herman,Bao Bach,Pranav Deshpande,Abhishek Som,Niraj Kumar,Marco Pistoia*

Main category: quant-ph

TL;DR: 论文提出了一种随机小批量k-means算法及其量子版本，通过均匀采样改进计算效率，显著优于之前的方法。


<details>
  <summary>Details</summary>
Motivation: 传统k-means算法在大数据应用中计算成本高，量子或量子启发算法虽有所改进，但仍存在局限性。

Method: 提出随机小批量k-means算法和量子算法，利用均匀采样保持问题对称性。

Result: 证明了最坏情况下的性能保证，显著优于之前算法的边界。

Conclusion: 均匀采样方法有效提升了算法的效率和性能。

Abstract: The $k$-means algorithm (Lloyd's algorithm) is a widely used method for
clustering unlabeled data. A key bottleneck of the $k$-means algorithm is that
each iteration requires time linear in the number of data points, which can be
expensive in big data applications. This was improved in recent works proposing
quantum and quantum-inspired classical algorithms to approximate the $k$-means
algorithm locally, in time depending only logarithmically on the number of data
points (along with data dependent parameters) [$q$-means: A quantum algorithm
for unsupervised machine learning; Kerenidis, Landman, Luongo, and Prakash,
NeurIPS 2019; Do you know what $q$-means?, Doriguello, Luongo, Tang]. In this
work, we describe a simple randomized mini-batch $k$-means algorithm and a
quantum algorithm inspired by the classical algorithm. We prove worse-case
guarantees that significantly improve upon the bounds for previous algorithms.
Our improvements are due to a careful use of uniform sampling, which preserves
certain symmetries of the $k$-means problem that are not preserved in previous
algorithms that use data norm-based sampling.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [277] [Fostering Self-Directed Growth with Generative AI: Toward a New Learning Analytics Framework](https://arxiv.org/abs/2504.20851)
*Qianrun Mao*

Main category: cs.CY

TL;DR: 论文提出了一种结合生成式人工智能和学习分析的新框架A2PL，旨在培养学习者的自我导向成长能力。


<details>
  <summary>Details</summary>
Motivation: 在分散化知识生态和AI普及的背景下，培养可持续的学习者能动性成为教育的关键需求。

Method: 研究提出了A2PL模型，重新定义了学习者愿望、复杂思维和总结性自我评估在GAI支持环境中的相互作用。

Result: 讨论了未来干预设计和学习分析应用的方法论意义。

Conclusion: 自我导向成长是数字时代发展公平、适应性强且可持续学习系统的关键。

Abstract: In an era increasingly shaped by decentralized knowledge ecosystems and
pervasive AI technologies, fostering sustainable learner agency has become a
critical educational imperative. This study introduces a novel conceptual
framework integrating Generative Artificial Intelligence and Learning Analytics
to cultivate Self-Directed Growth, a dynamic competency that enables learners
to iteratively drive their own developmental pathways across diverse
contexts.Building upon critical gaps in current research on Self Directed
Learning and AI-mediated education, the proposed Aspire to Potentials for
Learners (A2PL) model reconceptualizes the interplay of learner aspirations,
complex thinking, and summative self-assessment within GAI supported
environments.Methodological implications for future intervention design and
learning analytics applications are discussed, positioning Self-Directed Growth
as a pivotal axis for developing equitable, adaptive, and sustainable learning
systems in the digital era.

</details>


### [278] [When Testing AI Tests Us: Safeguarding Mental Health on the Digital Frontlines](https://arxiv.org/abs/2504.20910)
*Sachin R. Pendse,Darren Gergle,Rachel Kornfield,Jonah Meyerhoff,David Mohr,Jina Suh,Annie Wescott,Casey Williams,Jessica Schleider*

Main category: cs.CY

TL;DR: 本文探讨了AI红队测试人员的心理健康问题，提出了保护措施。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的黑箱特性要求红队测试人员通过互动模拟恶意行为，这种对抗性工作可能导致心理健康问题。

Method: 通过分析红队工作的心理健康影响，并借鉴其他职业的保护措施，提出应对策略。

Result: 提出了针对红队测试人员的个体和组织层面的心理健康保护策略。

Conclusion: 保护红队测试人员的心理健康是AI安全的重要组成部分，需借鉴其他职业的经验。

Abstract: Red-teaming is a core part of the infrastructure that ensures that AI models
do not produce harmful content. Unlike past technologies, the black box nature
of generative AI systems necessitates a uniquely interactional mode of testing,
one in which individuals on red teams actively interact with the system,
leveraging natural language to simulate malicious actors and solicit harmful
outputs. This interactional labor done by red teams can result in mental health
harms that are uniquely tied to the adversarial engagement strategies necessary
to effectively red team. The importance of ensuring that generative AI models
do not propagate societal or individual harm is widely recognized -- one less
visible foundation of end-to-end AI safety is also the protection of the mental
health and wellbeing of those who work to keep model outputs safe. In this
paper, we argue that the unmet mental health needs of AI red-teamers is a
critical workplace safety concern. Through analyzing the unique mental health
impacts associated with the labor done by red teams, we propose potential
individual and organizational strategies that could be used to meet these
needs, and safeguard the mental health of red-teamers. We develop our proposed
strategies through drawing parallels between common red-teaming practices and
interactional labor common to other professions (including actors, mental
health professionals, conflict photographers, and content moderators),
describing how individuals and organizations within these professional spaces
safeguard their mental health given similar psychological demands. Drawing on
these protective practices, we describe how safeguards could be adapted for the
distinct mental health challenges experienced by red teaming organizations as
they mitigate emerging technological risks on the new digital frontlines.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [279] [Heterogeneous network drug-target interaction prediction model based on graph wavelet transform and multi-level contrastive learning](https://arxiv.org/abs/2504.20103)
*Wenfeng Dai,Yanhong Wang,Shuai Yan,Qingzhi Yu,Xiang Cheng*

Main category: q-bio.QM

TL;DR: 本文提出了一种结合图神经网络和多尺度信号处理的异质网络药物-靶点相互作用预测框架，解决了传统方法的黑箱问题，并提供了多层次可解释性。


<details>
  <summary>Details</summary>
Motivation: 药物-靶点相互作用预测在药物开发和精准医疗中至关重要，但传统机器学习方法存在黑箱问题，难以揭示模型决策机制与生物分子相互作用模式的深层关联。

Method: 提出了一种异质图卷积神经网络（HGCN）结合多尺度信号处理的技术框架，包括局部-全局特征协同感知模块、多尺度图信号分解与生物解释模块，以及对比学习模块。

Result: 实验结果表明，该框架在所有数据集上均表现出优异的预测性能。

Conclusion: 该研究为从黑箱预测到机制解码的药物-靶点发现提供了完整解决方案，其方法学对复杂生物分子相互作用系统建模具有重要参考价值。

Abstract: Drug-target interaction (DTI) prediction is a core task in drug development
and precision medicine in the biomedical field. However, traditional machine
learning methods generally have the black box problem, which makes it difficult
to reveal the deep correlation between the model decision mechanism and the
interaction pattern between biological molecules. This study proposes a
heterogeneous network drug target interaction prediction framework, integrating
graph neural network and multi scale signal processing technology to construct
a model with both efficient prediction and multi level interpretability. Its
technical breakthroughs are mainly reflected in the following three
dimensions:Local global feature collaborative perception module. Based on
heterogeneous graph convolutional neural network (HGCN), a multi order neighbor
aggregation strategy is designed.Multi scale graph signal decomposition and
biological interpretation module. A deep hierarchical node feature transform
(GWT) architecture is proposed.Contrastive learning combining multi dimensional
perspectives and hierarchical representations. By comparing the learning
models, the node representations from the two perspectives of HGCN and GWT are
aligned and fused, so that the model can integrate multi dimensional
information and improve the prediction robustness. Experimental results show
that our framework shows excellent prediction performance on all datasets. This
study provides a complete solution for drug target discovery from black box
prediction to mechanism decoding, and its methodology has important reference
value for modeling complex biomolecular interaction systems.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [280] [Exploiting inter-agent coupling information for efficient reinforcement learning of cooperative LQR](https://arxiv.org/abs/2504.20927)
*Shahbaz P Qadri Syed,He Bai*

Main category: eess.SY

TL;DR: 提出了一种基于代理间耦合信息的精确分解方法，改进了多代理强化学习的样本和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在分解局部Q函数时的不精确问题，利用代理间耦合信息提升效率。

Method: 提出系统化的精确分解方法，开发近似最小二乘策略迭代算法，并设计两种架构学习局部Q函数。

Result: 证明了分解的最坏样本复杂度与集中式方法相同，并推导了提升样本效率的图形条件。数值实验验证了改进效果。

Conclusion: 该方法在多代理控制中显著提升了样本和计算效率，为实际应用提供了更高效的解决方案。

Abstract: Developing scalable and efficient reinforcement learning algorithms for
cooperative multi-agent control has received significant attention over the
past years. Existing literature has proposed inexact decompositions of local
Q-functions based on empirical information structures between the agents. In
this paper, we exploit inter-agent coupling information and propose a
systematic approach to exactly decompose the local Q-function of each agent. We
develop an approximate least square policy iteration algorithm based on the
proposed decomposition and identify two architectures to learn the local
Q-function for each agent. We establish that the worst-case sample complexity
of the decomposition is equal to the centralized case and derive necessary and
sufficient graphical conditions on the inter-agent couplings to achieve better
sample efficiency. We demonstrate the improved sample efficiency and
computational efficiency on numerical examples.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [281] [Narrative-Centered Emotional Reflection: Scaffolding Autonomous Emotional Literacy with AI](https://arxiv.org/abs/2504.20342)
*Shou-Tzu Han*

Main category: cs.HC

TL;DR: Reflexion是一个AI驱动的平台，通过情感检测、反思提示和隐喻故事生成，帮助用户进行深度情感探索。


<details>
  <summary>Details</summary>
Motivation: 基于表达性写作、认知重构、自我决定和批判意识发展理论，旨在提升情感表达和心理韧性。

Method: 整合实时情感检测、分层反思提示和隐喻故事生成技术。

Result: 初步研究表明，用户在情感表达、认知重构和心理韧性方面有积极改善。

Conclusion: Reflexion为可扩展的情感计算干预提供了有前景的方向，适用于教育、治疗和公共卫生领域。

Abstract: Reflexion is an AI-powered platform designed to enable structured emotional
self-reflection at scale. By integrating real-time emotion detection, layered
reflective prompting, and metaphorical storytelling generation, Reflexion
empowers users to engage in autonomous emotional exploration beyond basic
sentiment categorization. Grounded in theories of expressive writing, cognitive
restructuring, self-determination, and critical consciousness development, the
system scaffolds a progressive journey from surface-level emotional recognition
toward value-aligned action planning. Initial pilot studies with diverse
participants demonstrate positive outcomes in emotional articulation, cognitive
reframing, and perceived psychological resilience. Reflexion represents a
promising direction for scalable, theory-informed affective computing
interventions aimed at fostering emotional literacy and psychological growth
across educational, therapeutic, and public health contexts.

</details>


### [282] [In defence of post-hoc explanations in medical AI](https://arxiv.org/abs/2504.20741)
*Joshua Hatherley,Lauritz Munch,Jens Christian Bjerring*

Main category: cs.HC

TL;DR: 本文为后验解释在医疗AI中的价值辩护，认为尽管它们不复制黑盒系统的实际推理过程，但仍能提升用户功能理解、提高临床-AI团队准确性，并帮助临床医生证明AI决策。


<details>
  <summary>Details</summary>
Motivation: 回应后验解释被批评为夸大其词的观点，强调其在医疗AI中的实际价值。

Method: 通过论证后验解释的功能性作用，分析其对用户理解和临床决策的影响。

Result: 后验解释虽非完美解决方案，但在提升理解和决策支持方面仍具价值。

Conclusion: 后验解释是解决医疗AI黑盒问题的有效策略之一。

Abstract: Since the early days of the Explainable AI movement, post-hoc explanations
have been praised for their potential to improve user understanding, promote
trust, and reduce patient safety risks in black box medical AI systems.
Recently, however, critics have argued that the benefits of post-hoc
explanations are greatly exaggerated since they merely approximate, rather than
replicate, the actual reasoning processes that black box systems take to arrive
at their outputs. In this article, we aim to defend the value of post-hoc
explanations against this recent critique. We argue that even if post-hoc
explanations do not replicate the exact reasoning processes of black box
systems, they can still improve users' functional understanding of black box
systems, increase the accuracy of clinician-AI teams, and assist clinicians in
justifying their AI-informed decisions. While post-hoc explanations are not a
"silver bullet" solution to the black box problem in medical AI, we conclude
that they remain a useful strategy for addressing the black box problem in
medical AI.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [283] [EPSILON: Adaptive Fault Mitigation in Approximate Deep Neural Network using Statistical Signatures](https://arxiv.org/abs/2504.20074)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.DC

TL;DR: EPSILON是一种轻量级框架，用于高效检测和缓解近似计算神经网络（AxDNNs）中的故障，保持模型准确性并提升能效。


<details>
  <summary>Details</summary>
Motivation: 传统故障检测方法在AxDNNs中引入高开销和延迟，不适用于实时部署。EPSILON旨在解决这一问题。

Method: EPSILON利用预计算统计特征和层重要性指标，采用非参数模式匹配算法实现恒定时间故障检测，并根据权重分布和层关键性动态调整缓解策略。

Result: EPSILON在多种AxDNN架构和故障场景下保持80.05%的准确率，推理时间提升22%，能效提升28%。

Conclusion: EPSILON是安全关键边缘应用中部署可靠AxDNNs的实用解决方案。

Abstract: The increasing adoption of approximate computing in deep neural network
accelerators (AxDNNs) promises significant energy efficiency gains. However,
permanent faults in AxDNNs can severely degrade their performance compared to
their accurate counterparts (AccDNNs). Traditional fault detection and
mitigation approaches, while effective for AccDNNs, introduce substantial
overhead and latency, making them impractical for energy-constrained real-time
deployment. To address this, we introduce EPSILON, a lightweight framework that
leverages pre-computed statistical signatures and layer-wise importance metrics
for efficient fault detection and mitigation in AxDNNs. Our framework
introduces a novel non-parametric pattern-matching algorithm that enables
constant-time fault detection without interrupting normal execution while
dynamically adapting to different network architectures and fault patterns.
EPSILON maintains model accuracy by intelligently adjusting mitigation
strategies based on a statistical analysis of weight distribution and layer
criticality while preserving the energy benefits of approximate computing.
Extensive evaluations across various approximate multipliers, AxDNN
architectures, popular datasets (MNIST, CIFAR-10, CIFAR-100, ImageNet-1k), and
fault scenarios demonstrate that EPSILON maintains 80.05\% accuracy while
offering 22\% improvement in inference time and 28\% improvement in energy
efficiency, establishing EPSILON as a practical solution for deploying reliable
AxDNNs in safety-critical edge applications.

</details>


### [284] [GenTorrent: Scaling Large Language Model Serving with An Overley Network](https://arxiv.org/abs/2504.20101)
*Fei Fang,Yifan Hua,Shengze Wang,Ruilin Zhou,Yi Liu,Chen Qian,Xiaoxue Zhang*

Main category: cs.DC

TL;DR: GenTorrent提出了一种去中心化的LLM服务覆盖网络，通过利用分散的计算资源解决服务扩展性问题，显著降低了延迟并保持了安全性。


<details>
  <summary>Details</summary>
Motivation: 开源和低成本LLM的研究进展显著，但服务扩展性仍是小组织和个人的主要挑战，需要去中心化解决方案。

Method: 提出GenTorrent，一种LLM服务覆盖网络，解决四个关键问题：网络组织、隐私保护、资源效率转发和服务质量验证。

Result: 原型测试显示，相比基线设计，GenTorrent延迟降低50%以上，安全性功能对性能影响极小。

Conclusion: GenTorrent为未来AI服务的民主化和扩展提供了新方向。

Abstract: While significant progress has been made in research and development on
open-source and cost-efficient large-language models (LLMs), serving
scalability remains a critical challenge, particularly for small organizations
and individuals seeking to deploy and test their LLM innovations. Inspired by
peer-to-peer networks that leverage decentralized overlay nodes to increase
throughput and availability, we propose GenTorrent, an LLM serving overlay that
harnesses computing resources from decentralized contributors. We identify four
key research problems inherent to enabling such a decentralized infrastructure:
1) overlay network organization; 2) LLM communication privacy; 3) overlay
forwarding for resource efficiency; and 4) verification of serving quality.
This work presents the first systematic study of these fundamental problems in
the context of decentralized LLM serving. Evaluation results from a prototype
implemented on a set of decentralized nodes demonstrate that GenTorrent
achieves a latency reduction of over 50% compared to the baseline design
without overlay forwarding. Furthermore, the security features introduce
minimal overhead to serving latency and throughput. We believe this work
pioneers a new direction for democratizing and scaling future AI serving
capabilities.

</details>


### [285] [Electricity Cost Minimization for Multi-Workflow Allocation in Geo-Distributed Data Centers](https://arxiv.org/abs/2504.20105)
*Shuang Wang,He Zhang,Tianxing Wu,Yueyou Zhang,Wei Emma Zhang,Quan Z. Sheng*

Main category: cs.DC

TL;DR: 论文提出了一种地理分布式数据中心（GDCs）中电力成本感知的多工作流调度算法（ECMWS），通过四阶段调度策略和模型优化，显著降低了电力成本并满足工作流截止时间约束。


<details>
  <summary>Details</summary>
Motivation: 地理分布式数据中心（GDCs）的电力成本因地理位置和时间而异，如何在满足工作流截止时间的同时降低电力成本是一个重要问题。

Method: 提出ECMWS算法，包括工作流排序、截止时间划分、任务排序和资源分配四个阶段，结合图嵌入模型和策略网络解决马尔可夫决策过程（MDP）。

Result: 实验结果表明，ECMWS算法在电力成本上优于现有方法，提升超过15%，同时保持可接受的计算时间。

Conclusion: ECMWS算法有效解决了GDCs中电力成本优化问题，为实际应用提供了高效解决方案。

Abstract: Worldwide, Geo-distributed Data Centers (GDCs) provide computing and storage
services for massive workflow applications, resulting in high electricity costs
that vary depending on geographical locations and time. How to reduce
electricity costs while satisfying the deadline constraints of workflow
applications is important in GDCs, which is determined by the execution time of
servers, power, and electricity price. Determining the completion time of
workflows with different server frequencies can be challenging, especially in
scenarios with heterogeneous computing resources in GDCs. Moreover, the
electricity price is also different in geographical locations and may change
dynamically. To address these challenges, we develop a geo-distributed system
architecture and propose an Electricity Cost aware Multiple Workflows
Scheduling algorithm (ECMWS) for servers of GDCs with fixed frequency and
power. ECMWS comprises four stages, namely workflow sequencing, deadline
partitioning, task sequencing, and resource allocation where two graph
embedding models and a policy network are constructed to solve the Markov
Decision Process (MDP). After statistically calibrating parameters and
algorithm components over a comprehensive set of workflow instances, the
proposed algorithms are compared with the state-of-the-art methods over two
types of workflow instances. The experimental results demonstrate that our
proposed algorithm significantly outperforms other algorithms, achieving an
improvement of over 15\% while maintaining an acceptable computational time.
The source codes are available at
https://gitee.com/public-artifacts/ecmws-experiments.

</details>


### [286] [Tempo: Application-aware LLM Serving with Mixed SLO Requirements](https://arxiv.org/abs/2504.20068)
*Wei Zhang,Zhiyu Wu,Yi Mu,Banruo Liu,Myungjin Lee,Fan Lai*

Main category: cs.DC

TL;DR: Tempo是一种针对多样化LLM工作负载的SLO感知调度器，通过优化服务增益和资源分配，显著提升了性能和SLO满足率。


<details>
  <summary>Details</summary>
Motivation: 现有调度器无法满足多样化LLM工作负载的需求，尤其是动态依赖和不可预测的请求信息。

Method: Tempo采用混合调度策略，包括基于分位数的响应上限估计、依赖图匹配、服务增益密度优先级和在线决策优化。

Result: Tempo在多样化工作负载中，服务增益提升8.3倍，SLO吞吐量提升10.3倍。

Conclusion: Tempo通过SLO感知调度，有效优化了LLM工作负载的服务增益和资源利用率。

Abstract: The integration of Large Language Models (LLMs) into diverse applications,
ranging from interactive chatbots and cloud AIOps to intelligent agents, has
introduced a wide spectrum of Service Level Objectives (SLOs) for
responsiveness. These workloads include latency-sensitive requests focused on
per-token latency in streaming chat, throughput-intensive requests that require
rapid full responses to invoke tools, and collective requests with dynamic
dependencies arising from self-reflection or agent-based reasoning. This
workload diversity, amplified by unpredictable request information such as
response lengths and runtime dependencies, makes existing schedulers inadequate
even within their design envelopes.
  In this paper, we define service gain as the useful service delivered by
completing requests. We observe that as SLO directly reflects the actual
performance needs of requests, completing a request much faster than its SLO
(e.g., deadline) yields limited additional service gain. Based on this insight,
we introduce Tempo, the first systematic SLO-aware scheduler designed to
maximize service gain across diverse LLM workloads. Tempo allocates just enough
serving bandwidth to meet each SLO, maximizing residual capacity for others
best-effort workloads. Instead of assuming request information or none at all,
it adopts a hybrid scheduling strategy: using quantile-based response upper
bounds and dependency-graph matching for conservative initial estimates,
prioritizing requests by service gain density, and refining decisions online as
generation progresses. Our evaluation across diverse workloads, including chat,
reasoning, and agentic pipelines, shows that Tempo improves end-to-end service
gain by up to 8.3$\times$ and achieves up to 10.3$\times$ SLO goodput compared
to state-of-the-art designs

</details>


### [287] [Leveraging Neural Graph Compilers in Machine Learning Research for Edge-Cloud Systems](https://arxiv.org/abs/2504.20198)
*Alireza Furutanpey,Carmen Walser,Philipp Raith,Pantelis A. Frangoudis,Schahram Dustdar*

Main category: cs.DC

TL;DR: 本文评估了神经网络图编译器在异构硬件平台上的表现，揭示了编译器优化对性能比较的影响，并提出了新指标量化编译器能力。


<details>
  <summary>Details</summary>
Motivation: 解决理论优化技术与实际部署场景之间的关键差距，揭示编译器优化对性能比较的潜在影响。

Method: 通过系统分析和细粒度块级实验，评估不同架构和批量大小下的编译器性能模式。

Result: 发现编译器性能高度依赖架构和批量大小，特定编译器能利用简单架构的重复模式提升吞吐量。

Conclusion: 方法论为学术研究与实际部署搭建桥梁，为异构硬件环境中的优化提供实用见解。

Abstract: This work presents a comprehensive evaluation of neural network graph
compilers across heterogeneous hardware platforms, addressing the critical gap
between theoretical optimization techniques and practical deployment scenarios.
We demonstrate how vendor-specific optimizations can invalidate relative
performance comparisons between architectural archetypes, with performance
advantages sometimes completely reversing after compilation. Our systematic
analysis reveals that graph compilers exhibit performance patterns highly
dependent on both neural architecture and batch sizes. Through fine-grained
block-level experimentation, we establish that vendor-specific compilers can
leverage repeated patterns in simple architectures, yielding disproportionate
throughput gains as model depth increases. We introduce novel metrics to
quantify a compiler's ability to mitigate performance friction as batch size
increases. Our methodology bridges the gap between academic research and
practical deployment by incorporating compiler effects throughout the research
process, providing actionable insights for practitioners navigating complex
optimization landscapes across heterogeneous hardware environments.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [288] [CarbonCall: Sustainability-Aware Function Calling for Large Language Models on Edge Devices](https://arxiv.org/abs/2504.20348)
*Varatheepan Paramanayakam,Andreas Karatzas,Iraklis Anagnostopoulos,Dimitrios Stamoulis*

Main category: cs.PF

TL;DR: CarbonCall框架通过动态工具选择、碳感知执行和量化LLM适配，显著降低边缘AI系统的碳排放和功耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法在优化性能时忽视了可持续性，导致高能耗和碳排放，不适用于能源受限环境。

Method: CarbonCall结合动态工具选择、碳感知执行和量化LLM适配，根据实时碳强度预测调整功率阈值。

Result: 实验显示，CarbonCall减少碳排放52%、功耗30%和执行时间30%，同时保持高效率。

Conclusion: CarbonCall为边缘AI系统提供了一种可持续且高效的功能调用解决方案。

Abstract: Large Language Models (LLMs) enable real-time function calling in edge AI
systems but introduce significant computational overhead, leading to high power
consumption and carbon emissions. Existing methods optimize for performance
while neglecting sustainability, making them inefficient for energy-constrained
environments. We introduce CarbonCall, a sustainability-aware function-calling
framework that integrates dynamic tool selection, carbon-aware execution, and
quantized LLM adaptation. CarbonCall adjusts power thresholds based on
real-time carbon intensity forecasts and switches between model variants to
sustain high tokens-per-second throughput under power constraints. Experiments
on an NVIDIA Jetson AGX Orin show that CarbonCall reduces carbon emissions by
up to 52%, power consumption by 30%, and execution time by 30%, while
maintaining high efficiency.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [289] [Towards Easy and Realistic Network Infrastructure Testing for Large-scale Machine Learning](https://arxiv.org/abs/2504.20854)
*Jinsun Yoo,ChonLam Lao,Lianjie Cao,Bob Lantz,Minlan Yu,Tushar Krishna,Puneet Sharma*

Main category: cs.NI

TL;DR: Genie是一个测试框架，通过CPU模拟GPU通信，结合ASTRA-sim模拟器评估网络行为对ML性能的影响，无需昂贵GPU。


<details>
  <summary>Details</summary>
Motivation: 研究真实硬件网络行为对ML工作负载性能的影响，避免使用昂贵GPU资源。

Method: 利用CPU生成流量模拟GPU间通信，并改进ASTRA-sim模拟器以建模网络与ML工作负载的交互。

Result: Genie能够有效评估网络行为对ML性能的影响，降低测试成本。

Conclusion: Genie为ML性能测试提供了一种低成本、高效的解决方案。

Abstract: This paper lays the foundation for Genie, a testing framework that captures
the impact of real hardware network behavior on ML workload performance,
without requiring expensive GPUs. Genie uses CPU-initiated traffic over a
hardware testbed to emulate GPU to GPU communication, and adapts the ASTRA-sim
simulator to model interaction between the network and the ML workload.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [290] [Smart Water Security with AI and Blockchain-Enhanced Digital Twins](https://arxiv.org/abs/2504.20275)
*Mohammadhossein Homaei,Victor Gonzalez Morales,Oscar Mogollon Gutierrez,Ruben Molano Gomez,Andres Caro*

Main category: cs.CR

TL;DR: 本文提出了一种结合LoRaWAN、机器学习驱动的入侵检测系统（IDS）和区块链数字孪生（BC-DT）的集成框架，用于农村地区安全透明的供水管理。


<details>
  <summary>Details</summary>
Motivation: 农村供水系统面临实时监测不足、易受网络攻击和数据不可靠等问题，亟需一种安全高效的解决方案。

Method: 采用LSTM Autoencoder和Isolation Forest的IDS过滤异常数据，通过私有以太坊区块链和PoA共识记录验证数据，并构建实时数字孪生模型。

Result: 系统实现每秒80+交易（TPS）和低于2秒的延迟，支持1000个智能水表，成本低且可扩展。

Conclusion: 该框架为农村分散式供水基础设施提供了一种实用且安全的解决方案。

Abstract: Water distribution systems in rural areas face serious challenges such as a
lack of real-time monitoring, vulnerability to cyberattacks, and unreliable
data handling. This paper presents an integrated framework that combines
LoRaWAN-based data acquisition, a machine learning-driven Intrusion Detection
System (IDS), and a blockchain-enabled Digital Twin (BC-DT) platform for secure
and transparent water management. The IDS filters anomalous or spoofed data
using a Long Short-Term Memory (LSTM) Autoencoder and Isolation Forest before
validated data is logged via smart contracts on a private Ethereum blockchain
using Proof of Authority (PoA) consensus. The verified data feeds into a
real-time DT model supporting leak detection, consumption forecasting, and
predictive maintenance. Experimental results demonstrate that the system
achieves over 80 transactions per second (TPS) with under 2 seconds of latency
while remaining cost-effective and scalable for up to 1,000 smart meters. This
work demonstrates a practical and secure architecture for decentralized water
infrastructure in under-connected rural environments.

</details>


### [291] [Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM Reasoning via Adaptive Token Compression](https://arxiv.org/abs/2504.20493)
*Yu Cui,Yujun Cai,Yiwei Wang*

Main category: cs.CR

TL;DR: 论文提出了一种名为“推理中断攻击”的新型提示注入攻击方法，通过自适应令牌压缩显著减少触发漏洞所需的提示长度，同时保持攻击效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在推理任务中表现出色，但也存在安全漏洞，如“思考停止”漏洞。现有触发方法需要复杂且冗长的提示，限制了实际应用。

Method: 提出基于自适应令牌压缩的“推理中断攻击”，利用简单算术任务触发漏洞，并开发系统化方法收集攻击提示和压缩框架。

Result: 实验表明，压缩框架显著减少提示长度，同时保持攻击效果，并分析了漏洞的深层原因。

Conclusion: 研究为提升推理LLMs的安全性提供了重要见解，同时展示了简单任务触发漏洞的可行性。

Abstract: While reasoning large language models (LLMs) demonstrate remarkable
performance across various tasks, they also contain notable security
vulnerabilities. Recent research has uncovered a "thinking-stopped"
vulnerability in DeepSeek-R1, where model-generated reasoning tokens can
forcibly interrupt the inference process, resulting in empty responses that
compromise LLM-integrated applications. However, existing methods triggering
this vulnerability require complex mathematical word problems with long
prompts--even exceeding 5,000 tokens. To reduce the token cost and formally
define this vulnerability, we propose a novel prompt injection attack named
"Reasoning Interruption Attack", based on adaptive token compression. We
demonstrate that simple standalone arithmetic tasks can effectively trigger
this vulnerability, and the prompts based on such tasks exhibit simpler logical
structures than mathematical word problems. We develop a systematic approach to
efficiently collect attack prompts and an adaptive token compression framework
that utilizes LLMs to automatically compress these prompts. Experiments show
our compression framework significantly reduces prompt length while maintaining
effective attack capabilities. We further investigate the attack's performance
via output prefix and analyze the underlying causes of the vulnerability,
providing valuable insights for improving security in reasoning LLMs.

</details>


### [292] [The Hidden Risks of LLM-Generated Web Application Code: A Security-Centric Evaluation of Code Generation Capabilities in Large Language Models](https://arxiv.org/abs/2504.20612)
*Swaroop Dora,Deven Lunkad,Naziya Aslam,S. Venkatesan,Sandeep Kumar Shukla*

Main category: cs.CR

TL;DR: 论文评估了多种大型语言模型（如ChatGPT、DeepSeek等）生成的代码在安全性方面的表现，发现普遍存在漏洞，强调人类专家审查和更严格的安全评估框架的必要性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM提升了开发效率，但其生成的代码在安全性方面存在隐患，需评估其在实际应用中的可靠性。

Method: 使用预定义的安全参数对多种LLM生成的代码进行安全性评估。

Result: 发现认证机制、会话管理等关键漏洞，且无模型完全符合行业最佳实践。

Conclusion: 人类专家审查和更严格的安全评估框架对确保LLM生成代码的安全性至关重要。

Abstract: The rapid advancement of Large Language Models (LLMs) has enhanced software
development processes, minimizing the time and effort required for coding and
enhancing developer productivity. However, despite their potential benefits,
code generated by LLMs has been shown to generate insecure code in controlled
environments, raising critical concerns about their reliability and security in
real-world applications. This paper uses predefined security parameters to
evaluate the security compliance of LLM-generated code across multiple models,
such as ChatGPT, DeepSeek, Claude, Gemini and Grok. The analysis reveals
critical vulnerabilities in authentication mechanisms, session management,
input validation and HTTP security headers. Although some models implement
security measures to a limited extent, none fully align with industry best
practices, highlighting the associated risks in automated software development.
Our findings underscore that human expertise is crucial to ensure secure
software deployment or review of LLM-generated code. Also, there is a need for
robust security assessment frameworks to enhance the reliability of
LLM-generated code in real-world applications.

</details>


### [293] [Enhancing Vulnerability Reports with Automated and Augmented Description Summarization](https://arxiv.org/abs/2504.20726)
*Hattan Althebeiti,Mohammed Alkinoon,Manar Mohaisen,Saeed Salem,DaeHun Nyang,David Mohaisen*

Main category: cs.CR

TL;DR: Zad系统通过外部资源丰富NVD漏洞描述，解决描述简短和信息不足的问题，提升内容质量。


<details>
  <summary>Details</summary>
Motivation: 公共漏洞数据库（如NVD）的描述通常简短且信息不足，需要改进。

Method: Zad包含两个流程：一是收集和过滤外部数据构建详细数据集，二是微调预训练模型生成丰富描述。

Result: 评估显示Zad能有效提升漏洞信息的全面性和一致性。

Conclusion: Zad成功解决了NVD描述不足的问题，提升了漏洞信息的质量。

Abstract: Public vulnerability databases, such as the National Vulnerability Database
(NVD), document vulnerabilities and facilitate threat information sharing.
However, they often suffer from short descriptions and outdated or insufficient
information. In this paper, we introduce Zad, a system designed to enrich NVD
vulnerability descriptions by leveraging external resources. Zad consists of
two pipelines: one collects and filters supplementary data using two encoders
to build a detailed dataset, while the other fine-tunes a pre-trained model on
this dataset to generate enriched descriptions. By addressing brevity and
improving content quality, Zad produces more comprehensive and cohesive
vulnerability descriptions. We evaluate Zad using standard summarization
metrics and human assessments, demonstrating its effectiveness in enhancing
vulnerability information.

</details>


### [294] [A Virtual Cybersecurity Department for Securing Digital Twins in Water Distribution Systems](https://arxiv.org/abs/2504.20266)
*Mohammadhossein Homaei,Agustin Di Bartolo,Oscar Mogollon-Gutierrez,Fernando Broncano Morgado,Pablo Garcia Rodriguez*

Main category: cs.CR

TL;DR: 论文提出了一种面向中小企业的虚拟网络安全部门（VCD），利用开源工具和机器学习模型低成本提升水系统数字孪生的安全性。


<details>
  <summary>Details</summary>
Motivation: 数字孪生在水系统中易受网络攻击，但中小企业缺乏资源和预算建立强大的网络安全团队。

Method: VCD结合开源工具（如Zabbix、Suricata、Fail2Ban）和改进的集成机器学习模型（基于OD-IDS2022数据集）进行实时监控和威胁检测。

Result: 模型对暴力破解、远程代码执行等攻击的检测准确率达92%，误报率低。

Conclusion: VCD为中小企业提供了一种低成本、易管理的水系统网络安全解决方案。

Abstract: Digital twins (DTs) help improve real-time monitoring and decision-making in
water distribution systems. However, their connectivity makes them easy targets
for cyberattacks such as scanning, denial-of-service (DoS), and unauthorized
access. Small and medium-sized enterprises (SMEs) that manage these systems
often do not have enough budget or staff to build strong cybersecurity teams.
To solve this problem, we present a Virtual Cybersecurity Department (VCD), an
affordable and automated framework designed for SMEs. The VCD uses open-source
tools like Zabbix for real-time monitoring, Suricata for network intrusion
detection, Fail2Ban to block repeated login attempts, and simple firewall
settings. To improve threat detection, we also add a machine-learning-based IDS
trained on the OD-IDS2022 dataset using an improved ensemble model. This model
detects cyber threats such as brute-force attacks, remote code execution (RCE),
and network flooding, with 92\% accuracy and fewer false alarms. Our solution
gives SMEs a practical and efficient way to secure water systems using low-cost
and easy-to-manage tools.

</details>


### [295] [Dual Explanations via Subgraph Matching for Malware Detection](https://arxiv.org/abs/2504.20904)
*Hossein Shokouhinejad,Roozbeh Razavi-Far,Griffin Higgins,Ali A. Ghorbani*

Main category: cs.CR

TL;DR: 提出了一种基于双原型驱动的可解释框架，用于GNN恶意软件检测，通过子图匹配技术提高解释性。


<details>
  <summary>Details</summary>
Motivation: 传统GNN解释方法无法将重要区域与已知恶意或良性行为模式关联，限制了其在安全领域的实用性。

Method: 结合基础解释器与新型子图匹配解释器（SubMatch），为节点分配可解释分数，区分恶意与良性区域。

Result: 实验表明，该方法在保持高检测性能的同时显著提升了恶意软件分析的解释性。

Conclusion: 双原型驱动框架为GNN恶意软件检测提供了行为对齐的高解释性解决方案。

Abstract: Interpretable malware detection is crucial for understanding harmful
behaviors and building trust in automated security systems. Traditional
explainable methods for Graph Neural Networks (GNNs) often highlight important
regions within a graph but fail to associate them with known benign or
malicious behavioral patterns. This limitation reduces their utility in
security contexts, where alignment with verified prototypes is essential. In
this work, we introduce a novel dual prototype-driven explainable framework
that interprets GNN-based malware detection decisions. This dual explainable
framework integrates a base explainer (a state-of-the-art explainer) with a
novel second-level explainer which is designed by subgraph matching technique,
called SubMatch explainer. The proposed explainer assigns interpretable scores
to nodes based on their association with matched subgraphs, offering a
fine-grained distinction between benign and malicious regions. This
prototype-guided scoring mechanism enables more interpretable, behavior-aligned
explanations. Experimental results demonstrate that our method preserves high
detection performance while significantly improving interpretability in malware
analysis.

</details>


### [296] [GiBy: A Giant-Step Baby-Step Classifier For Anomaly Detection In Industrial Control Systems](https://arxiv.org/abs/2504.20906)
*Sarad Venugopalan,Sridhar Adepu*

Main category: cs.CR

TL;DR: 提出了一种基于线性化和降维的异常检测方法，用于工业控制系统（ICS），实验表明其响应速度快且可解释性强。


<details>
  <summary>Details</summary>
Motivation: 工业控制系统的安全运行需要实时监测传感器与执行器之间的异常，以确保系统安全和人员安全。

Method: 通过线性化非线性传感器-执行器关系，并利用降维技术降低时间复杂性，实现异常检测。

Result: 实验证明该方法能在毫秒级响应时间内检测异常，并提供其他AI/ML模型无法同时实现的可解释性。

Conclusion: 该方法在异常检测速度和可解释性方面优于现有技术，适用于工业控制系统的安全保障。

Abstract: The continuous monitoring of the interactions between cyber-physical
components of any industrial control system (ICS) is required to secure
automation of the system controls, and to guarantee plant processes are
fail-safe and remain in an acceptably safe state. Safety is achieved by
managing actuation (where electric signals are used to trigger physical
movement), dependent on corresponding sensor readings; used as ground truth in
decision making. Timely detection of anomalies (attacks, faults and
unascertained states) in ICSs is crucial for the safe running of a plant, the
safety of its personnel, and for the safe provision of any services provided.
We propose an anomaly detection method that involves accurate linearization of
the non-linear forms arising from sensor-actuator(s) relationships, primarily
because solving linear models is easier and well understood. Further, the time
complexity of the anomaly detection scenario/problem at hand is lowered using
dimensionality reduction of the actuator(s) in relationship with a sensor. We
accomplish this by using a well-known water treatment testbed as a use case.
Our experiments show millisecond time response to detect anomalies and provide
explainability; that are not simultaneously achieved by other state of the art
AI/ML models with eXplainable AI (XAI) used for the same purpose. Further, we
pin-point the sensor(s) and its actuation state for which anomaly was detected.

</details>


### [297] [ACE: A Security Architecture for LLM-Integrated App Systems](https://arxiv.org/abs/2504.20984)
*Evan Li,Tushin Mallick,Evan Rose,William Robertson,Alina Oprea,Cristina Nita-Rotaru*

Main category: cs.CR

TL;DR: 论文提出了一种新的安全架构ACE，用于保护LLM集成应用系统免受恶意应用攻击，确保规划和执行的完整性。


<details>
  <summary>Details</summary>
Motivation: LLM集成应用系统存在新的攻击向量，恶意应用可能破坏规划或执行的完整性、可用性或隐私。

Method: ACE架构将规划分为抽象计划和具体计划两阶段，并通过静态分析和执行屏障确保安全。

Result: 实验证明ACE能抵御INJECAGENT基准测试和新攻击，确保系统安全。

Conclusion: ACE是提升LLM系统安全性的重要进展，适用于信任度不同的系统设施。

Abstract: LLM-integrated app systems extend the utility of Large Language Models (LLMs)
with third-party apps that are invoked by a system LLM using interleaved
planning and execution phases to answer user queries. These systems introduce
new attack vectors where malicious apps can cause integrity violation of
planning or execution, availability breakdown, or privacy compromise during
execution.
  In this work, we identify new attacks impacting the integrity of planning, as
well as the integrity and availability of execution in LLM-integrated apps, and
demonstrate them against IsolateGPT, a recent solution designed to mitigate
attacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new
secure architecture for LLM-integrated app systems that provides security
guarantees for system planning and execution. Specifically, ACE decouples
planning into two phases by first creating an abstract execution plan using
only trusted information, and then mapping the abstract plan to a concrete plan
using installed system apps. We verify that the plans generated by our system
satisfy user-specified secure information flow constraints via static analysis
on the structured plan output. During execution, ACE enforces data and
capability barriers between apps, and ensures that the execution is conducted
according to the trusted abstract plan. We show experimentally that our system
is secure against attacks from the INJECAGENT benchmark, a standard benchmark
for control flow integrity in the face of indirect prompt injection attacks,
and our newly introduced attacks. Our architecture represents a significant
advancement towards hardening LLM-based systems containing system facilities of
varying levels of trustworthiness.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [298] [Self-Healing Software Systems: Lessons from Nature, Powered by AI](https://arxiv.org/abs/2504.20093)
*Mohammad Baqar,Rajat Khanda,Saba Naqvi*

Main category: cs.SE

TL;DR: 本文提出了一种受生物自愈启发的AI驱动自愈软件框架，通过系统观察工具、AI诊断和修复代理，减少停机时间并增强软件弹性。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统复杂性和规模增加，需要自主检测、诊断和恢复故障的能力。

Method: 结合日志分析、静态代码检查和AI生成的补丁或测试更新，模拟生物自愈模型。

Result: 通过案例研究和模拟验证，该框架比传统手动调试更有效。

Conclusion: 该研究为智能、自适应和自依赖的软件系统奠定了基础。

Abstract: As modern software systems grow in complexity and scale, their ability to
autonomously detect, diagnose, and recover from failures becomes increasingly
vital. Drawing inspiration from biological healing - where the human body
detects damage, signals the brain, and activates targeted recovery - this paper
explores the concept of self-healing software driven by artificial
intelligence. We propose a novel framework that mimics this biological model
system observability tools serve as sensory inputs, AI models function as the
cognitive core for diagnosis and repair, and healing agents apply targeted code
and test modifications. By combining log analysis, static code inspection, and
AI-driven generation of patches or test updates, our approach aims to reduce
downtime, accelerate debugging, and enhance software resilience. We evaluate
the effectiveness of this model through case studies and simulations, comparing
it against traditional manual debugging and recovery workflows. This work paves
the way toward intelligent, adaptive and self-reliant software systems capable
of continuous healing, akin to living organisms.

</details>


### [299] [ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies](https://arxiv.org/abs/2504.20117)
*Shubham Gandhi,Dhruv Shah,Manasi Patwardhan,Lovekesh Vig,Gautam Shroff*

Main category: cs.SE

TL;DR: ResearchCodeAgent是一个基于大型语言模型的多智能体系统，用于自动化机器学习文献中研究方法的代码实现。


<details>
  <summary>Details</summary>
Motivation: 解决研究概念与实际代码实现之间的鸿沟，帮助研究人员快速生成现有论文的代码以进行基准测试或进一步开发。

Method: 采用灵活的智能体架构和动态规划机制，结合短期和长期记忆，适应性地与研究环境交互。

Result: 在三个机器学习任务中，46.9%的生成代码质量高且无错误，25%优于基线实现，编码时间平均减少57.9%。

Conclusion: ResearchCodeAgent显著推动了研究实现过程的自动化，有望加速机器学习研究的进展。

Abstract: In this paper we introduce ResearchCodeAgent, a novel multi-agent system
leveraging large language models (LLMs) agents to automate the codification of
research methodologies described in machine learning literature. The system
bridges the gap between high-level research concepts and their practical
implementation, allowing researchers auto-generating code of existing research
papers for benchmarking or building on top-of existing methods specified in the
literature with availability of partial or complete starter code.
ResearchCodeAgent employs a flexible agent architecture with a comprehensive
action suite, enabling context-aware interactions with the research
environment. The system incorporates a dynamic planning mechanism, utilizing
both short and long-term memory to adapt its approach iteratively. We evaluate
ResearchCodeAgent on three distinct machine learning tasks with distinct task
complexity and representing different parts of the ML pipeline: data
augmentation, optimization, and data batching. Our results demonstrate the
system's effectiveness and generalizability, with 46.9% of generated code being
high-quality and error-free, and 25% showing performance improvements over
baseline implementations. Empirical analysis shows an average reduction of
57.9% in coding time compared to manual implementation. We observe higher gains
for more complex tasks. ResearchCodeAgent represents a significant step towards
automating the research implementation process, potentially accelerating the
pace of machine learning research.

</details>


### [300] [AutoP2C: An LLM-Based Agent Framework for Code Repository Generation from Multimodal Content in Academic Papers](https://arxiv.org/abs/2504.20115)
*Zijie Lin,Yiqing Shen,Qilin Cai,He Sun,Jinrui Zhou,Mingjun Xiao*

Main category: cs.SE

TL;DR: 论文提出了一种名为“Paper-to-Code”（P2C）的任务，旨在将科学论文中的多模态内容转化为可执行的代码仓库，并开发了AutoP2C框架来实现这一目标。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅能将文本描述转化为孤立的代码片段，而无法处理论文中的多模态内容（如文本、图表、表格等），导致代码生成效率低且需要大量专业知识。

Method: AutoP2C是一个基于大语言模型的多智能体框架，包含四个阶段：代码仓库蓝图提取、多模态内容解析、分层任务分解和迭代反馈调试。

Result: 在八篇研究论文的基准测试中，AutoP2C成功为所有论文生成了可执行的代码仓库，而其他方法仅能为一篇论文生成可运行代码。

Conclusion: AutoP2C显著提高了从论文到代码的转化效率，为机器学习研究提供了实用工具。

Abstract: Machine Learning (ML) research is spread through academic papers featuring
rich multimodal content, including text, diagrams, and tabular results.
However, translating these multimodal elements into executable code remains a
challenging and time-consuming process that requires substantial ML expertise.
We introduce ``Paper-to-Code'' (P2C), a novel task that transforms the
multimodal content of scientific publications into fully executable code
repositories, which extends beyond the existing formulation of code generation
that merely converts textual descriptions into isolated code snippets. To
automate the P2C process, we propose AutoP2C, a multi-agent framework based on
large language models that processes both textual and visual content from
research papers to generate complete code repositories. Specifically, AutoP2C
contains four stages: (1) repository blueprint extraction from established
codebases, (2) multimodal content parsing that integrates information from
text, equations, and figures, (3) hierarchical task decomposition for
structured code generation, and (4) iterative feedback-driven debugging to
ensure functionality and performance. Evaluation on a benchmark of eight
research papers demonstrates the effectiveness of AutoP2C, which can
successfully generate executable code repositories for all eight papers, while
OpenAI-o1 or DeepSeek-R1 can only produce runnable code for one paper. The code
is available at https://github.com/shoushouyu/Automated-Paper-to-Code.

</details>


### [301] [BLADE: Benchmark suite for LLM-driven Automated Design and Evolution of iterative optimisation heuristics](https://arxiv.org/abs/2504.20183)
*Niki van Stein,Anna V. Kononova,Haoran Yin,Thomas Bäck*

Main category: cs.SE

TL;DR: BLADE是一个模块化、可扩展的框架，用于评估LLM驱动的自动化算法设计方法，提供标准化基准测试和工具集成。


<details>
  <summary>Details</summary>
Motivation: 由于LLM驱动的自动化算法设计领域缺乏标准化基准测试，BLADE旨在填补这一空白，解决现有基准测试的不足。

Method: BLADE整合了多种基准问题、实例生成器和文本描述，支持能力测试（如泛化、专业化和信息利用），并提供标准化日志和可视化工具。

Result: BLADE通过两个用例展示了其有效性，探索了突变提示策略和函数专业化。

Conclusion: BLADE为系统评估LLM驱动的自动化算法设计方法提供了全面的解决方案。

Abstract: The application of Large Language Models (LLMs) for Automated Algorithm
Discovery (AAD), particularly for optimisation heuristics, is an emerging field
of research. This emergence necessitates robust, standardised benchmarking
practices to rigorously evaluate the capabilities and limitations of LLM-driven
AAD methods and the resulting generated algorithms, especially given the
opacity of their design process and known issues with existing benchmarks. To
address this need, we introduce BLADE (Benchmark suite for LLM-driven Automated
Design and Evolution), a modular and extensible framework specifically designed
for benchmarking LLM-driven AAD methods in a continuous black-box optimisation
context. BLADE integrates collections of benchmark problems (including MA-BBOB
and SBOX-COST among others) with instance generators and textual descriptions
aimed at capability-focused testing, such as generalisation, specialisation and
information exploitation. It offers flexible experimental setup options,
standardised logging for reproducibility and fair comparison, incorporates
methods for analysing the AAD process (e.g., Code Evolution Graphs and various
visualisation approaches) and facilitates comparison against human-designed
baselines through integration with established tools like IOHanalyser and
IOHexplainer. BLADE provides an `out-of-the-box' solution to systematically
evaluate LLM-driven AAD approaches. The framework is demonstrated through two
distinct use cases exploring mutation prompt strategies and function
specialisation.

</details>


### [302] [Prompting LLMs for Code Editing: Struggles and Remedies](https://arxiv.org/abs/2504.20196)
*Daye Nam,Ahmed Omran,Ambar Murillo,Saksham Thakur,Abner Araujo,Marcel Blistein,Alexander Frömmgen,Vincent Hellendoorn,Satish Chandra*

Main category: cs.SE

TL;DR: 论文研究了开发者如何在实际工作流中使用LLM驱动的代码编辑工具，发现频繁重提提示是使用困难的标志，并提出了自动优化提示的工具AutoPrompter。


<details>
  <summary>Details</summary>
Motivation: 理解开发者如何实际使用LLM驱动的代码编辑工具及其使用难点。

Method: 通过分析IDE中的使用日志和定性研究，识别提示不足的类别，并开发AutoPrompter工具优化提示。

Result: AutoPrompter将编辑正确率提高了27%。

Conclusion: 研究填补了开发者实际使用LLM工具的空白，并提出了有效的优化方法。

Abstract: Large Language Models (LLMs) are rapidly transforming software engineering,
with coding assistants embedded in an IDE becoming increasingly prevalent.
While research has focused on improving the tools and understanding developer
perceptions, a critical gap exists in understanding how developers actually use
these tools in their daily workflows, and, crucially, where they struggle. This
paper addresses part of this gap through a multi-phased investigation of
developer interactions with an LLM-powered code editing and transformation
feature, Transform Code, in an IDE widely used at Google. First, we analyze
telemetry logs of the feature usage, revealing that frequent re-prompting can
be an indicator of developer struggles with using Transform Code. Second, we
conduct a qualitative analysis of unsatisfactory requests, identifying five key
categories of information often missing from developer prompts. Finally, based
on these findings, we propose and evaluate a tool, AutoPrompter, for
automatically improving prompts by inferring missing information from the
surrounding code context, leading to a 27% improvement in edit correctness on
our test set.

</details>


### [303] [Automated Unit Test Case Generation: A Systematic Literature Review](https://arxiv.org/abs/2504.20357)
*Jason Wang,Basem Suleiman,Muhammad Johan Alibasa*

Main category: cs.SE

TL;DR: 本文是一篇系统文献综述，探讨了遗传算法和粒子群优化在自动化软件测试中的改进及其局限性，同时分析了当前领域的主要挑战。


<details>
  <summary>Details</summary>
Motivation: 软件测试成本高且耗时，自动化测试成为研究热点，但遗传算法和粒子群优化的改进及当前挑战存在信息缺口。

Method: 通过系统文献综述，整合现有知识，分析遗传算法和粒子群优化的改进（如混合算法、突变测试与神经网络的结合）及其局限性。

Result: 总结了进化算法的改进方法（如混合算法）和当前领域的主要挑战（如可读性、模拟等）。

Conclusion: 本文填补了遗传算法和粒子群优化在自动化测试中的知识缺口，为未来研究提供了方向。

Abstract: Software is omnipresent within all factors of society. It is thus important
to ensure that software are well tested to mitigate bad user experiences as
well as the potential for severe financial and human losses. Software testing
is however expensive and absorbs valuable time and resources. As a result, the
field of automated software testing has grown of interest to researchers in
past decades. In our review of present and past research papers, we have
identified an information gap in the areas of improvement for the Genetic
Algorithm and Particle Swarm Optimisation. A gap in knowledge in the current
challenges that face automated testing has also been identified. We therefore
present this systematic literature review in an effort to consolidate existing
knowledge in regards to the evolutionary approaches as well as their
improvements and resulting limitations. These improvements include hybrid
algorithm combinations as well as interoperability with mutation testing and
neural networks. We will also explore the main test criterion that are used in
these algorithms alongside the challenges currently faced in the field related
to readability, mocking and more.

</details>


### [304] [CrashFixer: A crash resolution agent for the Linux kernel](https://arxiv.org/abs/2504.20412)
*Alex Mathai,Chenxi Huang,Suwei Ma,Jihwan Kim,Hailie Mitchell,Aleksandr Nogikh,Petros Maniatis,Franjo Ivančić,Junfeng Yang,Baishakhi Ray*

Main category: cs.SE

TL;DR: 论文介绍了CrashFixer，首个适用于Linux内核bug的LLM修复工具，通过改进kGym平台为kGymSuite，并验证了修复策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有代码LLM评估基准规模有限，难以应对Linux内核等复杂系统的修复需求。

Method: 基于kGym平台改进为kGymSuite，结合开发者工作流设计修复策略，并验证假设生成的重要性。

Result: CrashFixer在未修复的bug中提出至少两个可行的补丁建议。

Conclusion: 显式生成假设对复杂系统修复至关重要，CrashFixer展示了LLM在内核bug修复中的潜力。

Abstract: Code large language models (LLMs) have shown impressive capabilities on a
multitude of software engineering tasks. In particular, they have demonstrated
remarkable utility in the task of code repair. However, common benchmarks used
to evaluate the performance of code LLMs are often limited to small-scale
settings. In this work, we build upon kGym, which shares a benchmark for
system-level Linux kernel bugs and a platform to run experiments on the Linux
kernel.
  This paper introduces CrashFixer, the first LLM-based software repair agent
that is applicable to Linux kernel bugs. Inspired by the typical workflow of a
kernel developer, we identify the key capabilities an expert developer
leverages to resolve a kernel crash. Using this as our guide, we revisit the
kGym platform and identify key system improvements needed to practically run
LLM-based agents at the scale of the Linux kernel (50K files and 20M lines of
code). We implement these changes by extending kGym to create an improved
platform - called kGymSuite, which will be open-sourced. Finally, the paper
presents an evaluation of various repair strategies for such complex kernel
bugs and showcases the value of explicitly generating a hypothesis before
attempting to fix bugs in complex systems such as the Linux kernel. We also
evaluated CrashFixer's capabilities on still open bugs, and found at least two
patch suggestions considered plausible to resolve the reported bug.

</details>


### [305] [ARCS: Agentic Retrieval-Augmented Code Synthesis with Iterative Refinement](https://arxiv.org/abs/2504.20434)
*Manish Bhattarai,Miguel Cordova,Javier Santos,Dan O'Malley*

Main category: cs.SE

TL;DR: ARCS框架结合检索增强生成与链式思维推理，提升代码生成、完成和翻译的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 在超级计算中，高效且优化的代码生成对充分利用高性能系统至关重要。

Method: ARCS整合检索增强生成与链式思维推理，通过基于代理的检索机制获取代码片段，并利用实时执行反馈优化候选方案。

Result: 在Geeks4Geeks和HumanEval基准测试中，ARCS显著优于传统提示方法。

Conclusion: ARCS为超级计算应用中的代码开发自动化和优化提供了变革性潜力。

Abstract: In supercomputing, efficient and optimized code generation is essential to
leverage high-performance systems effectively. We propose Agentic
Retrieval-Augmented Code Synthesis (ARCS), an advanced framework for accurate,
robust, and efficient code generation, completion, and translation. ARCS
integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (CoT)
reasoning to systematically break down and iteratively refine complex
programming tasks. An agent-based RAG mechanism retrieves relevant code
snippets, while real-time execution feedback drives the synthesis of candidate
solutions. This process is formalized as a state-action search tree
optimization, balancing code correctness with editing efficiency. Evaluations
on the Geeks4Geeks and HumanEval benchmarks demonstrate that ARCS significantly
outperforms traditional prompting methods in translation and generation
quality. By enabling scalable and precise code synthesis, ARCS offers
transformative potential for automating and optimizing code development in
supercomputing applications, enhancing computational resource utilization.

</details>


### [306] [Enhancing Cell Counting through MLOps: A Structured Approach for Automated Cell Analysis](https://arxiv.org/abs/2504.20126)
*Matteo Testi,Luca Clissa,Matteo Ballabio,Salvatore Ricciardi,Federico Baldo,Emanuele Frontoni,Sara Moccia,Gennario Vessio*

Main category: cs.SE

TL;DR: 本文提出了一种名为CC-MLOps的框架，旨在优化机器学习在细胞计数工作流程中的应用，涵盖数据处理、模型训练与监控等环节。


<details>
  <summary>Details</summary>
Motivation: 机器学习在细胞计数领域潜力巨大，但需要更高效的操作框架以实现其实际应用。

Method: 提出CC-MLOps框架，整合数据预处理、模型训练、监控和可解释性等功能。

Result: 通过实际案例展示，该框架提高了模型可靠性，减少了人为错误，并支持可扩展的解决方案。

Conclusion: CC-MLOps为研究人员提供了实用的指导，助力机器学习在细胞计数中的高效实施。

Abstract: Machine Learning (ML) models offer significant potential for advancing cell
counting applications in neuroscience, medical research, pharmaceutical
development, and environmental monitoring. However, implementing these models
effectively requires robust operational frameworks. This paper introduces Cell
Counting Machine Learning Operations (CC-MLOps), a comprehensive framework that
streamlines the integration of ML in cell counting workflows. CC-MLOps
encompasses data access and preprocessing, model training, monitoring,
explainability features, and sustainability considerations. Through a practical
use case, we demonstrate how MLOps principles can enhance model reliability,
reduce human error, and enable scalable Cell Counting solutions. This work
provides actionable guidance for researchers and laboratory professionals
seeking to implement machine learning (ML)- powered cell counting systems.

</details>


### [307] [CoCo-Bench: A Comprehensive Code Benchmark For Multi-task Large Language Model Evaluation](https://arxiv.org/abs/2504.20673)
*Wenjing Yin,Tianze Sun,Yijiong Yu,Jiawei Fang,Guangyao Su,Jiancheng Wang,Zekun Wang,Wei Wang,Ran Chen,Ziyun Dai,Shuai Yuan,Menghang Dong,Peng Luo,Dong Cao,Da Lei,Yajun Zhang,Hao Chen,Xiang Ma,Yong Liu,Weifeng Liu,Yuanjian Xu,Ji Pei*

Main category: cs.SE

TL;DR: CoCo-Bench是一个全面的代码基准测试工具，用于评估大型语言模型在代码理解、生成、修改和审查四个关键维度的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试范围狭窄，缺乏反映实际应用的综合评估框架。

Method: 设计CoCo-Bench，涵盖多种编程语言和任务难度，并进行严格的人工审核。

Result: CoCo-Bench与现有基准测试一致，同时揭示了模型性能的显著差异。

Conclusion: CoCo-Bench为代码导向的大型语言模型提供了可靠的综合评估基准。

Abstract: Large language models (LLMs) play a crucial role in software engineering,
excelling in tasks like code generation and maintenance. However, existing
benchmarks are often narrow in scope, focusing on a specific task and lack a
comprehensive evaluation framework that reflects real-world applications. To
address these gaps, we introduce CoCo-Bench (Comprehensive Code Benchmark),
designed to evaluate LLMs across four critical dimensions: code understanding,
code generation, code modification, and code review. These dimensions capture
essential developer needs, ensuring a more systematic and representative
evaluation. CoCo-Bench includes multiple programming languages and varying task
difficulties, with rigorous manual review to ensure data quality and accuracy.
Empirical results show that CoCo-Bench aligns with existing benchmarks while
uncovering significant variations in model performance, effectively
highlighting strengths and weaknesses. By offering a holistic and objective
evaluation, CoCo-Bench provides valuable insights to guide future research and
technological advancements in code-oriented LLMs, establishing a reliable
benchmark for the field.

</details>


### [308] [Using LLMs in Generating Design Rationale for Software Architecture Decisions](https://arxiv.org/abs/2504.20781)
*Xiyu Zhou,Ruiyin Li,Peng Liang,Beiqi Zhang,Mojtaba Shahin,Zengyang Li,Chen Yang*

Main category: cs.SE

TL;DR: 研究评估了大型语言模型（LLM）在生成软件架构设计理由（DR）中的表现，发现其生成的DR在精确度、召回率和F1分数上表现一般，但部分未提及的论点仍具参考价值。


<details>
  <summary>Details</summary>
Motivation: 软件架构设计理由（DR）在实践中常因开发者缺乏动力和努力而未能充分记录，LLM的文本理解和生成能力可能帮助解决这一问题。

Method: 收集100个架构相关问题，使用五种LLM和三种提示策略（零样本、思维链、LLM代理）生成DR，并与专家提供的DR进行对比评估。

Result: LLM生成的DR在精确度（0.267-0.278）、召回率（0.627-0.715）和F1分数（0.351-0.389）上表现一般，但64.45%-69.42%的未提及论点仍具参考价值。

Conclusion: LLM在生成DR方面有一定潜力，但需权衡其优缺点，并进一步优化提示策略。

Abstract: Design Rationale (DR) for software architecture decisions refers to the
reasoning underlying architectural choices, which provides valuable insights
into the different phases of the architecting process throughout software
development. However, in practice, DR is often inadequately documented due to a
lack of motivation and effort from developers. With the recent advancements in
Large Language Models (LLMs), their capabilities in text comprehension,
reasoning, and generation may enable the generation and recovery of DR for
architecture decisions. In this study, we evaluated the performance of LLMs in
generating DR for architecture decisions. First, we collected 50 Stack Overflow
(SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture
decisions to construct a dataset of 100 architecture-related problems. Then, we
selected five LLMs to generate DR for the architecture decisions with three
prompting strategies, including zero-shot, chain of thought (CoT), and
LLM-based agents. With the DR provided by human experts as ground truth, the
Precision of LLM-generated DR with the three prompting strategies ranges from
0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389.
Additionally, 64.45% to 69.42% of the arguments of DR not mentioned by human
experts are also helpful, 4.12% to 4.87% of the arguments have uncertain
correctness, and 1.59% to 3.24% of the arguments are potentially misleading.
Based on the results, we further discussed the pros and cons of the three
prompting strategies and the strengths and limitations of the DR generated by
LLMs.

</details>


### [309] [Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation, and Challenges](https://arxiv.org/abs/2504.20799)
*Yunseo Lee,John Youngeun Song,Dongsun Kim,Jindae Kim,Mijung Kim,Jaechang Nam*

Main category: cs.SE

TL;DR: 该论文调查了由代码专用大语言模型（CodeLLMs）生成的幻觉问题，分类了幻觉类型，回顾了现有基准和缓解策略，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于CodeLLMs在生成代码时容易产生难以察觉的幻觉代码，这对软件开发尤其是低代码环境带来潜在风险，因此需要系统研究这一问题。

Method: 通过综述近期相关研究，分类CodeLLMs生成的幻觉类型，并分析现有基准和缓解策略。

Result: 总结了CodeLLMs生成代码中的幻觉问题，提出了检测和消除这些幻觉的挑战。

Conclusion: 论文呼吁进一步研究CodeLLMs生成代码中的幻觉问题，并提出了未来研究方向。

Abstract: Recent technical breakthroughs in large language models (LLMs) have enabled
them to fluently generate source code. Software developers often leverage both
general-purpose and code-specialized LLMs to revise existing code or even
generate a whole function from scratch. These capabilities are also beneficial
in no-code or low-code contexts, in which one can write programs without a
technical background. However, due to their internal design, LLMs are prone to
generating hallucinations, which are incorrect, nonsensical, and not
justifiable information but difficult to identify its presence. This problem
also occurs when generating source code. Once hallucinated code is produced, it
is often challenging for users to identify and fix it, especially when such
hallucinations can be identified under specific execution paths. As a result,
the hallucinated code may remain unnoticed within the codebase. This survey
investigates recent studies and techniques relevant to hallucinations generated
by CodeLLMs. We categorize the types of hallucinations in the code generated by
CodeLLMs, review existing benchmarks and mitigation strategies, and identify
open challenges. Based on these findings, this survey outlines further research
directions in the detection and removal of hallucinations produced by CodeLLMs.

</details>
