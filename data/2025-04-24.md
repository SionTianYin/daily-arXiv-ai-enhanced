<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 36]
- [cs.CV](#cs.CV) [Total: 65]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.LG](#cs.LG) [Total: 51]
- [eess.SP](#eess.SP) [Total: 8]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.DC](#cs.DC) [Total: 3]
- [stat.ML](#stat.ML) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [eess.IV](#eess.IV) [Total: 3]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [cs.CR](#cs.CR) [Total: 14]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.GR](#cs.GR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 10]
- [cs.RO](#cs.RO) [Total: 7]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.HC](#cs.HC) [Total: 6]
- [math.NA](#math.NA) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking](https://arxiv.org/abs/2504.16188)
*Jabez Magomere,Elena Kochkina,Samuel Mensah,Simerjot Kaur,Charese H. Smiley*

Main category: cs.CL

TL;DR: FinNLI是一个金融自然语言推理基准数据集，包含21,304对前提-假设对，测试集由金融专家标注。评估显示领域转移显著降低通用NLI性能，金融LLMs表现不佳。


<details>
  <summary>Details</summary>
Motivation: 构建一个多样化的金融NLI数据集，以评估模型在金融领域的推理能力，并揭示当前模型的局限性。

Method: 通过SEC文件、年报和财报电话记录构建数据集，确保多样性并减少虚假相关性。测试集由金融专家标注。

Result: 预训练模型和大型语言模型的最高Macro F1分别为74.57%和78.62%，金融LLMs表现不佳。

Conclusion: FinNLI揭示了当前LLMs在金融推理中的弱点，表明仍有改进空间。

Abstract: We introduce FinNLI, a benchmark dataset for Financial Natural Language
Inference (FinNLI) across diverse financial texts like SEC Filings, Annual
Reports, and Earnings Call transcripts. Our dataset framework ensures diverse
premise-hypothesis pairs while minimizing spurious correlations. FinNLI
comprises 21,304 pairs, including a high-quality test set of 3,304 instances
annotated by finance experts. Evaluations show that domain shift significantly
degrades general-domain NLI performance. The highest Macro F1 scores for
pre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and
78.62%, respectively, highlighting the dataset's difficulty. Surprisingly,
instruction-tuned financial LLMs perform poorly, suggesting limited
generalizability. FinNLI exposes weaknesses in current LLMs for financial
reasoning, indicating room for improvement.

</details>


### [2] [The Language of Attachment: Modeling Attachment Dynamics in Psychotherapy](https://arxiv.org/abs/2504.16271)
*Frederik Bredgaard,Martin Lund Trinhammer,Elisa Bassignana*

Main category: cs.CL

TL;DR: 论文探讨了利用NLP技术自动识别患者依恋风格，以改进心理治疗中的心理健康服务。


<details>
  <summary>Details</summary>
Motivation: 目前依恋风格的评估依赖复杂且耗资源的人工方法（PACS），限制了广泛推广。自动化工具有望提升效率和可扩展性。

Method: 使用NLP分类模型对心理治疗转录文本进行分析，首次尝试自动评估患者依恋风格。

Result: 研究发现自动化工具在识别依恋风格时存在混淆（如‘沉迷型’与‘回避型’），可能对治疗结果产生负面影响。

Conclusion: 研究为个性化心理治疗和NLP在心理治疗机制研究中的应用开辟了新途径。

Abstract: The delivery of mental healthcare through psychotherapy stands to benefit
immensely from developments within Natural Language Processing (NLP), in
particular through the automatic identification of patient specific qualities,
such as attachment style. Currently, the assessment of attachment style is
performed manually using the Patient Attachment Coding System (PACS; Talia et
al., 2017), which is complex, resource-consuming and requires extensive
training. To enable wide and scalable adoption of attachment informed treatment
and research, we propose the first exploratory analysis into automatically
assessing patient attachment style from psychotherapy transcripts using NLP
classification models. We further analyze the results and discuss the
implications of using automated tools for this purpose -- e.g., confusing
`preoccupied' patients with `avoidant' likely has a more negative impact on
therapy outcomes with respect to other mislabeling. Our work opens an avenue of
research enabling more personalized psychotherapy and more targeted research
into the mechanisms of psychotherapy through advancements in NLP.

</details>


### [3] [The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation](https://arxiv.org/abs/2504.16286)
*Li Weigang,Pedro Carvalho Brom*

Main category: cs.CL

TL;DR: 研究评估了六种大型语言模型（LLMs）和三种传统翻译工具在中文-英文翻译中的表现，发现LLMs在科学摘要中表现较好，但在文化和文学保留上存在不足，并提出了一种新的BLEU变体。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在中文-英文翻译中保留诗意意图、文化遗产和处理专业术语的挑战。

Method: 构建多样化语料库，使用基于回译和Friedman测试的评估系统（BT-Fried），评估BLEU、CHRF、TER和语义相似度指标。

Result: 科学摘要适合回译，传统工具在语言差异大的文本中表现更好；LLMs在文化和文学保留上表现不佳；部分模型出现“逐字回译”现象。

Conclusion: 研究为中文NLP性能的实证评估提供了贡献，并推进了对AI翻译中文化保真度的理解。

Abstract: The rapid advancement of large language models (LLMs) has reshaped the
landscape of machine translation, yet challenges persist in preserving poetic
intent, cultural heritage, and handling specialized terminology in
Chinese-English translation. This study constructs a diverse corpus
encompassing Chinese scientific terminology, historical translation paradoxes,
and literary metaphors. Utilizing a back-translation and Friedman test-based
evaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic
similarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three
traditional translation tools. Key findings include: (1) Scientific abstracts
often benefit from back-translation, while traditional tools outperform LLMs in
linguistically distinct texts; (2) LLMs struggle with cultural and literary
retention, exemplifying the "paradox of poetic intent"; (3) Some models exhibit
"verbatim back-translation", reflecting emergent memory behavior; (4) A novel
BLEU variant using Jieba segmentation and n-gram weighting is proposed. The
study contributes to the empirical evaluation of Chinese NLP performance and
advances understanding of cultural fidelity in AI-mediated translation.

</details>


### [4] [Capturing Symmetry and Antisymmetry in Language Models through Symmetry-Aware Training Objectives](https://arxiv.org/abs/2504.16312)
*Zhangdie Yuan,Andreas Vlachos*

Main category: cs.CL

TL;DR: 论文提出了一种新的Wikidata衍生的自然语言推理数据集，用于评估大语言模型（LLMs）在对称和反对称关系理解上的表现，发现LLMs表现接近随机。通过对比学习重新训练编码器，性能与微调分类头相当，且具有少样本学习和抗灾难性遗忘的优势。


<details>
  <summary>Details</summary>
Motivation: 对称和反对称关系（如国家边界或亲子关系）的理解对多种应用至关重要，但现有LLMs在此类任务上表现不佳。

Method: 引入新的Wikidata数据集评估LLMs，并通过对比学习重新训练编码器。

Result: LLMs在关系理解任务上表现接近随机，重新训练的编码器性能与微调分类头相当，且在少样本学习和抗遗忘性上更优。

Conclusion: 研究表明LLMs在关系理解上存在不足，重新训练编码器是有效的改进方法，具有实际应用潜力。

Abstract: Capturing symmetric (e.g., country borders another country) and antisymmetric
(e.g., parent_of) relations is crucial for a variety of applications. This
paper tackles this challenge by introducing a novel Wikidata-derived natural
language inference dataset designed to evaluate large language models (LLMs).
Our findings reveal that LLMs perform comparably to random chance on this
benchmark, highlighting a gap in relational understanding. To address this, we
explore encoder retraining via contrastive learning with k-nearest neighbors.
The retrained encoder matches the performance of fine-tuned classification
heads while offering additional benefits, including greater efficiency in
few-shot learning and improved mitigation of catastrophic forgetting.

</details>


### [5] [Transformer-Based Extraction of Statutory Definitions from the U.S. Code](https://arxiv.org/abs/2504.16353)
*Arpana Hosabettu,Harsh Shah*

Main category: cs.CL

TL;DR: 论文提出了一种基于Transformer架构的NLP系统，用于从美国法典中自动提取定义术语及其定义和范围，显著提高了提取准确性。


<details>
  <summary>Details</summary>
Motivation: 提升对复杂法律文本（如美国法典）的理解和清晰度，解决自动识别法律定义、提取术语及其范围的挑战。

Method: 采用领域特定的Transformer模型（Legal-BERT），结合多阶段流程，包括文档结构分析和最先进的语言模型，分类段落并提取定义。

Result: 在多个美国法典标题上评估，最佳模型达到96.8%的精确率和98.9%的召回率（F1分数98.2%），显著优于传统方法。

Conclusion: 该研究提高了法律信息的可访问性和理解，并为下游法律推理任务奠定了基础。

Abstract: Automatic extraction of definitions from legal texts is critical for
enhancing the comprehension and clarity of complex legal corpora such as the
United States Code (U.S.C.). We present an advanced NLP system leveraging
transformer-based architectures to automatically extract defined terms, their
definitions, and their scope from the U.S.C. We address the challenges of
automatically identifying legal definitions, extracting defined terms, and
determining their scope within this complex corpus of over 200,000 pages of
federal statutory law. Building upon previous feature-based machine learning
methods, our updated model employs domain-specific transformers (Legal-BERT)
fine-tuned specifically for statutory texts, significantly improving extraction
accuracy. Our work implements a multi-stage pipeline that combines document
structure analysis with state-of-the-art language models to process legal text
from the XML version of the U.S. Code. Each paragraph is first classified using
a fine-tuned legal domain BERT model to determine if it contains a definition.
Our system then aggregates related paragraphs into coherent definitional units
and applies a combination of attention mechanisms and rule-based patterns to
extract defined terms and their jurisdictional scope. The definition extraction
system is evaluated on multiple titles of the U.S. Code containing thousands of
definitions, demonstrating significant improvements over previous approaches.
Our best model achieves 96.8% precision and 98.9% recall (98.2% F1-score),
substantially outperforming traditional machine learning classifiers. This work
contributes to improving accessibility and understanding of legal information
while establishing a foundation for downstream legal reasoning tasks.

</details>


### [6] [Text-to-TrajVis: Enabling Trajectory Data Visualizations from Natural Language Questions](https://arxiv.org/abs/2504.16358)
*Tian Bai,Huiyan Ying,Kailong Suo,Junqiu Wei,Tao Fan,Yuanfeng Song*

Main category: cs.CL

TL;DR: 论文提出了Text-to-TrajVis任务，将自然语言问题转化为轨迹数据可视化，并构建了首个大规模数据集TrajVL。


<details>
  <summary>Details</summary>
Motivation: 解决轨迹可视化系统中自然语言接口开发的数据集缺失问题。

Method: 设计了轨迹可视化语言（TVL），结合LLMs和人工标注构建数据集TrajVL，并评估多种LLMs的性能。

Result: 实验证明任务可行但具挑战性，TrajVL包含18,140对（问题，TVL）。

Conclusion: Text-to-TrajVis任务值得进一步研究。

Abstract: This paper introduces the Text-to-TrajVis task, which aims to transform
natural language questions into trajectory data visualizations, facilitating
the development of natural language interfaces for trajectory visualization
systems. As this is a novel task, there is currently no relevant dataset
available in the community. To address this gap, we first devised a new
visualization language called Trajectory Visualization Language (TVL) to
facilitate querying trajectory data and generating visualizations. Building on
this foundation, we further proposed a dataset construction method that
integrates Large Language Models (LLMs) with human efforts to create
high-quality data. Specifically, we first generate TVLs using a comprehensive
and systematic process, and then label each TVL with corresponding natural
language questions using LLMs. This process results in the creation of the
first large-scale Text-to-TrajVis dataset, named TrajVL, which contains 18,140
(question, TVL) pairs. Based on this dataset, we systematically evaluated the
performance of multiple LLMs (GPT, Qwen, Llama, etc.) on this task. The
experimental results demonstrate that this task is both feasible and highly
challenging and merits further exploration within the research community.

</details>


### [7] [SplitReason: Learning To Offload Reasoning](https://arxiv.org/abs/2504.16379)
*Yash Akhauri,Anthony Fei,Chi-Chih Chang,Ahmed F. AbouElhamayed,Yueying Li,Mohamed S. Abdelfattah*

Main category: cs.CL

TL;DR: 论文提出了一种通过将推理过程中最困难的部分卸载到更大模型的方法，以提高大型语言模型（LLMs）的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs在推理任务中生成较长的序列，导致解码阶段效率低下，但并非所有部分都同样难以生成。

Method: 通过标注困难部分并训练小模型识别和触发卸载，结合监督和强化学习微调。

Result: 在AIME24推理任务中，准确率提高了24%和28.3%，同时仅卸载了少量token。

Conclusion: SplitReason方法有效提升了推理效率和准确性，相关资源已开源。

Abstract: Reasoning in large language models (LLMs) tends to produce substantially
longer token generation sequences than simpler language modeling tasks. This
extended generation length reflects the multi-step, compositional nature of
reasoning and is often correlated with higher solution accuracy. From an
efficiency perspective, longer token generation exacerbates the inherently
sequential and memory-bound decoding phase of LLMs. However, not all parts of
this expensive reasoning process are equally difficult to generate. We leverage
this observation by offloading only the most challenging parts of the reasoning
process to a larger, more capable model, while performing most of the
generation with a smaller, more efficient model; furthermore, we teach the
smaller model to identify these difficult segments and independently trigger
offloading when needed. To enable this behavior, we annotate difficult segments
across 18k reasoning traces from the OpenR1-Math-220k chain-of-thought (CoT)
dataset. We then apply supervised fine-tuning (SFT) and reinforcement learning
fine-tuning (RLFT) to a 1.5B-parameter reasoning model, training it to learn to
offload the most challenging parts of its own reasoning process to a larger
model. This approach improves AIME24 reasoning accuracy by 24% and 28.3% while
offloading 1.35% and 5% of the generated tokens respectively. We open-source
our SplitReason model, data, code and logs.

</details>


### [8] [ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs](https://arxiv.org/abs/2504.16394)
*Fahmida Liza Piya,Rahmatollah Beheshti*

Main category: cs.CL

TL;DR: ConTextual框架结合上下文保留令牌过滤方法和领域知识图谱，提升临床文本摘要的临床相关性和语言连贯性。


<details>
  <summary>Details</summary>
Motivation: 利用非结构化临床数据的潜力，优化临床决策，解决现有方法忽略临床细微线索的问题。

Method: 提出ConTextual框架，整合上下文保留令牌过滤和领域知识图谱，增强上下文信息。

Result: 在两个公共基准数据集上表现优于其他基线方法。

Conclusion: 令牌级过滤和结构化检索互补，提升临床文本生成的精确性和可扩展性。

Abstract: Unstructured clinical data can serve as a unique and rich source of
information that can meaningfully inform clinical practice. Extracting the most
pertinent context from such data is critical for exploiting its true potential
toward optimal and timely decision-making in patient care. While prior research
has explored various methods for clinical text summarization, most prior
studies either process all input tokens uniformly or rely on heuristic-based
filters, which can overlook nuanced clinical cues and fail to prioritize
information critical for decision-making. In this study, we propose Contextual,
a novel framework that integrates a Context-Preserving Token Filtering method
with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By
preserving context-specific important tokens and enriching them with structured
knowledge, ConTextual improves both linguistic coherence and clinical fidelity.
Our extensive empirical evaluations on two public benchmark datasets
demonstrate that ConTextual consistently outperforms other baselines. Our
proposed approach highlights the complementary role of token-level filtering
and structured retrieval in enhancing both linguistic and clinical integrity,
as well as offering a scalable solution for improving precision in clinical
text generation.

</details>


### [9] [Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation](https://arxiv.org/abs/2504.16408)
*Jiahao Yuan,Xingzhe Sun,Xing Yu,Jingwen Wang,Dehui Du,Zhiqing Cui,Zixiang Di*

Main category: cs.CL

TL;DR: 论文介绍了一种名为'Less is More'的方法，在XLLM@ACL2025共享任务-III中取得第三名，通过多智能体框架和少量标注数据（24个示例）实现结构化推理。


<details>
  <summary>Details</summary>
Motivation: 解决低资源环境下结构化推理任务中生成可解释、逐步推理的挑战。

Method: 采用多智能体框架，结合反向提示归纳、检索增强推理合成（GPT-4o）和双阶段奖励引导过滤，优化问题解析、CoT解析和步骤验证。

Result: 通过结构验证和奖励过滤，在少样本和零样本提示下显著提升结构化推理质量。

Conclusion: 可控数据蒸馏在低资源条件下对结构化推理具有重要价值。

Abstract: The XLLM@ACL2025 Shared Task-III formulates a low-resource structural
reasoning task that challenges LLMs to generate interpretable, step-by-step
rationales with minimal labeled data. We present Less is More, the third-place
winning approach in the XLLM@ACL2025 Shared Task-III, which focuses on
structured reasoning from only 24 labeled examples. Our approach leverages a
multi-agent framework with reverse-prompt induction, retrieval-augmented
reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to
distill high-quality supervision across three subtasks: question parsing, CoT
parsing, and step-level verification. All modules are fine-tuned from
Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure
validation with reward filtering across few-shot and zero-shot prompts, our
pipeline consistently improves structure reasoning quality. These results
underscore the value of controllable data distillation in enhancing structured
inference under low-resource constraints. Our code is available at
https://github.com/Jiahao-Yuan/Less-is-More.

</details>


### [10] [Out-of-the-Box Conditional Text Embeddings from Large Language Models](https://arxiv.org/abs/2504.16411)
*Kosuke Yamada,Peinan Zhang*

Main category: cs.CL

TL;DR: PonTE是一种无监督的条件文本嵌入方法，利用因果大语言模型和条件提示，无需微调即可生成有用的嵌入，性能接近监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量训练数据进行微调，导致劳动力和资源成本高，PonTE旨在解决这一问题。

Method: 使用因果大语言模型和条件提示生成条件文本嵌入，无需微调。

Result: 在条件语义文本相似性和文本聚类任务中表现接近监督方法，嵌入具有可解释性。

Conclusion: PonTE是一种高效的无监督条件文本嵌入方法，性能优越且可解释。

Abstract: Conditional text embedding is a proposed representation that captures the
shift in perspective on texts when conditioned on a specific aspect. Previous
methods have relied on extensive training data for fine-tuning models, leading
to challenges in terms of labor and resource costs. We propose PonTE, a novel
unsupervised conditional text embedding method that leverages a causal large
language model and a conditional prompt. Through experiments on conditional
semantic text similarity and text clustering, we demonstrate that PonTE can
generate useful conditional text embeddings and achieve performance comparable
to supervised methods without fine-tuning. We also show the interpretability of
text embeddings with PonTE by analyzing word generation following prompts and
embedding visualization.

</details>


### [11] [Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study](https://arxiv.org/abs/2504.16414)
*Mohammad Khodadad,Ali Shiraee Kasmaee,Mahdi Astaraki,Nicholas Sherck,Hamidreza Mahyar,Soheila Samiee*

Main category: cs.CL

TL;DR: 该研究提出了一个新的基准测试，用于评估大型语言模型在化学领域的组合推理能力，揭示了当前模型的局限性，并展示了数据生成管道的潜力。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在化学领域的组合推理能力，并探索如何通过上下文增强提升其性能。

Method: 结合OpenAI推理模型和命名实体识别系统，构建知识图谱并生成多跳问题，评估模型在不同设置下的表现。

Result: 实验表明，即使是先进模型在多跳组合推理中也面临挑战，上下文增强能显著提升性能，但无法完全消除推理错误。

Conclusion: 该研究不仅揭示了当前模型的局限性，还提出了一种跨领域生成挑战性推理数据集的新方法，推动了计算语言学中对推理的理解。

Abstract: In this study, we introduced a new benchmark consisting of a curated dataset
and a defined evaluation process to assess the compositional reasoning
capabilities of large language models within the chemistry domain. We designed
and validated a fully automated pipeline, verified by subject matter experts,
to facilitate this task. Our approach integrates OpenAI reasoning models with
named entity recognition (NER) systems to extract chemical entities from recent
literature, which are then augmented with external knowledge bases to form a
comprehensive knowledge graph. By generating multi-hop questions across these
graphs, we assess LLM performance in both context-augmented and non-context
augmented settings. Our experiments reveal that even state-of-the-art models
face significant challenges in multi-hop compositional reasoning. The results
reflect the importance of augmenting LLMs with document retrieval, which can
have a substantial impact on improving their performance. However, even perfect
retrieval accuracy with full context does not eliminate reasoning errors,
underscoring the complexity of compositional reasoning. This work not only
benchmarks and highlights the limitations of current LLMs but also presents a
novel data generation pipeline capable of producing challenging reasoning
datasets across various domains. Overall, this research advances our
understanding of reasoning in computational linguistics.

</details>


### [12] [Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark](https://arxiv.org/abs/2504.16427)
*Hanlei Zhang,Zhuohang Li,Yeshuang Zhu,Hua Xu,Peiwu Wang,Jinchao Zhang,Jie Zhou,Haige Zhu*

Main category: cs.CL

TL;DR: 论文介绍了MMLA基准，用于评估多模态大语言模型（MLLMs）在理解认知级语义方面的能力，实验显示当前模型准确率仅60%~70%。


<details>
  <summary>Details</summary>
Motivation: 多模态语言分析领域缺乏对MLLMs理解认知级语义能力的研究，MMLA旨在填补这一空白。

Method: MMLA包含61K多模态话语，覆盖六个语义维度，评估了八种主流LLMs和MLLMs的三种方法：零样本推理、监督微调和指令调优。

Result: 实验表明，即使经过微调的模型准确率也仅为60%~70%，揭示了当前MLLMs的局限性。

Conclusion: MMLA为探索大语言模型在多模态语言分析中的潜力提供了基础，并开源了数据集和代码。

Abstract: Multimodal language analysis is a rapidly evolving field that leverages
multiple modalities to enhance the understanding of high-level semantics
underlying human conversational utterances. Despite its significance, little
research has investigated the capability of multimodal large language models
(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce
MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA
comprises over 61K multimodal utterances drawn from both staged and real-world
scenarios, covering six core dimensions of multimodal semantics: intent,
emotion, dialogue act, sentiment, speaking style, and communication behavior.
We evaluate eight mainstream branches of LLMs and MLLMs using three methods:
zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive
experiments reveal that even fine-tuned models achieve only about 60%~70%
accuracy, underscoring the limitations of current MLLMs in understanding
complex human language. We believe that MMLA will serve as a solid foundation
for exploring the potential of large language models in multimodal language
analysis and provide valuable resources to advance this field. The datasets and
code are open-sourced at https://github.com/thuiar/MMLA.

</details>


### [13] [EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records](https://arxiv.org/abs/2504.16448)
*Shuguang Zhao,Qiangzhong Feng,Zhiyang He,Peipei Sun,Yingying Wang,Xiaodong Tao,Xiaoliang Lu,Mei Cheng,Xinyue Wu,Yanyan Wang,Wei Liang*

Main category: cs.CL

TL;DR: EMRModel结合LoRA微调和代码风格提示设计，高效将医疗咨询对话转为结构化电子病历，F1分数达88.1%，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 医疗咨询对话的非结构化特性限制了其在诊断和治疗中的有效利用，传统方法难以捕捉深层语义。

Method: 提出EMRModel，整合LoRA微调和代码风格提示设计，构建高质量标注数据集，并引入细粒度评估基准。

Result: 实验结果显示，EMRModel的F1分数为88.1%，比标准预训练模型提升49.5%，优于传统LoRA方法。

Conclusion: EMRModel在结构化医疗记录提取任务中表现出色，推动了医疗NLP模型的优化。

Abstract: Medical consultation dialogues contain critical clinical information, yet
their unstructured nature hinders effective utilization in diagnosis and
treatment. Traditional methods, relying on rule-based or shallow machine
learning techniques, struggle to capture deep and implicit semantics. Recently,
large pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight
fine-tuning method, have shown promise for structured information extraction.
We propose EMRModel, a novel approach that integrates LoRA-based fine-tuning
with code-style prompt design, aiming to efficiently convert medical
consultation dialogues into structured electronic medical records (EMRs).
Additionally, we construct a high-quality, realistically grounded dataset of
medical consultation dialogues with detailed annotations. Furthermore, we
introduce a fine-grained evaluation benchmark for medical consultation
information extraction and provide a systematic evaluation methodology,
advancing the optimization of medical natural language processing (NLP) models.
Experimental results show EMRModel achieves an F1 score of 88.1%, improving
by49.5% over standard pre-trained models. Compared to traditional LoRA
fine-tuning methods, our model shows superior performance, highlighting its
effectiveness in structured medical record extraction tasks.

</details>


### [14] [T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning](https://arxiv.org/abs/2504.16460)
*Vignesh Ethiraj,Sidhanth Menon,Divya Vijay*

Main category: cs.CL

TL;DR: T-VEC是一种专为电信行业定制的嵌入模型，通过深度微调显著提升领域特定语义捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 电信行业的专业词汇和复杂概念对通用NLP模型构成挑战，需要领域特定的解决方案。

Method: 基于gte-Qwen2-1.5B-instruct模型，通过三重损失目标在电信数据集上深度微调，并开发专用分词器。

Result: T-VEC在MTEB评分（0.825）和电信特定评测（0.9380）中表现优异，显著优于现有模型。

Conclusion: T-VEC为电信AI提供了强大的开源工具，推动了领域创新。

Abstract: The specialized vocabulary and complex concepts of the telecommunications
industry present significant challenges for standard Natural Language
Processing models. Generic text embeddings often fail to capture
telecom-specific semantics, hindering downstream task performance. We introduce
T-VEC (Telecom Vectorization Model), a novel embedding model tailored for the
telecom domain through deep fine-tuning. Developed by NetoAI, T-VEC is created
by adapting the state-of-the-art gte-Qwen2-1.5B-instruct model using a triplet
loss objective on a meticulously curated, large-scale dataset of
telecom-specific data. Crucially, this process involved substantial
modification of weights across 338 layers of the base model, ensuring deep
integration of domain knowledge, far exceeding superficial adaptation
techniques. We quantify this deep change via weight difference analysis. A key
contribution is the development and open-sourcing (MIT License) of the first
dedicated telecom-specific tokenizer, enhancing the handling of industry
jargon. T-VEC achieves a leading average MTEB score (0.825) compared to
established models and demonstrates vastly superior performance (0.9380 vs.
less than 0.07) on our internal telecom-specific triplet evaluation benchmark,
indicating an exceptional grasp of domain-specific nuances, visually confirmed
by improved embedding separation. This work positions NetoAI at the forefront
of telecom AI innovation, providing the community with a powerful, deeply
adapted, open-source tool.

</details>


### [15] [QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining](https://arxiv.org/abs/2504.16511)
*Fengze Liu,Weidong Zhou,Binbin Liu,Zhimiao Yu,Yifan Zhang,Haobin Lin,Yifeng Yu,Xiaohuan Zhou,Taifeng Wang,Yong Cao*

Main category: cs.CL

TL;DR: QuaDMix是一个统一的数据选择框架，用于在LLM预训练中平衡数据质量和多样性，通过参数化采样函数和模拟实验优化，性能提升7.2%。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常单独优化数据质量或多样性，忽略了二者之间的权衡，需要联合考虑。

Method: 提出QuaDMix框架，通过多标准衡量数据质量，利用域分类评估多样性，并使用参数化采样函数优化数据分布。

Result: 实验表明，QuaDMix在多个基准测试中平均性能提升7.2%，优于独立优化策略。

Conclusion: QuaDMix证明了平衡数据质量和多样性的必要性和有效性。

Abstract: Quality and diversity are two critical metrics for the training data of large
language models (LLMs), positively impacting performance. Existing studies
often optimize these metrics separately, typically by first applying quality
filtering and then adjusting data proportions. However, these approaches
overlook the inherent trade-off between quality and diversity, necessitating
their joint consideration. Given a fixed training quota, it is essential to
evaluate both the quality of each data point and its complementary effect on
the overall dataset. In this paper, we introduce a unified data selection
framework called QuaDMix, which automatically optimizes the data distribution
for LLM pretraining while balancing both quality and diversity. Specifically,
we first propose multiple criteria to measure data quality and employ domain
classification to distinguish data points, thereby measuring overall diversity.
QuaDMix then employs a unified parameterized data sampling function that
determines the sampling probability of each data point based on these quality
and diversity related labels. To accelerate the search for the optimal
parameters involved in the QuaDMix framework, we conduct simulated experiments
on smaller models and use LightGBM for parameters searching, inspired by the
RegMix method. Our experiments across diverse models and datasets demonstrate
that QuaDMix achieves an average performance improvement of 7.2% across
multiple benchmarks. These results outperform the independent strategies for
quality and diversity, highlighting the necessity and ability to balance data
quality and diversity.

</details>


### [16] [Transformers for Complex Query Answering over Knowledge Hypergraphs](https://arxiv.org/abs/2504.16537)
*Hong Ting Tsang,Zihao Wang,Yangqiu Song*

Main category: cs.CL

TL;DR: 论文提出了一种两阶段Transformer模型LKHGT，用于回答知识超图的存在性一阶查询，并在新数据集上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据复杂，传统三元组知识图谱表达能力有限，需处理多模态和变元数关系。

Method: 提出LKHGT模型，包含投影编码器和逻辑编码器，结合类型感知偏置（TAB）捕捉交互。

Result: 实验表明LKHGT在知识超图上的复杂查询回答性能优异，并能泛化到分布外查询类型。

Conclusion: LKHGT是知识超图上复杂查询回答的先进方法，具有泛化能力。

Abstract: Complex Query Answering (CQA) has been extensively studied in recent years.
In order to model data that is closer to real-world distribution, knowledge
graphs with different modalities have been introduced. Triple KGs, as the
classic KGs composed of entities and relations of arity 2, have limited
representation of real-world facts. Real-world data is more sophisticated.
While hyper-relational graphs have been introduced, there are limitations in
representing relationships of varying arity that contain entities with equal
contributions. To address this gap, we sampled new CQA datasets: JF17k-HCQA and
M-FB15k-HCQA. Each dataset contains various query types that include logical
operations such as projection, negation, conjunction, and disjunction. In order
to answer knowledge hypergraph (KHG) existential first-order queries, we
propose a two-stage transformer model, the Logical Knowledge Hypergraph
Transformer (LKHGT), which consists of a Projection Encoder for atomic
projection and a Logical Encoder for complex logical operations. Both encoders
are equipped with Type Aware Bias (TAB) for capturing token interactions.
Experimental results on CQA datasets show that LKHGT is a state-of-the-art CQA
method over KHG and is able to generalize to out-of-distribution query types.

</details>


### [17] [PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression](https://arxiv.org/abs/2504.16574)
*Lizhe Chen,Binjia Zhou,Yuyao Ge,Jiayi Chen,Shiguang NI*

Main category: cs.CL

TL;DR: 论文提出了一种名为Prompt Importance Sampling (PIS)的新框架，通过动态采样重要token来压缩提示，结合token级和语义级的双重压缩机制，显著提升了压缩性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的高性能伴随高昂成本，限制了其广泛应用，现有压缩方法忽视LLMs的内在机制且缺乏系统性评估。

Method: PIS框架通过分析隐藏状态的注意力分数动态压缩提示，包括token级的自适应压缩（使用轻量级RL网络）和语义级的俄罗斯轮盘采样策略。

Result: 在多个领域基准测试中，PIS实现了最先进的压缩性能，并意外提升了推理效率。

Conclusion: PIS为LLMs的提示工程提供了理论基础和实践效率，优化了上下文管理。

Abstract: Large language models (LLMs) have achieved remarkable progress, demonstrating
unprecedented capabilities across various natural language processing tasks.
However, the high costs associated with such exceptional performance limit the
widespread adoption of LLMs, highlighting the need for prompt compression.
Existing prompt compression methods primarily rely on heuristic truncation or
abstractive summarization techniques, which fundamentally overlook the
intrinsic mechanisms of LLMs and lack a systematic evaluation of token
importance for generation. In this work, we introduce Prompt Importance
Sampling (PIS), a novel compression framework that dynamically compresses
prompts by sampling important tokens based on the analysis of attention scores
of hidden states. PIS employs a dual-level compression mechanism: 1) at the
token level, we quantify saliency using LLM-native attention scores and
implement adaptive compression through a lightweight 9-layer reinforcement
learning (RL) network; 2) at the semantic level, we propose a Russian roulette
sampling strategy for sentence-level importance sampling. Comprehensive
evaluations across multiple domain benchmarks demonstrate that our method
achieves state-of-the-art compression performance. Notably, our framework
serendipitously enhances reasoning efficiency through optimized context
structuring. This work advances prompt engineering by offering both theoretical
grounding and practical efficiency in context management for LLMs.

</details>


### [18] [Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study](https://arxiv.org/abs/2504.16601)
*Andy Li,Wei Zhou,Rashina Hoda,Chris Bain,Peter Poon*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型（LLMs）与传统机器翻译（MT）工具在将医学咨询摘要从英语翻译为阿拉伯语、中文和越南语时的表现。结果显示传统MT工具表现更好，而LLMs在简单摘要翻译中表现有潜力。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs与传统MT工具在医学翻译中的表现，以了解其在不同语言和文本复杂度下的适用性。

Method: 使用标准自动化指标评估患者友好型和临床医生导向型文本的翻译质量。

Result: 传统MT工具表现更优，尤其在复杂文本中；LLMs在越南语和中文的简单摘要翻译中表现较好。阿拉伯语翻译随复杂度提升而改善。

Conclusion: LLMs虽具上下文灵活性，但表现不稳定，且当前评估指标未能捕捉临床相关性。需领域特定训练、改进评估方法及人工监督。

Abstract: This study evaluates how well large language models (LLMs) and traditional
machine translation (MT) tools translate medical consultation summaries from
English into Arabic, Chinese, and Vietnamese. It assesses both patient,
friendly and clinician, focused texts using standard automated metrics. Results
showed that traditional MT tools generally performed better, especially for
complex texts, while LLMs showed promise, particularly in Vietnamese and
Chinese, when translating simpler summaries. Arabic translations improved with
complexity due to the language's morphology. Overall, while LLMs offer
contextual flexibility, they remain inconsistent, and current evaluation
metrics fail to capture clinical relevance. The study highlights the need for
domain-specific training, improved evaluation methods, and human oversight in
medical translation.

</details>


### [19] [Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories](https://arxiv.org/abs/2504.16604)
*Mareike Lisker,Christina Gottschalk,Helena Mihaljević*

Main category: cs.CL

TL;DR: 论文研究了利用大型语言模型（如GPT-4o、Llama 3和Mistral）生成针对阴谋论的对抗性言论，发现模型表现不佳，存在重复、泛泛而谈和虚构信息的问题。


<details>
  <summary>Details</summary>
Motivation: 专家驱动的对抗性言论难以规模化，而大型语言模型可能提供解决方案，但针对阴谋论的研究和数据缺乏。

Method: 通过结构化提示，评估GPT-4o、Llama 3和Mistral在生成对抗性言论中的表现。

Result: 模型生成的对抗性言论通常重复、泛泛而谈，且常虚构事实或数据。

Conclusion: 当前基于提示的大型语言模型在实际应用中存在局限性，需进一步改进。

Abstract: Counterspeech is a key strategy against harmful online content, but scaling
expert-driven efforts is challenging. Large Language Models (LLMs) present a
potential solution, though their use in countering conspiracy theories is
under-researched. Unlike for hate speech, no datasets exist that pair
conspiracy theory comments with expert-crafted counterspeech. We address this
gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively
apply counterspeech strategies derived from psychological research provided
through structured prompts. Our results show that the models often generate
generic, repetitive, or superficial results. Additionally, they
over-acknowledge fear and frequently hallucinate facts, sources, or figures,
making their prompt-based use in practical applications problematic.

</details>


### [20] [TIFIN India at SemEval-2025: Harnessing Translation to Overcome Multilingual IR Challenges in Fact-Checked Claim Retrieval](https://arxiv.org/abs/2504.16627)
*Prasanna Devadiga,Arya Suneesh,Pawan Kumar Rajpoot,Bharatdeep Hazarika,Aditya U Baliga*

Main category: cs.CL

TL;DR: 论文提出了一种两阶段策略，用于单语和跨语言环境下检索事实核查过的信息，结合了微调嵌入模型和LLM重排技术，并展示了LLM翻译在跨语言检索中的优势。


<details>
  <summary>Details</summary>
Motivation: 解决全球范围内虚假信息传播的挑战，特别是在单语和跨语言环境下检索事实核查信息的困难。

Method: 采用两阶段策略：1) 基于微调嵌入模型的基线检索系统；2) LLM重排技术，并利用LLM翻译解决跨语言检索问题。

Result: 最终系统在单语和跨语言测试集上的success@10得分分别为0.938和0.81025。

Conclusion: LLM翻译能有效解决跨语言检索难题，且系统设计可在消费级GPU上实现。

Abstract: We address the challenge of retrieving previously fact-checked claims in
monolingual and crosslingual settings - a critical task given the global
prevalence of disinformation. Our approach follows a two-stage strategy: a
reliable baseline retrieval system using a fine-tuned embedding model and an
LLM-based reranker. Our key contribution is demonstrating how LLM-based
translation can overcome the hurdles of multilingual information retrieval.
Additionally, we focus on ensuring that the bulk of the pipeline can be
replicated on a consumer GPU. Our final integrated system achieved a success@10
score of 0.938 and 0.81025 on the monolingual and crosslingual test sets,
respectively.

</details>


### [21] [A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics](https://arxiv.org/abs/2504.16677)
*Luisa Shimabucoro,Ahmet Ustun,Marzieh Fadaee,Sebastian Ruder*

Main category: cs.CL

TL;DR: 研究探讨了多语言数据微调后大语言模型的跨语言迁移动态，发现其表现受多种因素综合影响，并提出了有效迁移的条件。


<details>
  <summary>Details</summary>
Motivation: 理解跨语言迁移的动态机制，以提升大语言模型在全球范围内的实用性。

Method: 研究了两个模型家族（最大35B参数）在多语言数据混合下的表现，涵盖三种生成任务（摘要、指令跟随、数学推理）及单任务与多任务微调设置。

Result: 跨语言迁移和性能表现无法由单一变量解释，而是受多种因素综合影响。

Conclusion: 研究确定了实践中实现有效跨语言迁移的条件。

Abstract: In order for large language models to be useful across the globe, they are
fine-tuned to follow instructions on multilingual data. Despite the ubiquity of
such post-training, a clear understanding of the dynamics that enable
cross-lingual transfer remains elusive. This study examines cross-lingual
transfer (CLT) dynamics in realistic post-training settings. We study two model
families of up to 35B parameters in size trained on carefully controlled
mixtures of multilingual data on three generative tasks with varying levels of
complexity (summarization, instruction following, and mathematical reasoning)
in both single-task and multi-task instruction tuning settings. Overall, we
find that the dynamics of cross-lingual transfer and multilingual performance
cannot be explained by isolated variables, varying depending on the combination
of post-training settings. Finally, we identify the conditions that lead to
effective cross-lingual transfer in practice.

</details>


### [22] [HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations](https://arxiv.org/abs/2504.16754)
*Kwangseob Ahn*

Main category: cs.CL

TL;DR: HEMA是一种受人类认知启发的双记忆系统，通过结合紧凑记忆和向量记忆，显著提升了长对话的连贯性和事实回忆能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长对话中难以保持连贯性，HEMA旨在解决这一问题。

Method: HEMA结合了紧凑记忆（持续更新的单句摘要）和向量记忆（基于余弦相似度查询的块嵌入存储），并与6B参数变压器集成。

Result: 实验显示，HEMA在300轮对话中保持连贯，事实回忆准确率从41%提升至87%，人类评分连贯性从2.7提升至4.3。

Conclusion: HEMA通过结合逐字回忆和语义连续性，为隐私敏感的对话AI提供了实用的长对话解决方案。

Abstract: Large language models (LLMs) struggle with maintaining coherence in extended
conversations spanning hundreds of turns, despite performing well within their
context windows. This paper introduces HEMA (Hippocampus-Inspired Extended
Memory Architecture), a dual-memory system inspired by human cognitive
processes. HEMA combines Compact Memory - a continuously updated one-sentence
summary preserving global narrative coherence, and Vector Memory - an episodic
store of chunk embeddings queried via cosine similarity. When integrated with a
6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns
while keeping prompt length under 3,500 tokens. Experimental results show
substantial improvements: factual recall accuracy increases from 41% to 87%,
and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K
indexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling
the area under the precision-recall curve compared to summarization-only
approaches. Ablation studies reveal two key insights: semantic forgetting
through age-weighted pruning reduces retrieval latency by 34% with minimal
recall loss, and a two-level summary hierarchy prevents cascade errors in
ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that
combining verbatim recall with semantic continuity provides a practical
solution for privacy-aware conversational AI capable of month-long dialogues
without model retraining.

</details>


### [23] [How Effective are Generative Large Language Models in Performing Requirements Classification?](https://arxiv.org/abs/2504.16768)
*Waad Alhoshan,Alessio Ferrari,Liping Zhao*

Main category: cs.CL

TL;DR: 本文探讨了生成式大语言模型（如Bloom、Gemma和Llama）在需求分类任务中的表现，通过400多次实验发现提示设计和模型架构对性能有普遍影响，而数据集差异的影响则因任务复杂度而异。


<details>
  <summary>Details</summary>
Motivation: 研究生成式大语言模型在需求分类任务中的潜力，填补现有研究中对此类模型探索的空白。

Method: 设计了涵盖三个数据集（PROMISE NFR、Functional-Quality和SecReq）的400多次实验，评估三种生成式LLM在二元和多类需求分类中的表现。

Result: 提示设计和模型架构对分类性能有普遍影响，数据集差异的影响则因任务复杂度不同而有所变化。

Conclusion: 未来模型开发和部署应优化提示结构，并根据任务需求调整模型架构以提高性能。

Abstract: In recent years, transformer-based large language models (LLMs) have
revolutionised natural language processing (NLP), with generative models
opening new possibilities for tasks that require context-aware text generation.
Requirements engineering (RE) has also seen a surge in the experimentation of
LLMs for different tasks, including trace-link detection, regulatory
compliance, and others. Requirements classification is a common task in RE.
While non-generative LLMs like BERT have been successfully applied to this
task, there has been limited exploration of generative LLMs. This gap raises an
important question: how well can generative LLMs, which produce context-aware
outputs, perform in requirements classification? In this study, we explore the
effectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing
both binary and multi-class requirements classification. We design an extensive
experimental study involving over 400 experiments across three widely used
datasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes
that while factors like prompt design and LLM architecture are universally
important, others-such as dataset variations-have a more situational impact,
depending on the complexity of the classification task. This insight can guide
future model development and deployment strategies, focusing on optimising
prompt structures and aligning model architectures with task-specific needs for
improved performance.

</details>


### [24] [Evaluation Framework for AI Systems in "the Wild"](https://arxiv.org/abs/2504.16778)
*Sarah Jabbour,Trenton Chang,Anindya Das Antar,Joseph Peper,Insu Jang,Jiachen Liu,Jae-Won Chung,Shiqi He,Michael Wellman,Bryan Goodman,Elizabeth Bondi-Kelly,Kevin Samy,Rada Mihalcea,Mosharaf Chowhury,David Jurgens,Lu Wang*

Main category: cs.CL

TL;DR: 本文提出了一种全面的生成式AI评估框架，强调动态、多样化的输入和持续评估，以弥补传统方法与实际应用的差距。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI评估方法未能适应其广泛应用，传统方法依赖固定数据集，无法反映真实性能。

Method: 提出动态、多样化的输入和持续评估框架，结合人类与自动化评估，注重透明性。

Result: 框架确保生成式AI模型在技术、公平性和伦理上的全面表现。

Conclusion: 实施该框架可使生成式AI模型更具技术能力和伦理责任感。

Abstract: Generative AI (GenAI) models have become vital across industries, yet current
evaluation methods have not adapted to their widespread use. Traditional
evaluations often rely on benchmarks and fixed datasets, frequently failing to
reflect real-world performance, which creates a gap between lab-tested outcomes
and practical applications. This white paper proposes a comprehensive framework
for how we should evaluate real-world GenAI systems, emphasizing diverse,
evolving inputs and holistic, dynamic, and ongoing assessment approaches. The
paper offers guidance for practitioners on how to design evaluation methods
that accurately reflect real-time capabilities, and provides policymakers with
recommendations for crafting GenAI policies focused on societal impacts, rather
than fixed performance numbers or parameter sizes. We advocate for holistic
frameworks that integrate performance, fairness, and ethics and the use of
continuous, outcome-oriented methods that combine human and automated
assessments while also being transparent to foster trust among stakeholders.
Implementing these strategies ensures GenAI models are not only technically
proficient but also ethically responsible and impactful.

</details>


### [25] [MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores](https://arxiv.org/abs/2504.16786)
*Fengwei Zhou,Jiafei Song,Wenjin Jason Li,Gengjian Xue,Zhikang Zhao,Yichao Lu,Bailin Na*

Main category: cs.CL

TL;DR: MOOSComp是一种基于令牌分类的长上下文压缩方法，通过解决过平滑问题和引入异常值分数，提升了BERT压缩器的性能，并在资源受限环境中实现了显著的加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理长上下文输入时面临推理时间和资源消耗增加的挑战，尤其在资源受限环境中。

Method: 提出MOOSComp方法，通过添加类间余弦相似性损失项和异常值分数，优化令牌分类精度并保留关键令牌。

Result: 在多种压缩比下，MOOSComp在长上下文理解和推理任务中表现优异，并在资源受限设备上实现了3.3倍的加速。

Conclusion: MOOSComp有效解决了长上下文压缩中的性能与效率问题，具有广泛的任务通用性。

Abstract: Recent advances in large language models have significantly improved their
ability to process long-context input, but practical applications are
challenged by increased inference time and resource consumption, particularly
in resource-constrained environments. To address these challenges, we propose
MOOSComp, a token-classification-based long-context compression method that
enhances the performance of a BERT-based compressor by mitigating the
over-smoothing problem and incorporating outlier scores. In the training phase,
we add an inter-class cosine similarity loss term to penalize excessively
similar token representations, thereby improving the token classification
accuracy. During the compression phase, we introduce outlier scores to preserve
rare but critical tokens that are prone to be discarded in task-agnostic
compression. These scores are integrated with the classifier's output, making
the compressor more generalizable to various tasks. Superior performance is
achieved at various compression ratios on long-context understanding and
reasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x
compression ratio on a resource-constrained mobile device.

</details>


### [26] [Credible plan-driven RAG method for Multi-hop Question Answering](https://arxiv.org/abs/2504.16787)
*Ningning Zhang,Chi Zhang,Zhizhong Tan,Xingxing Yang,Weiping Deng,Wenyong Wang*

Main category: cs.CL

TL;DR: PAR RAG框架通过规划、执行和审查三阶段，减少多跳问答中的错误传播，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法在多跳问答中易因推理路径偏差或中间结果错误导致答案不准确，需解决此问题。

Method: 提出PAR RAG框架，分规划、执行、审查三阶段，结合多粒度验证确保推理路径准确性。

Result: 在多跳问答数据集上，PAR RAG在EM和F1分数上显著优于现有方法。

Conclusion: PAR RAG通过结构化推理和错误控制，为多跳问答提供了更可靠的方法。

Abstract: Multi-hop question answering (QA) presents a considerable challenge for
Retrieval-Augmented Generation (RAG), requiring the structured decomposition of
complex queries into logical reasoning paths and the generation of dependable
intermediate results. However, deviations in reasoning paths or errors in
intermediate results, which are common in current RAG methods, may propagate
and accumulate throughout the reasoning process, diminishing the accuracy of
the answer to complex queries. To address this challenge, we propose the
Plan-then-Act-and-Review (PAR RAG) framework, which is organized into three key
stages: planning, act, and review, and aims to offer an interpretable and
incremental reasoning paradigm for accurate and reliable multi-hop question
answering by mitigating error propagation.PAR RAG initially applies a top-down
problem decomposition strategy, formulating a comprehensive plan that
integrates multiple executable steps from a holistic viewpoint. This approach
avoids the pitfalls of local optima common in traditional RAG methods, ensuring
the accuracy of the entire reasoning path. Subsequently, PAR RAG incorporates a
plan execution mechanism based on multi-granularity verification. By utilizing
both coarse-grained similarity information and fine-grained relevant data, the
framework thoroughly checks and adjusts intermediate results, ensuring process
accuracy while effectively managing error propagation and amplification.
Experimental results on multi-hop QA datasets demonstrate that the PAR RAG
framework substantially outperforms existing state-of-the-art methods in key
metrics, including EM and F1 scores.

</details>


### [27] [Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention](https://arxiv.org/abs/2504.16795)
*Xiang Hu,Jiaqi Leng,Jun Zhao,Kewei Tu,Wei Wu*

Main category: cs.CL

TL;DR: 论文提出了一种名为HSA的分层稀疏注意力机制，结合RNN的优势，实现了长序列的高效建模和随机访问能力。


<details>
  <summary>Details</summary>
Motivation: 解决RNN无法随机访问历史上下文的问题，同时保持其计算效率优势。

Method: 通过将输入分块并选择top-k块，结合细粒度令牌级信息学习块间相关性，设计硬件对齐的内核。

Result: RAMba在64百万上下文长度下实现完美准确率，并在下游任务中表现优异，内存占用几乎恒定。

Conclusion: HSA和RAMba展示了在长上下文建模中的巨大潜力。

Abstract: A key advantage of Recurrent Neural Networks (RNNs) over Transformers is
their linear computational and space complexity enables faster training and
inference for long sequences. However, RNNs are fundamentally unable to
randomly access historical context, and simply integrating attention mechanisms
may undermine their efficiency advantages. To overcome this limitation, we
propose \textbf{H}ierarchical \textbf{S}parse \textbf{A}ttention (HSA), a novel
attention mechanism that enhances RNNs with long-range random access
flexibility while preserving their merits in efficiency and length
generalization. HSA divides inputs into chunks, selecting the top-$k$ chunks
and hierarchically aggregates information. The core innovation lies in learning
token-to-chunk relevance based on fine-grained token-level information inside
each chunk. This approach enhances the precision of chunk selection across both
in-domain and out-of-domain context lengths. To make HSA efficient, we further
introduce a hardware-aligned kernel design. By combining HSA with Mamba, we
introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64
million contexts despite pre-training on only 4K-length contexts, and
significant improvements on various downstream tasks, with nearly constant
memory footprint. These results show RAMba's huge potential in long-context
modeling.

</details>


### [28] [LLM-assisted Graph-RAG Information Extraction from IFC Data](https://arxiv.org/abs/2504.16813)
*Sima Iranmanesh,Hadeel Saadany,Edlira Vakaj*

Main category: cs.CL

TL;DR: 利用Graph-RAG技术结合LLM解析复杂的IFC数据，实现自然语言查询。


<details>
  <summary>Details</summary>
Motivation: IFC数据在建筑行业中复杂且多义，需要高效解析方法。

Method: 采用Graph-RAG技术，结合LLM（如GPT-4o）解析IFC数据，提取对象属性及关系。

Result: Graph-RAG增强了LLM的图知识能力，支持自然语言查询，简化了流程。

Conclusion: Graph-RAG技术有效提升了LLM处理IFC数据的能力，无需复杂管道。

Abstract: IFC data has become the general building information standard for
collaborative work in the construction industry. However, IFC data can be very
complicated because it allows for multiple ways to represent the same product
information. In this research, we utilise the capabilities of LLMs to parse the
IFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to
retrieve building object properties and their relations. We will show that,
despite limitations due to the complex hierarchy of the IFC data, the Graph-RAG
parsing enhances generative LLMs like GPT-4o with graph-based knowledge,
enabling natural language query-response retrieval without the need for a
complex pipeline.

</details>


### [29] [GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning](https://arxiv.org/abs/2504.16832)
*Luu Quy Tung,Hoang Quoc Viet,Vo Trong Thu*

Main category: cs.CL

TL;DR: 论文提出了一种基于Group Relative Policy Optimization的越南语推理模型GreenMind-Medium-14B-R1，通过高质量数据集和奖励函数解决了语言混合和事实正确性问题，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决Chain-of-Thought（CoT）方法在越南语任务中的语言混合和事实正确性问题。

Method: 采用Group Relative Policy Optimization微调策略，设计两个奖励函数：检测语言混合和确保事实正确性。

Result: 在VLSP 2023越南数据集和SeaExam多语言数据集上表现优于现有方法。

Conclusion: GreenMind-Medium-14B-R1在越南语推理任务中表现出色，解决了CoT的主要限制。

Abstract: Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that
require intermediate reasoning steps prior to generating a final answer. In
this paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model
inspired by the finetuning strategy based on Group Relative Policy
Optimization. We also leverage a high-quality Vietnamese synthesized reasoning
dataset and design two reward functions to tackle the main limitations of this
technique: (i) language mixing, where we explicitly detect the presence of
biased language characters during the process of sampling tokens, and (ii) we
leverage Sentence Transformer-based models to ensure that the generated
reasoning content maintains factual correctness and does not distort the final
output. Experimental results on the Vietnamese dataset from the VLSP 2023
Challenge demonstrate that our model outperforms prior works and enhances
linguistic consistency in its responses. Furthermore, we extend our evaluation
to SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of
our reasoning method compared to few-shot prompting techniques.

</details>


### [30] [Monte Carlo Planning with Large Language Model for Text-Based Game Agents](https://arxiv.org/abs/2504.16855)
*Zijing Shi,Meng Fang,Ling Chen*

Main category: cs.CL

TL;DR: MC-DML算法结合大型语言模型（LLM）和树搜索算法，提升文本游戏中的语言理解和推理能力，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于规划和学习的算法（如MCTS+RL）耗时长且缺乏语言理解能力，需要更高效的解决方案。

Method: 提出MC-DML算法，利用LLM的语言能力与动态记忆机制，结合树搜索进行规划和探索。

Result: 在Jericho基准测试中，MC-DML在初始规划阶段表现优异，优于需多次迭代的现有方法。

Conclusion: MC-DML为复杂环境中的语言规划提供了高效解决方案。

Abstract: Text-based games provide valuable environments for language-based autonomous
agents. However, planning-then-learning paradigms, such as those combining
Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably
time-consuming due to extensive iterations. Additionally, these algorithms
perform uncertainty-driven exploration but lack language understanding and
reasoning abilities. In this paper, we introduce the Monte Carlo planning with
Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages
the language understanding and reasoning capabilities of Large Language Models
(LLMs) alongside the exploratory advantages of tree search algorithms.
Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms,
enabling them to learn from past experiences and dynamically adjust action
evaluations during planning. We conduct experiments on a series of text-based
games from the Jericho benchmark. Our results demonstrate that the MC-DML
algorithm significantly enhances performance across various games at the
initial planning phase, outperforming strong contemporary methods that require
multiple iterations. This demonstrates the effectiveness of our algorithm,
paving the way for more efficient language-grounded planning in complex
environments.

</details>


### [31] [Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification](https://arxiv.org/abs/2504.16856)
*Alexander Shvets*

Main category: cs.CL

TL;DR: 论文提出了一种基于LLM的数据合成方法，用于生成多样化的情感分析训练数据，并通过轻量级BERT模型实现高效情感分类，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析数据集缺乏上下文且情感类别有限，而大型语言模型（如GPT-4）资源消耗高且容易过度预测情感。

Method: 设计了基于LLM的数据合成流程，利用Mistral-7b生成多样化训练数据，并通过BERT模型微调实现高效分类。

Result: 生成了100K带上下文和300K无上下文的示例，微调后的Emo Pillars模型在多个任务上达到SOTA性能。

Conclusion: 方法成功提升了数据多样性和上下文个性化，但需改进对非分类标签的处理。

Abstract: Most datasets for sentiment analysis lack context in which an opinion was
expressed, often crucial for emotion understanding, and are mainly limited by a
few emotion categories. Foundation large language models (LLMs) like GPT-4
suffer from over-predicting emotions and are too resource-intensive. We design
an LLM-based data synthesis pipeline and leverage a large model, Mistral-7b,
for the generation of training examples for more accessible, lightweight
BERT-type encoder models. We focus on enlarging the semantic diversity of
examples and propose grounding the generation into a corpus of narratives to
produce non-repetitive story-character-centered utterances with unique contexts
over 28 emotion classes. By running 700K inferences in 450 GPU hours, we
contribute with the dataset of 100K contextual and also 300K context-less
examples to cover both scenarios. We use it for fine-tuning pre-trained
encoders, which results in several Emo Pillars models. We show that Emo Pillars
models are highly adaptive to new domains when tuned to specific tasks such as
GoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on
the first three. We also validate our dataset, conducting statistical analysis
and human evaluation, and confirm the success of our measures in utterance
diversification (although less for the neutral class) and context
personalization, while pointing out the need for improved handling of
out-of-taxonomy labels within the pipeline.

</details>


### [32] [Planning with Diffusion Models for Target-Oriented Dialogue Systems](https://arxiv.org/abs/2504.16858)
*Hanwen Du,Bo Peng,Xia Ning*

Main category: cs.CL

TL;DR: DiffTOD是一个基于扩散模型的新型对话规划框架，用于解决目标导向对话中的非顺序规划问题，通过条件引导优化对话策略。


<details>
  <summary>Details</summary>
Motivation: 现有对话规划方法存在顺序生成导致的错误累积和短视行为，DiffTOD旨在通过非顺序规划解决这些问题。

Method: DiffTOD将对话规划建模为条件引导的轨迹生成问题，利用扩散语言模型估计对话轨迹的似然，并针对不同目标类型设计了三种引导机制。

Result: 实验表明，DiffTOD能有效进行非短视的前瞻探索，并在复杂多样的对话场景中表现出强大的灵活性。

Conclusion: DiffTOD通过非顺序对话规划优化长期动作策略，为目标导向对话提供了有效的解决方案。

Abstract: Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM
era, where strategic dialogue planning is crucial for directing conversations
toward specific targets. However, existing dialogue planning methods generate
dialogue plans in a step-by-step sequential manner, and may suffer from
compounding errors and myopic actions. To address these limitations, we
introduce a novel dialogue planning framework, DiffTOD, which leverages
diffusion models to enable non-sequential dialogue planning. DiffTOD formulates
dialogue planning as a trajectory generation problem with conditional guidance,
and leverages a diffusion language model to estimate the likelihood of the
dialogue trajectory. To optimize the dialogue action strategies, DiffTOD
introduces three tailored guidance mechanisms for different target types,
offering flexible guidance towards diverse TOD targets at test time. Extensive
experiments across three diverse TOD settings show that DiffTOD can effectively
perform non-myopic lookahead exploration and optimize action strategies over a
long horizon through non-sequential dialogue planning, and demonstrates strong
flexibility across complex and diverse dialogue scenarios. Our code and data
are accessible through https://anonymous.4open.science/r/DiffTOD.

</details>


### [33] [Do Large Language Models know who did what to whom?](https://arxiv.org/abs/2504.16884)
*Joseph M. Denning,Xiaohan,Guo,Bryor Snefjella,Idan A. Blank*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）是否能通过词预测任务学习句子中的主题角色（如施事和受事），发现其表征更依赖句法而非主题角色，但部分注意力头能独立捕获主题角色。


<details>
  <summary>Details</summary>
Motivation: 针对LLMs是否真正理解语言的质疑，研究聚焦于与语言紧密相关的主题角色理解，而非人类认知能力。

Method: 通过两个实验，分析了四种LLMs的句子表征，比较其与人类相似性判断的差异。

Result: LLMs的表征更依赖句法相似性，而非主题角色分配；部分注意力头能独立捕获主题角色。

Conclusion: LLMs能提取主题角色，但其表征中该信息的影响较弱，与人类不同。

Abstract: Large Language Models (LLMs) are commonly criticized for not understanding
language. However, many critiques focus on cognitive abilities that, in humans,
are distinct from language processing. Here, we instead study a kind of
understanding tightly linked to language: inferring who did what to whom
(thematic roles) in a sentence. Does the central training objective of
LLMs-word prediction-result in sentence representations that capture thematic
roles? In two experiments, we characterized sentence representations in four
LLMs. In contrast to human similarity judgments, in LLMs the overall
representational similarity of sentence pairs reflected syntactic similarity
but not whether their agent and patient assignments were identical vs.
reversed. Furthermore, we found little evidence that thematic role information
was available in any subset of hidden units. However, some attention heads
robustly captured thematic roles, independently of syntax. Therefore, LLMs can
extract thematic roles but, relative to humans, this information influences
their representations more weakly.

</details>


### [34] [Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text](https://arxiv.org/abs/2504.16913)
*Shifali Agrahari,Sanasam Ranbir Singh*

Main category: cs.CL

TL;DR: 提出了一种名为COT Fine-tuned的新框架，用于检测AI生成文本并识别生成文本的特定语言模型，通过双任务设计和Chain-of-Thought推理提高透明度和准确性。


<details>
  <summary>Details</summary>
Motivation: 近年来，AI生成文本的检测因涉及学术诚信、错误信息和伦理AI部署等问题变得至关重要。

Method: 采用双任务方法：任务A区分AI生成与人类撰写文本，任务B识别具体语言模型；利用Chain-of-Thought推理增强模型解释能力。

Result: 实验表明，COT Fine-tuned在两项任务中均表现优异，尤其在语言模型识别和人类-AI分类方面；Chain-of-Thought推理显著提升模型效果和可解释性。

Conclusion: COT Fine-tuned框架在检测AI生成文本和识别语言模型方面高效且透明，Chain-of-Thought推理是关键创新。

Abstract: In recent years, the detection of AI-generated text has become a critical
area of research due to concerns about academic integrity, misinformation, and
ethical AI deployment. This paper presents COT Fine-tuned, a novel framework
for detecting AI-generated text and identifying the specific language model.
responsible for generating the text. We propose a dual-task approach, where
Task A involves classifying text as AI-generated or human-written, and Task B
identifies the specific LLM behind the text. The key innovation of our method
lies in the use of Chain-of-Thought reasoning, which enables the model to
generate explanations for its predictions, enhancing transparency and
interpretability. Our experiments demonstrate that COT Fine-tuned achieves high
accuracy in both tasks, with strong performance in LLM identification and
human-AI classification. We also show that the CoT reasoning process
contributes significantly to the models effectiveness and interpretability.

</details>


### [35] [OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents](https://arxiv.org/abs/2504.16918)
*Raghav Thind,Youran Sun,Ling Liang,Haizhao Yang*

Main category: cs.CL

TL;DR: OptimAI是一个利用LLM驱动的AI代理解决自然语言描述的优化问题的框架，通过四个关键角色（formulator、planner、coder、code critic）实现多智能体协作，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决将自然语言描述的优化问题转化为数学形式并选择合适求解器的挑战，减少对领域专业知识的依赖。

Method: 框架包含四个角色：formulator（问题描述转数学形式）、planner（制定高层策略）、coder和code critic（交互与反思）。采用UCB-based debug调度动态切换计划。

Result: 在NLP4LP数据集上达到88.1%准确率，Optibench子集上71.2%，错误率分别降低58%和50%。

Conclusion: OptimAI通过多智能体协作显著提升优化问题的解决效率，验证了各角色的必要性。

Abstract: Optimization plays a vital role in scientific research and practical
applications, but formulating a concrete optimization problem described in
natural language into a mathematical form and selecting a suitable solver to
solve the problem requires substantial domain expertise. We introduce
\textbf{OptimAI}, a framework for solving \underline{Optim}ization problems
described in natural language by leveraging LLM-powered \underline{AI} agents,
achieving superior performance over current state-of-the-art methods. Our
framework is built upon four key roles: (1) a \emph{formulator} that translates
natural language problem descriptions into precise mathematical formulations;
(2) a \emph{planner} that constructs a high-level solution strategy prior to
execution; and (3) a \emph{coder} and a \emph{code critic} capable of
interacting with the environment and reflecting on outcomes to refine future
actions. Ablation studies confirm that all roles are essential; removing the
planner or code critic results in $5.8\times$ and $3.1\times$ drops in
productivity, respectively. Furthermore, we introduce UCB-based debug
scheduling to dynamically switch between alternative plans, yielding an
additional $3.3\times$ productivity gain. Our design emphasizes multi-agent
collaboration, allowing us to conveniently explore the synergistic effect of
combining diverse models within a unified system. Our approach attains 88.1\%
accuracy on the NLP4LP dataset and 71.2\% on the Optibench (non-linear w/o
table) subset, reducing error rates by 58\% and 50\% respectively over prior
best results.

</details>


### [36] [IberBench: LLM Evaluation on Iberian Languages](https://arxiv.org/abs/2504.16921)
*José Ángel González,Ian Borrego Obrador,Álvaro Romo Herrero,Areg Mikael Sarvazyan,Mara Chinea-Ríos,Angelo Basile,Marc Franco-Salvador*

Main category: cs.CL

TL;DR: IberBench是一个全面且可扩展的基准测试，用于评估大型语言模型（LLMs）在伊比利亚半岛和拉丁美洲语言中的表现，覆盖22个任务类别，并解决了现有基准测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要针对英语，缺乏对其他语言的多样性评估，且忽视了工业相关任务的需求。

Method: IberBench整合了101个数据集，支持持续更新和社区驱动的提交，评估了23个不同规模的LLMs。

Result: 研究发现LLMs在工业相关任务上表现较差，对某些语言（如加利西亚语和巴斯克语）表现更低，部分任务结果接近随机。

Conclusion: IberBench提供了开源实现和公开排行榜，为LLMs的多语言评估提供了新工具。

Abstract: Large Language Models (LLMs) remain difficult to evaluate comprehensively,
particularly for languages other than English, where high-quality data is often
limited. Existing benchmarks and leaderboards are predominantly
English-centric, with only a few addressing other languages. These benchmarks
fall short in several key areas: they overlook the diversity of language
varieties, prioritize fundamental Natural Language Processing (NLP)
capabilities over tasks of industrial relevance, and are static. With these
aspects in mind, we present IberBench, a comprehensive and extensible benchmark
designed to assess LLM performance on both fundamental and industry-relevant
NLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.
IberBench integrates 101 datasets from evaluation campaigns and recent
benchmarks, covering 22 task categories such as sentiment and emotion analysis,
toxicity detection, and summarization. The benchmark addresses key limitations
in current evaluation practices, such as the lack of linguistic diversity and
static evaluation setups by enabling continual updates and community-driven
model and dataset submissions moderated by a committee of experts. We evaluate
23 LLMs ranging from 100 million to 14 billion parameters and provide empirical
insights into their strengths and limitations. Our findings indicate that (i)
LLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)
performance is on average lower for Galician and Basque, (iii) some tasks show
results close to random, and (iv) in other tasks LLMs perform above random but
below shared task systems. IberBench offers open-source implementations for the
entire evaluation pipeline, including dataset normalization and hosting,
incremental evaluation of LLMs, and a publicly accessible leaderboard.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [37] [Audio and Multiscale Visual Cues Driven Cross-modal Transformer for Idling Vehicle Detection](https://arxiv.org/abs/2504.16102)
*Xiwen Li,Ross Whitaker,Tolga Tasdizen*

Main category: cs.CV

TL;DR: AVIVDNetv2是一种基于Transformer的端到端检测网络，通过跨模态Transformer和多尺度视觉特征融合模块，显著提高了车辆怠速检测的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有端到端模型在音频和视觉模态对齐上的不足，提升车辆怠速检测的准确性和鲁棒性。

Method: 提出AVIVDNetv2，包含跨模态Transformer、多尺度视觉特征融合模块和解耦检测头。

Result: 在AVIVD数据集上，mAP提升7.66（相比基线）和9.42（相比端到端基线），并在所有车辆类别中表现一致优异。

Conclusion: AVIVDNetv2在车辆怠速检测任务中表现卓越，为跨模态任务提供了新的性能基准。

Abstract: Idling vehicle detection (IVD) supports real-time systems that reduce
pollution and emissions by dynamically messaging drivers to curb excess idling
behavior. In computer vision, IVD has become an emerging task that leverages
video from surveillance cameras and audio from remote microphones to localize
and classify vehicles in each frame as moving, idling, or engine-off. As with
other cross-modal tasks, the key challenge lies in modeling the correspondence
between audio and visual modalities, which differ in representation but provide
complementary cues -- video offers spatial and motion context, while audio
conveys engine activity beyond the visual field. The previous end-to-end model,
which uses a basic attention mechanism, struggles to align these modalities
effectively, often missing vehicle detections. To address this issue, we
propose AVIVDNetv2, a transformer-based end-to-end detection network. It
incorporates a cross-modal transformer with global patch-level learning, a
multiscale visual feature fusion module, and decoupled detection heads.
Extensive experiments show that AVIVDNetv2 improves mAP by 7.66 over the
disjoint baseline and 9.42 over the E2E baseline, with consistent AP gains
across all vehicle categories. Furthermore, AVIVDNetv2 outperforms the
state-of-the-art method for sounding object localization, establishing a new
performance benchmark on the AVIVD dataset.

</details>


### [38] [Shape Your Ground: Refining Road Surfaces Beyond Planar Representations](https://arxiv.org/abs/2504.16103)
*Oussema Dhaouadi,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: FlexRoad框架通过NURBS曲面拟合3D道路点，结合ECSRC算法减少粗糙度和误差，显著提升道路表面重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有道路重建方法存在粗糙和不一致问题，影响下游任务精度，需一种更准确的方法。

Method: 使用NURBS曲面拟合3D道路点，结合ECSRC算法进行异常校正。

Result: FlexRoad在多个数据集上表现优于现有方法，对输入源和噪声不敏感。

Conclusion: FlexRoad是一种通用的高质量道路表面建模方法。

Abstract: Road surface reconstruction from aerial images is fundamental for autonomous
driving, urban planning, and virtual simulation, where smoothness, compactness,
and accuracy are critical quality factors. Existing reconstruction methods
often produce artifacts and inconsistencies that limit usability, while
downstream tasks have a tendency to represent roads as planes for simplicity
but at the cost of accuracy. We introduce FlexRoad, the first framework to
directly address road surface smoothing by fitting Non-Uniform Rational
B-Splines (NURBS) surfaces to 3D road points obtained from photogrammetric
reconstructions or geodata providers. Our method at its core utilizes the
Elevation-Constrained Spatial Road Clustering (ECSRC) algorithm for robust
anomaly correction, significantly reducing surface roughness and fitting
errors. To facilitate quantitative comparison between road surface
reconstruction methods, we present GeoRoad Dataset (GeRoD), a diverse
collection of road surface and terrain profiles derived from openly accessible
geodata. Experiments on GeRoD and the photogrammetry-based DeepScenario Open 3D
Dataset (DSC3D) demonstrate that FlexRoad considerably surpasses commonly used
road surface representations across various metrics while being insensitive to
various input sources, terrains, and noise types. By performing ablation
studies, we identify the key role of each component towards high-quality
reconstruction performance, making FlexRoad a generic method for realistic road
surface modeling.

</details>


### [39] [Persistence-based Hough Transform for Line Detection](https://arxiv.org/abs/2504.16114)
*Johannes Ferner,Stefan Huber,Saverio Messineo,Angel Pop,Martin Uray*

Main category: cs.CV

TL;DR: 提出了一种基于持久同调的新投票技术，用于霍夫空间中的峰值检测，显著优于传统阈值方法，并增强了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统霍夫变换的投票过程通过阈值处理，易受噪声和伪影影响，需要更稳健的方法。

Method: 利用持久同调技术替代传统阈值投票，检测霍夫空间中的峰值。

Result: 在合成数据上的实验表明，新方法显著优于原始方法，且鲁棒性更强。

Conclusion: 呼吁将拓扑数据分析技术更广泛地整合到现有方法中，并探讨霍夫变换的数学稳定性以提升其鲁棒性。

Abstract: The Hough transform is a popular and classical technique in computer vision
for the detection of lines (or more general objects). It maps a pixel into a
dual space -- the Hough space: each pixel is mapped to the set of lines through
this pixel, which forms a curve in Hough space. The detection of lines then
becomes a voting process to find those lines that received many votes by
pixels. However, this voting is done by thresholding, which is susceptible to
noise and other artifacts.
  In this work, we present an alternative voting technique to detect peaks in
the Hough space based on persistent homology, which very naturally addresses
limitations of simple thresholding. Experiments on synthetic data show that our
method significantly outperforms the original method, while also demonstrating
enhanced robustness.
  This work seeks to inspire future research in two key directions. First, we
highlight the untapped potential of Topological Data Analysis techniques and
advocate for their broader integration into existing methods, including
well-established ones. Secondly, we initiate a discussion on the mathematical
stability of the Hough transform, encouraging exploration of mathematically
grounded improvements to enhance its robustness.

</details>


### [40] [Context-Awareness and Interpretability of Rare Occurrences for Discovery and Formalization of Critical Failure Modes](https://arxiv.org/abs/2504.16117)
*Sridevi Polavaram,Xin Zhou,Meenu Ravi,Mohammad Zarei,Anmol Srivastava*

Main category: cs.CV

TL;DR: CAIRO是一个基于本体的人类辅助框架，用于检测和形式化AI模型中的罕见故障案例，特别关注自动驾驶系统中的对象检测失败。


<details>
  <summary>Details</summary>
Motivation: 视觉系统在关键领域（如监控和自动驾驶）中的脆弱性，尤其是在罕见或意外场景下，存在重大安全风险。

Method: CAIRO通过人类参与循环（human-in-the-loop）的方式，结合本体论和知识图谱（OWL/XML格式），形式化检测到的故障案例。

Result: 在自动驾驶系统中，CAIRO成功实现了对对象检测模型故障的可扩展和可解释的形式化，并生成了可共享和逻辑推理的知识图谱。

Conclusion: CAIRO为AI模型的故障检测和形式化提供了一种可扩展且可解释的方法，有助于提高系统安全性和问责性。

Abstract: Vision systems are increasingly deployed in critical domains such as
surveillance, law enforcement, and transportation. However, their
vulnerabilities to rare or unforeseen scenarios pose significant safety risks.
To address these challenges, we introduce Context-Awareness and
Interpretability of Rare Occurrences (CAIRO), an ontology-based human-assistive
discovery framework for failure cases (or CP - Critical Phenomena) detection
and formalization. CAIRO by design incentivizes human-in-the-loop for testing
and evaluation of criticality that arises from misdetections, adversarial
attacks, and hallucinations in AI black-box models. Our robust analysis of
object detection model(s) failures in automated driving systems (ADS) showcases
scalable and interpretable ways of formalizing the observed gaps between camera
perception and real-world contexts, resulting in test cases stored as explicit
knowledge graphs (in OWL/XML format) amenable for sharing, downstream analysis,
logical reasoning, and accountability.

</details>


### [41] [MonoTher-Depth: Enhancing Thermal Depth Estimation via Confidence-Aware Distillation](https://arxiv.org/abs/2504.16127)
*Xingxing Zuo,Nikhil Ranganathan,Connor Lee,Georgia Gkioxari,Soon-Jo Chung*

Main category: cs.CV

TL;DR: 论文提出了一种通过知识蒸馏从RGB单目深度估计模型增强热图像单目深度估计的新方法，显著提高了精度。


<details>
  <summary>Details</summary>
Motivation: 热图像的单目深度估计在恶劣条件下（如雾、烟、低光）对机器人系统至关重要，但缺乏标记数据限制了其泛化能力。

Method: 采用置信感知蒸馏方法，利用RGB模型的预测置信度选择性增强热图像模型。

Result: 在无标记深度的新场景中，该方法将绝对相对误差降低了22.88%。

Conclusion: 该方法显著提升了热图像深度估计的精度和适用性，无需依赖标记数据。

Abstract: Monocular depth estimation (MDE) from thermal images is a crucial technology
for robotic systems operating in challenging conditions such as fog, smoke, and
low light. The limited availability of labeled thermal data constrains the
generalization capabilities of thermal MDE models compared to foundational RGB
MDE models, which benefit from datasets of millions of images across diverse
scenarios. To address this challenge, we introduce a novel pipeline that
enhances thermal MDE through knowledge distillation from a versatile RGB MDE
model. Our approach features a confidence-aware distillation method that
utilizes the predicted confidence of the RGB MDE to selectively strengthen the
thermal MDE model, capitalizing on the strengths of the RGB model while
mitigating its weaknesses. Our method significantly improves the accuracy of
the thermal MDE, independent of the availability of labeled depth supervision,
and greatly expands its applicability to new scenarios. In our experiments on
new scenarios without labeled depth, the proposed confidence-aware distillation
method reduces the absolute relative error of thermal MDE by 22.88\% compared
to the baseline without distillation.

</details>


### [42] [Hybrid Knowledge Transfer through Attention and Logit Distillation for On-Device Vision Systems in Agricultural IoT](https://arxiv.org/abs/2504.16128)
*Stanley Mugisha,Rashid Kisitu,Florence Tushabe*

Main category: cs.CV

TL;DR: 提出了一种混合知识蒸馏框架，将Swin Transformer的高精度与MobileNetV3的高效性结合，解决了农业物联网中实时病害检测的挑战。


<details>
  <summary>Details</summary>
Motivation: 农业物联网系统需要高精度的病害检测，但现有ViT模型计算复杂，而轻量模型精度不足。

Method: 采用混合知识蒸馏框架，通过自适应注意力对齐和双损失函数，将Swin Transformer的知识迁移到MobileNetV3。

Result: 蒸馏后的MobileNetV3在PlantVillage-Tomato数据集上达到92.4%的准确率，计算量减少95%，推理延迟降低82%。

Conclusion: 该框架成功实现了在边缘设备上达到ViT级别的诊断精度，推动了实时高效农业监测的发展。

Abstract: Integrating deep learning applications into agricultural IoT systems faces a
serious challenge of balancing the high accuracy of Vision Transformers (ViTs)
with the efficiency demands of resource-constrained edge devices. Large
transformer models like the Swin Transformers excel in plant disease
classification by capturing global-local dependencies. However, their
computational complexity (34.1 GFLOPs) limits applications and renders them
impractical for real-time on-device inference. Lightweight models such as
MobileNetV3 and TinyML would be suitable for on-device inference but lack the
required spatial reasoning for fine-grained disease detection. To bridge this
gap, we propose a hybrid knowledge distillation framework that synergistically
transfers logit and attention knowledge from a Swin Transformer teacher to a
MobileNetV3 student model. Our method includes the introduction of adaptive
attention alignment to resolve cross-architecture mismatch (resolution,
channels) and a dual-loss function optimizing both class probabilities and
spatial focus. On the lantVillage-Tomato dataset (18,160 images), the distilled
MobileNetV3 attains 92.4% accuracy relative to 95.9% for Swin-L but at an 95%
reduction on PC and < 82% in inference latency on IoT devices. (23ms on PC CPU
and 86ms/image on smartphone CPUs). Key innovations include IoT-centric
validation metrics (13 MB memory, 0.22 GFLOPs) and dynamic resolution-matching
attention maps. Comparative experiments show significant improvements over
standalone CNNs and prior distillation methods, with a 3.5% accuracy gain over
MobileNetV3 baselines. Significantly, this work advances real-time,
energy-efficient crop monitoring in precision agriculture and demonstrates how
we can attain ViT-level diagnostic precision on edge devices. Code and models
will be made available for replication after acceptance.

</details>


### [43] [Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends](https://arxiv.org/abs/2504.16134)
*Mohammad Abu Tami,Mohammed Elhenawy,Huthaifa I. Ashqar*

Main category: cs.CV

TL;DR: 本文探讨了多模态大型语言模型（MLLMs）在提升交通安全性中的潜力，通过整合多模态数据实现全面场景理解，并分析了其感知、决策和对抗鲁棒性能力。


<details>
  <summary>Details</summary>
Motivation: 传统高级驾驶辅助系统（ADAS）在动态现实场景中表现不佳，亟需更先进的解决方案。

Method: 通过综述MLLM方法，整合视觉、空间和环境数据，实现场景理解。

Result: MLLMs能显著提升感知、决策和对抗鲁棒性，并利用关键数据集推动研究。

Conclusion: MLLMs有望成为下一代交通安全系统的核心，提供可扩展、情境感知的解决方案，改善道路安全。

Abstract: Traffic safety remains a critical global challenge, with traditional Advanced
Driver-Assistance Systems (ADAS) often struggling in dynamic real-world
scenarios due to fragmented sensor processing and susceptibility to adversarial
conditions. This paper reviews the transformative potential of Multimodal Large
Language Models (MLLMs) in addressing these limitations by integrating
cross-modal data such as visual, spatial, and environmental inputs to enable
holistic scene understanding. Through a comprehensive analysis of MLLM-based
approaches, we highlight their capabilities in enhancing perception,
decision-making, and adversarial robustness, while also examining the role of
key datasets (e.g., KITTI, DRAMA, ML4RoadSafety) in advancing research.
Furthermore, we outline future directions, including real-time edge deployment,
causality-driven reasoning, and human-AI collaboration. By positioning MLLMs as
a cornerstone for next-generation traffic safety systems, this review
underscores their potential to revolutionize the field, offering scalable,
context-aware solutions that proactively mitigate risks and improve overall
road safety.

</details>


### [44] [Progressive Language-guided Visual Learning for Multi-Task Visual Grounding](https://arxiv.org/abs/2504.16145)
*Jingchao Wang,Hong Wang,Wenlong Zhang,Kunhua Ji,Dingjiang Huang,Yefeng Zheng*

Main category: cs.CV

TL;DR: PLVL框架通过渐进式语言引导视觉学习，解决了多任务视觉定位中语言信息未充分利用和子任务协作不足的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在语言信息注入和子任务协作方面存在不足，PLVL旨在通过语言引导和协作预测改进多任务视觉定位。

Method: 提出PLVL框架，渐进式注入语言信息到视觉特征中，并设计多任务头实现REC和RES的协作预测。

Result: PLVL在多个基准数据集上显著优于现有方法，验证了其有效性。

Conclusion: PLVL通过语言引导和协作预测，成功提升了多任务视觉定位的性能。

Abstract: Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring
Expression Comprehension (REC) and Referring Expression Segmentation (RES). The
existing representative approaches generally follow the research pipeline which
mainly consists of three core procedures, including independent feature
extraction for visual and linguistic modalities, respectively, cross-modal
interaction module, and independent prediction heads for different sub-tasks.
Albeit achieving remarkable performance, this research line has two
limitations: 1) The linguistic content has not been fully injected into the
entire visual backbone for boosting more effective visual feature extraction
and it needs an extra cross-modal interaction module; 2) The relationship
between REC and RES tasks is not effectively exploited to help the
collaborative prediction for more accurate output. To deal with these problems,
in this paper, we propose a Progressive Language-guided Visual Learning
framework for multi-task visual grounding, called PLVL, which not only finely
mine the inherent feature expression of the visual modality itself but also
progressively inject the language information to help learn linguistic-related
visual features. In this manner, our PLVL does not need additional cross-modal
fusion module while fully introducing the language guidance. Furthermore, we
analyze that the localization center for REC would help identify the
to-be-segmented object region for RES to some extent. Inspired by this
investigation, we design a multi-task head to accomplish collaborative
predictions for these two sub-tasks. Extensive experiments conducted on several
benchmark datasets comprehensively substantiate that our PLVL obviously
outperforms the representative methods in both REC and RES tasks.
https://github.com/jcwang0602/PLVL

</details>


### [45] [Classification of Firn Data via Topological Features](https://arxiv.org/abs/2504.16150)
*Sarah Day,Jesse Dimino,Matt Jester,Kaitlin Keegan,Thomas Weighill*

Main category: cs.CV

TL;DR: 论文评估了拓扑特征在分类冰川雪层图像数据中的性能，探讨了拓扑特征的优缺点及权衡。


<details>
  <summary>Details</summary>
Motivation: 研究目标是理解拓扑特征在分析冰川雪层结构中的优势和局限性，以及不同方法之间的权衡。

Method: 使用两类拓扑特征（子水平集特征和距离变换特征）及持久性曲线，通过微CT图像预测样本深度。

Result: 实验表明，没有一种方法在所有场景中表现最优，揭示了准确性、可解释性和泛化性之间的复杂权衡。

Conclusion: 拓扑特征在分析冰川雪层结构时具有潜力，但需根据具体需求选择方法，权衡不同性能指标。

Abstract: In this paper we evaluate the performance of topological features for
generalizable and robust classification of firn image data, with the broader
goal of understanding the advantages, pitfalls, and trade-offs in topological
featurization. Firn refers to layers of granular snow within glaciers that
haven't been compressed into ice. This compactification process imposes
distinct topological and geometric structure on firn that varies with depth
within the firn column, making topological data analysis (TDA) a natural choice
for understanding the connection between depth and structure. We use two
classes of topological features, sublevel set features and distance transform
features, together with persistence curves, to predict sample depth from
microCT images. A range of challenging training-test scenarios reveals that no
one choice of method dominates in all categories, and uncoveres a web of
trade-offs between accuracy, interpretability, and generalizability.

</details>


### [46] [A detection-task-specific deep-learning method to improve the quality of sparse-view myocardial perfusion SPECT images](https://arxiv.org/abs/2504.16171)
*Zezhang Yang,Zitong Yu,Nuri Choi,Abhinav K. Jha*

Main category: cs.CV

TL;DR: 提出一种基于深度学习的稀疏视角心肌灌注成像方法，显著提升灌注缺陷检测性能并减少扫描时间。


<details>
  <summary>Details</summary>
Motivation: 传统SPECT成像扫描时间长，易导致患者不适和运动伪影，减少投影角度虽缩短时间但影响图像质量。

Method: 采用检测任务特定的深度学习方法，结合观察者损失项以优化灌注缺陷检测性能。

Result: 在检测心肌灌注缺陷任务中，AUC显著高于稀疏视角协议，并能恢复左心室壁结构。

Conclusion: 初步结果验证了方法的有效性，值得进一步评估。

Abstract: Myocardial perfusion imaging (MPI) with single-photon emission computed
tomography (SPECT) is a widely used and cost-effective diagnostic tool for
coronary artery disease. However, the lengthy scanning time in this imaging
procedure can cause patient discomfort, motion artifacts, and potentially
inaccurate diagnoses due to misalignment between the SPECT scans and the
CT-scans which are acquired for attenuation compensation. Reducing projection
angles is a potential way to shorten scanning time, but this can adversely
impact the quality of the reconstructed images. To address this issue, we
propose a detection-task-specific deep-learning method for sparse-view MPI
SPECT images. This method integrates an observer loss term that penalizes the
loss of anthropomorphic channel features with the goal of improving performance
in perfusion defect-detection task. We observed that, on the task of detecting
myocardial perfusion defects, the proposed method yielded an area under the
receiver operating characteristic (ROC) curve (AUC) significantly larger than
the sparse-view protocol. Further, the proposed method was observed to be able
to restore the structure of the left ventricle wall, demonstrating ability to
overcome sparse-sampling artifacts. Our preliminary results motivate further
evaluations of the method.

</details>


### [47] [CLIP-IT: CLIP-based Pairing for Histology Images Classification](https://arxiv.org/abs/2504.16181)
*Banafsheh Karimian,Giulia Avanzato,Soufian Belharbi,Luke McCaffrey,Mohammadhadi Shateri,Eric Granger*

Main category: cs.CV

TL;DR: CLIP-IT方法通过外部文本信息增强医学图像分类，无需手动配对数据，提升了单模态分类器的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态学习中需要大量配对数据的隐私和成本问题。

Method: 利用CLIP模型匹配图像与外部文本，通过知识蒸馏将文本信息融入图像分类器，采用参数高效微调方法。

Result: 在PCAM、CRC和BACH数据集上表现优于单模态分类器。

Conclusion: CLIP-IT提供了一种成本效益高的方法，利用外部文本信息提升医学图像分析性能。

Abstract: Multimodal learning has shown significant promise for improving medical image
analysis by integrating information from complementary data sources. This is
widely employed for training vision-language models (VLMs) for cancer detection
based on histology images and text reports. However, one of the main
limitations in training these VLMs is the requirement for large paired
datasets, raising concerns over privacy, and data collection, annotation, and
maintenance costs. To address this challenge, we introduce CLIP-IT method to
train a vision backbone model to classify histology images by pairing them with
privileged textual information from an external source. At first, the modality
pairing step relies on a CLIP-based model to match histology images with
semantically relevant textual report data from external sources, creating an
augmented multimodal dataset without the need for manually paired samples.
Then, we propose a multimodal training procedure that distills the knowledge
from the paired text modality to the unimodal image classifier for enhanced
performance without the need for the textual data during inference. A
parameter-efficient fine-tuning method is used to efficiently address the
misalignment between the main (image) and paired (text) modalities. During
inference, the improved unimodal histology classifier is used, with only
minimal additional computational complexity. Our experiments on challenging
PCAM, CRC, and BACH histology image datasets show that CLIP-IT can provide a
cost-effective approach to leverage privileged textual information and
outperform unimodal classifiers for histology.

</details>


### [48] [DeepCS-TRD, a Deep Learning-based Cross-Section Tree Ring Detector](https://arxiv.org/abs/2504.16242)
*Henry Marichal,Verónica Casaravilla,Candice Power,Karolain Mello,Joaquín Mazarino,Christine Lucas,Ludmila Profumo,Diego Passarella,Gregory Randall*

Main category: cs.CV

TL;DR: Deep CS-TRD是一种基于深度学习的自动算法，用于检测树木横截面的年轮，替代了传统边缘检测方法，适用于多种图像域和树种。


<details>
  <summary>Details</summary>
Motivation: 传统方法在不同图像域和树种上的适用性有限，需要一种更通用的自动检测方法。

Method: 采用U-Net深度学习模型替代传统边缘检测步骤，适用于显微镜、扫描仪或智能手机获取的图像，并测试了三种不同树种。

Result: 在宏观图像（Pinus taeda和Gleditsia triacanthos）上优于现有方法，但在显微镜图像（Salix glauca）上表现稍逊。

Conclusion: Deep CS-TRD是首个针对多种树种和采集条件进行自动年轮检测的研究，提供了公开数据集和源代码。

Abstract: Here, we propose Deep CS-TRD, a new automatic algorithm for detecting tree
rings in whole cross-sections. It substitutes the edge detection step of CS-TRD
by a deep-learning-based approach (U-Net), which allows the application of the
method to different image domains: microscopy, scanner or smartphone acquired,
and species (Pinus taeda, Gleditsia triachantos and Salix glauca).
Additionally, we introduce two publicly available datasets of annotated images
to the community. The proposed method outperforms state-of-the-art approaches
in macro images (Pinus taeda and Gleditsia triacanthos) while showing slightly
lower performance in microscopy images of Salix glauca. To our knowledge, this
is the first paper that studies automatic tree ring detection for such
different species and acquisition conditions. The dataset and source code are
available in https://github.com/hmarichal93/deepcstrd

</details>


### [49] [Naturally Computed Scale Invariance in the Residual Stream of ResNet18](https://arxiv.org/abs/2504.16290)
*André Longon*

Main category: cs.CV

TL;DR: 论文研究了ResNet18如何通过残差流实现尺度不变性，填补了InceptionV1未涵盖的架构研究空白。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络如何实现视觉对象识别中对图像变换（如尺度变化）的不变性，尤其是不同架构（如ResNet18）的机制。

Method: 分析ResNet18的残差流，观察中间块的卷积通道如何通过尺度等变表示的残差求和实现尺度不变性，并进行消融实验验证。

Result: 发现中间块的卷积通道具有尺度不变性，残差流通过尺度等变表示的求和实现这一特性，可能与尺度鲁棒的物体识别行为相关。

Conclusion: 残差流可能是ResNet18实现尺度不变性的关键机制，为理解神经网络的行为提供了新视角。

Abstract: An important capacity in visual object recognition is invariance to
image-altering variables which leave the identity of objects unchanged, such as
lighting, rotation, and scale. How do neural networks achieve this? Prior
mechanistic interpretability research has illuminated some invariance-building
circuitry in InceptionV1, but the results are limited and networks with
different architectures have remained largely unexplored. This work
investigates ResNet18 with a particular focus on its residual stream, an
architectural component which InceptionV1 lacks. We observe that many
convolutional channels in intermediate blocks exhibit scale invariant
properties, computed by the element-wise residual summation of scale
equivariant representations: the block input's smaller-scale copy with the
block pre-sum output's larger-scale copy. Through subsequent ablation
experiments, we attempt to causally link these neural properties with
scale-robust object recognition behavior. Our tentative findings suggest how
the residual stream computes scale invariance and its possible role in
behavior. Code is available at:
https://github.com/cest-andre/residual-stream-interp

</details>


### [50] [MetaHarm: Harmful YouTube Video Dataset Annotated by Domain Experts, GPT-4-Turbo, and Crowdworkers](https://arxiv.org/abs/2504.16304)
*Wonjeong Jo,Magdalena Wojcieszak*

Main category: cs.CV

TL;DR: 该研究提出了两个大规模数据集，用于测量和分类短视频平台上的有害内容，包括YouTube视频和标注数据，以支持未来对在线危害的研究和缓解。


<details>
  <summary>Details</summary>
Motivation: 短视频平台（如YouTube、Instagram或TikTok）存在大量有害内容，但目前缺乏全面的理解和测量方法。

Method: 研究构建了两个数据集：60,906个潜在有害的YouTube视频和19,422个由专家、GPT-4-Turbo和众包工人标注的视频，涵盖六类危害。

Result: 数据集提供了多模态和多类别的标注结果，包括二进制分类和六类危害的详细标注，为未来研究提供了基础。

Conclusion: 这些数据集将促进在线危害研究，支持多模态分类，并帮助识别和缓解视频平台上的有害内容。

Abstract: Short video platforms, such as YouTube, Instagram, or TikTok, are used by
billions of users. These platforms expose users to harmful content, ranging
from clickbait or physical harms to hate or misinformation. Yet, we lack a
comprehensive understanding and measurement of online harm on short video
platforms. Toward this end, we present two large-scale datasets of multi-modal
and multi-categorical online harm: (1) 60,906 systematically selected
potentially harmful YouTube videos and (2) 19,422 videos annotated by three
labeling actors: trained domain experts, GPT-4-Turbo (using 14 image frames, 1
thumbnail, and text metadata), and crowdworkers (Amazon Mechanical Turk master
workers). The annotated dataset includes both (a) binary classification
(harmful vs. harmless) and (b) multi-label categorizations of six harm
categories: Information, Hate and harassment, Addictive, Clickbait, Sexual, and
Physical harms. Furthermore, the annotated dataset provides (1) ground truth
data with videos annotated consistently across (a) all three actors and (b) the
majority of the labeling actors, and (2) three data subsets labeled by
individual actors. These datasets are expected to facilitate future work on
online harm, aid in (multi-modal) classification efforts, and advance the
identification and potential mitigation of harmful content on video platforms.

</details>


### [51] [SignX: The Foundation Model for Sign Recognition](https://arxiv.org/abs/2504.16315)
*Sen Fang,Chunyu Sui,Hongwei Yi,Carol Neidle,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: SignX是一个用于手语识别的框架，通过两阶段训练（Pose2Gloss和Video2Pose）实现高精度识别。


<details>
  <summary>Details</summary>
Motivation: 手语数据处理复杂，现有方法依赖不一致的ID glosses，需要统一框架。

Method: 提出SignX框架，包含基于逆扩散模型的Pose2Gloss和基于ViT的Video2Pose模块。

Result: 实验表明SignX识别精度优于现有方法。

Conclusion: SignX为手语识别提供了兼容现有姿态格式的基础模型。

Abstract: The complexity of sign language data processing brings many challenges. The
current approach to recognition of ASL signs aims to translate RGB sign
language videos through pose information into English-based ID glosses, which
serve to uniquely identify ASL signs. Note that there is no shared convention
for assigning such glosses to ASL signs, so it is essential that the same
glossing conventions are used for all of the data in the datasets that are
employed. This paper proposes SignX, a foundation model framework for sign
recognition. It is a concise yet powerful framework applicable to multiple
human activity recognition scenarios. First, we developed a Pose2Gloss
component based on an inverse diffusion model, which contains a multi-track
pose fusion layer that unifies five of the most powerful pose information
sources--SMPLer-X, DWPose, Mediapipe, PrimeDepth, and Sapiens
Segmentation--into a single latent pose representation. Second, we trained a
Video2Pose module based on ViT that can directly convert raw video into signer
pose representation. Through this 2-stage training framework, we enable sign
language recognition models to be compatible with existing pose formats, laying
the foundation for the common pose estimation necessary for sign recognition.
Experimental results show that SignX can recognize signs from sign language
video, producing predicted gloss representations with greater accuracy than has
been reported in prior work.

</details>


### [52] [Almost Right: Making First-layer Kernels Nearly Orthogonal Improves Model Generalization](https://arxiv.org/abs/2504.16362)
*Colton R. Crum,Adam Czajka*

Main category: cs.CV

TL;DR: 论文提出了一种新的损失组件，通过正则化第一卷积层的滤波核使其接近正交，从而提高模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 提高计算机视觉模型的泛化能力，受人类感知智能启发。

Method: 提出一种损失组件，灵活正则化滤波核的正交性，不修改网络架构。

Result: 在三种架构和两个开放集识别任务中，泛化性能显著提升。

Conclusion: 该方法优于现有正交化和显著性正则化方法，具有实际应用潜力。

Abstract: An ongoing research challenge within several domains in computer vision is
how to increase model generalization capabilities. Several attempts to improve
model generalization performance are heavily inspired by human perceptual
intelligence, which is remarkable in both its performance and efficiency to
generalize to unknown samples. Many of these methods attempt to force portions
of the network to be orthogonal, following some observation within neuroscience
related to early vision processes. In this paper, we propose a loss component
that regularizes the filtering kernels in the first convolutional layer of a
network to make them nearly orthogonal. Deviating from previous works, we give
the network flexibility in which pairs of kernels it makes orthogonal, allowing
the network to navigate to a better solution space, imposing harsh penalties.
Without architectural modifications, we report substantial gains in
generalization performance using the proposed loss against previous works
(including orthogonalization- and saliency-based regularization methods) across
three different architectures (ResNet-50, DenseNet-121, ViT-b-16) and two
difficult open-set recognition tasks: presentation attack detection in iris
biometrics, and anomaly detection in chest X-ray images.

</details>


### [53] [CLPSTNet: A Progressive Multi-Scale Convolutional Steganography Model Integrating Curriculum Learning](https://arxiv.org/abs/2504.16364)
*Fengchun Liu,Tong Zhang,Chunying Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种基于课程学习的渐进式隐写网络（CLPSTNet），通过多尺度卷积模块提升图像隐写的不可见性和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统隐写方法依赖手工特征和先验知识，而CNN在图像隐写中存在不可见性和安全性问题，因此需要改进。

Method: CLPSTNet采用渐进式多尺度卷积模块，结合Inception结构和空洞卷积，逐步提取从细到粗的特征。

Result: 实验显示CLPSTNet在多个数据集上表现优异，PSNR、SSIM和解码精度高，且隐写图像的低隐写分析得分。

Conclusion: CLPSTNet有效提升了图像隐写的不可见性和安全性，具有实际应用潜力。

Abstract: In recent years, a large number of works have introduced Convolutional Neural
Networks (CNNs) into image steganography, which transform traditional
steganography methods such as hand-crafted features and prior knowledge design
into steganography methods that neural networks autonomically learn information
embedding. However, due to the inherent complexity of digital images, issues of
invisibility and security persist when using CNN models for information
embedding. In this paper, we propose Curriculum Learning Progressive Steganophy
Network (CLPSTNet). The network consists of multiple progressive multi-scale
convolutional modules that integrate Inception structures and dilated
convolutions. The module contains multiple branching pathways, starting from a
smaller convolutional kernel and dilatation rate, extracting the basic, local
feature information from the feature map, and gradually expanding to the
convolution with a larger convolutional kernel and dilatation rate for
perceiving the feature information of a larger receptive field, so as to
realize the multi-scale feature extraction from shallow to deep, and from fine
to coarse, allowing the shallow secret information features to be refined in
different fusion stages. The experimental results show that the proposed
CLPSTNet not only has high PSNR , SSIM metrics and decoding accuracy on three
large public datasets, ALASKA2, VOC2012 and ImageNet, but also the
steganographic images generated by CLPSTNet have low steganalysis scores.You
can find our code at
\href{https://github.com/chaos-boops/CLPSTNet}{https://github.com/chaos-boops/CLPSTNet}.

</details>


### [54] [Revisiting Radar Camera Alignment by Contrastive Learning for 3D Object Detection](https://arxiv.org/abs/2504.16368)
*Linhua Kong,Dongxia Chang,Lian Liu,Zisen Kong,Pengyuan Li,Yao Zhao*

Main category: cs.CV

TL;DR: 提出了一种名为RCAlign的新对齐模型，通过双路径对齐模块和雷达特征增强模块，解决了雷达与相机特征对齐问题，并在nuScenes基准测试中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在雷达与相机特征对齐中存在特征交互不足或空间位置对齐不准确的问题，影响了3D目标检测的性能。

Method: 设计了基于对比学习的双路径对齐模块（DRA）和雷达特征增强模块（RFE），结合知识蒸馏损失提升雷达BEV特征的密度。

Result: 在nuScenes基准测试中，RCAlign实现了雷达相机融合3D目标检测的最新性能，显著优于现有方法（RCBEVDet）。

Conclusion: RCAlign通过改进特征对齐和增强雷达特征，显著提升了3D目标检测的性能，为自动驾驶感知任务提供了有效解决方案。

Abstract: Recently, 3D object detection algorithms based on radar and camera fusion
have shown excellent performance, setting the stage for their application in
autonomous driving perception tasks. Existing methods have focused on dealing
with feature misalignment caused by the domain gap between radar and camera.
However, existing methods either neglect inter-modal features interaction
during alignment or fail to effectively align features at the same spatial
location across modalities. To alleviate the above problems, we propose a new
alignment model called Radar Camera Alignment (RCAlign). Specifically, we
design a Dual-Route Alignment (DRA) module based on contrastive learning to
align and fuse the features between radar and camera. Moreover, considering the
sparsity of radar BEV features, a Radar Feature Enhancement (RFE) module is
proposed to improve the densification of radar BEV features with the knowledge
distillation loss. Experiments show RCAlign achieves a new state-of-the-art on
the public nuScenes benchmark in radar camera fusion for 3D Object Detection.
Furthermore, the RCAlign achieves a significant performance gain (4.3\% NDS and
8.4\% mAP) in real-time 3D detection compared to the latest state-of-the-art
method (RCBEVDet).

</details>


### [55] [SaENeRF: Suppressing Artifacts in Event-based Neural Radiance Fields](https://arxiv.org/abs/2504.16389)
*Yuanjian Wang,Yufei Deng,Rong Xiao,Jiahao Fan,Chenwei Tang,Deng Xiong,Jiancheng Lv*

Main category: cs.CV

TL;DR: SaENeRF是一种自监督框架，通过事件流实现高质量3D重建，减少噪声和伪影。


<details>
  <summary>Details</summary>
Motivation: 事件相机在高速场景中具有优势，但现有方法在3D重建中存在伪影和噪声问题。

Method: 利用事件极性归一化预测辐射变化，并引入正则化损失抑制伪影。

Result: 实验表明，SaENeRF显著减少伪影，重建质量优于现有方法。

Conclusion: SaENeRF为事件相机的3D重建提供了高效解决方案。

Abstract: Event cameras are neuromorphic vision sensors that asynchronously capture
changes in logarithmic brightness changes, offering significant advantages such
as low latency, low power consumption, low bandwidth, and high dynamic range.
While these characteristics make them ideal for high-speed scenarios,
reconstructing geometrically consistent and photometrically accurate 3D
representations from event data remains fundamentally challenging. Current
event-based Neural Radiance Fields (NeRF) methods partially address these
challenges but suffer from persistent artifacts caused by aggressive network
learning in early stages and the inherent noise of event cameras. To overcome
these limitations, we present SaENeRF, a novel self-supervised framework that
effectively suppresses artifacts and enables 3D-consistent, dense, and
photorealistic NeRF reconstruction of static scenes solely from event streams.
Our approach normalizes predicted radiance variations based on accumulated
event polarities, facilitating progressive and rapid learning for scene
representation construction. Additionally, we introduce regularization losses
specifically designed to suppress artifacts in regions where photometric
changes fall below the event threshold and simultaneously enhance the light
intensity difference of non-zero events, thereby improving the visual fidelity
of the reconstructed scene. Extensive qualitative and quantitative experiments
demonstrate that our method significantly reduces artifacts and achieves
superior reconstruction quality compared to existing methods. The code is
available at https://github.com/Mr-firework/SaENeRF.

</details>


### [56] [Assessing the Feasibility of Internet-Sourced Video for Automatic Cattle Lameness Detection](https://arxiv.org/abs/2504.16404)
*Md Fahimuzzman Sohan*

Main category: cs.CV

TL;DR: 该研究提出了一种基于深度学习的模型，用于通过视频数据检测牛跛行或步态异常，3D CNN模型表现最佳，准确率达90%。


<details>
  <summary>Details</summary>
Motivation: 牛跛行常由蹄部损伤或趾间皮炎引起，影响其行走、进食等生理活动，亟需高效检测方法。

Method: 使用公开视频数据，通过数据增强提升模型鲁棒性，采用ConvLSTM2D和3D CNN两种深度学习模型进行分类。

Result: 3D CNN模型视频级分类准确率为90%，各项指标均优于ConvLSTM2D模型（准确率85%）。

Conclusion: 研究表明，3D CNN能有效简化流程并准确分类牛跛行，为传统多阶段方法提供了替代方案。

Abstract: Cattle lameness is often caused by hoof injuries or interdigital dermatitis,
leads to pain and significantly impacts essential physiological activities such
as walking, feeding, and drinking. This study presents a deep learning-based
model for detecting cattle lameness, sickness, or gait abnormalities using
publicly available video data. The dataset consists of 50 unique videos from 40
individual cattle, recorded from various angles in both indoor and outdoor
environments. Half of the dataset represents naturally walking
(normal/non-lame) cattle, while the other half consists of cattle exhibiting
gait abnormalities (lame). To enhance model robustness and generalizability,
data augmentation was applied to the training data. The pre-processed videos
were then classified using two deep learning models: ConvLSTM2D and 3D CNN. A
comparative analysis of the results demonstrates strong classification
performance. Specifically, the 3D CNN model achieved a video-level
classification accuracy of 90%, with precision, recall, and f1-score of 90.9%,
90.9%, and 90.91% respectively. The ConvLSTM2D model exhibited a slightly lower
accuracy of 85%. This study highlights the effectiveness of directly applying
classification models to learn spatiotemporal features from video data,
offering an alternative to traditional multi-stage approaches that typically
involve object detection, pose estimation, and feature extraction. Besides, the
findings demonstrate that the proposed deep learning models, particularly the
3D CNN, effectively classify and detect lameness in cattle while simplifying
the processing pipeline.

</details>


### [57] [PixelWeb: The First Web GUI Dataset with Pixel-Wise Labels](https://arxiv.org/abs/2504.16419)
*Qi Yang,Weichen Bi,Haiyang Shen,Yaoqi Guo,Yun Ma*

Main category: cs.CV

TL;DR: PixelWeb是一个大规模GUI数据集，通过结合视觉特征提取和DOM结构分析，提供高质量BBox注释，显著提升GUI元素检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有GUI数据集因自动标注导致BBox注释不准确，且仅提供视觉注释，限制了GUI下游任务的发展。

Method: 采用通道派生和层分析两模块，结合BGRA四通道位图注释和DOM分析，确保GUI元素精确定位。

Result: PixelWeb在mAP95指标上比现有数据集性能提升3-7倍。

Conclusion: PixelWeb为GUI生成和自动化用户交互等下游任务提供了巨大潜力。

Abstract: Graphical User Interface (GUI) datasets are crucial for various downstream
tasks. However, GUI datasets often generate annotation information through
automatic labeling, which commonly results in inaccurate GUI element BBox
annotations, including missing, duplicate, or meaningless BBoxes. These issues
can degrade the performance of models trained on these datasets, limiting their
effectiveness in real-world applications. Additionally, existing GUI datasets
only provide BBox annotations visually, which restricts the development of
visually related GUI downstream tasks. To address these issues, we introduce
PixelWeb, a large-scale GUI dataset containing over 100,000 annotated web
pages. PixelWeb is constructed using a novel automatic annotation approach that
integrates visual feature extraction and Document Object Model (DOM) structure
analysis through two core modules: channel derivation and layer analysis.
Channel derivation ensures accurate localization of GUI elements in cases of
occlusion and overlapping elements by extracting BGRA four-channel bitmap
annotations. Layer analysis uses the DOM to determine the visibility and
stacking order of elements, providing precise BBox annotations. Additionally,
PixelWeb includes comprehensive metadata such as element images, contours, and
mask annotations. Manual verification by three independent annotators confirms
the high quality and accuracy of PixelWeb annotations. Experimental results on
GUI element detection tasks show that PixelWeb achieves performance on the
mAP95 metric that is 3-7 times better than existing datasets. We believe that
PixelWeb has great potential for performance improvement in downstream tasks
such as GUI generation and automated user interaction.

</details>


### [58] [FrogDogNet: Fourier frequency Retained visual prompt Output Guidance for Domain Generalization of CLIP in Remote Sensing](https://arxiv.org/abs/2504.16433)
*Hariseetharam Gunduboina,Muhammad Haris Khan,Biplab Banerjee*

Main category: cs.CV

TL;DR: FrogDogNet是一种新型提示学习框架，通过傅里叶频率过滤和自注意力机制提升遥感场景分类和领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型（如CLIP）在通用计算机视觉中表现优异，但在遥感领域的领域泛化潜力尚未充分探索。现有方法依赖全图像特征，引入噪声和背景干扰，导致分类错误。

Method: FrogDogNet结合傅里叶频率过滤和自注意力机制，选择性保留不变低频成分，消除噪声和无关背景，提取关键特征用于提示学习。

Result: 在四个遥感数据集和三个领域泛化任务上的实验表明，FrogDogNet优于现有提示学习方法，展现出更强的跨领域适应性。

Conclusion: 频率不变特征保留在泛化中效果显著，为更广泛应用铺平了道路。

Abstract: In recent years, large-scale vision-language models (VLMs) like CLIP have
gained attention for their zero-shot inference using instructional text
prompts. While these models excel in general computer vision, their potential
for domain generalization in remote sensing (RS) remains underexplored.
Existing approaches enhance prompt learning by generating visual prompt tokens
but rely on full-image features, introducing noise and background artifacts
that vary within a class, causing misclassification. To address this, we
propose FrogDogNet, a novel prompt learning framework integrating Fourier
frequency filtering and self-attention to improve RS scene classification and
domain generalization. FrogDogNet selectively retains invariant low-frequency
components while eliminating noise and irrelevant backgrounds, ensuring robust
feature representation across domains. The model first extracts significant
features via projection and self-attention, then applies frequency-based
filtering to preserve essential structural information for prompt learning.
Extensive experiments on four RS datasets and three domain generalization tasks
show that FrogDogNet consistently outperforms state-of-the-art prompt learning
methods, demonstrating superior adaptability across domain shifts. Our findings
highlight the effectiveness of frequency-based invariant feature retention in
generalization, paving the way for broader applications. Our code is available
at https://github.com/HariseetharamG/FrogDogNet

</details>


### [59] [Marginalized Generalized IoU (MGIoU): A Unified Objective Function for Optimizing Any Convex Parametric Shapes](https://arxiv.org/abs/2504.16443)
*Duy-Tho Le,Trung Pham,Jianfei Cai,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: 论文提出了一种新的损失函数MGIoU和MGIoU+，用于统一参数化形状优化，解决了现有方法的不足，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有优化方法（如L1/L2损失、IoU损失）在参数化形状优化中存在不足，如缺乏相关性、不稳定或计算复杂，导致领域内方法分散。

Method: 通过将结构化凸形状投影到其独特的形状法线上，计算一维归一化GIoU，提出MGIoU和MGIoU+损失函数。

Result: 实验表明，MGIoU和MGIoU+在性能上优于现有方法，计算延迟降低10-40倍，且满足度量性质和尺度不变性。

Conclusion: MGIoU和MGIoU+为参数化形状优化提供了统一、高效且鲁棒的解决方案。

Abstract: Optimizing the similarity between parametric shapes is crucial for numerous
computer vision tasks, where Intersection over Union (IoU) stands as the
canonical measure. However, existing optimization methods exhibit significant
shortcomings: regression-based losses like L1/L2 lack correlation with IoU,
IoU-based losses are unstable and limited to simple shapes, and task-specific
methods are computationally intensive and not generalizable accross domains. As
a result, the current landscape of parametric shape objective functions has
become scattered, with each domain proposing distinct IoU approximations. To
address this, we unify the parametric shape optimization objective functions by
introducing Marginalized Generalized IoU (MGIoU), a novel loss function that
overcomes these challenges by projecting structured convex shapes onto their
unique shape Normals to compute one-dimensional normalized GIoU. MGIoU offers a
simple, efficient, fully differentiable approximation strongly correlated with
IoU. We then extend MGIoU to MGIoU+ that supports optimizing unstructured
convex shapes. Together, MGIoU and MGIoU+ unify parametric shape optimization
across diverse applications. Experiments on standard benchmarks demonstrate
that MGIoU and MGIoU+ consistently outperform existing losses while reducing
loss computation latency by 10-40x. Additionally, MGIoU and MGIoU+ satisfy
metric properties and scale-invariance, ensuring robustness as an objective
function. We further propose MGIoU- for minimizing overlaps in tasks like
collision-free trajectory prediction. Code is available at
https://ldtho.github.io/MGIoU

</details>


### [60] [Cross Paradigm Representation and Alignment Transformer for Image Deraining](https://arxiv.org/abs/2504.16455)
*Shun Zou,Yi Zou,Juncheng Li,Guangwei Gao,Guojun Qi*

Main category: cs.CV

TL;DR: 提出了一种新型跨范式表示与对齐Transformer（CPRAformer），通过整合全局-局部和空间-通道表示，解决了图像去雨任务中不规则雨纹和复杂几何重叠的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer架构在图像去雨中表现优异，但单一范式难以应对不规则雨纹和复杂几何重叠，需要统一框架整合互补表示。

Method: 采用分层表示与对齐，结合稀疏提示通道自注意力（SPC-SA）和空间像素细化自注意力（SPR-SA），并通过自适应对齐频率模块（AAFM）实现特征对齐与交互。

Result: 在八个基准数据集上达到最先进性能，并在其他图像修复任务中验证了模型的鲁棒性。

Conclusion: CPRAformer通过跨范式动态交互框架，有效提取并融合互补信息，显著提升了图像去雨任务的性能。

Abstract: Transformer-based networks have achieved strong performance in low-level
vision tasks like image deraining by utilizing spatial or channel-wise
self-attention. However, irregular rain patterns and complex geometric overlaps
challenge single-paradigm architectures, necessitating a unified framework to
integrate complementary global-local and spatial-channel representations. To
address this, we propose a novel Cross Paradigm Representation and Alignment
Transformer (CPRAformer). Its core idea is the hierarchical representation and
alignment, leveraging the strengths of both paradigms (spatial-channel and
global-local) to aid image reconstruction. It bridges the gap within and
between paradigms, aligning and coordinating them to enable deep interaction
and fusion of features. Specifically, we use two types of self-attention in the
Transformer blocks: sparse prompt channel self-attention (SPC-SA) and spatial
pixel refinement self-attention (SPR-SA). SPC-SA enhances global channel
dependencies through dynamic sparsity, while SPR-SA focuses on spatial rain
distribution and fine-grained texture recovery. To address the feature
misalignment and knowledge differences between them, we introduce the Adaptive
Alignment Frequency Module (AAFM), which aligns and interacts with features in
a two-stage progressive manner, enabling adaptive guidance and complementarity.
This reduces the information gap within and between paradigms. Through this
unified cross-paradigm dynamic interaction framework, we achieve the extraction
of the most valuable interactive fusion information from the two paradigms.
Extensive experiments demonstrate that our model achieves state-of-the-art
performance on eight benchmark datasets and further validates CPRAformer's
robustness in other image restoration tasks and downstream applications.

</details>


### [61] [MTSGL: Multi-Task Structure Guided Learning for Robust and Interpretable SAR Aircraft Recognition](https://arxiv.org/abs/2504.16467)
*Qishan He,Lingjun Zhao,Ru Luo,Siqian Zhang,Lin Lei,Kefeng Ji,Gangyao Kuang*

Main category: cs.CV

TL;DR: 论文提出了一种基于结构的多任务学习网络（MTSGL），用于SAR图像中的飞机识别，通过结构语义感知和一致性正则化模块提升模型的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前SAR图像飞机识别算法缺乏对飞机结构知识的深入理解，需要结合结构信息提升识别效果。

Method: 引入基于结构的SAR飞机标注方法，提出MTSGL网络，包含分类任务、结构语义感知（SSA）模块和结构一致性正则化（SCR）模块。

Result: 在自建数据集MT-SARD上的实验表明，MTSGL在鲁棒性和可解释性方面表现优越。

Conclusion: MTSGL通过结合专家级飞机先验知识和结构引导学习，实现了类似人类认知的飞机概念理解。

Abstract: Aircraft recognition in synthetic aperture radar (SAR) imagery is a
fundamental mission in both military and civilian applications. Recently deep
learning (DL) has emerged a dominant paradigm for its explosive performance on
extracting discriminative features. However, current classification algorithms
focus primarily on learning decision hyperplane without enough comprehension on
aircraft structural knowledge. Inspired by the fined aircraft annotation
methods for optical remote sensing images (RSI), we first introduce a
structure-based SAR aircraft annotations approach to provide structural and
compositional supplement information. On this basis, we propose a multi-task
structure guided learning (MTSGL) network for robust and interpretable SAR
aircraft recognition. Besides the classification task, MTSGL includes a
structural semantic awareness (SSA) module and a structural consistency
regularization (SCR) module. The SSA is designed to capture structure semantic
information, which is conducive to gain human-like comprehension of aircraft
knowledge. The SCR helps maintain the geometric consistency between the
aircraft structure in SAR imagery and the proposed annotation. In this process,
the structural attribute can be disentangled in a geometrically meaningful
manner. In conclusion, the MTSGL is presented with the expert-level aircraft
prior knowledge and structure guided learning paradigm, aiming to comprehend
the aircraft concept in a way analogous to the human cognitive process.
Extensive experiments are conducted on a self-constructed multi-task SAR
aircraft recognition dataset (MT-SARD) and the effective results illustrate the
superiority of robustness and interpretation ability of the proposed MTSGL.

</details>


### [62] [RGB-D Video Object Segmentation via Enhanced Multi-store Feature Memory](https://arxiv.org/abs/2504.16471)
*Boyue Xu,Ruichao Hou,Tongwei Ren,Gangshan Wu*

Main category: cs.CV

TL;DR: 提出了一种基于多存储特征记忆的RGB-D视频对象分割方法，通过自适应模态选择和融合，结合SAM模型优化分割结果。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-D分割方法未能充分利用跨模态信息，且在长期预测中易出现对象漂移问题。

Method: 设计了分层模态选择和融合机制，开发了分割细化模块，利用SAM模型优化分割掩码，并通过时空嵌入和模态嵌入提升性能。

Result: 在最新RGB-D VOS基准测试中取得了最先进的性能。

Conclusion: 该方法通过多模态特征融合和SAM模型的结合，显著提升了RGB-D视频对象分割的鲁棒性和准确性。

Abstract: The RGB-Depth (RGB-D) Video Object Segmentation (VOS) aims to integrate the
fine-grained texture information of RGB with the spatial geometric clues of
depth modality, boosting the performance of segmentation. However,
off-the-shelf RGB-D segmentation methods fail to fully explore cross-modal
information and suffer from object drift during long-term prediction. In this
paper, we propose a novel RGB-D VOS method via multi-store feature memory for
robust segmentation. Specifically, we design the hierarchical modality
selection and fusion, which adaptively combines features from both modalities.
Additionally, we develop a segmentation refinement module that effectively
utilizes the Segmentation Anything Model (SAM) to refine the segmentation mask,
ensuring more reliable results as memory to guide subsequent segmentation
tasks. By leveraging spatio-temporal embedding and modality embedding, mixed
prompts and fused images are fed into SAM to unleash its potential in RGB-D
VOS. Experimental results show that the proposed method achieves
state-of-the-art performance on the latest RGB-D VOS benchmark.

</details>


### [63] [Rethinking Generalizable Infrared Small Target Detection: A Real-scene Benchmark and Cross-view Representation Learning](https://arxiv.org/abs/2504.16487)
*Yahao Lu,Yuehui Li,Xingyuan Guo,Shuai Yuan,Yukai Shi,Liang Lin*

Main category: cs.CV

TL;DR: 本文提出了一种基于域适应的红外小目标检测框架，通过跨视图通道对齐和噪声引导表示学习，提升了模型在不同场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测受传感器类型、观测条件和目标特性的影响，导致数据分布差异（域偏移），限制了模型的泛化能力。

Method: 提出跨视图通道对齐（CCA）和跨视图Top-K融合策略，结合噪声引导表示学习，优化特征提取和噪声鲁棒性。

Result: 在检测概率（Pd）、误报率（Fa）和交并比（IoU）上优于现有方法，并发布了RealScene-ISTD数据集。

Conclusion: 该框架有效解决了域偏移问题，提升了红外小目标检测的泛化性能和噪声鲁棒性。

Abstract: Infrared small target detection (ISTD) is highly sensitive to sensor type,
observation conditions, and the intrinsic properties of the target. These
factors can introduce substantial variations in the distribution of acquired
infrared image data, a phenomenon known as domain shift. Such distribution
discrepancies significantly hinder the generalization capability of ISTD models
across diverse scenarios. To tackle this challenge, this paper introduces an
ISTD framework enhanced by domain adaptation. To alleviate distribution shift
between datasets and achieve cross-sample alignment, we introduce Cross-view
Channel Alignment (CCA). Additionally, we propose the Cross-view Top-K Fusion
strategy, which integrates target information with diverse background features,
enhancing the model' s ability to extract critical data characteristics. To
further mitigate the impact of noise on ISTD, we develop a Noise-guided
Representation learning strategy. This approach enables the model to learn more
noise-resistant feature representations, to improve its generalization
capability across diverse noisy domains. Finally, we develop a dedicated
infrared small target dataset, RealScene-ISTD. Compared to state-of-the-art
methods, our approach demonstrates superior performance in terms of detection
probability (Pd), false alarm rate (Fa), and intersection over union (IoU). The
code is available at: https://github.com/luy0222/RealScene-ISTD.

</details>


### [64] [PRaDA: Projective Radial Distortion Averaging](https://arxiv.org/abs/2504.16499)
*Daniil Sinitsyn,Linus Härenstam-Nielsen,Daniel Cremers*

Main category: cs.CV

TL;DR: 提出了一种在投影空间中解耦径向畸变校准与3D重建的方法，避免了传统方法的复杂性，同时保持了高精度。


<details>
  <summary>Details</summary>
Motivation: 解决传统径向畸变校准方法需要大量重叠图像或依赖学习方法的局限性，提出一种更高效且准确的解决方案。

Method: 在投影空间中工作，利用单应性封装除畸变外的所有相机参数，提出“投影径向畸变平均”方法，无需3D点重建或全局优化。

Result: 方法在保持SfM精度的同时，避免了复杂性和大量图像需求，支持任意特征匹配方法。

Conclusion: 该方法为径向畸变校准提供了一种高效且准确的替代方案，适用于多种场景。

Abstract: We tackle the problem of automatic calibration of radially distorted cameras
in challenging conditions. Accurately determining distortion parameters
typically requires either 1) solving the full Structure from Motion (SfM)
problem involving camera poses, 3D points, and the distortion parameters, which
is only possible if many images with sufficient overlap are provided, or 2)
relying heavily on learning-based methods that are comparatively less accurate.
In this work, we demonstrate that distortion calibration can be decoupled from
3D reconstruction, maintaining the accuracy of SfM-based methods while avoiding
many of the associated complexities. This is achieved by working in Projective
Space, where the geometry is unique up to a homography, which encapsulates all
camera parameters except for distortion. Our proposed method, Projective Radial
Distortion Averaging, averages multiple distortion estimates in a fully
projective framework without creating 3d points and full bundle adjustment. By
relying on pairwise projective relations, our methods support any
feature-matching approaches without constructing point tracks across multiple
images.

</details>


### [65] [TraveLLaMA: Facilitating Multi-modal Large Language Models to Understand Urban Scenes and Provide Travel Assistance](https://arxiv.org/abs/2504.16505)
*Meng Chu,Yukang Chen,Haokun Gui,Shaozuo Yu,Yi Wang,Jiaya Jia*

Main category: cs.CV

TL;DR: TraveLLaMA是一个专为城市场景理解和旅行辅助设计的多模态语言模型，通过大规模数据集和微调实验显著提升了旅行相关任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统缺乏对城市环境的专业知识和上下文理解，难以满足旅行规划需求。

Method: 构建包含22万问答对的数据集，结合文本和视觉语言数据，并对先进视觉语言模型进行微调。

Result: 性能提升6.5%-9.4%，在旅行推荐、地图理解和场景理解方面表现优异。

Conclusion: TraveLLaMA在多模态旅行辅助系统中树立了新标杆，显著优于通用模型。

Abstract: Tourism and travel planning increasingly rely on digital assistance, yet
existing multimodal AI systems often lack specialized knowledge and contextual
understanding of urban environments. We present TraveLLaMA, a specialized
multimodal language model designed for urban scene understanding and travel
assistance. Our work addresses the fundamental challenge of developing
practical AI travel assistants through a novel large-scale dataset of 220k
question-answer pairs. This comprehensive dataset uniquely combines 130k text
QA pairs meticulously curated from authentic travel forums with GPT-enhanced
responses, alongside 90k vision-language QA pairs specifically focused on map
understanding and scene comprehension. Through extensive fine-tuning
experiments on state-of-the-art vision-language models (LLaVA, Qwen-VL,
Shikra), we demonstrate significant performance improvements ranging from
6.5\%-9.4\% in both pure text travel understanding and visual question
answering tasks. Our model exhibits exceptional capabilities in providing
contextual travel recommendations, interpreting map locations, and
understanding place-specific imagery while offering practical information such
as operating hours and visitor reviews. Comparative evaluations show TraveLLaMA
significantly outperforms general-purpose models in travel-specific tasks,
establishing a new benchmark for multi-modal travel assistance systems.

</details>


### [66] [Federated Learning of Low-Rank One-Shot Image Detection Models in Edge Devices with Scalable Accuracy and Compute Complexity](https://arxiv.org/abs/2504.16515)
*Abdul Hannaan,Zubair Shah,Aiman Erbad,Amr Mohamed,Ali Safa*

Main category: cs.CV

TL;DR: LoRa-FL是一种新颖的联邦学习框架，用于在边缘设备上训练低秩单次图像检测模型，显著降低计算和通信开销，同时保持可扩展的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备上资源受限的问题，同时减少联邦学习中的计算和通信开销。

Method: 结合低秩适应技术和单次检测架构，利用联邦学习协作训练轻量级图像识别模型。

Result: 在MNIST和CIFAR10数据集上，无论是IID还是非IID设置，均表现出竞争性的检测性能，同时显著降低通信带宽和计算复杂度。

Conclusion: LoRa-FL是一种有前景的解决方案，能够在减少通信和计算开销的同时不牺牲模型准确性。

Abstract: This paper introduces a novel federated learning framework termed LoRa-FL
designed for training low-rank one-shot image detection models deployed on edge
devices. By incorporating low-rank adaptation techniques into one-shot
detection architectures, our method significantly reduces both computational
and communication overhead while maintaining scalable accuracy. The proposed
framework leverages federated learning to collaboratively train lightweight
image recognition models, enabling rapid adaptation and efficient deployment
across heterogeneous, resource-constrained devices. Experimental evaluations on
the MNIST and CIFAR10 benchmark datasets, both in an
independent-and-identically-distributed (IID) and non-IID setting, demonstrate
that our approach achieves competitive detection performance while
significantly reducing communication bandwidth and compute complexity. This
makes it a promising solution for adaptively reducing the communication and
compute power overheads, while not sacrificing model accuracy.

</details>


### [67] [Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion and Reasoning for Vision-and-Language Navigation](https://arxiv.org/abs/2504.16516)
*Junrong Yue,Yifan Zhang,Chuan Qin,Bo Li,Xiaomin Lie,Xinlei Yu,Wenxin Zhang,Zhendong Zhao*

Main category: cs.CV

TL;DR: MFRA提出了一种多级融合和推理架构，通过分层融合多模态特征和指令引导的推理，提升了视觉与语言导航任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉跨模态复杂交互方面不足，MFRA旨在通过多级特征融合和推理提升导航准确性。

Method: MFRA采用分层融合机制整合多级特征，并设计推理模块通过指令引导的注意力动态整合上下文。

Result: 在REVERIE、R2R和SOON等基准数据集上，MFRA性能优于现有方法。

Conclusion: 多级模态融合对具身导航任务有效，MFRA展示了其在复杂导航场景中的优势。

Abstract: Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow
natural language instructions and reach target locations in real-world
environments. While prior methods often rely on either global scene
representations or object-level features, these approaches are insufficient for
capturing the complex interactions across modalities required for accurate
navigation. In this paper, we propose a Multi-level Fusion and Reasoning
Architecture (MFRA) to enhance the agent's ability to reason over visual
observations, language instructions and navigation history. Specifically, MFRA
introduces a hierarchical fusion mechanism that aggregates multi-level
features-ranging from low-level visual cues to high-level semantic
concepts-across multiple modalities. We further design a reasoning module that
leverages fused representations to infer navigation actions through
instruction-guided attention and dynamic context integration. By selectively
capturing and combining relevant visual, linguistic, and temporal signals, MFRA
improves decision-making accuracy in complex navigation scenarios. Extensive
experiments on benchmark VLN datasets including REVERIE, R2R, and SOON
demonstrate that MFRA achieves superior performance compared to
state-of-the-art methods, validating the effectiveness of multi-level modal
fusion for embodied navigation.

</details>


### [68] [A Few-Shot Metric Learning Method with Dual-Channel Attention for Cross-Modal Same-Neuron Identification](https://arxiv.org/abs/2504.16520)
*Wenwei Li,Liyi Cai,Wu Chen,Anan Li*

Main category: cs.CV

TL;DR: 提出了一种基于双通道注意力机制和预训练视觉Transformer的少样本度量学习方法，用于跨模态神经元识别，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决神经科学研究中跨模态单神经元匹配的挑战，如模态差异和有限标注。

Method: 采用双通道注意力机制（局部和全局通道）提取神经元形态和上下文信息，结合硬样本挖掘策略和Circle Loss函数。

Result: 在双光子和fMOST数据集上表现出更高的Top-K准确率和召回率，并通过消融实验验证了模块有效性。

Conclusion: 该方法为单细胞水平匹配和多模态神经影像整合提供了有效的技术解决方案。

Abstract: In neuroscience research, achieving single-neuron matching across different
imaging modalities is critical for understanding the relationship between
neuronal structure and function. However, modality gaps and limited annotations
present significant challenges. We propose a few-shot metric learning method
with a dual-channel attention mechanism and a pretrained vision transformer to
enable robust cross-modal neuron identification. The local and global channels
extract soma morphology and fiber context, respectively, and a gating mechanism
fuses their outputs. To enhance the model's fine-grained discrimination
capability, we introduce a hard sample mining strategy based on the
MultiSimilarityMiner algorithm, along with the Circle Loss function.
Experiments on two-photon and fMOST datasets demonstrate superior Top-K
accuracy and recall compared to existing methods. Ablation studies and t-SNE
visualizations validate the effectiveness of each module. The method also
achieves a favorable trade-off between accuracy and training efficiency under
different fine-tuning strategies. These results suggest that the proposed
approach offers a promising technical solution for accurate single-cell level
matching and multimodal neuroimaging integration.

</details>


### [69] [Streetscape Analysis with Generative AI (SAGAI): Vision-Language Assessment and Mapping of Urban Scenes](https://arxiv.org/abs/2504.16538)
*Joan Perez,Giovanni Fusco*

Main category: cs.CV

TL;DR: SAGAI是一种利用生成式人工智能分析城市街景的模块化工作流，结合开源数据和视觉语言模型，支持可扩展的城市环境分析。


<details>
  <summary>Details</summary>
Motivation: 目前街景评估局限于形态学特征或需要人工定性分析，SAGAI旨在提供一种自动化、可扩展的解决方案。

Method: 整合OpenStreetMap、Google街景图像和轻量级LLaVA模型，通过自然语言提示生成结构化空间指标。

Result: 在尼斯和维也纳的案例中，SAGAI在城乡分类、商业特征检测和人行道宽度估计方面表现良好。

Conclusion: SAGAI无需特定训练或专有软件，适用于多种城市研究主题，如步行性、安全性和城市设计。

Abstract: Streetscapes are an essential component of urban space. Their assessment is
presently either limited to morphometric properties of their mass skeleton or
requires labor-intensive qualitative evaluations of visually perceived
qualities. This paper introduces SAGAI: Streetscape Analysis with Generative
Artificial Intelligence, a modular workflow for scoring street-level urban
scenes using open-access data and vision-language models. SAGAI integrates
OpenStreetMap geometries, Google Street View imagery, and a lightweight version
of the LLaVA model to generate structured spatial indicators from images via
customizable natural language prompts. The pipeline includes an automated
mapping module that aggregates visual scores at both the point and street
levels, enabling direct cartographic interpretation. It operates without
task-specific training or proprietary software dependencies, supporting
scalable and interpretable analysis of urban environments. Two exploratory case
studies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial
outputs from vision-language inference. The initial results show strong
performance for binary urban-rural scene classification, moderate precision in
commercial feature detection, and lower estimates, but still informative, of
sidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a
wide range of urban research themes, such as walkability, safety, or urban
design, through prompt modification alone.

</details>


### [70] [ToF-Splatting: Dense SLAM using Sparse Time-of-Flight Depth and Multi-Frame Integration](https://arxiv.org/abs/2504.16545)
*Andrea Conti,Matteo Poggi,Valerio Cambareri,Martin R. Oswald,Stefano Mattoccia*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯泼溅的SLAM方法（ToF-Splatting），用于处理极稀疏的ToF深度数据，通过多帧集成模块生成密集深度图。


<details>
  <summary>Details</summary>
Motivation: 解决极稀疏ToF数据在SLAM中应用的限制，满足移动和AR/VR设备的低功耗需求。

Method: 结合极稀疏ToF深度、单目彩色和多视图几何信息，通过多帧集成模块生成密集深度图。

Result: 在合成和真实稀疏ToF数据集上表现优异，达到当前最佳跟踪和建图性能。

Conclusion: ToF-Splatting为极稀疏ToF数据提供了一种高效的SLAM解决方案。

Abstract: Time-of-Flight (ToF) sensors provide efficient active depth sensing at
relatively low power budgets; among such designs, only very sparse measurements
from low-resolution sensors are considered to meet the increasingly limited
power constraints of mobile and AR/VR devices. However, such extreme sparsity
levels limit the seamless usage of ToF depth in SLAM. In this work, we propose
ToF-Splatting, the first 3D Gaussian Splatting-based SLAM pipeline tailored for
using effectively very sparse ToF input data. Our approach improves upon the
state of the art by introducing a multi-frame integration module, which
produces dense depth maps by merging cues from extremely sparse ToF depth,
monocular color, and multi-view geometry. Extensive experiments on both
synthetic and real sparse ToF datasets demonstrate the viability of our
approach, as it achieves state-of-the-art tracking and mapping performances on
reference datasets.

</details>


### [71] [Beyond Anonymization: Object Scrubbing for Privacy-Preserving 2D and 3D Vision Tasks](https://arxiv.org/abs/2504.16557)
*Murat Bilgehan Ertan,Ronak Sahu,Phuong Ha Nguyen,Kaleel Mahmood,Marten van Dijk*

Main category: cs.CV

TL;DR: ROAR是一种隐私保护的数据模糊化框架，通过移除敏感对象而非修改它们，结合实例分割和生成修复技术，在保持场景完整性的同时实现隐私保护。


<details>
  <summary>Details</summary>
Motivation: 解决隐私保护数据模糊化中敏感对象移除的问题，同时保持数据集的有效性和场景完整性。

Method: 整合实例分割与生成修复技术，移除可识别实体。

Result: 在2D对象检测中达到基线AP的87.5%，3D重建中PSNR损失最多1.66 dB，同时保持SSIM和改进LPIPS。

Conclusion: ROAR作为一种隐私保护框架，在保证隐私的同时最小化性能损失，为隐私保护视觉系统奠定基础。

Abstract: We introduce ROAR (Robust Object Removal and Re-annotation), a scalable
framework for privacy-preserving dataset obfuscation that eliminates sensitive
objects instead of modifying them. Our method integrates instance segmentation
with generative inpainting to remove identifiable entities while preserving
scene integrity. Extensive evaluations on 2D COCO-based object detection show
that ROAR achieves 87.5% of the baseline detection average precision (AP),
whereas image dropping achieves only 74.2% of the baseline AP, highlighting the
advantage of scrubbing in preserving dataset utility. The degradation is even
more severe for small objects due to occlusion and loss of fine-grained
details. Furthermore, in NeRF-based 3D reconstruction, our method incurs a PSNR
loss of at most 1.66 dB while maintaining SSIM and improving LPIPS,
demonstrating superior perceptual quality. Our findings establish object
removal as an effective privacy framework, achieving strong privacy guarantees
with minimal performance trade-offs. The results highlight key challenges in
generative inpainting, occlusion-robust segmentation, and task-specific
scrubbing, setting the foundation for future advancements in privacy-preserving
vision systems.

</details>


### [72] [SAIP-Net: Enhancing Remote Sensing Image Segmentation via Spectral Adaptive Information Propagation](https://arxiv.org/abs/2504.16564)
*Zhongtao Wang,Xizhe Cao,Yisong Chen,Guoping Wang*

Main category: cs.CV

TL;DR: SAIP-Net是一种基于频率感知的分割框架，通过自适应频率滤波和多尺度感受野增强，显著提升了遥感图像语义分割的性能。


<details>
  <summary>Details</summary>
Motivation: 传统层次模型在遥感图像语义分割中难以满足精确空间边界和类内一致性的需求，SAIP-Net旨在解决空间域特征融合和感受野不足的问题。

Method: 采用频谱自适应信息传播（Spectral Adaptive Information Propagation），结合自适应频率滤波和多尺度感受野增强技术。

Result: 实验表明，SAIP-Net在性能上显著优于现有方法，尤其在抑制类内特征不一致性和边界锐化方面表现突出。

Conclusion: 频谱自适应策略与扩展感受野的结合，为遥感图像分割提供了一种高效解决方案。

Abstract: Semantic segmentation of remote sensing imagery demands precise spatial
boundaries and robust intra-class consistency, challenging conventional
hierarchical models. To address limitations arising from spatial domain feature
fusion and insufficient receptive fields, this paper introduces SAIP-Net, a
novel frequency-aware segmentation framework that leverages Spectral Adaptive
Information Propagation. SAIP-Net employs adaptive frequency filtering and
multi-scale receptive field enhancement to effectively suppress intra-class
feature inconsistencies and sharpen boundary lines. Comprehensive experiments
demonstrate significant performance improvements over state-of-the-art methods,
highlighting the effectiveness of spectral-adaptive strategies combined with
expanded receptive fields for remote sensing image segmentation.

</details>


### [73] [CountingDINO: A Training-free Pipeline for Class-Agnostic Counting using Unsupervised Backbones](https://arxiv.org/abs/2504.16570)
*Giacomo Pacini,Lorenzo Bianchi,Luca Ciampi,Nicola Messina,Giuseppe Amato,Fabrizio Falchi*

Main category: cs.CV

TL;DR: CountingDINO是首个无需训练的类别无关计数框架，利用无监督特征提取器，在FSC-147基准测试中表现优于基线，并与监督方法竞争。


<details>
  <summary>Details</summary>
Motivation: 当前基于示例的类别无关计数方法依赖标注数据训练，限制了扩展性和泛化能力。

Method: 使用自监督视觉主干提取对象感知特征，通过ROI-Align提取潜在对象原型，生成相似性图并转化为密度图。

Result: 在FSC-147基准测试中优于基线，与监督方法竞争甚至超越。

Conclusion: 无需训练的类别无关计数方法具有扩展性和竞争力。

Abstract: Class-agnostic counting (CAC) aims to estimate the number of objects in
images without being restricted to predefined categories. However, while
current exemplar-based CAC methods offer flexibility at inference time, they
still rely heavily on labeled data for training, which limits scalability and
generalization to many downstream use cases. In this paper, we introduce
CountingDINO, the first training-free exemplar-based CAC framework that
exploits a fully unsupervised feature extractor. Specifically, our approach
employs self-supervised vision-only backbones to extract object-aware features,
and it eliminates the need for annotated data throughout the entire proposed
pipeline. At inference time, we extract latent object prototypes via ROI-Align
from DINO features and use them as convolutional kernels to generate similarity
maps. These are then transformed into density maps through a simple yet
effective normalization scheme. We evaluate our approach on the FSC-147
benchmark, where we outperform a baseline under the same label-free setting.
Our method also achieves competitive -- and in some cases superior -- results
compared to training-free approaches relying on supervised backbones, as well
as several fully supervised state-of-the-art methods. This demonstrates that
training-free CAC can be both scalable and competitive. Website:
https://lorebianchi98.github.io/CountingDINO/

</details>


### [74] [JEPA for RL: Investigating Joint-Embedding Predictive Architectures for Reinforcement Learning](https://arxiv.org/abs/2504.16591)
*Tristan Kenneweg,Philip Kenneweg,Barbara Hammer*

Main category: cs.CV

TL;DR: JEPA架构在自监督学习中表现优异，本文将其应用于图像强化学习，解决了模型崩溃问题，并在Cart Pole任务中验证了效果。


<details>
  <summary>Details</summary>
Motivation: 探索JEPA架构在强化学习中的应用潜力，尤其是从图像输入中学习。

Method: 将JEPA架构适配到图像强化学习任务中，并提出防止模型崩溃的方法。

Result: 在经典的Cart Pole任务中验证了方法的有效性。

Conclusion: JEPA架构适用于图像强化学习，且通过特定方法可避免模型崩溃。

Abstract: Joint-Embedding Predictive Architectures (JEPA) have recently become popular
as promising architectures for self-supervised learning. Vision transformers
have been trained using JEPA to produce embeddings from images and videos,
which have been shown to be highly suitable for downstream tasks like
classification and segmentation. In this paper, we show how to adapt the JEPA
architecture to reinforcement learning from images. We discuss model collapse,
show how to prevent it, and provide exemplary data on the classical Cart Pole
task.

</details>


### [75] [Federated EndoViT: Pretraining Vision Transformers via Federated Learning on Endoscopic Image Collections](https://arxiv.org/abs/2504.16612)
*Max Kirchner,Alexander C. Jenke,Sebastian Bodenstedt,Fiona R. Kolbinger,Oliver Saldanha,Jakob N. Kather,Martin Wagner,Stefanie Speidel*

Main category: cs.CV

TL;DR: 研究探讨了利用联邦学习训练基础模型，解决数据共享限制，实现无需数据传输的协作模型训练，应用于微创手术。


<details>
  <summary>Details</summary>
Motivation: 解决数据共享限制，实现隐私保护的协作模型训练，适用于微创手术领域。

Method: 基于EndoViT研究，改进Masked Autoencoder，结合自适应FedSAM和SWA，在Endo700k数据集上进行预训练，并在下游任务中微调和评估。

Result: 自适应FedSAM提升了预训练效果，减少了重建损失。FL-EndoViT在手术任务中表现与CEN-EndoViT相当，并在数据有限时优于后者。

Conclusion: 联邦学习为手术基础模型的隐私保护训练提供了潜力，未来可探索视频模型以增强时空动态能力。

Abstract: Purpose: In this study, we investigate the training of foundation models
using federated learning to address data-sharing limitations and enable
collaborative model training without data transfer for minimally invasive
surgery. Methods: Inspired by the EndoViT study, we adapt the Masked
Autoencoder for federated learning, enhancing it with adaptive Sharpness-Aware
Minimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is
pretrained on the Endo700k dataset collection and later fine-tuned and
evaluated for tasks such as Semantic Segmentation, Action Triplet Recognition,
and Surgical Phase Recognition. Results: Our findings demonstrate that
integrating adaptive FedSAM into the federated MAE approach improves
pretraining, leading to a reduction in reconstruction loss per patch. The
application of FL-EndoViT in surgical downstream tasks results in performance
comparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over
CEN-EndoViT in surgical scene segmentation when data is limited and in action
triplet recognition when large datasets are used. Conclusion: These findings
highlight the potential of federated learning for privacy-preserving training
of surgical foundation models, offering a robust and generalizable solution for
surgical data science. Effective collaboration requires adapting federated
learning methods, such as the integration of FedSAM, which can accommodate the
inherent data heterogeneity across institutions. In future, exploring FL in
video-based models may enhance these capabilities by incorporating
spatiotemporal dynamics crucial for real-world surgical environments.

</details>


### [76] [EHGCN: Hierarchical Euclidean-Hyperbolic Fusion via Motion-Aware GCN for Hybrid Event Stream Perception](https://arxiv.org/abs/2504.16616)
*Haosheng Chen,Lian Luo,Mengjingcheng Mo,Zhanjie Wu,Guobao Xiao,Ji Gan,Jiaxu Leng,Xinbo Gao*

Main category: cs.CV

TL;DR: EHGCN是一种新颖的图神经网络方法，结合欧几里得和双曲空间处理事件流，通过自适应采样和运动感知超边生成提升事件感知性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法在欧几里得空间中难以捕捉事件流的长程依赖和层次结构，限制了事件感知任务的性能。

Method: 提出EHGCN，包含自适应采样策略、基于马尔可夫向量场的运动感知超边生成方法，以及欧几里得-双曲GCN融合信息。

Result: 在物体检测和识别等任务中验证了方法的有效性。

Conclusion: EHGCN通过混合空间建模显著提升了事件感知能力。

Abstract: Event cameras, with microsecond temporal resolution and high dynamic range
(HDR) characteristics, emit high-speed event stream for perception tasks.
Despite the recent advancement in GNN-based perception methods, they are prone
to use straightforward pairwise connectivity mechanisms in the pure Euclidean
space where they struggle to capture long-range dependencies and fail to
effectively characterize the inherent hierarchical structures of non-uniformly
distributed event stream. To this end, in this paper we propose a novel
approach named EHGCN, which is a pioneer to perceive event stream in both
Euclidean and hyperbolic spaces for event vision. In EHGCN, we introduce an
adaptive sampling strategy to dynamically regulate sampling rates, retaining
discriminative events while attenuating chaotic noise. Then we present a Markov
Vector Field (MVF)-driven motion-aware hyperedge generation method based on
motion state transition probabilities, thereby eliminating cross-target
spurious associations and providing critically topological priors while
capturing long-range dependencies between events. Finally, we propose a
Euclidean-Hyperbolic GCN to fuse the information locally aggregated and
globally hierarchically modeled in Euclidean and hyperbolic spaces,
respectively, to achieve hybrid event perception. Experimental results on event
perception tasks such as object detection and recognition validate the
effectiveness of our approach.

</details>


### [77] [Dual-Camera All-in-Focus Neural Radiance Fields](https://arxiv.org/abs/2504.16636)
*Xianrui Luo,Zijin Wu,Juewen Peng,Huiqiang Sun,Zhiguo Cao,Guosheng Lin*

Main category: cs.CV

TL;DR: 提出首个无需手动对焦的全焦点神经辐射场（NeRF）合成框架，利用智能手机双摄像头（主摄和超广角）实现高质量全焦点恢复。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法因固定对焦导致模糊和缺乏清晰参考，无法实现全焦点。智能手机双摄像头（主摄高分辨率、超广角大景深）为全焦点恢复提供可能。

Method: 通过空间扭曲和颜色匹配对齐双摄像头，利用可学习参数的离焦感知融合模块预测离焦图并融合图像。构建多视角数据集验证方法。

Result: 实验表明，DC-NeRF能生成高质量全焦点新视图，定量和定性优于基线方法，并支持景深调整应用（如重新对焦和分光镜）。

Conclusion: DC-NeRF首次实现无需手动对焦的全焦点NeRF合成，为景深相关应用提供新可能。

Abstract: We present the first framework capable of synthesizing the all-in-focus
neural radiance field (NeRF) from inputs without manual refocusing. Without
refocusing, the camera will automatically focus on the fixed object for all
views, and current NeRF methods typically using one camera fail due to the
consistent defocus blur and a lack of sharp reference. To restore the
all-in-focus NeRF, we introduce the dual-camera from smartphones, where the
ultra-wide camera has a wider depth-of-field (DoF) and the main camera
possesses a higher resolution. The dual camera pair saves the high-fidelity
details from the main camera and uses the ultra-wide camera's deep DoF as
reference for all-in-focus restoration. To this end, we first implement spatial
warping and color matching to align the dual camera, followed by a
defocus-aware fusion module with learnable defocus parameters to predict a
defocus map and fuse the aligned camera pair. We also build a multi-view
dataset that includes image pairs of the main and ultra-wide cameras in a
smartphone. Extensive experiments on this dataset verify that our solution,
termed DC-NeRF, can produce high-quality all-in-focus novel views and compares
favorably against strong baselines quantitatively and qualitatively. We further
show DoF applications of DC-NeRF with adjustable blur intensity and focal
plane, including refocusing and split diopter.

</details>


### [78] [RouteWinFormer: A Route-Window Transformer for Middle-range Attention in Image Restoration](https://arxiv.org/abs/2504.16637)
*Qifan Li,Tianyi Liang,Xingtao Wang,Xiaopeng Fan*

Main category: cs.CV

TL;DR: RouteWinFormer是一种基于窗口的Transformer模型，通过动态选择附近窗口进行注意力聚合，高效扩展感受野至中范围，适用于图像恢复任务。


<details>
  <summary>Details</summary>
Motivation: Transformer在图像恢复中因长距离像素依赖而受关注，但长距离注意力常带来不必要的计算开销，而实际退化与上下文通常是局部的。

Method: 提出RouteWinFormer，包含动态选择附近窗口的Route-Windows Attention Module，并结合多尺度结构正则化训练。

Result: 在9个数据集的多种图像恢复任务中，RouteWinFormer优于现有方法。

Conclusion: 中范围注意力足以满足图像恢复需求，RouteWinFormer通过高效建模中范围上下文，显著提升性能。

Abstract: Transformer models have recently garnered significant attention in image
restoration due to their ability to capture long-range pixel dependencies.
However, long-range attention often results in computational overhead without
practical necessity, as degradation and context are typically localized.
Normalized average attention distance across various degradation datasets shows
that middle-range attention is enough for image restoration. Building on this
insight, we propose RouteWinFormer, a novel window-based Transformer that
models middle-range context for image restoration. RouteWinFormer incorporates
Route-Windows Attnetion Module, which dynamically selects relevant nearby
windows based on regional similarity for attention aggregation, extending the
receptive field to a mid-range size efficiently. In addition, we introduce
Multi-Scale Structure Regularization during training, enabling the sub-scale of
the U-shaped network to focus on structural information, while the
original-scale learns degradation patterns based on generalized image structure
priors. Extensive experiments demonstrate that RouteWinFormer outperforms
state-of-the-art methods across 9 datasets in various image restoration tasks.

</details>


### [79] [SSLR: A Semi-Supervised Learning Method for Isolated Sign Language Recognition](https://arxiv.org/abs/2504.16640)
*Hasan Algafri,Hamzah Luqman,Sarah Alyami,Issam Laradji*

Main category: cs.CV

TL;DR: 提出了一种半监督学习方法（SSLR），通过伪标签标注未标记样本，解决了手语识别中标注数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 手语是听力障碍者的主要交流语言，但手语识别系统面临标注数据稀缺的挑战。

Method: 使用姿态信息表示手语手势，并作为Transformer模型的输入，采用伪标签方法标注未标记样本。

Result: 在WLASL-100数据集上，半监督学习方法在较少标注数据的情况下，性能优于全监督学习方法。

Conclusion: 半监督学习方法可以有效解决手语识别中标注数据稀缺的问题，并在性能上优于传统方法。

Abstract: Sign language is the primary communication language for people with disabling
hearing loss. Sign language recognition (SLR) systems aim to recognize sign
gestures and translate them into spoken language. One of the main challenges in
SLR is the scarcity of annotated datasets. To address this issue, we propose a
semi-supervised learning (SSL) approach for SLR (SSLR), employing a
pseudo-label method to annotate unlabeled samples. The sign gestures are
represented using pose information that encodes the signer's skeletal joint
points. This information is used as input for the Transformer backbone model
utilized in the proposed approach. To demonstrate the learning capabilities of
SSL across various labeled data sizes, several experiments were conducted using
different percentages of labeled data with varying numbers of classes. The
performance of the SSL approach was compared with a fully supervised
learning-based model on the WLASL-100 dataset. The obtained results of the SSL
model outperformed the supervised learning-based model with less labeled data
in many cases.

</details>


### [80] [WiFi based Human Fall and Activity Recognition using Transformer based Encoder Decoder and Graph Neural Networks](https://arxiv.org/abs/2504.16655)
*Younggeol Cho,Elisa Motta,Olivia Nocentini,Marta Lagomarsino,Andrea Merello,Marco Crepaldi,Arash Ajoudani*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的编码器-解码器网络（TED Net），用于从WiFi信道状态信息（CSI）中估计人体骨骼姿态，并结合定向图神经网络（DGNN）进行动作识别。实验表明，该方法在姿态估计和动作分类上优于现有方法，且在家庭环境（如老年人跌倒检测）中具有隐私保护优势。


<details>
  <summary>Details</summary>
Motivation: 人体姿态估计和动作识别在医疗监测、康复和辅助技术中具有重要作用，但基于视觉的方法可能引发隐私问题。WiFi信号提供了一种隐私友好的替代方案。

Method: 提出TED Net，结合卷积编码器和Transformer注意力机制从CSI信号中提取时空特征，并使用DGNN进行动作识别。

Result: TED Net在姿态估计上优于现有方法，DGNN的动作分类性能与基于RGB的系统相当，且在跌倒和非跌倒场景中表现稳健。

Conclusion: CSI驱动的人体骨骼估计在动作识别中具有潜力，尤其适用于家庭环境中的隐私敏感应用，如老年人跌倒检测。

Abstract: Human pose estimation and action recognition have received attention due to
their critical roles in healthcare monitoring, rehabilitation, and assistive
technologies. In this study, we proposed a novel architecture named Transformer
based Encoder Decoder Network (TED Net) designed for estimating human skeleton
poses from WiFi Channel State Information (CSI). TED Net integrates
convolutional encoders with transformer based attention mechanisms to capture
spatiotemporal features from CSI signals. The estimated skeleton poses were
used as input to a customized Directed Graph Neural Network (DGNN) for action
recognition. We validated our model on two datasets: a publicly available multi
modal dataset for assessing general pose estimation, and a newly collected
dataset focused on fall related scenarios involving 20 participants.
Experimental results demonstrated that TED Net outperformed existing approaches
in pose estimation, and that the DGNN achieves reliable action classification
using CSI based skeletons, with performance comparable to RGB based systems.
Notably, TED Net maintains robust performance across both fall and non fall
cases. These findings highlight the potential of CSI driven human skeleton
estimation for effective action recognition, particularly in home environments
such as elderly fall detection. In such settings, WiFi signals are often
readily available, offering a privacy preserving alternative to vision based
methods, which may raise concerns about continuous camera monitoring.

</details>


### [81] [Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning](https://arxiv.org/abs/2504.16656)
*Chris,Yichen Wei,Yi Peng,Xiaokun Wang,Weijie Qiu,Wei Shen,Tianyidan Xie,Jiangbo Pei,Jianhao Zhang,Yunzhuo Hao,Xuchen Song,Yang Liu,Yahui Zhou*

Main category: cs.CV

TL;DR: Skywork R1V2是一个多模态推理模型，通过混合强化学习范式解决了推理能力与泛化能力的平衡问题，并引入SSB机制提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统模型在复杂推理与广泛泛化之间的平衡问题，并提升训练效率。

Method: 采用混合强化学习范式结合奖励模型与规则策略，引入SSB机制优化训练样本选择。

Result: 在多个基准测试中表现优异，如OlympiadBench 62.6，AIME2024 79.0，LiveCodeBench 63.6，MMMU 74.0。

Conclusion: Skywork R1V2在性能上超越现有开源模型，缩小了与顶级专有系统的差距，并公开了模型权重以促进开放性和可复现性。

Abstract: We present Skywork R1V2, a next-generation multimodal reasoning model and a
major leap forward from its predecessor, Skywork R1V. At its core, R1V2
introduces a hybrid reinforcement learning paradigm that harmonizes
reward-model guidance with rule-based strategies, thereby addressing the
long-standing challenge of balancing sophisticated reasoning capabilities with
broad generalization. To further enhance training efficiency, we propose the
Selective Sample Buffer (SSB) mechanism, which effectively counters the
``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization
(GRPO) by prioritizing high-value samples throughout the optimization process.
Notably, we observe that excessive reinforcement signals can induce visual
hallucinations--a phenomenon we systematically monitor and mitigate through
calibrated reward thresholds throughout the training process. Empirical results
affirm the exceptional capability of R1V2, with benchmark-leading performances
such as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and
74.0 on MMMU. These results underscore R1V2's superiority over existing
open-source models and demonstrate significant progress in closing the
performance gap with premier proprietary systems, including Gemini 2.5 and
OpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to
promote openness and reproducibility
https://huggingface.co/Skywork/Skywork-R1V2-38B.

</details>


### [82] [A Time Series Dataset of NIR Spectra and RGB and NIR-HSI Images of the Barley Germination Process](https://arxiv.org/abs/2504.16658)
*Ole-Christian Galbo Engstrøm,Erik Schou Dreier,Birthe Møller Jespersen,Kim Steenstrup Pedersen*

Main category: cs.CV

TL;DR: 开源数据集包含2242个大麦籽粒的RGB和NIR-HSI图像、分割掩码及NIR光谱，用于研究发芽时间。


<details>
  <summary>Details</summary>
Motivation: 提供高质量数据集以支持基于RGB、NIR光谱或NIR-HSI的大麦籽粒发芽时间分析。

Method: 采集大麦籽粒在湿度暴露前后的图像，标记发芽状态，使用黑色滤纸背景简化分割。

Result: 数据集支持多种分析方法，包括RGB图像、NIR光谱和NIR-HSI的时序分析。

Conclusion: 该数据集为研究大麦籽粒发芽提供了多模态分析的基础。

Abstract: We provide an open-source dataset of RGB and NIR-HSI (near-infrared
hyperspectral imaging) images with associated segmentation masks and NIR
spectra of 2242 individual malting barley kernels. We imaged every kernel
pre-exposure to moisture and every 24 hours after exposure to moisture for five
consecutive days. Every barley kernel was labeled as germinated or not
germinated during each image acquisition. The barley kernels were imaged with
black filter paper as the background, facilitating straight-forward intensity
threshold-based segmentation, e.g., by Otsu's method. This dataset facilitates
time series analysis of germination time for barley kernels using either RGB
image analysis, NIR spectral analysis, NIR-HSI analysis, or a combination
hereof.

</details>


### [83] [A Diff-Attention Aware State Space Fusion Model for Remote Sensing Classification](https://arxiv.org/abs/2504.16665)
*Wenping Ma,Boyou Xue,Mengru Ma,Chuang Chen,Hekai Zhang,Hao Zhu*

Main category: cs.CV

TL;DR: 提出了一种基于选择性状态空间模型的多模态遥感图像分类方法，通过跨模态差异注意力模块分离共同和主导特征，并结合注意力感知线性融合模块优化特征融合。


<details>
  <summary>Details</summary>
Motivation: 多光谱和全色图像具有相似信息和各自优势，但融合阶段存在特征冗余问题，需分离相似信息并保留各自优势。

Method: 设计跨模态差异注意力模块（CMDA-Module）分离特征，空间保留视觉Mamba（SPVM）捕获局部特征，提出注意力感知线性融合模块（AALF-Module）实现像素级融合。

Result: 实验表明，该方法优于其他替代方法。

Conclusion: DAS2F-Model有效分离和融合多模态图像特征，提升了分类性能。

Abstract: Multispectral (MS) and panchromatic (PAN) images describe the same land
surface, so these images not only have their own advantages, but also have a
lot of similar information. In order to separate these similar information and
their respective advantages, reduce the feature redundancy in the fusion stage.
This paper introduces a diff-attention aware state space fusion model
(DAS2F-Model) for multimodal remote sensing image classification. Based on the
selective state space model, a cross-modal diff-attention module (CMDA-Module)
is designed to extract and separate the common features and their respective
dominant features of MS and PAN images. Among this, space preserving visual
mamba (SPVM) retains image spatial features and captures local features by
optimizing visual mamba's input reasonably. Considering that features in the
fusion stage will have large semantic differences after feature separation and
simple fusion operations struggle to effectively integrate these significantly
different features, an attention-aware linear fusion module (AALF-Module) is
proposed. It performs pixel-wise linear fusion by calculating influence
coefficients. This mechanism can fuse features with large semantic differences
while keeping the feature size unchanged. Empirical evaluations indicate that
the presented method achieves better results than alternative approaches. The
relevant code can be found at:https://github.com/AVKSKVL/DAS-F-Model

</details>


### [84] [SemanticSugarBeets: A Multi-Task Framework and Dataset for Inspecting Harvest and Storage Characteristics of Sugar Beets](https://arxiv.org/abs/2504.16684)
*Gerardus Croonen,Andreas Trondl,Julia Simon,Daniel Steininger*

Main category: cs.CV

TL;DR: 论文提出了一种基于单目RGB图像的两阶段方法，用于检测、语义分割和质量估计收获后和储存后的甜菜，并提供了一个高质量标注数据集。实验表明，该方法在检测和分割任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 甜菜在储存过程中因微生物和多余植被导致糖分损失，自动视觉检测可提高糖生产链的效率和质量保证。

Method: 提出了一种两阶段方法，包括检测、语义分割和质量估计，并评估了不同图像尺寸、模型架构和环境条件的影响。

Result: 实验结果显示，甜菜检测的mAP50-95达到98.8，最佳分割模型的mIoU为64.0。

Conclusion: 该方法在甜菜检测和分割任务上表现优异，有望提升糖生产链的效率和质量控制。

Abstract: While sugar beets are stored prior to processing, they lose sugar due to
factors such as microorganisms present in adherent soil and excess vegetation.
Their automated visual inspection promises to aide in quality assurance and
thereby increase efficiency throughout the processing chain of sugar
production. In this work, we present a novel high-quality annotated dataset and
two-stage method for the detection, semantic segmentation and mass estimation
of post-harvest and post-storage sugar beets in monocular RGB images. We
conduct extensive ablation experiments for the detection of sugar beets and
their fine-grained semantic segmentation regarding damages, rot, soil adhesion
and excess vegetation. For these tasks, we evaluate multiple image sizes, model
architectures and encoders, as well as the influence of environmental
conditions. Our experiments show an mAP50-95 of 98.8 for sugar-beet detection
and an mIoU of 64.0 for the best-performing segmentation model.

</details>


### [85] [Energy-Based Pseudo-Label Refining for Source-free Domain Adaptation](https://arxiv.org/abs/2504.16692)
*Xinru Meng,Han Sun,Jiamei Liu,Ningzhong Liu,Huiyu Zhou*

Main category: cs.CV

TL;DR: EBPR方法通过能量分数生成伪标签，并利用全局和类别能量阈值过滤噪声，结合对比学习提升特征判别性，显著优于现有SFDA方法。


<details>
  <summary>Details</summary>
Motivation: 解决无源域适应（SFDA）中伪标签噪声导致的负迁移问题。

Method: 提出基于能量的伪标签细化（EBPR），利用能量分数生成伪标签，并通过全局和类别能量阈值过滤噪声，结合对比学习优化特征。

Result: 在Office-31、Office-Home和VisDA-C数据集上表现优于现有方法。

Conclusion: EBPR通过能量阈值和对比学习有效减少伪标签噪声，提升了SFDA性能。

Abstract: Source-free domain adaptation (SFDA), which involves adapting models without
access to source data, is both demanding and challenging. Existing SFDA
techniques typically rely on pseudo-labels generated from confidence levels,
leading to negative transfer due to significant noise. To tackle this problem,
Energy-Based Pseudo-Label Refining (EBPR) is proposed for SFDA. Pseudo-labels
are created for all sample clusters according to their energy scores. Global
and class energy thresholds are computed to selectively filter pseudo-labels.
Furthermore, a contrastive learning strategy is introduced to filter difficult
samples, aligning them with their augmented versions to learn more
discriminative features. Our method is validated on the Office-31, Office-Home,
and VisDA-C datasets, consistently finding that our model outperformed
state-of-the-art methods.

</details>


### [86] [PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum Learning](https://arxiv.org/abs/2504.16722)
*Yingjie Xi,Jian Jun Zhang,Xiaosong Yang*

Main category: cs.CV

TL;DR: ProMoGen是一种新颖的运动生成框架，结合轨迹引导和稀疏锚点控制，实现更可控、高保真的人类动作合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成符合用户意图的人类动作时存在局限性，如文本方法难以描述复杂动作，轨迹方法无法生成精确动作，锚点方法仅支持简单动作。

Method: ProMoGen通过解耦轨迹和锚点控制，支持双控制和单控制范式，并引入SAP-CL课程学习策略稳定训练。

Result: 实验表明，ProMoGen能生成生动多样的动作，显著优于现有方法。

Conclusion: ProMoGen成功整合了结构化引导和个性化动作，为复杂动作合成提供了高效解决方案。

Abstract: In computer animation, game design, and human-computer interaction,
synthesizing human motion that aligns with user intent remains a significant
challenge. Existing methods have notable limitations: textual approaches offer
high-level semantic guidance but struggle to describe complex actions
accurately; trajectory-based techniques provide intuitive global motion
direction yet often fall short in generating precise or customized character
movements; and anchor poses-guided methods are typically confined to synthesize
only simple motion patterns. To generate more controllable and precise human
motions, we propose \textbf{ProMoGen (Progressive Motion Generation)}, a novel
framework that integrates trajectory guidance with sparse anchor motion
control. Global trajectories ensure consistency in spatial direction and
displacement, while sparse anchor motions only deliver precise action guidance
without displacement. This decoupling enables independent refinement of both
aspects, resulting in a more controllable, high-fidelity, and sophisticated
motion synthesis. ProMoGen supports both dual and single control paradigms
within a unified training process. Moreover, we recognize that direct learning
from sparse motions is inherently unstable, we introduce \textbf{SAP-CL (Sparse
Anchor Posture Curriculum Learning)}, a curriculum learning strategy that
progressively adjusts the number of anchors used for guidance, thereby enabling
more precise and stable convergence. Extensive experiments demonstrate that
ProMoGen excels in synthesizing vivid and diverse motions guided by predefined
trajectory and arbitrary anchor frames. Our approach seamlessly integrates
personalized motion with structured guidance, significantly outperforming
state-of-the-art methods across multiple control scenarios.

</details>


### [87] [Detecting and Understanding Hateful Contents in Memes Through Captioning and Visual Question-Answering](https://arxiv.org/abs/2504.16723)
*Ali Anaissi,Junaid Akram,Kunal Chaturvedi,Ali Braytee*

Main category: cs.CV

TL;DR: 提出了一种多模态仇恨内容检测框架，结合OCR、字幕生成、子标签分类、RAG和VQA技术，显著优于传统单模态和多模态模型。


<details>
  <summary>Details</summary>
Motivation: 因仇恨内容常通过多模态模因传播，传统单模态检测方法难以捕捉其隐含信号，需开发更有效的多模态检测方法。

Method: 整合OCR提取文本、字幕生成描述图像、子标签分类细化仇恨内容、RAG检索上下文、VQA分析符号线索，形成多模态检测框架。

Result: 在Facebook Hateful Memes数据集上，该框架在准确率和AUC-ROC上均优于传统模型。

Conclusion: 多模态框架能有效检测隐含仇恨内容，为复杂模因分析提供了新思路。

Abstract: Memes are widely used for humor and cultural commentary, but they are
increasingly exploited to spread hateful content. Due to their multimodal
nature, hateful memes often evade traditional text-only or image-only detection
systems, particularly when they employ subtle or coded references. To address
these challenges, we propose a multimodal hate detection framework that
integrates key components: OCR to extract embedded text, captioning to describe
visual content neutrally, sub-label classification for granular categorization
of hateful content, RAG for contextually relevant retrieval, and VQA for
iterative analysis of symbolic and contextual cues. This enables the framework
to uncover latent signals that simpler pipelines fail to detect. Experimental
results on the Facebook Hateful Memes dataset reveal that the proposed
framework exceeds the performance of unimodal and conventional multimodal
models in both accuracy and AUC-ROC.

</details>


### [88] [V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations](https://arxiv.org/abs/2504.16727)
*Zhiyuan Fan,Yumeng Wang,Sandeep Polisetty,Yi R.,Fung*

Main category: cs.CV

TL;DR: V$^2$R-Bench是一个评估大型视觉语言模型（LVLMs）对视觉变化鲁棒性的基准框架，揭示了模型在简单任务中的脆弱性，并指出其源于架构缺陷。


<details>
  <summary>Details</summary>
Motivation: 探索LVLMs对视觉变化（如位置、尺度、方向等）的鲁棒性，填补现有研究的空白。

Method: 提出V$^2$R-Bench框架，包括自动化数据集生成和评估指标，对21个LVLMs进行全面测试。

Result: 发现模型在视觉变化下表现脆弱，存在位置偏差和人类视觉敏锐度阈值，问题源于架构缺陷和多模态对齐不足。

Conclusion: 未来LVLM设计需创新架构以提升鲁棒性。

Abstract: Large Vision Language Models (LVLMs) excel in various vision-language tasks.
Yet, their robustness to visual variations in position, scale, orientation, and
context that objects in natural scenes inevitably exhibit due to changes in
viewpoint and environment remains largely underexplored. To bridge this gap, we
introduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating
Visual Variation Robustness of LVLMs, which encompasses automated evaluation
dataset generation and principled metrics for thorough robustness assessment.
Through extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability
to visual variations, in which even advanced models that excel at complex
vision-language tasks significantly underperform on simple tasks such as object
recognition. Interestingly, these models exhibit a distinct visual position
bias that contradicts theories of effective receptive fields, and demonstrate a
human-like visual acuity threshold. To identify the source of these
vulnerabilities, we present a systematic framework for component-level
analysis, featuring a novel visualization approach for aligned visual features.
Results show that these vulnerabilities stem from error accumulation in the
pipeline architecture and inadequate multimodal alignment. Complementary
experiments with synthetic data further demonstrate that these limitations are
fundamentally architectural deficiencies, scoring the need for architectural
innovations in future LVLM designs.

</details>


### [89] [Prompt-Tuning SAM: From Generalist to Specialist with only 2048 Parameters and 16 Training Images](https://arxiv.org/abs/2504.16739)
*Tristan Piater,Björn Barz,Alexander Freytag*

Main category: cs.CV

TL;DR: PTSAM通过仅调整少量参数（2048个），将SAM模型优化为特定任务的专家，显著提升了在非自然领域（如显微图像）的性能，且仅需16张标注图像即可训练。


<details>
  <summary>Details</summary>
Motivation: SAM在非自然领域（如显微图像）表现不佳，且需要精确提示，不适合自动化生物医学应用。

Method: 提出PTSAM方法，采用参数高效的提示调优技术，仅调整SAM的掩码解码器和图像编码器。

Result: PTSAM性能与现有技术相当，但参数少2000倍；调整图像编码器后，分割精度提升18%。

Conclusion: PTSAM适用于训练数据有限和领域迁移的场景，显著提升了SAM在非自然领域的实用性。

Abstract: The Segment Anything Model (SAM) is widely used for segmenting a diverse
range of objects in natural images from simple user prompts like points or
bounding boxes. However, SAM's performance decreases substantially when applied
to non-natural domains like microscopic imaging. Furthermore, due to SAM's
interactive design, it requires a precise prompt for each image and object,
which is unfeasible in many automated biomedical applications. Previous
solutions adapt SAM by training millions of parameters via fine-tuning large
parts of the model or of adapter layers. In contrast, we show that as little as
2,048 additional parameters are sufficient for turning SAM into a use-case
specialist for a certain downstream task. Our novel PTSAM (prompt-tuned SAM)
method uses prompt-tuning, a parameter-efficient fine-tuning technique, to
adapt SAM for a specific task. We validate the performance of our approach on
multiple microscopic and one medical dataset. Our results show that
prompt-tuning only SAM's mask decoder already leads to a performance on-par
with state-of-the-art techniques while requiring roughly 2,000x less trainable
parameters. For addressing domain gaps, we find that additionally prompt-tuning
SAM's image encoder is beneficial, further improving segmentation accuracy by
up to 18% over state-of-the-art results. Since PTSAM can be reliably trained
with as little as 16 annotated images, we find it particularly helpful for
applications with limited training data and domain shifts.

</details>


### [90] [Gaussian Splatting is an Effective Data Generator for 3D Object Detection](https://arxiv.org/abs/2504.16740)
*Farhad G. Zanjani,Davide Abati,Auke Wiggers,Dimitris Kalatzis,Jens Petersen,Hong Cai,Amirhossein Habibian*

Main category: cs.CV

TL;DR: 研究提出了一种基于高斯溅射的3D重建方法，用于自动驾驶场景中的3D物体放置，通过显式几何变换提升数据增强效果，优于现有扩散方法。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过3D数据增强提升自动驾驶中3D物体检测的性能，尤其是物理合理性和几何多样性对检测效果的影响。

Method: 利用高斯溅射技术进行3D重建，直接在3D空间中放置物体，并施加显式几何变换。

Result: 实验表明，该方法显著提升了3D物体检测性能，几何多样性比外观多样性更重要，且生成困难样本效果不佳。

Conclusion: 显式几何变换和物理合理性是3D数据增强的关键，几何多样性对检测性能的提升效果显著。

Abstract: We investigate data augmentation for 3D object detection in autonomous
driving. We utilize recent advancements in 3D reconstruction based on Gaussian
Splatting for 3D object placement in driving scenes. Unlike existing
diffusion-based methods that synthesize images conditioned on BEV layouts, our
approach places 3D objects directly in the reconstructed 3D space with
explicitly imposed geometric transformations. This ensures both the physical
plausibility of object placement and highly accurate 3D pose and position
annotations.
  Our experiments demonstrate that even by integrating a limited number of
external 3D objects into real scenes, the augmented data significantly enhances
3D object detection performance and outperforms existing diffusion-based 3D
augmentation for object detection. Extensive testing on the nuScenes dataset
reveals that imposing high geometric diversity in object placement has a
greater impact compared to the appearance diversity of objects. Additionally,
we show that generating hard examples, either by maximizing detection loss or
imposing high visual occlusion in camera images, does not lead to more
efficient 3D data augmentation for camera-based 3D object detection in
autonomous driving.

</details>


### [91] [Feature Mixing Approach for Detecting Intraoperative Adverse Events in Laparoscopic Roux-en-Y Gastric Bypass Surgery](https://arxiv.org/abs/2504.16749)
*Rupak Bose,Chinedu Innocent Nwoye,Jorge Lazo,Joël Lukas Lavanchy,Nicolas Padoy*

Main category: cs.CV

TL;DR: BetaMixer是一种新型深度学习模型，通过Beta分布混合方法解决术中不良事件（IAEs）检测中的数据集不平衡问题，实现了精确的严重性回归和分类。


<details>
  <summary>Details</summary>
Motivation: 术中不良事件（IAEs）如出血或热损伤，若未被检测到可能导致严重的术后并发症，但其罕见性导致数据集高度不平衡，为AI检测带来挑战。

Method: BetaMixer采用Beta分布采样增强少数类，通过生成方法对齐特征空间，并使用Transformer实现分类和严重性回归。

Result: 在MultiBypass140数据集上，BetaMixer的加权F1得分为0.76，召回率为0.81，PPV为0.73，NPV为0.84，表现优异。

Conclusion: BetaMixer通过结合Beta分布采样、特征混合和生成建模，为临床环境中的IAE检测和量化提供了稳健解决方案。

Abstract: Intraoperative adverse events (IAEs), such as bleeding or thermal injury, can
lead to severe postoperative complications if undetected. However, their rarity
results in highly imbalanced datasets, posing challenges for AI-based detection
and severity quantification. We propose BetaMixer, a novel deep learning model
that addresses these challenges through a Beta distribution-based mixing
approach, converting discrete IAE severity scores into continuous values for
precise severity regression (0-5 scale). BetaMixer employs Beta
distribution-based sampling to enhance underrepresented classes and regularizes
intermediate embeddings to maintain a structured feature space. A generative
approach aligns the feature space with sampled IAE severity, enabling robust
classification and severity regression via a transformer. Evaluated on the
MultiBypass140 dataset, which we extended with IAE labels, BetaMixer achieves a
weighted F1 score of 0.76, recall of 0.81, PPV of 0.73, and NPV of 0.84,
demonstrating strong performance on imbalanced data. By integrating Beta
distribution-based sampling, feature mixing, and generative modeling, BetaMixer
offers a robust solution for IAE detection and quantification in clinical
settings.

</details>


### [92] [Tri-FusionNet: Enhancing Image Description Generation with Transformer-based Fusion Network and Dual Attention Mechanism](https://arxiv.org/abs/2504.16761)
*Lakshita Agarwal,Bindu Verma*

Main category: cs.CV

TL;DR: Tri-FusionNet是一种结合ViT、RoBERTa和CLIP的图像描述生成模型，通过双注意力机制提升性能，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 图像描述生成对可访问性和AI视觉内容理解至关重要，深度学习的最新进展为这一领域提供了新机遇。

Method: 模型整合了ViT编码器（带双注意力）、RoBERTa解码器和CLIP模块，通过对比学习对齐视觉与文本数据。

Result: 在Flickr30k、Flickr8k和MS-COCO数据集上取得高BLEU、CIDEr、METEOR和ROUGE-L分数。

Conclusion: Tri-FusionNet能生成高质量、上下文丰富的图像描述，验证了其有效性。

Abstract: Image description generation is essential for accessibility and AI
understanding of visual content. Recent advancements in deep learning have
significantly improved natural language processing and computer vision. In this
work, we propose Tri-FusionNet, a novel image description generation model that
integrates transformer modules: a Vision Transformer (ViT) encoder module with
dual-attention mechanism, a Robustly Optimized BERT Approach (RoBERTa) decoder
module, and a Contrastive Language-Image Pre-Training (CLIP) integrating
module. The ViT encoder, enhanced with dual attention, focuses on relevant
spatial regions and linguistic context, improving image feature extraction. The
RoBERTa decoder is employed to generate precise textual descriptions. CLIP's
integrating module aligns visual and textual data through contrastive learning,
ensuring effective combination of both modalities. This fusion of ViT, RoBERTa,
and CLIP, along with dual attention, enables the model to produce more
accurate, contextually rich, and flexible descriptions. The proposed framework
demonstrated competitive performance on the Flickr30k and Flickr8k datasets,
with BLEU scores ranging from 0.767 to 0.456 and 0.784 to 0.479, CIDEr scores
of 1.679 and 1.483, METEOR scores of 0.478 and 0.358, and ROUGE-L scores of
0.567 and 0.789, respectively. On MS-COCO, the framework obtained BLEU scores
of 0.893 (B-1), 0.821 (B-2), 0.794 (B-3), and 0.725 (B-4). The results
demonstrate the effectiveness of Tri-FusionNet in generating high-quality image
descriptions.

</details>


### [93] [Towards Explainable AI: Multi-Modal Transformer for Video-based Image Description Generation](https://arxiv.org/abs/2504.16788)
*Lakshita Agarwal,Bindu Verma*

Main category: cs.CV

TL;DR: 论文提出了一种结合文本和视觉模态的框架，用于从视频数据生成自然语言描述，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 理解和分析视频动作对于智能监控和自主系统等应用至关重要，需要生成上下文相关的描述。

Method: 使用ResNet50提取视频帧的视觉特征，通过GPT-2的编码器-解码器模型结合多头部自注意力和交叉注意力技术对齐文本与视觉表示。

Result: 在BDD-X和MSVD数据集上，BLEU-4、CIDEr、METEOR和ROUGE-L得分均优于传统方法。

Conclusion: 该研究通过生成高质量描述，提升了可解释AI的实用性，推动了实际应用的发展。

Abstract: Understanding and analyzing video actions are essential for producing
insightful and contextualized descriptions, especially for video-based
applications like intelligent monitoring and autonomous systems. The proposed
work introduces a novel framework for generating natural language descriptions
from video datasets by combining textual and visual modalities. The suggested
architecture makes use of ResNet50 to extract visual features from video frames
that are taken from the Microsoft Research Video Description Corpus (MSVD), and
Berkeley DeepDrive eXplanation (BDD-X) datasets. The extracted visual
characteristics are converted into patch embeddings and then run through an
encoder-decoder model based on Generative Pre-trained Transformer-2 (GPT-2). In
order to align textual and visual representations and guarantee high-quality
description production, the system uses multi-head self-attention and
cross-attention techniques. The model's efficacy is demonstrated by performance
evaluation using BLEU (1-4), CIDEr, METEOR, and ROUGE-L. The suggested
framework outperforms traditional methods with BLEU-4 scores of 0.755 (BDD-X)
and 0.778 (MSVD), CIDEr scores of 1.235 (BDD-X) and 1.315 (MSVD), METEOR scores
of 0.312 (BDD-X) and 0.329 (MSVD), and ROUGE-L scores of 0.782 (BDD-X) and
0.795 (MSVD). By producing human-like, contextually relevant descriptions,
strengthening interpretability, and improving real-world applications, this
research advances explainable AI.

</details>


### [94] [Decoupled Global-Local Alignment for Improving Compositional Understanding](https://arxiv.org/abs/2504.16801)
*Xiaoxing Hu,Kaicheng Yang,Jun Wang,Haoran Xu,Ziyong Feng,Yupei Wang*

Main category: cs.CV

TL;DR: DeGLA框架通过解耦全局-局部对齐和改进对比损失，提升了CLIP模型的组合理解能力，同时保留其通用能力。


<details>
  <summary>Details</summary>
Motivation: CLIP的全局对比学习限制了其对组合概念（如关系和属性）的理解能力，现有方法在改进组合理解时会损害模型的通用能力。

Method: 提出DeGLA框架，结合自蒸馏机制和基于LLM的高质量负样本生成，设计了IGC和TGC损失函数。

Result: 在VALSE、SugarCrepe和ARO基准上平均提升3.5%，在11个零样本分类任务上平均提升13.0%。

Conclusion: DeGLA在提升组合理解的同时有效保留了模型的通用能力，实验证明了其有效性。

Abstract: Contrastive Language-Image Pre-training (CLIP) has achieved success on
multiple downstream tasks by aligning image and text modalities. However, the
nature of global contrastive learning limits CLIP's ability to comprehend
compositional concepts, such as relations and attributes. Although recent
studies employ global hard negative samples to improve compositional
understanding, these methods significantly compromise the model's inherent
general capabilities by forcibly distancing textual negative samples from
images in the embedding space. To overcome this limitation, we introduce a
Decoupled Global-Local Alignment (DeGLA) framework that improves compositional
understanding while substantially mitigating losses in general capabilities. To
optimize the retention of the model's inherent capabilities, we incorporate a
self-distillation mechanism within the global alignment process, aligning the
learnable image-text encoder with a frozen teacher model derived from an
exponential moving average. Under the constraint of self-distillation, it
effectively mitigates the catastrophic forgetting of pretrained knowledge
during fine-tuning. To improve compositional understanding, we first leverage
the in-context learning capability of Large Language Models (LLMs) to construct
about 2M high-quality negative captions across five types. Subsequently, we
propose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)
loss to enhance vision-language compositionally. Extensive experimental results
demonstrate the effectiveness of the DeGLA framework. Compared to previous
state-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across
the VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average
performance improvement of 13.0% on zero-shot classification tasks across
eleven datasets. Our code will be released at
https://github.com/xiaoxing2001/DeGLA

</details>


### [95] [A Low-Cost Photogrammetry System for 3D Plant Modeling and Phenotyping](https://arxiv.org/abs/2504.16840)
*Joe Hrzich,Michael A. Beck,Christopher P. Bidinosti,Christopher J. Henry,Kalhari Manawasinghe,Karen Tanino*

Main category: cs.CV

TL;DR: 提出了一种低成本的开源摄影测量系统，用于3D植物建模和表型分析，以小麦为例展示了如何从点云中计算多种表型特征。


<details>
  <summary>Details</summary>
Motivation: 开发一种低成本、开源的系统，用于植物3D建模和表型分析，以解决传统手动测量方法的繁琐问题。

Method: 采用运动恢复结构（SfM）方法，通过点云重建植物的3D模型，并从中计算表型特征。

Result: 系统能够轻松测量植物高度、半径等标准特征，以及叶片角度和凸包等复杂特征，并可用于小麦冠层结构的分类。

Conclusion: 该系统为植物表型分析提供了一种高效、低成本的解决方案，尤其适用于复杂特征的测量和冠层结构分类。

Abstract: We present an open-source, low-cost photogrammetry system for 3D plant
modeling and phenotyping. The system uses a structure-from-motion approach to
reconstruct 3D representations of the plants via point clouds. Using wheat as
an example, we demonstrate how various phenotypic traits can be computed easily
from the point clouds. These include standard measurements such as plant height
and radius, as well as features that would be more cumbersome to measure by
hand, such as leaf angles and convex hull. We further demonstrate the utility
of the system through the investigation of specific metrics that may yield
objective classifications of erectophile versus planophile wheat canopy
architectures.

</details>


### [96] [Hyperspectral Vision Transformers for Greenhouse Gas Estimations from Space](https://arxiv.org/abs/2504.16851)
*Ruben Gonzalez Avilés,Linus Scheibenreif,Nassim Ait Ali Braham,Benedikt Blumenstiel,Thomas Brunschwiler,Ranjini Guruprasad,Damian Borth,Conrad Albrecht,Paolo Fraccaro,Devyani Lambhate,Johannes Jakubik*

Main category: cs.CV

TL;DR: 提出了一种光谱变换器模型，通过多光谱数据合成高光谱数据，以弥补高光谱覆盖范围有限和多光谱光谱细节不足的问题，从而提高温室气体监测的准确性。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像在温室气体监测中具有潜力，但受限于空间覆盖和重访频率；多光谱成像覆盖广但光谱细节不足。

Method: 采用预训练的波段掩码自编码器，并在时空对齐的多光谱-高光谱图像对上微调，生成合成高光谱数据。

Result: 合成的高光谱数据保留了多光谱的空间和时间优势，同时提高了温室气体预测的准确性。

Conclusion: 该方法通过自监督深度学习结合高光谱和多光谱系统的优势，为大气监测提供了新思路。

Abstract: Hyperspectral imaging provides detailed spectral information and holds
significant potential for monitoring of greenhouse gases (GHGs). However, its
application is constrained by limited spatial coverage and infrequent revisit
times. In contrast, multispectral imaging offers broader spatial and temporal
coverage but often lacks the spectral detail that can enhance GHG detection. To
address these challenges, this study proposes a spectral transformer model that
synthesizes hyperspectral data from multispectral inputs. The model is
pre-trained via a band-wise masked autoencoder and subsequently fine-tuned on
spatio-temporally aligned multispectral-hyperspectral image pairs. The
resulting synthetic hyperspectral data retain the spatial and temporal benefits
of multispectral imagery and improve GHG prediction accuracy relative to using
multispectral data alone. This approach effectively bridges the trade-off
between spectral resolution and coverage, highlighting its potential to advance
atmospheric monitoring by combining the strengths of hyperspectral and
multispectral systems with self-supervised deep learning.

</details>


### [97] [High-Quality Cloud-Free Optical Image Synthesis Using Multi-Temporal SAR and Contaminated Optical Data](https://arxiv.org/abs/2504.16870)
*Chenxi Duan*

Main category: cs.CV

TL;DR: CRSynthNet是一种新型图像合成网络，通过创新的DownUp Block和Fusion Attention模块，显著提升了云覆盖下光学数据合成的准确性。实验证明其在结构细节恢复、光谱一致性保持和视觉效果上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决卫星重访周期长和云覆盖导致的光学数据缺失问题，为遥感应用提供关键数据支持。

Method: 提出CRSynthNet网络，结合DownUp Block和Fusion Attention模块，优化图像合成。

Result: PSNR为26.978，SSIM为0.648，RMSE为0.050，效果显著优于对比方法。同时创建了TCSEN12数据集。

Conclusion: CRSynthNet为光学卫星图像合成提供了实用方法和宝贵资源。

Abstract: Addressing gaps caused by cloud cover and the long revisit cycle of
satellites is vital for providing essential data to support remote sensing
applications. This paper tackles the challenges of missing optical data
synthesis, particularly in complex scenarios with cloud cover. We propose
CRSynthNet, a novel image synthesis network that incorporates innovative
designed modules such as the DownUp Block and Fusion Attention to enhance
accuracy. Experimental results validate the effectiveness of CRSynthNet,
demonstrating substantial improvements in restoring structural details,
preserving spectral consist, and achieving superior visual effects that far
exceed those produced by comparison methods. It achieves quantitative
improvements across multiple metrics: a peak signal-to-noise ratio (PSNR) of
26.978, a structural similarity index measure (SSIM) of 0.648, and a root mean
square error (RMSE) of 0.050. Furthermore, this study creates the TCSEN12
dataset, a valuable resource specifically designed to address cloud cover
challenges in missing optical data synthesis study. The dataset uniquely
includes cloud-covered images and leverages earlier image to predict later
image, offering a realistic representation of real-world scenarios. This study
offer practical method and valuable resources for optical satellite image
synthesis task.

</details>


### [98] [BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation](https://arxiv.org/abs/2504.16907)
*Ruotong Wang,Mingli Zhu,Jiarong Ou,Rui Chen,Xin Tao,Pengfei Wan,Baoyuan Wu*

Main category: cs.CV

TL;DR: BadVideo是首个针对文本到视频（T2V）生成模型的后门攻击框架，利用视频中的冗余信息嵌入恶意内容，具有高隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 探索T2V生成模型的对抗性漏洞，揭示其潜在风险。

Method: 采用时空组合和动态元素变换策略，将恶意目标与用户文本指令无缝结合。

Result: 实验表明BadVideo攻击成功率高，且不影响原始语义和干净输入的性能。

Conclusion: 揭示了T2V模型的对抗性漏洞，呼吁关注潜在风险和滥用问题。

Abstract: Text-to-video (T2V) generative models have rapidly advanced and found
widespread applications across fields like entertainment, education, and
marketing. However, the adversarial vulnerabilities of these models remain
rarely explored. We observe that in T2V generation tasks, the generated videos
often contain substantial redundant information not explicitly specified in the
text prompts, such as environmental elements, secondary objects, and additional
details, providing opportunities for malicious attackers to embed hidden
harmful content. Exploiting this inherent redundancy, we introduce BadVideo,
the first backdoor attack framework tailored for T2V generation. Our attack
focuses on designing target adversarial outputs through two key strategies: (1)
Spatio-Temporal Composition, which combines different spatiotemporal features
to encode malicious information; (2) Dynamic Element Transformation, which
introduces transformations in redundant elements over time to convey malicious
information. Based on these strategies, the attacker's malicious target
seamlessly integrates with the user's textual instructions, providing high
stealthiness. Moreover, by exploiting the temporal dimension of videos, our
attack successfully evades traditional content moderation systems that
primarily analyze spatial information within individual frames. Extensive
experiments demonstrate that BadVideo achieves high attack success rates while
preserving original semantics and maintaining excellent performance on clean
inputs. Overall, our work reveals the adversarial vulnerability of T2V models,
calling attention to potential risks and misuse. Our project page is at
https://wrt2000.github.io/BadVideo2025/.

</details>


### [99] [DreamO: A Unified Framework for Image Customization](https://arxiv.org/abs/2504.16915)
*Chong Mou,Yanze Wu,Wenxu Wu,Zinan Guo,Pengze Zhang,Yufeng Cheng,Yiming Luo,Fei Ding,Shiwen Zhang,Xinghui Li,Mengtian Li,Songtao Zhao,Jian Zhang,Qian He,Xinglong Wu*

Main category: cs.CV

TL;DR: DreamO是一个统一的图像定制框架，支持多种任务和条件集成，采用扩散变换器（DiT）和渐进训练策略，实验证明其高效灵活。


<details>
  <summary>Details</summary>
Motivation: 现有图像定制方法多为特定任务设计，缺乏通用性，DreamO旨在解决这一问题。

Method: 基于扩散变换器（DiT）框架，引入特征路由约束和占位符策略，采用三阶段渐进训练。

Result: 实验表明DreamO能高质量完成多种图像定制任务，并灵活整合不同控制条件。

Conclusion: DreamO为图像定制提供了一个统一且高效的解决方案。

Abstract: Recently, extensive research on image customization (e.g., identity, subject,
style, background, etc.) demonstrates strong customization capabilities in
large-scale generative models. However, most approaches are designed for
specific tasks, restricting their generalizability to combine different types
of condition. Developing a unified framework for image customization remains an
open challenge. In this paper, we present DreamO, an image customization
framework designed to support a wide range of tasks while facilitating seamless
integration of multiple conditions. Specifically, DreamO utilizes a diffusion
transformer (DiT) framework to uniformly process input of different types.
During training, we construct a large-scale training dataset that includes
various customization tasks, and we introduce a feature routing constraint to
facilitate the precise querying of relevant information from reference images.
Additionally, we design a placeholder strategy that associates specific
placeholders with conditions at particular positions, enabling control over the
placement of conditions in the generated results. Moreover, we employ a
progressive training strategy consisting of three stages: an initial stage
focused on simple tasks with limited data to establish baseline consistency, a
full-scale training stage to comprehensively enhance the customization
capabilities, and a final quality alignment stage to correct quality biases
introduced by low-quality data. Extensive experiments demonstrate that the
proposed DreamO can effectively perform various image customization tasks with
high quality and flexibly integrate different types of control conditions.

</details>


### [100] [Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light](https://arxiv.org/abs/2504.16922)
*Ali Hassani,Fengzhe Zhou,Aditya Kane,Jiannan Huang,Chieh-Yun Chen,Min Shi,Steven Walton,Markus Hoehnerbach,Vijay Thakkar,Michael Isaev,Qinsheng Zhang,Bing Xu,Haicheng Wu,Wen-mei Hwu,Ming-Yu Liu,Humphrey Shi*

Main category: cs.CV

TL;DR: 论文研究了基于局部性的稀疏注意力机制，提出了广义邻域注意力（GNA），并通过模拟器和硬件实现验证了其性能提升。


<details>
  <summary>Details</summary>
Motivation: 稀疏注意力机制在速度和复杂度上未能超越自注意力基线，而许多先进模型受限于注意力计算的高复杂度。

Method: 引入GNA描述多种稀疏注意力模式，设计模拟器预测性能上限，并在NVIDIA Blackwell架构上实现高效内核。

Result: GNA在理想块稀疏情况下实现理论最大加速，FP16下有效利用率为1.3 petaFLOPs/秒，并在多个生成模型中实现28%-46%的端到端加速。

Conclusion: GNA为稀疏注意力提供了可靠性能提升，硬件实现验证了其潜力，相关工具将开源。

Abstract: Many sparse attention mechanisms such as Neighborhood Attention have
typically failed to consistently deliver speedup over the self attention
baseline. This is largely due to the level of complexity in attention
infrastructure, and the rapid evolution of AI hardware architecture. At the
same time, many state-of-the-art foundational models, particularly in computer
vision, are heavily bound by attention, and need reliable sparsity to escape
the O(n^2) complexity. In this paper, we study a class of promising sparse
attention mechanisms that focus on locality, and aim to develop a better
analytical model of their performance improvements. We first introduce
Generalized Neighborhood Attention (GNA), which can describe sliding window,
strided sliding window, and blocked attention. We then consider possible design
choices in implementing these approaches, and create a simulator that can
provide much more realistic speedup upper bounds for any given setting.
Finally, we implement GNA on top of a state-of-the-art fused multi-headed
attention (FMHA) kernel designed for the NVIDIA Blackwell architecture in
CUTLASS. Our implementation can fully realize the maximum speedup theoretically
possible in many perfectly block-sparse cases, and achieves an effective
utilization of 1.3 petaFLOPs/second in FP16. In addition, we plug various GNA
configurations into off-the-shelf generative models, such as Cosmos-7B,
HunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end
speedup on B200 without any fine-tuning. We will open source our simulator and
Blackwell kernels directly through the NATTEN project.

</details>


### [101] [Procedural Dataset Generation for Zero-Shot Stereo Matching](https://arxiv.org/abs/2504.16930)
*David Yan,Alexander Raistrick,Jia Deng*

Main category: cs.CV

TL;DR: 论文研究了合成立体数据集的优化设计，提出了Infinigen-Stereo生成器，显著提升了零样本立体匹配性能。


<details>
  <summary>Details</summary>
Motivation: 探索合成立体数据集的设计空间，以提升零样本立体匹配的效果。

Method: 通过调整数据集生成器的参数，生成不同配置的合成数据集，并评估其对零样本立体匹配性能的影响。

Result: 提出的Infinigen-Stereo生成器显著优于现有合成数据集和公开检查点。

Conclusion: 开源Infinigen-Stereo生成器，为立体数据集研究提供了新工具。

Abstract: Synthetic datasets are a crucial ingredient for training stereo matching
networks, but the question of what makes a stereo dataset effective remains
largely unexplored. We investigate the design space of synthetic datasets by
varying the parameters of a procedural dataset generator, and report the
effects on zero-shot stereo matching performance using standard benchmarks. We
collect the best settings to produce Infinigen-Stereo, a procedural generator
specifically optimized for zero-shot stereo datasets. Models trained only on
data from our system outperform robust baselines trained on a combination of
existing synthetic datasets and have stronger zero-shot stereo matching
performance than public checkpoints from prior works. We open source our system
at https://github.com/princeton-vl/InfinigenStereo to enable further research
on procedural stereo datasets.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [102] [A Framework for Objective-Driven Dynamical Stochastic Fields](https://arxiv.org/abs/2504.16115)
*Yibo Jacky Zhang,Sanmi Koyejo*

Main category: cs.AI

TL;DR: 论文提出三个基本原则（完全配置、局部性和目的性）来建立智能场的理论框架，并探索其在人工智能应用中的设计方法。


<details>
  <summary>Details</summary>
Motivation: 由于复杂动态随机系统的固有复杂性，缺乏对其智能行为的正式理论描述和实际应用方法。

Method: 提出三个基本原则（完全配置、局部性和目的性）作为理论框架，并探讨其在人工智能应用中的设计方法。

Result: 初步建立了智能场的理论基础，为未来理论和实践发展奠定基础。

Conclusion: 该研究为理解和利用目标驱动的动态随机场的潜力提供了初步框架。

Abstract: Fields offer a versatile approach for describing complex systems composed of
interacting and dynamic components. In particular, some of these dynamical and
stochastic systems may exhibit goal-directed behaviors aimed at achieving
specific objectives, which we refer to as $\textit{intelligent fields}$.
However, due to their inherent complexity, it remains challenging to develop a
formal theoretical description of such systems and to effectively translate
these descriptions into practical applications. In this paper, we propose three
fundamental principles -- complete configuration, locality, and purposefulness
-- to establish a theoretical framework for understanding intelligent fields.
Moreover, we explore methodologies for designing such fields from the
perspective of artificial intelligence applications. This initial investigation
aims to lay the groundwork for future theoretical developments and practical
advances in understanding and harnessing the potential of such objective-driven
dynamical stochastic fields.

</details>


### [103] [HTN Plan Repair Algorithms Compared: Strengths and Weaknesses of Different Methods](https://arxiv.org/abs/2504.16209)
*Paul Zaidins,Robert P. Goldman,Ugur Kuter,Dana Nau,Mark Roberts*

Main category: cs.AI

TL;DR: 本文比较了三种分层计划修复算法的理论和实证表现，分析了它们的定义、搜索空间和修复能力差异，并通过基准测试评估了它们的运行时性能和问题覆盖率。


<details>
  <summary>Details</summary>
Motivation: 理解不同计划修复算法的定义和能力差异，为实际应用中选择合适的修复方法提供依据。

Method: 通过理论分析和实证评估（基准测试）比较SHOPFixer、IPyHOPPER和Rewrite三种算法。

Result: 理论分析揭示了算法在定义、搜索空间和修复能力上的差异；实证结果显示它们在运行时性能和问题覆盖率上的具体表现。

Conclusion: 选择合适的计划修复算法需考虑其定义和能力差异，实证结果为实际应用提供了详细参考。

Abstract: This paper provides theoretical and empirical comparisons of three recent
hierarchical plan repair algorithms: SHOPFixer, IPyHOPPER, and Rewrite. Our
theoretical results show that the three algorithms correspond to three
different definitions of the plan repair problem, leading to differences in the
algorithms' search spaces, the repair problems they can solve, and the kinds of
repairs they can make. Understanding these distinctions is important when
choosing a repair method for any given application.
  Building on the theoretical results, we evaluate the algorithms empirically
in a series of benchmark planning problems. Our empirical results provide more
detailed insight into the runtime repair performance of these systems and the
coverage of the repair problems solved, based on algorithmic properties such as
replanning, chronological backtracking, and backjumping over plan trees.

</details>


### [104] [Investigating LLMs in Clinical Triage: Promising Capabilities, Persistent Intersectional Biases](https://arxiv.org/abs/2504.16273)
*Joseph Lee,Tianqi Shang,Jae Young Baik,Duy Duong-Tran,Shu Yang,Lingyao Li,Li Shen*

Main category: cs.AI

TL;DR: LLMs在急诊分诊中表现出优越的鲁棒性，但在性别和种族的交叉分析中存在偏好差异。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在急诊分诊中的应用潜力，特别是在数据分布变化和缺失数据下的鲁棒性，以及性别和种族的交叉偏见。

Method: 通过持续预训练和上下文学习等多种LLM方法，结合机器学习方法，评估其在急诊分诊中的表现。

Result: LLMs表现出较强的鲁棒性，但在某些性别和种族的组合中存在明显的偏好差异。

Conclusion: LLMs在临床决策支持中具有潜力，但需注意其可能隐含的人口统计学偏好。

Abstract: Large Language Models (LLMs) have shown promise in clinical decision support,
yet their application to triage remains underexplored. We systematically
investigate the capabilities of LLMs in emergency department triage through two
key dimensions: (1) robustness to distribution shifts and missing data, and (2)
counterfactual analysis of intersectional biases across sex and race. We assess
multiple LLM-based approaches, ranging from continued pre-training to
in-context learning, as well as machine learning approaches. Our results
indicate that LLMs exhibit superior robustness, and we investigate the key
factors contributing to the promising LLM-based approaches. Furthermore, in
this setting, we identify gaps in LLM preferences that emerge in particular
intersections of sex and race. LLMs generally exhibit sex-based differences,
but they are most pronounced in certain racial groups. These findings suggest
that LLMs encode demographic preferences that may emerge in specific clinical
contexts or particular combinations of characteristics.

</details>


### [105] [Cognitive Silicon: An Architectural Blueprint for Post-Industrial Computing Systems](https://arxiv.org/abs/2504.16622)
*Christoforus Yoga Haryanto,Emily Lomempow*

Main category: cs.AI

TL;DR: 本文提出了一种名为“Cognitive Silicon”的假设性全栈架构框架，探索2035年认知计算系统设计的可能路径，整合符号化支撑、受控内存、运行时道德一致性和对齐感知执行，旨在构建一种道德可控的认知基础设施。


<details>
  <summary>Details</summary>
Motivation: 揭示确定性、人类编写的计算架构的基础性限制，探索认知计算系统设计的未来方向。

Method: 通过符号化支撑、受控内存、运行时道德一致性和对齐感知执行的整合，设计一种全栈架构框架，并与大型语言模型进行辩证协同设计。

Result: 提出了一种理论框架，该框架与自由能原理收敛，可能为认知系统如何通过预测误差最小化维持身份提供形式化解释。

Conclusion: 该框架旨在通过不可逆的硬件约束和抗复制或颠覆的身份绑定认知机制，构建一种道德可控且与人类对齐的认知基础设施。

Abstract: Autonomous AI systems reveal foundational limitations in deterministic,
human-authored computing architectures. This paper presents Cognitive Silicon:
a hypothetical full-stack architectural framework projected toward 2035,
exploring a possible trajectory for cognitive computing system design. The
proposed architecture would integrate symbolic scaffolding, governed memory,
runtime moral coherence, and alignment-aware execution across
silicon-to-semantics layers. Our design grammar has emerged from dialectical
co-design with LLMs under asymmetric epistemic conditions--creating structured
friction to expose blind spots and trade-offs. The envisioned framework would
establish mortality as a natural consequence of physical constraints,
non-copyable tacit knowledge, and non-cloneable identity keys as
cognitive-embodiment primitives. Core tensions (trust/agency,
scaffolding/emergence, execution/governance) would function as central
architectural pressures rather than edge cases. The architecture theoretically
converges with the Free Energy Principle, potentially offering a formal account
of how cognitive systems could maintain identity through prediction error
minimization across physical and computational boundaries. The resulting
framework aims to deliver a morally tractable cognitive infrastructure that
could maintain human-alignment through irreversible hardware constraints and
identity-bound epistemic mechanisms resistant to replication or subversion.

</details>


### [106] [Bridging Econometrics and AI: VaR Estimation via Reinforcement Learning and GARCH Models](https://arxiv.org/abs/2504.16635)
*Fredy Pokou,Jules Sadefo Kamdem,François Benhmad*

Main category: cs.AI

TL;DR: 提出了一种结合GARCH模型和深度强化学习的混合框架，用于动态调整VaR估计，显著提高了准确性并减少了违规次数和资本需求。


<details>
  <summary>Details</summary>
Motivation: 传统GARCH模型假设过于刚性，难以适应复杂市场动态，需要更灵活的风险估计方法。

Method: 结合GARCH模型和Double Deep Q-Network (DDQN)，将任务视为不平衡分类问题，动态调整风险水平预测。

Result: 在Eurostoxx 50数据上的实证验证显示，VaR估计准确性显著提高，违规次数和资本需求减少。

Conclusion: 该模型能实时调整风险水平，适用于现代主动风险管理。

Abstract: In an environment of increasingly volatile financial markets, the accurate
estimation of risk remains a major challenge. Traditional econometric models,
such as GARCH and its variants, are based on assumptions that are often too
rigid to adapt to the complexity of the current market dynamics. To overcome
these limitations, we propose a hybrid framework for Value-at-Risk (VaR)
estimation, combining GARCH volatility models with deep reinforcement learning.
Our approach incorporates directional market forecasting using the Double Deep
Q-Network (DDQN) model, treating the task as an imbalanced classification
problem. This architecture enables the dynamic adjustment of risk-level
forecasts according to market conditions. Empirical validation on daily
Eurostoxx 50 data covering periods of crisis and high volatility shows a
significant improvement in the accuracy of VaR estimates, as well as a
reduction in the number of breaches and also in capital requirements, while
respecting regulatory risk thresholds. The ability of the model to adjust risk
levels in real time reinforces its relevance to modern and proactive risk
management.

</details>


### [107] [IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery](https://arxiv.org/abs/2504.16728)
*Aniketh Garikaparthi,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.AI

TL;DR: IRIS是一个开源平台，利用大语言模型（LLM）辅助科学假设生成，结合人类参与（HITL）和蒙特卡洛树搜索（MCTS）等技术，提升研究人员的控制力和洞察力。


<details>
  <summary>Details</summary>
Motivation: 解决现有自动化假设生成方法缺乏透明性和可操控性的问题，通过人类与LLM的协同工作加速科学发现。

Method: 开发IRIS平台，整合MCTS、细粒度反馈机制和基于查询的文献合成技术，支持研究人员在假设生成过程中获得更多控制和洞察。

Result: 用户研究表明，IRIS能有效提升跨学科研究人员的假设生成能力。

Conclusion: IRIS为科学假设生成提供了透明、可控的解决方案，有望推动科学研究的效率。

Abstract: The rapid advancement in capabilities of large language models (LLMs) raises
a pivotal question: How can LLMs accelerate scientific discovery? This work
tackles the crucial first stage of research, generating novel hypotheses. While
recent work on automated hypothesis generation focuses on multi-agent
frameworks and extending test-time compute, none of the approaches effectively
incorporate transparency and steerability through a synergistic
Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:
Interactive Research Ideation System, an open-source platform designed for
researchers to leverage LLM-assisted scientific ideation. IRIS incorporates
innovative features to enhance ideation, including adaptive test-time compute
expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,
and query-based literature synthesis. Designed to empower researchers with
greater control and insight throughout the ideation process. We additionally
conduct a user study with researchers across diverse disciplines, validating
the effectiveness of our system in enhancing ideation. We open-source our code
at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System

</details>


### [108] [A Survey of AI Agent Protocols](https://arxiv.org/abs/2504.16736)
*Yingxuan Yang,Huacan Chai,Yuanyi Song,Siyuan Qi,Muning Wen,Ning Li,Junwei Liao,Haoyi Hu,Jianghao Lin,Gaowei Chang,Weiwen Liu,Ying Wen,Yong Yu,Weinan Zhang*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLM）代理间缺乏统一通信协议的问题，提出了分类和性能分析，并展望了未来挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理的广泛应用，缺乏标准化通信协议限制了其协作和扩展能力，亟需统一解决方案。

Method: 系统综述现有通信协议，将其分为四类，并进行性能分析（如安全性、可扩展性和延迟）。

Result: 提供了协议分类和性能比较，帮助用户选择适合的协议，并识别了未来挑战。

Conclusion: 统一通信协议将促进LLM代理协作，本文为设计和评估通信基础设施提供了实用参考。

Abstract: The rapid development of large language models (LLMs) has led to the
widespread deployment of LLM agents across diverse industries, including
customer service, content generation, data analysis, and even healthcare.
However, as more LLM agents are deployed, a major issue has emerged: there is
no standard way for these agents to communicate with external tools or data
sources. This lack of standardized protocols makes it difficult for agents to
work together or scale effectively, and it limits their ability to tackle
complex, real-world tasks. A unified communication protocol for LLM agents
could change this. It would allow agents and tools to interact more smoothly,
encourage collaboration, and triggering the formation of collective
intelligence. In this paper, we provide a systematic overview of existing
communication protocols for LLM agents. We classify them into four main
categories and make an analysis to help users and developers select the most
suitable protocols for specific applications. Additionally, we conduct a
comparative performance analysis of these protocols across key dimensions such
as security, scalability, and latency. Finally, we explore future challenges,
such as how protocols can adapt and survive in fast-evolving environments, and
what qualities future protocols might need to support the next generation of
LLM agent ecosystems. We expect this work to serve as a practical reference for
both researchers and engineers seeking to design, evaluate, or integrate robust
communication infrastructures for intelligent agents.

</details>


### [109] [Lightweight Latent Verifiers for Efficient Meta-Generation Strategies](https://arxiv.org/abs/2504.16760)
*Bartosz Piotrowski,Witold Drzewakowski,Konrad Staniszewski,Piotr Miłoś*

Main category: cs.AI

TL;DR: LiLaVe是一种轻量级验证方法，通过提取基础LLM隐藏状态中的正确性信号，显著降低计算成本，同时提升生成任务的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统验证器通常是大型LLM，计算成本高，LiLaVe旨在提供一种资源高效的替代方案。

Method: LiLaVe从基础LLM的隐藏状态中提取正确性信号，结合元生成策略（如best-of-n或自一致性）或设计新方法（如条件自校正或条件多数投票）。

Result: LiLaVe显著提升了生成任务的准确性和效率，尤其适用于小型LLM。

Conclusion: LiLaVe展示了从LLM隐藏状态中提取潜在信息的有效性，为资源高效解决推理密集型任务提供了新思路。

Abstract: Verifiers are auxiliary models that assess the correctness of outputs
generated by base large language models (LLMs). They play a crucial role in
many strategies for solving reasoning-intensive problems with LLMs. Typically,
verifiers are LLMs themselves, often as large (or larger) than the base model
they support, making them computationally expensive. In this work, we introduce
a novel lightweight verification approach, LiLaVe, which reliably extracts
correctness signals from the hidden states of the base LLM. A key advantage of
LiLaVe is its ability to operate with only a small fraction of the
computational budget required by traditional LLM-based verifiers. To
demonstrate its practicality, we couple LiLaVe with popular meta-generation
strategies, like best-of-n or self-consistency. Moreover, we design novel
LiLaVe-based approaches, like conditional self-correction or conditional
majority voting, that significantly improve both accuracy and efficiency in
generation tasks with smaller LLMs. Our work demonstrates the fruitfulness of
extracting latent information from the hidden states of LLMs, and opens the
door to scalable and resource-efficient solutions for reasoning-intensive
applications.

</details>


### [110] [AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset](https://arxiv.org/abs/2504.16891)
*Ivan Moshkov,Darragh Hanley,Ivan Sorokin,Shubham Toshniwal,Christof Henkel,Benedikt Schifferer,Wei Du,Igor Gitman*

Main category: cs.AI

TL;DR: 论文介绍了在AIMO-2竞赛中获胜的模型，基于大规模数据集、工具集成推理方法和生成式解决方案选择技术，实现了数学推理领域的领先成果。


<details>
  <summary>Details</summary>
Motivation: 构建高性能数学推理模型，通过整合代码执行和长推理模型，提升解决复杂数学问题的能力。

Method: 1. 创建包含54万高质量数学问题及320万长推理解决方案的数据集；2. 开发迭代训练方法，生成170万高质量工具集成推理解决方案；3. 训练模型从候选方案中选择最优解（GenSelect）。

Result: 模型在数学推理基准测试中达到领先水平，GenSelect显著优于多数投票基线。

Conclusion: 通过数据集、工具集成和生成式选择技术的结合，实现了数学推理领域的突破，并开源了代码和数据集。

Abstract: This paper presents our winning submission to the AI Mathematical Olympiad -
Progress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art
mathematical reasoning models relies on three key pillars. First, we create a
large-scale dataset comprising 540K unique high-quality math problems,
including olympiad-level problems, and their 3.2M long-reasoning solutions.
Second, we develop a novel method to integrate code execution with long
reasoning models through iterative training, generation, and quality filtering,
resulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we
create a pipeline to train models to select the most promising solution from
many candidates. We show that such generative solution selection (GenSelect)
can significantly improve upon majority voting baseline. Combining these ideas,
we train a series of models that achieve state-of-the-art results on
mathematical reasoning benchmarks. To facilitate further research, we release
our code, models, and the complete OpenMathReasoning dataset under a
commercially permissive license.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [111] [Representation Learning for Tabular Data: A Comprehensive Survey](https://arxiv.org/abs/2504.16109)
*Jun-Peng Jiang,Si-Yang Liu,Hao-Run Cai,Qile Zhou,Han-Jia Ye*

Main category: cs.LG

TL;DR: 本文系统介绍了表格表示学习领域，包括背景、挑战、基准测试，以及使用深度神经网络的优缺点，并将现有方法分为三类：专用模型、可迁移模型和通用模型。


<details>
  <summary>Details</summary>
Motivation: 表格数据是机器学习中最常见的数据类型之一，深度神经网络在表格表示学习中展现出潜力，因此需要系统梳理该领域的进展和方法。

Method: 将现有方法分为三类：专用模型（专注于同一数据分布的任务）、可迁移模型（通过预训练和微调）、通用模型（无需微调直接应用）。同时探讨了集成方法和扩展应用。

Result: 提出了一个层次化分类法，详细分析了特征和样本级别的表示策略，并讨论了跨数据集适应的方法。

Conclusion: 表格表示学习领域具有广阔的应用前景，未来可探索开放环境、多模态学习和表格理解等方向。

Abstract: Tabular data, structured as rows and columns, is among the most prevalent
data types in machine learning classification and regression applications.
Models for learning from tabular data have continuously evolved, with Deep
Neural Networks (DNNs) recently demonstrating promising results through their
capability of representation learning. In this survey, we systematically
introduce the field of tabular representation learning, covering the
background, challenges, and benchmarks, along with the pros and cons of using
DNNs. We organize existing methods into three main categories according to
their generalization capabilities: specialized, transferable, and general
models. Specialized models focus on tasks where training and evaluation occur
within the same data distribution. We introduce a hierarchical taxonomy for
specialized models based on the key aspects of tabular data -- features,
samples, and objectives -- and delve into detailed strategies for obtaining
high-quality feature- and sample-level representations. Transferable models are
pre-trained on one or more datasets and subsequently fine-tuned on downstream
tasks, leveraging knowledge acquired from homogeneous or heterogeneous sources,
or even cross-modalities such as vision and language. General models, also
known as tabular foundation models, extend this concept further, allowing
direct application to downstream tasks without fine-tuning. We group these
general models based on the strategies used to adapt across heterogeneous
datasets. Additionally, we explore ensemble methods, which integrate the
strengths of multiple tabular models. Finally, we discuss representative
extensions of tabular learning, including open-environment tabular machine
learning, multimodal learning with tabular data, and tabular understanding.
More information can be found in the following repository:
https://github.com/LAMDA-Tabular/Tabular-Survey.

</details>


### [112] [Active Learning Methods for Efficient Data Utilization and Model Performance Enhancement](https://arxiv.org/abs/2504.16136)
*Chiung-Yi Tseng,Junhao Song,Ziqian Bi,Tianyang Wang,Chia Xin Liang,Ming Liu*

Main category: cs.LG

TL;DR: 本文概述了主动学习（AL）在机器学习中的应用，探讨了其基本概念、多领域应用及研究热点，同时指出了当前挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 解决数据丰富但标注稀缺的瓶颈，提升机器学习模型的性能与效率。

Method: 介绍主动学习的基本概念，分析其在计算机视觉、自然语言处理等领域的应用，并探讨关键研究主题如不确定性估计和公平性。

Result: 主动学习通常优于被动学习，尤其在采用良好评估指标时表现更佳。

Conclusion: 本文为研究者和实践者提供了关键见解，并提出了主动学习未来发展的方向。

Abstract: In the era of data-driven intelligence, the paradox of data abundance and
annotation scarcity has emerged as a critical bottleneck in the advancement of
machine learning. This paper gives a detailed overview of Active Learning (AL),
which is a strategy in machine learning that helps models achieve better
performance using fewer labeled examples. It introduces the basic concepts of
AL and discusses how it is used in various fields such as computer vision,
natural language processing, transfer learning, and real-world applications.
The paper focuses on important research topics such as uncertainty estimation,
handling of class imbalance, domain adaptation, fairness, and the creation of
strong evaluation metrics and benchmarks. It also shows that learning methods
inspired by humans and guided by questions can improve data efficiency and help
models learn more effectively. In addition, this paper talks about current
challenges in the field, including the need to rebuild trust, ensure
reproducibility, and deal with inconsistent methodologies. It points out that
AL often gives better results than passive learning, especially when good
evaluation measures are used. This work aims to be useful for both researchers
and practitioners by providing key insights and proposing directions for future
progress in active learning.

</details>


### [113] [SparseJEPA: Sparse Representation Learning of Joint Embedding Predictive Architectures](https://arxiv.org/abs/2504.16140)
*Max Hartman,Lav Varshney*

Main category: cs.LG

TL;DR: SparseJEPA通过稀疏表示学习增强JEPA框架，提升表示质量与可解释性，并在CIFAR-100数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决JEPA框架中表示密集导致的可解释性差和效率低的问题。

Method: 引入稀疏表示学习，通过惩罚方法促进语义相关特征的潜在变量共享，保持预测性能。

Result: 在CIFAR-100数据集上训练轻量级Vision Transformer，提升线性探测迁移学习性能，理论证明分组机制优化表示质量。

Conclusion: 稀疏性不仅优化潜在空间，还促进更可解释的表示学习，未来将探索基于对象中心的表示学习。

Abstract: Joint Embedding Predictive Architectures (JEPA) have emerged as a powerful
framework for learning general-purpose representations. However, these models
often lack interpretability and suffer from inefficiencies due to dense
embedding representations. We propose SparseJEPA, an extension that integrates
sparse representation learning into the JEPA framework to enhance the quality
of learned representations. SparseJEPA employs a penalty method that encourages
latent space variables to be shared among data features with strong semantic
relationships, while maintaining predictive performance. We demonstrate the
effectiveness of SparseJEPA by training on the CIFAR-100 dataset and
pre-training a lightweight Vision Transformer. The improved embeddings are
utilized in linear-probe transfer learning for both image classification and
low-level tasks, showcasing the architecture's versatility across different
transfer tasks. Furthermore, we provide a theoretical proof that demonstrates
that the grouping mechanism enhances representation quality. This was done by
displaying that grouping reduces Multiinformation among latent-variables,
including proofing the Data Processing Inequality for Multiinformation. Our
results indicate that incorporating sparsity not only refines the latent space
but also facilitates the learning of more meaningful and interpretable
representations. In further work, hope to further extend this method by finding
new ways to leverage the grouping mechanism through object-centric
representation learning.

</details>


### [114] [Deep Learning Meets Process-Based Models: A Hybrid Approach to Agricultural Challenges](https://arxiv.org/abs/2504.16141)
*Yue Shi,Liangxiu Han,Xin Zhang,Tam Sobeih,Thomas Gaiser,Nguyen Huu Thuy,Dominik Behrend,Amit Kumar Srivastava,Krishnagopal Halder,Frank Ewert*

Main category: cs.LG

TL;DR: 本文系统综述了基于过程的模型（PBMs）和深度学习（DL）在农业建模中的应用，并探讨了混合PBM-DL框架的优势。通过案例研究，证明了混合模型在数据质量和空间条件下的优越性。


<details>
  <summary>Details</summary>
Motivation: PBMs和DL各有优缺点，PBMs提供可解释性但难以扩展，DL擅长处理复杂数据但缺乏可解释性。研究旨在结合两者优势，推动农业建模的发展。

Method: 通过系统综述和案例研究，分类混合PBM-DL方法（DL-informed PBMs和PBM-informed DL），并比较其性能。

Result: 混合模型在作物干生物量预测中表现优于单独PBMs和DL模型，尤其在噪声数据和未见地点上更具鲁棒性。

Conclusion: 混合模型结合了PBMs和DL的优势，为农业建模提供了可扩展、可解释的解决方案，但仍需解决可解释性和数据需求等挑战。

Abstract: Process-based models (PBMs) and deep learning (DL) are two key approaches in
agricultural modelling, each offering distinct advantages and limitations. PBMs
provide mechanistic insights based on physical and biological principles,
ensuring interpretability and scientific rigour. However, they often struggle
with scalability, parameterisation, and adaptation to heterogeneous
environments. In contrast, DL models excel at capturing complex, nonlinear
patterns from large datasets but may suffer from limited interpretability, high
computational demands, and overfitting in data-scarce scenarios.
  This study presents a systematic review of PBMs, DL models, and hybrid PBM-DL
frameworks, highlighting their applications in agricultural and environmental
modelling. We classify hybrid PBM-DL approaches into DL-informed PBMs, where
neural networks refine process-based models, and PBM-informed DL, where
physical constraints guide deep learning predictions. Additionally, we conduct
a case study on crop dry biomass prediction, comparing hybrid models against
standalone PBMs and DL models under varying data quality, sample sizes, and
spatial conditions. The results demonstrate that hybrid models consistently
outperform traditional PBMs and DL models, offering greater robustness to noisy
data and improved generalisation across unseen locations.
  Finally, we discuss key challenges, including model interpretability,
scalability, and data requirements, alongside actionable recommendations for
advancing hybrid modelling in agriculture. By integrating domain knowledge with
AI-driven approaches, this study contributes to the development of scalable,
interpretable, and reproducible agricultural models that support data-driven
decision-making for sustainable agriculture.

</details>


### [115] [Hexcute: A Tile-based Programming Language with Automatic Layout and Task-Mapping Synthesis](https://arxiv.org/abs/2504.16214)
*Xiao Zhang,Yaoyao Ding,Yang Hu,Gennady Pekhimenko*

Main category: cs.LG

TL;DR: Hexcute是一种基于分块的编程语言，旨在简化混合数据类型矩阵乘法算子的GPU优化，通过共享内存和寄存器抽象实现细粒度优化，并利用任务映射和自动化布局合成减少编程工作量。


<details>
  <summary>Details</summary>
Motivation: 深度学习量化技术需要处理混合数据类型的矩阵乘法算子，现有编译器在表达性和编程复杂性上存在不足。

Method: Hexcute通过分块编程语言、共享内存和寄存器抽象、任务映射调度以及自动化布局和任务映射合成算法实现优化。

Result: Hexcute在混合类型算子上的性能优于现有DL编译器，速度提升1.7-11.28倍，端到端评估中最高提升2.91倍。

Conclusion: Hexcute在表达性和工程效率之间取得了平衡，适用于广泛的深度学习算子优化。

Abstract: Deep learning (DL) workloads mainly run on accelerators like GPUs. Recent DL
quantization techniques demand a new matrix multiplication operator with mixed
input data types, further complicating GPU optimization. Prior high-level
compilers like Triton lack the expressiveness to implement key optimizations
like fine-grained data pipelines and hardware-friendly memory layouts for these
operators, while low-level programming models, such as Hidet, Graphene, and
CUTLASS, require significant programming efforts. To balance expressiveness
with engineering effort, we propose Hexcute, a tile-based programming language
that exposes shared memory and register abstractions to enable fine-grained
optimization for these operators. Additionally, Hexcute leverages task mapping
to schedule the GPU program, and to reduce programming efforts, it automates
layout and task mapping synthesis with a novel type-inference-based algorithm.
Our evaluation shows that Hexcute generalizes to a wide range of DL operators,
achieves 1.7-11.28$\times$ speedup over existing DL compilers for mixed-type
operators, and brings up to 2.91$\times$ speedup in the end-to-end evaluation.

</details>


### [116] [Using Phonemes in cascaded S2S translation pipeline](https://arxiv.org/abs/2504.16234)
*Rene Pilz,Johannes Schneider*

Main category: cs.LG

TL;DR: 论文探讨了在语音翻译中使用音素代替传统文本表示的方法，实验表明音素方法性能相当且更具优势。


<details>
  <summary>Details</summary>
Motivation: 研究音素作为文本表示在语音翻译中的潜力，以解决传统文本表示的局限性。

Method: 在WMT17数据集上训练序列到序列模型，比较文本和音素两种表示方法的性能。

Result: 音素方法在BLEU指标上表现相当，且资源需求更低，更适合低资源语言。

Conclusion: 音素表示在语音翻译中是一种可行且高效的替代方案。

Abstract: This paper explores the idea of using phonemes as a textual representation
within a conventional multilingual simultaneous speech-to-speech translation
pipeline, as opposed to the traditional reliance on text-based language
representations. To investigate this, we trained an open-source
sequence-to-sequence model on the WMT17 dataset in two formats: one using
standard textual representation and the other employing phonemic
representation. The performance of both approaches was assessed using the BLEU
metric. Our findings shows that the phonemic approach provides comparable
quality but offers several advantages, including lower resource requirements or
better suitability for low-resource languages.

</details>


### [117] [General Post-Processing Framework for Fairness Adjustment of Machine Learning Models](https://arxiv.org/abs/2504.16238)
*Léandre Eberhard,Nirek Sharma,Filipp Shelobolin,Aalok Ganesh Shanbhag*

Main category: cs.LG

TL;DR: 提出了一种新颖的公平性调整框架，适用于多种机器学习任务，通过解耦公平性调整与模型训练，保持模型性能的同时提供灵活性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在关键领域（如信贷、公共政策、人才招聘）的应用增加，确保公平性成为法律和伦理的迫切需求。

Method: 将传统的“处理中”技术调整为“后处理”步骤，无需自定义损失函数，支持不同数据集调整公平性，兼容黑盒模型。

Result: 与Adversarial Debiasing相比，该框架在真实数据集上实现了类似的公平性/准确性权衡。

Conclusion: 该框架为公平性调整提供了灵活且可解释的解决方案，适用于多种场景。

Abstract: As machine learning increasingly influences critical domains such as credit
underwriting, public policy, and talent acquisition, ensuring compliance with
fairness constraints is both a legal and ethical imperative. This paper
introduces a novel framework for fairness adjustments that applies to diverse
machine learning tasks, including regression and classification, and
accommodates a wide range of fairness metrics. Unlike traditional approaches
categorized as pre-processing, in-processing, or post-processing, our method
adapts in-processing techniques for use as a post-processing step. By
decoupling fairness adjustments from the model training process, our framework
preserves model performance on average while enabling greater flexibility in
model development. Key advantages include eliminating the need for custom loss
functions, enabling fairness tuning using different datasets, accommodating
proprietary models as black-box systems, and providing interpretable insights
into the fairness adjustments. We demonstrate the effectiveness of this
approach by comparing it to Adversarial Debiasing, showing that our framework
achieves a comparable fairness/accuracy tradeoff on real-world datasets.

</details>


### [118] [FairPlay: A Collaborative Approach to Mitigate Bias in Datasets for Improved AI Fairness](https://arxiv.org/abs/2504.16255)
*Tina Behzad,Mithilesh Kumar Singh,Anthony J. Ripa,Klaus Mueller*

Main category: cs.LG

TL;DR: FairPlay是一款基于网络的软件工具，帮助多方利益相关者通过协商达成公平的数据集去偏结果，无需统一的公平标准。


<details>
  <summary>Details</summary>
Motivation: 解决决策公平性问题，尤其是面对多方利益相关者对公平的不同且互不相容的需求。

Method: 通过战略互动视角，开发了FairPlay软件，支持用户协作去偏数据集并进行协商。

Result: 用户研究表明，FairPlay能在约五轮游戏内帮助用户达成共识，展示了其在提升AI系统公平性方面的潜力。

Conclusion: FairPlay提供了一种无需统一公平标准的协商工具，有效解决了多方利益相关者的公平需求问题。

Abstract: The issue of fairness in decision-making is a critical one, especially given
the variety of stakeholder demands for differing and mutually incompatible
versions of fairness. Adopting a strategic interaction of perspectives provides
an alternative to enforcing a singular standard of fairness. We present a
web-based software application, FairPlay, that enables multiple stakeholders to
debias datasets collaboratively. With FairPlay, users can negotiate and arrive
at a mutually acceptable outcome without a universally agreed-upon theory of
fairness. In the absence of such a tool, reaching a consensus would be highly
challenging due to the lack of a systematic negotiation process and the
inability to modify and observe changes. We have conducted user studies that
demonstrate the success of FairPlay, as users could reach a consensus within
about five rounds of gameplay, illustrating the application's potential for
enhancing fairness in AI systems.

</details>


### [119] [Learning Energy-Based Generative Models via Potential Flow: A Variational Principle Approach to Probability Density Homotopy Matching](https://arxiv.org/abs/2504.16262)
*Junn Yong Loo,Michelle Adeline,Julia Kaiwen Lau,Fang Yu Leong,Hwa Hui Tew,Arghya Pal,Vishnu Monn Baskaran,Chee-Ming Ting,Raphaël C. -W. Phan*

Main category: cs.LG

TL;DR: VPFB是一种新的基于能量的生成框架，通过变分损失学习能量参数化的势流，避免了隐式MCMC采样，提高了生成建模的效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 探索势流与显式EBMs之间的关系，并解决高维设置中对比散度训练的不稳定性和高成本问题。

Method: VPFB通过构建流驱动的密度同伦，并通过变分损失最小化KL散度来匹配数据分布，无需隐式MCMC采样或辅助网络。

Result: 在图像生成、插值、分布外检测和组合生成等任务中，VPFB在样本质量和多功能性上表现出色。

Conclusion: VPFB提供了一种高效、稳定的生成建模方法，同时保持了EBMs的可解释性。

Abstract: Energy-based models (EBMs) are a powerful class of probabilistic generative
models due to their flexibility and interpretability. However, relationships
between potential flows and explicit EBMs remain underexplored, while
contrastive divergence training via implicit Markov chain Monte Carlo (MCMC)
sampling is often unstable and expensive in high-dimensional settings. In this
paper, we propose Variational Potential Flow Bayes (VPFB), a new energy-based
generative framework that eliminates the need for implicit MCMC sampling and
does not rely on auxiliary networks or cooperative training. VPFB learns an
energy-parameterized potential flow by constructing a flow-driven density
homotopy that is matched to the data distribution through a variational loss
minimizing the Kullback-Leibler divergence between the flow-driven and marginal
homotopies. This principled formulation enables robust and efficient generative
modeling while preserving the interpretability of EBMs. Experimental results on
image generation, interpolation, out-of-distribution detection, and
compositional generation confirm the effectiveness of VPFB, showing that our
method performs competitively with existing approaches in terms of sample
quality and versatility across diverse generative modeling tasks.

</details>


### [120] [Gradient-Optimized Fuzzy Classifier: A Benchmark Study Against State-of-the-Art Models](https://arxiv.org/abs/2504.16263)
*Magnus Sieverding,Nathan Steffen,Kelly Cohen*

Main category: cs.LG

TL;DR: 本文对梯度优化模糊推理系统（GF）分类器进行了性能基准测试，并与多种先进机器学习模型（如随机森林、XGBoost等）进行了比较。结果显示GF在分类准确性、训练效率及鲁棒性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证梯度优化的模糊推理系统是否能在保持高效训练的同时，提供与复杂深度学习模型相媲美的分类性能。

Method: 使用梯度下降优化模糊推理系统，并在五个UCI数据集上与其他机器学习模型进行对比实验。

Result: GF模型在分类准确性上表现优异，训练时间短，且对噪声数据和多样特征集具有鲁棒性。

Conclusion: 梯度优化模糊系统可作为监督学习中高效、可解释且适应性强的替代方案。

Abstract: This paper presents a performance benchmarking study of a Gradient-Optimized
Fuzzy Inference System (GF) classifier against several state-of-the-art machine
learning models, including Random Forest, XGBoost, Logistic Regression, Support
Vector Machines, and Neural Networks. The evaluation was conducted across five
datasets from the UCI Machine Learning Repository, each chosen for their
diversity in input types, class distributions, and classification complexity.
Unlike traditional Fuzzy Inference Systems that rely on derivative-free
optimization methods, the GF leverages gradient descent to significantly
improving training efficiency and predictive performance. Results demonstrate
that the GF model achieved competitive, and in several cases superior,
classification accuracy while maintaining high precision and exceptionally low
training times. In particular, the GF exhibited strong consistency across folds
and datasets, underscoring its robustness in handling noisy data and variable
feature sets. These findings support the potential of gradient optimized fuzzy
systems as interpretable, efficient, and adaptable alternatives to more complex
deep learning models in supervised learning tasks.

</details>


### [121] [Boosting Classifier Performance with Opposition-Based Data Transformation](https://arxiv.org/abs/2504.16268)
*Abdesslem Layeb*

Main category: cs.LG

TL;DR: 提出了一种基于对立学习（OBL）的新型数据转换框架，显著提升传统分类算法的性能。


<details>
  <summary>Details</summary>
Motivation: 利用OBL生成合成对立样本，优化决策边界，提升分类性能。

Method: 探索了三种OBL变体（全局OBL、类内OBL和局部类内OBL），并与KNN、SVM、LR和DT等分类器结合。

Result: 在26个异构高维数据集上，OBL增强的分类器在准确率和F1分数上均优于标准版本，且计算效率提升。

Conclusion: OBL是一种轻量级但强大的数据转换策略，特别适用于复杂或稀疏学习环境。

Abstract: In this paper, we introduce a novel data transformation framework based on
Opposition-Based Learning (OBL) to boost the performance of traditional
classification algorithms. Originally developed to accelerate convergence in
optimization tasks, OBL is leveraged here to generate synthetic opposite
samples that replace the acutely training data and improve decision boundary
formation. We explore three OBL variants; Global OBL, Class-Wise OBL, and
Localized Class-Wise OBL; and integrate them with several widely used
classifiers, including K-Nearest Neighbors (KNN), Support Vector Machines
(SVM), Logistic Regression (LR), and Decision Tree (DT). Extensive experiments
conducted on 26 heterogeneous and high-dimensional datasets demonstrate that
OBL-enhanced classifiers consistently outperform their standard counterparts in
terms of accuracy and F1-score, frequently achieving near-perfect or perfect
classification. Furthermore, OBL contributes to improved computational
efficiency, particularly in SVM and LR. These findings underscore the potential
of OBL as a lightweight yet powerful data transformation strategy for enhancing
classification performance, especially in complex or sparse learning
environments.

</details>


### [122] [Learning Explainable Dense Reward Shapes via Bayesian Optimization](https://arxiv.org/abs/2504.16272)
*Ryan Koo,Ian Yang,Vipul Raheja,Mingyi Hong,Kwang-Sung Jun,Dongyeop Kang*

Main category: cs.LG

TL;DR: 论文提出了一种基于可解释性方法的奖励塑造函数，通过优化令牌级信用分配来改进RLHF流水线。


<details>
  <summary>Details</summary>
Motivation: 当前RLHF流水线使用标量奖励和最终令牌作为序列质量的代理指标，导致稀疏反馈和次优的令牌级信用分配。

Method: 提出一种奖励塑造函数，利用SHAP和LIME等可解释性方法估计每令牌奖励，并通过双层优化框架学习其参数。

Result: 实验表明，改进令牌级奖励分配能提升下游任务性能并加速训练。理论证明特征可加性可解释方法保持最优策略。

Conclusion: 通过优化令牌级信用分配，可显著提升RLHF流水线的效果和效率。

Abstract: Current reinforcement learning from human feedback (RLHF) pipelines for large
language model (LLM) alignment typically assign scalar rewards to sequences,
using the final token as a surrogate indicator for the quality of the entire
sequence. However, this leads to sparse feedback and suboptimal token-level
credit assignment. In this work, we frame reward shaping as an optimization
problem focused on token-level credit assignment. We propose a reward-shaping
function leveraging explainability methods such as SHAP and LIME to estimate
per-token rewards from the reward model. To learn parameters of this shaping
function, we employ a bilevel optimization framework that integrates Bayesian
Optimization and policy training to handle noise from the token reward
estimates. Our experiments show that achieving a better balance of token-level
reward attribution leads to performance improvements over baselines on
downstream tasks and finds an optimal policy faster during training.
Furthermore, we show theoretically that explainability methods that are feature
additive attribution functions maintain the optimal policy as the original
reward.

</details>


### [123] [Quantum Doubly Stochastic Transformers](https://arxiv.org/abs/2504.16275)
*Jannis Born,Filip Skogh,Kahn Rhrissorrakrai,Filippo Utro,Nico Wagner,Aleksandros Sobczyk*

Main category: cs.LG

TL;DR: 论文提出了一种混合经典-量子双随机Transformer（QDSFormer），用变分量子电路替代自注意力层中的Softmax，提升了性能和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer中的Softmax归一化可能导致训练不稳定，而现有的双随机矩阵方法（如Sinkhorn算法）存在局限性。量子电路提供了一种新的双随机矩阵生成方式，具有更强的表达能力。

Method: QDSFormer使用变分量子电路生成双随机矩阵，替代Softmax。研究了电路的表达能力，并与经典方法（如Sinkformer和基于QR分解的量子启发方法）进行比较。

Result: QDSFormer在小规模目标识别任务中表现优于标准Vision Transformer和其他双随机Transformer，同时提升了训练稳定性和性能一致性。

Conclusion: QDSFormer展示了量子电路在Transformer中的潜力，为小规模数据上的训练不稳定问题提供了解决方案。

Abstract: At the core of the Transformer, the Softmax normalizes the attention matrix
to be right stochastic. Previous research has shown that this often
destabilizes training and that enforcing the attention matrix to be doubly
stochastic (through Sinkhorn's algorithm) consistently improves performance
across different tasks, domains and Transformer flavors. However, Sinkhorn's
algorithm is iterative, approximative, non-parametric and thus inflexible
w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been
proven that DSMs can be obtained with a parametric quantum circuit, yielding a
novel quantum inductive bias for DSMs with no known classical analogue.
Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum
doubly stochastic Transformer (QDSFormer) that replaces the Softmax in the
self-attention layer with a variational quantum circuit. We study the
expressive power of the circuit and find that it yields more diverse DSMs that
better preserve information than classical operators. Across multiple
small-scale object recognition tasks, we find that our QDSFormer consistently
surpasses both a standard Vision Transformer and other doubly stochastic
Transformers. Beyond the established Sinkformer, this comparison includes a
novel quantum-inspired doubly stochastic Transformer (based on QR
decomposition) that can be of independent interest. The QDSFormer also shows
improved training stability and lower performance variation suggesting that it
may mitigate the notoriously unstable training of ViTs on small-scale data.

</details>


### [124] [An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon](https://arxiv.org/abs/2504.16276)
*Abhishek Jana,Moeumu Uili,James Atherton,Mark O'Brien,Joe Wood,Leandra Brickson*

Main category: cs.LG

TL;DR: 提出了一种针对稀有鸟类的自动单次鸟鸣分类方法，适用于仅有1-3个录音样本的物种，解决了现有分类器无法处理稀有物种的问题。


<details>
  <summary>Details</summary>
Motivation: 现有公开分类器（如BirdNET和Perch）对常见鸟类表现良好，但对稀有物种（如濒危鸟类）缺乏支持，限制了保护工作的有效性。

Method: 利用大型鸟类分类网络的嵌入空间，结合余弦相似度分类器、过滤和降噪预处理技术，优化了在极少训练数据下的检测效果。

Result: 在模拟场景和真实测试（极度濒危的齿嘴鸽）中，模型召回率达1.0，准确率达0.95。

Conclusion: 该方法为保护濒危物种提供了一种实用工具，适用于野外监测。

Abstract: This paper presents an automated one-shot bird call classification pipeline
designed for rare species absent from large publicly available classifiers like
BirdNET and Perch. While these models excel at detecting common birds with
abundant training data, they lack options for species with only 1-3 known
recordings-a critical limitation for conservationists monitoring the last
remaining individuals of endangered birds. To address this, we leverage the
embedding space of large bird classification networks and develop a classifier
using cosine similarity, combined with filtering and denoising preprocessing
techniques, to optimize detection with minimal training data. We evaluate
various embedding spaces using clustering metrics and validate our approach in
both a simulated scenario with Xeno-Canto recordings and a real-world test on
the critically endangered tooth-billed pigeon (Didunculus strigirostris), which
has no existing classifiers and only three confirmed recordings. The final
model achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon
calls, making it practical for use in the field. This open-source system
provides a practical tool for conservationists seeking to detect and monitor
rare species on the brink of extinction.

</details>


### [125] [DataS^3: Dataset Subset Selection for Specialization](https://arxiv.org/abs/2504.16277)
*Neha Hulkund,Alaa Maalouf,Levi Cai,Daniel Yang,Tsun-Hsuan Wang,Abigail O'Neil,Timm Haucke,Sandeep Mukherjee,Vikram Ramaswamy,Judy Hansen Shen,Gabriel Tseng,Mike Walmsley,Daniela Rus,Ken Goldberg,Hannah Kerner,Irene Chen,Yogesh Girdhar,Sara Beery*

Main category: cs.LG

TL;DR: 论文提出了一种针对特定部署场景的数据子集选择方法（DS3），并介绍了首个专门用于DS3问题的数据集和基准DataS^3。研究发现，通用分布方法在部署特定任务上表现不佳，而专家手动筛选的子集能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的机器学习应用需要在特定部署场景（如特定医院或国家公园）表现良好，但这些场景的数据分布往往不平衡且独特。训练分布与部署分布的不一致会导致性能下降，因此需要选择适合部署的特定训练数据子集。

Method: 论文提出了DS3问题，即从通用分布的训练数据中选择子集以优化部署性能。为此，作者创建了DataS^3数据集和基准，并评估了多种算法（如核心集、数据过滤和数据整理）在DS3任务上的表现。

Result: 研究发现，通用分布方法在部署特定任务上表现不佳，而手动筛选的专家子集能显著提升性能（最高提升51.3%的准确率）。

Conclusion: 论文强调了针对部署特定分布的数据整理在提升性能和训练效率中的重要性，并认为随着全球公共数据集的普及，这一需求将愈发重要。

Abstract: In many real-world machine learning (ML) applications (e.g. detecting broken
bones in x-ray images, detecting species in camera traps), in practice models
need to perform well on specific deployments (e.g. a specific hospital, a
specific national park) rather than the domain broadly. However, deployments
often have imbalanced, unique data distributions. Discrepancy between the
training distribution and the deployment distribution can lead to suboptimal
performance, highlighting the need to select deployment-specialized subsets
from the available training data. We formalize dataset subset selection for
specialization (DS3): given a training set drawn from a general distribution
and a (potentially unlabeled) query set drawn from the desired
deployment-specific distribution, the goal is to select a subset of the
training data that optimizes deployment performance.
  We introduce DataS^3; the first dataset and benchmark designed specifically
for the DS3 problem. DataS^3 encompasses diverse real-world application
domains, each with a set of distinct deployments to specialize in. We conduct a
comprehensive study evaluating algorithms from various families--including
coresets, data filtering, and data curation--on DataS^3, and find that
general-distribution methods consistently fail on deployment-specific tasks.
Additionally, we demonstrate the existence of manually curated
(deployment-specific) expert subsets that outperform training on all available
data with accuracy gains up to 51.3 percent. Our benchmark highlights the
critical role of tailored dataset curation in enhancing performance and
training efficiency on deployment-specific distributions, which we posit will
only become more important as global, public datasets become available across
domains and ML models are deployed in the real world.

</details>


### [126] [Affect Models Have Weak Generalizability to Atypical Speech](https://arxiv.org/abs/2504.16283)
*Jaya Narain,Amrit Romana,Vikramjit Mitra,Colin Lea,Shirley Ren*

Main category: cs.LG

TL;DR: 论文研究了语音异常对情感识别模型的影响，发现异常语音显著影响模型输出，并提出通过伪标签数据微调模型以提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 语音异常可能影响情感识别模型的性能，研究旨在评估现有模型在异常语音上的表现，并提出改进方法。

Method: 比较公开的情感识别模型在典型和非典型语音数据集上的表现，分析语音异常的三个维度（清晰度、单调音、刺耳度），并进行分布趋势和相关性分析。

Result: 异常语音显著影响情感预测结果，例如预测为“悲伤”的比例更高；通过伪标签数据微调模型可提高对异常语音的识别能力。

Conclusion: 研究强调需要更广泛的训练和评估数据集，以及开发对语音差异更鲁棒的情感识别模型。

Abstract: Speech and voice conditions can alter the acoustic properties of speech,
which could impact the performance of paralinguistic models for affect for
people with atypical speech. We evaluate publicly available models for
recognizing categorical and dimensional affect from speech on a dataset of
atypical speech, comparing results to datasets of typical speech. We
investigate three dimensions of speech atypicality: intelligibility, which is
related to pronounciation; monopitch, which is related to prosody, and
harshness, which is related to voice quality. We look at (1) distributional
trends of categorical affect predictions within the dataset, (2) distributional
comparisons of categorical affect predictions to similar datasets of typical
speech, and (3) correlation strengths between text and speech predictions for
spontaneous speech for valence and arousal. We find that the output of affect
models is significantly impacted by the presence and degree of speech
atypicalities. For instance, the percentage of speech predicted as sad is
significantly higher for all types and grades of atypical speech when compared
to similar typical speech datasets. In a preliminary investigation on improving
robustness for atypical speech, we find that fine-tuning models on
pseudo-labeled atypical speech data improves performance on atypical speech
without impacting performance on typical speech. Our results emphasize the need
for broader training and evaluation datasets for speech emotion models, and for
modeling approaches that are robust to voice and speech differences.

</details>


### [127] [Semantics at an Angle: When Cosine Similarity Works Until It Doesn't](https://arxiv.org/abs/2504.16318)
*Kisung You*

Main category: cs.LG

TL;DR: 本文反思了余弦相似度在嵌入比较中的广泛应用，探讨了其优势和局限性，并介绍了新兴的替代方法。


<details>
  <summary>Details</summary>
Motivation: 余弦相似度因其尺度不变性和与模型训练目标的一致性而被广泛使用，但其局限性在嵌入范数具有语义信息时显现。本文旨在提供概念清晰性和实践视角。

Method: 通过选择性回顾余弦相似度的演变、优势和局限性，分析其适用场景和失效情况，并探讨新兴替代方法。

Result: 余弦相似度在许多场景中表现良好，但在嵌入范数具有语义信息时存在局限性。新兴方法正在解决其盲点。

Conclusion: 本文为定量科学家提供了对嵌入的几何和哲学视角，强调了余弦相似度的适用性和替代方法的潜力。

Abstract: Cosine similarity has become a standard metric for comparing embeddings in
modern machine learning. Its scale-invariance and alignment with model training
objectives have contributed to its widespread adoption. However, recent studies
have revealed important limitations, particularly when embedding norms carry
meaningful semantic information. This informal article offers a reflective and
selective examination of the evolution, strengths, and limitations of cosine
similarity. We highlight why it performs well in many settings, where it tends
to break down, and how emerging alternatives are beginning to address its blind
spots. We hope to offer a mix of conceptual clarity and practical perspective,
especially for quantitative scientists who think about embeddings not just as
vectors, but as geometric and philosophical objects.

</details>


### [128] [Disentangled Graph Representation Based on Substructure-Aware Graph Optimal Matching Kernel Convolutional Networks](https://arxiv.org/abs/2504.16360)
*Mao Wang,Tao Wu,Xingping Xian,Shaojie Qiao,Weina Niu,Canyixing Cui*

Main category: cs.LG

TL;DR: 论文提出了一种名为GOMKCN的新方法，通过图最优匹配核卷积网络解决现有图表示学习方法在结构模式分析上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法对图结构的表征往往隐含且粗糙，限制了结构模式分析。

Method: 将图视为以节点为中心的子图，引入图最优匹配核（GOMK）作为卷积算子，计算子图与可学习图滤波器之间的相似性。

Result: 实验证明GOMKCN在图模式挖掘和预测中具有更高的准确性和可解释性。

Conclusion: GOMKCN为解耦图表示学习提供了理论基础，并提升了性能。

Abstract: Graphs effectively characterize relational data, driving graph representation
learning methods that uncover underlying predictive information. As
state-of-the-art approaches, Graph Neural Networks (GNNs) enable end-to-end
learning for diverse tasks. Recent disentangled graph representation learning
enhances interpretability by decoupling independent factors in graph data.
However, existing methods often implicitly and coarsely characterize graph
structures, limiting structural pattern analysis within the graph. This paper
proposes the Graph Optimal Matching Kernel Convolutional Network (GOMKCN) to
address this limitation. We view graphs as node-centric subgraphs, where each
subgraph acts as a structural factor encoding position-specific information.
This transforms graph prediction into structural pattern recognition. Inspired
by CNNs, GOMKCN introduces the Graph Optimal Matching Kernel (GOMK) as a
convolutional operator, computing similarities between subgraphs and learnable
graph filters. Mathematically, GOMK maps subgraphs and filters into a Hilbert
space, representing graphs as point sets. Disentangled representations emerge
from projecting subgraphs onto task-optimized filters, which adaptively capture
relevant structural patterns via gradient descent. Crucially, GOMK incorporates
local correspondences in similarity measurement, resolving the trade-off
between differentiability and accuracy in graph kernels. Experiments validate
that GOMKCN achieves superior accuracy and interpretability in graph pattern
mining and prediction. The framework advances the theoretical foundation for
disentangled graph representation learning.

</details>


### [129] [Natural Policy Gradient for Average Reward Non-Stationary RL](https://arxiv.org/abs/2504.16415)
*Neharika Jali,Eshika Pathak,Pranay Sharma,Guannan Qu,Gauri Joshi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem of non-stationary reinforcement learning (RL) in the
infinite-horizon average-reward setting. We model it by a Markov Decision
Process with time-varying rewards and transition probabilities, with a
variation budget of $\Delta_T$. Existing non-stationary RL algorithms focus on
model-based and model-free value-based methods. Policy-based methods despite
their flexibility in practice are not theoretically well understood in
non-stationary RL. We propose and analyze the first model-free policy-based
algorithm, Non-Stationary Natural Actor-Critic (NS-NAC), a policy gradient
method with a restart based exploration for change and a novel interpretation
of learning rates as adapting factors. Further, we present a bandit-over-RL
based parameter-free algorithm BORL-NS-NAC that does not require prior
knowledge of the variation budget $\Delta_T$. We present a dynamic regret of
$\tilde{\mathscr O}(|S|^{1/2}|A|^{1/2}\Delta_T^{1/6}T^{5/6})$ for both
algorithms, where $T$ is the time horizon, and $|S|$, $|A|$ are the sizes of
the state and action spaces. The regret analysis leverages a novel adaptation
of the Lyapunov function analysis of NAC to dynamic environments and
characterizes the effects of simultaneous updates in policy, value function
estimate and changes in the environment.

</details>


### [130] [MAGIC: Near-Optimal Data Attribution for Deep Learning](https://arxiv.org/abs/2504.16430)
*Andrew Ilyas,Logan Engstrom*

Main category: cs.LG

TL;DR: 提出了一种新方法（MAGIC），结合经典方法和元微分技术，用于估计训练数据增减对模型预测的影响。


<details>
  <summary>Details</summary>
Motivation: 在非凸大规模场景中，现有方法对数据增减影响的估计效果不佳。

Method: 结合经典方法和元微分技术，开发了MAGIC方法。

Result: MAGIC方法能够近乎最优地估计训练数据增减对预测的影响。

Conclusion: MAGIC方法在非凸大规模场景中显著提升了数据属性估计的准确性。

Abstract: The goal of predictive data attribution is to estimate how adding or removing
a given set of training datapoints will affect model predictions. In convex
settings, this goal is straightforward (i.e., via the infinitesimal jackknife).
In large-scale (non-convex) settings, however, existing methods are far less
successful -- current methods' estimates often only weakly correlate with
ground truth. In this work, we present a new data attribution method (MAGIC)
that combines classical methods and recent advances in metadifferentiation to
(nearly) optimally estimate the effect of adding or removing training data on
model predictions.

</details>


### [131] [Target Concrete Score Matching: A Holistic Framework for Discrete Diffusion](https://arxiv.org/abs/2504.16431)
*Ruixiang Zhang,Shuangfei Zhai,Yizhe Zhang,James Thornton,Zijing Ou,Joshua Susskind,Navdeep Jaitly*

Main category: cs.LG

TL;DR: TCSM是一种新的目标函数，用于训练和微调离散扩散模型，具有广泛适用性，支持预训练和微调，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在离散数据建模和生成中表现优异，但现有方法缺乏灵活性和通用性。TCSM旨在提供一个更通用的框架。

Method: TCSM通过估计目标分布的具体分数，支持预训练、微调（如奖励函数或偏好数据）和知识蒸馏。

Result: 实验表明，TCSM在语言建模任务中表现优于或与现有方法相当，且更具灵活性和样本效率。

Conclusion: TCSM为离散扩散模型提供了一个通用且高效的框架，适用于多种场景。

Abstract: Discrete diffusion is a promising framework for modeling and generating
discrete data. In this work, we present Target Concrete Score Matching (TCSM),
a novel and versatile objective for training and fine-tuning discrete diffusion
models. TCSM provides a general framework with broad applicability. It supports
pre-training discrete diffusion models directly from data samples, and many
existing discrete diffusion approaches naturally emerge as special cases of our
more general TCSM framework. Furthermore, the same TCSM objective extends to
post-training of discrete diffusion models, including fine-tuning using reward
functions or preference data, and distillation of knowledge from pre-trained
autoregressive models. These new capabilities stem from the core idea of TCSM,
estimating the concrete score of the target distribution, which resides in the
original (clean) data space. This allows seamless integration with reward
functions and pre-trained models, which inherently only operate in the clean
data space rather than the noisy intermediate spaces of diffusion processes.
Our experiments on language modeling tasks demonstrate that TCSM matches or
surpasses current methods. Additionally, TCSM is versatile, applicable to both
pre-training and post-training scenarios, offering greater flexibility and
sample efficiency.

</details>


### [132] [iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network](https://arxiv.org/abs/2504.16432)
*Ziran Liang,Rui An,Wenqi Fan,Yanghui Rao,Yuxuan Liang*

Main category: cs.LG

TL;DR: 提出了一种可解释的时间序列预测模型iTFKAN，通过模型符号化实现高解释性，并结合先验知识注入和时频协同学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度预测方法性能优越但缺乏解释性，限制了其在安全关键领域的应用。

Method: iTFKAN通过模型符号化实现解释性，并采用先验知识注入和时频协同学习策略。

Result: 实验表明iTFKAN在预测性能和解释能力上均表现优异。

Conclusion: iTFKAN是一种兼具高性能和高解释性的可信时间序列预测模型。

Abstract: As time evolves, data within specific domains exhibit predictability that
motivates time series forecasting to predict future trends from historical
data. However, current deep forecasting methods can achieve promising
performance but generally lack interpretability, hindering trustworthiness and
practical deployment in safety-critical applications such as auto-driving and
healthcare. In this paper, we propose a novel interpretable model, iTFKAN, for
credible time series forecasting. iTFKAN enables further exploration of model
decision rationales and underlying data patterns due to its interpretability
achieved through model symbolization. Besides, iTFKAN develops two strategies,
prior knowledge injection, and time-frequency synergy learning, to effectively
guide model learning under complex intertwined time series data. Extensive
experimental results demonstrated that iTFKAN can achieve promising forecasting
performance while simultaneously possessing high interpretive capabilities.

</details>


### [133] [Private Federated Learning using Preference-Optimized Synthetic Data](https://arxiv.org/abs/2504.16438)
*Charlie Hou,Mei-Yu Wang,Yige Zhu,Daniel Lazar,Giulia Fanti*

Main category: cs.LG

TL;DR: POPri利用偏好优化算法（如DPO）改进DP合成数据生成，显著提升联邦学习中隐私数据的效用，缩小与非隐私设置的性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前DP-FL方法依赖公共信息或迭代客户端反馈生成DP合成数据，POPri提出将客户端反馈视为偏好排名，以优化生成更高质量的DP合成数据。

Method: POPri通过偏好优化算法（如DPO）微调LLM，生成高质量DP合成数据，并发布LargeFedBench作为评估基准。

Result: POPri在LargeFedBench上显著提升DP合成数据的效用，将隐私与非隐私设置间的性能差距缩小至68%，优于现有方法。

Conclusion: POPri通过偏好优化有效提升DP合成数据的质量，为联邦学习中的隐私数据生成提供了新方向。

Abstract: In practical settings, differentially private Federated learning (DP-FL) is
the dominant method for training models from private, on-device client data.
Recent work has suggested that DP-FL may be enhanced or outperformed by methods
that use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary
algorithms for generating DP synthetic data for FL applications require careful
prompt engineering based on public information and/or iterative private client
feedback. Our key insight is that the private client feedback collected by
prior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be
viewed as a preference ranking. Our algorithm, Preference Optimization for
Private Client Data (POPri) harnesses client feedback using preference
optimization algorithms such as Direct Preference Optimization (DPO) to
fine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri,
we release LargeFedBench, a new federated text benchmark for uncontaminated LLM
evaluations on federated client data. POPri substantially improves the utility
of DP synthetic data relative to prior work on LargeFedBench datasets and an
existing benchmark from Xie et al. (2024). POPri closes the gap between
next-token prediction accuracy in the fully-private and non-private settings by
up to 68%, compared to 52% for prior synthetic data methods, and 10% for
state-of-the-art DP federated learning methods. The code and data are available
at https://github.com/meiyuw/POPri.

</details>


### [134] [Node Assigned physics-informed neural networks for thermal-hydraulic system simulation: CVH/FL module](https://arxiv.org/abs/2504.16447)
*Jeesuk Shin,Cheolwoong Kim,Sunwoong Yang,Minseo Lee,Sung Joong Kim,Joongoo Jeon*

Main category: cs.LG

TL;DR: 该研究提出了一种基于物理信息神经网络（PINN）的新数值方法，用于改进核电站严重事故的热工水力系统代码，解决了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统热工水力系统代码（如MELCOR和MAAP）在模拟严重事故时存在不一致的有限差分格式和单向耦合问题，限制了多物理场分析的准确性。

Method: 研究开发了一种节点分配PINN（NA-PINN），为每个系统代码的节点分配单独的网络，以学习纯时间解，从而排除空间信息的影响。

Result: 在6个水箱模拟中，PINN和NA-PINN的最大绝对误差分别为1.678和0.007，仅NA-PINN表现出可接受的精度。

Conclusion: NA-PINN首次成功应用于系统代码，未来将扩展为多物理场求解器并开发为替代模型。

Abstract: Severe accidents (SAs) in nuclear power plants have been analyzed using
thermal-hydraulic (TH) system codes such as MELCOR and MAAP. These codes
efficiently simulate the progression of SAs, while they still have inherent
limitations due to their inconsistent finite difference schemes. The use of
empirical schemes incorporating both implicit and explicit formulations
inherently induces unidirectional coupling in multi-physics analyses. The
objective of this study is to develop a novel numerical method for TH system
codes using physics-informed neural network (PINN). They have shown strength in
solving multi-physics due to the innate feature of neural networks-automatic
differentiation. We propose a node-assigned PINN (NA-PINN) that is suitable for
the control volume approach-based system codes. NA-PINN addresses the issue of
spatial governing equation variation by assigning an individual network to each
nodalization of the system code, such that spatial information is excluded from
both the input and output domains, and each subnetwork learns to approximate a
purely temporal solution. In this phase, we evaluated the accuracy of the PINN
methods for the hydrodynamic module. In the 6 water tank simulation, PINN and
NA-PINN showed maximum absolute errors of 1.678 and 0.007, respectively. It
should be noted that only NA-PINN demonstrated acceptable accuracy. To the best
of the authors' knowledge, this is the first study to successfully implement a
system code using PINN. Our future work involves extending NA-PINN to a
multi-physics solver and developing it in a surrogate manner.

</details>


### [135] [An Effective Gram Matrix Characterizes Generalization in Deep Networks](https://arxiv.org/abs/2504.16450)
*Rubing Yang,Pratik Chaudhari*

Main category: cs.LG

TL;DR: 论文通过微分方程分析深度网络在梯度下降训练中的泛化差距演变，提出有效Gram矩阵预测测试损失，并探讨数据与架构匹配对泛化的影响。


<details>
  <summary>Details</summary>
Motivation: 研究深度网络训练过程中泛化差距的演变机制，以理解其与数据及架构的关系。

Method: 推导控制泛化差距演变的微分方程，分析有效Gram矩阵与初始残差的匹配关系。

Result: 实证表明有效Gram矩阵能准确预测测试损失，训练过程良性，数据与架构匹配决定泛化好坏。

Conclusion: 泛化差距受数据与架构匹配影响，有效Gram矩阵是预测泛化性能的关键。

Abstract: We derive a differential equation that governs the evolution of the
generalization gap when a deep network is trained by gradient descent. This
differential equation is controlled by two quantities, a contraction factor
that brings together trajectories corresponding to slightly different datasets,
and a perturbation factor that accounts for them training on different
datasets. We analyze this differential equation to compute an ``effective Gram
matrix'' that characterizes the generalization gap after training in terms of
the alignment between this Gram matrix and a certain initial ``residual''.
Empirical evaluations on image classification datasets indicate that this
analysis can predict the test loss accurately. Further, at any point during
training, the residual predominantly lies in the subspace of the effective Gram
matrix with the smallest eigenvalues. This indicates that the training process
is benign, i.e., it does not lead to significant deterioration of the
generalization gap (which is zero at initialization). The alignment between the
effective Gram matrix and the residual is different for different datasets and
architectures. The match/mismatch of the data and the architecture is primarily
responsible for good/bad generalization.

</details>


### [136] [Dynamic Time-aware Continual User Representation Learning](https://arxiv.org/abs/2504.16501)
*Seungyoon Choi,Sein Kim,Hongseok Kang,Wonjoong Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: 论文提出了一种动态时间感知的持续用户表示学习框架DITTO，解决了传统方法在任务随时间变化时忽略新项目分布变化的问题。


<details>
  <summary>Details</summary>
Motivation: 传统用户建模方法局限于单一任务，缺乏泛化能力。现有持续学习方法未考虑任务随时间变化带来的新项目分布变化。

Method: 提出DITTO框架，动态适应任务随时间变化带来的项目分布变化，缓解灾难性遗忘。

Result: 实验表明DITTO在实用评估场景下优于现有方法。

Conclusion: DITTO为持续学习用户表示提供了更实用的解决方案。

Abstract: Traditional user modeling (UM) approaches have primarily focused on designing
models for a single specific task, but they face limitations in generalization
and adaptability across various tasks. Recognizing these challenges, recent
studies have shifted towards continual learning (CL)-based universal user
representation learning aiming to develop a single model capable of handling
multiple tasks. Despite advancements, existing methods are in fact evaluated
under an unrealistic scenario that does not consider the passage of time as
tasks progress, which overlooks newly emerged items that may change the item
distribution of previous tasks. In this paper, we introduce a practical
evaluation scenario on which CL-based universal user representation learning
approaches should be evaluated, which takes into account the passage of time as
tasks progress. Then, we propose a novel framework Dynamic Time-aware continual
user representation learner, named DITTO, designed to alleviate catastrophic
forgetting despite continuous shifts in item distribution, while also allowing
the knowledge acquired from previous tasks to adapt to the current shifted item
distribution. Through our extensive experiments, we demonstrate the superiority
of DITTO over state-of-the-art methods under a practical evaluation scenario.
Our source code is available at
https://github.com/seungyoon-Choi/DITTO_official.

</details>


### [137] [A Comprehensive Survey of Synthetic Tabular Data Generation](https://arxiv.org/abs/2504.16506)
*Ruxue Shi,Yili Wang,Mengnan Du,Xu Shen,Xin Wang*

Main category: cs.LG

TL;DR: 这篇论文综述了合成表格数据生成的方法，提出了一个全面的分类法，并分析了传统方法、扩散模型和基于LLM的模型，同时探讨了挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 表格数据在机器学习中的应用受到数据稀缺、隐私问题和类别不平衡等限制，合成数据生成成为解决方案，但现有研究缺乏对LLM和扩散模型的全面分析。

Method: 提出了一个分类法，将方法分为传统方法、扩散模型和基于LLM的模型，并详细描述了合成表格数据的完整流程。

Result: 提供了对现有方法的比较分析，并识别了主要挑战和开放性问题。

Conclusion: 论文为合成表格数据生成领域提供了系统化的综述，并指出了未来研究方向。

Abstract: Tabular data remains one of the most prevalent and critical data formats
across diverse real-world applications. However, its effective use in machine
learning (ML) is often constrained by challenges such as data scarcity, privacy
concerns, and class imbalance. Synthetic data generation has emerged as a
promising solution, leveraging generative models to learn the distribution of
real datasets and produce high-fidelity, privacy-preserving samples. Various
generative paradigms have been explored, including energy-based models (EBMs),
variational autoencoders (VAEs), generative adversarial networks (GANs), large
language models (LLMs), and diffusion models. While several surveys have
investigated synthetic tabular data generation, most focus on narrow subdomains
or specific generative methods, such as GANs, diffusion models, or
privacy-preserving techniques. This limited scope often results in fragmented
insights, lacking a comprehensive synthesis that bridges diverse approaches. In
particular, recent advances driven by LLMs and diffusion-based models remain
underexplored. This gap hinders a holistic understanding of the field`s
evolution, methodological interplay, and open challenges. To address this, our
survey provides a unified and systematic review of synthetic tabular data
generation. Our contributions are threefold: (1) we propose a comprehensive
taxonomy that organizes existing methods into traditional approaches,
diffusion-based methods, and LLM-based models, and provide an in-depth
comparative analysis; (2) we detail the complete pipeline for synthetic tabular
data generation, including data synthesis, post-processing, and evaluation; (3)
we identify major challenges, explore real-world applications, and outline open
research questions and future directions to guide future work in this rapidly
evolving area.

</details>


### [138] [Least-Squares-Embedded Optimization for Accelerated Convergence of PINNs in Acoustic Wavefield Simulations](https://arxiv.org/abs/2504.16553)
*Mohammad Mahdi Abedi,David Pardo,Tariq Alkhalifah*

Main category: cs.LG

TL;DR: 论文提出了一种混合优化框架，通过将最小二乘（LS）求解器嵌入梯度下降（GD）损失函数中，加速了基于物理信息神经网络（PINN）的训练收敛，解决了传统方法在高频波场中的慢收敛和不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 传统PINN在求解偏微分方程（如Helmholtz方程）时，梯度下降训练方法收敛慢且不稳定，尤其是在高频波场中。本文旨在解决这一问题。

Method: 提出了一种混合优化框架，将LS求解器直接嵌入GD损失函数中，优化线性输出层的更新。该方法适用于有或无完美匹配层（PML）的情况，并提供了基于张量的实现。

Result: 数值实验表明，该方法比传统PINN训练收敛更快、精度更高、稳定性更好，尤其是在标准GD训练失败的情况下仍能快速收敛。LS求解器计算开销小，适合大规模波场模拟。

Conclusion: 该方法显著提升了PINN在求解Helmholtz方程时的效率和稳定性，为大规模波场模拟提供了可行的解决方案。

Abstract: Physics-Informed Neural Networks (PINNs) have shown promise in solving
partial differential equations (PDEs), including the frequency-domain Helmholtz
equation. However, standard training of PINNs using gradient descent (GD)
suffers from slow convergence and instability, particularly for high-frequency
wavefields. For scattered acoustic wavefield simulation based on Helmholtz
equation, we derive a hybrid optimization framework that accelerates training
convergence by embedding a least-squares (LS) solver directly into the GD loss
function. This formulation enables optimal updates for the linear output layer.
Our method is applicable with or without perfectly matched layers (PML), and we
provide practical tensor-based implementations for both scenarios. Numerical
experiments on benchmark velocity models demonstrate that our approach achieves
faster convergence, higher accuracy, and improved stability compared to
conventional PINN training. In particular, our results show that the
LS-enhanced method converges rapidly even in cases where standard GD-based
training fails. The LS solver operates on a small normal matrix, ensuring
minimal computational overhead and making the method scalable for large-scale
wavefield simulations.

</details>


### [139] [Unified Molecule Generation and Property Prediction](https://arxiv.org/abs/2504.16559)
*Adam Izdebski,Jan Olszewski,Pankhil Gawade,Krzysztof Koras,Serra Korkmaz,Valentin Rauscher,Jakub M. Tomczak,Ewa Szczurek*

Main category: cs.LG

TL;DR: Hyformer是一个基于Transformer的联合模型，通过交替注意力掩码和统一预训练方案，成功融合了生成和预测功能，性能优于其他联合模型及当前最优的分子生成与属性预测模型。


<details>
  <summary>Details</summary>
Motivation: 联合建模数据样本及其属性的分布可以构建一个兼具生成和预测功能的单一模型，但训练联合模型面临架构和优化的挑战。

Method: 提出Hyformer，采用交替注意力掩码和统一预训练方案，结合生成与预测功能。

Result: Hyformer在分子生成和属性预测任务中表现优异，并在分子表示学习、命中识别和抗菌肽设计等下游任务中展现了联合建模的优势。

Conclusion: Hyformer证明了联合建模的潜力，为生成与预测任务的协同提供了有效解决方案。

Abstract: Modeling the joint distribution of the data samples and their properties
allows to construct a single model for both data generation and property
prediction, with synergistic capabilities reaching beyond purely generative or
predictive models. However, training joint models presents daunting
architectural and optimization challenges. Here, we propose Hyformer, a
transformer-based joint model that successfully blends the generative and
predictive functionalities, using an alternating attention mask together with a
unified pre-training scheme. We show that Hyformer rivals other joint models,
as well as state-of-the-art molecule generation and property prediction models.
Additionally, we show the benefits of joint modeling in downstream tasks of
molecular representation learning, hit identification and antimicrobial peptide
design.

</details>


### [140] [Hyper-Transforming Latent Diffusion Models](https://arxiv.org/abs/2504.16580)
*Ignacio Peis,Batuhan Koyuncu,Isabel Valera,Jes Frellsen*

Main category: cs.LG

TL;DR: 提出了一种结合隐式神经表示（INRs）和基于Transformer的超网络的生成框架，解决了传统MLP超网络的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖MLP超网络，存在可扩展性限制，需要一种更高效且表达能力强的生成框架。

Method: 使用基于Transformer的解码器从潜在变量生成INR参数，扩展了潜在扩散模型（LDMs），并支持从头训练或通过超变换微调解码器。

Result: 框架在表示能力和计算效率上表现优越，能够高效适配现有生成模型到INR表示。

Conclusion: 该方法为生成模型提供了更灵活和高效的INR表示生成途径。

Abstract: We introduce a novel generative framework for functions by integrating
Implicit Neural Representations (INRs) and Transformer-based hypernetworks into
latent variable models. Unlike prior approaches that rely on MLP-based
hypernetworks with scalability limitations, our method employs a
Transformer-based decoder to generate INR parameters from latent variables,
addressing both representation capacity and computational efficiency. Our
framework extends latent diffusion models (LDMs) to INR generation by replacing
standard decoders with a Transformer-based hypernetwork, which can be trained
either from scratch or via hyper-transforming-a strategy that fine-tunes only
the decoder while freezing the pre-trained latent space. This enables efficient
adaptation of existing generative models to INR-based representations without
requiring full retraining.

</details>


### [141] [Enhancing Variable Selection in Large-scale Logistic Regression: Leveraging Manual Labeling with Beneficial Noise](https://arxiv.org/abs/2504.16585)
*Xiaofei Wu,Rongmei Liang*

Main category: cs.LG

TL;DR: 本文研究了带标签噪声的惩罚逻辑回归（PLR）在大规模监督学习中的表现，提出了一种基于ADMM的分区不敏感并行算法，证明标签噪声对变量选择有益，并提高了估计和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 在大规模监督学习中，惩罚逻辑回归（PLR）通过正则化解决过拟合问题，但其性能依赖于高效的变量选择策略。本文旨在探讨手动标注引入的标签噪声对PLR变量选择的潜在益处。

Method: 提出了一种基于ADMM的分区不敏感并行算法，用于处理大规模PLR模型，并证明该算法在分布式存储数据时具有全局收敛性和次线性收敛速率。

Result: 实验结果表明，与传统变量选择方法相比，使用带标签噪声的PLR在多个大规模数据集上实现了更高的估计和分类准确性。

Conclusion: 标签噪声在PLR变量选择中具有积极作用，提出的并行算法有效解决了大规模数据存储问题，并提升了模型性能。

Abstract: In large-scale supervised learning, penalized logistic regression (PLR)
effectively addresses the overfitting problem by introducing regularization
terms yet its performance still depends on efficient variable selection
strategies. This paper theoretically demonstrates that label noise stemming
from manual labeling, which is solely related to classification difficulty,
represents a type of beneficial noise for variable selection in PLR. This
benefit is reflected in a more accurate estimation of the selected non-zero
coefficients when compared with the case where only truth labels are used.
Under large-scale settings, the sample size for PLR can become very large,
making it infeasible to store on a single machine. In such cases, distributed
computing methods are required to handle PLR model with manual labeling. This
paper presents a partition-insensitive parallel algorithm founded on the ADMM
(alternating direction method of multipliers) algorithm to address PLR by
incorporating manual labeling. The partition insensitivity of the proposed
algorithm refers to the fact that the solutions obtained by the algorithm will
not change with the distributed storage of data. In addition, the algorithm has
global convergence and a sublinear convergence rate. Experimental results
indicate that, as compared with traditional variable selection classification
techniques, the PLR with manually-labeled noisy data achieves higher estimation
and classification accuracy across multiple large-scale datasets.

</details>


### [142] [Compositional Active Learning of Synchronous Systems through Automated Alphabet Refinement](https://arxiv.org/abs/2504.16624)
*Leo Henry,Thomas Neele,Mohammad Mousavi,Matteo Sammartino*

Main category: cs.LG

TL;DR: 论文提出了一种组合式学习方法，用于同步并行系统的自动机学习，通过自动细化全局字母表为组件字母表，并开发了理论框架和算法CoalA，实验显示其显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 现有的组合式学习方法在处理未知分解的同步并行系统时存在局限性，需要更通用的技术来解决这一问题。

Method: 开发了一种组合式学习算法，自动细化全局字母表为组件字母表，并理论分析了字母表分布及其一致性恢复方法。

Result: 实验在630多个系统中验证了CoalA的效率提升，成员查询减少了五个数量级，并发系统中等价查询的可扩展性也更好。

Conclusion: 论文提出的组合式学习方法显著提升了同步并行系统自动机学习的效率和可扩展性。

Abstract: Active automata learning infers automaton models of systems from behavioral
observations, a technique successfully applied to a wide range of domains.
Compositional approaches for concurrent systems have recently emerged. We take
a significant step beyond available results, including those by the authors,
and develop a general technique for compositional learning of a synchronizing
parallel system with an unknown decomposition. Our approach automatically
refines the global alphabet into component alphabets while learning the
component models. We develop a theoretical treatment of distributions of
alphabets, i.e., sets of possibly overlapping component alphabets. We
characterize counter-examples that reveal inconsistencies with global
observations, and show how to systematically update the distribution to restore
consistency. We present a compositional learning algorithm implementing these
ideas, where learning counterexamples precisely correspond to distribution
counterexamples under well-defined conditions. We provide an implementation,
called CoalA, using the state-of-the-art active learning library LearnLib. Our
experiments show that in more than 630 subject systems, CoalA delivers orders
of magnitude improvements (up to five orders) in membership queries and in
systems with significant concurrency, it also achieves better scalability in
the number of equivalence queries.

</details>


### [143] [ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data](https://arxiv.org/abs/2504.16628)
*Haoran Gu,Handing Wang,Yi Mei,Mengjie Zhang,Yaochu Jin*

Main category: cs.LG

TL;DR: 论文提出ParetoHqD方法，通过将人类偏好表示为目标空间中的偏好方向，并利用帕累托前沿附近的高质量数据，解决了多目标对齐算法中的偏好表示和奖励不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型能够满足多样化的用户需求，需要与多种人类期望和价值观对齐。现有方法如Rewards-in-Context算法存在偏好表示不当和奖励不平衡的问题。

Method: 提出ParetoHqD方法，将偏好表示为目标空间的方向，利用帕累托前沿数据作为高质量数据，并采用两阶段监督微调过程。

Result: 实验表明，ParetoHqD在两个多目标对齐任务上优于五种基线方法。

Conclusion: ParetoHqD通过改进偏好表示和数据选择，显著提升了多目标对齐的性能。

Abstract: Aligning large language models with multiple human expectations and values is
crucial for ensuring that they adequately serve a variety of user needs. To
this end, offline multiobjective alignment algorithms such as the
Rewards-in-Context algorithm have shown strong performance and efficiency.
However, inappropriate preference representations and training with imbalanced
reward scores limit the performance of such algorithms. In this work, we
introduce ParetoHqD that addresses the above issues by representing human
preferences as preference directions in the objective space and regarding data
near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD
follows a two-stage supervised fine-tuning process, where each stage uses an
individual Pareto high-quality training set that best matches its preference
direction. The experimental results have demonstrated the superiority of
ParetoHqD over five baselines on two multiobjective alignment tasks.

</details>


### [144] [DAPLSR: Data Augmentation Partial Least Squares Regression Model via Manifold Optimization](https://arxiv.org/abs/2504.16639)
*Haoran Chen,Jiapeng Liu,Jiafan Wang,Wenjun Shi*

Main category: cs.LG

TL;DR: 本文提出了一种基于流形优化的数据增强偏最小二乘回归（DAPLSR）模型，通过SMOTE增加样本数量，并使用VDM选择近邻样本生成合成样本，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统偏最小二乘回归（PLSR）在处理类别不平衡数据时表现不佳，需要改进。

Method: 提出DAPLSR模型，结合SMOTE和VDM生成合成样本，并采用流形优化方法提高数值解的准确性。

Result: 实验表明，DAPLSR在多个数据集上分类性能优异，显著优于现有方法。

Conclusion: DAPLSR通过数据增强和流形优化，有效解决了PLSR在类别不平衡数据中的问题，性能显著提升。

Abstract: Traditional Partial Least Squares Regression (PLSR) models frequently
underperform when handling data characterized by uneven categories. To address
the issue, this paper proposes a Data Augmentation Partial Least Squares
Regression (DAPLSR) model via manifold optimization. The DAPLSR model
introduces the Synthetic Minority Over-sampling Technique (SMOTE) to increase
the number of samples and utilizes the Value Difference Metric (VDM) to select
the nearest neighbor samples that closely resemble the original samples for
generating synthetic samples. In solving the model, in order to obtain a more
accurate numerical solution for PLSR, this paper proposes a manifold
optimization method that uses the geometric properties of the constraint space
to improve model degradation and optimization. Comprehensive experiments show
that the proposed DAPLSR model achieves superior classification performance and
outstanding evaluation metrics on various datasets, significantly outperforming
existing methods.

</details>


### [145] [Representation Learning via Non-Contrastive Mutual Information](https://arxiv.org/abs/2504.16667)
*Zhaohan Daniel Guo,Bernardo Avila Pires,Khimya Khetarpal,Dale Schuurmans,Bo Dai*

Main category: cs.LG

TL;DR: 论文提出了一种结合对比和非对比自监督学习优势的新目标函数MINC，通过改进谱对比损失，降低了方差并防止崩溃，在ImageNet上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 数据标注成本高，自监督学习能从无标签数据中学习有用表示，但现有方法各有不足（对比方法方差高，非对比方法易崩溃），需结合两者优势。

Method: 基于谱对比损失，将其转化为非对比形式（MINC），避免成对比较但保留互信息，降低方差且防止崩溃。

Result: 在ImageNet上测试，MINC表现优于谱对比损失基线。

Conclusion: MINC结合了对比和非对比方法的优点，是一种有效的自监督学习目标函数。

Abstract: Labeling data is often very time consuming and expensive, leaving us with a
majority of unlabeled data. Self-supervised representation learning methods
such as SimCLR (Chen et al., 2020) or BYOL (Grill et al., 2020) have been very
successful at learning meaningful latent representations from unlabeled image
data, resulting in much more general and transferable representations for
downstream tasks. Broadly, self-supervised methods fall into two types: 1)
Contrastive methods, such as SimCLR; and 2) Non-Contrastive methods, such as
BYOL. Contrastive methods are generally trying to maximize mutual information
between related data points, so they need to compare every data point to every
other data point, resulting in high variance, and thus requiring large batch
sizes to work well. Non-contrastive methods like BYOL have much lower variance
as they do not need to make pairwise comparisons, but are much trickier to
implement as they have the possibility of collapsing to a constant vector. In
this paper, we aim to develop a self-supervised objective that combines the
strength of both types. We start with a particular contrastive method called
the Spectral Contrastive Loss (HaoChen et al., 2021; Lu et al., 2024), and we
convert it into a more general non-contrastive form; this removes the pairwise
comparisons resulting in lower variance, but keeps the mutual information
formulation of the contrastive method preventing collapse. We call our new
objective the Mutual Information Non-Contrastive (MINC) loss. We test MINC by
learning image representations on ImageNet (similar to SimCLR and BYOL) and
show that it consistently improves upon the Spectral Contrastive loss baseline.

</details>


### [146] [Efficient Data Valuation Approximation in Federated Learning: A Sampling-based Approach](https://arxiv.org/abs/2504.16668)
*Shuyue Wei,Yongxin Tong,Zimu Zhou,Tianran He,Yi Xu*

Main category: cs.LG

TL;DR: 论文提出了一种高效近似计算Shapley值的方法IPSS，用于联邦学习中的数据估值，通过选择关键数据集组合显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中数据提供者因计算Shapley值的高开销而难以公平评估数据价值的问题。

Method: 提出分层采样框架，分析并选择适合的计算方案，识别关键组合，开发IPSS算法。

Result: IPSS在效率和准确性上优于现有基线方法。

Conclusion: IPSS为联邦学习中的数据估值提供了实用且高效的解决方案。

Abstract: Federated learning paradigm to utilize datasets across multiple data
providers. In FL, cross-silo data providers often hesitate to share their
high-quality dataset unless their data value can be fairly assessed. Shapley
value (SV) has been advocated as the standard metric for data valuation in FL
due to its desirable properties. However, the computational overhead of SV is
prohibitive in practice, as it inherently requires training and evaluating an
FL model across an exponential number of dataset combinations. Furthermore,
existing solutions fail to achieve high accuracy and efficiency, making
practical use of SV still out of reach, because they ignore choosing suitable
computation scheme for approximation framework and overlook the property of
utility function in FL. We first propose a unified stratified-sampling
framework for two widely-used schemes. Then, we analyze and choose the more
promising scheme under the FL linear regression assumption. After that, we
identify a phenomenon termed key combinations, where only limited dataset
combinations have a high-impact on final data value. Building on these
insights, we propose a practical approximation algorithm, IPSS, which
strategically selects high-impact dataset combinations rather than evaluating
all possible combinations, thus substantially reducing time cost with minor
approximation error. Furthermore, we conduct extensive evaluations on the FL
benchmark datasets to demonstrate that our proposed algorithm outperforms a
series of representative baselines in terms of efficiency and effectiveness.

</details>


### [147] [Provable wavelet-based neural approximation](https://arxiv.org/abs/2504.16682)
*Youngmi Hur,Hyojae Lim,Mikyoung Lim*

Main category: cs.LG

TL;DR: 本文提出了一种基于小波的框架，用于分析神经网络在多种激活函数下的通用逼近能力，并给出了误差估计。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络的通用逼近能力，特别是针对不同平滑性和振荡行为的激活函数。

Method: 利用小波框架理论，推导出激活函数的充分条件，确保神经网络能够逼近给定空间中的任意函数，并控制误差。

Result: 结果表明，该方法适用于多种平滑激活函数，甚至包括非平滑激活函数，误差可通过距离明确控制。

Conclusion: 该框架为网络架构设计提供了更大的灵活性，扩展了神经网络的应用范围。

Abstract: In this paper, we develop a wavelet-based theoretical framework for analyzing
the universal approximation capabilities of neural networks over a wide range
of activation functions. Leveraging wavelet frame theory on the spaces of
homogeneous type, we derive sufficient conditions on activation functions to
ensure that the associated neural network approximates any functions in the
given space, along with an error estimate. These sufficient conditions
accommodate a variety of smooth activation functions, including those that
exhibit oscillatory behavior. Furthermore, by considering the $L^2$-distance
between smooth and non-smooth activation functions, we establish a generalized
approximation result that is applicable to non-smooth activations, with the
error explicitly controlled by this distance. This provides increased
flexibility in the design of network architectures.

</details>


### [148] [MCMC for Bayesian estimation of Differential Privacy from Membership Inference Attacks](https://arxiv.org/abs/2504.16683)
*Ceren Yildirim,Kamer Kaya,Sinan Yildirim,Erkay Savas*

Main category: cs.LG

TL;DR: 提出了一种基于贝叶斯估计的差分隐私框架，结合多成员推理攻击（MIA）证据，通过MCMC算法估计隐私参数的后验分布。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设隐私审计基于最强大的攻击和最坏情况，不现实，需要更谨慎的隐私分析方法。

Method: 使用MCMC-DP-Est算法联合估计MIA强度和训练算法的隐私性，并提供经济高效的MIA性能测量方法。

Result: 通过数值实验（人工和真实数据）验证了方法的有效性。

Conclusion: 该方法提供了一种更现实的隐私分析框架，适用于实际场景。

Abstract: We propose a new framework for Bayesian estimation of differential privacy,
incorporating evidence from multiple membership inference attacks (MIA).
Bayesian estimation is carried out via a Markov chain Monte Carlo (MCMC)
algorithm, named MCMC-DP-Est, which provides an estimate of the full posterior
distribution of the privacy parameter (e.g., instead of just credible
intervals). Critically, the proposed method does not assume that privacy
auditing is performed with the most powerful attack on the worst-case (dataset,
challenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est
jointly estimates the strengths of MIAs used and the privacy of the training
algorithm, yielding a more cautious privacy analysis. We also present an
economical way to generate measurements for the performance of an MIA that is
to be used by the MCMC method to estimate privacy. We present the use of the
methods with numerical examples with both artificial and real data.

</details>


### [149] [PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation](https://arxiv.org/abs/2504.16693)
*Wenxuan Li,Hang Zhao,Zhiyuan Yu,Yu Du,Qin Zou,Ruizhen Hu,Kai Xu*

Main category: cs.LG

TL;DR: 论文提出了一种基于物理信息的世界模型PIN-WM，用于学习3D刚体动力学，并通过模型强化学习实现非抓取操作的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 非抓取操作（如推/戳）对复杂物理交互（如摩擦和恢复）高度敏感，学习鲁棒策略具有挑战性。

Method: 采用可微分物理模拟，通过视觉观察端到端学习3D刚体动力学模型PIN-WM，无需状态估计，并通过物理感知随机化生成数字变体以弥合Sim2Real差距。

Result: 在仿真和真实测试中，PIN-WM结合数字变体显著提升了非抓取操作技能的鲁棒性和Sim2Real迁移能力，超越现有方法。

Conclusion: PIN-WM通过物理信息建模和随机化策略，有效解决了非抓取操作的学习难题，并实现了高效的Sim2Real迁移。

Abstract: While non-prehensile manipulation (e.g., controlled pushing/poking)
constitutes a foundational robotic skill, its learning remains challenging due
to the high sensitivity to complex physical interactions involving friction and
restitution. To achieve robust policy learning and generalization, we opt to
learn a world model of the 3D rigid body dynamics involved in non-prehensile
manipulations and use it for model-based reinforcement learning. We propose
PIN-WM, a Physics-INformed World Model that enables efficient end-to-end
identification of a 3D rigid body dynamical system from visual observations.
Adopting differentiable physics simulation, PIN-WM can be learned with only
few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM
is learned with observational loss induced by Gaussian Splatting without
needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM
into a group of Digital Cousins via physics-aware randomizations which perturb
physics and rendering parameters to generate diverse and meaningful variations
of the PIN-WM. Extensive evaluations on both simulation and real-world tests
demonstrate that PIN-WM, enhanced with physics-aware digital cousins,
facilitates learning robust non-prehensile manipulation skills with Sim2Real
transfer, surpassing the Real2Sim2Real state-of-the-arts.

</details>


### [150] [A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization](https://arxiv.org/abs/2504.16711)
*Shiyin Tan,Jaeeon Park,Dongyuan Li,Renhe Jiang,Manabu Okumura*

Main category: cs.LG

TL;DR: 提出了一种新的检索框架，通过整合查询选择和文档排序缩短过程，解决了多文档摘要中输入长度限制的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的模型在多文档摘要中受限于输入长度，且依赖人工查询和粗粒度检索。

Method: 利用潜在查询（EDUs）指导文档排序，过滤无关内容以适配上下文长度。

Result: 在多个数据集上ROUGE指标提升，验证了框架的扩展性和灵活性。

Conclusion: 该框架有效解决了上下文长度限制，成为多文档摘要的可靠解决方案。

Abstract: In the field of multi-document summarization (MDS), transformer-based models
have demonstrated remarkable success, yet they suffer an input length
limitation. Current methods apply truncation after the retrieval process to fit
the context length; however, they heavily depend on manually well-crafted
queries, which are impractical to create for each document set for MDS.
Additionally, these methods retrieve information at a coarse granularity,
leading to the inclusion of irrelevant content. To address these issues, we
propose a novel retrieval-based framework that integrates query selection and
document ranking and shortening into a unified process. Our approach identifies
the most salient elementary discourse units (EDUs) from input documents and
utilizes them as latent queries. These queries guide the document ranking by
calculating relevance scores. Instead of traditional truncation, our approach
filters out irrelevant EDUs to fit the context length, ensuring that only
critical information is preserved for summarization. We evaluate our framework
on multiple MDS datasets, demonstrating consistent improvements in ROUGE
metrics while confirming its scalability and flexibility across diverse model
architectures. Additionally, we validate its effectiveness through an in-depth
analysis, emphasizing its ability to dynamically select appropriate queries and
accurately rank documents based on their relevance scores. These results
demonstrate that our framework effectively addresses context-length
constraints, establishing it as a robust and reliable solution for MDS.

</details>


### [151] [Simple Graph Contrastive Learning via Fractional-order Neural Diffusion Networks](https://arxiv.org/abs/2504.16748)
*Yanan Zhao,Feng Ji,Kai Zhao,Xuhao Li,Qiyu Kang,Wenfei Liang,Yahya Alkhatib,Xingchao Jian,Wee Peng Tay*

Main category: cs.LG

TL;DR: 提出了一种基于图神经扩散模型的无增强图对比学习框架，无需负样本训练，适用于同质和异质数据集，性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决现有图对比学习方法依赖复杂数据增强或负样本的问题，提出一种更高效的无增强方法。

Method: 利用分数微分方程（FDE）的可学习编码器，通过调整微分算子参数生成多样视图，用于对比学习。

Result: 在多个数据集上验证了模型的有效性，达到了最先进的性能。

Conclusion: 该框架为无增强图对比学习提供了新思路，具有广泛适用性和高效性。

Abstract: Graph Contrastive Learning (GCL) has recently made progress as an
unsupervised graph representation learning paradigm. GCL approaches can be
categorized into augmentation-based and augmentation-free methods. The former
relies on complex data augmentations, while the latter depends on encoders that
can generate distinct views of the same input. Both approaches may require
negative samples for training. In this paper, we introduce a novel
augmentation-free GCL framework based on graph neural diffusion models.
Specifically, we utilize learnable encoders governed by Fractional Differential
Equations (FDE). Each FDE is characterized by an order parameter of the
differential operator. We demonstrate that varying these parameters allows us
to produce learnable encoders that generate diverse views, capturing either
local or global information, for contrastive learning. Our model does not
require negative samples for training and is applicable to both homophilic and
heterophilic datasets. We demonstrate its effectiveness across various
datasets, achieving state-of-the-art performance.

</details>


### [152] [QAOA-PCA: Enhancing Efficiency in the Quantum Approximate Optimization Algorithm via Principal Component Analysis](https://arxiv.org/abs/2504.16755)
*Owain Parry,Phil McMinn*

Main category: cs.LG

TL;DR: QAOA-PCA是一种基于主成分分析（PCA）的新参数化方法，用于减少QAOA算法的参数空间维度，从而提升优化效率。


<details>
  <summary>Details</summary>
Motivation: 随着QAOA电路层数增加，参数优化所需的计算负担加重，QAOA-PCA旨在通过降维解决这一问题。

Method: 利用PCA从较小问题实例的优化参数中提取主成分，以减少较大实例的参数数量。

Result: 在MaxCut问题上，QAOA-PCA比标准QAOA需要更少的迭代次数，效率显著提升，但近似比略有下降。

Conclusion: QAOA-PCA在效率和性能之间取得了良好平衡，显著减少了优化开销，同时未显著影响解的质量。

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is a promising
variational algorithm for solving combinatorial optimization problems on
near-term devices. However, as the number of layers in a QAOA circuit
increases, which is correlated with the quality of the solution, the number of
parameters to optimize grows linearly. This results in more iterations required
by the classical optimizer, which results in an increasing computational burden
as more circuit executions are needed. To mitigate this issue, we introduce
QAOA-PCA, a novel reparameterization technique that employs Principal Component
Analysis (PCA) to reduce the dimensionality of the QAOA parameter space. By
extracting principal components from optimized parameters of smaller problem
instances, QAOA-PCA facilitates efficient optimization with fewer parameters on
larger instances. Our empirical evaluation on the prominent MaxCut problem
demonstrates that QAOA-PCA consistently requires fewer iterations than standard
QAOA, achieving substantial efficiency gains. While this comes at the cost of a
slight reduction in approximation ratio compared to QAOA with the same number
of layers, QAOA-PCA almost always outperforms standard QAOA when matched by
parameter count. QAOA-PCA strikes a favorable balance between efficiency and
performance, reducing optimization overhead without significantly compromising
solution quality.

</details>


### [153] [Noise-Tolerant Coreset-Based Class Incremental Continual Learning](https://arxiv.org/abs/2504.16763)
*Edison Mucllari,Aswin Raghavan,Zachary Alan Daniels*

Main category: cs.LG

TL;DR: 论文研究了在类增量学习中标签噪声和实例噪声对持续学习方法的影响，提出了两种抗噪声的持续学习算法，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉应用需要适应新数据分布，持续学习是关键，但噪声会干扰学习过程。本文旨在理解噪声对基于Coresets的持续学习方法的影响。

Method: 提出了一种新的理论界限来分析噪声鲁棒性，并设计了两种抗噪声的持续学习算法，构建噪声容忍的重放缓冲区。

Result: 实验表明，现有方法对噪声不鲁棒，而提出的方法在噪声环境下显著提高了分类准确率并减少了遗忘。

Conclusion: 提出的算法在噪声环境下表现优越，为持续学习中的噪声问题提供了有效解决方案。

Abstract: Many applications of computer vision require the ability to adapt to novel
data distributions after deployment. Adaptation requires algorithms capable of
continual learning (CL). Continual learners must be plastic to adapt to novel
tasks while minimizing forgetting of previous tasks.However, CL opens up
avenues for noise to enter the training pipeline and disrupt the CL. This work
focuses on label noise and instance noise in the context of class-incremental
learning (CIL), where new classes are added to a classifier over time, and
there is no access to external data from past classes. We aim to understand the
sensitivity of CL methods that work by replaying items from a memory
constructed using the idea of Coresets. We derive a new bound for the
robustness of such a method to uncorrelated instance noise under a general
additive noise threat model, revealing several insights. Putting the theory
into practice, we create two continual learning algorithms to construct
noise-tolerant replay buffers. We empirically compare the effectiveness of
prior memory-based continual learners and the proposed algorithms under label
and uncorrelated instance noise on five diverse datasets. We show that existing
memory-based CL are not robust whereas the proposed methods exhibit significant
improvements in maximizing classification accuracy and minimizing forgetting in
the noisy CIL setting.

</details>


### [154] [Online model learning with data-assimilated reservoir computers](https://arxiv.org/abs/2504.16767)
*Andrea Nóvoa,Luca Magri*

Main category: cs.LG

TL;DR: 提出了一种在线学习框架，用于预测非线性时空信号，结合了降维、广义自回归模型和在线适应技术，并在圆柱尾流实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决非线性时空信号预测的挑战，通过结合数据驱动降阶建模和贝叶斯数据同化，实现在线模型学习。

Method: 集成POD降维、储层计算器（广义自回归模型）和集合序贯数据同化（在线适应），并在Navier-Stokes方程控制的圆柱尾流中进行实验。

Result: 两重和三重状态估计策略显著提高了集合收敛性和重建精度，三重重策略支持部分训练储层计算器的在线训练。

Conclusion: 该框架为非线性时间序列预测提供了可扩展的在线模型学习方法，结合了数据驱动和贝叶斯技术的优势。

Abstract: We propose an online learning framework for forecasting nonlinear
spatio-temporal signals (fields). The method integrates (i) dimensionality
reduction, here, a simple proper orthogonal decomposition (POD) projection;
(ii) a generalized autoregressive model to forecast reduced dynamics, here, a
reservoir computer; (iii) online adaptation to update the reservoir computer
(the model), here, ensemble sequential data assimilation.We demonstrate the
framework on a wake past a cylinder governed by the Navier-Stokes equations,
exploring the assimilation of full flow fields (projected onto POD modes) and
sparse sensors. Three scenarios are examined: a na\"ive physical state
estimation; a two-fold estimation of physical and reservoir states; and a
three-fold estimation that also adjusts the model parameters. The two-fold
strategy significantly improves ensemble convergence and reduces reconstruction
error compared to the na\"ive approach. The three-fold approach enables robust
online training of partially-trained reservoir computers, overcoming
limitations of a priori training. By unifying data-driven reduced order
modelling with Bayesian data assimilation, this work opens new opportunities
for scalable online model learning for nonlinear time series forecasting.

</details>


### [155] [Process Reward Models That Think](https://arxiv.org/abs/2504.16828)
*Muhammad Khalifa,Rishabh Agarwal,Lajanugen Logeswaran,Jaekyeom Kim,Hao Peng,Moontae Lee,Honglak Lee,Lu Wang*

Main category: cs.LG

TL;DR: ThinkPRM是一种基于生成式长链思维验证模型（CoT），通过极少量过程标签训练，优于传统判别式验证模型和LLM-as-a-Judge方法。


<details>
  <summary>Details</summary>
Motivation: 传统过程奖励模型（PRMs）需要大量步骤级监督数据，训练成本高。本研究旨在开发数据高效的PRMs，通过生成验证链式思维（CoT）来验证每一步。

Method: 提出ThinkPRM，利用长链CoT模型的推理能力，仅需1%的过程标签（PRM800K数据集）进行微调。

Result: 在多个基准测试（ProcessBench、MATH-500、AIME '24等）中表现优于基线方法，且在域外评估（GPQA-Diamond和LiveCodeBench）中分别提升8%和4.5%。

Conclusion: 生成式长链CoT PRMs能够高效扩展验证计算，同时减少训练监督需求，具有显著优势。

Abstract: Step-by-step verifiers -- also known as process reward models (PRMs) -- are a
key ingredient for test-time scaling. PRMs require step-level supervision,
making them expensive to train. This work aims to build data-efficient PRMs as
verbalized step-wise reward models that verify every step in the solution by
generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long
CoT verifier fine-tuned on orders of magnitude fewer process labels than those
required by discriminative PRMs. Our approach capitalizes on the inherent
reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and
discriminative verifiers -- using only 1% of the process labels in PRM800K --
across several challenging benchmarks. Specifically, ThinkPRM beats the
baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and
reward-guided search. In an out-of-domain evaluation on a subset of
GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers
trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the
same token budget, ThinkPRM scales up verification compute more effectively
compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of
ProcessBench. Our work highlights the value of generative, long CoT PRMs that
can scale test-time compute for verification while requiring minimal
supervision for training. Our code, data, and models will be released at
https://github.com/mukhal/thinkprm.

</details>


### [156] [Evaluating Autoencoders for Parametric and Invertible Multidimensional Projections](https://arxiv.org/abs/2504.16831)
*Frederik L. Dennig,Nina Geyer,Daniela Blumberg,Yannick Metz,Daniel A. Keim*

Main category: cs.LG

TL;DR: 该论文研究了如何通过自编码器（AE）实现参数化和可逆的多维数据投影，并通过定量和定性分析验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 探索同时实现参数化和可逆投影的方法，填补了现有研究的空白。

Method: 评估了三种AE架构，训练其学习2D空间和原始空间的映射，并使用t-SNE在四个数据集上进行比较。

Result: 结果表明，定制损失函数的AE能生成更平滑的参数化和逆投影，且用户可控制平滑强度。

Conclusion: AE在实现参数化和可逆投影方面优于前馈神经网络，具有实际应用潜力。

Abstract: Recently, neural networks have gained attention for creating parametric and
invertible multidimensional data projections. Parametric projections allow for
embedding previously unseen data without recomputing the projection as a whole,
while invertible projections enable the generation of new data points. However,
these properties have never been explored simultaneously for arbitrary
projection methods. We evaluate three autoencoder (AE) architectures for
creating parametric and invertible projections. Based on a given projection, we
train AEs to learn a mapping into 2D space and an inverse mapping into the
original space. We perform a quantitative and qualitative comparison on four
datasets of varying dimensionality and pattern complexity using t-SNE. Our
results indicate that AEs with a customized loss function can create smoother
parametric and inverse projections than feed-forward neural networks while
giving users control over the strength of the smoothing effect.

</details>


### [157] [Improving Significant Wave Height Prediction Using Chronos Models](https://arxiv.org/abs/2504.16834)
*Yilin Zhai,Hongyuan Shi,Chao Zhan,Qing Wang,Zaijin You,Nan Wang*

Main category: cs.LG

TL;DR: Chronos是一种基于大型语言模型（LLM）的时间架构，用于波浪高度预测，显著提升了计算效率和预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统物理模型和机器学习方法在计算效率和非线性动态建模方面存在不足，需要更高效的解决方案。

Method: 利用LLM增强的时间模式识别技术，分析西北太平洋三个海域的历史波浪数据。

Result: 训练时间减少14.3%，推理速度提升2.5倍，短期和长期预测表现优异，具备零样本能力。

Conclusion: Chronos为波浪预测设定了新标准，提供了高效且可迁移的复杂地球物理系统建模框架。

Abstract: Accurate wave height prediction is critical for maritime safety and coastal
resilience, yet conventional physics-based models and traditional machine
learning methods face challenges in computational efficiency and nonlinear
dynamics modeling. This study introduces Chronos, the first implementation of a
large language model (LLM)-powered temporal architecture (Chronos) optimized
for wave forecasting. Through advanced temporal pattern recognition applied to
historical wave data from three strategically chosen marine zones in the
Northwest Pacific basin, our framework achieves multimodal improvements: (1)
14.3% reduction in training time with 2.5x faster inference speed compared to
PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units;
(2) superior short-term forecasting (1-24h) across comprehensive metrics; (3)
sustained predictive leadership in extended-range forecasts (1-120h); and (4)
demonstrated zero-shot capability maintaining median performance (rank 4/12)
against specialized operational models. This LLM-enhanced temporal modeling
paradigm establishes a new standard in wave prediction, offering both
computationally efficient solutions and a transferable framework for complex
geophysical systems modeling.

</details>


### [158] [An Adaptive ML Framework for Power Converter Monitoring via Federated Transfer Learning](https://arxiv.org/abs/2504.16866)
*Panagiotis Kakosimos,Alireza Nemat Saberi,Luca Peretti*

Main category: cs.LG

TL;DR: 该研究提出了一种结合迁移学习（TL）和联邦学习（FL）的分段框架，用于适应热机器学习模型在功率转换器中的应用，解决了数据共享限制和安全性问题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决功率转换器中热机器学习模型在不同操作条件下的适应性问题，同时应对数据共享和安全的挑战。

Method: 采用分段框架，结合TL和FL，使用微调、TCA和DDA三种域适应技术，并通过Flower框架实现FL。

Result: 验证表明微调方法简单且高精度，适合实际应用；本地FL在数据聚合不可行时表现更优，云端FL在客户数量增加时更具扩展性。

Conclusion: 该框架为热机器学习模型在功率转换器中的应用提供了灵活且高效的解决方案，适应不同场景需求。

Abstract: This study explores alternative framework configurations for adapting thermal
machine learning (ML) models for power converters by combining transfer
learning (TL) and federated learning (FL) in a piecewise manner. This approach
inherently addresses challenges such as varying operating conditions, data
sharing limitations, and security implications. The framework starts with a
base model that is incrementally adapted by multiple clients via adapting three
state-of-the-art domain adaptation techniques: Fine-tuning, Transfer Component
Analysis (TCA), and Deep Domain Adaptation (DDA). The Flower framework is
employed for FL, using Federated Averaging for aggregation. Validation with
field data demonstrates that fine-tuning offers a straightforward TL approach
with high accuracy, making it suitable for practical applications. Benchmarking
results reveal a comprehensive comparison of these methods, showcasing their
respective strengths and weaknesses when applied in different scenarios.
Locally hosted FL enhances performance when data aggregation is not feasible,
while cloud-based FL becomes more practical with a significant increase in the
number of clients, addressing scalability and connectivity challenges.

</details>


### [159] [Exploring How LLMs Capture and Represent Domain-Specific Knowledge](https://arxiv.org/abs/2504.16871)
*Mirian Hipolito Garcia,Camille Couturier,Daniel Madrigal Diaz,Ankur Mallick,Anastasios Kyrillidis,Robert Sim,Victor Ruhle,Saravan Rajmohan*

Main category: cs.LG

TL;DR: 研究大型语言模型（LLMs）是否自然捕捉领域特定语言细微差别，通过隐藏状态分析其领域敏感性，并利用领域轨迹优化模型选择。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否能够区分不同领域的查询，并研究其内部对领域的识别能力。

Method: 通过预填充阶段的隐藏状态分析领域敏感性，利用领域轨迹进行模型选择。

Result: LLMs能区分相关领域查询，且微调模型并非总是最准确。

Conclusion: LLMs具备领域识别能力，模型选择方法适用于封闭和开放生成任务。

Abstract: We study whether Large Language Models (LLMs) inherently capture
domain-specific nuances in natural language. Our experiments probe the domain
sensitivity of LLMs by examining their ability to distinguish queries from
different domains using hidden states generated during the prefill phase. We
reveal latent domain-related trajectories that indicate the model's internal
recognition of query domains. We also study the robustness of these domain
representations to variations in prompt styles and sources. Our approach
leverages these representations for model selection, mapping the LLM that best
matches the domain trace of the input query (i.e., the model with the highest
performance on similar traces). Our findings show that LLMs can differentiate
queries for related domains, and that the fine-tuned model is not always the
most accurate. Unlike previous work, our interpretations apply to both closed
and open-ended generative tasks

</details>


### [160] [Hybrid Reinforcement Learning and Model Predictive Control for Adaptive Control of Hydrogen-Diesel Dual-Fuel Combustion](https://arxiv.org/abs/2504.16875)
*Julian Bedei,Murray McBain,Charles Robert Koch,Jakob Andert,David Gordon*

Main category: cs.LG

TL;DR: 论文提出了一种结合强化学习（RL）和机器学习模型预测控制（ML-MPC）的混合方法，用于优化氢-柴油双燃料发动机控制，解决了单独使用RL或ML-MPC的局限性。


<details>
  <summary>Details</summary>
Motivation: RL和ML-MPC在控制多输入多输出系统和非线性过程中各有优势，但单独使用时存在RL早期学习阶段的高风险行为和ML-MPC对系统漂移的适应性不足的问题。

Method: 采用混合方法，ML-MPC提供安全控制，RL动态调整ML-MPC的负载跟踪参考以适应环境变化。

Result: 实验表明，混合方法在燃料压力变化时显著降低了负载跟踪的均方根误差（RMSE从0.57降至0.44 bar）。

Conclusion: 混合方法结合了RL的适应性和ML-MPC的安全性，有效提升了发动机控制的性能。

Abstract: Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive
Control (ML-MPC) are promising approaches for optimizing hydrogen-diesel
dual-fuel engine control, as they can effectively control multiple-input
multiple-output systems and nonlinear processes. ML-MPC is advantageous for
providing safe and optimal controls, ensuring the engine operates within
predefined safety limits. In contrast, RL is distinguished by its adaptability
to changing conditions through its learning-based approach. However, the
practical implementation of either method alone poses challenges. RL requires
high variance in control inputs during early learning phases, which can pose
risks to the system by potentially executing unsafe actions, leading to
mechanical damage. Conversely, ML-MPC relies on an accurate system model to
generate optimal control inputs and has limited adaptability to system drifts,
such as injector aging, which naturally occur in engine applications. To
address these limitations, this study proposes a hybrid RL and ML-MPC approach
that uses an ML-MPC framework while incorporating an RL agent to dynamically
adjust the ML-MPC load tracking reference in response to changes in the
environment. At the same time, the ML-MPC ensures that actions stay safe
throughout the RL agent's exploration. To evaluate the effectiveness of this
approach, fuel pressure is deliberately varied to introduce a model-plant
mismatch between the ML-MPC and the engine test bench. The result of this
mismatch is a root mean square error (RMSE) in indicated mean effective
pressure of 0.57 bar when running the ML-MPC. The experimental results
demonstrate that RL successfully adapts to changing boundary conditions by
altering the tracking reference while ML-MPC ensures safe control inputs. The
quantitative improvement in load tracking by implementing RL is an RSME of 0.44
bar.

</details>


### [161] [I-Con: A Unifying Framework for Representation Learning](https://arxiv.org/abs/2504.16929)
*Shaden Alshammari,John Hershey,Axel Feldmann,William T. Freeman,Mark Hamilton*

Main category: cs.LG

TL;DR: 该论文提出了一种信息论框架，统一了多种现代机器学习损失函数，揭示了聚类、谱方法、降维、对比学习和监督学习背后的信息几何结构，并开发了新的损失函数，在无监督图像分类上实现了8%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着表示学习领域的发展，出现了大量针对不同问题的损失函数。作者希望通过一个统一的框架来概括这些损失函数，揭示其背后的共同信息几何结构。

Method: 引入一个信息论方程，将多种机器学习方法统一为最小化两个条件分布（监督表示和学习表示）之间的KL散度。通过这一框架，结合文献中的成功技术开发新的损失函数。

Result: 理论连接了23种不同方法，并在无监督图像分类上实现了8%的性能提升。此外，还开发了去偏方法，改进了对比表示学习。

Conclusion: 该框架不仅统一了多种机器学习方法，还推动了新损失函数的开发，显著提升了无监督学习的性能。

Abstract: As the field of representation learning grows, there has been a proliferation
of different loss functions to solve different classes of problems. We
introduce a single information-theoretic equation that generalizes a large
collection of modern loss functions in machine learning. In particular, we
introduce a framework that shows that several broad classes of machine learning
methods are precisely minimizing an integrated KL divergence between two
conditional distributions: the supervisory and learned representations. This
viewpoint exposes a hidden information geometry underlying clustering, spectral
methods, dimensionality reduction, contrastive learning, and supervised
learning. This framework enables the development of new loss functions by
combining successful techniques from across the literature. We not only present
a wide array of proofs, connecting over 23 different approaches, but we also
leverage these theoretical results to create state-of-the-art unsupervised
image classifiers that achieve a +8% improvement over the prior
state-of-the-art on unsupervised classification on ImageNet-1K. We also
demonstrate that I-Con can be used to derive principled debiasing methods which
improve contrastive representation learners.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [162] [A CNN-based Local-Global Self-Attention via Averaged Window Embeddings for Hierarchical ECG Analysis](https://arxiv.org/abs/2504.16097)
*Arthur Buzelin,Pedro Robles Dutenhefner,Turi Rezende,Luisa G. Porfirio,Pedro Bento,Yan Aquino,Jose Fernandes,Caio Santana,Gabriela Miana,Gisele L. Pappa,Antonio Ribeiro,Wagner Meira Jr*

Main category: eess.SP

TL;DR: 提出了一种结合局部和全局注意力的LGA-ECG模型，用于提升心电图分析的准确性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，需要高效的心电图诊断工具。传统Transformer难以捕捉局部形态特征，影响分析准确性。

Method: 提出LGA-ECG模型，结合卷积归纳偏置和全局自注意力机制，通过重叠卷积窗口提取查询，同时建模全局上下文。

Result: 在CODE-15数据集上，LGA-ECG优于现有模型，验证了局部-全局注意力策略的有效性。

Conclusion: LGA-ECG通过捕捉心电图信号的层次时间依赖和形态模式，展示了在临床自动化分类中的潜力。

Abstract: Cardiovascular diseases remain the leading cause of global mortality,
emphasizing the critical need for efficient diagnostic tools such as
electrocardiograms (ECGs). Recent advancements in deep learning, particularly
transformers, have revolutionized ECG analysis by capturing detailed waveform
features as well as global rhythm patterns. However, traditional transformers
struggle to effectively capture local morphological features that are critical
for accurate ECG interpretation. We propose a novel Local-Global Attention ECG
model (LGA-ECG) to address this limitation, integrating convolutional inductive
biases with global self-attention mechanisms. Our approach extracts queries by
averaging embeddings obtained from overlapping convolutional windows, enabling
fine-grained morphological analysis, while simultaneously modeling global
context through attention to keys and values derived from the entire sequence.
Experiments conducted on the CODE-15 dataset demonstrate that LGA-ECG
outperforms state-of-the-art models and ablation studies validate the
effectiveness of the local-global attention strategy. By capturing the
hierarchical temporal dependencies and morphological patterns in ECG signals,
this new design showcases its potential for clinical deployment with robust
automated ECG classification.

</details>


### [163] [Two-Timescale Joint Transmit and Pinching Beamforming for Pinching-Antenna Systems](https://arxiv.org/abs/2504.16099)
*Luyuan Zhang,Xidong Mu,An Liu,Yuanwei Liu*

Main category: eess.SP

TL;DR: 本文提出了一种基于Pinching天线系统（PASS）的两时间尺度联合发射和Pinching波束成形设计，以最大化多用户多输入单输出系统的总速率。


<details>
  <summary>Details</summary>
Motivation: PASS作为一种革命性的灵活天线技术，通过低成本的可调Pinching天线实现视距链路，但其在多用户系统中的波束成形设计尚未充分研究。

Method: 采用原始对偶分解方法将问题分解为短期发射波束成形和长期Pinching波束成形两个子问题，分别通过KKT引导的双学习方法和随机连续凸近似方法求解。

Result: 仿真结果表明，所提出的两时间尺度算法相比基线方法具有显著的性能提升。

Conclusion: 该研究为PASS在多用户系统中的高效波束成形设计提供了有效解决方案。

Abstract: Pinching antenna systems (PASS) have been proposed as a revolutionary
flexible antenna technology which facilitates line-of-sight links via numerous
low-cost pinching antennas with adjustable activation positions over
waveguides. This letter proposes a two-timescale joint transmit and pinching
beamforming design for the maximization of sum rate of a PASS-based downlink
multi-user multiple input single output system. A primal dual decomposition
method is developed to decouple the two-timescale problem into two
sub-problems: 1) A Karush-Kuhn-Tucker-guided dual learning-based approach is
proposed to solve the short-term transmit beamforming design sub-problem; 2)
The long-term pinching beamforming design sub-problem is tackled by adopting a
stochastic successive convex approximation method. Simulation results
demonstrate that the proposed two-timescale algorithm achieves a significant
performance gain compared to other baselines.

</details>


### [164] [Towards Accurate Forecasting of Renewable Energy : Building Datasets and Benchmarking Machine Learning Models for Solar and Wind Power in France](https://arxiv.org/abs/2504.16100)
*Eloi Lindas,Yannig Goude,Philippe Ciais*

Main category: eess.SP

TL;DR: 该研究提出了一种基于机器学习的法国太阳能和风能发电预测方法，利用空间天气数据和产能信息，比较了多种建模方法，发现神经网络优于传统树模型。


<details>
  <summary>Details</summary>
Motivation: 非调度可再生能源的准确预测对电网稳定和电价预测至关重要，现有方法未能充分利用空间数据。

Method: 使用ERA5天气数据、产能信息和电价作为输入特征，探索了空间平均、主成分分析和计算机视觉架构三种建模方法。

Result: 神经网络表现最佳，误差在4%-10%之间，与单厂级模型相当。

Conclusion: 该方法展示了区域电力供应预测的潜力，尤其是神经网络在处理空间数据时的优势。

Abstract: Accurate prediction of non-dispatchable renewable energy sources is essential
for grid stability and price prediction. Regional power supply forecasts are
usually indirect through a bottom-up approach of plant-level forecasts,
incorporate lagged power values, and do not use the potential of spatially
resolved data. This study presents a comprehensive methodology for predicting
solar and wind power production at country scale in France using machine
learning models trained with spatially explicit weather data combined with
spatial information about production sites capacity. A dataset is built
spanning from 2012 to 2023, using daily power production data from RTE (the
national grid operator) as the target variable, with daily weather data from
ERA5, production sites capacity and location, and electricity prices as input
features. Three modeling approaches are explored to handle spatially resolved
weather data: spatial averaging over the country, dimension reduction through
principal component analysis, and a computer vision architecture to exploit
complex spatial relationships. The study benchmarks state-of-the-art machine
learning models as well as hyperparameter tuning approaches based on
cross-validation methods on daily power production data. Results indicate that
cross-validation tailored to time series is best suited to reach low error. We
found that neural networks tend to outperform traditional tree-based models,
which face challenges in extrapolation due to the increasing renewable capacity
over time. Model performance ranges from 4% to 10% in nRMSE for midterm
horizon, achieving similar error metrics to local models established at a
single-plant level, highlighting the potential of these methods for regional
power supply forecasting.

</details>


### [165] [xLSTM-ECG: Multi-label ECG Classification via Feature Fusion with xLSTM](https://arxiv.org/abs/2504.16101)
*Lei Kang,Xuanshuo Fu,Javier Vazquez-Corral,Ernest Valveny,Dimosthenis Karatzas*

Main category: eess.SP

TL;DR: 提出了一种基于xLSTM网络的多标签ECG信号分类方法xLSTM-ECG，显著提高了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，ECG手动解读耗时且易错，需要高效准确的诊断工具。

Method: 使用STFT将ECG信号转换到频域，设计xLSTM网络捕捉12导联ECG的局部和全局特征。

Result: 在PTB-XL数据集上表现优异，并在Georgia 12-Lead数据集上验证了鲁棒性和高效性。

Conclusion: xLSTM-ECG显著提升了ECG分类准确性，有助于临床诊断和患者护理。

Abstract: Cardiovascular diseases (CVDs) remain the leading cause of mortality
worldwide, highlighting the critical need for efficient and accurate diagnostic
tools. Electrocardiograms (ECGs) are indispensable in diagnosing various heart
conditions; however, their manual interpretation is time-consuming and
error-prone. In this paper, we propose xLSTM-ECG, a novel approach that
leverages an extended Long Short-Term Memory (xLSTM) network for multi-label
classification of ECG signals, using the PTB-XL dataset. To the best of our
knowledge, this work represents the first design and application of xLSTM
modules specifically adapted for multi-label ECG classification. Our method
employs a Short-Time Fourier Transform (STFT) to convert time-series ECG
waveforms into the frequency domain, thereby enhancing feature extraction. The
xLSTM architecture is specifically tailored to address the complexities of
12-lead ECG recordings by capturing both local and global signal features.
Comprehensive experiments on the PTB-XL dataset reveal that our model achieves
strong multi-label classification performance, while additional tests on the
Georgia 12-Lead dataset underscore its robustness and efficiency. This approach
significantly improves ECG classification accuracy, thereby advancing clinical
diagnostics and patient care. The code will be publicly available upon
acceptance.

</details>


### [166] [A Self-supervised Learning Method for Raman Spectroscopy based on Masked Autoencoders](https://arxiv.org/abs/2504.16130)
*Pengju Ren,Ri-gui Zhou,Yaochong Li*

Main category: eess.SP

TL;DR: 提出了一种基于掩码自编码器（SMAE）的自监督学习方法，用于拉曼光谱分析，解决了标注数据不足的问题，并在噪声抑制和分类准确性上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有监督学习方法依赖大量标注数据，而标注成本高且数据有限，限制了其性能。自监督学习可以从未标注数据中提取特征，解决这一问题。

Method: 采用掩码自编码器（SMAE）进行自监督预训练，通过随机掩码和重建光谱信息学习特征。预训练后，利用少量标注数据进行微调。

Result: SMAE在预训练中表现出噪声抑制能力（SNR提升两倍以上），并在30类细菌数据集上实现80%以上的聚类准确率。微调后测试集识别准确率达83.90%，与监督学习方法相当。

Conclusion: SMAE是一种高效的自监督学习方法，适用于拉曼光谱分析，尤其在标注数据不足时表现优异，且具有噪声抑制能力。

Abstract: Raman spectroscopy serves as a powerful and reliable tool for analyzing the
chemical information of substances. The integration of Raman spectroscopy with
deep learning methods enables rapid qualitative and quantitative analysis of
materials. Most existing approaches adopt supervised learning methods. Although
supervised learning has achieved satisfactory accuracy in spectral analysis, it
is still constrained by costly and limited well-annotated spectral datasets for
training. When spectral annotation is challenging or the amount of annotated
data is insufficient, the performance of supervised learning in spectral
material identification declines. In order to address the challenge of feature
extraction from unannotated spectra, we propose a self-supervised learning
paradigm for Raman Spectroscopy based on a Masked AutoEncoder, termed SMAE.
SMAE does not require any spectral annotations during pre-training. By randomly
masking and then reconstructing the spectral information, the model learns
essential spectral features. The reconstructed spectra exhibit certain
denoising properties, improving the signal-to-noise ratio (SNR) by more than
twofold. Utilizing the network weights obtained from masked pre-training, SMAE
achieves clustering accuracy of over 80% for 30 classes of isolated bacteria in
a pathogenic bacterial dataset, demonstrating significant improvements compared
to classical unsupervised methods and other state-of-the-art deep clustering
methods. After fine-tuning the network with a limited amount of annotated data,
SMAE achieves an identification accuracy of 83.90% on the test set, presenting
competitive performance against the supervised ResNet (83.40%).

</details>


### [167] [A Non-Invasive Load Monitoring Method for Edge Computing Based on MobileNetV3 and Dynamic Time Regulation](https://arxiv.org/abs/2504.16142)
*Hangxu Liu,Yaojie Sun,Yu Wang*

Main category: eess.SP

TL;DR: 该论文提出了一种基于动态时间规整（DTW）算法的非侵入式负载监测（NILM）技术，通过优化时间-频率域特征提取，显著降低了计算和存储成本，并在边缘MCU上实现了95%的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习和深度学习的NILM方法计算和存储成本高，难以在资源受限的边缘MCU上部署。

Method: 提出了一种创新的DTW算法，并系统比较了六种机器学习技术，优化了频率域特征提取过程。

Result: 在边缘MCU上实现了95%的识别准确率，运行时间减少55.55%，存储开销降低约34.6%。

Conclusion: 未来研究将聚焦于消除电压变压器设计以降低成本，为NILM的实际应用提供更具性价比的解决方案。

Abstract: In recent years, non-intrusive load monitoring (NILM) technology has
attracted much attention in the related research field by virtue of its unique
advantage of utilizing single meter data to achieve accurate decomposition of
device-level energy consumption. Cutting-edge methods based on machine learning
and deep learning have achieved remarkable results in load decomposition
accuracy by fusing time-frequency domain features. However, these methods
generally suffer from high computational costs and huge memory requirements,
which become the main obstacles for their deployment on resource-constrained
microcontroller units (MCUs). To address these challenges, this study proposes
an innovative Dynamic Time Warping (DTW) algorithm in the time-frequency domain
and systematically compares and analyzes the performance of six machine
learning techniques in home electricity scenarios. Through complete
experimental validation on edge MCUs, this scheme successfully achieves a
recognition accuracy of 95%. Meanwhile, this study deeply optimizes the
frequency domain feature extraction process, which effectively reduces the
running time by 55.55% and the storage overhead by about 34.6%. The algorithm
performance will be further optimized in future research work. Considering that
the elimination of voltage transformer design can significantly reduce the
cost, the subsequent research will focus on this direction, and is committed to
providing more cost-effective solutions for the practical application of NILM,
and providing a solid theoretical foundation and feasible technical paths for
the design of efficient NILM systems in edge computing environments.

</details>


### [168] [SeizureFormer: A Transformer Model for IEA-Based Seizure Risk Forecasting](https://arxiv.org/abs/2504.16098)
*Tianning Feng,Junting Ni,Ezequiel Gleichgerrcht,Wei Jin*

Main category: eess.SP

TL;DR: SeizureFormer是一种基于Transformer的模型，用于长期癫痫风险预测，结合了IEA和LE生物标志物，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够利用临床相关特征并整合多种技术（如CNN和多头自注意力）的模型，以提升癫痫风险预测的准确性和泛化能力。

Method: 结合CNN-based patch embedding、多头自注意力机制和squeeze-and-excitation模块，建模短期动态和长期癫痫周期。

Result: 在5名患者和多个预测窗口（1至14天）上测试，平均ROC AUC为79.44%，PR AUC为76.29%，优于统计、机器学习和深度学习基线。

Conclusion: SeizureFormer为个性化癫痫管理提供了可解释且稳健的预测工具，支持未来临床集成。

Abstract: We present SeizureFormer, a Transformer-based model for long-term seizure
risk forecasting using interictal epileptiform activity (IEA) surrogate
biomarkers and long episode (LE) biomarkers from responsive neurostimulation
(RNS) systems. Unlike raw scalp EEG-based models, SeizureFormer leverages
structured, clinically relevant features and integrates CNN-based patch
embedding, multi-head self-attention, and squeeze-and-excitation blocks to
model both short-term dynamics and long-term seizure cycles. Tested across five
patients and multiple prediction windows (1 to 14 days), SeizureFormer achieved
state-of-the-art performance with mean ROC AUC of 79.44 percent and mean PR AUC
of 76.29 percent. Compared to statistical, machine learning, and deep learning
baselines, it demonstrates enhanced generalizability and seizure risk
forecasting performance under class imbalance. This work supports future
clinical integration of interpretable and robust seizure forecasting tools for
personalized epilepsy management.

</details>


### [169] [A Statistical Approach for Synthetic EEG Data Generation](https://arxiv.org/abs/2504.16143)
*Gideon Vos,Maryam Ebrahimpour,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: eess.SP

TL;DR: 论文提出了一种结合相关性分析和随机采样的方法，用于生成高质量的合成EEG数据，以解决真实数据收集成本高的问题。


<details>
  <summary>Details</summary>
Motivation: EEG数据对心理健康诊断至关重要，但大规模收集成本高且耗时。合成数据生成可以扩展数据集，但保留情感和心理健康信号的高质量合成EEG仍具挑战性。

Method: 通过相关性分析EEG频段间的相互依赖关系，并基于此结构通过随机采样生成合成样本。保留与真实数据相关性高的样本，并通过分布分析和分类任务评估。

Result: 生成的合成数据在统计和结构特性上与真实EEG高度匹配，随机森林模型无法区分合成与真实数据，表明高保真度。

Conclusion: 该方法为EEG数据集提供了一种可扩展且隐私保护的增强方式，有助于心理健康研究中更高效的模型训练。

Abstract: Electroencephalogram (EEG) data is crucial for diagnosing mental health
conditions but is costly and time-consuming to collect at scale. Synthetic data
generation offers a promising solution to augment datasets for machine learning
applications. However, generating high-quality synthetic EEG that preserves
emotional and mental health signals remains challenging. This study proposes a
method combining correlation analysis and random sampling to generate realistic
synthetic EEG data.
  We first analyze interdependencies between EEG frequency bands using
correlation analysis. Guided by this structure, we generate synthetic samples
via random sampling. Samples with high correlation to real data are retained
and evaluated through distribution analysis and classification tasks. A Random
Forest model trained to distinguish synthetic from real EEG performs at chance
level, indicating high fidelity.
  The generated synthetic data closely match the statistical and structural
properties of the original EEG, with similar correlation coefficients and no
significant differences in PERMANOVA tests. This method provides a scalable,
privacy-preserving approach for augmenting EEG datasets, enabling more
efficient model training in mental health research.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [170] [Mining Software Repositories for Expert Recommendation](https://arxiv.org/abs/2504.16343)
*Chad Marshall,Andrew Barovic,Armin Moin*

Main category: cs.SE

TL;DR: 提出了一种基于BERTopic和TopicMiner的自动化方法，用于将开源软件项目中的bug分配给合适的开发者。


<details>
  <summary>Details</summary>
Motivation: 帮助人工bug分类员更高效地为新报告的bug找到具有相关专业知识的开发者。

Method: 利用问题跟踪系统中的开发历史，结合bug报告的特征（如产品、组件、优先级和严重性），并使用BERTopic和TopicMiner技术对开发者进行排序。

Result: 通过Top-k准确率评估，与TopicMiner MTM、BUGZIE、BT-RL和LDA-SVM等方法相比，表现更优。

Conclusion: 该方法能有效提升bug分配的准确性和效率，适用于大型开源项目。

Abstract: We propose an automated approach to bug assignment to developers in large
open-source software projects. This way, we assist human bug triagers who are
in charge of finding the best developer with the right level of expertise in a
particular area to be assigned to a newly reported issue. Our approach is based
on the history of software development as documented in the issue tracking
systems. We deploy BERTopic and techniques from TopicMiner. Our approach works
based on the bug reports' features, such as the corresponding products and
components, as well as their priority and severity levels. We sort developers
based on their experience with specific combinations of new reports. The
evaluation is performed using Top-k accuracy, and the results are compared with
the reported results in prior work, namely TopicMiner MTM, BUGZIE, Bug triaging
via deep Reinforcement Learning BT-RL, and LDA-SVM. The evaluation data come
from various Eclipse and Mozilla projects, such as JDT, Firefox, and
Thunderbird.

</details>


### [171] [Harden and Catch for Just-in-Time Assured LLM-Based Software Testing: Open Research Challenges](https://arxiv.org/abs/2504.16472)
*Mark Harman,Peter O'Hearn,Shubho Sengupta*

Main category: cs.SE

TL;DR: 论文探讨了自动化软件测试中的新概念（硬化测试和捕获测试），并提出了“及时捕获测试”挑战，展示了基于大语言模型的测试生成潜力。


<details>
  <summary>Details</summary>
Motivation: 自动化软件测试中的某些基础概念尚未明确定义，但具有巨大实际影响潜力，尤其是在大语言模型背景下。

Method: 正式定义并研究了硬化测试和捕获测试的特性，提出了“及时捕获测试”挑战，并讨论了部署选项和初步结果。

Result: 展示了基于大语言模型的硬化测试生成方法，并探讨了其在捕获潜在缺陷中的应用。

Conclusion: 论文为自动化测试领域提供了新视角，并提出了未来研究方向，尤其是在大语言模型和及时测试生成方面。

Abstract: Despite decades of research and practice in automated software testing,
several fundamental concepts remain ill-defined and under-explored, yet offer
enormous potential real-world impact. We show that these concepts raise
exciting new challenges in the context of Large Language Models for software
test generation. More specifically, we formally define and investigate the
properties of hardening and catching tests. A hardening test is one that seeks
to protect against future regressions, while a catching test is one that
catches such a regression or a fault in new functionality introduced by a code
change. Hardening tests can be generated at any time and may become catching
tests when a future regression is caught. We also define and motivate the
Catching `Just-in-Time' (JiTTest) Challenge, in which tests are generated
`just-in-time' to catch new faults before they land into production. We show
that any solution to Catching JiTTest generation can also be repurposed to
catch latent faults in legacy code. We enumerate possible outcomes for
hardening and catching tests and JiTTests, and discuss open research problems,
deployment options, and initial results from our work on automated LLM-based
hardening at Meta. This paper\footnote{Author order is alphabetical. The
corresponding author is Mark Harman.} was written to accompany the keynote by
the authors at the ACM International Conference on the Foundations of Software
Engineering (FSE) 2025.

</details>


### [172] [On Developers' Self-Declaration of AI-Generated Code: An Analysis of Practices](https://arxiv.org/abs/2504.16485)
*Syed Mohammad Kashif,Peng Liang,Amjed Tahir*

Main category: cs.SE

TL;DR: 研究探讨开发者如何自我声明AI生成代码及其原因，发现多数开发者会声明，少数不声明，并提供了相关指南。


<details>
  <summary>Details</summary>
Motivation: 现实开发中需区分AI生成与人工代码，但现有研究多关注代码质量，缺乏对开发者自我声明行为的研究。

Method: 混合方法研究：1) 挖掘GitHub仓库收集613个AI生成代码片段；2) 工业调查获取111份有效回复。

Result: 76.6%开发者会声明AI生成代码，原因包括追踪需求和伦理考量；23.4%不声明，因修改多或认为不必要。

Conclusion: 提供指南以帮助开发者声明AI生成代码，解决伦理和代码质量问题。

Abstract: AI code generation tools have gained significant popularity among developers,
who use them to assist in software development due to their capability to
generate code. Existing studies mainly explored the quality, e.g., correctness
and security, of AI-generated code, while in real-world software development,
the prerequisite is to distinguish AI-generated code from human-written code,
which emphasizes the need to explicitly declare AI-generated code by
developers. To this end, this study intends to understand the ways developers
use to self-declare AI-generated code and explore the reasons why developers
choose to self-declare or not. We conducted a mixed-methods study consisting of
two phases. In the first phase, we mined GitHub repositories and collected 613
instances of AI-generated code snippets. In the second phase, we conducted a
follow-up industrial survey, which received 111 valid responses. Our research
revealed the practices followed by developers to self-declare AI-generated
code. Most practitioners (76.6%) always or sometimes self-declare AI-generated
code. In contrast, other practitioners (23.4%) noted that they never
self-declare AI-generated code. The reasons for self-declaring AI-generated
code include the need to track and monitor the code for future review and
debugging, and ethical considerations. The reasons for not self-declaring
AI-generated code include extensive modifications to AI-generated code and the
developers' perception that self-declaration is an unnecessary activity. We
finally provided guidelines for practitioners to self-declare AI-generated
code, addressing ethical and code quality concerns.

</details>


### [173] [ClarifyCoder: Clarification-Aware Fine-Tuning for Programmatic Problem Solving](https://arxiv.org/abs/2504.16331)
*Jie JW Wu,Manav Chaudhary,Davit Abrahamyan,Arhaan Khaku,Anjiang Wei,Fatemeh H. Fard*

Main category: cs.SE

TL;DR: ClarifyCoder是一个新框架，通过合成数据和指令调优，使大语言模型（LLMs）在代码生成前识别模糊需求并请求澄清，显著提升其沟通能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在代码生成任务中表现优异，但与人类工程师相比，缺乏主动澄清模糊需求的能力。

Method: 1. 合成数据技术，生成需要澄清的训练数据；2. 微调策略，使模型优先请求澄清而非直接生成代码。

Result: 实验表明，ClarifyCoder显著提升了LLMs的沟通能力，同时保持代码生成能力。

Conclusion: ClarifyCoder为LLMs提供了识别和查询模糊需求的内在能力，填补了与人类工程师的差距。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code generation tasks. However, a significant gap remains between their current
performance and that of expert software engineers. A key differentiator is that
human engineers actively seek clarification when faced with ambiguous
requirements, while LLMs typically generate code regardless of uncertainties in
the problem description. We present ClarifyCoder, a novel framework with
synthetic data generation and instruction-tuning that enables LLMs to identify
ambiguities and request clarification before proceeding with code generation.
While recent work has focused on LLM-based agents for iterative code
generation, we argue that the fundamental ability to recognize and query
ambiguous requirements should be intrinsic to the models themselves. Our
approach consists of two main components: (1) a data synthesis technique that
augments existing programming datasets with scenarios requiring clarification
to generate clarification-aware training data, and (2) a fine-tuning strategy
that teaches models to prioritize seeking clarification over immediate code
generation when faced with incomplete or ambiguous requirements. We further
provide an empirical analysis of integrating ClarifyCoder with standard
fine-tuning for a joint optimization of both clarify-awareness and coding
ability. Experimental results demonstrate that ClarifyCoder significantly
improves the communication capabilities of Code LLMs through meaningful
clarification dialogues while maintaining code generation capabilities.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [174] [DP2FL: Dual Prompt Personalized Federated Learning in Foundation Models](https://arxiv.org/abs/2504.16357)
*Ying Chang,Xiaohu Shi,Xiaohui Zhao,Zhaohuang Chen,Deyin Ma*

Main category: cs.DC

TL;DR: 提出了一种名为DP2FL的双提示个性化联邦学习框架，通过双提示和自适应聚合策略解决数据分布异质性和新客户端集成问题。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户端数据分布异质性及新客户端集成问题，提升模型在有限本地数据下的性能。

Method: 引入双提示和自适应聚合策略，结合全局任务意识和本地数据驱动，实现有效泛化和适应特定数据分布。

Result: 实验验证了DP2FL在高度异质环境中的有效性，支持新数据源预测和新客户端无缝集成。

Conclusion: DP2FL框架通过双提示和自适应聚合策略，显著提升了联邦学习的性能和适应性。

Abstract: Personalized federated learning (PFL) has garnered significant attention for
its ability to address heterogeneous client data distributions while preserving
data privacy. However, when local client data is limited, deep learning models
often suffer from insufficient training, leading to suboptimal performance.
Foundation models, such as CLIP (Contrastive Language-Image Pretraining),
exhibit strong feature extraction capabilities and can alleviate this issue by
fine-tuning on limited local data. Despite their potential, foundation models
are rarely utilized in federated learning scenarios, and challenges related to
integrating new clients remain largely unresolved. To address these challenges,
we propose the Dual Prompt Personalized Federated Learning (DP2FL) framework,
which introduces dual prompts and an adaptive aggregation strategy. DP2FL
combines global task awareness with local data-driven insights, enabling local
models to achieve effective generalization while remaining adaptable to
specific data distributions. Moreover, DP2FL introduces a global model that
enables prediction on new data sources and seamlessly integrates newly added
clients without requiring retraining. Experimental results in highly
heterogeneous environments validate the effectiveness of DP2FL's prompt design
and aggregation strategy, underscoring the advantages of prediction on novel
data sources and demonstrating the seamless integration of new clients into the
federated learning framework.

</details>


### [175] [Towards a Distributed Federated Learning Aggregation Placement using Particle Swarm Intelligence](https://arxiv.org/abs/2504.16227)
*Amir Ali-Pour,Sadra Bekrani,Laya Samizadeh,Julien Gascon-Samson*

Main category: cs.DC

TL;DR: Flag-Swap是一种基于粒子群优化（PSO）的方法，用于在分层半分散联邦学习（SDFL）中优化聚合位置，仅依赖处理延迟，减少对系统数据的依赖。实验表明其性能优于随机和均匀放置策略。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要频繁交换系统数据以监控节点性能，而Flag-Swap旨在通过仅依赖处理延迟来优化聚合位置，减少系统开销。

Method: 提出Flag-Swap，一种基于PSO的优化方法，用于在SDFL中动态选择聚合节点。

Result: 模拟和实际Docker实现显示，Flag-Swap比随机放置快43%，比均匀放置快32%。

Conclusion: Flag-Swap在减少系统数据依赖的同时，显著提升了聚合效率。

Abstract: Federated learning has become a promising distributed learning concept with
extra insurance on data privacy. Extensive studies on various models of
Federated learning have been done since the coinage of its term. One of the
important derivatives of federated learning is hierarchical semi-decentralized
federated learning, which distributes the load of the aggregation task over
multiple nodes and parallelizes the aggregation workload at the breadth of each
level of the hierarchy. Various methods have also been proposed to perform
inter-cluster and intra-cluster aggregation optimally. Most of the solutions,
nonetheless, require monitoring the nodes' performance and resource consumption
at each round, which necessitates frequently exchanging systematic data. To
optimally perform distributed aggregation in SDFL with minimal reliance on
systematic data, we propose Flag-Swap, a Particle Swarm Optimization (PSO)
method that optimizes the aggregation placement according only to the
processing delay. Our simulation results show that PSO-based placement can find
the optimal placement relatively fast, even in scenarios with many clients as
candidates for aggregation. Our real-world docker-based implementation of
Flag-Swap over the recently emerged FL framework shows superior performance
compared to black-box-based deterministic placement strategies, with about 43%
minutes faster than random placement, and 32% minutes faster than uniform
placement, in terms of total processing time.

</details>


### [176] [Simplified Swarm Learning Framework for Robust and Scalable Diagnostic Services in Cancer Histopathology](https://arxiv.org/abs/2504.16732)
*Yanjie Wu,Yuhao Ji,Saiho Lee,Juniad Akram,Ali Braytee,Ali Anaissi*

Main category: cs.DC

TL;DR: 论文提出了一种简化的点对点群体学习框架（P2P-SL），用于解决医疗数据隐私和资源受限问题，替代依赖区块链的传统群体学习，并在癌症病理诊断中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 医疗数据的复杂性（如隐私问题、数据集不平衡和互操作性）需要创新的机器学习解决方案。传统群体学习依赖区块链，限制了可访问性和扩展性。

Method: 提出P2P-SL框架，去除区块链依赖，采用轻量级点对点通信，结合优化的预训练模型（如TorchXRayVision和DenseNet解码器）。

Result: 实验表明，该框架在隐私保护下处理不平衡和偏差数据集时，性能与集中式模型相当。

Conclusion: P2P-SL为医疗领域提供了一种可扩展、高效且隐私保护的机器学习解决方案，推动了先进技术的民主化应用。

Abstract: The complexities of healthcare data, including privacy concerns, imbalanced
datasets, and interoperability issues, necessitate innovative machine learning
solutions. Swarm Learning (SL), a decentralized alternative to Federated
Learning, offers privacy-preserving distributed training, but its reliance on
blockchain technology hinders accessibility and scalability. This paper
introduces a \textit{Simplified Peer-to-Peer Swarm Learning (P2P-SL) Framework}
tailored for resource-constrained environments. By eliminating blockchain
dependencies and adopting lightweight peer-to-peer communication, the proposed
framework ensures robust model synchronization while maintaining data privacy.
Applied to cancer histopathology, the framework integrates optimized
pre-trained models, such as TorchXRayVision, enhanced with DenseNet decoders,
to improve diagnostic accuracy. Extensive experiments demonstrate the
framework's efficacy in handling imbalanced and biased datasets, achieving
comparable performance to centralized models while preserving privacy. This
study paves the way for democratizing advanced machine learning in healthcare,
offering a scalable, accessible, and efficient solution for privacy-sensitive
diagnostic applications.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [177] [Behavior of prediction performance metrics with rare events](https://arxiv.org/abs/2504.16185)
*Emily Minus,R. Yates Coley,Susan M. Shortreed,Brian D. Williamson*

Main category: stat.ML

TL;DR: AUC在罕见事件中的表现不稳定，但研究发现其可靠性取决于事件总数而非事件率。


<details>
  <summary>Details</summary>
Motivation: 评估AUC在罕见事件预测中的可靠性，解决其可能误导性能的问题。

Method: 通过模拟研究分析AUC的偏差和方差，以及其他性能指标（如PPV、准确率、敏感性和特异性）的行为。

Result: AUC的可靠性由最小类别大小决定，而非事件率；敏感性和特异性分别由事件和非事件数量驱动。

Conclusion: 在罕见事件中，只要事件总数足够大，AUC仍可靠。

Abstract: Area under the receiving operator characteristic curve (AUC) is commonly
reported alongside binary prediction models. However, there are concerns that
AUC might be a misleading measure of prediction performance in the rare event
setting. This setting is common since many events of clinical importance are
rare events. We conducted a simulation study to determine when or whether AUC
is unstable in the rare event setting. Specifically, we aimed to determine
whether the bias and variance of AUC are driven by the number of events or the
event rate. We also investigated the behavior of other commonly used measures
of prediction performance, including positive predictive value, accuracy,
sensitivity, and specificity. Our results indicate that poor AUC behavior -- as
measured by empirical bias, variability of cross-validated AUC estimates, and
empirical coverage of confidence intervals -- is driven by the minimum class
size, not event rate. Performance of sensitivity is driven by the number of
events, while that of specificity is driven by the number of non-events. Other
measures, including positive predictive value and accuracy, depend on the event
rate even in large samples. AUC is reliable in the rare event setting provided
that the total number of events is moderately large.

</details>


### [178] [Covariate-dependent Graphical Model Estimation via Neural Networks with Statistical Guarantees](https://arxiv.org/abs/2504.16356)
*Jiahe Lin,Yikai Zhang,George Michailidis*

Main category: stat.ML

TL;DR: 论文提出了一种基于深度神经网络的方法，用于估计协变量依赖的图结构，适用于非高斯数据，并在理论和实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究协变量依赖的图结构估计问题，传统方法通常假设高斯性，而本文方法放宽了这一限制。

Method: 采用深度神经网络建模协变量与图结构的灵活函数关系，基于经验风险最小化框架。

Result: 理论上有PAC保证，实验在合成数据和真实数据（神经科学和金融）中表现优于现有方法。

Conclusion: 该方法在非高斯数据下表现良好，具有实际应用价值。

Abstract: Graphical models are widely used in diverse application domains to model the
conditional dependencies amongst a collection of random variables. In this
paper, we consider settings where the graph structure is covariate-dependent,
and investigate a deep neural network-based approach to estimate it. The method
allows for flexible functional dependency on the covariate, and fits the data
reasonably well in the absence of a Gaussianity assumption. Theoretical results
with PAC guarantees are established for the method, under assumptions commonly
used in an Empirical Risk Minimization framework. The performance of the
proposed method is evaluated on several synthetic data settings and benchmarked
against existing approaches. The method is further illustrated on real datasets
involving data from neuroscience and finance, respectively, and produces
interpretable results.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [179] [Heterogeneous networks in drug-target interaction prediction](https://arxiv.org/abs/2504.16152)
*Mohammad Molaee,Nasrollah Moghadam Charkari*

Main category: q-bio.BM

TL;DR: 综述论文探讨了基于图机器学习的药物-靶标相互作用预测方法，总结了其框架、贡献、数据集和代码，并讨论了未来挑战。


<details>
  <summary>Details</summary>
Motivation: 药物发现耗时耗力，计算预测方法可缩小实验范围，提高效率。

Method: 综述了2020-2024年的图机器学习方法，包括框架、贡献、数据集和性能评估。

Result: 图机器学习在药物-靶标相互作用预测中表现优异。

Conclusion: 未来需进一步探索相关挑战和关键领域。

Abstract: Drug discovery requires a tremendous amount of time and cost. Computational
drug-target interaction prediction, a significant part of this process, can
reduce these requirements by narrowing the search space for wet lab
experiments. In this survey, we provide comprehensive details of graph machine
learning-based methods in predicting drug-target interaction, as they have
shown promising results in this field. These details include the overall
framework, main contribution, datasets, and their source codes. The selected
papers were mainly published from 2020 to 2024. Prior to discussing papers, we
briefly introduce the datasets commonly used with these methods and
measurements to assess their performance. Finally, future challenges and some
crucial areas that need to be explored are discussed.

</details>


### [180] [The Dance of Atoms-De Novo Protein Design with Diffusion Model](https://arxiv.org/abs/2504.16479)
*Yujie Qin,Ming He,Changyong Yu,Ming Ni,Xian Liu,Xiaochen Bo*

Main category: q-bio.BM

TL;DR: 生成式AI模型（尤其是扩散模型）在蛋白质从头设计中取得突破，显著提高成功率并降低成本，其中RFDiffusion表现突出。


<details>
  <summary>Details</summary>
Motivation: 利用高质量蛋白质结构和序列数据，结合生成式AI模型，解决传统蛋白质设计方法的局限性。

Method: 采用扩散模型生成蛋白质骨架和序列，对比不同模型的优缺点。

Result: 扩散模型在25项蛋白质设计任务中表现优于传统方法和其他AI方法。

Conclusion: 扩散模型在蛋白质设计中前景广阔，未来需进一步探索其潜力和优化方向。

Abstract: The de novo design of proteins refers to creating proteins with specific
structures and functions that do not naturally exist. In recent years, the
accumulation of high-quality protein structure and sequence data and
technological advancements have paved the way for the successful application of
generative artificial intelligence (AI) models in protein design. These models
have surpassed traditional approaches that rely on fragments and
bioinformatics. They have significantly enhanced the success rate of de novo
protein design, and reduced experimental costs, leading to breakthroughs in the
field. Among various generative AI models, diffusion models have yielded the
most promising results in protein design. In the past two to three years, more
than ten protein design models based on diffusion models have emerged. Among
them, the representative model, RFDiffusion, has demonstrated success rates in
25 protein design tasks that far exceed those of traditional methods, and other
AI-based approaches like RFjoint and hallucination. This review will
systematically examine the application of diffusion models in generating
protein backbones and sequences. We will explore the strengths and limitations
of different models, summarize successful cases of protein design using
diffusion models, and discuss future development directions.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [181] [PINN-MEP: Continuous Neural Representations for Minimum-Energy Path Discovery in Molecular Systems](https://arxiv.org/abs/2504.16381)
*Magnus Petersen,Roberto Covino*

Main category: physics.chem-ph

TL;DR: 论文提出了一种基于物理信息神经网络（PINNs）的方法，用于高效生成分子系统的构象转变路径，避免了传统采样方法的高计算成本。


<details>
  <summary>Details</summary>
Motivation: 分子系统的构象转变（如离子通道蛋白的开关状态变化）在生物学中具有重要意义，但传统方法（如分子动力学或MCMC）因高维性和高能垒而难以高效模拟。

Method: 将转变路径生成问题转化为连续优化问题，利用PINNs和可微分分子动力学力场，通过自动微分实现物理合理的路径发现。

Result: 方法在两种蛋白质（包括一个含8,300多个原子的水合BPTI系统）上验证了有效性。

Conclusion: 该方法为构象转变的高效模拟提供了新思路，避免了传统采样方法的计算瓶颈。

Abstract: Characterizing conformational transitions in physical systems remains a
fundamental challenge in the computational sciences. Traditional sampling
methods like molecular dynamics (MD) or MCMC often struggle with the
high-dimensional nature of molecular systems and the high energy barriers of
transitions between stable states. While these transitions are rare events in
simulation timescales, they often represent the most biologically significant
processes - for example, the conformational change of an ion channel protein
from its closed to open state, which controls cellular ion flow and is crucial
for neural signaling. Such transitions in real systems may take milliseconds to
seconds but could require months or years of continuous simulation to observe
even once. We present a method that reformulates transition path generation as
a continuous optimization problem solved through physics-informed neural
networks (PINNs) inspired by string methods for minimum-energy path (MEP)
generation. By representing transition paths as implicit neural functions and
leveraging automatic differentiation with differentiable molecular dynamics
force fields, our method enables the efficient discovery of physically
realistic transition pathways without requiring expensive path sampling. We
demonstrate our method's effectiveness on two proteins, including an explicitly
hydrated bovine pancreatic trypsin inhibitor (BPTI) system with over 8,300
atoms.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [182] [TinyML for Speech Recognition](https://arxiv.org/abs/2504.16213)
*Andrew Barovic,Armin Moin*

Main category: cs.SD

TL;DR: 论文提出了一种在资源受限的IoT边缘设备上部署量化1D卷积神经网络模型进行语音识别的方法，并在Arduino Nano 33 BLE Sense上验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 为智能家居和辅助生活等IoT应用提供高效的语音识别解决方案。

Method: 创建新数据集，利用Edge Impulse技术优化模型，并在Arduino Nano 33 BLE Sense上实现原型。

Result: 模型在数据集上达到97%的准确率，支持23种关键词识别。

Conclusion: 该方法在资源受限设备上实现了高效的语音识别，扩展了复杂命令处理能力。

Abstract: We train and deploy a quantized 1D convolutional neural network model to
conduct speech recognition on a highly resource-constrained IoT edge device.
This can be useful in various Internet of Things (IoT) applications, such as
smart homes and ambient assisted living for the elderly and people with
disabilities, just to name a few examples. In this paper, we first create a new
dataset with over one hour of audio data that enables our research and will be
useful to future studies in this field. Second, we utilize the technologies
provided by Edge Impulse to enhance our model's performance and achieve a high
Accuracy of up to 97% on our dataset. For the validation, we implement our
prototype using the Arduino Nano 33 BLE Sense microcontroller board. This
microcontroller board is specifically designed for IoT and AI applications,
making it an ideal choice for our target use case scenarios. While most
existing research focuses on a limited set of keywords, our model can process
23 different keywords, enabling complex commands.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [183] [4D Multimodal Co-attention Fusion Network with Latent Contrastive Alignment for Alzheimer's Diagnosis](https://arxiv.org/abs/2504.16798)
*Yuxiang Wei,Yanteng Zhang,Xi Xiao,Tianyang Wang,Xiao Wang,Vince D. Calhoun*

Main category: cs.MM

TL;DR: M2M-AlignNet是一种几何感知的多模态共注意力网络，通过潜在对齐整合sMRI和fMRI数据，用于早期阿尔茨海默病诊断。


<details>
  <summary>Details</summary>
Motivation: 多模态神经影像数据（如sMRI和fMRI）的异质性对特征融合提出了挑战，需要一种方法来解决这种差异并提升诊断敏感性。

Method: 提出M2M-AlignNet，采用多补丁对比损失函数和潜在查询共注意力模块，实现fMRI和sMRI的无约束对齐与融合。

Result: 实验验证了方法的有效性，并揭示了fMRI和sMRI作为AD生物标志物的对应关系。

Conclusion: M2M-AlignNet通过几何感知对齐和自主融合模式发现，显著提升了早期AD诊断的准确性。

Abstract: Multimodal neuroimaging provides complementary structural and functional
insights into both human brain organization and disease-related dynamics.
Recent studies demonstrate enhanced diagnostic sensitivity for Alzheimer's
disease (AD) through synergistic integration of neuroimaging data (e.g., sMRI,
fMRI) with behavioral cognitive scores tabular data biomarkers. However, the
intrinsic heterogeneity across modalities (e.g., 4D spatiotemporal fMRI
dynamics vs. 3D anatomical sMRI structure) presents critical challenges for
discriminative feature fusion. To bridge this gap, we propose M2M-AlignNet: a
geometry-aware multimodal co-attention network with latent alignment for early
AD diagnosis using sMRI and fMRI. At the core of our approach is a
multi-patch-to-multi-patch (M2M) contrastive loss function that quantifies and
reduces representational discrepancies via geometry-weighted patch
correspondence, explicitly aligning fMRI components across brain regions with
their sMRI structural substrates without one-to-one constraints. Additionally,
we propose a latent-as-query co-attention module to autonomously discover
fusion patterns, circumventing modality prioritization biases while minimizing
feature redundancy. We conduct extensive experiments to confirm the
effectiveness of our method and highlight the correspondance between fMRI and
sMRI as AD biomarkers.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [184] [Breaking scaling relations with inverse catalysts: a machine learning exploration of trends in $\mathrm{CO_2}$ hydrogenation energy barriers](https://arxiv.org/abs/2504.16493)
*Luuk H. E. Kempen,Marius Juul Nielsen,Mie Andersen*

Main category: cond-mat.mtrl-sci

TL;DR: 论文提出了一种基于机器学习的工作流程，用于探索逆催化剂中反应过渡态，以加速CO2转化为甲醇的催化剂开发。


<details>
  <summary>Details</summary>
Motivation: 减少CO2排放并替代化石燃料的需求，但传统催化剂开发成本高且耗时。

Method: 利用神经网络机器学习势能训练，探索氧化铟纳米团簇在Cu(111)上的反应过渡态。

Result: 该方法显著加速了活性位点的探索，揭示了团簇边缘与内部的结构-活性关系，并打破了线性标度关系。

Conclusion: 逆催化剂在实验中表现优异的原因可能与打破线性标度关系有关，该方法为催化剂设计提供了新思路。

Abstract: The conversion of $\mathrm{CO_2}$ into useful products such as methanol is a
key strategy for abating climate change and our dependence on fossil fuels.
Developing new catalysts for this process is costly and time-consuming and can
thus benefit from computational exploration of possible active sites. However,
this is complicated by the complexity of the materials and reaction networks.
Here, we present a workflow for exploring transition states of elementary
reaction steps at inverse catalysts, which is based on the training of a neural
network-based machine learning interatomic potential. We focus on the crucial
formate intermediate and its formation over nanoclusters of indium oxide
supported on Cu(111). The speedup compared to an approach purely based on
density functional theory allows us to probe a wide variety of active sites
found at nanoclusters of different sizes and stoichiometries. Analysis of the
obtained set of transition state geometries reveals different
structure--activity trends at the edge or interior of the nanoclusters.
Furthermore, the identified geometries allow for the breaking of linear scaling
relations, which could be a key underlying reason for the excellent catalytic
performance of inverse catalysts observed in experiments.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [185] [Data-Assimilated Model-Based Reinforcement Learning for Partially Observed Chaotic Flows](https://arxiv.org/abs/2504.16588)
*Defne E. Ozan,Andrea Nóvoa,Luca Magri*

Main category: eess.SY

TL;DR: 提出了一种结合数据同化的模型强化学习框架（DA-MBRL），用于部分可观测和噪声测量系统的湍流控制。


<details>
  <summary>Details</summary>
Motivation: 湍流控制因混沌动力学和高维度而困难，传统方法需要完整状态信息，但实验中往往无法获取。

Method: 使用控制感知的Echo State Network进行动力学预测，集成Ensemble Kalman Filter进行实时状态估计，结合离策略actor-critic算法学习最优控制策略。

Result: 在Kuramoto-Sivashinsky方程上测试，成功从噪声和部分测量中稳定时空混沌流。

Conclusion: DA-MBRL框架在部分可观测和噪声环境下有效，为湍流控制提供了新方法。

Abstract: The goal of many applications in energy and transport sectors is to control
turbulent flows. However, because of chaotic dynamics and high dimensionality,
the control of turbulent flows is exceedingly difficult. Model-free
reinforcement learning (RL) methods can discover optimal control policies by
interacting with the environment, but they require full state information,
which is often unavailable in experimental settings. We propose a
data-assimilated model-based RL (DA-MBRL) framework for systems with partial
observability and noisy measurements. Our framework employs a control-aware
Echo State Network for data-driven prediction of the dynamics, and integrates
data assimilation with an Ensemble Kalman Filter for real-time state
estimation. An off-policy actor-critic algorithm is employed to learn optimal
control strategies from state estimates. The framework is tested on the
Kuramoto-Sivashinsky equation, demonstrating its effectiveness in stabilizing a
spatiotemporally chaotic flow from noisy and partial measurements.

</details>


### [186] [Learning Verifiable Control Policies Using Relaxed Verification](https://arxiv.org/abs/2504.16879)
*Puja Chaudhury,Alexander Estornell,Michael Everett*

Main category: eess.SY

TL;DR: 论文提出了一种在训练过程中进行验证的方法，以确保学习控制策略满足安全规范，避免了传统验证方法的保守性和局限性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在训练结束后进行验证可能导致策略无法满足规范或验证算法过于保守，无法提供安全保证。

Method: 使用可微可达性分析，并将新组件纳入损失函数，以在训练过程中进行验证。

Result: 在四旋翼模型和独轮车模型上的数值实验表明，该方法能生成满足可达避免和不变性规范的策略。

Conclusion: 通过在训练过程中进行验证，可以生成满足安全规范的策略，同时减少验证算法的保守性。

Abstract: To provide safety guarantees for learning-based control systems, recent work
has developed formal verification methods to apply after training ends.
However, if the trained policy does not meet the specifications, or there is
conservatism in the verification algorithm, establishing these guarantees may
not be possible. Instead, this work proposes to perform verification throughout
training to ultimately aim for policies whose properties can be evaluated
throughout runtime with lightweight, relaxed verification algorithms. The
approach is to use differentiable reachability analysis and incorporate new
components into the loss function. Numerical experiments on a quadrotor model
and unicycle model highlight the ability of this approach to lead to learned
control policies that satisfy desired reach-avoid and invariance
specifications.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [187] [Comprehensive Evaluation of Quantitative Measurements from Automated Deep Segmentations of PSMA PET/CT Images](https://arxiv.org/abs/2504.16237)
*Obed Korshie Dzikunu,Amirhossein Toosi,Shadab Ahamed,Sara Harsini,Francois Benard,Xiaoxiao Li,Arman Rahmim*

Main category: eess.IV

TL;DR: 该研究评估了基于深度学习的自动分割方法提取的六种定量指标，提出了一种新的损失函数L1DFL，并在PSMA PET/CT扫描数据上验证了其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统Dice相似系数评估存在局限性，需更全面的定量指标来评估分割方法的性能。

Method: 使用U-Net、Attention U-Net和SegResNet模型，结合四种损失函数（包括新提出的L1DFL），在380例PSMA PET/CT扫描数据上进行训练和评估。

Result: Attention U-Net与L1DFL组合表现最佳，与真实值相关性最高（0.90-0.99）。L1DFL在SUV指标、病灶计数和TLA上表现优异，但肿瘤体积和病灶扩散的变异性较大。

Conclusion: L1DFL显著减少了临床测量中的变异性，为定量分割提供了更可靠的评估方法。

Abstract: This study performs a comprehensive evaluation of quantitative measurements
as extracted from automated deep-learning-based segmentation methods, beyond
traditional Dice Similarity Coefficient assessments, focusing on six
quantitative metrics, namely SUVmax, SUVmean, total lesion activity (TLA),
tumor volume (TMTV), lesion count, and lesion spread. We analyzed 380
prostate-specific membrane antigen (PSMA) targeted [18F]DCFPyL PET/CT scans of
patients with biochemical recurrence of prostate cancer, training deep neural
networks, U-Net, Attention U-Net and SegResNet with four loss functions: Dice
Loss, Dice Cross Entropy, Dice Focal Loss, and our proposed L1 weighted Dice
Focal Loss (L1DFL). Evaluations indicated that Attention U-Net paired with
L1DFL achieved the strongest correlation with the ground truth (concordance
correlation = 0.90-0.99 for SUVmax and TLA), whereas models employing the Dice
Loss and the other two compound losses, particularly with SegResNet,
underperformed. Equivalence testing (TOST, alpha = 0.05, Delta = 20%) confirmed
high performance for SUV metrics, lesion count and TLA, with L1DFL yielding the
best performance. By contrast, tumor volume and lesion spread exhibited greater
variability. Bland-Altman, Coverage Probability, and Total Deviation Index
analyses further highlighted that our proposed L1DFL minimizes variability in
quantification of the ground truth clinical measures. The code is publicly
available at: https://github.com/ObedDzik/pca\_segment.git.

</details>


### [188] [Frequency-Compensated Network for Daily Arctic Sea Ice Concentration Prediction](https://arxiv.org/abs/2504.16745)
*Jialiang Zhang,Feng Gao,Yanhai Gan,Junyu Dong,Qian Du*

Main category: eess.IV

TL;DR: 提出了一种频率补偿网络（FCNet）用于北极海冰浓度（SIC）预测，通过双分支网络分别提取频率和卷积特征，解决了现有方法在长期频率依赖性和高频细节保留上的不足。


<details>
  <summary>Details</summary>
Motivation: 北极海冰浓度的准确预测对全球生态系统和航行安全至关重要，但现有方法在长期频率依赖性和高频细节保留方面存在不足。

Method: 设计了双分支网络（频率特征提取和卷积特征提取），包括自适应频率滤波块和高频增强块，结合通道注意力和时间注意力单元。

Result: 在卫星SIC数据集上的实验验证了FCNet的有效性，能够更精确地预测边缘和细节。

Conclusion: FCNet通过结合频率和卷积特征提取，显著提升了北极海冰浓度的预测精度，代码和数据将公开。

Abstract: Accurately forecasting sea ice concentration (SIC) in the Arctic is critical
to global ecosystem health and navigation safety. However, current methods
still is confronted with two challenges: 1) these methods rarely explore the
long-term feature dependencies in the frequency domain. 2) they can hardly
preserve the high-frequency details, and the changes in the marginal area of
the sea ice cannot be accurately captured. To this end, we present a
Frequency-Compensated Network (FCNet) for Arctic SIC prediction on a daily
basis. In particular, we design a dual-branch network, including branches for
frequency feature extraction and convolutional feature extraction. For
frequency feature extraction, we design an adaptive frequency filter block,
which integrates trainable layers with Fourier-based filters. By adding
frequency features, the FCNet can achieve refined prediction of edges and
details. For convolutional feature extraction, we propose a high-frequency
enhancement block to separate high and low-frequency information. Moreover,
high-frequency features are enhanced via channel-wise attention, and temporal
attention unit is employed for low-frequency feature extraction to capture
long-range sea ice changes. Extensive experiments are conducted on a
satellite-derived daily SIC dataset, and the results verify the effectiveness
of the proposed FCNet. Our codes and data will be made public available at:
https://github.com/oucailab/FCNet .

</details>


### [189] [Advanced Chest X-Ray Analysis via Transformer-Based Image Descriptors and Cross-Model Attention Mechanism](https://arxiv.org/abs/2504.16774)
*Lakshita Agarwal,Bindu Verma*

Main category: eess.IV

TL;DR: 该研究提出了一种结合Vision Transformer编码器和GPT-4解码器的新模型，用于生成胸部X光图像的描述，显著提升了描述的准确性和丰富性。


<details>
  <summary>Details</summary>
Motivation: 胸部X光图像检查对诊断胸部疾病至关重要，但传统方法在生成准确且丰富的图像描述方面存在不足。

Method: 模型采用Vision Transformer（ViT）编码器提取高质量视觉特征，并通过跨模态注意力与文本数据融合，再由GPT-4解码器生成描述。

Result: 在IU和NIH数据集上，模型在多项指标（如BLEU、CIDEr、METEOR、ROUGE-L）上表现优异，尤其在NIH数据集上全面领先。

Conclusion: 该框架有望提升胸部X光评估的效率和精确性，辅助放射科医生进行更准确的诊断。

Abstract: The examination of chest X-ray images is a crucial component in detecting
various thoracic illnesses. This study introduces a new image description
generation model that integrates a Vision Transformer (ViT) encoder with
cross-modal attention and a GPT-4-based transformer decoder. The ViT captures
high-quality visual features from chest X-rays, which are fused with text data
through cross-modal attention to improve the accuracy, context, and richness of
image descriptions. The GPT-4 decoder transforms these fused features into
accurate and relevant captions. The model was tested on the National Institutes
of Health (NIH) and Indiana University (IU) Chest X-ray datasets. On the IU
dataset, it achieved scores of 0.854 (B-1), 0.883 (CIDEr), 0.759 (METEOR), and
0.712 (ROUGE-L). On the NIH dataset, it achieved the best performance on all
metrics: BLEU 1--4 (0.825, 0.788, 0.765, 0.752), CIDEr (0.857), METEOR (0.726),
and ROUGE-L (0.705). This framework has the potential to enhance chest X-ray
evaluation, assisting radiologists in more precise and efficient diagnosis.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [190] [Radiometer Calibration using Machine Learning](https://arxiv.org/abs/2504.16791)
*S. A. K. Leeney,H. T. J. Bevins,E. de Lera Acedo,W. J. Handley,C. Kirkham,R. S. Patel,J. Zhu,D. Molnar,J. Cumner,D. Anstey,K. Artuc,G. Bernardi,M. Bucher,S. Carey,J. Cavillot,R. Chiello,W. Croukamp,D. I. L. de Villiers,J. A. Ely,A. Fialkov,T. Gessey-Jones,G. Kulkarni,A. Magro,P. D. Meerburg,S. Mittal,J. H. N. Pattison,S. Pegwal,C. M. Pieterse,J. R. Pritchard,E. Puchwein,N. Razavi-Ghods,I. L. V. Roque,A. Saxena,K. H. Scheutwinkel,P. Scott,E. Shen,P. H. Sims,M. Spinelli*

Main category: astro-ph.IM

TL;DR: 论文提出了一种基于机器学习的校准框架，用于提高射电天文辐射计的精度，特别针对探测21厘米线的实验。


<details>
  <summary>Details</summary>
Motivation: 传统校准方法（如Dicke切换）在处理天线与接收器阻抗不匹配时存在局限性，机器学习为复杂系统的校准提供了新思路。

Method: 使用神经网络，通过已知信号源训练模型，以校准辐射计系统。

Result: 首次测试并验证了机器学习校准框架在21厘米线探测实验中的高精度能力。

Conclusion: 机器学习校准框架为射电天文中的高精度辐射测量提供了有效解决方案。

Abstract: Radiometers are crucial instruments in radio astronomy, forming the primary
component of nearly all radio telescopes. They measure the intensity of
electromagnetic radiation, converting this radiation into electrical signals. A
radiometer's primary components are an antenna and a Low Noise Amplifier (LNA),
which is the core of the ``receiver'' chain. Instrumental effects introduced by
the receiver are typically corrected or removed during calibration. However,
impedance mismatches between the antenna and receiver can introduce unwanted
signal reflections and distortions. Traditional calibration methods, such as
Dicke switching, alternate the receiver input between the antenna and a
well-characterised reference source to mitigate errors by comparison. Recent
advances in Machine Learning (ML) offer promising alternatives. Neural
networks, which are trained using known signal sources, provide a powerful
means to model and calibrate complex systems where traditional analytical
approaches struggle. These methods are especially relevant for detecting the
faint sky-averaged 21-cm signal from atomic hydrogen at high redshifts. This is
one of the main challenges in observational Cosmology today. Here, for the
first time, we introduce and test a machine learning-based calibration
framework capable of achieving the precision required for radiometric
experiments aiming to detect the 21-cm line.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [191] [A Geometric Approach to Problems in Optimization and Data Science](https://arxiv.org/abs/2504.16270)
*Naren Sarayu Manoj*

Main category: math.OC

TL;DR: 论文分为两部分：第一部分关注计算优化问题，提出新算法；第二部分研究数据科学问题的统计保证，分析后门数据攻击和图聚类算法的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 利用高维几何和概率工具解决计算和统计机器学习中的问题。

Method: 第一部分提出近似凸多面体、稀疏化、鲁棒最小二乘回归和对偶优化的新算法；第二部分分析后门数据攻击的统计性质和图聚类算法的鲁棒性。

Result: 提出了针对优化和数据科学问题的新算法和统计模型。

Conclusion: 通过高维几何和概率工具，为计算和统计机器学习提供了新的解决方案。

Abstract: We give new results for problems in computational and statistical machine
learning using tools from high-dimensional geometry and probability.
  We break up our treatment into two parts. In Part I, we focus on
computational considerations in optimization. Specifically, we give new
algorithms for approximating convex polytopes in a stream, sparsification and
robust least squares regression, and dueling optimization.
  In Part II, we give new statistical guarantees for data science problems. In
particular, we formulate a new model in which we analyze statistical properties
of backdoor data poisoning attacks, and we study the robustness of graph
clustering algorithms to ``helpful'' misspecification.

</details>


### [192] [The Safety-Privacy Tradeoff in Linear Bandits](https://arxiv.org/abs/2504.16371)
*Arghavan Zibaie,Spencer Hutchinson,Ramtin Pedarsani,Mahnoosh Alizadeh*

Main category: math.OC

TL;DR: 论文研究了多智能体线性随机老虎机问题，在全局安全约束和局部差分隐私（LDP）下，协调器需平衡隐私保护、安全性和遗憾最小化的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决在多智能体环境下，如何在保护隐私的同时满足全局安全约束并最小化遗憾的问题。

Method: 通过安全集的锐度衡量几何性质对遗憾增长的影响，并提出给定最大遗憾预算下各智能体的最优隐私级别向量。

Result: 提出了一个在隐私、安全性和遗憾之间权衡的框架，并定义了不可单边改进的隐私级别向量。

Conclusion: 研究为多智能体系统中的隐私保护与性能优化提供了理论支持。

Abstract: We consider a collection of linear stochastic bandit problems, each modeling
the random response of different agents to proposed interventions, coupled
together by a global safety constraint. We assume a central coordinator must
choose actions to play on each bandit with the objective of regret
minimization, while also ensuring that the expected response of all agents
satisfies the global safety constraints at each round, in spite of uncertainty
about the bandits' parameters. The agents consider their observed responses to
be private and in order to protect their sensitive information, the data
sharing with the central coordinator is performed under local differential
privacy (LDP). However, providing higher level of privacy to different agents
would have consequences in terms of safety and regret. We formalize these
tradeoffs by building on the notion of the sharpness of the safety set - a
measure of how the geometric properties of the safe set affects the growth of
regret - and propose a unilaterally unimprovable vector of privacy levels for
different agents given a maximum regret budget.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [193] [Confidence Sequences for Generalized Linear Models via Regret Analysis](https://arxiv.org/abs/2504.16555)
*Eugenio Clerico,Hamish Flynn,Wojciech Kotłowski,Gergely Neu*

Main category: math.ST

TL;DR: 论文提出了一种通过序列预测构建统计模型参数置信集的方法，将问题转化为算法问题，并分为解析和算法两种转换方案。


<details>
  <summary>Details</summary>
Motivation: 旨在通过序列预测游戏降低广义线性模型（GLM）参数的超额似然，从而构建高概率置信集。

Method: 提出在线到置信集的转换方案，包括解析转换（基于低遗憾算法存在性）和算法转换（利用在线算法输出）。

Result: 该方法统一了现有最优置信集构造，并提出了文献中未见过的新类型。

Conclusion: 通过序列预测框架，论文为统计模型参数置信集提供了一种通用且创新的解决方案。

Abstract: We develop a methodology for constructing confidence sets for parameters of
statistical models via a reduction to sequential prediction. Our key
observation is that for any generalized linear model (GLM), one can construct
an associated game of sequential probability assignment such that achieving low
regret in the game implies a high-probability upper bound on the excess
likelihood of the true parameter of the GLM. This allows us to develop a scheme
that we call online-to-confidence-set conversions, which effectively reduces
the problem of proving the desired statistical claim to an algorithmic
question. We study two varieties of this conversion scheme: 1) analytical
conversions that only require proving the existence of algorithms with low
regret and provide confidence sets centered at the maximum-likelihood estimator
2) algorithmic conversions that actively leverage the output of the online
algorithm to construct confidence sets (and may be centered at other,
adaptively constructed point estimators). The resulting methodology recovers
all state-of-the-art confidence set constructions within a single framework,
and also provides several new types of confidence sets that were previously
unknown in the literature.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [194] [Security-First AI: Foundations for Robust and Trustworthy Systems](https://arxiv.org/abs/2504.16110)
*Krti Tallam*

Main category: cs.CR

TL;DR: 论文主张将AI安全作为基础层优先考虑，提出分层视角区分安全与保障，并强调安全优先方法对构建可信赖AI系统的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前AI讨论多关注安全性、透明度等，但AI安全（对抗性操纵的防护）是这些目标的基础，需优先考虑。

Method: 提出分层视角区分AI安全与保障，讨论核心威胁模型、攻击向量及防御机制，主张以指标驱动的方法。

Result: 强调AI安全是确保AI系统稳健性、透明度和问责制的关键。

Conclusion: AI安全应作为基础优先考虑，以指标驱动的方法能有效提升AI系统的可信赖性和韧性。

Abstract: The conversation around artificial intelligence (AI) often focuses on safety,
transparency, accountability, alignment, and responsibility. However, AI
security (i.e., the safeguarding of data, models, and pipelines from
adversarial manipulation) underpins all of these efforts. This manuscript
posits that AI security must be prioritized as a foundational layer. We present
a hierarchical view of AI challenges, distinguishing security from safety, and
argue for a security-first approach to enable trustworthy and resilient AI
systems. We discuss core threat models, key attack vectors, and emerging
defense mechanisms, concluding that a metric-driven approach to AI security is
essential for robust AI safety, transparency, and accountability.

</details>


### [195] [AI-Based Vulnerability Analysis of NFT Smart Contracts](https://arxiv.org/abs/2504.16113)
*Xin Wang,Xiaoqi Li*

Main category: cs.CR

TL;DR: 本文通过收集和分类智能合约代码，识别常见缺陷，并利用决策树和随机森林模型进行分析，最终比较不同模型的性能。


<details>
  <summary>Details</summary>
Motivation: 研究智能合约中的常见缺陷，并开发有效的模型来识别和分析这些缺陷。

Method: 1. 收集和分类智能合约代码；2. 使用Python处理数据；3. 构建决策树模型并进行特征提取；4. 引入随机森林模型并优化参数；5. 比较不同模型性能。

Result: 通过决策树和随机森林模型成功识别和分析智能合约中的常见缺陷。

Conclusion: 决策树和随机森林模型在智能合约缺陷分析中表现良好，随机森林模型在性能上更优。

Abstract: In the research experiment of this article, our research work is divided into
several stages. Firstly, we collected a large number of smart contract codes
and classified them, identifying several common defects, including Risky
Mutably Porxy, ERC-721 Recentrancy, Unlimited Mining, Missing Requirements, and
Public Burns. Secondly, we used Python to process the smart contracts. On the
one hand, we modified the file names, and on the other hand, we batched the
process of the content for analysis and application. Next, we built a model of
the decision tree. Firstly, we carried out the feature extraction. We selected
the algorithm and divided the data. After comparing and processing, we chose
the CART classification tree to process. By gene coefficient, we analyzed and
sorted the data, and got the initial model of the decision tree. Then, we
introduced the random forest model on the basis of the decision tree. From
abstracting the same amount of samples to selecting features randomly.From
adjusting and optimizing parameters to completing the construction of the
forest model. Finally, we compared and analyzed the decision tree, random
forest, and self-built model in the paper and drew general conclusions.

</details>


### [196] [DMind Benchmark: The First Comprehensive Benchmark for LLM Evaluation in the Web3 Domain](https://arxiv.org/abs/2504.16116)
*Miracle Master,Rainy Sun,Anya Reese,Joey Ouyang,Alex Chen,Winter Dong,Frank Li,James Yi,Garry Zhao,Tony Ling,Hobert Wong,Lowes Yang*

Main category: cs.CR

TL;DR: 论文介绍了DMind Benchmark，一个评估大型语言模型（LLMs）在Web3领域表现的框架，涵盖九大类别，并揭示了模型在Web3特定任务中的性能差距。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自然语言处理任务中表现优异，但在Web3等专业且快速发展的领域中的有效性尚未充分探索。

Method: 开发了DMind Benchmark，包含多类任务（如智能合约审计、链上数据推理等），并评估了15种流行LLMs的表现。

Result: 发现LLMs在Web3特定任务（如代币经济学和模因概念）中存在性能差距，尤其在安全漏洞识别和复杂DeFi机制分析方面表现不佳。

Conclusion: 公开了数据集和评估工具，以促进Web3领域LLMs的进一步研究和开发。

Abstract: Recent advances in Large Language Models (LLMs) have led to significant
progress on a wide range of natural language processing tasks. However, their
effectiveness in specialized and rapidly evolving domains such as Web3 remains
underexplored. In this paper, we introduce DMind Benchmark, a novel framework
that systematically tests LLMs across nine key categories encompassing
blockchain fundamentals, infrastructure, smart contract analysis, decentralized
finance (DeFi), decentralized autonomous organizations (DAOs), non-fungible
tokens (NFTs), token economics, meme concepts, and security vulnerabilities.
  DMind Benchmark goes beyond conventional multiple-choice questions by
incorporating domain-specific subjective tasks (e.g., smart contract code
auditing and repair, numeric reasoning on on-chain data, and fill-in
assessments), thereby capturing real-world complexities and stress-testing
model adaptability. We evaluate fifteen popular LLMs (from ChatGPT, DeepSeek,
Claude, and Gemini series) on DMind Benchmark, uncovering performance gaps in
Web3-specific reasoning and application, particularly in emerging areas like
token economics and meme concepts. Even the strongest models face significant
challenges in identifying subtle security vulnerabilities and analyzing complex
DeFi mechanisms. To foster progress in this area, we publicly release our
benchmark dataset, evaluation pipeline, and annotated results at
http://www.dmind.ai, offering a valuable resource for advancing specialized
domain adaptation and the development of more robust Web3-enabled LLMs.

</details>


### [197] [Towards Explainable and Lightweight AI for Real-Time Cyber Threat Hunting in Edge Networks](https://arxiv.org/abs/2504.16118)
*Milad Rahmati*

Main category: cs.CR

TL;DR: 该论文提出了一种可解释且轻量级的AI框架（ELAI），用于边缘网络中的实时网络威胁检测，解决了传统深度学习模型在可解释性和计算成本上的不足。


<details>
  <summary>Details</summary>
Motivation: 边缘网络的分布式特性和资源限制使得网络安全面临挑战，而传统AI模型缺乏可解释性且计算成本高，限制了其实际应用。

Method: 结合可解释的机器学习算法和优化的轻量级深度学习技术，采用决策树、注意力机制和联邦学习来提高检测准确性和可解释性。

Result: 在CICIDS和UNSW-NB15等数据集上的实验表明，ELAI实现了高检测率和低误报率，同时显著降低了计算需求。

Conclusion: ELAI为边缘计算环境提供了一种高效、可解释的网络安全解决方案，兼具高性能和低计算成本。

Abstract: As cyber threats continue to evolve, securing edge networks has become
increasingly challenging due to their distributed nature and resource
limitations. Many AI-driven threat detection systems rely on complex deep
learning models, which, despite their high accuracy, suffer from two major
drawbacks: lack of interpretability and high computational cost. Black-box AI
models make it difficult for security analysts to understand the reasoning
behind their predictions, limiting their practical deployment. Moreover,
conventional deep learning techniques demand significant computational
resources, rendering them unsuitable for edge devices with limited processing
power. To address these issues, this study introduces an Explainable and
Lightweight AI (ELAI) framework designed for real-time cyber threat detection
in edge networks. Our approach integrates interpretable machine learning
algorithms with optimized lightweight deep learning techniques, ensuring both
transparency and computational efficiency. The proposed system leverages
decision trees, attention-based deep learning, and federated learning to
enhance detection accuracy while maintaining explainability. We evaluate ELAI
using benchmark cybersecurity datasets, such as CICIDS and UNSW-NB15, assessing
its performance across diverse cyberattack scenarios. Experimental results
demonstrate that the proposed framework achieves high detection rates with
minimal false positives, all while significantly reducing computational demands
compared to traditional deep learning methods. The key contributions of this
work include: (1) a novel interpretable AI-based cybersecurity model tailored
for edge computing environments, (2) an optimized lightweight deep learning
approach for real-time cyber threat detection, and (3) a comprehensive analysis
of explainability techniques in AI-driven cybersecurity applications.

</details>


### [198] [A Data-Centric Approach for Safe and Secure Large Language Models against Threatening and Toxic Content](https://arxiv.org/abs/2504.16120)
*Chaima Njeh,Haïfa Nakouri,Fehmi Jaafar*

Main category: cs.CR

TL;DR: 论文提出了一种后生成修正机制（BART-Corrective Model），用于减少大语言模型（LLM）生成内容中的偏见和有害信息，显著降低了毒性分数和越狱分数。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型取得了显著进展，但其潜在的偏见和有害内容问题仍然存在，需要一种安全且伦理的解决方案。

Method: 采用数据为中心的后生成修正机制（BART-Corrective Model），而非仅依赖模型微调或提示工程。

Result: 在多个毒性数据集上实验显示，该方法显著降低了毒性分数和越狱分数（如GPT-4降低15%和21%，PaLM2降低28%和5%等）。

Conclusion: 该方法能有效提升LLM的安全性和适用性，适合实际应用。

Abstract: Large Language Models (LLM) have made remarkable progress, but concerns about
potential biases and harmful content persist. To address these apprehensions,
we introduce a practical solution for ensuring LLM's safe and ethical use. Our
novel approach focuses on a post-generation correction mechanism, the
BART-Corrective Model, which adjusts generated content to ensure safety and
security. Unlike relying solely on model fine-tuning or prompt engineering, our
method provides a robust data-centric alternative for mitigating harmful
content. We demonstrate the effectiveness of our approach through experiments
on multiple toxic datasets, which show a significant reduction in mean toxicity
and jail-breaking scores after integration. Specifically, our results show a
reduction of 15% and 21% in mean toxicity and jail-breaking scores with GPT-4,
a substantial reduction of 28% and 5% with PaLM2, a reduction of approximately
26% and 23% with Mistral-7B, and a reduction of 11.1% and 19% with Gemma-2b-it.
These results demonstrate the potential of our approach to improve the safety
and security of LLM, making them more suitable for real-world applications.

</details>


### [199] [Blockchain Meets Adaptive Honeypots: A Trust-Aware Approach to Next-Gen IoT Security](https://arxiv.org/abs/2504.16226)
*Yazan Otoum,Arghavan Asad,Amiya Nayak*

Main category: cs.CR

TL;DR: 提出了一种动态攻击检测与防御框架，结合区块链认证、双阶段入侵检测系统、信任感知服务迁移和虚拟蜜罐技术，显著提升了NGWN-IoT的安全性。


<details>
  <summary>Details</summary>
Motivation: NGWN-IoT面临不断演变的网络威胁，现有入侵检测方法效果有限，需动态防御方案。

Method: 采用区块链认证（DAA）、双阶段入侵检测（IRF和DCRNN）、信任感知服务迁移（HBO）和虚拟蜜罐（BLISS存储攻击模式）。

Result: 在NS3环境中验证，框架在准确性、检测率等多项指标上优于现有方法。

Conclusion: 该框架显著增强了NGWN-IoT生态系统的安全性。

Abstract: Edge computing-based Next-Generation Wireless Networks (NGWN)-IoT offer
enhanced bandwidth capacity for large-scale service provisioning but remain
vulnerable to evolving cyber threats. Existing intrusion detection and
prevention methods provide limited security as adversaries continually adapt
their attack strategies. We propose a dynamic attack detection and prevention
approach to address this challenge. First, blockchain-based authentication uses
the Deoxys Authentication Algorithm (DAA) to verify IoT device legitimacy
before data transmission. Next, a bi-stage intrusion detection system is
introduced: the first stage uses signature-based detection via an Improved
Random Forest (IRF) algorithm. In contrast, the second stage applies
feature-based anomaly detection using a Diffusion Convolution Recurrent Neural
Network (DCRNN). To ensure Quality of Service (QoS) and maintain Service Level
Agreements (SLA), trust-aware service migration is performed using Heap-Based
Optimization (HBO). Additionally, on-demand virtual High-Interaction honeypots
deceive attackers and extract attack patterns, which are securely stored using
the Bimodal Lattice Signature Scheme (BLISS) to enhance signature-based
Intrusion Detection Systems (IDS). The proposed framework is implemented in the
NS3 simulation environment and evaluated against existing methods across
multiple performance metrics, including accuracy, attack detection rate, false
negative rate, precision, recall, ROC curve, memory usage, CPU usage, and
execution time. Experimental results demonstrate that the framework
significantly outperforms existing approaches, reinforcing the security of
NGWN-enabled IoT ecosystems

</details>


### [200] [On the Consistency of GNN Explanations for Malware Detection](https://arxiv.org/abs/2504.16316)
*Hossein Shokouhinejad,Griffin Higgins,Roozbeh Razavi-Far,Hesamodin Mohammadian,Ali A. Ghorbani*

Main category: cs.CR

TL;DR: 提出了一种结合规则编码和自动编码器的动态CFG构建与节点特征嵌入框架，利用GNN分类器检测恶意行为，并通过多种解释性技术和新方法RankFusion提升模型可解释性。


<details>
  <summary>Details</summary>
Motivation: CFG在程序分析和恶意软件检测中至关重要，但现有方法在动态构建和解释性方面存在不足。

Method: 动态构建CFG，结合规则编码和自动编码器嵌入节点特征，使用GNN分类器检测恶意行为，并应用多种解释性技术（如GNNExplainer、PGExplainer等）和RankFusion方法提升解释质量。

Result: 框架在准确性、保真度和一致性方面表现优异，能有效识别恶意软件并生成可靠解释。

Conclusion: 提出的框架在恶意软件检测和解释性方面具有显著优势，为未来研究提供了新方向。

Abstract: Control Flow Graphs (CFGs) are critical for analyzing program execution and
characterizing malware behavior. With the growing adoption of Graph Neural
Networks (GNNs), CFG-based representations have proven highly effective for
malware detection. This study proposes a novel framework that dynamically
constructs CFGs and embeds node features using a hybrid approach combining
rule-based encoding and autoencoder-based embedding. A GNN-based classifier is
then constructed to detect malicious behavior from the resulting graph
representations. To improve model interpretability, we apply state-of-the-art
explainability techniques, including GNNExplainer, PGExplainer, and
CaptumExplainer, the latter is utilized three attribution methods: Integrated
Gradients, Guided Backpropagation, and Saliency. In addition, we introduce a
novel aggregation method, called RankFusion, that integrates the outputs of the
top-performing explainers to enhance the explanation quality. We also evaluate
explanations using two subgraph extraction strategies, including the proposed
Greedy Edge-wise Composition (GEC) method for improved structural coherence. A
comprehensive evaluation using accuracy, fidelity, and consistency metrics
demonstrates the effectiveness of the proposed framework in terms of accurate
identification of malware samples and generating reliable and interpretable
explanations.

</details>


### [201] [Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate](https://arxiv.org/abs/2504.16489)
*Senmao Qi,Yifei Zou,Peng Li,Ziyi Lin,Xiuzhen Cheng,Dongxiao Yu*

Main category: cs.CR

TL;DR: 该论文研究了多智能体辩论（MAD）框架在大型语言模型（LLMs）中的安全漏洞，特别是其对越狱攻击的敏感性，并提出了一种新型攻击方法。


<details>
  <summary>Details</summary>
Motivation: 探讨MAD框架在复杂任务中增强推理能力的同时，其迭代对话和角色扮演特性可能带来的安全隐患，尤其是越狱攻击的风险。

Method: 提出了一种结构化提示重写框架，通过叙事封装、角色驱动升级、迭代优化和修辞混淆来利用MAD动态。

Result: 实验表明，MAD系统比单智能体设置更易受攻击，攻击方法将平均危害性从28.14%提高到80.34%，攻击成功率在某些场景下高达80%。

Conclusion: MAD架构存在固有漏洞，需在现实部署前开发专门的防御措施。

Abstract: Multi-Agent Debate (MAD), leveraging collaborative interactions among Large
Language Models (LLMs), aim to enhance reasoning capabilities in complex tasks.
However, the security implications of their iterative dialogues and
role-playing characteristics, particularly susceptibility to jailbreak attacks
eliciting harmful content, remain critically underexplored. This paper
systematically investigates the jailbreak vulnerabilities of four prominent MAD
frameworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo,
and DeepSeek) without compromising internal agents. We introduce a novel
structured prompt-rewriting framework specifically designed to exploit MAD
dynamics via narrative encapsulation, role-driven escalation, iterative
refinement, and rhetorical obfuscation. Our extensive experiments demonstrate
that MAD systems are inherently more vulnerable than single-agent setups.
Crucially, our proposed attack methodology significantly amplifies this
fragility, increasing average harmfulness from 28.14% to 80.34% and achieving
attack success rates as high as 80% in certain scenarios. These findings reveal
intrinsic vulnerabilities in MAD architectures and underscore the urgent need
for robust, specialized defenses prior to real-world deployment.

</details>


### [202] [Property-Preserving Hashing for $\ell_1$-Distance Predicates: Applications to Countering Adversarial Input Attacks](https://arxiv.org/abs/2504.16355)
*Hassan Asghar,Chenhan Zhang,Dali Kaafar*

Main category: cs.CR

TL;DR: 本文提出了一种基于ℓ1距离的财产保留哈希（PPH）构造，用于在对抗性环境下检测相似图像，具有高效性和强正确性保证。


<details>
  <summary>Details</summary>
Motivation: 对抗性攻击能够通过微小修改图像绕过感知哈希检测，因此需要一种能够保留输入属性的哈希方法以增强安全性。

Method: 提出了一种基于ℓ1距离的PPH构造，通过设定阈值t，迫使攻击者添加显著噪声以规避检测，从而降低图像质量。

Result: 方案高效，运行时间为O(t²)，在28×28灰度图像上0.0784秒完成检测，224×224 RGB图像通过分块处理实现快速检测。

Conclusion: 该PPH构造在对抗性环境下有效检测相似图像，同时显著提高攻击成本，具有实际应用潜力。

Abstract: Perceptual hashing is used to detect whether an input image is similar to a
reference image with a variety of security applications. Recently, they have
been shown to succumb to adversarial input attacks which make small
imperceptible changes to the input image yet the hashing algorithm does not
detect its similarity to the original image. Property-preserving hashing (PPH)
is a recent construct in cryptography, which preserves some property
(predicate) of its inputs in the hash domain. Researchers have so far shown
constructions of PPH for Hamming distance predicates, which, for instance,
outputs 1 if two inputs are within Hamming distance $t$. A key feature of PPH
is its strong correctness guarantee, i.e., the probability that the predicate
will not be correctly evaluated in the hash domain is negligible. Motivated by
the use case of detecting similar images under adversarial setting, we propose
the first PPH construction for an $\ell_1$-distance predicate. Roughly, this
predicate checks if the two one-sided $\ell_1$-distances between two images are
within a threshold $t$. Since many adversarial attacks use $\ell_2$-distance
(related to $\ell_1$-distance) as the objective function to perturb the input
image, by appropriately choosing the threshold $t$, we can force the attacker
to add considerable noise to evade detection, and hence significantly
deteriorate the image quality. Our proposed scheme is highly efficient, and
runs in time $O(t^2)$. For grayscale images of size $28 \times 28$, we can
evaluate the predicate in $0.0784$ seconds when pixel values are perturbed by
up to $1 \%$. For larger RGB images of size $224 \times 224$, by dividing the
image into 1,000 blocks, we achieve times of $0.0128$ seconds per block for $1
\%$ change, and up to $0.2641$ seconds per block for $14\%$ change.

</details>


### [203] [Case Study: Fine-tuning Small Language Models for Accurate and Private CWE Detection in Python Code](https://arxiv.org/abs/2504.16584)
*Md. Azizul Hakim Bappy,Hossen A Mustafa,Prottoy Saha,Rajinus Salehat*

Main category: cs.CR

TL;DR: 研究探讨了小型语言模型（SLMs）作为大型语言模型（LLMs）的替代方案，用于在本地准确检测代码漏洞。通过微调350M参数的预训练模型，实现了99%的准确率。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码漏洞检测中表现优异，但依赖云端且计算成本高，隐私问题突出。SLMs可能提供本地化、隐私保护的解决方案。

Method: 使用半监督方法生成500个Python代码漏洞样本，微调预训练的codegen-mono模型。

Result: 微调后的SLM在测试集上表现优异：准确率99%，精确率98.08%，召回率100%，F1分数99.04%。

Conclusion: 微调SLMs可作为高效、隐私保护的漏洞检测工具，适合集成到开发流程中。

Abstract: Large Language Models (LLMs) have demonstrated significant capabilities in
understanding and analyzing code for security vulnerabilities, such as Common
Weakness Enumerations (CWEs). However, their reliance on cloud infrastructure
and substantial computational requirements pose challenges for analyzing
sensitive or proprietary codebases due to privacy concerns and inference costs.
This work explores the potential of Small Language Models (SLMs) as a viable
alternative for accurate, on-premise vulnerability detection. We investigated
whether a 350-million parameter pre-trained code model (codegen-mono) could be
effectively fine-tuned to detect the MITRE Top 25 CWEs specifically within
Python code. To facilitate this, we developed a targeted dataset of 500
examples using a semi-supervised approach involving LLM-driven synthetic data
generation coupled with meticulous human review. Initial tests confirmed that
the base codegen-mono model completely failed to identify CWEs in our samples.
However, after applying instruction-following fine-tuning, the specialized SLM
achieved remarkable performance on our test set, yielding approximately 99%
accuracy, 98.08% precision, 100% recall, and a 99.04% F1-score. These results
strongly suggest that fine-tuned SLMs can serve as highly accurate and
efficient tools for CWE detection, offering a practical and privacy-preserving
solution for integrating advanced security analysis directly into development
workflows.

</details>


### [204] [From Past to Present: A Survey of Malicious URL Detection Techniques, Datasets and Code Repositories](https://arxiv.org/abs/2504.16449)
*Ye Tian,Yanqiu Yu,Jianguo Sun,Yanbin Wang*

Main category: cs.CR

TL;DR: 本文全面综述了恶意URL检测技术，提出了一种基于模态的分类法，并整理了公开数据集和开源实现，为未来研究提供了方向。


<details>
  <summary>Details</summary>
Motivation: 恶意URL对网络安全构成持续威胁，现有研究存在算法分类模糊、未涵盖LLM/Transformer防御、缺乏开源实现和数据集覆盖不足等问题。

Method: 提出基于模态的分类法，系统分析从传统黑名单到深度学习方法（如Transformer、GNNs和LLMs）的技术，并整理公开数据集和开源实现。

Result: 建立了恶意URL检测的多模态分类体系，提供了标准化基准测试的数据集和开源实现，并提出了产品级实现的设计原则。

Conclusion: 总结了当前研究的不足，提出了未来研究方向，并维护了一个GitHub仓库以持续更新数据集和开源实现。

Abstract: Malicious URLs persistently threaten the cybersecurity ecosystem, by either
deceiving users into divulging private data or distributing harmful payloads to
infiltrate host systems. Gaining timely insights into the current state of this
ongoing battle holds significant importance. However, existing reviews exhibit
4 critical gaps: 1) Their reliance on algorithm-centric taxonomies obscures
understanding of how detection approaches exploit specific modal information
channels; 2) They fail to incorporate pivotal LLM/Transformer-based defenses;
3) No open-source implementations are collected to facilitate benchmarking; 4)
Insufficient dataset coverage.This paper presents a comprehensive review of
malicious URL detection technologies, systematically analyzing methods from
traditional blacklisting to advanced deep learning approaches (e.g.
Transformer, GNNs, and LLMs). Unlike prior surveys, we propose a novel
modality-based taxonomy that categorizes existing works according to their
primary data modalities (URL, HTML, Visual, etc.). This hierarchical
classification enables both rigorous technical analysis and clear understanding
of multimodal information utilization. Furthermore, to establish a profile of
accessible datasets and address the lack of standardized benchmarking (where
current studies often lack proper baseline comparisons), we curate and analyze:
1) publicly available datasets (2016-2024), and 2) open-source implementations
from published works(2013-2025). Then, we outline essential design principles
and architectural frameworks for product-level implementations. The review
concludes by examining emerging challenges and proposing actionable directions
for future research. We maintain a GitHub repository for ongoing curating
datasets and open-source implementations:
https://github.com/sevenolu7/Malicious-URL-Detection-Open-Source/tree/master.

</details>


### [205] [Seeking Flat Minima over Diverse Surrogates for Improved Adversarial Transferability: A Theoretical Framework and Algorithmic Instantiation](https://arxiv.org/abs/2504.16474)
*Meixi Zheng,Kehan Wu,Yanbo Fan,Rui Huang,Baoyuan Wu*

Main category: cs.CR

TL;DR: 本文提出了一种基于理论的可转移性边界，用于提升黑盒对抗攻击的迁移能力，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的黑盒对抗攻击方法缺乏理论基础，本文旨在填补这一空白，提供理论支持并指导攻击算法的设计。

Method: 通过优化对抗样本在代理模型集上的平坦性，并控制代理-目标模型间的对抗模型差异，提出了一种通用的攻击框架。

Result: 实验证明，该方法在NIPS2017和CIFAR-10数据集上对多种目标模型具有显著攻击效果。

Conclusion: 本文的理论和算法为对抗样本的可转移性提供了新的视角，并展示了实际应用中的有效性。

Abstract: The transfer-based black-box adversarial attack setting poses the challenge
of crafting an adversarial example (AE) on known surrogate models that remain
effective against unseen target models. Due to the practical importance of this
task, numerous methods have been proposed to address this challenge. However,
most previous methods are heuristically designed and intuitively justified,
lacking a theoretical foundation. To bridge this gap, we derive a novel
transferability bound that offers provable guarantees for adversarial
transferability. Our theoretical analysis has the advantages of \textit{(i)}
deepening our understanding of previous methods by building a general attack
framework and \textit{(ii)} providing guidance for designing an effective
attack algorithm. Our theoretical results demonstrate that optimizing AEs
toward flat minima over the surrogate model set, while controlling the
surrogate-target model shift measured by the adversarial model discrepancy,
yields a comprehensive guarantee for AE transferability. The results further
lead to a general transfer-based attack framework, within which we observe that
previous methods consider only partial factors contributing to the
transferability. Algorithmically, inspired by our theoretical results, we first
elaborately construct the surrogate model set in which models exhibit diverse
adversarial vulnerabilities with respect to AEs to narrow an instantiated
adversarial model discrepancy. Then, a \textit{model-Diversity-compatible
Reverse Adversarial Perturbation} (DRAP) is generated to effectively promote
the flatness of AEs over diverse surrogate models to improve transferability.
Extensive experiments on NIPS2017 and CIFAR-10 datasets against various target
models demonstrate the effectiveness of our proposed attack.

</details>


### [206] [MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark](https://arxiv.org/abs/2504.16651)
*William Corrias,Fabio De Gaspari,Dorjan Hitaj,Luigi V. Mancini*

Main category: cs.CR

TL;DR: MAYA是一个统一的密码基准测试框架，用于评估生成式密码猜测模型，通过标准化测试和真实数据集验证了六种先进方法的性能。


<details>
  <summary>Details</summary>
Motivation: 生成模型在密码猜测领域的应用缺乏一致性和严格评估，阻碍了对其潜力的全面理解。

Method: 引入MAYA框架，提供标准化评估方法，包括高级测试场景和八个真实密码数据集，重新实现并评估六种先进模型。

Result: 模型能捕捉人类密码分布的不同方面，但长密码效果差异大；序列模型表现最佳，多模型攻击优于单一模型。

Conclusion: MAYA为密码生成技术提供了可靠的基准测试工具，推动进一步研究。

Abstract: The rapid evolution of generative models has led to their integration across
various fields, including password guessing, aiming to generate passwords that
resemble human-created ones in complexity, structure, and patterns. Despite
generative model's promise, inconsistencies in prior research and a lack of
rigorous evaluation have hindered a comprehensive understanding of their true
potential. In this paper, we introduce MAYA, a unified, customizable,
plug-and-play password benchmarking framework. MAYA provides a standardized
approach for evaluating generative password-guessing models through a rigorous
set of advanced testing scenarios and a collection of eight real-life password
datasets. Using MAYA, we comprehensively evaluate six state-of-the-art
approaches, which have been re-implemented and adapted to ensure
standardization, for a total of over 15,000 hours of computation. Our findings
indicate that these models effectively capture different aspects of human
password distribution and exhibit strong generalization capabilities. However,
their effectiveness varies significantly with long and complex passwords.
Through our evaluation, sequential models consistently outperform other
generative architectures and traditional password-guessing tools, demonstrating
unique capabilities in generating accurate and complex guesses. Moreover,
models learn and generate different password distributions, enabling a
multi-model attack that outperforms the best individual model. By releasing
MAYA, we aim to foster further research, providing the community with a new
tool to consistently and reliably benchmark password-generation techniques. Our
framework is publicly available at
https://github.com/williamcorrias/MAYA-Password-Benchmarking

</details>


### [207] [Building A Secure Agentic AI Application Leveraging A2A Protocol](https://arxiv.org/abs/2504.16902)
*Idan Habler,Ken Huang,Vineeth Sai Narajala,Prashant Kulkarni*

Main category: cs.CR

TL;DR: 本文对Google的Agent2Agent（A2A）协议进行了全面的安全分析，提出了基于MAESTRO框架的威胁建模方法，并推荐了安全开发实践和架构优化建议。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从简单工作流发展为复杂的多智能体协作，确保A2A协议的安全实现成为关键需求。

Method: 使用MAESTRO框架进行主动威胁建模，分析A2A协议的核心要素和运行机制。

Result: 提出了针对A2A部署的安全开发方法和架构最佳实践，探讨了A2A与MCP协议的协同效应。

Conclusion: 本文为开发者提供了构建安全、可靠的下一代智能体应用所需的知识和实践指导。

Abstract: As Agentic AI systems evolve from basic workflows to complex multi agent
collaboration, robust protocols such as Google's Agent2Agent (A2A) become
essential enablers. To foster secure adoption and ensure the reliability of
these complex interactions, understanding the secure implementation of A2A is
essential. This paper addresses this goal by providing a comprehensive security
analysis centered on the A2A protocol. We examine its fundamental elements and
operational dynamics, situating it within the framework of agent communication
development. Utilizing the MAESTRO framework, specifically designed for AI
risks, we apply proactive threat modeling to assess potential security issues
in A2A deployments, focusing on aspects such as Agent Card management, task
execution integrity, and authentication methodologies.
  Based on these insights, we recommend practical secure development
methodologies and architectural best practices designed to build resilient and
effective A2A systems. Our analysis also explores how the synergy between A2A
and the Model Context Protocol (MCP) can further enhance secure
interoperability. This paper equips developers and architects with the
knowledge and practical guidance needed to confidently leverage the A2A
protocol for building robust and secure next generation agentic applications.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [208] [Common Functional Decompositions Can Mis-attribute Differences in Outcomes Between Populations](https://arxiv.org/abs/2504.16864)
*Manuel Quintero,William T. Stephenson,Advik Shreekumar,Tamara Broderick*

Main category: stat.ME

TL;DR: 论文探讨了如何扩展Kitagawa-Oaxaca-Blinder（KOB）分解方法以处理非线性关系，发现常见的非线性分解方法（如函数ANOVA和ALE）在简单情况下会错误归因差异，并提出避免错误归因的条件。


<details>
  <summary>Details</summary>
Motivation: 在社会科学中，解释两个群体间结果差异的原因是一个重要问题。传统的KOB分解方法假设线性关系，但现实中可能存在非线性关系，因此需要扩展该方法。

Method: 研究分析了两种常见的非线性分解方法（函数ANOVA和ALE），并展示了它们在简单情况下如何错误归因差异。作者提出了一个避免错误归因的一般性质，并探讨了分解方法与其输入分布的关系。

Result: 研究发现，依赖协变量分布的非线性分解方法可能导致错误归因，而独立于输入分布的分解方法可以避免这一问题。

Conclusion: 论文指出，任何合理的加性分解方法如果依赖协变量分布，都可能产生错误归因，而独立于输入分布的分解方法更为可靠。

Abstract: In science and social science, we often wish to explain why an outcome is
different in two populations. For instance, if a jobs program benefits members
of one city more than another, is that due to differences in program
participants (particular covariates) or the local labor markets (outcomes given
covariates)? The Kitagawa-Oaxaca-Blinder (KOB) decomposition is a standard tool
in econometrics that explains the difference in the mean outcome across two
populations. However, the KOB decomposition assumes a linear relationship
between covariates and outcomes, while the true relationship may be
meaningfully nonlinear. Modern machine learning boasts a variety of nonlinear
functional decompositions for the relationship between outcomes and covariates
in one population. It seems natural to extend the KOB decomposition using these
functional decompositions. We observe that a successful extension should not
attribute the differences to covariates -- or, respectively, to outcomes given
covariates -- if those are the same in the two populations. Unfortunately, we
demonstrate that, even in simple examples, two common decompositions --
functional ANOVA and Accumulated Local Effects -- can attribute differences to
outcomes given covariates, even when they are identical in two populations. We
provide a characterization of when functional ANOVA misattributes, as well as a
general property that any discrete decomposition must satisfy to avoid
misattribution. We show that if the decomposition is independent of its input
distribution, it does not misattribute. We further conjecture that
misattribution arises in any reasonable additive decomposition that depends on
the distribution of the covariates.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [209] [Detecting Actionable Requests and Offers on Social Media During Crises Using LLMs](https://arxiv.org/abs/2504.16144)
*Ahmed El Fekih Zguir,Ferda Ofli,Muhammad Imran*

Main category: cs.IR

TL;DR: 提出了一种基于LLM的细粒度分类方法QSF Learning，用于在自然灾害期间高效分类社交媒体信息，并评估其可操作性。


<details>
  <summary>Details</summary>
Motivation: 自然灾害期间社交媒体信息激增，需要系统化分类以提升人道主义组织的响应效率。

Method: 提出QSF Learning方法，利用LLM从嵌入数据库中检索类特定示例，优化分类性能，并评估信息的可操作性。

Result: 实验表明，该方法优于基线提示策略，能有效识别和优先处理可操作的请求和援助信息。

Conclusion: QSF Learning为自然灾害中的信息分类和优先级处理提供了高效解决方案。

Abstract: Natural disasters often result in a surge of social media activity, including
requests for assistance, offers of help, sentiments, and general updates. To
enable humanitarian organizations to respond more efficiently, we propose a
fine-grained hierarchical taxonomy to systematically organize crisis-related
information about requests and offers into three critical dimensions: supplies,
emergency personnel, and actions. Leveraging the capabilities of Large Language
Models (LLMs), we introduce Query-Specific Few-shot Learning (QSF Learning)
that retrieves class-specific labeled examples from an embedding database to
enhance the model's performance in detecting and classifying posts. Beyond
classification, we assess the actionability of messages to prioritize posts
requiring immediate attention. Extensive experiments demonstrate that our
approach outperforms baseline prompting strategies, effectively identifying and
prioritizing actionable requests and offers.

</details>


### [210] [LegalRAG: A Hybrid RAG System for Multilingual Legal Information Retrieval](https://arxiv.org/abs/2504.16121)
*Muhammad Rafsan Kabir,Rafeed Mohammad Sultan,Fuad Rahman,Mohammad Ruhul Amin,Sifat Momen,Nabeel Mohammed,Shafin Rahman*

Main category: cs.IR

TL;DR: 开发了一个高效的双语问答框架，用于处理孟加拉警察公报中的法律文件，采用改进的RAG方法提升检索和回答性能。


<details>
  <summary>Details</summary>
Motivation: 解决NLP在法律和监管任务中应用不足的问题，特别是针对双语法律文件的问答需求。

Method: 结合现代RAG管道和提出的高级RAG方法，优化信息检索和回答生成。

Result: 在孟加拉警察公报的测试集上，提出的方法在所有评估指标上均优于现有方法。

Conclusion: 改进的RAG框架显著提升了法律文件问答的效率和准确性，为法律信息获取提供了更便捷的途径。

Abstract: Natural Language Processing (NLP) and computational linguistic techniques are
increasingly being applied across various domains, yet their use in legal and
regulatory tasks remains limited. To address this gap, we develop an efficient
bilingual question-answering framework for regulatory documents, specifically
the Bangladesh Police Gazettes, which contain both English and Bangla text. Our
approach employs modern Retrieval Augmented Generation (RAG) pipelines to
enhance information retrieval and response generation. In addition to
conventional RAG pipelines, we propose an advanced RAG-based approach that
improves retrieval performance, leading to more precise answers. This system
enables efficient searching for specific government legal notices, making legal
information more accessible. We evaluate both our proposed and conventional RAG
systems on a diverse test set on Bangladesh Police Gazettes, demonstrating that
our approach consistently outperforms existing methods across all evaluation
metrics.

</details>


### [211] [CLIRudit: Cross-Lingual Information Retrieval of Scientific Documents](https://arxiv.org/abs/2504.16264)
*Francisco Valentini,Diego Kozlowski,Vincent Larivière*

Main category: cs.IR

TL;DR: CLIRudit是一个新的跨语言学术搜索数据集，用于评估英语查询和法语文档的检索性能。研究比较了多种零样本检索方法，发现大型密集检索器表现优异，无需机器翻译即可媲美人工翻译效果。


<details>
  <summary>Details</summary>
Motivation: 解决跨语言学术搜索中语言障碍问题，促进科学知识的跨语言可访问性。

Method: 使用Erudit平台的双语文章元数据构建CLIRudit数据集，并评估多种零样本检索方法（密集/稀疏检索器、机器翻译等）。

Result: 大型密集检索器在零样本任务中表现优异，稀疏检索器结合文档翻译也具竞争力。

Conclusion: 研究为跨语言学术检索提供了新数据集和方法框架，推动科学知识的跨语言共享。

Abstract: Cross-lingual information retrieval (CLIR) consists in finding relevant
documents in a language that differs from the language of the queries. This
paper presents CLIRudit, a new dataset created to evaluate cross-lingual
academic search, focusing on English queries and French documents. The dataset
is built using bilingual article metadata from \'Erudit, a Canadian publishing
platform, and is designed to represent scenarios in which researchers search
for scholarly content in languages other than English. We perform a
comprehensive benchmarking of different zero-shot first-stage retrieval methods
on the dataset, including dense and sparse retrievers, query and document
machine translation, and state-of-the-art multilingual retrievers. Our results
show that large dense retrievers, not necessarily trained for the cross-lingual
retrieval task, can achieve zero-shot performance comparable to using ground
truth human translations, without the need for machine translation. Sparse
retrievers, such as BM25 or SPLADE, combined with document translation, show
competitive results, providing an efficient alternative to large dense models.
This research advances the understanding of cross-lingual academic information
retrieval and provides a framework that others can use to build comparable
datasets across different languages and disciplines. By making the dataset and
code publicly available, we aim to facilitate further research that will help
make scientific knowledge more accessible across language barriers.

</details>


### [212] [Disentangling and Generating Modalities for Recommendation in Missing Modality Scenarios](https://arxiv.org/abs/2504.16352)
*Jiwan Kim,Hongseok Kang,Sein Kim,Kibum Kim,Chanyoung Park*

Main category: cs.IR

TL;DR: DGMRec提出了一种新的多模态推荐系统框架，专门解决模态缺失问题，通过解耦和生成模态特征提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有MRS在模态缺失场景下表现不佳，且未充分利用模态特征的独特性。

Method: DGMRec将模态特征解耦为通用和特定特征，并生成缺失模态特征。

Result: 实验表明DGMRec在模态缺失和新物品场景中优于现有方法，并支持跨模态检索。

Conclusion: DGMRec具有实际应用潜力，尤其在模态缺失场景下表现优越。

Abstract: Multi-modal recommender systems (MRSs) have achieved notable success in
improving personalization by leveraging diverse modalities such as images,
text, and audio. However, two key challenges remain insufficiently addressed:
(1) Insufficient consideration of missing modality scenarios and (2) the
overlooking of unique characteristics of modality features. These challenges
result in significant performance degradation in realistic situations where
modalities are missing. To address these issues, we propose Disentangling and
Generating Modality Recommender (DGMRec), a novel framework tailored for
missing modality scenarios. DGMRec disentangles modality features into general
and specific modality features from an information-based perspective, enabling
richer representations for recommendation. Building on this, it generates
missing modality features by integrating aligned features from other modalities
and leveraging user modality preferences. Extensive experiments show that
DGMRec consistently outperforms state-of-the-art MRSs in challenging scenarios,
including missing modalities and new item settings as well as diverse missing
ratios and varying levels of missing modalities. Moreover, DGMRec's
generation-based approach enables cross-modal retrieval, a task inapplicable
for existing MRSs, highlighting its adaptability and potential for real-world
applications. Our code is available at https://github.com/ptkjw1997/DGMRec.

</details>


### [213] [A Survey of Foundation Model-Powered Recommender Systems: From Feature-Based, Generative to Agentic Paradigms](https://arxiv.org/abs/2504.16420)
*Chengkai Huang,Hongtao Huang,Tong Yu,Kaige Xie,Junda Wu,Shuai Zhang,Julian Mcauley,Dietmar Jannach,Lina Yao*

Main category: cs.IR

TL;DR: 该论文综述了基础模型（FMs）在推荐系统（RS）中的应用，探讨了三种集成范式：特征增强、生成式推荐和代理交互系统，并分析了其优势与挑战。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型（如GPT、LLaMA、CLIP）的兴起，推荐系统领域面临范式转变，需要系统性地研究其整合方式与潜在影响。

Method: 论文通过综述现有研究，分析基础模型在推荐系统中的三种应用范式：特征增强、生成式推荐和代理交互系统。

Result: 基础模型为推荐系统带来了新的能力（如多模态推理），但也面临数据依赖性和计算成本等挑战。

Conclusion: 论文总结了基础模型在推荐系统中的潜力与问题，并提出了未来研究方向。

Abstract: Recommender systems (RS) have become essential in filtering information and
personalizing content for users. RS techniques have traditionally relied on
modeling interactions between users and items as well as the features of
content using models specific to each task. The emergence of foundation models
(FMs), large scale models trained on vast amounts of data such as GPT, LLaMA
and CLIP, is reshaping the recommendation paradigm. This survey provides a
comprehensive overview of the Foundation Models for Recommender Systems
(FM4RecSys), covering their integration in three paradigms: (1) Feature-Based
augmentation of representations, (2) Generative recommendation approaches, and
(3) Agentic interactive systems. We first review the data foundations of RS,
from traditional explicit or implicit feedback to multimodal content sources.
We then introduce FMs and their capabilities for representation learning,
natural language understanding, and multi-modal reasoning in RS contexts. The
core of the survey discusses how FMs enhance RS under different paradigms.
Afterward, we examine FM applications in various recommendation tasks. Through
an analysis of recent research, we highlight key opportunities that have been
realized as well as challenges encountered. Finally, we outline open research
directions and technical challenges for next-generation FM4RecSys. This survey
not only reviews the state-of-the-art methods but also provides a critical
analysis of the trade-offs among the feature-based, the generative, and the
agentic paradigms, outlining key open issues and future research directions.

</details>


### [214] [MMHCL: Multi-Modal Hypergraph Contrastive Learning for Recommendation](https://arxiv.org/abs/2504.16576)
*Xu Guo,Tong Zhang,Fuyun Wang,Xudong Wang,Xiaoya Zhang,Xin Liu,Zhen Cui*

Main category: cs.IR

TL;DR: 论文提出了一种多模态超图对比学习框架（MMHCL），通过构建用户和物品的超图来挖掘共享偏好和语义相似性，缓解数据稀疏和冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 多模态内容共享平台的兴起推动了推荐系统的发展，但现有方法在数据稀疏和冷启动问题上表现不佳，且未能充分挖掘多模态数据中的语义关联。

Method: 构建用户-用户和物品-物品超图，挖掘共享偏好和语义相似性；通过对比学习增强特征区分性，融合一阶和二阶语义信息。

Result: MMHCL通过超图和对比学习有效缓解了数据稀疏和冷启动问题，实验证明了其优越性。

Conclusion: MMHCL框架通过多模态超图和对比学习显著提升了推荐系统的性能，解决了数据稀疏和冷启动问题。

Abstract: The burgeoning presence of multimodal content-sharing platforms propels the
development of personalized recommender systems. Previous works usually suffer
from data sparsity and cold-start problems, and may fail to adequately explore
semantic user-product associations from multimodal data. To address these
issues, we propose a novel Multi-Modal Hypergraph Contrastive Learning (MMHCL)
framework for user recommendation. For a comprehensive information exploration
from user-product relations, we construct two hypergraphs, i.e. a user-to-user
(u2u) hypergraph and an item-to-item (i2i) hypergraph, to mine shared
preferences among users and intricate multimodal semantic resemblance among
items, respectively. This process yields denser second-order semantics that are
fused with first-order user-item interaction as complementary to alleviate the
data sparsity issue. Then, we design a contrastive feature enhancement paradigm
by applying synergistic contrastive learning. By maximizing/minimizing the
mutual information between second-order (e.g. shared preference pattern for
users) and first-order (information of selected items for users) embeddings of
the same/different users and items, the feature distinguishability can be
effectively enhanced. Compared with using sparse primary user-item interaction
only, our MMHCL obtains denser second-order hypergraphs and excavates more
abundant shared attributes to explore the user-product associations, which to a
certain extent alleviates the problems of data sparsity and cold-start.
Extensive experiments have comprehensively demonstrated the effectiveness of
our method. Our code is publicly available at: https://github.com/Xu107/MMHCL.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [215] [HUG: Hierarchical Urban Gaussian Splatting with Block-Based Reconstruction](https://arxiv.org/abs/2504.16606)
*Zhongtao Wang,Mai Su,Huishan Au,Yilong Li,Xizhe Cao,Chengwei Pan,Yisong Chen,Guoping Wang*

Main category: cs.GR

TL;DR: HUG提出了一种基于3D高斯泼溅的层次化神经高斯表示方法，优化大规模城市场景的重建与渲染效率。


<details>
  <summary>Details</summary>
Motivation: 随着城市场景3D化需求增加，高效重建与渲染技术变得至关重要。

Method: 采用层次化神经高斯表示和增强的块状重建流程，减少冗余训练区域。

Result: 在公共基准测试中取得领先效果，证明其在大规模城市场景中的优势。

Conclusion: HUG通过层次化方法实现了高质量渲染与低计算成本，适用于复杂城市场景。

Abstract: As urban 3D scenes become increasingly complex and the demand for
high-quality rendering grows, efficient scene reconstruction and rendering
techniques become crucial. We present HUG, a novel approach to address
inefficiencies in handling large-scale urban environments and intricate details
based on 3D Gaussian splatting. Our method optimizes data partitioning and the
reconstruction pipeline by incorporating a hierarchical neural Gaussian
representation. We employ an enhanced block-based reconstruction pipeline
focusing on improving reconstruction quality within each block and reducing the
need for redundant training regions around block boundaries. By integrating
neural Gaussian representation with a hierarchical architecture, we achieve
high-quality scene rendering at a low computational cost. This is demonstrated
by our state-of-the-art results on public benchmarks, which prove the
effectiveness and advantages in large-scale urban scene representation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [216] [Introduction to Quantum Machine Learning and Quantum Architecture Search](https://arxiv.org/abs/2504.16131)
*Samuel Yen-Chi Chen,Zhiding Liang*

Main category: quant-ph

TL;DR: 量子计算与机器学习的结合推动了量子机器学习（QML）的发展，旨在通过量子原理提升ML算法性能，并探索自动化设计高性能量子电路架构的方法。


<details>
  <summary>Details</summary>
Motivation: 量子计算和机器学习的快速发展促使研究者探索两者的结合，以扩展QML的应用范围。

Method: 利用量子原理增强ML算法，并开发自动化设计量子电路架构的方法。

Result: QML在多个领域展现出潜力，为研究者提供了量子增强工具。

Conclusion: QML的突破有望进一步推动其在各领域的应用。

Abstract: Recent advancements in quantum computing (QC) and machine learning (ML) have
fueled significant research efforts aimed at integrating these two
transformative technologies. Quantum machine learning (QML), an emerging
interdisciplinary field, leverages quantum principles to enhance the
performance of ML algorithms. Concurrently, the exploration of systematic and
automated approaches for designing high-performance quantum circuit
architectures for QML tasks has gained prominence, as these methods empower
researchers outside the quantum computing domain to effectively utilize
quantum-enhanced tools. This tutorial will provide an in-depth overview of
recent breakthroughs in both areas, highlighting their potential to expand the
application landscape of QML across diverse fields.

</details>


### [217] [QAOA-GPT: Efficient Generation of Adaptive and Regular Quantum Approximate Optimization Algorithm Circuits](https://arxiv.org/abs/2504.16350)
*Ilya Tyagin,Marwa H. Farag,Kyle Sherbert,Karunya Shirali,Yuri Alexeev,Ilya Safro*

Main category: quant-ph

TL;DR: QAOA-GPT利用生成式预训练Transformer直接合成量子电路，用于解决二次无约束二进制优化问题，并在MaxCut问题上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 量子计算在解决某些经典计算机难以处理的优化问题上具有潜力，但现有方法（如QAOA）存在计算开销大和参数优化复杂的问题。

Method: 通过生成式预训练Transformer（GPT）直接合成量子电路，并使用自适应QAOA方法生成合成数据集以多样化训练电路。

Result: QAOA-GPT能够为未见过的图实例生成高质量量子电路，并成功参数化QAOA，显著降低计算开销和参数优化复杂度。

Conclusion: 生成式AI为可扩展地生成紧凑量子电路提供了新途径。

Abstract: Quantum computing has the potential to improve our ability to solve certain
optimization problems that are computationally difficult for classical
computers, by offering new algorithmic approaches that may provide speedups
under specific conditions. In this work, we introduce QAOA-GPT, a generative
framework that leverages Generative Pretrained Transformers (GPT) to directly
synthesize quantum circuits for solving quadratic unconstrained binary
optimization problems, and demonstrate it on the MaxCut problem on graphs. To
diversify the training circuits and ensure their quality, we have generated a
synthetic dataset using the adaptive QAOA approach, a method that incrementally
builds and optimizes problem-specific circuits. The experiments conducted on a
curated set of graph instances demonstrate that QAOA-GPT, generates high
quality quantum circuits for new problem instances unseen in the training as
well as successfully parametrizes QAOA. Our results show that using QAOA-GPT to
generate quantum circuits will significantly decrease both the computational
overhead of classical QAOA and adaptive approaches that often use gradient
evaluation to generate the circuit and the classical optimization of the
circuit parameters. Our work shows that generative AI could be a promising
avenue to generate compact quantum circuits in a scalable way.

</details>


### [218] [Deep Neural Network Emulation of the Quantum-Classical Transition via Learned Wigner Function Dynamics](https://arxiv.org/abs/2504.16334)
*Kamran Majid*

Main category: quant-ph

TL;DR: 论文提出了一种利用深度神经网络直接从量子态参数和普朗克常数预测维格纳函数动态映射的新方法，成功模拟了量子-经典过渡。


<details>
  <summary>Details</summary>
Motivation: 研究量子力学中普朗克常数趋近零时经典行为的涌现问题。

Method: 使用深度前馈神经网络，通过分析生成的维格纳函数数据集，学习从初始量子态参数到时间演化维格纳函数的映射。

Result: 网络训练损失约为0.0390，能够准确捕捉维格纳函数动态映射，直接模拟量子-经典过渡。

Conclusion: 该方法为研究量子力学经典涌现提供了新的计算视角，超越了以往基于可观测映射的研究。

Abstract: The emergence of classical behavior from quantum mechanics as Planck's
constant $\hbar$ approaches zero remains a fundamental challenge in physics
[1-3]. This paper introduces a novel approach employing deep neural networks to
directly learn the dynamical mapping from initial quantum state parameters (for
Gaussian wave packets of the one-dimensional harmonic oscillator) and $\hbar$
to the parameters of the time-evolved Wigner function in phase space [4-6]. A
comprehensive dataset of analytically derived time-evolved Wigner functions was
generated, and a deep feedforward neural network with an enhanced architecture
was successfully trained for this prediction task, achieving a final training
loss of ~ 0.0390. The network demonstrates a significant and previously
unrealized ability to accurately capture the underlying mapping of the Wigner
function dynamics. This allows for a direct emulation of the quantum-classical
transition by predicting the evolution of phase-space distributions as $\hbar$
is systematically varied. The implications of these findings for providing a
new computational lens on the emergence of classicality are discussed,
highlighting the potential of this direct phase-space learning approach for
studying fundamental aspects of quantum mechanics. This work presents a
significant advancement beyond previous efforts that focused on learning
observable mappings [7], offering a direct route via the phase-space
representation.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [219] [Probabilistic Emulation of the Community Radiative Transfer Model Using Machine Learning](https://arxiv.org/abs/2504.16192)
*Lucas Howard,Aneesh C. Subramanian,Gregory Thompson,Benjamin Johnson,Thomas Auligne*

Main category: physics.ao-ph

TL;DR: 利用机器学习构建了一个高效的神经网络概率模拟器，用于模拟社区辐射传输模型（CRTM），显著降低了计算成本，同时保持了高精度。


<details>
  <summary>Details</summary>
Motivation: 卫星观测数据的增加对天气预报技能提升至关重要，但辐射传输模型的计算成本高，导致大量数据未被充分利用。

Method: 采用机器学习方法构建CRTM的神经网络概率模拟器，应用于GOES Advanced Baseline Imager，预测亮度温度及其误差。

Result: 模拟器预测的亮度温度RMSE为0.3 K，晴空条件下9/10红外通道的RMSE低于0.1 K，误差预测可靠。

Conclusion: 神经网络模拟器不仅高效，还能复现相关物理过程，增强了模型对新数据的适应性和可靠性。

Abstract: The continuous improvement in weather forecast skill over the past several
decades is largely due to the increasing quantity of available satellite
observations and their assimilation into operational forecast systems.
Assimilating these observations requires observation operators in the form of
radiative transfer models. Significant efforts have been dedicated to enhancing
the computational efficiency of these models. Computational cost remains a
bottleneck, and a large fraction of available data goes unused for
assimilation. To address this, we used machine learning to build an efficient
neural network based probabilistic emulator of the Community Radiative Transfer
Model (CRTM), applied to the GOES Advanced Baseline Imager. The trained NN
emulator predicts brightness temperatures output by CRTM and the corresponding
error with respect to CRTM. RMSE of the predicted brightness temperature is 0.3
K averaged across all channels. For clear sky conditions, the RMSE is less than
0.1 K for 9 out of 10 infrared channels. The error predictions are generally
reliable across a wide range of conditions. Explainable AI methods demonstrate
that the trained emulator reproduces the relevant physics, increasing
confidence that the model will perform well when presented with new data.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [220] [Circinus: Efficient Query Planner for Compound ML Serving](https://arxiv.org/abs/2504.16397)
*Banruo Liu,Wei-Yu Lin,Minghao Fang,Yihan Jiang,Fan Lai*

Main category: cs.DB

TL;DR: Circinus是一个面向大规模复合AI工作负载的SLO感知查询规划器，通过分解多查询规划和多维度SLO目标，显著提升服务吞吐量和规划效率。


<details>
  <summary>Details</summary>
Motivation: 复合AI服务的兴起带来了高服务吞吐量的需求，但现有解决方案在实时服务和成本效益部署方面存在局限性。

Method: Circinus通过利用查询内和跨查询的计划相似性减少搜索步骤，并采用精度感知的计划分析器提高每步效率。

Result: 在真实场景中，Circinus将服务吞吐量提升3.2-5.0倍，查询规划速度加快4.2-5.8倍，同时降低部署成本3.2-4.0倍。

Conclusion: Circinus在提升复合AI服务性能和成本效益方面表现出色，优于现有技术。

Abstract: The rise of compound AI serving -- integrating multiple operators in a
pipeline that may span edge and cloud tiers -- enables end-user applications
such as autonomous driving, generative AI-powered meeting companions, and
immersive gaming. Achieving high service goodput -- i.e., meeting service level
objectives (SLOs) for pipeline latency, accuracy, and costs -- requires
effective planning of operator placement, configuration, and resource
allocation across infrastructure tiers. However, the diverse SLO requirements,
varying edge capabilities, and high query volumes create an enormous planning
search space, rendering current solutions fundamentally limited for real-time
serving and cost-efficient deployments.
  This paper presents Circinus, an SLO-aware query planner for large-scale
compound AI workloads. Circinus novelly decomposes multi-query planning and
multi-dimensional SLO objectives while preserving global decision quality. By
exploiting plan similarities within and across queries, it significantly
reduces search steps. It further improves per-step efficiency with a
precision-aware plan profiler that incrementally profiles and strategically
applies early stopping based on imprecise estimates of plan performance. At
scale, Circinus selects query-plan combinations to maximize global SLO goodput.
Evaluations in real-world settings show that Circinus improves service goodput
by 3.2-5.0$\times$, accelerates query planning by 4.2-5.8$\times$, achieving
query response in seconds, while reducing deployment costs by 3.2-4.0$\times$
over state of the arts even in their intended single-tier deployments.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [221] [BrainPrompt: Multi-Level Brain Prompt Enhancement for Neurological Condition Identification](https://arxiv.org/abs/2504.16096)
*Jiaxing Xu,Kai He,Yue Tang,Wei Li,Mengcheng Lan,Xia Dong,Yiping Ke,Mengling Feng*

Main category: q-bio.NC

TL;DR: BrainPrompt结合图神经网络和大型语言模型，通过多级知识驱动提示增强神经疾病诊断能力。


<details>
  <summary>Details</summary>
Motivation: 现有脑网络分析方法主要依赖影像数据，忽视了非影像因素，限制了模型的预测能力和可解释性。

Method: BrainPrompt整合ROI级、受试者级和疾病级知识驱动提示，结合LLMs增强多模态信息利用。

Result: 在两项fMRI数据集上表现优于现有方法，并能提取与神经科学领域知识一致的可解释信息。

Conclusion: BrainPrompt通过知识增强的多模态信息整合，提升了神经疾病诊断的预测能力和可解释性。

Abstract: Neurological conditions, such as Alzheimer's Disease, are challenging to
diagnose, particularly in the early stages where symptoms closely resemble
healthy controls. Existing brain network analysis methods primarily focus on
graph-based models that rely solely on imaging data, which may overlook
important non-imaging factors and limit the model's predictive power and
interpretability. In this paper, we present BrainPrompt, an innovative
framework that enhances Graph Neural Networks (GNNs) by integrating Large
Language Models (LLMs) with knowledge-driven prompts, enabling more effective
capture of complex, non-imaging information and external knowledge for
neurological disease identification. BrainPrompt integrates three types of
knowledge-driven prompts: (1) ROI-level prompts to encode the identity and
function of each brain region, (2) subject-level prompts that incorporate
demographic information, and (3) disease-level prompts to capture the temporal
progression of disease. By leveraging these multi-level prompts, BrainPrompt
effectively harnesses knowledge-enhanced multi-modal information from LLMs,
enhancing the model's capability to predict neurological disease stages and
meanwhile offers more interpretable results. We evaluate BrainPrompt on two
resting-state functional Magnetic Resonance Imaging (fMRI) datasets from
neurological disorders, showing its superiority over state-of-the-art methods.
Additionally, a biomarker study demonstrates the framework's ability to extract
valuable and interpretable information aligned with domain knowledge in
neuroscience.

</details>


### [222] [Application of an attention-based CNN-BiLSTM framework for in vivo two-photon calcium imaging of neuronal ensembles: decoding complex bilateral forelimb movements from unilateral M1](https://arxiv.org/abs/2504.16917)
*Ghazal Mirzaee,Jonathan Chang,Shahrzad Latifi*

Main category: q-bio.NC

TL;DR: 本文提出了一种基于注意力机制的CNN-BiLSTM混合深度学习模型，用于解码复杂前肢运动，展示了其在神经元网络活动时空依赖性分析中的高效性。


<details>
  <summary>Details</summary>
Motivation: 解码多尺度脑网络中的行为（如运动）是神经科学的核心目标，而人工智能和机器学习在揭示运动功能的神经机制中扮演重要角色。

Method: 采用注意力机制的CNN-BiLSTM混合深度学习框架，利用双光子钙成像信号解码复杂前肢运动。

Result: 研究发现，单侧M1神经元群可以准确解码同侧和对侧前肢的复杂运动。

Conclusion: 结果表明，先进的混合深度学习模型能有效捕捉与复杂运动执行相关的神经元网络活动的时空依赖性。

Abstract: Decoding behavior, such as movement, from multiscale brain networks remains a
central objective in neuroscience. Over the past decades, artificial
intelligence and machine learning have played an increasingly significant role
in elucidating the neural mechanisms underlying motor function. The advancement
of brain-monitoring technologies, capable of capturing complex neuronal signals
with high spatial and temporal resolution, necessitates the development and
application of more sophisticated machine learning models for behavioral
decoding. In this study, we employ a hybrid deep learning framework, an
attention-based CNN-BiLSTM model, to decode skilled and complex forelimb
movements using signals obtained from in vivo two-photon calcium imaging. Our
findings demonstrate that the intricate movements of both ipsilateral and
contralateral forelimbs can be accurately decoded from unilateral M1 neuronal
ensembles. These results highlight the efficacy of advanced hybrid deep
learning models in capturing the spatiotemporal dependencies of neuronal
networks activity linked to complex movement execution.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [223] [Exploring zero-shot structure-based protein fitness prediction](https://arxiv.org/abs/2504.16886)
*Arnav Sharma,Anthony Gitter*

Main category: q-bio.QM

TL;DR: 论文探讨了基于预训练机器学习模型的零样本蛋白质适应性预测能力，及其在遗传变异解释和蛋白质工程中的应用。同时研究了结构预测工具对模型性能的影响，并评估了无序区域的预测挑战。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用预训练模型和结构预测工具，提升蛋白质适应性预测的准确性，尤其是在零样本和无序区域的应用场景。

Method: 通过实验评估了多种基于结构的建模选择，并测试了无序区域的预测性能。最后在ProteinGym基准上验证了多模态集成模型的效果。

Result: 研究发现结构匹配对预测性能至关重要，无序区域的预测可能误导模型。多模态集成模型在基准测试中表现优异。

Conclusion: 零样本适应性预测模型在有序区域表现良好，但在无序区域需谨慎。结构匹配和多模态集成是提升性能的关键。

Abstract: The ability to make zero-shot predictions about the fitness consequences of
protein sequence changes with pre-trained machine learning models enables many
practical applications. Such models can be applied for downstream tasks like
genetic variant interpretation and protein engineering without additional
labeled data. The advent of capable protein structure prediction tools has led
to the availability of orders of magnitude more precomputed predicted
structures, giving rise to powerful structure-based fitness prediction models.
Through our experiments, we assess several modeling choices for structure-based
models and their effects on downstream fitness prediction. Zero-shot fitness
prediction models can struggle to assess the fitness landscape within
disordered regions of proteins, those that lack a fixed 3D structure. We
confirm the importance of matching protein structures to fitness assays and
find that predicted structures for disordered regions can be misleading and
affect predictive performance. Lastly, we evaluate an additional
structure-based model on the ProteinGym substitution benchmark and show that
simple multi-modal ensembles are strong baselines.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [224] [SOTOPIA-S4: a user-friendly system for flexible, customizable, and large-scale social simulation](https://arxiv.org/abs/2504.16122)
*Xuhui Zhou,Zhe Su,Sophie Feng,Jiaxu Zhou,Jen-tse Huang,Hsien-Te Kao,Spencer Lynch,Svitlana Volkova,Tongshuang Sherry Wu,Anita Woolley,Hao Zhu,Maarten Sap*

Main category: cs.CY

TL;DR: SOTOPIA-S4是一个快速、灵活且可扩展的社交模拟系统，用于通过LLM代理探索和验证社会科学假设，支持多轮和多方的交互，并提供可定制的评估指标。


<details>
  <summary>Details</summary>
Motivation: 解决当前社交模拟框架的技术障碍，为研究人员提供无需编程的模拟工具，支持社会科学研究和LLM代理行为分析。

Method: 开发了包含模拟引擎、API服务器和Web界面的pip包，支持多轮和多方的LLM交互，并提供灵活的RESTful API和用户友好的界面。

Result: 通过招聘谈判和多方规划两个案例展示了系统的实用性。

Conclusion: SOTOPIA-S4是一个高效的工具，适用于社会科学研究和LLM代理行为模拟，降低了技术门槛。

Abstract: Social simulation through large language model (LLM) agents is a promising
approach to explore and validate hypotheses related to social science questions
and LLM agents behavior. We present SOTOPIA-S4, a fast, flexible, and scalable
social simulation system that addresses the technical barriers of current
frameworks while enabling practitioners to generate multi-turn and multi-party
LLM-based interactions with customizable evaluation metrics for hypothesis
testing. SOTOPIA-S4 comes as a pip package that contains a simulation engine,
an API server with flexible RESTful APIs for simulation management, and a web
interface that enables both technical and non-technical users to design, run,
and analyze simulations without programming. We demonstrate the usefulness of
SOTOPIA-S4 with two use cases involving dyadic hiring negotiation and
multi-party planning scenarios.

</details>


### [225] [Efficacy of a Computer Tutor that Models Expert Human Tutors](https://arxiv.org/abs/2504.16132)
*Andrew M. Olney,Sidney K. D'Mello,Natalie Person,Whitney Cade,Patrick Hays,Claire W. Dempsey,Blair Lehman,Betsy Williams,Art Graesser*

Main category: cs.CY

TL;DR: 研究表明，智能辅导系统（ITS）和人类专家导师在即时和延迟测试中均显著提升学习效果，但导师的专业知识对辅导效果的影响仍需进一步探讨。


<details>
  <summary>Details</summary>
Motivation: 探讨专业知识对辅导效果的影响，并比较智能辅导系统与人类导师的效果。

Method: 进行为期9周的生物学学习效果研究，比较智能辅导系统、人类专家导师和无辅导条件的效果，使用逻辑混合效应模型分析数据。

Result: 智能辅导系统和人类导师在即时测试（d=.71和d=.66）和延迟测试（d=.36和d=.39）中均表现出显著的正向效果。

Conclusion: 专业知识对辅导效果的影响仍需深入研究，未来研究应进一步探讨辅导设计的优化。

Abstract: Tutoring is highly effective for promoting learning. However, the
contribution of expertise to tutoring effectiveness is unclear and continues to
be debated. We conducted a 9-week learning efficacy study of an intelligent
tutoring system (ITS) for biology modeled on expert human tutors with two
control conditions: human tutors who were experts in the domain but not in
tutoring and a no-tutoring condition. All conditions were supplemental to
classroom instruction, and students took learning tests immediately before and
after tutoring sessions as well as delayed tests 1-2 weeks later. Analysis
using logistic mixed-effects modeling indicates significant positive effects on
the immediate post-test for the ITS (d =.71) and human tutors (d =.66) which
are in the 99th percentile of meta-analytic effects, as well as significant
positive effects on the delayed post-test for the ITS (d =.36) and human tutors
(d =.39). We discuss implications for the role of expertise in tutoring and the
design of future studies.

</details>


### [226] [A Conceptual Framework for AI-based Decision Systems in Critical Infrastructures](https://arxiv.org/abs/2504.16133)
*Milad Leyli-abadi,Ricardo J. Bessa,Jan Viebahn,Daniel Boos,Clark Borst,Alberto Castagna,Ricardo Chavarriaga,Mohamed Hassouna,Bruno Lemetayer,Giulia Leto,Antoine Marot,Maroua Meddeb,Manuel Meyer,Viola Schiaffonati,Manuel Schneider,Toni Waefler*

Main category: cs.CY

TL;DR: 本文提出了一种综合性的概念框架，用于解决人类与AI在安全关键系统中的交互挑战，整合了多学科知识并展示了其灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有框架未能完全解决人类与AI在安全关键系统中的交互挑战，尤其是在透明度、信任、可解释性以及安全决策方面的需求。

Method: 采用跨学科方法，整合数学、决策理论、计算机科学、哲学、心理学和认知工程等领域，并结合能源、交通和航空等专业工程领域。

Result: 提出了一个全面的概念框架，并通过在现有框架上的实例化展示了其灵活性。

Conclusion: 该框架为设计和部署安全有效的系统提供了新的思路，填补了现有研究的空白。

Abstract: The interaction between humans and AI in safety-critical systems presents a
unique set of challenges that remain partially addressed by existing
frameworks. These challenges stem from the complex interplay of requirements
for transparency, trust, and explainability, coupled with the necessity for
robust and safe decision-making. A framework that holistically integrates human
and AI capabilities while addressing these concerns is notably required,
bridging the critical gaps in designing, deploying, and maintaining safe and
effective systems. This paper proposes a holistic conceptual framework for
critical infrastructures by adopting an interdisciplinary approach. It
integrates traditionally distinct fields such as mathematics, decision theory,
computer science, philosophy, psychology, and cognitive engineering and draws
on specialized engineering domains, particularly energy, mobility, and
aeronautics. The flexibility in its adoption is also demonstrated through its
instantiation on an already existing framework.

</details>


### [227] [Trends in Frontier AI Model Count: A Forecast to 2028](https://arxiv.org/abs/2504.16138)
*Iyngkarran Kumar,Sam Manning*

Main category: cs.CY

TL;DR: 论文探讨了基于训练计算量的AI模型监管阈值对模型数量的影响，预测未来几年超过阈值的模型数量将超线性增长。


<details>
  <summary>Details</summary>
Motivation: 研究政府基于训练计算量对AI模型施加监管要求的趋势，并预测未来几年可能受影响的模型数量。

Method: 通过分析现有监管框架（如欧盟AI法案和美国AI Diffusion Framework）的计算量阈值，预测未来几年超过这些阈值的模型数量。

Result: 预测到2028年，超过欧盟$10^{25}$ FLOP阈值的模型将有103-306个，超过美国$10^{26}$ FLOP阈值的模型将有45-148个。

Conclusion: 基于绝对计算量的监管阈值将导致受影响的模型数量超线性增长，而基于相对计算量的阈值则趋势更稳定。

Abstract: Governments are starting to impose requirements on AI models based on how
much compute was used to train them. For example, the EU AI Act imposes
requirements on providers of general-purpose AI with systemic risk, which
includes systems trained using greater than $10^{25}$ floating point operations
(FLOP). In the United States' AI Diffusion Framework, a training compute
threshold of $10^{26}$ FLOP is used to identify "controlled models" which face
a number of requirements. We explore how many models such training compute
thresholds will capture over time. We estimate that by the end of 2028, there
will be between 103-306 foundation models exceeding the $10^{25}$ FLOP
threshold put forward in the EU AI Act (90% CI), and 45-148 models exceeding
the $10^{26}$ FLOP threshold that defines controlled models in the AI Diffusion
Framework (90% CI). We also find that the number of models exceeding these
absolute compute thresholds each year will increase superlinearly -- that is,
each successive year will see more new models captured within the threshold
than the year before. Thresholds that are defined with respect to the largest
training run to date (for example, such that all models within one order of
magnitude of the largest training run to date are captured by the threshold)
see a more stable trend, with a median forecast of 14-16 models being captured
by this definition annually from 2025-2028.

</details>


### [228] [Enhancing Trust Through Standards: A Comparative Risk-Impact Framework for Aligning ISO AI Standards with Global Ethical and Regulatory Contexts](https://arxiv.org/abs/2504.16139)
*Sridharan Sankaran*

Main category: cs.CY

TL;DR: 论文提出了一种比较风险影响评估框架，用于评估ISO AI标准在不同监管环境中的有效性，并提出了改进建议以增强其全球适用性。


<details>
  <summary>Details</summary>
Motivation: 随着AI重塑行业和社会，确保其可信赖性（如减少偏见、不透明性和问责缺失等伦理风险）是全球性挑战。ISO AI标准旨在通过嵌入公平性、透明性和风险管理来促进负责任的AI发展，但其效果因监管环境不同而异。

Method: 论文引入了一种比较风险影响评估框架，通过将ISO标准映射到欧盟AI法案，并调查十个地区的监管框架，建立伦理对齐基准。框架应用于欧盟、美国科罗拉多州和中国的案例研究。

Result: 研究发现，自愿性ISO标准在执法方面存在不足（如科罗拉多州），并低估了地区特定风险（如中国的隐私问题）。建议包括强制风险审计、地区特定附录和隐私模块。

Conclusion: 该框架为全球AI标准化与伦理要求的对齐提供了可复制的工具，有助于提升互操作性和信任。政策制定者和标准机构可利用这些见解推动AI治理的发展。

Abstract: As artificial intelligence (AI) reshapes industries and societies, ensuring
its trustworthiness-through mitigating ethical risks like bias, opacity, and
accountability deficits-remains a global challenge. International Organization
for Standardization (ISO) AI standards, such as ISO/IEC 24027 and 24368, aim to
foster responsible development by embedding fairness, transparency, and risk
management into AI systems. However, their effectiveness varies across diverse
regulatory landscapes, from the EU's risk-based AI Act to China's
stability-focused measures and the U.S.'s fragmented state-led initiatives.
This paper introduces a novel Comparative Risk-Impact Assessment Framework to
evaluate how well ISO standards address ethical risks within these contexts,
proposing enhancements to strengthen their global applicability. By mapping ISO
standards to the EU AI Act and surveying regulatory frameworks in ten
regions-including the UK, Canada, India, Japan, Singapore, South Korea, and
Brazil-we establish a baseline for ethical alignment. The framework, applied to
case studies in the EU, US-Colorado, and China, reveals gaps: voluntary ISO
standards falter in enforcement (e.g., Colorado) and undervalue region-specific
risks like privacy (China). We recommend mandatory risk audits, region-specific
annexes, and a privacy-focused module to enhance ISO's adaptability. This
approach not only synthesizes global trends but also offers a replicable tool
for aligning standardization with ethical imperatives, fostering
interoperability and trust in AI worldwide. Policymakers and standards bodies
can leverage these insights to evolve AI governance, ensuring it meets diverse
societal needs as the technology advances.

</details>


### [229] [Cooperative Speech, Semantic Competence, and AI](https://arxiv.org/abs/2504.16092)
*Mahrad Almotahari*

Main category: cs.CY

TL;DR: 论文探讨了合作性言语的道德基础，认为大型语言模型（LLMs）缺乏作为合作对话者的道德地位，因此无法进行真正的断言。


<details>
  <summary>Details</summary>
Motivation: 研究合作性言语的道德维度，特别是尊重在对话中的作用，以及LLMs是否具备这种道德地位。

Method: 通过分析合作性言语的定义和道德要求，论证LLMs缺乏作为合作对话者的资格。

Result: LLMs不具备合作对话所需的道德地位，因此无法进行断言，这对其语义能力提出了质疑。

Conclusion: 语义知识不仅是认知心理学的研究对象，也涉及道德心理学，LLMs的局限性揭示了这一点。

Abstract: Cooperative speech is purposive. From the speaker's perspective, one crucial
purpose is the transmission of knowledge. Cooperative speakers care about
getting things right for their conversational partners. This attitude is a kind
of respect. Cooperative speech is an ideal form of communication because
participants have respect for each other. And having respect within a
cooperative enterprise is sufficient for a particular kind of moral standing:
we ought to respect those who have respect for us. Respect demands reciprocity.
I maintain that large language models aren't owed the kind of respect that
partly constitutes a cooperative conversation. This implies that they aren't
cooperative interlocutors, otherwise we would be obliged to reciprocate the
attitude. Leveraging this conclusion, I argue that present-day LLMs are
incapable of assertion and that this raises an overlooked doubt about their
semantic competence. One upshot of this argument is that knowledge of meaning
isn't just a subject for the cognitive psychologist. It's also a subject for
the moral psychologist.

</details>


### [230] [Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room](https://arxiv.org/abs/2504.16148)
*Danial Hooshyar,Gustav Šír,Yeongwook Yang,Eve Kikas,Raija Hämäläinen,Tommi Kärkkäinen,Dragan Gašević,Roger Azevedo*

Main category: cs.CY

TL;DR: 论文分析了AI在教育领域的九个未解决关键问题，并提出神经符号AI作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在教育领域取得进展，但仍存在公平性、透明度和有效性等问题，需要系统性解决。

Method: 通过理论和实证研究，识别并分析九个主要挑战，并提出神经符号AI作为应对方法。

Result: 研究表明，神经符号AI能有效解决当前AI在教育中的问题，支持负责任和可信赖的AI系统。

Conclusion: 神经符号AI为解决教育中AI的关键问题提供了可行路径，是未来发展的基础。

Abstract: Despite significant advancements in AI-driven educational systems and ongoing
calls for responsible AI for education, several critical issues remain
unresolved -- acting as the elephant in the room within AI in education,
learning analytics, educational data mining, learning sciences, and educational
psychology communities. This critical analysis identifies and examines nine
persistent challenges that continue to undermine the fairness, transparency,
and effectiveness of current AI methods and applications in education. These
include: (1) the lack of clarity around what AI for education truly means --
often ignoring the distinct purposes, strengths, and limitations of different
AI families -- and the trend of equating it with domain-agnostic,
company-driven large language models; (2) the widespread neglect of essential
learning processes such as motivation, emotion, and (meta)cognition in
AI-driven learner modelling and their contextual nature; (3) limited
integration of domain knowledge and lack of stakeholder involvement in AI
design and development; (4) continued use of non-sequential machine learning
models on temporal educational data; (5) misuse of non-sequential metrics to
evaluate sequential models; (6) use of unreliable explainable AI methods to
provide explanations for black-box models; (7) ignoring ethical guidelines in
addressing data inconsistencies during model training; (8) use of mainstream AI
methods for pattern discovery and learning analytics without systematic
benchmarking; and (9) overemphasis on global prescriptions while overlooking
localised, student-specific recommendations. Supported by theoretical and
empirical research, we demonstrate how hybrid AI methods -- specifically
neural-symbolic AI -- can address the elephant in the room and serve as the
foundation for responsible, trustworthy AI systems in education.

</details>


### [231] [Reflexive Prompt Engineering: A Framework for Responsible Prompt Engineering and Interaction Design](https://arxiv.org/abs/2504.16204)
*Christian Djeffal*

Main category: cs.CY

TL;DR: 论文探讨了负责任提示工程的重要性，提出一个包含五个组件的框架，以在生成式AI中嵌入伦理和法律考量。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的普及，提示工程对社会公平、问责和透明性有深远影响，需超越技术优化，融入伦理和社会价值。

Method: 提出一个包含提示设计、系统选择、系统配置、性能评估和提示管理的框架，结合实证证据分析其应用。

Result: 研究表明，有效的提示工程需平衡技术精确性与伦理意识，通过实际案例展示了其作为AI开发与部署桥梁的作用。

Conclusion: 负责任提示工程是AI伦理实现的关键，未来研究需进一步探索其应用和指南。

Abstract: Responsible prompt engineering has emerged as a critical framework for
ensuring that generative artificial intelligence (AI) systems serve society's
needs while minimizing potential harms. As generative AI applications become
increasingly powerful and ubiquitous, the way we instruct and interact with
them through prompts has profound implications for fairness, accountability,
and transparency. This article examines how strategic prompt engineering can
embed ethical and legal considerations and societal values directly into AI
interactions, moving beyond mere technical optimization for functionality. This
article proposes a comprehensive framework for responsible prompt engineering
that encompasses five interconnected components: prompt design, system
selection, system configuration, performance evaluation, and prompt management.
Drawing from empirical evidence, the paper demonstrates how each component can
be leveraged to promote improved societal outcomes while mitigating potential
risks. The analysis reveals that effective prompt engineering requires a
delicate balance between technical precision and ethical consciousness,
combining the systematic rigor and focus on functionality with the nuanced
understanding of social impact. Through examination of real-world and emerging
practices, the article illustrates how responsible prompt engineering serves as
a crucial bridge between AI development and deployment, enabling organizations
to fine-tune AI outputs without modifying underlying model architectures. This
approach aligns with broader "Responsibility by Design" principles, embedding
ethical considerations directly into the implementation process rather than
treating them as post-hoc additions. The article concludes by identifying key
research directions and practical guidelines for advancing the field of
responsible prompt engineering.

</details>


### [232] [Leveraging Social Media Analytics for Sustainability Trend Detection in Saudi Arabias Evolving Market](https://arxiv.org/abs/2504.16153)
*Kanwal Aalijah*

Main category: cs.CY

TL;DR: 本文探讨了如何利用AI和社交媒体分析实时追踪沙特阿拉伯在《2030愿景》下的经济和社会趋势，重点关注可持续发展领域。


<details>
  <summary>Details</summary>
Motivation: 沙特阿拉伯的快速经济增长和社会变革为实时追踪新兴趋势提供了独特机会，这为商业和投资开辟了新途径。

Method: 采用AI驱动的方法，处理数百万条社交媒体帖子、新闻和博客，以识别和监测可持续发展趋势。

Result: 研究提出了一种AI方法，为经济学家、企业和政府提供了可靠、实时的市场趋势分析，支持决策制定。

Conclusion: 该方法不仅适用于沙特阿拉伯，还可推广至其他地区，展示了AI在理解公众对倡议的接受度和趋势增长方面的潜力。

Abstract: Saudi Arabias rapid economic growth and social evolution under Vision 2030
present a unique opportunity to track emerging trends in real time. Uncovering
trends in real time can open up new avenues for business and investment
opportunities. This paper explores how AI and social media analytics can
uncover and monitor these trends across sectors like sustainability,
construction, food beverages industry, tourism, technology, and entertainment.
This paper focus on use of AI-driven methodology to identify sustainability
trends across Saudi Arabia. We processed millions of social media posts, news,
blogs in order to understand sustainability trends in the region. The paper
presents an AI approach that can help economists, businesses, government to
understand sustainability trends and make better decisions around them. This
approach offers both sector-specific and cross-sector insights, giving
decision-makers a reliable, up to date snapshot of Saudi Arabias market shifts.
Beyond Saudi Arabia, this framework also shows potential for adapting to other
regions. Overall, our findings highlight how by using AI-methodologies, give
decision makers a reliable method to understand how initiatives are perceived
and adopted by the public and understand growth of trends.

</details>


### [233] [Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark](https://arxiv.org/abs/2504.16137)
*Jasper Götting,Pedro Medeiros,Jon G Sanders,Nathaniel Li,Long Phan,Karam Elabd,Lennart Justen,Dan Hendrycks,Seth Donoughe*

Main category: cs.CY

TL;DR: VCT是一个评估大语言模型在复杂病毒学实验室协议中故障排除能力的基准测试，结果显示OpenAI的o3模型在专家子领域表现优于94%的专家病毒学家，引发了对双用途技术治理的讨论。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在病毒学实验室实践中的能力，并探讨其双用途潜在风险。

Method: 构建了包含322个多模态问题的VCT基准测试，涵盖基础、隐性及视觉知识，由数十位博士级专家病毒学家提供输入。

Result: 专家病毒学家在其子领域平均得分22.1%，而OpenAI的o3模型达到43.8%准确率，显著优于专家。

Conclusion: LLM在病毒学故障排除中的专家级能力需纳入现有双用途技术治理框架。

Abstract: We present the Virology Capabilities Test (VCT), a large language model (LLM)
benchmark that measures the capability to troubleshoot complex virology
laboratory protocols. Constructed from the inputs of dozens of PhD-level expert
virologists, VCT consists of $322$ multimodal questions covering fundamental,
tacit, and visual knowledge that is essential for practical work in virology
laboratories. VCT is difficult: expert virologists with access to the internet
score an average of $22.1\%$ on questions specifically in their sub-areas of
expertise. However, the most performant LLM, OpenAI's o3, reaches $43.8\%$
accuracy, outperforming $94\%$ of expert virologists even within their
sub-areas of specialization. The ability to provide expert-level virology
troubleshooting is inherently dual-use: it is useful for beneficial research,
but it can also be misused. Therefore, the fact that publicly available models
outperform virologists on VCT raises pressing governance considerations. We
propose that the capability of LLMs to provide expert-level troubleshooting of
dual-use virology work should be integrated into existing frameworks for
handling dual-use technologies in the life sciences.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [234] [ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree and Visual Guidance](https://arxiv.org/abs/2504.16464)
*Ying Li,Xiaobao Wei,Xiaowei Chi,Yuming Li,Zhongyu Zhao,Hao Wang,Ningning Ma,Ming Lu,Shanghang Zhang*

Main category: cs.RO

TL;DR: ManipDreamer通过动作树和视觉引导提升机器人操作视频合成的指令跟随能力和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RoboDreamer未考虑指令原语间的关系且忽视视觉引导，导致指令跟随和视觉质量不足。

Method: 提出动作树表示指令并分配嵌入，结合深度和语义引导适配器增强世界模型。

Result: 在视频质量指标（PSNR、SSIM、Flow Error）和任务成功率上显著优于RoboDreamer。

Conclusion: ManipDreamer通过动作树和视觉引导显著提升了机器人操作视频合成的性能。

Abstract: While recent advancements in robotic manipulation video synthesis have shown
promise, significant challenges persist in ensuring effective
instruction-following and achieving high visual quality. Recent methods, like
RoboDreamer, utilize linguistic decomposition to divide instructions into
separate lower-level primitives, conditioning the world model on these
primitives to achieve compositional instruction-following. However, these
separate primitives do not consider the relationships that exist between them.
Furthermore, recent methods neglect valuable visual guidance, including depth
and semantic guidance, both crucial for enhancing visual quality. This paper
introduces ManipDreamer, an advanced world model based on the action tree and
visual guidance. To better learn the relationships between instruction
primitives, we represent the instruction as the action tree and assign
embeddings to tree nodes, each instruction can acquire its embeddings by
navigating through the action tree. The instruction embeddings can be used to
guide the world model. To enhance visual quality, we combine depth and semantic
guidance by introducing a visual guidance adapter compatible with the world
model. This visual adapter enhances both the temporal and physical consistency
of video generation. Based on the action tree and visual guidance, ManipDreamer
significantly boosts the instruction-following ability and visual quality.
Comprehensive evaluations on robotic manipulation benchmarks reveal that
ManipDreamer achieves large improvements in video quality metrics in both seen
and unseen tasks, with PSNR improved from 19.55 to 21.05, SSIM improved from
0.7474 to 0.7982 and reduced Flow Error from 3.506 to 3.201 in unseen tasks,
compared to the recent RoboDreamer model. Additionally, our method increases
the success rate of robotic manipulation tasks by 2.5% in 6 RLbench tasks on
average.

</details>


### [235] [PCF-Grasp: Converting Point Completion to Geometry Feature to Enhance 6-DoF Grasp](https://arxiv.org/abs/2504.16320)
*Yaofeng Cheng,Fusheng Zha,Wei Guo,Pengfei Wang,Chao Zeng,Lining Sun,Chenguang Yang*

Main category: cs.RO

TL;DR: 提出了一种基于点云补全的6自由度抓取框架，通过将补全结果作为形状特征训练网络，并结合评分过滤器提升抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于单视角点云的抓取方法因信息不完整导致抓取精度低，受人类利用几何经验估计物体形状启发，提出新方法。

Method: 将点云补全结果作为形状特征训练6自由度抓取网络，并集成评分过滤器筛选可执行抓取方案。

Result: 实验表明，该方法生成更准确的抓取方案，真实实验中成功率比现有方法高17.8%。

Conclusion: 通过点云补全和评分过滤器的结合，显著提升了机器人抓取的准确性和可靠性。

Abstract: The 6-Degree of Freedom (DoF) grasp method based on point clouds has shown
significant potential in enabling robots to grasp target objects. However, most
existing methods are based on the point clouds (2.5D points) generated from
single-view depth images. These point clouds only have one surface side of the
object providing incomplete geometry information, which mislead the grasping
algorithm to judge the shape of the target object, resulting in low grasping
accuracy. Humans can accurately grasp objects from a single view by leveraging
their geometry experience to estimate object shapes. Inspired by humans, we
propose a novel 6-DoF grasping framework that converts the point completion
results as object shape features to train the 6-DoF grasp network. Here, point
completion can generate approximate complete points from the 2.5D points
similar to the human geometry experience, and converting it as shape features
is the way to utilize it to improve grasp efficiency. Furthermore, due to the
gap between the network generation and actual execution, we integrate a score
filter into our framework to select more executable grasp proposals for the
real robot. This enables our method to maintain a high grasp quality in any
camera viewpoint. Extensive experiments demonstrate that utilizing complete
point features enables the generation of significantly more accurate grasp
proposals and the inclusion of a score filter greatly enhances the credibility
of real-world robot grasping. Our method achieves a 17.8\% success rate higher
than the state-of-the-art method in real-world experiments.

</details>


### [236] [Offline Robotic World Model: Learning Robotic Policies without a Physics Simulator](https://arxiv.org/abs/2504.16680)
*Chenhao Li,Andreas Krause,Marco Hutter*

Main category: cs.RO

TL;DR: RWM-O是一种基于模型的离线强化学习方法，通过显式估计认知不确定性来提升策略学习，无需依赖物理模拟器。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在机器人控制中存在分布偏移问题，而现有基于模型的方法缺乏鲁棒的不确定性估计，导致误差累积。

Method: 提出RWM-O方法，通过估计认知不确定性并将其整合到策略优化中，减少对模型错误的过拟合并提升稳定性。

Result: 实验表明，RWM-O能提升策略的泛化能力和安全性，仅需真实世界数据即可学习。

Conclusion: RWM-O为机器人领域提供了高效、可扩展的强化学习解决方案。

Abstract: Reinforcement Learning (RL) has demonstrated impressive capabilities in
robotic control but remains challenging due to high sample complexity, safety
concerns, and the sim-to-real gap. While offline RL eliminates the need for
risky real-world exploration by learning from pre-collected data, it suffers
from distributional shift, limiting policy generalization. Model-Based RL
(MBRL) addresses this by leveraging predictive models for synthetic rollouts,
yet existing approaches often lack robust uncertainty estimation, leading to
compounding errors in offline settings. We introduce Offline Robotic World
Model (RWM-O), a model-based approach that explicitly estimates epistemic
uncertainty to improve policy learning without reliance on a physics simulator.
By integrating these uncertainty estimates into policy optimization, our
approach penalizes unreliable transitions, reducing overfitting to model errors
and enhancing stability. Experimental results show that RWM-O improves
generalization and safety, enabling policy learning purely from real-world data
and advancing scalable, data-efficient RL for robotics.

</details>


### [237] [HERB: Human-augmented Efficient Reinforcement learning for Bin-packing](https://arxiv.org/abs/2504.16595)
*Gojko Perovic,Nuno Ferreira Duarte,Atabak Dehban,Gonçalo Teixeira,Egidio Falotico,José Santos-Victor*

Main category: cs.RO

TL;DR: HERB是一个结合人类演示和强化学习的框架，用于高效打包不规则3D物体，优于传统几何优化和纯强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统打包方法难以处理不规则3D物体的形状和稳定性问题，而纯强化学习方法训练效率低且计算成本高。

Method: 通过人类演示学习最佳打包顺序，结合视觉信息训练放置算法，优化空间利用和稳定性。

Result: 实验表明HERB在打包效率和适应性上优于几何和纯强化学习方法。

Conclusion: 结合人类直觉的强化学习框架能有效解决复杂打包问题，具有实际应用潜力。

Abstract: Packing objects efficiently is a fundamental problem in logistics, warehouse
automation, and robotics. While traditional packing solutions focus on
geometric optimization, packing irregular, 3D objects presents significant
challenges due to variations in shape and stability. Reinforcement
Learning~(RL) has gained popularity in robotic packing tasks, but training
purely from simulation can be inefficient and computationally expensive. In
this work, we propose HERB, a human-augmented RL framework for packing
irregular objects. We first leverage human demonstrations to learn the best
sequence of objects to pack, incorporating latent factors such as space
optimization, stability, and object relationships that are difficult to model
explicitly. Next, we train a placement algorithm that uses visual information
to determine the optimal object positioning inside a packing container. Our
approach is validated through extensive performance evaluations, analyzing both
packing efficiency and latency. Finally, we demonstrate the real-world
feasibility of our method on a robotic system. Experimental results show that
our method outperforms geometric and purely RL-based approaches by leveraging
human intuition, improving both packing robustness and adaptability. This work
highlights the potential of combining human expertise-driven RL to tackle
complex real-world packing challenges in robotic systems.

</details>


### [238] [MOSAIC: A Skill-Centric Algorithmic Framework for Long-Horizon Manipulation Planning](https://arxiv.org/abs/2504.16738)
*Itamar Mishani,Yorai Shaoul,Maxim Likhachev*

Main category: cs.RO

TL;DR: MOSAIC是一个技能导向的框架，通过技能本身指导规划过程，解决机器人长期运动规划问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在技能组合探索、通用技能利用和符号世界表示依赖方面存在不足，需要统一解决方案以实现复杂任务的鲁棒性和可扩展性。

Method: MOSAIC使用两类技能：生成器计算可执行轨迹和世界配置，连接器通过解决边界值问题链接技能轨迹。

Result: MOSAIC在模拟和真实机器人操作任务中成功解决了复杂的长期规划问题。

Conclusion: MOSAIC通过技能导向的规划方法，为复杂长期任务提供了高效解决方案。

Abstract: Planning long-horizon motions using a set of predefined skills is a key
challenge in robotics and AI. Addressing this challenge requires methods that
systematically explore skill combinations to uncover task-solving sequences,
harness generic, easy-to-learn skills (e.g., pushing, grasping) to generalize
across unseen tasks, and bypass reliance on symbolic world representations that
demand extensive domain and task-specific knowledge. Despite significant
progress, these elements remain largely disjoint in existing approaches,
leaving a critical gap in achieving robust, scalable solutions for complex,
long-horizon problems. In this work, we present MOSAIC, a skill-centric
framework that unifies these elements by using the skills themselves to guide
the planning process. MOSAIC uses two families of skills: Generators compute
executable trajectories and world configurations, and Connectors link these
independently generated skill trajectories by solving boundary value problems,
enabling progress toward completing the overall task. By breaking away from the
conventional paradigm of incrementally discovering skills from predefined start
or goal states--a limitation that significantly restricts exploration--MOSAIC
focuses planning efforts on regions where skills are inherently effective. We
demonstrate the efficacy of MOSAIC in both simulated and real-world robotic
manipulation tasks, showcasing its ability to solve complex long-horizon
planning problems using a diverse set of skills incorporating generative
diffusion models, motion planning algorithms, and manipulation-specific models.
Visit https://skill-mosaic.github.io for demonstrations and examples.

</details>


### [239] [Meta-Learning Online Dynamics Model Adaptation in Off-Road Autonomous Driving](https://arxiv.org/abs/2504.16923)
*Jacob Levy,Jason Gibson,Bogdan Vlahov,Erica Tevere,Evangelos Theodorou,David Fridovich-Keil,Patrick Spieler*

Main category: cs.RO

TL;DR: 论文提出了一种结合卡尔曼滤波在线适应与元学习参数的新框架，用于高速越野自动驾驶中动态模型的实时调整，提升了预测精度和安全性。


<details>
  <summary>Details</summary>
Motivation: 高速越野自动驾驶面临复杂多变的地形和难以建模的地形-车辆交互问题，现有动态模型难以泛化到未知地形，需实时适应。

Method: 通过离线元学习优化适应参数和基函数，结合卡尔曼滤波在线动态调整动态模型。

Result: 实验表明，该方法在预测精度、性能和安全性上优于基线方法，尤其在安全关键场景中表现突出。

Conclusion: 元学习动态模型适应方法有效，推动了自动驾驶系统在多样化未知环境中的可靠导航能力。

Abstract: High-speed off-road autonomous driving presents unique challenges due to
complex, evolving terrain characteristics and the difficulty of accurately
modeling terrain-vehicle interactions. While dynamics models used in
model-based control can be learned from real-world data, they often struggle to
generalize to unseen terrain, making real-time adaptation essential. We propose
a novel framework that combines a Kalman filter-based online adaptation scheme
with meta-learned parameters to address these challenges. Offline meta-learning
optimizes the basis functions along which adaptation occurs, as well as the
adaptation parameters, while online adaptation dynamically adjusts the onboard
dynamics model in real time for model-based control. We validate our approach
through extensive experiments, including real-world testing on a full-scale
autonomous off-road vehicle, demonstrating that our method outperforms baseline
approaches in prediction accuracy, performance, and safety metrics,
particularly in safety-critical scenarios. Our results underscore the
effectiveness of meta-learned dynamics model adaptation, advancing the
development of reliable autonomous systems capable of navigating diverse and
unseen environments. Video is available at: https://youtu.be/cCKHHrDRQEA

</details>


### [240] [Latent Diffusion Planning for Imitation Learning](https://arxiv.org/abs/2504.16925)
*Amber Xie,Oleh Rybkin,Dorsa Sadigh,Chelsea Finn*

Main category: cs.RO

TL;DR: 提出了一种名为Latent Diffusion Planning (LDP)的模块化方法，通过利用无动作演示和次优数据，在模拟视觉机器人任务中表现优于现有模仿学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法依赖大量专家演示，限制了其应用范围。LDP旨在通过利用次优和无动作数据来解决这一问题。

Method: LDP包括一个规划器和一个逆动力学模型，两者在学习的潜在空间中操作。首先通过变分自编码器学习紧凑潜在空间，然后使用扩散目标训练规划器和逆动力学模型。

Result: 在模拟视觉机器人操作任务中，LDP优于现有模仿学习方法，因其能利用额外数据。

Conclusion: LDP通过模块化设计和潜在空间学习，有效利用了次优和无动作数据，提升了模仿学习的性能。

Abstract: Recent progress in imitation learning has been enabled by policy
architectures that scale to complex visuomotor tasks, multimodal distributions,
and large datasets. However, these methods often rely on learning from large
amount of expert demonstrations. To address these shortcomings, we propose
Latent Diffusion Planning (LDP), a modular approach consisting of a planner
which can leverage action-free demonstrations, and an inverse dynamics model
which can leverage suboptimal data, that both operate over a learned latent
space. First, we learn a compact latent space through a variational
autoencoder, enabling effective forecasting of future states in image-based
domains. Then, we train a planner and an inverse dynamics model with diffusion
objectives. By separating planning from action prediction, LDP can benefit from
the denser supervision signals of suboptimal and action-free data. On simulated
visual robotic manipulation tasks, LDP outperforms state-of-the-art imitation
learning approaches, as they cannot leverage such additional data.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [241] [Efficient Portfolio Selection through Preference Aggregation with Quicksort and the Bradley--Terry Model](https://arxiv.org/abs/2504.16093)
*Yurun Ge,Lucas Böttcher,Tom Chou,Maria R. D'Orsogna*

Main category: q-fin.PM

TL;DR: 论文提出了一种基于Quicksort和Bradley-Terry模型的比较规则，用于在不确定性下分配资源以最大化长期效益。


<details>
  <summary>Details</summary>
Motivation: 解决在不确定性下如何分配有限资源以最大化长期效益的问题，例如创新项目评估、研究资金分配和参与式预算。

Method: 采用Quicksort和Bradley-Terry模型，通过代理对项目的成对比较概率进行聚合和排名。

Result: 提出的方法优于现有最有效的两种聚合方法，并能结合采样技术减少比较次数。

Conclusion: Bradley-Terry模型在项目组合选择中具有实际应用潜力，能够高效地优化资源分配。

Abstract: How to allocate limited resources to projects that will yield the greatest
long-term benefits is a problem that often arises in decision-making under
uncertainty. For example, organizations may need to evaluate and select
innovation projects with risky returns. Similarly, when allocating resources to
research projects, funding agencies are tasked with identifying the most
promising proposals based on idiosyncratic criteria. Finally, in participatory
budgeting, a local community may need to select a subset of public projects to
fund. Regardless of context, agents must estimate the uncertain values of a
potentially large number of projects. Developing parsimonious methods to
compare these projects, and aggregating agent evaluations so that the overall
benefit is maximized, are critical in assembling the best project portfolio.
Unlike in standard sorting algorithms, evaluating projects on the basis of
uncertain long-term benefits introduces additional complexities. We propose
comparison rules based on Quicksort and the Bradley--Terry model, which
connects rankings to pairwise "win" probabilities. In our model, each agent
determines win probabilities of a pair of projects based on his or her specific
evaluation of the projects' long-term benefit. The win probabilities are then
appropriately aggregated and used to rank projects. Several of the methods we
propose perform better than the two most effective aggregation methods
currently available. Additionally, our methods can be combined with sampling
techniques to significantly reduce the number of pairwise comparisons. We also
discuss how the Bradley--Terry portfolio selection approach can be implemented
in practice.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [242] [Regularizing Differentiable Architecture Search with Smooth Activation](https://arxiv.org/abs/2504.16306)
*Yanlin Zhou,Mostafa El-Khamy,Kee-Bong Song*

Main category: cs.NE

TL;DR: SA-DARTS通过平滑激活函数解决DARTS中的跳跃操作主导问题，提升NAS的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: DARTS存在鲁棒性、泛化性和差异性问题，尤其是跳跃操作主导导致性能崩溃。

Method: 提出SA-DARTS，利用平滑激活函数作为辅助损失，平衡权重自由操作的优势。

Result: 在NAS-Bench-201、分类和超分辨率任务中取得SOTA结果，并提升现有模型的参数效率。

Conclusion: SA-DARTS是一种简单有效的方法，解决了DARTS的核心问题，并在多个任务中表现优异。

Abstract: Differentiable Architecture Search (DARTS) is an efficient Neural
Architecture Search (NAS) method but suffers from robustness, generalization,
and discrepancy issues. Many efforts have been made towards the performance
collapse issue caused by skip dominance with various regularization techniques
towards operation weights, path weights, noise injection, and super-network
redesign. It had become questionable at a certain point if there could exist a
better and more elegant way to retract the search to its intended goal -- NAS
is a selection problem. In this paper, we undertake a simple but effective
approach, named Smooth Activation DARTS (SA-DARTS), to overcome skip dominance
and discretization discrepancy challenges. By leveraging a smooth activation
function on architecture weights as an auxiliary loss, our SA-DARTS mitigates
the unfair advantage of weight-free operations, converging to fanned-out
architecture weight values, and can recover the search process from
skip-dominance initialization. Through theoretical and empirical analysis, we
demonstrate that the SA-DARTS can yield new state-of-the-art (SOTA) results on
NAS-Bench-201, classification, and super-resolution. Further, we show that
SA-DARTS can help improve the performance of SOTA models with fewer parameters,
such as Information Multi-distillation Network on the super-resolution task.

</details>


### [243] [Neuro-Evolutionary Approach to Physics-Aware Symbolic Regression](https://arxiv.org/abs/2504.16503)
*Jiří Kubalík,Robert Babuška*

Main category: cs.NE

TL;DR: 提出了一种结合神经进化与梯度优化的符号回归方法，通过记忆策略和种群扰动提升模型质量。


<details>
  <summary>Details</summary>
Motivation: 传统遗传编程和神经网络方法在符号回归中存在结构优化不足或过早收敛的问题。

Method: 结合进化搜索与梯度优化，采用记忆策略和种群扰动，减少训练迭代次数。

Result: 在三个实际问题上表现优于其他基于神经网络的方法。

Conclusion: 该方法有效提升了符号回归的模型质量。

Abstract: Symbolic regression is a technique that can automatically derive analytic
models from data. Traditionally, symbolic regression has been implemented
primarily through genetic programming that evolves populations of candidate
solutions sampled by genetic operators, crossover and mutation. More recently,
neural networks have been employed to learn the entire analytical model, i.e.,
its structure and coefficients, using regularized gradient-based optimization.
Although this approach tunes the model's coefficients better, it is prone to
premature convergence to suboptimal model structures. Here, we propose a
neuro-evolutionary symbolic regression method that combines the strengths of
evolutionary-based search for optimal neural network (NN) topologies with
gradient-based tuning of the network's parameters. Due to the inherent high
computational demand of evolutionary algorithms, it is not feasible to learn
the parameters of every candidate NN topology to full convergence. Thus, our
method employs a memory-based strategy and population perturbations to enhance
exploitation and reduce the risk of being trapped in suboptimal NNs. In this
way, each NN topology can be trained using only a short sequence of
backpropagation iterations. The proposed method was experimentally evaluated on
three real-world test problems and has been shown to outperform other NN-based
approaches regarding the quality of the models obtained.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [244] [A Statistical Evaluation of Indoor LoRaWAN Environment-Aware Propagation for 6G: MLR, ANOVA, and Residual Distribution Analysis](https://arxiv.org/abs/2504.16688)
*Nahshon Mokua,Obiri,Kristof,Van Laerhoven*

Main category: cs.NI

TL;DR: 该研究提出了一种两阶段方法，用于建模室内LoRaWAN技术部署中的路径损耗，通过引入环境变量和混合高斯模型显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 室内LoRaWAN技术部署中的路径损耗建模因结构障碍、人员密度和活动以及环境条件波动而具有挑战性。

Method: 研究采用两阶段方法：1) 使用多元线性回归模型，包括传统传播指标和新增的环境变量；2) 通过拟合五种概率分布分析残差分布。

Result: 添加环境变量可将未解释方差减少42.32%，四组分高斯混合模型在捕捉信号传播残差异质性方面表现最佳。

Conclusion: 环境感知建模可显著改善动态室内物联网部署中的LoRaWAN网络设计，为6G网络的超可靠、上下文感知通信提供支持。

Abstract: Modeling path loss in indoor LoRaWAN technology deployments is inherently
challenging due to structural obstructions, occupant density and activities,
and fluctuating environmental conditions. This study proposes a two-stage
approach to capture and analyze these complexities using an extensive dataset
of 1,328,334 field measurements collected over six months in a single-floor
office at the University of Siegen's Hoelderlinstrasse Campus, Germany. First,
we implement a multiple linear regression model that includes traditional
propagation metrics (distance, structural walls) and an extension with proposed
environmental variables (relative humidity, temperature, carbon dioxide,
particulate matter, and barometric pressure). Using analysis of variance, we
demonstrate that adding these environmental factors can reduce unexplained
variance by 42.32 percent. Secondly, we examine residual distributions by
fitting five candidate probability distributions: Normal, Skew-Normal, Cauchy,
Student's t, and Gaussian Mixture Models with one to five components. Our
results show that a four-component Gaussian Mixture Model captures the residual
heterogeneity of indoor signal propagation most accurately, significantly
outperforming single-distribution approaches. Given the push toward
ultra-reliable, context-aware communications in 6G networks, our analysis shows
that environment-aware modeling can substantially improve LoRaWAN network
design in dynamic indoor IoT deployments.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [245] [Quality of explanation of xAI from the prespective of Italian end-users: Italian version of System Causability Scale (SCS)](https://arxiv.org/abs/2504.16193)
*Carmine Attanasio,Alireza Mortezapour*

Main category: cs.HC

TL;DR: 研究验证了意大利版系统可解释性量表（I-SCS）的有效性，用于衡量xAI系统提供的解释质量。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能应用范围的扩大，研究者关注如何提供高质量的算法解释。本研究旨在验证意大利版量表的有效性。

Method: 采用前向-后向翻译方法，计算内容效度指数/比率，并进行认知访谈。

Result: 原问卷10个问题中，1个因效度不足被移除，最终意大利版包含9个问题，用户理解良好。

Conclusion: 意大利版量表可用于未来研究及xAI开发者，衡量意大利文化中的解释质量。

Abstract: Background and aim: Considering the scope of the application of artificial
intelligence beyond the field of computer science, one of the concerns of
researchers is to provide quality explanations about the functioning of
algorithms based on artificial intelligence and the data extracted from it. The
purpose of the present study is to validate the Italian version of system
causability scale (I-SCS) to measure the quality of explanations provided in a
xAI.
  Method: For this purpose, the English version, initially provided in 2020 in
coordination with the main developer, was utilized. The forward-backward
translation method was applied to ensure accuracy. Finally, these nine steps
were completed by calculating the content validity index/ratio and conducting
cognitive interviews with representative end users.
  Results: The original version of the questionnaire consisted of 10 questions.
However, based on the obtained indexes (CVR below 0.49), one question (Question
8) was entirely removed. After completing the aforementioned steps, the Italian
version contained 9 questions. The representative sample of Italian end users
fully comprehended the meaning and content of the questions in the Italian
version.
  Conclusion: The Italian version obtained in this study can be used in future
research studies as well as in the field by xAI developers. This tool can be
used to measure the quality of explanations provided for an xAI system in
Italian culture.

</details>


### [246] [Cyberoception: Finding a Painlessly-Measurable New Sense in the Cyberworld Towards Emotion-Awareness in Computing](https://arxiv.org/abs/2504.16378)
*Tadashi Okoshi,Zexiong Gao,Tan Yi Zhen,Takumi Karasawa,Takeshi Miki,Wataru Sasaki,Rajesh K. Balan*

Main category: cs.HC

TL;DR: 论文提出了一种新概念“cyberoception”，用于通过智能手机传感器在用户日常生活中测量类似内感受的状态，以更准确地识别情绪。


<details>
  <summary>Details</summary>
Motivation: 现有内感受测量方法依赖实验室环境和高精度设备，难以在现实生活中监测用户的内感受状态。

Method: 提出“cyberoception”概念，并通过10天的实验室与野外混合实验验证其与情绪的相关性。

Result: 发现一种特定类型的“cyberoception”（“Turn On”）与用户情绪显著相关。

Conclusion: “cyberoception”可作为开发更“情绪感知”应用的基础。

Abstract: In Affective computing, recognizing users' emotions accurately is the basis
of affective human-computer interaction. Understanding users' interoception
contributes to a better understanding of individually different emotional
abilities, which is essential for achieving inter-individually accurate emotion
estimation. However, existing interoception measurement methods, such as the
heart rate discrimination task, have several limitations, including their
dependence on a well-controlled laboratory environment and precision apparatus,
making monitoring users' interoception challenging. This study aims to
determine other forms of data that can explain users' interoceptive or similar
states in their real-world lives and propose a novel hypothetical concept
"cyberoception," a new sense (1) which has properties similar to interoception
in terms of the correlation with other emotion-related abilities, and (2) which
can be measured only by the sensors embedded inside commodity smartphone
devices in users' daily lives. Results from a 10-day-long in-lab/in-the-wild
hybrid experiment reveal a specific cyberoception type "Turn On" (users'
subjective sensory perception about the frequency of turning-on behavior on
their smartphones), significantly related to participants' emotional valence.
We anticipate that cyberoception to serve as a fundamental building block for
developing more "emotion-aware", user-friendly applications and services.

</details>


### [247] [FeedQUAC: Quick Unobtrusive AI-Generated Commentary](https://arxiv.org/abs/2504.16416)
*Tao Long,Kendra Wannamaker,Jo Vermeulen,George Fitzmaurice,Justin Matejka*

Main category: cs.HC

TL;DR: 论文探讨了AI如何通过实时反馈工具FeedQUAC提升设计流程，研究发现其轻量级反馈能增强创意工作流。


<details>
  <summary>Details</summary>
Motivation: 设计过程中持续获取反馈耗时且干扰性强，AI可以填补这一空白，提供无干扰的实时反馈。

Method: 引入FeedQUAC工具，通过多角色AI生成实时评论，并进行8人设计探针研究。

Result: 参与者反馈工具便捷、有趣、提升信心和灵感，同时建议增加聊天互动和上下文管理功能。

Conclusion: AI反馈在设计中有潜力，需平衡用户参与，未来创意支持系统应考虑环境交互设计。

Abstract: Design thrives on feedback. However, gathering constant feedback throughout
the design process can be labor-intensive and disruptive. We explore how AI can
bridge this gap by providing effortless, ambient feedback. We introduce
FeedQUAC, a design companion that delivers real-time AI-generated commentary
from a variety of perspectives through different personas. A design probe study
with eight participants highlights how designers can leverage quick yet ambient
AI feedback to enhance their creative workflows. Participants highlight
benefits such as convenience, playfulness, confidence boost, and inspiration
from this lightweight feedback agent, while suggesting additional features,
like chat interaction and context curation. We discuss the role of AI feedback,
its strengths and limitations, and how to integrate it into existing design
workflows while balancing user involvement. Our findings also suggest that
ambient interaction is a valuable consideration for both the design and
evaluation of future creativity support systems.

</details>


### [248] [Exploring human-SAV interaction using large language models: The impact of psychological ownership and anthropomorphism on user experience](https://arxiv.org/abs/2504.16548)
*Lirui Guo,Michael G. Burke,Wynita M. Griggs*

Main category: cs.HC

TL;DR: 研究探讨了基于LLM的SAV用户界面中提示策略如何影响用户感知、体验和采用意图，发现更具拟人化和心理所有权触发的设计能提升用户体验和接受度。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注LLM驱动的SAV用户界面中提示策略对用户心理因素的影响，尤其是心理所有权。

Method: 设计了四种不同拟人化程度和心理所有权触发的SAV界面，通过定量和定性方法收集用户反馈。

Result: 更具拟人化和心理所有权触发的界面提升了用户对SAV拟人化特质的感知和情感反馈。

Conclusion: 研究结果为设计提升用户体验和SAV采用率的LLM对话界面提供了实用指导。

Abstract: There has been extensive prior work exploring how psychological factors such
as anthropomorphism affect the adoption of shared autonomous vehicles (SAVs).
However, limited research has been conducted on how prompt strategies in large
language model (LLM)-powered SAV User Interfaces (UIs) affect users'
perceptions, experiences, and intentions to adopt such technology. In this
work, we investigate how conversational UIs powered by LLMs drive these
psychological factors and psychological ownership, the sense of possession a
user may come to feel towards an entity or object they may not legally own. We
designed four SAV UIs with varying levels of anthropomorphic characteristics
and psychological ownership triggers. Quantitative measures of psychological
ownership, anthropomorphism, quality of service, disclosure tendency, sentiment
of SAV responses, and overall acceptance were collected after participants
interacted with each SAV. Qualitative feedback was also gathered regarding the
experience of psychological ownership during the interactions. The results
indicate that an SAV conversational UI designed to be more anthropomorphic and
to induce psychological ownership improved users' perceptions of the SAV's
human-like qualities and improved the sentiment of responses compared to a
control condition. These findings provide practical guidance for designing
LLM-based conversational UIs that enhance user experience and adoption of SAVs.

</details>


### [249] [A Vision for AI-Driven Adaptation of Dynamic AR Content to Users and Environments](https://arxiv.org/abs/2504.16562)
*Julian Rasch,Florian Müller,Francesco Chiossi*

Main category: cs.HC

TL;DR: 论文探讨了AI驱动的动态AR内容布局，旨在通过机器学习优化AR体验，减少用户认知负担。


<details>
  <summary>Details</summary>
Motivation: 现有AR系统难以有效管理交互可能性，需通过AI动态调整内容布局以适应环境和用户行为。

Method: 利用机器学习方法，动态调整AR内容在环境中的分布，平衡动态投影与静态内容。

Result: 提出了一种更直观、高效的AI驱动AR体验愿景，适用于导航、工作、学习等多个领域。

Conclusion: AI驱动的动态AR内容布局有望推动AR技术的创新，提升用户体验和行业应用潜力。

Abstract: Augmented Reality (AR) is transforming the way we interact with virtual
information in the physical world. By overlaying digital content in real-world
environments, AR enables new forms of immersive and engaging experiences.
However, existing AR systems often struggle to effectively manage the many
interactive possibilities that AR presents. This vision paper speculates on
AI-driven approaches for adaptive AR content placement, dynamically adjusting
to user movement and environmental changes. By leveraging machine learning
methods, such a system would intelligently manage content distribution between
AR projections integrated into the external environment and fixed static
content, enabling seamless UI layout and potentially reducing users' cognitive
load. By exploring the possibilities of AI-driven dynamic AR content placement,
we aim to envision new opportunities for innovation and improvement in various
industries, from urban navigation and workplace productivity to immersive
learning and beyond. This paper outlines a vision for the development of more
intuitive, engaging, and effective AI-powered AR experiences.

</details>


### [250] [PsyCounAssist: A Full-Cycle AI-Powered Psychological Counseling Assistant System](https://arxiv.org/abs/2504.16573)
*Xianghe Liu,Jiaqi Xu,Tao Sun*

Main category: cs.HC

TL;DR: PsyCounAssist是一个AI驱动的心理咨询辅助系统，结合多模态情感识别和自动化报告功能，提升心理咨询效率。


<details>
  <summary>Details</summary>
Motivation: 心理咨询需要实时监控情绪变化和记录会话内容，传统方法效率较低，AI可提供更高效的支持。

Method: 系统整合语音和PPG信号进行实时情感分析，利用大语言模型生成结构化报告，并提供个性化后续支持。

Result: 实验验证了PPG情感分类的可靠性，系统在真实场景中表现出实用性和隐私保护性。

Conclusion: PsyCounAssist为AI在心理咨询中的伦理和有效整合提供了新思路。

Abstract: Psychological counseling is a highly personalized and dynamic process that
requires therapists to continuously monitor emotional changes, document session
insights, and maintain therapeutic continuity. In this paper, we introduce
PsyCounAssist, a comprehensive AI-powered counseling assistant system
specifically designed to augment psychological counseling practices.
PsyCounAssist integrates multimodal emotion recognition combining speech and
photoplethysmography (PPG) signals for accurate real-time affective analysis,
automated structured session reporting using large language models (LLMs), and
personalized AI-generated follow-up support. Deployed on Android-based tablet
devices, the system demonstrates practical applicability and flexibility in
real-world counseling scenarios. Experimental evaluation confirms the
reliability of PPG-based emotional classification and highlights the system's
potential for non-intrusive, privacy-aware emotional support. PsyCounAssist
represents a novel approach to ethically and effectively integrating AI into
psychological counseling workflows.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [251] [Physics-Informed Inference Time Scaling via Simulation-Calibrated Scientific Machine Learning](https://arxiv.org/abs/2504.16172)
*Zexi Fan,Yan Sun,Shihao Yang,Yiping Lu*

Main category: math.NA

TL;DR: SCaSML是一种物理信息框架，通过动态修正和去偏SciML预测，显著提升高维PDE求解的精度。


<details>
  <summary>Details</summary>
Motivation: 高维PDE在多个领域存在计算挑战，现有SciML方法存在偏差且忽略物理规律。

Method: 提出SCaSML框架，利用物理定律动态修正预测，结合Feynman-Kac和Elworthy-Bismut-Li公式的蒙特卡洛求解器。

Result: 数值实验显示SCaSML比基础模型减少20-50%误差，首次实现在推理阶段优化高维PDE解。

Conclusion: SCaSML通过物理信息动态修正，显著提升高维PDE求解精度，具有理论和实践价值。

Abstract: High-dimensional partial differential equations (PDEs) pose significant
computational challenges across fields ranging from quantum chemistry to
economics and finance. Although scientific machine learning (SciML) techniques
offer approximate solutions, they often suffer from bias and neglect crucial
physical insights. Inspired by inference-time scaling strategies in language
models, we propose Simulation-Calibrated Scientific Machine Learning (SCaSML),
a physics-informed framework that dynamically refines and debiases the SCiML
predictions during inference by enforcing the physical laws. SCaSML leverages
derived new physical laws that quantifies systematic errors and employs Monte
Carlo solvers based on the Feynman-Kac and Elworthy-Bismut-Li formulas to
dynamically correct the prediction. Both numerical and theoretical analysis
confirms enhanced convergence rates via compute-optimal inference methods. Our
numerical experiments demonstrate that SCaSML reduces errors by 20-50% compared
to the base surrogate model, establishing it as the first algorithm to refine
approximated solutions to high-dimensional PDE during inference. Code of SCaSML
is available at https://github.com/Francis-Fan-create/SCaSML.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [252] [MARFT: Multi-Agent Reinforcement Fine-Tuning](https://arxiv.org/abs/2504.16129)
*Junwei Liao,Muning Wen,Jun Wang,Weinan Zhang*

Main category: cs.MA

TL;DR: 论文提出了一种名为多智能体强化微调（MARFT）的新范式，用于优化基于LLM的多智能体系统（LaMAS），并提供了一个通用的算法框架和开源实现。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习（RL）方法在优化LaMAS时存在挑战，且多智能体强化学习（MARL）直接应用于LaMAS时效果有限，因此需要一种新的方法。

Method: 提出了多智能体强化微调（MARFT）框架，包括算法设计、实现策略和开源代码。

Result: MARFT为LaMAS提供了一种可扩展且稳健的优化方法，并公开了实现代码以促进研究。

Conclusion: MARFT为多智能体系统的优化提供了新方向，并有望推动自适应和鲁棒解决方案的发展。

Abstract: LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in
addressing complex, agentic tasks requiring multifaceted reasoning and
collaboration, from generating high-quality presentation slides to conducting
sophisticated scientific research. Meanwhile, RL has been widely recognized for
its effectiveness in enhancing agent intelligence, but limited research has
investigated the fine-tuning of LaMAS using foundational RL techniques.
Moreover, the direct application of MARL methodologies to LaMAS introduces
significant challenges, stemming from the unique characteristics and mechanisms
inherent to LaMAS. To address these challenges, this article presents a
comprehensive study of LLM-based MARL and proposes a novel paradigm termed
Multi-Agent Reinforcement Fine-Tuning (MARFT). We introduce a universal
algorithmic framework tailored for LaMAS, outlining the conceptual foundations,
key distinctions, and practical implementation strategies. We begin by
reviewing the evolution from RL to Reinforcement Fine-Tuning, setting the stage
for a parallel analysis in the multi-agent domain. In the context of LaMAS, we
elucidate critical differences between MARL and MARFT. These differences
motivate a transition toward a novel, LaMAS-oriented formulation of RFT.
Central to this work is the presentation of a robust and scalable MARFT
framework. We detail the core algorithm and provide a complete, open-source
implementation to facilitate adoption and further research. The latter sections
of the paper explore real-world application perspectives and opening challenges
in MARFT. By bridging theoretical underpinnings with practical methodologies,
this work aims to serve as a roadmap for researchers seeking to advance MARFT
toward resilient and adaptive solutions in agentic systems. Our implementation
of the proposed framework is publicly available at:
https://github.com/jwliao-ai/MARFT.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [253] [Approximating Optimal Labelings for Temporal Connectivity](https://arxiv.org/abs/2504.16837)
*Daniele Carnevale,Gianlorenzo D'Angelo,Martin Olsen*

Main category: cs.DS

TL;DR: 研究了在时间图中最小化边标签数量以确保所有顶点对在给定时间内连通的问题（MAL），证明了其近似难度并提出了近似算法。


<details>
  <summary>Details</summary>
Motivation: MAL问题在物流、调度和信息传播中有重要应用，优化时间标签可显著降低成本或排放。

Method: 分析了MAL的复杂性和近似性，证明了近似难度下界，并设计了近似算法。

Result: 证明了MAL在特定条件下的近似难度，并提出了匹配下界的近似算法。

Conclusion: MAL问题在理论和应用上具有重要意义，其近似算法和复杂性分析为实际应用提供了指导。

Abstract: In a temporal graph the edge set dynamically changes over time according to a
set of time-labels associated with each edge that indicates at which time-steps
the edge is available. Two vertices are connected if there is a path connecting
them in which the edges are traversed in increasing order of their labels. We
study the problem of scheduling the availability time of the edges of a
temporal graph in such a way that all pairs of vertices are connected within a
given maximum allowed time $a$ and the overall number of labels is minimized.
  The problem, known as \emph{Minimum Aged Labeling} (MAL), has several
applications in logistics, distribution scheduling, and information spreading
in social networks, where carefully choosing the time-labels can significantly
reduce infrastructure costs, fuel consumption, or greenhouse gases.
  The problem MAL has previously been proved to be NP-complete on undirected
graphs and \APX-hard on directed graphs. In this paper, we extend our knowledge
on the complexity and approximability of MAL in several directions. We first
show that the problem cannot be approximated within a factor better than
$O(\log n)$ when $a\geq 2$, unless $\text{P} = \text{NP}$, and a factor better
than $2^{\log ^{1-\epsilon} n}$ when $a\geq 3$, unless $\text{NP}\subseteq
\text{DTIME}(2^{\text{polylog}(n)})$, where $n$ is the number of vertices in
the graph. Then we give a set of approximation algorithms that, under some
conditions, almost match these lower bounds. In particular, we show that the
approximation depends on a relation between $a$ and the diameter of the input
graph.
  We further establish a connection with a foundational optimization problem on
static graphs called \emph{Diameter Constrained Spanning Subgraph} (DCSS) and
show that our hardness results also apply to DCSS.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [254] [HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing](https://arxiv.org/abs/2504.16112)
*Myunghyun Rhee,Joonseop Sim,Taeyoung Ahn,Seungyong Lee,Daegun Yoon,Euiseok Kim,Kyoung Park,Youngpyo Joo,Hosik Kim*

Main category: cs.AR

TL;DR: 论文提出了一种高带宽处理单元（HPU），作为GPU的协处理器，通过卸载内存密集型任务提升LLM推理效率，性能提升4.1倍，能效提升4.6倍。


<details>
  <summary>Details</summary>
Motivation: 当前GPU系统在处理Transformer LLMs的注意力层时效率低下，主要由于低操作强度和KV缓存的高内存需求。

Method: 设计并实现了一种基于FPGA的HPU协处理器，通过PCIe与GPU系统连接，卸载内存密集型任务。

Result: HPU原型在GPU-HPU异构系统中实现了4.1倍的性能提升和4.6倍的能效提升。

Conclusion: HPU作为一种扩展卡，能够在不增加GPU数量的情况下提升LLM推理的效率和可扩展性。

Abstract: The attention layer, a core component of Transformer-based LLMs, brings out
inefficiencies in current GPU systems due to its low operational intensity and
the substantial memory requirements of KV caches. We propose a High-bandwidth
Processing Unit (HPU), a memoryintensive co-processor that enhances GPU
resource utilization during large-batched LLM inference. By offloading
memory-bound operations, the HPU allows the GPU to focus on compute-intensive
tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales
out to accommodate surging memory demands driven by large batch sizes and
extended sequence lengths. In this paper, we show the HPU prototype implemented
with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU
heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy
efficiency improvements over a GPUonly system, providing scalability without
increasing the number of GPUs.

</details>


### [255] [FPGA-Based Neural Network Accelerators for Space Applications: A Survey](https://arxiv.org/abs/2504.16173)
*Pedro Antunes,Artur Podobas*

Main category: cs.AR

TL;DR: 本文综述了FPGA在航天任务中作为神经网络加速器的潜力，分析了现有研究并提出了未来方向。


<details>
  <summary>Details</summary>
Motivation: 航天任务对高性能计算的需求增加，FPGA因其灵活性和抗辐射能力成为理想选择，神经网络在自主操作和数据分析中的优势推动了这一研究。

Method: 通过文献分析，识别趋势和差距，并提出未来研究方向。

Result: FPGA加速器有望提升航天器计算系统性能。

Conclusion: FPGA与神经网络的结合为航天任务提供了高效的计算解决方案，未来研究需进一步探索其潜力。

Abstract: Space missions are becoming increasingly ambitious, necessitating
high-performance onboard spacecraft computing systems. In response,
field-programmable gate arrays (FPGAs) have garnered significant interest due
to their flexibility, cost-effectiveness, and radiation tolerance potential.
Concurrently, neural networks (NNs) are being recognized for their capability
to execute space mission tasks such as autonomous operations, sensor data
analysis, and data compression. This survey serves as a valuable resource for
researchers aiming to implement FPGA-based NN accelerators in space
applications. By analyzing existing literature, identifying trends and gaps,
and proposing future research directions, this work highlights the potential of
these accelerators to enhance onboard computing systems.

</details>


### [256] [TeLLMe: An Energy-Efficient Ternary LLM Accelerator for Prefilling and Decoding on Edge FPGAs](https://arxiv.org/abs/2504.16266)
*Ye Qiao,Zhiheng Cheng,Yifan Zhang,Yian Wang,Sitao Huang*

Main category: cs.AR

TL;DR: TeLLMe是一种针对低功耗FPGA的三元大语言模型加速器，支持1.58位权重和8位激活，显著提升边缘设备上的能效和性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在边缘设备上部署时的高计算和内存需求问题，尤其是预填充阶段的延迟和资源限制。

Method: 采用表查找矩阵引擎、融合的带宽高效注意力模块以及优化的归一化和量化-反量化单元。

Result: 在7W功耗下，TeLLMe实现9 tokens/s的吞吐量，预填充延迟为0.55--1.15秒，显著提升能效。

Conclusion: TeLLMe为生成式AI在边缘FPGA上设定了新的能效基准。

Abstract: Deploying large language models (LLMs) on edge platforms is challenged by
their high computational and memory demands. Although recent low-bit
quantization methods (e.g., BitNet, DeepSeek) compress weights to as little as
1.58 bits with minimal accuracy loss, edge deployment is still constrained by
limited on-chip resources, power budgets, and the often-neglected latency of
the prefill phase. We present TeLLMe, the first ternary LLM accelerator for
low-power FPGAs (e.g., AMD KV260) that fully supports both prefill and
autoregressive decoding using 1.58-bit weights and 8-bit activations. Our
contributions include: (1) a table-lookup matrix engine for ternary matmul that
merges grouped activations with online precomputation to minimize resource use;
(2) a fused, bandwidth-efficient attention module featuring a reversed
reordering scheme to accelerate prefill; and (3) a tightly integrated
normalization and quantization--dequantization unit optimized for ultra-low-bit
inference. Under a 7W power budget, TeLLMe delivers up to 9 tokens/s throughput
over 1,024-token contexts and prefill latencies of 0.55--1.15 s for 64--128
token prompts, marking a significant energy-efficiency advance and establishing
a new edge FPGA benchmark for generative AI.

</details>


### [257] [COBRA: Algorithm-Architecture Co-optimized Binary Transformer Accelerator for Edge Inference](https://arxiv.org/abs/2504.16269)
*Ye Qiao,Zhiheng Cheng,Yian Wang,Yifan Zhang,Yunzhe Deng,Sitao Huang*

Main category: cs.AR

TL;DR: COBRA是一种针对边缘计算的算法-架构协同优化的二进制Transformer加速器，通过1位二进制乘法单元和硬件友好优化，显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在边缘平台部署时面临计算、内存和通信的高需求问题，现有二进制Transformer因缺乏硬件优化而效率低下。

Method: 引入COBRA，采用1位二进制乘法单元和注意力块硬件友好优化，支持-1、0和+1的矩阵运算。

Result: 在边缘FPGA上实现3,894.7 GOPS吞吐量和448.7 GOPS/Watt能效，比GPU提升311倍能效，比现有二进制加速器提升3.5倍吞吐量。

Conclusion: COBRA为边缘计算提供了一种高效、低功耗的二进制Transformer加速方案，且精度损失可忽略。

Abstract: Transformer-based models have demonstrated superior performance in various
fields, including natural language processing and computer vision. However,
their enormous model size and high demands in computation, memory, and
communication limit their deployment to edge platforms for local, secure
inference. Binary transformers offer a compact, low-complexity solution for
edge deployment with reduced bandwidth needs and acceptable accuracy. However,
existing binary transformers perform inefficiently on current hardware due to
the lack of binary specific optimizations. To address this, we introduce COBRA,
an algorithm-architecture co-optimized binary Transformer accelerator for edge
computing. COBRA features a real 1-bit binary multiplication unit, enabling
matrix operations with -1, 0, and +1 values, surpassing ternary methods. With
further hardware-friendly optimizations in the attention block, COBRA achieves
up to 3,894.7 GOPS throughput and 448.7 GOPS/Watt energy efficiency on edge
FPGAs, delivering a 311x energy efficiency improvement over GPUs and a 3.5x
throughput improvement over the state-of-the-art binary accelerator, with only
negligible inference accuracy degradation.

</details>
