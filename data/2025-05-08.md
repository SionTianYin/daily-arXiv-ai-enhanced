<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 22]
- [cs.CV](#cs.CV) [Total: 68]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 99]
- [cs.SI](#cs.SI) [Total: 4]
- [math.OC](#math.OC) [Total: 5]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.SE](#cs.SE) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.NE](#cs.NE) [Total: 3]
- [physics.optics](#physics.optics) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [math.HO](#math.HO) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.HC](#cs.HC) [Total: 4]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.DC](#cs.DC) [Total: 4]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.GR](#cs.GR) [Total: 6]
- [eess.AS](#eess.AS) [Total: 3]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.RO](#cs.RO) [Total: 5]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding](https://arxiv.org/abs/2505.03788)
*Trilok Padhi,Ramneet Kaur,Adam D. Cobb,Manoj Acharya,Anirban Roy,Colin Samplawski,Brian Matejek,Alexander M. Berenbeim,Nathaniel D. Bastian,Susmit Jha*

Main category: cs.CL

TL;DR: 提出了一种新颖的多模态大语言模型（LLM）不确定性校准方法，通过结合跨模态一致性和自一致性，显著提升了校准效果。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化（UQ）方法在多模态LLM中表现不佳，尤其在模型一致错误时仍报告高置信度，导致校准效果差。

Method: 利用视觉输入对文本响应进行基础校准，并通过温度缩放技术校准基础模型的置信度。

Result: 在医疗问答（Slake）和视觉问答（VQAv2）任务中，实验表明该方法显著提升了校准效果。

Conclusion: 结合跨模态一致性和温度缩放技术，显著改进了多模态LLM的不确定性校准。

Abstract: We introduce a novel approach for calibrating uncertainty quantification (UQ)
tailored for multi-modal large language models (LLMs). Existing
state-of-the-art UQ methods rely on consistency among multiple responses
generated by the LLM on an input query under diverse settings. However, these
approaches often report higher confidence in scenarios where the LLM is
consistently incorrect. This leads to a poorly calibrated confidence with
respect to accuracy. To address this, we leverage cross-modal consistency in
addition to self-consistency to improve the calibration of the multi-modal
models. Specifically, we ground the textual responses to the visual inputs. The
confidence from the grounding model is used to calibrate the overall
confidence. Given that using a grounding model adds its own uncertainty in the
pipeline, we apply temperature scaling - a widely accepted parametric
calibration technique - to calibrate the grounding model's confidence in the
accuracy of generated responses. We evaluate the proposed approach across
multiple multi-modal tasks, such as medical question answering (Slake) and
visual question answering (VQAv2), considering multi-modal models such as
LLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework
achieves significantly improved calibration on both tasks.

</details>


### [2] [Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty](https://arxiv.org/abs/2505.03910)
*Gianluca Manzo,Julia Ive*

Main category: cs.CL

TL;DR: 该论文研究了深度学习模型在胸片解读中的不确定性量化问题，探讨了贝叶斯深度学习预测不确定性与人类语言不确定性之间的关系，并评估了不同方法的效果。


<details>
  <summary>Details</summary>
Motivation: 自动化胸片解读可优化临床工作流程，但仅优化预测性能不足，需量化不确定性以提升实用性。

Method: 使用BERT模型，评估不同二值化方法，并比较蒙特卡洛Dropout和深度集成在预测不确定性估计中的效果。

Result: 模型表现良好，但预测不确定性与语言不确定性相关性较弱，表明机器与人类不确定性对齐存在挑战。

Conclusion: 贝叶斯近似提供了有用的不确定性估计，但需进一步改进以更好地捕捉人类不确定性，适用于临床。

Abstract: Automating chest radiograph interpretation using Deep Learning (DL) models
has the potential to significantly improve clinical workflows, decision-making,
and large-scale health screening. However, in medical settings, merely
optimising predictive performance is insufficient, as the quantification of
uncertainty is equally crucial. This paper investigates the relationship
between predictive uncertainty, derived from Bayesian Deep Learning
approximations, and human/linguistic uncertainty, as estimated from free-text
radiology reports labelled by rule-based labellers. Utilising BERT as the model
of choice, this study evaluates different binarisation methods for uncertainty
labels and explores the efficacy of Monte Carlo Dropout and Deep Ensembles in
estimating predictive uncertainty. The results demonstrate good model
performance, but also a modest correlation between predictive and linguistic
uncertainty, highlighting the challenges in aligning machine uncertainty with
human interpretation nuances. Our findings suggest that while Bayesian
approximations provide valuable uncertainty estimates, further refinement is
necessary to fully capture and utilise the subtleties of human uncertainty in
clinical applications.

</details>


### [3] [A Reasoning-Focused Legal Retrieval Benchmark](https://arxiv.org/abs/2505.03970)
*Lucia Zheng,Neel Guha,Javokhir Arifov,Sarah Zhang,Michal Skreta,Christopher D. Manning,Peter Henderson,Daniel E. Ho*

Main category: cs.CL

TL;DR: 论文介绍了两个新的法律RAG基准测试（Bar Exam QA和Housing Statute QA），以解决缺乏真实法律RAG基准的问题，并评估现有检索管道的性能。


<details>
  <summary>Details</summary>
Motivation: 法律AI开发者使用RAG系统提升性能，但缺乏真实的法律RAG基准来模拟复杂的法律检索和问答任务。

Method: 通过类似法律研究的标注流程，构建了两个新的法律RAG基准测试。

Result: 结果显示法律RAG仍具挑战性，需要进一步研究。

Conclusion: 论文提出的基准测试为未来法律RAG研究提供了方向。

Abstract: As the legal community increasingly examines the use of large language models
(LLMs) for various legal applications, legal AI developers have turned to
retrieval-augmented LLMs ("RAG" systems) to improve system performance and
robustness. An obstacle to the development of specialized RAG systems is the
lack of realistic legal RAG benchmarks which capture the complexity of both
legal retrieval and downstream legal question-answering. To address this, we
introduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA.
Our tasks correspond to real-world legal research tasks, and were produced
through annotation processes which resemble legal research. We describe the
construction of these benchmarks and the performance of existing retriever
pipelines. Our results suggest that legal RAG remains a challenging
application, thus motivating future research.

</details>


### [4] [Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale](https://arxiv.org/abs/2505.03973)
*Jiale Liu,Yifan Zeng,Shaokun Zhang,Chi Zhang,Malte Højmark-Bertelsen,Marie Normann Gadeberg,Huazheng Wang,Qingyun Wu*

Main category: cs.CL

TL;DR: 论文提出了一种细粒度优化（FGO）框架，通过将大型优化任务分解为可管理的子集，解决了传统LLM优化方法在处理大规模数据集时的上下文窗口溢出和模式识别退化问题。


<details>
  <summary>Details</summary>
Motivation: 传统LLM优化方法在处理大规模数据集时存在上下文窗口溢出和模式识别退化的问题，限制了其扩展性。

Method: 提出FGO框架，将大型优化任务分解为子集，进行针对性优化，并通过渐进合并系统化组合优化后的组件。

Result: 在ALFWorld、LogisticsQA和GAIA基准测试中，FGO性能优于现有方法1.6-8.6%，同时平均提示令牌消耗减少56.3%。

Conclusion: FGO为扩展LLM优化提供了实用解决方案，展示了其在大规模数据集上的可扩展性和高效性。

Abstract: LLM-based optimization has shown remarkable potential in enhancing agentic
systems. However, the conventional approach of prompting LLM optimizer with the
whole training trajectories on training dataset in a single pass becomes
untenable as datasets grow, leading to context window overflow and degraded
pattern recognition. To address these challenges, we propose Fine-Grained
Optimization (FGO), a scalable framework that divides large optimization tasks
into manageable subsets, performs targeted optimizations, and systematically
combines optimized components through progressive merging. Evaluation across
ALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms
existing approaches by 1.6-8.6% while reducing average prompt token consumption
by 56.3%. Our framework provides a practical solution for scaling up LLM-based
optimization of increasingly sophisticated agent systems. Further analysis
demonstrates that FGO achieves the most consistent performance gain in all
training dataset sizes, showcasing its scalability and efficiency.

</details>


### [5] [X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains](https://arxiv.org/abs/2505.03981)
*Qianchu Liu,Sheng Zhang,Guanghui Qin,Timothy Ossowski,Yu Gu,Ying Jin,Sid Kiblawi,Sam Preston,Mu Wei,Paul Vozila,Tristan Naumann,Hoifung Poon*

Main category: cs.CL

TL;DR: 论文探讨了跨模态和领域的推理能力是否可推广，并提出X-Reasoner模型，通过文本后训练实现通用推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有开源研究集中于文本推理模型，缺乏跨模态和领域的研究，探索推理能力的通用性。

Method: 采用两阶段方法：监督微调（长链思维蒸馏）和强化学习（可验证奖励）。

Result: X-Reasoner在跨模态和领域任务中表现优异，X-Reasoner-Med在医疗领域创下新纪录。

Conclusion: 通用文本后训练可推广推理能力，X-Reasoner及其医疗变体验证了方法的有效性。

Abstract: Recent proprietary models (e.g., o3) have begun to demonstrate strong
multimodal reasoning capabilities. Yet, most existing open-source research
concentrates on training text-only reasoning models, with evaluations limited
to mainly mathematical and general-domain tasks. Therefore, it remains unclear
how to effectively extend reasoning capabilities beyond text input and general
domains. This paper explores a fundamental research question: Is reasoning
generalizable across modalities and domains? Our findings support an
affirmative answer: General-domain text-based post-training can enable such
strong generalizable reasoning. Leveraging this finding, we introduce
X-Reasoner, a vision-language model post-trained solely on general-domain text
for generalizable reasoning, using a two-stage approach: an initial supervised
fine-tuning phase with distilled long chain-of-thoughts, followed by
reinforcement learning with verifiable rewards. Experiments show that
X-Reasoner successfully transfers reasoning capabilities to both multimodal and
out-of-domain settings, outperforming existing state-of-the-art models trained
with in-domain and multimodal data across various general and medical
benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in
specialized domains can be further enhanced through continued training on
domain-specific text-only data. Building upon this, we introduce
X-Reasoner-Med, a medical-specialized variant that achieves new state of the
art on numerous text-only and multimodal medical benchmarks.

</details>


### [6] [SLOT: Structuring the Output of Large Language Models](https://arxiv.org/abs/2505.04016)
*Darren Yow-Bang Wang,Zhengyuan Shen,Soumya Smruti Mishra,Zhichao Xu,Yifei Teng,Haibo Ding*

Main category: cs.CL

TL;DR: SLOT是一种模型无关的方法，通过微调轻量级语言模型将非结构化LLM输出转换为精确的结构化格式，显著提升了模式准确性和内容保真度。


<details>
  <summary>Details</summary>
Motivation: LLM在关键应用中常生成偏离预定义模式的输出，影响可靠应用开发，需要一种灵活且通用的解决方案。

Method: SLOT采用微调的轻量级语言模型作为后处理层，结合数据合成和评估方法，支持多种LLM和模式规范。

Result: 微调的Mistral-7B模型在模式准确性和内容相似性上表现优异（99.5%和94.0%），优于Claude-3.5-Sonnet。

Conclusion: SLOT使轻量模型（如Llama-3.2-1B）也能实现与大型专有模型相当的结构化输出能力，适用于资源受限环境。

Abstract: Structured outputs are essential for large language models (LLMs) in critical
applications like agents and information extraction. Despite their
capabilities, LLMs often generate outputs that deviate from predefined schemas,
significantly hampering reliable application development. We present SLOT
(Structured LLM Output Transformer), a model-agnostic approach that transforms
unstructured LLM outputs into precise structured formats. While existing
solutions predominantly rely on constrained decoding techniques or are tightly
coupled with specific models, SLOT employs a fine-tuned lightweight language
model as a post-processing layer, achieving flexibility across various LLMs and
schema specifications. We introduce a systematic pipeline for data curation and
synthesis alongside a formal evaluation methodology that quantifies both schema
accuracy and content fidelity. Our results demonstrate that fine-tuned
Mistral-7B model with constrained decoding achieves near perfect schema
accuracy (99.5%) and content similarity (94.0%), outperforming
Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points,
respectively). Notably, even compact models like Llama-3.2-1B can match or
exceed the structured output capabilities of much larger proprietary models
when equipped with SLOT, enabling reliable structured generation in
resource-constrained environments.

</details>


### [7] [Advancing and Benchmarking Personalized Tool Invocation for LLMs](https://arxiv.org/abs/2505.04072)
*Xu Huang,Yuefeng Huang,Weiwen Liu,Xingshan Zeng,Yasheng Wang,Ruiming Tang,Hong Xie,Defu Lian*

Main category: cs.CL

TL;DR: 论文提出了个性化工具调用的概念，并定义了工具偏好和基于个人资料的查询两项任务，提出了PTool框架和PTBench基准，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLMs调用工具的基本能力，而忽略了工具调用中的个性化约束。

Method: 提出了PTool数据合成框架和PTBench基准，并对开源模型进行了微调。

Result: 验证了PTool框架的有效性，并提供了有价值的见解。

Conclusion: 个性化工具调用是一个重要方向，PTool和PTBench为未来研究提供了基础。

Abstract: Tool invocation is a crucial mechanism for extending the capabilities of
Large Language Models (LLMs) and has recently garnered significant attention.
It enables LLMs to solve complex problems through tool calls while accessing
up-to-date world knowledge. However, existing work primarily focuses on the
fundamental ability of LLMs to invoke tools for problem-solving, without
considering personalized constraints in tool invocation. In this work, we
introduce the concept of Personalized Tool Invocation and define two key tasks:
Tool Preference and Profile-dependent Query. Tool Preference addresses user
preferences when selecting among functionally similar tools, while
Profile-dependent Query considers cases where a user query lacks certain tool
parameters, requiring the model to infer them from the user profile. To tackle
these challenges, we propose PTool, a data synthesis framework designed for
personalized tool invocation. Additionally, we construct \textbf{PTBench}, the
first benchmark for evaluating personalized tool invocation. We then fine-tune
various open-source models, demonstrating the effectiveness of our framework
and providing valuable insights. Our benchmark is public at
https://github.com/hyfshadow/PTBench.

</details>


### [8] [Natural Language Generation in Healthcare: A Review of Methods and Applications](https://arxiv.org/abs/2505.04073)
*Mengxian Lyu,Xiaohan Li,Ziyi Chen,Jinqian Pan,Cheng Peng,Sankalp Talankar,Yonghui Wu*

Main category: cs.CL

TL;DR: 本文综述了自然语言生成（NLG）在医疗领域的应用，分析了113篇相关文献，涵盖数据模态、模型架构、临床应用及评估方法，并总结了其潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的突破，NLG在医疗领域的应用潜力巨大，但缺乏对其方法和应用的全面综述。

Method: 系统回顾了113篇文献，遵循PRISMA指南，分类关键方法并评估其能力与限制。

Result: 总结了NLG在医疗中的关键技术和应用，揭示了其潜力与挑战。

Conclusion: 本文为未来研究提供了有价值的见解，以利用NLG推动医疗发现和健康护理的变革。

Abstract: Natural language generation (NLG) is the key technology to achieve generative
artificial intelligence (AI). With the breakthroughs in large language models
(LLMs), NLG has been widely used in various medical applications, demonstrating
the potential to enhance clinical workflows, support clinical decision-making,
and improve clinical documentation. Heterogeneous and diverse medical data
modalities, such as medical text, images, and knowledge bases, are utilized in
NLG. Researchers have proposed many generative models and applied them in a
number of healthcare applications. There is a need for a comprehensive review
of NLG methods and applications in the medical domain. In this study, we
systematically reviewed 113 scientific publications from a total of 3,988
NLG-related articles identified using a literature search, focusing on data
modality, model architecture, clinical applications, and evaluation methods.
Following PRISMA (Preferred Reporting Items for Systematic reviews and
Meta-Analyses) guidelines, we categorize key methods, identify clinical
applications, and assess their capabilities, limitations, and emerging
challenges. This timely review covers the key NLG technologies and medical
applications and provides valuable insights for future studies to leverage NLG
to transform medical discovery and healthcare.

</details>


### [9] [Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model](https://arxiv.org/abs/2505.04132)
*Mingruo Yuan,Ben Kao,Tien-Hsuan Wu,Michael M. K. Cheung,Henry W. H. Chan,Anne S. Y. Cheung,Felix W. H. Chan,Yongxi Chen*

Main category: cs.CL

TL;DR: 论文提出了一种三步法，将法律信息转化为易于公众理解的形式，包括生成法律知识片段、构建法律问题库和设计交互式推荐系统。重点探讨了利用预训练语言模型生成法律问题的技术。


<details>
  <summary>Details</summary>
Motivation: 解决法律信息对非专业人士难以理解和导航的问题，提升公众对法律知识的可及性。

Method: 1. 将法律条文转化为易于理解的片段（CLIC-pages）；2. 构建法律问题库（LQB）；3. 设计交互式推荐系统（CRec）。

Result: 机器生成的问题（MGQs）更具扩展性和多样性，而人工编写的问题（HCQs）更精确。

Conclusion: 三步法有效提升了法律知识的可及性，预训练模型在生成法律问题方面具有潜力。

Abstract: Access to legal information is fundamental to access to justice. Yet
accessibility refers not only to making legal documents available to the
public, but also rendering legal information comprehensible to them. A vexing
problem in bringing legal information to the public is how to turn formal legal
documents such as legislation and judgments, which are often highly technical,
to easily navigable and comprehensible knowledge to those without legal
education. In this study, we formulate a three-step approach for bringing legal
knowledge to laypersons, tackling the issues of navigability and
comprehensibility. First, we translate selected sections of the law into
snippets (called CLIC-pages), each being a small piece of article that focuses
on explaining certain technical legal concept in layperson's terms. Second, we
construct a Legal Question Bank (LQB), which is a collection of legal questions
whose answers can be found in the CLIC-pages. Third, we design an interactive
CLIC Recommender (CRec). Given a user's verbal description of a legal situation
that requires a legal solution, CRec interprets the user's input and shortlists
questions from the question bank that are most likely relevant to the given
legal situation and recommends their corresponding CLIC pages where relevant
legal knowledge can be found. In this paper we focus on the technical aspects
of creating an LQB. We show how large-scale pre-trained language models, such
as GPT-3, can be used to generate legal questions. We compare machine-generated
questions (MGQs) against human-composed questions (HCQs) and find that MGQs are
more scalable, cost-effective, and more diversified, while HCQs are more
precise. We also show a prototype of CRec and illustrate through an example how
our 3-step approach effectively brings relevant legal knowledge to the public.

</details>


### [10] [Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models](https://arxiv.org/abs/2505.04135)
*Vihaan Miriyala,Smrithi Bukkapatnam,Lavanya Prahallad*

Main category: cs.CL

TL;DR: CoT提示法在应用商店评论的细粒度情感分类中显著提升准确率，从84%提高到93%。


<details>
  <summary>Details</summary>
Motivation: 传统数字和极性评分无法捕捉用户反馈中的细微情感。

Method: 比较CoT提示法和简单提示法在2000条亚马逊应用评论上的表现。

Result: CoT提示法将分类准确率从84%提升至93%。

Conclusion: 显式推理能显著提升情感分析性能。

Abstract: We explore the use of Chain-of-Thought (CoT) prompting with large language
models (LLMs) to improve the accuracy of granular sentiment categorization in
app store reviews. Traditional numeric and polarity-based ratings often fail to
capture the nuanced sentiment embedded in user feedback. We evaluated the
effectiveness of CoT prompting versus simple prompting on 2000 Amazon app
reviews by comparing each method's predictions to human judgements. CoT
prompting improved classification accuracy from 84% to 93% highlighting the
benefit of explicit reasoning in enhancing sentiment analysis performance.

</details>


### [11] [Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety](https://arxiv.org/abs/2505.04146)
*Variath Madhupal Gautham Nair,Vishal Varma Dantuluri*

Main category: cs.CL

TL;DR: 论文提出了一种动态可扩展的基准数据集UTCB，用于评估大语言模型在图像生成中的漏洞，结合多语言混淆和结构化提示工程。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在图像生成任务中表现优异，但其内容安全检查易受提示攻击，导致生成不良内容。

Method: 采用结构化提示工程、多语言混淆（如祖鲁语、盖尔语、Base64），并使用Groq托管的LLaMA-3进行评估，支持零样本和回退提示策略。

Result: UTCB数据集分为Bronze、Silver和Gold三个层级，支持动态更新，以应对新数据源和模型行为。

Conclusion: UTCB为评估和改进大语言模型的安全性提供了有效工具，需持续更新以应对新威胁。

Abstract: Existing large language models (LLMs) are advancing rapidly and produce
outstanding results in image generation tasks, yet their content safety checks
remain vulnerable to prompt-based jailbreaks. Through preliminary testing on
platforms such as ChatGPT, MetaAI, and Grok, we observed that even short,
natural prompts could lead to the generation of compromising images ranging
from realistic depictions of forged documents to manipulated images of public
figures.
  We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and
scalable benchmark dataset to evaluate LLM vulnerability in image generation.
Our methodology combines structured prompt engineering, multilingual
obfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted
LLaMA-3. The pipeline supports both zero-shot and fallback prompting
strategies, risk scoring, and automated tagging. All generations are stored
with rich metadata and curated into Bronze (non-verified), Silver (LLM-aided
verification), and Gold (manually verified) tiers. UTCB is designed to evolve
over time with new data sources, prompt templates, and model behaviors.
  Warning: This paper includes visual examples of adversarial inputs designed
to test model safety. All outputs have been redacted to ensure responsible
disclosure.

</details>


### [12] [Can Language Models Understand Social Behavior in Clinical Conversations?](https://arxiv.org/abs/2505.04152)
*Manas Satish Bedmutha,Feng Chen,Andrea Hartzler,Trevor Cohen,Nadir Weibel*

Main category: cs.CL

TL;DR: 论文探讨了利用大型语言模型（LLMs）自动分析医患对话中的社交信号，以提升沟通效果。


<details>
  <summary>Details</summary>
Motivation: 医患沟通的效果不仅依赖于临床信息交换，还受非语言社交信号影响。LLMs在文本分析中展现情感和社交行为推断能力，为自动化分析提供了可能。

Method: 设计了任务特定的提示，使用包含20种社交信号的标注数据集，评估不同架构和提示风格的LLMs性能。

Result: 开发了首个能追踪20种社交信号的系统，并揭示了LLMs的行为模式。

Conclusion: 通过分析模型配置和临床背景，为提升LLMs在医疗社交信号处理任务中的性能提供了见解。

Abstract: Effective communication between providers and their patients influences
health and care outcomes. The effectiveness of such conversations has been
linked not only to the exchange of clinical information, but also to a range of
interpersonal behaviors; commonly referred to as social signals, which are
often conveyed through non-verbal cues and shape the quality of the
patient-provider relationship. Recent advances in large language models (LLMs)
have demonstrated an increasing ability to infer emotional and social behaviors
even when analyzing only textual information. As automation increases also in
clinical settings, such as for transcription of patient-provider conversations,
there is growing potential for LLMs to automatically analyze and extract social
behaviors from these interactions. To explore the foundational capabilities of
LLMs in tracking social signals in clinical dialogue, we designed task-specific
prompts and evaluated model performance across multiple architectures and
prompting styles using a highly imbalanced, annotated dataset spanning 20
distinct social signals such as provider dominance, patient warmth, etc. We
present the first system capable of tracking all these 20 coded signals, and
uncover patterns in LLM behavior. Further analysis of model configurations and
clinical context provides insights for enhancing LLM performance on social
signal processing tasks in healthcare settings.

</details>


### [13] [LLM-Independent Adaptive RAG: Let the Question Speak for Itself](https://arxiv.org/abs/2505.04253)
*Maria Marina,Nikolay Ivanov,Sergey Pletenev,Mikhail Salnikov,Daria Galimzianova,Nikita Krayko,Vasily Konovalov,Alexander Panchenko,Viktor Moskvoretskii*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级、不依赖LLM的自适应检索方法，通过外部信息减少计算成本并避免错误信息，性能与复杂LLM方法相当但效率更高。


<details>
  <summary>Details</summary>
Motivation: LLM易产生幻觉，RAG虽能缓解但计算成本高且可能传播错误信息，现有自适应检索方法依赖LLM不确定性估计，效率低且不实用。

Method: 研究基于外部信息的轻量级自适应检索方法，探索27个特征（分7组）及其混合组合，在6个QA数据集上评估性能与效率。

Result: 结果显示，该方法性能与复杂LLM方法相当，同时显著提升效率，验证了外部信息在自适应检索中的潜力。

Conclusion: 轻量级自适应检索方法通过外部信息实现高效且准确的检索，为实际应用提供了可行方案。

Abstract: Large Language Models~(LLMs) are prone to hallucinations, and
Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high
computational cost while risking misinformation. Adaptive retrieval aims to
retrieve only when necessary, but existing approaches rely on LLM-based
uncertainty estimation, which remain inefficient and impractical. In this
study, we introduce lightweight LLM-independent adaptive retrieval methods
based on external information. We investigated 27 features, organized into 7
groups, and their hybrid combinations. We evaluated these methods on 6 QA
datasets, assessing the QA performance and efficiency. The results show that
our approach matches the performance of complex LLM-based methods while
achieving significant efficiency gains, demonstrating the potential of external
information for adaptive retrieval.

</details>


### [14] [GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance](https://arxiv.org/abs/2505.04284)
*Sofia Jamil,Aryan Dabad,Bollampalli Areen Reddy,Sriparna Saha,Rajiv Misra,Adil A. Shakur*

Main category: cs.CL

TL;DR: 该论文提出了一种针对癌症治疗中患者报告的不良药物事件（ADEs）的分组摘要任务，并引入了MCADRS数据集和GASCADE框架，结合LLMs和T5模型，提升了药物决策的效率和个性化癌症护理。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注一般疾病，缺乏针对癌症的不良药物事件总结方法，限制了药物决策的精准性。

Method: 提出MCADRS数据集和GASCADE框架，结合LLMs的信息提取能力和T5模型的摘要能力，并应用对齐技术。

Result: GASCADE在多种指标上表现优异，通过自动和人工评估验证。

Conclusion: 该研究为癌症药物决策和个性化护理提供了新工具，推动了相关领域的发展。

Abstract: In the realm of cancer treatment, summarizing adverse drug events (ADEs)
reported by patients using prescribed drugs is crucial for enhancing
pharmacovigilance practices and improving drug-related decision-making. While
the volume and complexity of pharmacovigilance data have increased, existing
research in this field has predominantly focused on general diseases rather
than specifically addressing cancer. This work introduces the task of grouped
summarization of adverse drug events reported by multiple patients using the
same drug for cancer treatment. To address the challenge of limited resources
in cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug
Reaction and Summarization (MCADRS) dataset. This dataset includes
pharmacovigilance posts detailing patient concerns regarding drug efficacy and
adverse effects, along with extracted labels for drug names, adverse drug
events, severity, and adversity of reactions, as well as summaries of ADEs for
each drug. Additionally, we propose the Grouping and Abstractive Summarization
of Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that
combines the information extraction capabilities of Large Language Models
(LLMs) with the summarization power of the encoder-decoder T5 model. Our work
is the first to apply alignment techniques, including advanced algorithms like
Direct Preference Optimization, to encoder-decoder models using synthetic
datasets for summarization tasks. Through extensive experiments, we demonstrate
the superior performance of GASCADE across various metrics, validated through
both automated assessments and human evaluations. This multitasking approach
enhances drug-related decision-making and fosters a deeper understanding of
patient concerns, paving the way for advancements in personalized and
responsive cancer care. The code and dataset used in this work are publicly
available.

</details>


### [15] [The Aloe Family Recipe for Open and Specialized Healthcare LLMs](https://arxiv.org/abs/2505.04388)
*Dario Garcia-Gasulla,Jordi Bayarri-Planas,Ashwin Kumar Gururajan,Enrique Lopez-Cuena,Adrian Tormos,Daniel Hinjos,Pablo Bernabeu-Perez,Anna Arias-Duart,Pablo Agustin Martin-Torres,Marta Gonzalez-Mallo,Sergio Alvarez-Napagao,Eduard Ayguadé-Parra,Ulises Cortés*

Main category: cs.CL

TL;DR: 论文提出了一种开源医疗大语言模型Aloe Beta，通过优化数据预处理和训练阶段，结合DPO和RAG提升模型安全性和效能，并定义了新的评估标准。


<details>
  <summary>Details</summary>
Motivation: 随着医疗领域大语言模型的发展，需要开源模型以保护公共利益。

Method: 基于Llama 3.1和Qwen 2.5等基础模型，使用自定义数据集和合成思维链示例，通过DPO对齐模型，并进行多类型评估。

Result: Aloe系列模型在医疗基准测试中表现优异，安全性显著提升，并附有详细风险评估。

Conclusion: Aloe Beta模型及其开发方法为开源医疗LLM领域树立了新标准，兼顾高性能与伦理要求。

Abstract: Purpose: With advancements in Large Language Models (LLMs) for healthcare,
the need arises for competitive open-source models to protect the public
interest. This work contributes to the field of open medical LLMs by optimizing
key stages of data preprocessing and training, while showing how to improve
model safety (through DPO) and efficacy (through RAG). The evaluation
methodology used, which includes four different types of tests, defines a new
standard for the field. The resultant models, shown to be competitive with the
best private alternatives, are released with a permisive license.
  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,
Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of
Thought examples. The models undergo alignment with Direct Preference
Optimization, emphasizing ethical and policy-aligned performance in the
presence of jailbreaking attacks. Evaluation includes close-ended, open-ended,
safety and human assessments, to maximize the reliability of results.
  Results: Recommendations are made across the entire pipeline, backed by the
solid performance of the Aloe Family. These models deliver competitive
performance across healthcare benchmarks and medical fields, and are often
preferred by healthcare professionals. On bias and toxicity, the Aloe Beta
models significantly improve safety, showing resilience to unseen jailbreaking
attacks. For a responsible release, a detailed risk assessment specific to
healthcare is attached to the Aloe Family models.
  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a
significant contribution to the open-source medical LLM field, offering
top-of-the-line performance while maintaining high ethical requirements. This
work sets a new standard for developing and reporting aligned LLMs in
healthcare.

</details>


### [16] [Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters](https://arxiv.org/abs/2505.04393)
*David Exler,Mark Schutera,Markus Reischl,Luca Rettenberger*

Main category: cs.CL

TL;DR: 论文量化了大型语言模型（LLMs）的政治偏见，发现其对左倾政党有偏好，并探讨了语言、模型来源和发布时间对偏见的影响。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的普及，评估LLMs的偏见至关重要，以避免其对用户决策和公共舆论的负面影响。

Method: 使用Wahl-O-Mat评分量化LLMs的政治偏见，分析模型大小、语言、来源和发布时间对偏见的影响。

Result: 发现LLMs对左倾政党有明显偏见，且语言和模型特性会影响其政治倾向。

Conclusion: LLMs存在政治偏见，开发公司需承担责任以减少这些偏见对公众的影响。

Abstract: With the increasing prevalence of artificial intelligence, careful evaluation
of inherent biases needs to be conducted to form the basis for alleviating the
effects these predispositions can have on users. Large language models (LLMs)
are predominantly used by many as a primary source of information for various
topics. LLMs frequently make factual errors, fabricate data (hallucinations),
or present biases, exposing users to misinformation and influencing opinions.
Educating users on their risks is key to responsible use, as bias, unlike
hallucinations, cannot be caught through data verification. We quantify the
political bias of popular LLMs in the context of the recent vote of the German
Bundestag using the score produced by the Wahl-O-Mat. This metric measures the
alignment between an individual's political views and the positions of German
political parties. We compare the models' alignment scores to identify factors
influencing their political preferences. Doing so, we discover a bias toward
left-leaning parties, most dominant in larger LLMs. Also, we find that the
language we use to communicate with the models affects their political views.
Additionally, we analyze the influence of a model's origin and release date and
compare the results to the outcome of the recent vote of the Bundestag. Our
results imply that LLMs are prone to exhibiting political bias. Large
corporations with the necessary means to develop LLMs, thus, knowingly or
unknowingly, have a responsibility to contain these biases, as they can
influence each voter's decision-making process and inform public opinion in
general and at scale.

</details>


### [17] [YABLoCo: Yet Another Benchmark for Long Context Code Generation](https://arxiv.org/abs/2505.04406)
*Aidar Valeev,Roman Garaev,Vadim Lomshakov,Irina Piontkovskaya,Vladimir Ivanov,Israel Adewuyi*

Main category: cs.CL

TL;DR: 论文提出了一个针对C和C++的长上下文代码生成基准（YABLoCo），填补了现有基准在大型代码库（200K至2,000K行代码）中的空白。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要针对小型或中型代码库，而实际软件项目可能包含数百万行代码，因此需要一个新的基准来评估大型代码库中的代码生成能力。

Method: 构建了一个包含215个函数的测试集，涵盖四种大型代码库，提供函数元数据、依赖上下文、文档字符串、函数体和调用图。

Result: 提出了一个可扩展的评估管道和代码可视化工具，支持在大型C和C++代码库中高效评估代码生成。

Conclusion: YABLoCo基准为评估大型代码库中的代码生成提供了全面支持，填补了现有研究的空白。

Abstract: Large Language Models demonstrate the ability to solve various programming
tasks, including code generation. Typically, the performance of LLMs is
measured on benchmarks with small or medium-sized context windows of thousands
of lines of code. At the same time, in real-world software projects,
repositories can span up to millions of LoC. This paper closes this gap by
contributing to the long context code generation benchmark (YABLoCo). The
benchmark featured a test set of 215 functions selected from four large
repositories with thousands of functions. The dataset contained metadata of
functions, contexts of the functions with different levels of dependencies,
docstrings, functions bodies, and call graphs for each repository. This paper
presents three key aspects of the contribution. First, the benchmark aims at
function body generation in large repositories in C and C++, two languages not
covered by previous benchmarks. Second, the benchmark contains large
repositories from 200K to 2,000K LoC. Third, we contribute a scalable
evaluation pipeline for efficient computing of the target metrics and a tool
for visual analysis of generated code. Overall, these three aspects allow for
evaluating code generation in large repositories in C and C++.

</details>


### [18] [OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models](https://arxiv.org/abs/2505.04416)
*Xiaoyu Xu,Minxin Du,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TL;DR: OBLIVIATE是一种高效的遗忘框架，用于从大型语言模型中移除敏感、受版权保护或有毒内容，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能记忆敏感或有害内容，需要一种方法来安全移除这些数据而不影响模型的其他功能。

Method: OBLIVIATE通过提取目标标记、构建保留集和使用包含掩码、蒸馏和世界事实的损失函数进行微调，结合LoRA技术提高效率。

Result: 实验表明，OBLIVIATE能有效抵抗成员推理攻击，减少对保留数据的影响，并在多样场景中保持鲁棒性。

Conclusion: OBLIVIATE提供了一种实用且高效的方法，用于从语言模型中安全移除目标数据。

Abstract: Large language models (LLMs) trained over extensive corpora risk memorizing
sensitive, copyrighted, or toxic content. To address this, we propose
OBLIVIATE, a robust unlearning framework that removes targeted data while
preserving model utility. The framework follows a structured process:
extracting target tokens, building retain sets, and fine-tuning with a tailored
loss function comprising three components -- masking, distillation, and world
fact. Using low-rank adapters (LoRA), it ensures efficiency without
compromising unlearning quality. We conduct experiments on multiple datasets,
including the Harry Potter series, WMDP, and TOFU, using a comprehensive suite
of metrics: forget quality (new document-level memorization score), model
utility, and fluency. Results demonstrate its effectiveness in resisting
membership inference attacks, minimizing the impact on retained data, and
maintaining robustness across diverse scenarios.

</details>


### [19] [Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts](https://arxiv.org/abs/2505.04507)
*Ilya Koziev*

Main category: cs.CL

TL;DR: 论文提出利用自动语言异常检测提升生成模型训练数据的质量，通过比较无监督和监督方法，并引入RUPOR数据集。


<details>
  <summary>Details</summary>
Motivation: 互联网来源的训练文本质量参差不齐，影响生成模型的性能，特别是在诗歌等创意任务中。

Method: 比较无监督和监督文本异常检测方法，使用合成和人工标注数据集，并引入RUPOR数据集。

Result: 提供了工具和见解，帮助提升创意领域生成模型的训练数据质量。

Conclusion: 自动化语言异常检测能有效改善训练数据集质量，推动创意生成模型的进步。

Abstract: The quality of natural language texts in fine-tuning datasets plays a
critical role in the performance of generative models, particularly in
computational creativity tasks such as poem or song lyric generation. Fluency
defects in generated poems significantly reduce their value. However, training
texts are often sourced from internet-based platforms without stringent quality
control, posing a challenge for data engineers to manage defect levels
effectively.
  To address this issue, we propose the use of automated linguistic anomaly
detection to identify and filter out low-quality texts from training datasets
for creative models. In this paper, we present a comprehensive comparison of
unsupervised and supervised text anomaly detection approaches, utilizing both
synthetic and human-labeled datasets. We also introduce the RUPOR dataset, a
collection of Russian-language human-labeled poems designed for cross-sentence
grammatical error detection, and provide the full evaluation code. Our work
aims to empower the community with tools and insights to improve the quality of
training datasets for generative models in creative domains.

</details>


### [20] [Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs](https://arxiv.org/abs/2505.04519)
*Yehui Tang,Yichun Yin,Yaoyuan Wang,Hang Zhou,Yu Pan,Wei Guo,Ziyang Zhang,Miao Rang,Fangcheng Liu,Naifu Zhang,Binghan Li,Yonghan Dong,Xiaojun Meng,Yasheng Wang,Dong Li,Yin Li,Dandan Tu,Can Chen,Youliang Yan,Fisher Yu,Ruiming Tang,Yunhe Wang,Botian Huang,Bo Wang,Boxiao Liu,Changzheng Zhang,Da Kuang,Fei Liu,Gang Huang,Jiansheng Wei,Jiarui Qin,Jie Ran,Jinpeng Li,Jun Zhao,Liang Dai,Lin Li,Liqun Deng,Peifeng Qin,Pengyuan Zeng,Qiang Gu,Shaohua Tang,Shengjun Cheng,Tao Gao,Tao Yu,Tianshu Li,Tianyu Bi,Wei He,Weikai Mao,Wenyong Huang,Wulong Liu,Xiabing Li,Xianzhi Yu,Xueyu Wu,Xu He,Yangkai Du,Yan Xu,Ye Tian,Yimeng Wu,Yongbing Huang,Yong Tian,Yong Zhu,Yue Li,Yufei Wang,Yuhang Gai,Yujun Li,Yu Luo,Yunsheng Ni,Yusen Sun,Zelin Chen,Zhe Liu,Zhicheng Liu,Zhipeng Tu,Zilin Ding,Zongyuan Zhan*

Main category: cs.CL

TL;DR: 论文提出了一种在Ascend NPUs上高效训练大规模稀疏语言模型（如Pangu Ultra MoE）的方法，通过模拟优化模型配置，优化通信和内存效率，最终实现了30.0%的MFU。


<details>
  <summary>Details</summary>
Motivation: 大规模稀疏语言模型（如MoE）在软件和硬件系统上面临巨大挑战，研究旨在探索如何在Ascend NPUs上高效利用计算资源。

Method: 利用模拟比较模型超参数，优化Expert Parallelism以减少通信开销，并提升设备内存效率。

Result: 训练Pangu Ultra MoE（718B参数）时实现了30.0%的MFU，性能与DeepSeek R1相当。

Conclusion: Ascend系统能够高效支持最先进语言模型的训练，为未来大规模稀疏模型研究提供了参考。

Abstract: Sparse large language models (LLMs) with Mixture of Experts (MoE) and close
to a trillion parameters are dominating the realm of most capable language
models. However, the massive model scale poses significant challenges for the
underlying software and hardware systems. In this paper, we aim to uncover a
recipe to harness such scale on Ascend NPUs. The key goals are better usage of
the computing resources under the dynamic sparse model structures and
materializing the expected performance gain on the actual hardware. To select
model configurations suitable for Ascend NPUs without repeatedly running the
expensive experiments, we leverage simulation to compare the trade-off of
various model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM
with 718 billion parameters, and we conducted experiments on the model to
verify the simulation results. On the system side, we dig into Expert
Parallelism to optimize the communication between NPU devices to reduce the
synchronization overhead. We also optimize the memory efficiency within the
devices to further reduce the parameter and activation management overhead. In
the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with
performance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and
demonstrate that the Ascend system is capable of harnessing all the training
stages of the state-of-the-art language models. Extensive experiments indicate
that our recipe can lead to efficient training of large-scale sparse language
models with MoE. We also study the behaviors of such models for future
reference.

</details>


### [21] [Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review](https://arxiv.org/abs/2505.04531)
*Josh McGiff,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 本文系统回顾了解决低资源语言生成模型数据稀缺的策略，总结了技术方法、架构选择及评估趋势，并提出了扩展方法和开放挑战的建议。


<details>
  <summary>Details</summary>
Motivation: 生成模型如ChatGPT和Google Gemini主要服务于高资源语言，加剧了NLP中的语言不平等问题。本文旨在填补低资源语言生成模型数据稀缺策略的系统性研究空白。

Method: 通过分析54项研究，分类评估了单语数据增强、回译、多语言训练和提示工程等技术方法，并分析了架构选择、语言族代表性和评估方法。

Result: 研究发现对基于Transformer的模型依赖性强，研究集中在少数低资源语言上，且评估方法不一致。

Conclusion: 建议扩展方法至更多低资源语言，并解决开放挑战，以支持构建包容性AI工具，保护语言多样性。

Abstract: Generative language modelling has surged in popularity with the emergence of
services such as ChatGPT and Google Gemini. While these models have
demonstrated transformative potential in productivity and communication, they
overwhelmingly cater to high-resource languages like English. This has
amplified concerns over linguistic inequality in natural language processing
(NLP). This paper presents the first systematic review focused specifically on
strategies to address data scarcity in generative language modelling for
low-resource languages (LRL). Drawing from 54 studies, we identify, categorise
and evaluate technical approaches, including monolingual data augmentation,
back-translation, multilingual training, and prompt engineering, across
generative tasks. We also analyse trends in architecture choices, language
family representation, and evaluation methods. Our findings highlight a strong
reliance on transformer-based models, a concentration on a small subset of
LRLs, and a lack of consistent evaluation across studies. We conclude with
recommendations for extending these methods to a wider range of LRLs and
outline open challenges in building equitable generative language systems.
Ultimately, this review aims to support researchers and developers in building
inclusive AI tools for underrepresented languages, a necessary step toward
empowering LRL speakers and the preservation of linguistic diversity in a world
increasingly shaped by large-scale language technologies.

</details>


### [22] [ZeroSearch: Incentivize the Search Capability of LLMs without Searching](https://arxiv.org/abs/2505.04588)
*Hao Sun,Zile Qiao,Jiayan Guo,Xuanbo Fan,Yingyan Hou,Yong Jiang,Pengjun Xie,Fei Huang,Yan Zhang*

Main category: cs.CL

TL;DR: ZeroSearch是一种强化学习框架，通过避免与真实搜索引擎交互来提升LLM的搜索能力，解决了文档质量不可控和API成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型（LLM）的搜索能力，同时解决现有方法中因搜索引擎返回文档质量不可控和API成本过高带来的挑战。

Method: 通过轻量级监督微调将LLM转化为检索模块，生成相关和噪声文档；在强化学习训练中采用基于课程的策略逐步降低生成文档质量。

Result: 实验表明，ZeroSearch能有效提升LLM的搜索能力，3B参数的LLM表现良好，7B参数模块性能接近真实搜索引擎，14B模块甚至超越。

Conclusion: ZeroSearch是一种高效且可扩展的解决方案，适用于不同参数规模的LLM和多种强化学习算法。

Abstract: Effective information searching is essential for enhancing the reasoning and
generation capabilities of large language models (LLMs). Recent research has
explored using reinforcement learning (RL) to improve LLMs' search capabilities
by interacting with live search engines in real-world environments. While these
approaches show promising results, they face two major challenges: (1)
Uncontrolled Document Quality: The quality of documents returned by search
engines is often unpredictable, introducing noise and instability into the
training process. (2) Prohibitively High API Costs: RL training requires
frequent rollouts, potentially involving hundreds of thousands of search
requests, which incur substantial API expenses and severely constrain
scalability. To address these challenges, we introduce ZeroSearch, a
reinforcement learning framework that incentivizes the search capabilities of
LLMs without interacting with real search engines. Our approach begins with
lightweight supervised fine-tuning to transform the LLM into a retrieval module
capable of generating both relevant and noisy documents in response to a query.
During RL training, we employ a curriculum-based rollout strategy that
incrementally degrades the quality of generated documents, progressively
eliciting the model's reasoning ability by exposing it to increasingly
challenging retrieval scenarios. Extensive experiments demonstrate that
ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B
LLM as the retrieval module. Remarkably, a 7B retrieval module achieves
comparable performance to the real search engine, while a 14B retrieval module
even surpasses it. Furthermore, it generalizes well across both base and
instruction-tuned models of various parameter sizes and is compatible with a
wide range of RL algorithms.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [23] [Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models](https://arxiv.org/abs/2505.03821)
*Gracjan Góral,Alicja Ziarko,Piotr Miłoś,Michał Nauman,Maciej Wołczyk,Michał Kosiński*

Main category: cs.CV

TL;DR: 研究探讨了视觉语言模型（VLMs）在视觉视角任务中的表现，发现其在场景理解上表现优异，但在空间推理和视角任务上表现较差。


<details>
  <summary>Details</summary>
Motivation: 评估VLMs在复杂视觉任务中的能力，尤其是空间推理和视角任务，以揭示其局限性。

Method: 通过设计144种视觉任务，结合7个诊断问题，测试多种先进模型（如GPT-4-Turbo、Llama-3.2等）。

Result: 模型在场景理解上表现良好，但在空间推理和视角任务上表现显著下降。

Conclusion: 未来VLM开发需整合显式几何表示和针对性训练，以提升复杂视觉任务能力。

Abstract: We investigate the ability of Vision Language Models (VLMs) to perform visual
perspective taking using a novel set of visual tasks inspired by established
human tests. Our approach leverages carefully controlled scenes, in which a
single humanoid minifigure is paired with a single object. By systematically
varying spatial configurations - such as object position relative to the
humanoid minifigure and the humanoid minifigure's orientation - and using both
bird's-eye and surface-level views, we created 144 unique visual tasks. Each
visual task is paired with a series of 7 diagnostic questions designed to
assess three levels of visual cognition: scene understanding, spatial
reasoning, and visual perspective taking. Our evaluation of several
state-of-the-art models, including GPT-4-Turbo, GPT-4o,
Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that
while they excel in scene understanding, the performance declines significantly
on spatial reasoning and further deteriorates on perspective-taking. Our
analysis suggests a gap between surface-level object recognition and the deeper
spatial and perspective reasoning required for complex visual tasks, pointing
to the need for integrating explicit geometric representations and tailored
training protocols in future VLM development.

</details>


### [24] [In-situ and Non-contact Etch Depth Prediction in Plasma Etching via Machine Learning (ANN & BNN) and Digital Image Colorimetry](https://arxiv.org/abs/2505.03826)
*Minji Kang,Seongho Kim,Eunseo Go,Donghyeon Paek,Geon Lim,Muyoung Kim,Soyeun Kim,Sung Kyu Jang,Min Sup Choi,Woo Seok Kang,Jaehyun Kim,Jaekwang Kim,Hyeong-U Kim*

Main category: cs.CV

TL;DR: 论文提出了一种基于机器学习的非接触式原位蚀刻深度预测框架，解决了传统方法的时间延迟和污染问题。


<details>
  <summary>Details</summary>
Motivation: 半导体制造中，精确监测蚀刻深度和绝缘材料厚度对设备性能和良率至关重要，但传统方法存在时间延迟和污染风险。

Method: 研究采用人工神经网络（ANN）和贝叶斯神经网络（BNN）预测蚀刻深度，并探索了使用数字图像比色法（DIC）数据的可行性。

Result: ANN在预测平均蚀刻深度时表现优于线性模型，BNN能可靠估计不确定性，DIC数据输入也表现出色。

Conclusion: 结合DIC和ML的方法为等离子蚀刻过程提供了实时、原位、非侵入式监测的可行方案，提升了工艺稳定性和制造效率。

Abstract: Precise monitoring of etch depth and the thickness of insulating materials,
such as Silicon dioxide and silicon nitride, is critical to ensuring device
performance and yield in semiconductor manufacturing. While conventional
ex-situ analysis methods are accurate, they are constrained by time delays and
contamination risks. To address these limitations, this study proposes a
non-contact, in-situ etch depth prediction framework based on machine learning
(ML) techniques. Two scenarios are explored. In the first scenario, an
artificial neural network (ANN) is trained to predict average etch depth from
process parameters, achieving a significantly lower mean squared error (MSE)
compared to a linear baseline model. The approach is then extended to
incorporate variability from repeated measurements using a Bayesian Neural
Network (BNN) to capture both aleatoric and epistemic uncertainty. Coverage
analysis confirms the BNN's capability to provide reliable uncertainty
estimates. In the second scenario, we demonstrate the feasibility of using RGB
data from digital image colorimetry (DIC) as input for etch depth prediction,
achieving strong performance even in the absence of explicit process
parameters. These results suggest that the integration of DIC and ML offers a
viable, cost-effective alternative for real-time, in-situ, and non-invasive
monitoring in plasma etching processes, contributing to enhanced process
stability, and manufacturing efficiency.

</details>


### [25] [VideoLLM Benchmarks and Evaluation: A Survey](https://arxiv.org/abs/2505.03829)
*Yogesh Kumar*

Main category: cs.CV

TL;DR: 本文综述了针对视频大语言模型（VideoLLMs）的基准和评估方法，分析了现有视频理解基准的特点、协议和局限性，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的快速发展，视频理解技术取得显著进展，但缺乏对VideoLLMs评估的系统性分析。本文旨在填补这一空白。

Method: 通过分析现有视频理解基准和评估方法（如闭集、开集和时空任务评估），总结VideoLLMs的性能趋势和评估框架的挑战。

Result: 揭示了当前评估框架的局限性，并提出了改进方向，如多样化、多模态和可解释性基准的设计。

Conclusion: 本文为研究者提供了评估VideoLLMs的系统性指南，并指出了推动视频理解领域发展的潜在方向。

Abstract: The rapid development of Large Language Models (LLMs) has catalyzed
significant advancements in video understanding technologies. This survey
provides a comprehensive analysis of benchmarks and evaluation methodologies
specifically designed or used for Video Large Language Models (VideoLLMs). We
examine the current landscape of video understanding benchmarks, discussing
their characteristics, evaluation protocols, and limitations. The paper
analyzes various evaluation methodologies, including closed-set, open-set, and
specialized evaluations for temporal and spatiotemporal understanding tasks. We
highlight the performance trends of state-of-the-art VideoLLMs across these
benchmarks and identify key challenges in current evaluation frameworks.
Additionally, we propose future research directions to enhance benchmark
design, evaluation metrics, and protocols, including the need for more diverse,
multimodal, and interpretability-focused benchmarks. This survey aims to equip
researchers with a structured understanding of how to effectively evaluate
VideoLLMs and identify promising avenues for advancing the field of video
understanding with large language models.

</details>


### [26] [Video Forgery Detection for Surveillance Cameras: A Review](https://arxiv.org/abs/2505.03832)
*Noor B. Tayfor,Tarik A. Rashid,Shko M. Qader,Bryar A. Hassan,Mohammed H. Abdalla,Jafar Majidpour,Aram M. Ahmed,Hussein M. Ali,Aso M. Aladdin,Abdulhady A. Abdullah,Ahmed S. Shamsaldin,Haval M. Sidqi,Abdulrahman Salih,Zaher M. Yaseen,Azad A. Ameen,Janmenjoy Nayak,Mahmood Yashar Hamza*

Main category: cs.CV

TL;DR: 本文综述了用于检测监控视频伪造的现有法医技术，强调了其验证视频真实性的有效性，并指出需要更强大的技术以应对不断演变的伪造方法。


<details>
  <summary>Details</summary>
Motivation: 随着视频编辑工具的普及，监控视频的篡改变得容易，威胁到其作为法律证据的可信度，因此需要确保视频的真实性。

Method: 探讨了多种技术，包括基于压缩的分析、帧重复检测和基于机器学习的方法。

Result: 研究发现现有技术虽有效，但需进一步发展以应对更复杂的伪造手段。

Conclusion: 加强视频法医能力是确保监控视频可信并可作为法律证据的关键。

Abstract: The widespread availability of video recording through smartphones and
digital devices has made video-based evidence more accessible than ever.
Surveillance footage plays a crucial role in security, law enforcement, and
judicial processes. However, with the rise of advanced video editing tools,
tampering with digital recordings has become increasingly easy, raising
concerns about their authenticity. Ensuring the integrity of surveillance
videos is essential, as manipulated footage can lead to misinformation and
undermine judicial decisions. This paper provides a comprehensive review of
existing forensic techniques used to detect video forgery, focusing on their
effectiveness in verifying the authenticity of surveillance recordings. Various
methods, including compression-based analysis, frame duplication detection, and
machine learning-based approaches, are explored. The findings highlight the
growing necessity for more robust forensic techniques to counteract evolving
forgery methods. Strengthening video forensic capabilities will ensure that
surveillance recordings remain credible and admissible as legal evidence.

</details>


### [27] [PointExplainer: Towards Transparent Parkinson's Disease Diagnosis](https://arxiv.org/abs/2505.03833)
*Xuechao Wang,Sven Nomm,Junqing Huang,Kadri Medijainen,Aaro Toomela,Michael Ruzhansky*

Main category: cs.CV

TL;DR: PointExplainer是一种可解释的诊断策略，用于识别手绘区域对模型诊断的影响，通过分配离散属性值量化其贡献，同时保持诊断性能。


<details>
  <summary>Details</summary>
Motivation: 现有诊断方法缺乏清晰的可解释性，影响了临床信任，因此需要一种可解释的诊断策略。

Method: PointExplainer包括诊断模块（将手绘信号编码为3D点云）和解释模块（训练可解释的替代模型近似黑盒模型行为），并引入一致性度量确保解释的忠实性。

Result: 在两个基准数据集和新构建的数据集上，PointExplainer能提供直观解释且不影响诊断性能。

Conclusion: PointExplainer通过可解释的诊断策略解决了黑盒模型的可信问题，同时保持了诊断准确性。

Abstract: Deep neural networks have shown potential in analyzing digitized hand-drawn
signals for early diagnosis of Parkinson's disease. However, the lack of clear
interpretability in existing diagnostic methods presents a challenge to
clinical trust. In this paper, we propose PointExplainer, an explainable
diagnostic strategy to identify hand-drawn regions that drive model diagnosis.
Specifically, PointExplainer assigns discrete attribution values to hand-drawn
segments, explicitly quantifying their relative contributions to the model's
decision. Its key components include: (i) a diagnosis module, which encodes
hand-drawn signals into 3D point clouds to represent hand-drawn trajectories,
and (ii) an explanation module, which trains an interpretable surrogate model
to approximate the local behavior of the black-box diagnostic model. We also
introduce consistency measures to further address the issue of faithfulness in
explanations. Extensive experiments on two benchmark datasets and a newly
constructed dataset show that PointExplainer can provide intuitive explanations
with no diagnostic performance degradation. The source code is available at
https://github.com/chaoxuewang/PointExplainer.

</details>


### [28] [Explainable Face Recognition via Improved Localization](https://arxiv.org/abs/2505.03837)
*Rashik Shadman,Daqing Hou,Faraz Hussain,M G Sarwar Murshed*

Main category: cs.CV

TL;DR: 论文提出了一种基于Scaled Directed Divergence (SDD)的可解释人脸识别方法，通过精细定位相关面部特征，提高深度学习模型的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的人脸识别系统缺乏解释性，用户难以信任其决策。本文旨在解决这一问题，提供透明的视觉解释。

Method: 采用SDD类激活映射技术，精细定位深度学习模型决策相关的面部特征，并与传统CAM方法对比。

Result: 实验表明，SDD CAM能更精确、更具体地突出相关面部特征，优于传统CAM。

Conclusion: SDD CAM提供的视觉解释增强了深度学习人脸识别系统的透明度和用户信任度。

Abstract: Biometric authentication has become one of the most widely used tools in the
current technological era to authenticate users and to distinguish between
genuine users and imposters. Face is the most common form of biometric modality
that has proven effective. Deep learning-based face recognition systems are now
commonly used across different domains. However, these systems usually operate
like black-box models that do not provide necessary explanations or
justifications for their decisions. This is a major disadvantage because users
cannot trust such artificial intelligence-based biometric systems and may not
feel comfortable using them when clear explanations or justifications are not
provided. This paper addresses this problem by applying an efficient method for
explainable face recognition systems. We use a Class Activation Mapping
(CAM)-based discriminative localization (very narrow/specific localization)
technique called Scaled Directed Divergence (SDD) to visually explain the
results of deep learning-based face recognition systems. We perform fine
localization of the face features relevant to the deep learning model for its
prediction/decision. Our experiments show that the SDD Class Activation Map
(CAM) highlights the relevant face features very specifically compared to the
traditional CAM and very accurately. The provided visual explanations with
narrow localization of relevant features can ensure much-needed transparency
and trust for deep learning-based face recognition systems.

</details>


### [29] [GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation](https://arxiv.org/abs/2505.03846)
*Kangsheng Wang,Yuhang Li,Chengwei Ye,Yufei Lin,Huanzhen Zhang,Bohan Hu,Linuo Xu,Shuyan Liu*

Main category: cs.CV

TL;DR: GAME是一种图增强多模态编码器，用于从短视频中预测人格特质，通过融合视觉、听觉和文本特征，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 短视频中的人格分析因多源特征的复杂交互而具有挑战性，需要一种鲁棒的多模态融合方法。

Method: GAME结合了图卷积网络（GCNs）和卷积神经网络（CNNs）的双分支Geo Two-Stream Network，以及BiGRU和注意力机制，融合视觉、音频和文本特征。

Result: GAME在多个基准测试中表现优于现有方法，验证了其有效性和泛化能力。

Conclusion: GAME为短视频人格分析提供了一种高效的多模态融合解决方案。

Abstract: Apparent personality analysis from short videos poses significant chal-lenges
due to the complex interplay of visual, auditory, and textual cues. In this
paper, we propose GAME, a Graph-Augmented Multimodal Encoder designed to
robustly model and fuse multi-source features for automatic personality
prediction. For the visual stream, we construct a facial graph and introduce a
dual-branch Geo Two-Stream Network, which combines Graph Convolutional Networks
(GCNs) and Convolutional Neural Net-works (CNNs) with attention mechanisms to
capture both structural and appearance-based facial cues. Complementing this,
global context and iden-tity features are extracted using pretrained ResNet18
and VGGFace back-bones. To capture temporal dynamics, frame-level features are
processed by a BiGRU enhanced with temporal attention modules. Meanwhile, audio
representations are derived from the VGGish network, and linguistic se-mantics
are captured via the XLM-Roberta transformer. To achieve effective multimodal
integration, we propose a Channel Attention-based Fusion module, followed by a
Multi-Layer Perceptron (MLP) regression head for predicting personality traits.
Extensive experiments show that GAME con-sistently outperforms existing methods
across multiple benchmarks, vali-dating its effectiveness and generalizability.

</details>


### [30] [Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques](https://arxiv.org/abs/2505.03848)
*Janhavi Giri,Attila Lengyel,Don Kent,Edward Kibardin*

Main category: cs.CV

TL;DR: 论文提出了一种结合深度拓扑数据分析（TDA）、自监督学习和迁移学习的聚类框架，用于半导体制造中的缺陷识别和图像聚类。


<details>
  <summary>Details</summary>
Motivation: 半导体制造中产生的大量图像数据难以通过人工检查处理，传统聚类方法对高维无标签数据效果不佳，需要更有效的解决方案。

Method: 框架整合了TDA、自监督学习和迁移学习，TDA提取拓扑特征，自监督学习从无标签数据中学习表示，迁移学习提升适应性和可扩展性。

Result: 在合成和开源半导体图像数据集上验证，框架成功识别出与缺陷模式相关的聚类。

Conclusion: 该研究展示了TDA、自监督学习和迁移学习结合的潜力，为半导体制造等领域的大规模图像数据分析提供了可扩展的解决方案。

Abstract: Semiconductor manufacturing generates vast amounts of image data, crucial for
defect identification and yield optimization, yet often exceeds manual
inspection capabilities. Traditional clustering techniques struggle with
high-dimensional, unlabeled data, limiting their effectiveness in capturing
nuanced patterns. This paper introduces an advanced clustering framework that
integrates deep Topological Data Analysis (TDA) with self-supervised and
transfer learning techniques, offering a novel approach to unsupervised image
clustering. TDA captures intrinsic topological features, while self-supervised
learning extracts meaningful representations from unlabeled data, reducing
reliance on labeled datasets. Transfer learning enhances the framework's
adaptability and scalability, allowing fine-tuning to new datasets without
retraining from scratch. Validated on synthetic and open-source semiconductor
image datasets, the framework successfully identifies clusters aligned with
defect patterns and process variations. This study highlights the
transformative potential of combining TDA, self-supervised learning, and
transfer learning, providing a scalable solution for proactive process
monitoring and quality control in semiconductor manufacturing and other domains
with large-scale image datasets.

</details>


### [31] [An Active Inference Model of Covert and Overt Visual Attention](https://arxiv.org/abs/2505.03856)
*Tin Mišić,Karlo Koledić,Fabio Bonsignorio,Ivan Petrović,Ivan Marković*

Main category: cs.CV

TL;DR: 该论文提出了一种基于主动推理的视觉注意力模型，通过动态优化感官精度来最小化自由能，研究了外源性和内源性注意力的交互作用，并验证了其在Posner提示任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究如何在复杂感官输入中选择性关注相关刺激并过滤干扰，这对于处理高维感官输入的智能体至关重要。

Method: 利用主动推理框架，动态优化感官精度，结合环境信念和感官输入分配注意力，并在Posner提示任务和简单目标聚焦任务中测试模型。

Result: 外源性和有效提示通常导致更快的反应时间，模型表现出类似返回抑制的行为，且反射性眼动比有意眼动更快但适应性较差。

Conclusion: 模型成功模拟了注意力的动态分配和眼动行为，为理解外源性和内源性注意力机制提供了新视角。

Abstract: The ability to selectively attend to relevant stimuli while filtering out
distractions is essential for agents that process complex, high-dimensional
sensory input. This paper introduces a model of covert and overt visual
attention through the framework of active inference, utilizing dynamic
optimization of sensory precisions to minimize free-energy. The model
determines visual sensory precisions based on both current environmental
beliefs and sensory input, influencing attentional allocation in both covert
and overt modalities. To test the effectiveness of the model, we analyze its
behavior in the Posner cueing task and a simple target focus task using
two-dimensional(2D) visual data. Reaction times are measured to investigate the
interplay between exogenous and endogenous attention, as well as valid and
invalid cueing. The results show that exogenous and valid cues generally lead
to faster reaction times compared to endogenous and invalid cues. Furthermore,
the model exhibits behavior similar to inhibition of return, where previously
attended locations become suppressed after a specific cue-target onset
asynchrony interval. Lastly, we investigate different aspects of overt
attention and show that involuntary, reflexive saccades occur faster than
intentional ones, but at the expense of adaptability.

</details>


### [32] [Novel Extraction of Discriminative Fine-Grained Feature to Improve Retinal Vessel Segmentation](https://arxiv.org/abs/2505.03896)
*Shuang Zeng,Chee Hong Lee,Micky C Nnamdi,Wenqi Shi,J Ben Tamo,Lei Zhu,Hangzhou He,Xinliang Zhang,Qian Chen,May D. Wang,Yanye Lu,Qiushi Ren*

Main category: cs.CV

TL;DR: 提出了一种名为AttUKAN的新型注意力U形Kolmogorov-Arnold网络和标签引导的像素级对比损失，用于视网膜血管分割，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注解码器输出与标签的差异，而忽略了编码器特征级细粒度表示的充分利用。

Method: 在Kolmogorov-Arnold网络中引入注意力门控增强模型敏感性和可解释性，并设计标签引导的像素级对比损失以提取更具区分性的特征。

Result: 在多个公共和私有数据集上取得最高F1和MIoU分数，优于11种现有网络。

Conclusion: AttUKAN在视网膜血管分割任务中实现了最先进的性能。

Abstract: Retinal vessel segmentation is a vital early detection method for several
severe ocular diseases. Despite significant progress in retinal vessel
segmentation with the advancement of Neural Networks, there are still
challenges to overcome. Specifically, retinal vessel segmentation aims to
predict the class label for every pixel within a fundus image, with a primary
focus on intra-image discrimination, making it vital for models to extract more
discriminative features. Nevertheless, existing methods primarily focus on
minimizing the difference between the output from the decoder and the label,
but ignore fully using feature-level fine-grained representations from the
encoder. To address these issues, we propose a novel Attention U-shaped
Kolmogorov-Arnold Network named AttUKAN along with a novel Label-guided
Pixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we
implement Attention Gates into Kolmogorov-Arnold Networks to enhance model
sensitivity by suppressing irrelevant feature activations and model
interpretability by non-linear modeling of KAN blocks. Additionally, we also
design a novel Label-guided Pixel-wise Contrastive Loss to supervise our
proposed AttUKAN to extract more discriminative features by distinguishing
between foreground vessel-pixel pairs and background pairs. Experiments are
conducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF
and our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%,
80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and
66.94% in the above datasets, which are the highest compared to 11 networks for
retinal vessel segmentation. Quantitative and qualitative results show that our
AttUKAN achieves state-of-the-art performance and outperforms existing retinal
vessel segmentation methods. Our code will be available at
https://github.com/stevezs315/AttUKAN.

</details>


### [33] [Deep Learning Framework for Infrastructure Maintenance: Crack Detection and High-Resolution Imaging of Infrastructure Surfaces](https://arxiv.org/abs/2505.03974)
*Nikhil M. Pawar,Jorge A. Prozzi,Feng Hong,Surya Sarat Chandra Congress*

Main category: cs.CV

TL;DR: 该研究提出了一种结合CNN和ESPCNN的框架，用于高效超分辨率和减少误报，以提升基础设施图像的质量和检测准确性。


<details>
  <summary>Details</summary>
Motivation: 无人机等数据采集平台在基础设施管理中应用广泛，但图像分辨率低、误报率高的问题限制了其效果。

Method: 使用CNN分类正负损伤图像，再用轻量级ESPCNN对正损伤图像进行超分辨率处理。

Result: ESPCNN在超分辨率性能上优于双三次插值，且框架有效减少了计算成本和误报。

Conclusion: 该框架有望帮助高速公路机构更准确地进行损伤检测和资产管理。

Abstract: Recently, there has been an impetus for the application of cutting-edge data
collection platforms such as drones mounted with camera sensors for
infrastructure asset management. However, the sensor characteristics, proximity
to the structure, hard-to-reach access, and environmental conditions often
limit the resolution of the datasets. A few studies used super-resolution
techniques to address the problem of low-resolution images. Nevertheless, these
techniques were observed to increase computational cost and false alarms of
distress detection due to the consideration of all the infrastructure images
i.e., positive and negative distress classes. In order to address the
pre-processing of false alarm and achieve efficient super-resolution, this
study developed a framework consisting of convolutional neural network (CNN)
and efficient sub-pixel convolutional neural network (ESPCNN). CNN accurately
classified both the classes. ESPCNN, which is the lightweight super-resolution
technique, generated high-resolution infrastructure image of positive distress
obtained from CNN. The ESPCNN outperformed bicubic interpolation in all the
evaluation metrics for super-resolution. Based on the performance metrics, the
combination of CNN and ESPCNN was observed to be effective in preprocessing the
infrastructure images with negative distress, reducing the computational cost
and false alarms in the next step of super-resolution. The visual inspection
showed that EPSCNN is able to capture crack propagation, complex geometry of
even minor cracks. The proposed framework is expected to help the highway
agencies in accurately performing distress detection and assist in efficient
asset management practices.

</details>


### [34] [Action Spotting and Precise Event Detection in Sports: Datasets, Methods, and Challenges](https://arxiv.org/abs/2505.03991)
*Hao Xu,Arbind Agrahari Baniya,Sam Well,Mohamed Reda Bouadjenek,Richard Dazeley,Sunil Aryal*

Main category: cs.CV

TL;DR: 这篇论文综述了视频事件检测在体育分析中的重要性，重点介绍了三种关键任务（TAL、AS、PES）及其方法、数据集和评估指标，并探讨了多模态技术和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 体育分析中视频事件检测的需求日益增长，深度学习技术的进步为自动化识别关键时刻提供了可能，但需要系统性的综述来指导未来研究。

Method: 论文通过综述和分类现有数据集、评估指标及技术方法（如多模态、自监督学习和知识蒸馏），分析其优缺点。

Result: 总结了当前最先进的技术，并指出了体育事件检测中的开放挑战和未来研究方向。

Conclusion: 该综述为未来开发更通用、高效和鲁棒的体育事件检测框架奠定了基础。

Abstract: Video event detection has become an essential component of sports analytics,
enabling automated identification of key moments and enhancing performance
analysis, viewer engagement, and broadcast efficiency. Recent advancements in
deep learning, particularly Convolutional Neural Networks (CNNs) and
Transformers, have significantly improved accuracy and efficiency in Temporal
Action Localization (TAL), Action Spotting (AS), and Precise Event Spotting
(PES). This survey provides a comprehensive overview of these three key tasks,
emphasizing their differences, applications, and the evolution of
methodological approaches. We thoroughly review and categorize existing
datasets and evaluation metrics specifically tailored for sports contexts,
highlighting the strengths and limitations of each. Furthermore, we analyze
state-of-the-art techniques, including multi-modal approaches that integrate
audio and visual information, methods utilizing self-supervised learning and
knowledge distillation, and approaches aimed at generalizing across multiple
sports. Finally, we discuss critical open challenges and outline promising
research directions toward developing more generalized, efficient, and robust
event detection frameworks applicable to diverse sports. This survey serves as
a foundation for future research on efficient, generalizable, and multi-modal
sports event detection.

</details>


### [35] [The Eye as a Window to Systemic Health: A Survey of Retinal Imaging from Classical Techniques to Oculomics](https://arxiv.org/abs/2505.04006)
*Inamullah,Imran Razzak,Shoaib Jameel*

Main category: cs.CV

TL;DR: 视网膜成像技术结合AI分析，为眼部和全身疾病提供早期检测和干预的新方法，推动了眼科学的新领域——眼组学的发展。


<details>
  <summary>Details</summary>
Motivation: 利用视网膜独特的血管化结构作为健康监测窗口，结合AI技术填补眼与健康之间的研究空白。

Method: 综述视网膜成像技术的演变、AI驱动的分析需求，以及从传统技术向眼组学的转变。

Result: 眼组学为眼部和全身疾病提供了非侵入性标记物，揭示了系统性健康信息。

Conclusion: 眼组学发展面临挑战，需填补研究空白并探索未来方向。

Abstract: The unique vascularized anatomy of the human eye, encased in the retina,
provides an opportunity to act as a window for human health. The retinal
structure assists in assessing the early detection, monitoring of disease
progression and intervention for both ocular and non-ocular diseases. The
advancement in imaging technology leveraging Artificial Intelligence has seized
this opportunity to bridge the gap between the eye and human health. This track
paves the way for unveiling systemic health insight from the ocular system and
surrogating non-invasive markers for timely intervention and identification.
The new frontiers of oculomics in ophthalmology cover both ocular and systemic
diseases, and getting more attention to explore them. In this survey paper, we
explore the evolution of retinal imaging techniques, the dire need for the
integration of AI-driven analysis, and the shift of retinal imaging from
classical techniques to oculomics. We also discuss some hurdles that may be
faced in the progression of oculomics, highlighting the research gaps and
future directions.

</details>


### [36] [FoodTrack: Estimating Handheld Food Portions with Egocentric Video](https://arxiv.org/abs/2505.04055)
*Ervin Wang,Yuhao Chen*

Main category: cs.CV

TL;DR: FoodTrack框架通过第一视角视频直接测量手持食物体积，克服了传统方法对手势或固定角度的依赖，提高了食物消费跟踪的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统食物消费跟踪方法依赖特定摄像头角度或手势识别，无法直接测量食物体积，限制了准确性和灵活性。

Method: 提出FoodTrack框架，利用第一视角视频直接估计食物体积，无需依赖手势或固定假设。

Result: 在手持食物对象上实现了约7.01%的绝对百分比误差，优于之前方法的16.40%。

Conclusion: FoodTrack提供了一种更准确、适应性更强的食物消费跟踪解决方案。

Abstract: Accurately tracking food consumption is crucial for nutrition and health
monitoring. Traditional approaches typically require specific camera angles,
non-occluded images, or rely on gesture recognition to estimate intake, making
assumptions about bite size rather than directly measuring food volume. We
propose the FoodTrack framework for tracking and measuring the volume of
hand-held food items using egocentric video which is robust to hand occlusions
and flexible with varying camera and object poses. FoodTrack estimates food
volume directly, without relying on intake gestures or fixed assumptions about
bite size, offering a more accurate and adaptable solution for tracking food
consumption. We achieve absolute percentage loss of approximately 7.01% on a
handheld food object, improving upon a previous approach that achieved a 16.40%
mean absolute percentage error in its best case, under less flexible
conditions.

</details>


### [37] [AS3D: 2D-Assisted Cross-Modal Understanding with Semantic-Spatial Scene Graphs for 3D Visual Grounding](https://arxiv.org/abs/2505.04058)
*Feng Xiao,Hongbin Xu,Guocan Zhao,Wenxiong Kang*

Main category: cs.CV

TL;DR: 提出了一种2D辅助的3D视觉定位框架，通过构建语义-空间场景图和双分支视觉编码器，提升多模态对象编码和关系感知能力。


<details>
  <summary>Details</summary>
Motivation: 3D视觉定位中，语言和3D模态间的显著差异使得通过空间关系区分多个相似物体成为挑战。现有方法忽略了对参考对象的感知。

Method: 提出2D辅助的3D视觉定位框架，包括双分支视觉编码器和跨模态交互模块，利用图注意力促进关系导向的信息融合。

Result: 在主流基准测试中表现优于现有方法，尤其在处理多个相似干扰物时效果显著。

Conclusion: 该框架通过增强对象表示和迭代关系学习，实现了3D视觉与语言描述的有效对齐。

Abstract: 3D visual grounding aims to localize the unique target described by natural
languages in 3D scenes. The significant gap between 3D and language modalities
makes it a notable challenge to distinguish multiple similar objects through
the described spatial relationships. Current methods attempt to achieve
cross-modal understanding in complex scenes via a target-centered learning
mechanism, ignoring the perception of referred objects. We propose a novel
2D-assisted 3D visual grounding framework that constructs semantic-spatial
scene graphs with referred object discrimination for relationship perception.
The framework incorporates a dual-branch visual encoder that utilizes 2D
pre-trained attributes to guide the multi-modal object encoding. Furthermore,
our cross-modal interaction module uses graph attention to facilitate
relationship-oriented information fusion. The enhanced object representation
and iterative relational learning enable the model to establish effective
alignment between 3D vision and referential descriptions. Experimental results
on the popular benchmarks demonstrate our superior performance compared to
state-of-the-art methods, especially in addressing the challenges of multiple
similar distractors.

</details>


### [38] [SEVA: Leveraging Single-Step Ensemble of Vicinal Augmentations for Test-Time Adaptation](https://arxiv.org/abs/2505.04087)
*Zixuan Hu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

TL;DR: 论文提出了一种名为SEVA的新方法，通过单步集成邻近增强策略，在不增加计算负担的情况下提升测试时适应（TTA）的效率。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法通常依赖基于熵的无监督训练，但单轮训练难以充分利用可靠样本，限制了适应效率。

Method: SEVA通过理论框架探索多轮增强对模型适应的影响，并优化熵损失的上界，将多轮增强训练效果集成到单步中。

Result: 实验表明，SEVA在多种网络架构和测试场景下表现出色，且具有广泛适应性。

Conclusion: SEVA通过高效损失和互补选择策略，显著提升了可靠样本的潜力，同时满足TTA的实时要求。

Abstract: Test-Time adaptation (TTA) aims to enhance model robustness against
distribution shifts through rapid model adaptation during inference. While
existing TTA methods often rely on entropy-based unsupervised training and
achieve promising results, the common practice of a single round of entropy
training is typically unable to adequately utilize reliable samples, hindering
adaptation efficiency. In this paper, we discover augmentation strategies can
effectively unleash the potential of reliable samples, but the rapidly growing
computational cost impedes their real-time application. To address this
limitation, we propose a novel TTA approach named Single-step Ensemble of
Vicinal Augmentations (SEVA), which can take advantage of data augmentations
without increasing the computational burden. Specifically, instead of
explicitly utilizing the augmentation strategy to generate new data, SEVA
develops a theoretical framework to explore the impacts of multiple
augmentations on model adaptation and proposes to optimize an upper bound of
the entropy loss to integrate the effects of multiple rounds of augmentation
training into a single step. Furthermore, we discover and verify that using the
upper bound as the loss is more conducive to the selection mechanism, as it can
effectively filter out harmful samples that confuse the model. Combining these
two key advantages, the proposed efficient loss and a complementary selection
strategy can simultaneously boost the potential of reliable samples and meet
the stringent time requirements of TTA. The comprehensive experiments on
various network architectures across challenging testing scenarios demonstrate
impressive performances and the broad adaptability of SEVA. The code will be
publicly available.

</details>


### [39] [SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking](https://arxiv.org/abs/2505.04088)
*Shang Zhang,Huanbin Zhang,Dali Feng,Yujie Cui,Ruoyan Xiong,Cen He*

Main category: cs.CV

TL;DR: 提出了一种新型的Siamese Motion Mamba Tracker (SMMT)，结合双向状态空间模型和自注意力机制，解决了TIR目标跟踪中的遮挡、运动模糊和背景干扰问题。


<details>
  <summary>Details</summary>
Motivation: TIR目标跟踪常因目标遮挡、运动模糊和背景干扰导致性能下降，需改进现有方法。

Method: 引入Motion Mamba模块和Siamese参数共享策略，结合双向建模和自注意力机制提取运动特征并恢复边缘细节。

Result: 在四个TIR跟踪基准测试中表现优异，显著提升了跟踪精度。

Conclusion: SMMT通过创新设计和优化策略，有效提升了TIR目标跟踪的性能。

Abstract: Thermal infrared (TIR) object tracking often suffers from challenges such as
target occlusion, motion blur, and background clutter, which significantly
degrade the performance of trackers. To address these issues, this paper
pro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a
bidirectional state-space model and a self-attention mechanism. Specifically,
we introduce the Motion Mamba module into the Siamese architecture to ex-tract
motion features and recover overlooked edge details using bidirectional
modeling and self-attention. We propose a Siamese parameter-sharing strate-gy
that allows certain convolutional layers to share weights. This approach
reduces computational redundancy while preserving strong feature
represen-tation. In addition, we design a motion edge-aware regression loss to
improve tracking accuracy, especially for motion-blurred targets. Extensive
experi-ments are conducted on four TIR tracking benchmarks, including
LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT
achieves superior performance in TIR target tracking.

</details>


### [40] [MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction](https://arxiv.org/abs/2505.04105)
*Andrew Zhang,Hao Wang,Shuchang Ye,Michael Fulham,Jinman Kim*

Main category: cs.CV

TL;DR: 论文提出MAISY方法，通过动态学习运动特征和引入VS-SSIM损失，显著提升医学图像去运动伪影的效果。


<details>
  <summary>Details</summary>
Motivation: 现有GAN方法在去运动伪影时忽视局部特征且SSIM损失对像素强度变化敏感，导致效果受限。

Method: 结合Segment Anything Model动态学习运动特征，并设计VS-SSIM损失自适应处理高方差区域。

Result: 在胸部和头部CT数据上，PSNR提升40%，SSIM提升10%，Dice提升16%。

Conclusion: MAISY方法在去运动伪影任务中优于现有方法，尤其擅长保留关键解剖细节。

Abstract: Patient motion during medical image acquisition causes blurring, ghosting,
and distorts organs, which makes image interpretation challenging.Current
state-of-the-art algorithms using Generative Adversarial Network (GAN)-based
methods with their ability to learn the mappings between corrupted images and
their ground truth via Structural Similarity Index Measure (SSIM) loss
effectively generate motion-free images. However, we identified the following
limitations: (i) they mainly focus on global structural characteristics and
therefore overlook localized features that often carry critical pathological
information, and (ii) the SSIM loss function struggles to handle images with
varying pixel intensities, luminance factors, and variance. In this study, we
propose Motion-Aware Image SYnthesis (MAISY) which initially characterize
motion and then uses it for correction by: (a) leveraging the foundation model
Segment Anything Model (SAM), to dynamically learn spatial patterns along
anatomical boundaries where motion artifacts are most pronounced and, (b)
introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively
emphasizes spatial regions with high pixel variance to preserve essential
anatomical details during artifact correction. Experiments on chest and head CT
datasets demonstrate that our model outperformed the state-of-the-art
counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by
10%, and Dice by 16%.

</details>


### [41] [One2Any: One-Reference 6D Pose Estimation for Any Object](https://arxiv.org/abs/2505.04109)
*Mengya Liu,Siyuan Li,Ajad Chhatkuli,Prune Truong,Luc Van Gool,Federico Tombari*

Main category: cs.CV

TL;DR: One2Any是一种新方法，通过单参考-单查询RGB-D图像估计6自由度物体姿态，无需3D模型或多视图数据。


<details>
  <summary>Details</summary>
Motivation: 解决6D物体姿态估计对完整3D模型、多视图图像或特定类别训练的依赖问题，提高对新物体的泛化能力。

Method: 将姿态估计视为编码-解码过程，通过参考物体姿态嵌入（ROPE）和U-Net解码模块生成新视图的参考物体坐标（ROC）。

Result: 在多个基准数据集上表现优异，泛化能力强，计算效率高，甚至优于需要多视图或CAD输入的方法。

Conclusion: One2Any方法简单高效，适用于大规模训练，具有广泛的应用潜力。

Abstract: 6D object pose estimation remains challenging for many applications due to
dependencies on complete 3D models, multi-view images, or training limited to
specific object categories. These requirements make generalization to novel
objects difficult for which neither 3D models nor multi-view images may be
available. To address this, we propose a novel method One2Any that estimates
the relative 6-degrees of freedom (DOF) object pose using only a single
reference-single query RGB-D image, without prior knowledge of its 3D model,
multi-view data, or category constraints. We treat object pose estimation as an
encoding-decoding process, first, we obtain a comprehensive Reference Object
Pose Embedding (ROPE) that encodes an object shape, orientation, and texture
from a single reference view. Using this embedding, a U-Net-based pose decoding
module produces Reference Object Coordinate (ROC) for new views, enabling fast
and accurate pose estimation. This simple encoding-decoding framework allows
our model to be trained on any pair-wise pose data, enabling large-scale
training and demonstrating great scalability. Experiments on multiple benchmark
datasets demonstrate that our model generalizes well to novel objects,
achieving state-of-the-art accuracy and robustness even rivaling methods that
require multi-view or CAD inputs, at a fraction of compute.

</details>


### [42] [GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model](https://arxiv.org/abs/2505.04119)
*Zixiang Ai,Zichen Liu,Yuanhang Lei,Zhenyu Cui,Xu Zou,Jiahuan Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种几何感知的点云提示方法（GAPrompt），通过几何线索增强3D视觉模型的适应性，显著优于现有参数高效微调方法，且仅需少量可训练参数。


<details>
  <summary>Details</summary>
Motivation: 预训练的3D视觉模型在点云数据上表现优异，但完全微调计算和存储成本高。现有参数高效微调方法因难以捕捉几何信息而性能不足。

Method: 提出GAPrompt，包括点提示（Point Prompt）辅助输入捕捉几何细节，点移位提示器（Point Shift Prompter）提取全局形状信息，以及提示传播机制（Prompt Propagation）增强特征提取。

Result: 实验表明，GAPrompt显著优于现有方法，性能接近完全微调，仅需2.19%的可训练参数。

Conclusion: GAPrompt通过几何感知提示有效提升了3D视觉模型的适应性，为参数高效微调提供了新思路。

Abstract: Pre-trained 3D vision models have gained significant attention for their
promising performance on point cloud data. However, fully fine-tuning these
models for downstream tasks is computationally expensive and storage-intensive.
Existing parameter-efficient fine-tuning (PEFT) approaches, which focus
primarily on input token prompting, struggle to achieve competitive performance
due to their limited ability to capture the geometric information inherent in
point clouds. To address this challenge, we propose a novel Geometry-Aware
Point Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the
adaptability of 3D vision models. First, we introduce a Point Prompt that
serves as an auxiliary input alongside the original point cloud, explicitly
guiding the model to capture fine-grained geometric details. Additionally, we
present a Point Shift Prompter designed to extract global shape information
from the point cloud, enabling instance-specific geometric adjustments at the
input level. Moreover, our proposed Prompt Propagation mechanism incorporates
the shape information into the model's feature extraction process, further
strengthening its ability to capture essential geometric characteristics.
Extensive experiments demonstrate that GAPrompt significantly outperforms
state-of-the-art PEFT methods and achieves competitive results compared to full
fine-tuning on various benchmarks, while utilizing only 2.19% of trainable
parameters. Our code is available at
https://github.com/zhoujiahuan1991/ICML2025-VGP.

</details>


### [43] [Vision Graph Prompting via Semantic Low-Rank Decomposition](https://arxiv.org/abs/2505.04121)
*Zixiang Ai,Zichen Liu,Jiahuan Zhou*

Main category: cs.CV

TL;DR: ViG通过图结构表示图像，优于传统网格或序列方法。现有提示方法忽略图结构的拓扑关系，本文提出VGP框架，利用低秩语义特征提升ViG在下游任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉提示方法主要针对Transformer模型，忽略了图结构的拓扑关系，限制了复杂语义建模能力。

Method: 提出VGP框架，基于低秩语义特征分解，结合视觉图拓扑提示，捕捉全局结构和细粒度语义依赖。

Result: 实验显示VGP显著提升ViG在下游任务中的迁移性能，接近全微调效果且参数高效。

Conclusion: VGP为图结构视觉模型提供了一种高效的提示方法，平衡性能和参数效率。

Abstract: Vision GNN (ViG) demonstrates superior performance by representing images as
graph structures, providing a more natural way to capture irregular semantic
patterns beyond traditional grid or sequence-based representations. To
efficiently adapt ViG to downstream tasks, parameter-efficient fine-tuning
techniques like visual prompting become increasingly essential. However,
existing prompting methods are primarily designed for Transformer-based models,
neglecting the rich topological relationships among nodes and edges in
graph-based representations, limiting their capacity to model complex
semantics. In this paper, we propose Vision Graph Prompting (VGP), a novel
framework tailored for vision graph structures. Our core insight reveals that
semantically connected components in the graph exhibit low-rank properties.
Building on this observation, we introduce a semantic low-rank prompting method
that decomposes low-rank semantic features and integrates them with prompts on
vision graph topologies, capturing both global structural patterns and
fine-grained semantic dependencies. Extensive experiments demonstrate our
method significantly improves ViG's transfer performance on diverse downstream
tasks, achieving results comparable to full fine-tuning while maintaining
parameter efficiency. Our code is available at
https://github.com/zhoujiahuan1991/ICML2025-VGP.

</details>


### [44] [R^3-VQA: "Read the Room" by Video Social Reasoning](https://arxiv.org/abs/2505.04147)
*Lixing Niu,Jiapeng Li,Xingping Yu,Shu Wang,Ruining Feng,Bo Wu,Ping Wei,Yisen Wang,Lifeng Fan*

Main category: cs.CV

TL;DR: 论文提出了一个高质量的视频数据集R^3-VQA，用于评估复杂社交场景中的社会推理能力，并发现当前大型视觉语言模型在社交推理上仍远不及人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有社交推理任务和数据集缺乏复杂性，无法反映真实社交互动的挑战，因此需要更全面的数据集和任务。

Method: 构建了R^3-VQA数据集，包含精细标注的社交事件、心理状态和社交因果链，并设计了三个任务：社交事件理解、心理状态估计和社交因果推理。

Result: 实验表明，当前大型视觉语言模型在复杂社交推理中表现不佳，但通过心理理论提示可以提升其性能。

Conclusion: R^3-VQA为社交推理研究提供了新基准，揭示了现有模型的局限性，并提出了改进方向。

Abstract: "Read the room" is a significant social reasoning capability in human daily
life. Humans can infer others' mental states from subtle social cues. Previous
social reasoning tasks and datasets lack complexity (e.g., simple scenes, basic
interactions, incomplete mental state variables, single-step reasoning, etc.)
and fall far short of the challenges present in real-life social interactions.
In this paper, we contribute a valuable, high-quality, and comprehensive video
dataset named R^3-VQA with precise and fine-grained annotations of social
events and mental states (i.e., belief, intent, desire, and emotion) as well as
corresponding social causal chains in complex social scenarios. Moreover, we
include human-annotated and model-generated QAs. Our task R^3-VQA includes
three aspects: Social Event Understanding, Mental State Estimation, and Social
Causal Reasoning. As a benchmark, we comprehensively evaluate the social
reasoning capabilities and consistencies of current state-of-the-art large
vision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs
are still far from human-level consistent social reasoning in complex social
scenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on
social reasoning tasks. We provide some of our dataset and codes in
supplementary material and will release our full dataset and codes upon
acceptance.

</details>


### [45] [Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages](https://arxiv.org/abs/2505.04150)
*Yu Yamaoka or Weng Ian Chan,Shigeto Seno,Soichiro Fukada,Hideo Matsuda*

Main category: cs.CV

TL;DR: 论文提出了一种名为OSLSP的弱监督学习方法，用于自动化评估肌肉组织再生过程，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统肌肉组织再生评估依赖人工视觉检查，缺乏定量和客观性，且现有弱监督学习方法无法适应肌肉组织特征和保留类别顺序信息。

Method: 提出OSLSP方法，利用相似性比例损失和类别比例注意力机制，更新特征提取器并保留类别顺序信息。

Result: OSLSP模型在骨骼肌恢复阶段分类任务中表现优于大规模预训练和微调模型。

Conclusion: OSLSP为肌肉组织再生评估提供了一种自动化、定量且保留顺序信息的解决方案。

Abstract: Evaluating the regeneration process of damaged muscle tissue is a fundamental
analysis in muscle research to measure experimental effect sizes and uncover
mechanisms behind muscle weakness due to aging and disease. The conventional
approach to assessing muscle tissue regeneration involves whole-slide imaging
and expert visual inspection of the recovery stages based on the morphological
information of cells and fibers. There is a need to replace these tasks with
automated methods incorporating machine learning techniques to ensure a
quantitative and objective analysis. Given the limited availability of fully
labeled data, a possible approach is Learning from Label Proportions (LLP), a
weakly supervised learning method using class label proportions. However,
current LLP methods have two limitations: (1) they cannot adapt the feature
extractor for muscle tissues, and (2) they treat the classes representing
recovery stages and cell morphological changes as nominal, resulting in the
loss of ordinal information. To address these issues, we propose Ordinal Scale
Learning from Similarity Proportion (OSLSP), which uses a similarity proportion
loss derived from two bag combinations. OSLSP can update the feature extractor
by using class proportion attention to the ordinal scale of the class. Our
model with OSLSP outperforms large-scale pre-trained and fine-tuning models in
classification tasks of skeletal muscle recovery stages.

</details>


### [46] [DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation](https://arxiv.org/abs/2505.04175)
*Naphat Nithisopa,Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: 本文提出了一种结合ResNet和Vision Transformer的新型端到端框架，用于提升自然图像中的文本识别性能，通过Deformable Convolutions、Retrieval-Augmented Generation和CRF等方法显著提高了OCR效果。


<details>
  <summary>Details</summary>
Motivation: 自然图像中的文本识别是一个重要但具有挑战性的任务，广泛应用于计算机视觉和自然语言处理领域。

Method: 框架采用ResNet和Vision Transformer作为主干网络，引入Deformable Convolutions、自适应dropout和CRF等技术优化特征表示和序列建模。

Result: 在六个基准数据集上验证了方法的有效性，平均准确率达77.77%，部分数据集表现优异（如IC13达97.32%）。

Conclusion: 该方法在文本识别任务中达到了新的最优性能，展现了其鲁棒性和广泛适用性。

Abstract: Text recognition in natural images remains a challenging yet essential task,
with broad applications spanning computer vision and natural language
processing. This paper introduces a novel end-to-end framework that combines
ResNet and Vision Transformer backbones with advanced methodologies, including
Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random
Fields (CRF). These innovations collectively enhance feature representation and
improve Optical Character Recognition (OCR) performance. Specifically, the
framework substitutes standard convolution layers in the third and fourth
blocks with Deformable Convolutions, leverages adaptive dropout for
regularization, and incorporates CRF for more refined sequence modeling.
Extensive experiments conducted on six benchmark datasets IC13, IC15, SVT,
IIIT5K, SVTP, and CUTE80 validate the proposed method's efficacy, achieving
notable accuracies: 97.32% on IC13, 58.26% on IC15, 88.10% on SVT, 74.13% on
IIIT5K, 82.17% on SVTP, and 66.67% on CUTE80, resulting in an average accuracy
of 77.77%. These results establish a new state-of-the-art for text recognition,
demonstrating the robustness of the approach across diverse and challenging
datasets.

</details>


### [47] [S3D: Sketch-Driven 3D Model Generation](https://arxiv.org/abs/2505.04185)
*Hail Song,Wonsik Shin,Naeun Lee,Soomin Chung,Nojun Kwak,Woontack Woo*

Main category: cs.CV

TL;DR: S3D框架通过U-Net架构将手绘草图转换为3D模型，引入风格对齐损失提升重建质量，并通过数据增强提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决从稀疏且模糊的2D草图生成高质量3D模型的挑战。

Method: 使用U-Net编码器-解码器架构生成面部分割掩码，结合风格对齐损失和数据增强。

Result: 能够从草图输入生成高质量3D模型，并支持多视角渲染。

Conclusion: S3D框架在草图到3D模型转换任务中表现出色，代码已开源。

Abstract: Generating high-quality 3D models from 2D sketches is a challenging task due
to the inherent ambiguity and sparsity of sketch data. In this paper, we
present S3D, a novel framework that converts simple hand-drawn sketches into
detailed 3D models. Our method utilizes a U-Net-based encoder-decoder
architecture to convert sketches into face segmentation masks, which are then
used to generate a 3D representation that can be rendered from novel views. To
ensure robust consistency between the sketch domain and the 3D output, we
introduce a novel style-alignment loss that aligns the U-Net bottleneck
features with the initial encoder outputs of the 3D generation module,
significantly enhancing reconstruction fidelity. To further enhance the
network's robustness, we apply augmentation techniques to the sketch dataset.
This streamlined framework demonstrates the effectiveness of S3D in generating
high-quality 3D models from sketch inputs. The source code for this project is
publicly available at https://github.com/hailsong/S3D.

</details>


### [48] [VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning](https://arxiv.org/abs/2505.04192)
*Trinh T. L. Vuong,Jin Tae Kwak*

Main category: cs.CV

TL;DR: VideoPath-LLaVA是首个整合三种图像场景（单张切片图像、自动提取的关键帧剪辑和手动分割的视频病理图像）的大型多模态模型，模拟病理学家的自然诊断过程。


<details>
  <summary>Details</summary>
Motivation: 通过生成详细的组织学描述和最终诊断，将视觉叙事与诊断推理结合，提升病理视频分析的准确性。

Method: 利用VideoPath-Instruct数据集（4278个视频和诊断相关的思维链指令对），从单图像指令数据集迁移知识，先训练弱标注的关键帧剪辑，再微调手动分割视频。

Result: VideoPath-LLaVA在病理视频分析中设立了新基准，为未来支持临床决策的AI系统奠定了基础。

Conclusion: 该模型通过整合视觉和诊断推理，展示了在病理学中的潜力，代码和数据已开源。

Abstract: We present VideoPath-LLaVA, the first large multimodal model (LMM) in
computational pathology that integrates three distinct image scenarios, single
patch images, automatically keyframe-extracted clips, and manually segmented
video pathology images, to mimic the natural diagnostic process of
pathologists. By generating detailed histological descriptions and culminating
in a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives
with diagnostic reasoning.
  Central to our approach is the VideoPath-Instruct dataset, comprising 4278
video and diagnosis-specific chain-of-thought instructional pairs sourced from
educational histopathology videos on YouTube. Although high-quality data is
critical for enhancing diagnostic reasoning, its creation is time-intensive and
limited in volume. To overcome this challenge, we transfer knowledge from
existing single-image instruction datasets to train on weakly annotated,
keyframe-extracted clips, followed by fine-tuning on manually segmented videos.
VideoPath-LLaVA establishes a new benchmark in pathology video analysis and
offers a promising foundation for future AI systems that support clinical
decision-making through integrated visual and diagnostic reasoning. Our code,
data, and model are publicly available at
https://github.com/trinhvg/VideoPath-LLaVA.

</details>


### [49] [SToLa: Self-Adaptive Touch-Language Framework with Tactile Commonsense Reasoning in Open-Ended Scenarios](https://arxiv.org/abs/2505.04201)
*Ning Cheng,Jinan Xu,Jialing Chen,Wenjuan Han*

Main category: cs.CV

TL;DR: 论文探讨了将触觉感知融入智能系统进行多模态推理的挑战，提出了SToLa框架解决模态差异和触觉数据稀缺问题，并构建了新数据集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决触觉与语言模态间的差异及触觉数据稀缺问题，以支持开放物理世界的常识推理。

Method: 提出SToLa框架，利用Mixture of Experts动态处理触觉与语言模态，并构建触觉常识推理数据集。

Result: SToLa在PhysiCLeAR基准和自建数据集上表现优异，验证了其多模态管理能力。

Conclusion: SToLa框架有效解决了触觉推理中的模态差异和数据稀缺问题，为开放场景任务提供了性能优势。

Abstract: This paper explores the challenges of integrating tactile sensing into
intelligent systems for multimodal reasoning, particularly in enabling
commonsense reasoning about the open-ended physical world. We identify two key
challenges: modality discrepancy, where existing large touch-language models
often treat touch as a mere sub-modality of language, and open-ended tactile
data scarcity, where current datasets lack the diversity, open-endness and
complexity needed for reasoning. To overcome these challenges, we introduce
SToLa, a Self-Adaptive Touch-Language framework. SToLa utilizes Mixture of
Experts (MoE) to dynamically process, unify, and manage tactile and language
modalities, capturing their unique characteristics. Crucially, we also present
a comprehensive tactile commonsense reasoning dataset and benchmark featuring
free-form questions and responses, 8 physical properties, 4 interactive
characteristics, and diverse commonsense knowledge. Experiments show SToLa
exhibits competitive performance compared to existing models on the PhysiCLeAR
benchmark and self-constructed datasets, proving the effectiveness of the
Mixture of Experts architecture in multimodal management and the performance
advantages for open-scenario tactile commonsense reasoning tasks.

</details>


### [50] [An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement](https://arxiv.org/abs/2505.04207)
*Mustafa Yurdakul,Şakir Tasdemir*

Main category: cs.CV

TL;DR: 论文提出了一种基于改进YOLOv8的模型，用于坑洼检测及其物理特征分析，性能优于标准模型。


<details>
  <summary>Details</summary>
Motivation: 坑洼导致车辆损坏和交通事故，现有方法仅基于2D RGB图像，无法准确分析物理特征。

Method: 创建RGB-D数据集PothRGBD，改进YOLOv8n-seg模型，加入DSConv、SimAM和GELU模块。

Result: 改进模型在精度、召回率和mAP上分别提升1.96%、6.13%和2.07%。

Conclusion: 模型轻量高效，适用于实时智能交通解决方案。

Abstract: Potholes cause vehicle damage and traffic accidents, creating serious safety
and economic problems. Therefore, early and accurate detection of potholes is
crucial. Existing detection methods are usually only based on 2D RGB images and
cannot accurately analyze the physical characteristics of potholes. In this
paper, a publicly available dataset of RGB-D images (PothRGBD) is created and
an improved YOLOv8-based model is proposed for both pothole detection and
pothole physical features analysis. The Intel RealSense D415 depth camera was
used to collect RGB and depth data from the road surfaces, resulting in a
PothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable
for segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg
architecture, which is structurally improved with Dynamic Snake Convolution
(DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit
(GELU). The proposed model segmented potholes with irregular edge structure
more accurately, and performed perimeter and depth measurements on depth maps
with high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision,
85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to
93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in
precision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model
performs pothole detection as well as perimeter and depth measurement with high
accuracy and is suitable for real-time applications due to its low model
complexity. In this way, a lightweight and effective model that can be used in
deep learning-based intelligent transportation solutions has been acquired.

</details>


### [51] [CM1 -- A Dataset for Evaluating Few-Shot Information Extraction with Large Vision Language Models](https://arxiv.org/abs/2505.04214)
*Fabian Wolf,Oliver Tüselmann,Arthur Matei,Lukas Hennies,Christoph Rass,Gernot A. Fink*

Main category: cs.CV

TL;DR: 该论文提出了一种用于评估大型视觉语言模型（LVLM）少样本能力的新数据集CM1，并比较了LVLM与传统全页提取模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决手写文档中关键信息提取的挑战，尤其是在标注数据稀缺的情况下，推动档案大规模数字化。

Method: 设计了一个基于历史表单的数据集CM1，包含姓名和出生日期提取任务，并测试了不同训练集规模下LVLM的性能。

Result: 传统全页模型表现优异，但在少样本情况下，LVLM凭借规模和预训练优势超越传统方法。

Conclusion: LVLM在少样本场景下具有潜力，为手写文档信息提取提供了新思路。

Abstract: The automatic extraction of key-value information from handwritten documents
is a key challenge in document analysis. A reliable extraction is a
prerequisite for the mass digitization efforts of many archives. Large Vision
Language Models (LVLM) are a promising technology to tackle this problem
especially in scenarios where little annotated training data is available. In
this work, we present a novel dataset specifically designed to evaluate the
few-shot capabilities of LVLMs. The CM1 documents are a historic collection of
forms with handwritten entries created in Europe to administer the Care and
Maintenance program after World War Two. The dataset establishes three
benchmarks on extracting name and birthdate information and, furthermore,
considers different training set sizes. We provide baseline results for two
different LVLMs and compare performances to an established full-page extraction
model. While the traditional full-page model achieves highly competitive
performances, our experiments show that when only a few training samples are
available the considered LVLMs benefit from their size and heavy pretraining
and outperform the classical approach.

</details>


### [52] [A Weak Supervision Learning Approach Towards an Equitable Parking Lot Occupancy Estimation](https://arxiv.org/abs/2505.04229)
*Theophilus Aidoo,Till Koebe,Akansh Maurya,Hewan Shrestha,Ingmar Weber*

Main category: cs.CV

TL;DR: 提出了一种弱监督框架，利用3米分辨率卫星图像和粗时间标签估计停车场占用率，减少对高分辨率图像的依赖。


<details>
  <summary>Details</summary>
Motivation: 高分辨率标记图像稀缺且昂贵，尤其在低收入地区，限制了遥感应用的发展。

Method: 利用粗时间标签（假设德国大型超市和五金店停车场周六满、周日空），训练成对比较模型。

Result: 模型在大型停车场上的AUC达到0.92。

Conclusion: 该方法可扩展用于城市流动性分析，并适用于评估弱势社区的交通模式和资源分配。

Abstract: The scarcity and high cost of labeled high-resolution imagery have long
challenged remote sensing applications, particularly in low-income regions
where high-resolution data are scarce. In this study, we propose a weak
supervision framework that estimates parking lot occupancy using 3m resolution
satellite imagery. By leveraging coarse temporal labels -- based on the
assumption that parking lots of major supermarkets and hardware stores in
Germany are typically full on Saturdays and empty on Sundays -- we train a
pairwise comparison model that achieves an AUC of 0.92 on large parking lots.
The proposed approach minimizes the reliance on expensive high-resolution
images and holds promise for scalable urban mobility analysis. Moreover, the
method can be adapted to assess transit patterns and resource allocation in
vulnerable communities, providing a data-driven basis to improve the well-being
of those most in need.

</details>


### [53] [Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting](https://arxiv.org/abs/2505.04262)
*Feng Yang,Wenliang Qian,Wangmeng Zuo,Hui Li*

Main category: cs.CV

TL;DR: 本文提出Coupled Score Distillation (CSD)框架，通过耦合多视角联合分布先验，解决Score Distillation Sampling (SDS)在文本到3D生成中的几何不一致问题，并直接优化3D Gaussian Splatting。


<details>
  <summary>Details</summary>
Motivation: SDS在文本到3D生成中忽略了多视角相关性，导致几何不一致和多面伪影。

Method: 提出CSD框架，将优化问题重新表述为多视角联合优化，并直接优化3D Gaussian Splatting。还使用可变形四面体网格进行细化。

Result: 实验结果表明，该方法在效率和生成质量上具有竞争力。

Conclusion: CSD框架能够生成几何一致的3D内容，并直接优化3D Gaussian Splatting，提升生成质量。

Abstract: Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to
advance text-to-3D generation but neglects multi-view correlations, being prone
to geometric inconsistencies and multi-face artifacts in the generated 3D
content. In this work, we propose Coupled Score Distillation (CSD), a framework
that couples multi-view joint distribution priors to ensure geometrically
consistent 3D generation while enabling the stable and direct optimization of
3D Gaussian Splatting. Specifically, by reformulating the optimization as a
multi-view joint optimization problem, we derive an effective optimization rule
that effectively couples multi-view priors to guide optimization across
different viewpoints while preserving the diversity of generated 3D assets.
Additionally, we propose a framework that directly optimizes 3D Gaussian
Splatting (3D-GS) with random initialization to generate geometrically
consistent 3D content. We further employ a deformable tetrahedral grid,
initialized from 3D-GS and refined through CSD, to produce high-quality,
refined meshes. Quantitative and qualitative experimental results demonstrate
the efficiency and competitive quality of our approach.

</details>


### [54] [Object-Shot Enhanced Grounding Network for Egocentric Video](https://arxiv.org/abs/2505.04270)
*Yisen Feng,Haoyu Zhang,Meng Liu,Weili Guan,Liqiang Nie*

Main category: cs.CV

TL;DR: OSGNet提出了一种针对第一人称视频的物体-镜头增强定位网络，通过提取物体信息和镜头运动特征来提升视频表示和模态对齐能力，实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了第一人称视频的关键特性和问题型查询的细粒度信息，OSGNet旨在解决这些不足。

Method: 提取视频中的物体信息以丰富视频表示，并分析镜头运动特征以捕捉佩戴者的注意力信息。

Result: 在三个数据集上的实验表明，OSGNet实现了最先进的性能。

Conclusion: OSGNet通过结合物体和镜头特征，有效提升了第一人称视频定位任务的性能。

Abstract: Egocentric video grounding is a crucial task for embodied intelligence
applications, distinct from exocentric video moment localization. Existing
methods primarily focus on the distributional differences between egocentric
and exocentric videos but often neglect key characteristics of egocentric
videos and the fine-grained information emphasized by question-type queries. To
address these limitations, we propose OSGNet, an Object-Shot enhanced Grounding
Network for egocentric video. Specifically, we extract object information from
videos to enrich video representation, particularly for objects highlighted in
the textual query but not directly captured in the video features.
Additionally, we analyze the frequent shot movements inherent to egocentric
videos, leveraging these features to extract the wearer's attention
information, which enhances the model's ability to perform modality alignment.
Experiments conducted on three datasets demonstrate that OSGNet achieves
state-of-the-art performance, validating the effectiveness of our approach. Our
code can be found at https://github.com/Yisen-Feng/OSGNet.

</details>


### [55] [HDiffTG: A Lightweight Hybrid Diffusion-Transformer-GCN Architecture for 3D Human Pose Estimation](https://arxiv.org/abs/2505.04276)
*Yajie Fu,Chaorui Huang,Junwei Li,Hui Kong,Yibin Tian,Huakang Li,Zhiyuan Zhang*

Main category: cs.CV

TL;DR: HDiffTG是一种新颖的3D人体姿态估计方法，结合了Transformer、GCN和扩散模型，显著提升了精度和鲁棒性，同时保持轻量化设计。


<details>
  <summary>Details</summary>
Motivation: 传统方法在复杂场景和遮挡情况下表现不佳，HDiffTG通过整合多种技术解决这一问题。

Method: 结合Transformer捕捉全局时空依赖、GCN建模局部骨骼结构、扩散模型逐步优化，并引入轻量化优化和目标函数设计。

Result: 在Human3.6M和MPI-INF-3DHP数据集上达到SOTA性能，且在噪声和遮挡环境下表现优异。

Conclusion: HDiffTG在精度、效率和鲁棒性上均表现突出，为3D人体姿态估计提供了有效解决方案。

Abstract: We propose HDiffTG, a novel 3D Human Pose Estimation (3DHPE) method that
integrates Transformer, Graph Convolutional Network (GCN), and diffusion model
into a unified framework. HDiffTG leverages the strengths of these techniques
to significantly improve pose estimation accuracy and robustness while
maintaining a lightweight design. The Transformer captures global
spatiotemporal dependencies, the GCN models local skeletal structures, and the
diffusion model provides step-by-step optimization for fine-tuning, achieving a
complementary balance between global and local features. This integration
enhances the model's ability to handle pose estimation under occlusions and in
complex scenarios. Furthermore, we introduce lightweight optimizations to the
integrated model and refine the objective function design to reduce
computational overhead without compromising performance. Evaluation results on
the Human3.6M and MPI-INF-3DHP datasets demonstrate that HDiffTG achieves
state-of-the-art (SOTA) performance on the MPI-INF-3DHP dataset while excelling
in both accuracy and computational efficiency. Additionally, the model exhibits
exceptional robustness in noisy and occluded environments. Source codes and
models are available at https://github.com/CirceJie/HDiffTG

</details>


### [56] [TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement](https://arxiv.org/abs/2505.04281)
*Yi Li,Zhiyuan Zhang,Jiangnan Xia,Jianghan Cheng,Qilong Wu,Junwei Li,Yibin Tian,Hui Kong*

Main category: cs.CV

TL;DR: TS-Diff是一种新型的两阶段扩散模型，用于增强极低光RAW图像。通过预训练和对齐阶段，结合噪声空间和特定相机特征，实现了去噪、泛化和色彩一致性。


<details>
  <summary>Details</summary>
Motivation: 解决极低光条件下RAW图像的去噪和色彩一致性问题，提升图像质量。

Method: 采用两阶段扩散模型（TS-Diff），预训练阶段通过虚拟相机合成噪声图像，对齐阶段使用少量真实数据微调。引入颜色校正器和结构重参数化技术。

Result: 在多个数据集上表现优异，包括去噪、泛化和色彩一致性。

Conclusion: TS-Diff在极低光条件下具有鲁棒性和多功能性，是实用的解决方案。

Abstract: This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing
extremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes
noisy images by constructing multiple virtual cameras based on a noise space.
Camera Feature Integration (CFI) modules are then designed to enable the model
to learn generalizable features across diverse virtual cameras. During the
aligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is
fine-tuned using a small amount of real RAW data to adapt to the noise
characteristics of specific cameras. A structural reparameterization technique
further simplifies CFI$^T$ for efficient deployment. To address color shifts
during the diffusion process, a color corrector is introduced to ensure color
consistency by dynamically adjusting global color distributions. Additionally,
a novel dataset, QID, is constructed, featuring quantifiable illumination
levels and a wide dynamic range, providing a comprehensive benchmark for
training and evaluation under extreme low-light conditions. Experimental
results demonstrate that TS-Diff achieves state-of-the-art performance on
multiple datasets, including QID, SID, and ELD, excelling in denoising,
generalization, and color consistency across various cameras and illumination
levels. These findings highlight the robustness and versatility of TS-Diff,
making it a practical solution for low-light imaging applications. Source codes
and models are available at https://github.com/CircccleK/TS-Diff

</details>


### [57] [MoDE: Mixture of Diffusion Experts for Any Occluded Face Recognition](https://arxiv.org/abs/2505.04306)
*Qiannan Fan,Zhuoyang Li,Jitong Li,Chenyang Cao*

Main category: cs.CV

TL;DR: 提出了一种基于扩散专家混合模型（MoDE）的遮挡人脸识别方法，通过身份门控网络自适应整合多重建人脸信息，提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 当前遮挡人脸识别算法缺乏对遮挡的先验知识，导致实际应用中性能不佳，影响日常生活便利性。

Method: 使用扩散生成专家估计遮挡人脸的完整图像，并通过身份门控网络评估和整合多重建人脸的信息。

Result: 在三个公开人脸数据集和两个真实数据集上验证了方法的优越性。

Conclusion: MoDE是一种即插即用模块，显著提升了遮挡人脸识别的性能。

Abstract: With the continuous impact of epidemics, people have become accustomed to
wearing masks. However, most current occluded face recognition (OFR) algorithms
lack prior knowledge of occlusions, resulting in poor performance when dealing
with occluded faces of varying types and severity in reality. Recognizing
occluded faces is still a significant challenge, which greatly affects the
convenience of people's daily lives. In this paper, we propose an
identity-gated mixture of diffusion experts (MoDE) for OFR. Each
diffusion-based generative expert estimates one possible complete image for
occluded faces. Considering the random sampling process of the diffusion model,
which introduces inevitable differences and variations between the inpainted
faces and the real ones. To ensemble effective information from
multi-reconstructed faces, we introduce an identity-gating network to evaluate
the contribution of each reconstructed face to the identity and adaptively
integrate the predictions in the decision space. Moreover, our MoDE is a
plug-and-play module for most existing face recognition models. Extensive
experiments on three public face datasets and two datasets in the wild validate
our advanced performance for various occlusions in comparison with the
competing methods.

</details>


### [58] [Multi-turn Consistent Image Editing](https://arxiv.org/abs/2505.04320)
*Zijun Zhou,Yingying Deng,Xiangyu He,Weiming Dong,Fan Tang*

Main category: cs.CV

TL;DR: 提出了一种多轮图像编辑框架，通过迭代优化解决单步编辑的不足，提升编辑效果和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法多为单步操作，难以处理模糊用户意图或复杂变换，导致结果不一致或不符合预期。

Method: 采用流匹配实现精确图像反演，双目标线性二次调节器（LQR）稳定采样，自适应注意力增强方法提升编辑连贯性。

Result: 实验表明，该框架显著提高了编辑成功率和视觉保真度。

Conclusion: 多轮迭代编辑框架有效解决了单步编辑的局限性，提升了编辑质量和用户体验。

Abstract: Many real-world applications, such as interactive photo retouching, artistic
content creation, and product design, require flexible and iterative image
editing. However, existing image editing methods primarily focus on achieving
the desired modifications in a single step, which often struggles with
ambiguous user intent, complex transformations, or the need for progressive
refinements. As a result, these methods frequently produce inconsistent
outcomes or fail to meet user expectations. To address these challenges, we
propose a multi-turn image editing framework that enables users to iteratively
refine their edits, progressively achieving more satisfactory results. Our
approach leverages flow matching for accurate image inversion and a
dual-objective Linear Quadratic Regulators (LQR) for stable sampling,
effectively mitigating error accumulation. Additionally, by analyzing the
layer-wise roles of transformers, we introduce a adaptive attention
highlighting method that enhances editability while preserving multi-turn
coherence. Extensive experiments demonstrate that our framework significantly
improves edit success rates and visual fidelity compared to existing methods.

</details>


### [59] [CountDiffusion: Text-to-Image Synthesis with Training-Free Counting-Guidance Diffusion](https://arxiv.org/abs/2505.04347)
*Yanyu Li,Pencheng Wan,Liang Han,Yaowei Wang,Liqiang Nie,Min Zhang*

Main category: cs.CV

TL;DR: CountDiffusion是一个无需训练的框架，通过两阶段方法改进文本到图像生成中对象数量的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像生成模型在对象数量准确性上的不足，避免高计算成本和抽象概念学习的困难。

Method: 分为两阶段：首先生成中间去噪结果并计数对象数量，然后通过注意力图校正对象数量。

Result: 实验表明CountDiffusion显著提升了文本到图像模型生成准确对象数量的能力。

Conclusion: CountDiffusion是一种高效且无需额外训练的解决方案，可广泛应用于扩散模型。

Abstract: Stable Diffusion has advanced text-to-image synthesis, but training models to
generate images with accurate object quantity is still difficult due to the
high computational cost and the challenge of teaching models the abstract
concept of quantity. In this paper, we propose CountDiffusion, a training-free
framework aiming at generating images with correct object quantity from textual
descriptions. CountDiffusion consists of two stages. In the first stage, an
intermediate denoising result is generated by the diffusion model to predict
the final synthesized image with one-step denoising, and a counting model is
used to count the number of objects in this image. In the second stage, a
correction module is used to correct the object quantity by changing the
attention map of the object with universal guidance. The proposed
CountDiffusion can be plugged into any diffusion-based text-to-image (T2I)
generation models without further training. Experiment results demonstrate the
superiority of our proposed CountDiffusion, which improves the accurate object
quantity generation ability of T2I models by a large margin.

</details>


### [60] [WDMamba: When Wavelet Degradation Prior Meets Vision Mamba for Image Dehazing](https://arxiv.org/abs/2505.04369)
*Jie Sun,Heng Liu,Yongzhen Wang,Xiao-Ping Zhang,Mingqiang Wei*

Main category: cs.CV

TL;DR: 论文提出了一种基于小波变换的去雾框架WDMamba，通过低频恢复和细节增强两阶段策略，结合Mamba块和自引导对比正则化，显著提升了去雾效果。


<details>
  <summary>Details</summary>
Motivation: 通过小波变换分析发现雾霾信息主要存在于低频分量，因此提出分阶段去雾框架以更高效地恢复图像。

Method: WDMamba框架分为低频恢复（使用Mamba块）和细节增强两阶段，并引入自引导对比正则化优化训练。

Result: 在公开去雾基准测试中，WDMamba在质量和数量上均优于现有方法。

Conclusion: WDMamba通过分阶段策略和正则化技术，实现了高效且高质量的去雾效果。

Abstract: In this paper, we reveal a novel haze-specific wavelet degradation prior
observed through wavelet transform analysis, which shows that haze-related
information predominantly resides in low-frequency components. Exploiting this
insight, we propose a novel dehazing framework, WDMamba, which decomposes the
image dehazing task into two sequential stages: low-frequency restoration
followed by detail enhancement. This coarse-to-fine strategy enables WDMamba to
effectively capture features specific to each stage of the dehazing process,
resulting in high-quality restored images. Specifically, in the low-frequency
restoration stage, we integrate Mamba blocks to reconstruct global structures
with linear complexity, efficiently removing overall haze and producing a
coarse restored image. Thereafter, the detail enhancement stage reinstates
fine-grained information that may have been overlooked during the previous
phase, culminating in the final dehazed output. Furthermore, to enhance detail
retention and achieve more natural dehazing, we introduce a self-guided
contrastive regularization during network training. By utilizing the coarse
restored output as a hard negative example, our model learns more
discriminative representations, substantially boosting the overall dehazing
performance. Extensive evaluations on public dehazing benchmarks demonstrate
that our method surpasses state-of-the-art approaches both qualitatively and
quantitatively. Code is available at https://github.com/SunJ000/WDMamba.

</details>


### [61] [Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise](https://arxiv.org/abs/2505.04375)
*Moseli Mots'oehli,Hope Mogale,Kyungim Baek*

Main category: cs.CV

TL;DR: 研究了不同规模的视觉变换器在标签噪声下的表现，发现较大的ViT模型表现更优，而Swin变换器鲁棒性较弱。小补丁尺寸不一定更好，主动学习策略在中等噪声下有效。


<details>
  <summary>Details</summary>
Motivation: 探索视觉变换器在低预算和标签噪声场景下的实用性，填补模型规模对性能影响的空白。

Method: 评估四种ViT和三种Swin变换器配置在CIFAR10/100数据集上的分类准确性和校准性，测试不同标签噪声率。

Result: 大ViT模型（如ViTl32）在噪声下表现更优，Swin变换器鲁棒性较差。小补丁尺寸不一定更好，主动学习策略在中等噪声下有效。

Conclusion: 为资源受限环境下部署视觉变换器提供了实用建议，需平衡模型复杂度、噪声和计算效率。

Abstract: Fine-tuning pre-trained convolutional neural networks on ImageNet for
downstream tasks is well-established. Still, the impact of model size on the
performance of vision transformers in similar scenarios, particularly under
label noise, remains largely unexplored. Given the utility and versatility of
transformer architectures, this study investigates their practicality under
low-budget constraints and noisy labels. We explore how classification accuracy
and calibration are affected by symmetric label noise in active learning
settings, evaluating four vision transformer configurations (Base and Large
with 16x16 and 32x32 patch sizes) and three Swin Transformer configurations
(Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label
noise rates. Our findings show that larger ViT models (ViTl32 in particular)
consistently outperform their smaller counterparts in both accuracy and
calibration, even under moderate to high label noise, while Swin Transformers
exhibit weaker robustness across all noise levels. We find that smaller patch
sizes do not always lead to better performance, as ViTl16 performs consistently
worse than ViTl32 while incurring a higher computational cost. We also find
that information-based Active Learning strategies only provide meaningful
accuracy improvements at moderate label noise rates, but they result in poorer
calibration compared to models trained on randomly acquired labels, especially
at high label noise rates. We hope these insights provide actionable guidance
for practitioners looking to deploy vision transformers in resource-constrained
environments, where balancing model complexity, label noise, and compute
efficiency is critical in model fine-tuning or distillation.

</details>


### [62] [Label-efficient Single Photon Images Classification via Active Learning](https://arxiv.org/abs/2505.04376)
*Zili Zhang,Ziting Wen,Yiheng Qiang,Hongzhou Dong,Wenle Dong,Xinyang Li,Xiaofan Wang,Xiaoqiang Ren*

Main category: cs.CV

TL;DR: 该论文提出了一种主动学习框架，用于单光子图像的分类，通过选择性标注信息量最大的样本，显著减少了标注需求，并在合成和真实数据上取得了高分类精度。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注从稀疏光子事件重建3D场景，而单光子图像的语义解释因高标注成本和低效标注策略而未被充分探索。

Method: 提出了一种成像条件感知的采样策略，结合合成增强技术，选择模型不确定且对成像条件敏感的样本进行标注。

Result: 在合成数据上仅需1.5%标注样本即达到97%准确率，真实数据上仅需8%标注样本达到90.63%准确率，优于基线方法。

Conclusion: 主动学习使单光子图像分类性能达到传统图像水平，为单光子数据的大规模应用铺平道路。

Abstract: Single-photon LiDAR achieves high-precision 3D imaging in extreme
environments through quantum-level photon detection technology. Current
research primarily focuses on reconstructing 3D scenes from sparse photon
events, whereas the semantic interpretation of single-photon images remains
underexplored, due to high annotation costs and inefficient labeling
strategies. This paper presents the first active learning framework for
single-photon image classification. The core contribution is an imaging
condition-aware sampling strategy that integrates synthetic augmentation to
model variability across imaging conditions. By identifying samples where the
model is both uncertain and sensitive to these conditions, the proposed method
selectively annotates only the most informative examples. Experiments on both
synthetic and real-world datasets show that our approach outperforms all
baselines and achieves high classification accuracy with significantly fewer
labeled samples. Specifically, our approach achieves 97% accuracy on synthetic
single-photon data using only 1.5% labeled samples. On real-world data, we
maintain 90.63% accuracy with just 8% labeled samples, which is 4.51% higher
than the best-performing baseline. This illustrates that active learning
enables the same level of classification performance on single-photon images as
on classical images, opening doors to large-scale integration of single-photon
data in real-world applications.

</details>


### [63] [Tetrahedron-Net for Medical Image Registration](https://arxiv.org/abs/2505.04380)
*Jinhai Xiang,Shuai Guo,Qianru Han,Dantong Shi,Xinwei He,Xiang Bai*

Main category: cs.CV

TL;DR: 论文提出了一种名为Tetrahedron-Net的新架构，通过增加一个额外的解码器来增强医学图像配准的表征能力。


<details>
  <summary>Details</summary>
Motivation: 现有U-Net类网络在单编码器-单解码器架构下未能充分利用特征交互，限制了配准质量。

Method: 设计了一个简洁的Tetrahedron-Net，包含一个编码器和两个解码器，新解码器与原编码器和解码器交互。

Result: 实验证明该方法在多个医学图像配准基准上表现优异，并可轻松集成到现有U-Net类网络中。

Conclusion: Tetrahedron-Net通过简单的结构改进显著提升了配准性能，具有通用性和易集成性。

Abstract: Medical image registration plays a vital role in medical image processing.
Extracting expressive representations for medical images is crucial for
improving the registration quality. One common practice for this end is
constructing a convolutional backbone to enable interactions with skip
connections among feature extraction layers. The de facto structure, U-Net-like
networks, has attempted to design skip connections such as nested or full-scale
ones to connect one single encoder and one single decoder to improve its
representation capacity. Despite being effective, it still does not fully
explore interactions with a single encoder and decoder architectures. In this
paper, we embrace this observation and introduce a simple yet effective
alternative strategy to enhance the representations for registrations by
appending one additional decoder. The new decoder is designed to interact with
both the original encoder and decoder. In this way, it not only reuses feature
presentation from corresponding layers in the encoder but also interacts with
the original decoder to corporately give more accurate registration results.
The new architecture is concise yet generalized, with only one encoder and two
decoders forming a ``Tetrahedron'' structure, thereby dubbed Tetrahedron-Net.
Three instantiations of Tetrahedron-Net are further constructed regarding the
different structures of the appended decoder. Our extensive experiments prove
that superior performance can be obtained on several representative benchmarks
of medical image registration. Finally, such a ``Tetrahedron'' design can also
be easily integrated into popular U-Net-like architectures including
VoxelMorph, ViT-V-Net, and TransMorph, leading to consistent performance gains.

</details>


### [64] [DATA: Multi-Disentanglement based Contrastive Learning for Open-World Semi-Supervised Deepfake Attribution](https://arxiv.org/abs/2505.04384)
*Ming-Hui Liu,Xiao-Qian Liu,Xin Luo,Xin-Shun Xu*

Main category: cs.CV

TL;DR: 论文提出了一种名为DATA的多解缠对比学习框架，用于提升开放世界半监督深度伪造溯源任务（OSS-DFA）的泛化能力，通过正交深度伪造基和增强记忆机制解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造溯源方法仅关注特定方法的线索，容易过拟合且忽略共同伪造特征，同时在开放世界场景中难以区分不确定的新类别。

Method: DATA框架首次定义‘正交深度伪造基’概念，解缠特定方法特征以减少无关信息过拟合；设计增强记忆机制辅助新类别发现和对比学习；使用基对比损失和中心对比损失增强特征标准化和区分性。

Result: 实验表明，DATA在OSS-DFA基准测试中表现最优，不同设置下准确率提升2.55%和5.7%。

Conclusion: DATA通过解缠和对比学习显著提升了深度伪造溯源的泛化能力和新类别识别效果。

Abstract: Deepfake attribution (DFA) aims to perform multiclassification on different
facial manipulation techniques, thereby mitigating the detrimental effects of
forgery content on the social order and personal reputations. However, previous
methods focus only on method-specific clues, which easily lead to overfitting,
while overlooking the crucial role of common forgery features. Additionally,
they struggle to distinguish between uncertain novel classes in more practical
open-world scenarios. To address these issues, in this paper we propose an
innovative multi-DisentAnglement based conTrastive leArning framework, DATA, to
enhance the generalization ability on novel classes for the open-world
semi-supervised deepfake attribution (OSS-DFA) task. Specifically, since all
generation techniques can be abstracted into a similar architecture, DATA
defines the concept of 'Orthonormal Deepfake Basis' for the first time and
utilizes it to disentangle method-specific features, thereby reducing the
overfitting on forgery-irrelevant information. Furthermore, an augmented-memory
mechanism is designed to assist in novel class discovery and contrastive
learning, which aims to obtain clear class boundaries for the novel classes
through instance-level disentanglements. Additionally, to enhance the
standardization and discrimination of features, DATA uses bases contrastive
loss and center contrastive loss as auxiliaries for the aforementioned modules.
Extensive experimental evaluations show that DATA achieves state-of-the-art
performance on the OSS-DFA benchmark, e.g., there are notable accuracy
improvements in 2.55% / 5.7% under different settings, compared with the
existing methods.

</details>


### [65] [Predicting Road Surface Anomalies by Visual Tracking of a Preceding Vehicle](https://arxiv.org/abs/2505.04392)
*Petr Jahoda,Jan Cech*

Main category: cs.CV

TL;DR: 提出了一种通过视觉跟踪前车来检测道路异常的新方法，适用于低能见度或密集交通场景，并能提前预测异常。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖直接观察和训练视觉检测器，难以应对低能见度或遮挡情况。新方法通过跟踪前车动态预测道路异常，提升安全性和适应性。

Method: 利用相机跟踪前车，通过鲁棒迭代估计器补偿相机俯仰旋转，以消除车辆振动干扰，实现实时异常检测。

Result: 实验表明，即使在复杂路况下，该方法能可靠地远距离检测道路异常，并在标准硬件上实时运行。

Conclusion: 该方法高效、通用，适用于自动驾驶和车辆系统预配置，具有实际应用潜力。

Abstract: A novel approach to detect road surface anomalies by visual tracking of a
preceding vehicle is proposed. The method is versatile, predicting any kind of
road anomalies, such as potholes, bumps, debris, etc., unlike direct
observation methods that rely on training visual detectors of those cases. The
method operates in low visibility conditions or in dense traffic where the
anomaly is occluded by a preceding vehicle. Anomalies are detected
predictively, i.e., before a vehicle encounters them, which allows to
pre-configure low-level vehicle systems (such as chassis) or to plan an
avoidance maneuver in case of autonomous driving. A challenge is that the
signal coming from camera-based tracking of a preceding vehicle may be weak and
disturbed by camera ego motion due to vibrations affecting the ego vehicle.
Therefore, we propose an efficient method to compensate camera pitch rotation
by an iterative robust estimator. Our experiments on both controlled setup and
normal traffic conditions show that road anomalies can be detected reliably at
a distance even in challenging cases where the ego vehicle traverses imperfect
road surfaces. The method is effective and performs in real time on standard
consumer hardware.

</details>


### [66] [SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin Transformer](https://arxiv.org/abs/2505.04394)
*Young-Hu Park,Rae-Hong Park,Hyung-Min Park*

Main category: cs.CV

TL;DR: 提出了一种基于Swin Transformer的高效视觉语音编码器SwinLip，用于唇读任务，显著降低了计算复杂度并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于ResNet的唇读方法计算复杂度高，不适合高效捕捉唇读特征，且复杂视觉模型在多模态研究中导致延迟。

Method: 采用Swin Transformer的分层结构和窗口自注意力机制，结合改进的Conformer时间嵌入和传统空间嵌入，构建轻量级SwinLip编码器。

Result: SwinLip在英语LRW和汉语LRW-1000数据集上表现优异，计算量更少且在汉语LRW-1000上达到SOTA性能。

Conclusion: SwinLip通过高效结构设计，显著提升了唇读网络的性能和推理速度，适用于多种任务。

Abstract: This paper presents an efficient visual speech encoder for lip reading. While
most recent lip reading studies have been based on the ResNet architecture and
have achieved significant success, they are not sufficiently suitable for
efficiently capturing lip reading features due to high computational complexity
in modeling spatio-temporal information. Additionally, using a complex visual
model not only increases the complexity of lip reading models but also induces
delays in the overall network for multi-modal studies (e.g., audio-visual
speech recognition, speech enhancement, and speech separation). To overcome the
limitations of Convolutional Neural Network (CNN)-based models, we apply the
hierarchical structure and window self-attention of the Swin Transformer to lip
reading. We configure a new lightweight scale of the Swin Transformer suitable
for processing lip reading data and present the SwinLip visual speech encoder,
which efficiently reduces computational load by integrating modified
Convolution-augmented Transformer (Conformer) temporal embeddings with
conventional spatial embeddings in the hierarchical structure. Through
extensive experiments, we have validated that our SwinLip successfully improves
the performance and inference speed of the lip reading network when applied to
various backbones for word and sentence recognition, reducing computational
load. In particular, our SwinLip demonstrated robust performance in both
English LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art
performance on the Mandarin LRW-1000 dataset with less computation compared to
the existing state-of-the-art model.

</details>


### [67] [Deep residual learning with product units](https://arxiv.org/abs/2505.04397)
*Ziyuan Li,Uwe Jaekel,Babette Dellen*

Main category: cs.CV

TL;DR: PURe是一种结合乘积单元的深度残差网络，通过乘法特征交互提升表达能力和参数效率，在多个数据集上表现优于传统ResNet，且收敛更快、参数更少。


<details>
  <summary>Details</summary>
Motivation: 传统残差网络主要依赖加法神经元，限制了复杂模式的表达能力。PURe引入乘积单元以增强特征交互，提升模型性能。

Method: 在残差块的第二层用2D乘积单元替代传统卷积层，去除非线性激活函数以保留结构信息。

Result: 在Galaxy10 DECaLS、ImageNet和CIFAR-10上，PURe均优于ResNet，准确率更高、收敛更快、参数更少，且对噪声更鲁棒。

Conclusion: PURe在准确性、效率和鲁棒性之间取得了良好平衡，展示了乘积单元架构在计算机视觉中的潜力。

Abstract: We propose a deep product-unit residual neural network (PURe) that integrates
product units into residual blocks to improve the expressiveness and parameter
efficiency of deep convolutional networks. Unlike standard summation neurons,
product units enable multiplicative feature interactions, potentially offering
a more powerful representation of complex patterns. PURe replaces conventional
convolutional layers with 2D product units in the second layer of each residual
block, eliminating nonlinear activation functions to preserve structural
information. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS,
PURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper
ResNet152, while converging nearly five times faster and demonstrating strong
robustness to Poisson noise. On ImageNet, PURe architectures outperform
standard ResNet models at similar depths, with PURe34 achieving a top-1
accuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet
variants (ResNet50, ResNet101) while utilizing significantly fewer parameters
and computational resources. On CIFAR-10, PURe consistently outperforms ResNet
variants across varying depths, with PURe272 reaching 95.01% test accuracy,
comparable to ResNet1001 but at less than half the model size. These results
demonstrate that PURe achieves a favorable balance between accuracy,
efficiency, and robustness. Compared to traditional residual networks, PURe not
only achieves competitive classification performance with faster convergence
and fewer parameters, but also demonstrates greater robustness to noise. Its
effectiveness across diverse datasets highlights the potential of
product-unit-based architectures for scalable and reliable deep learning in
computer vision.

</details>


### [68] [MFSeg: Efficient Multi-frame 3D Semantic Segmentation](https://arxiv.org/abs/2505.04408)
*Chengjie Huang,Krzysztof Czarnecki*

Main category: cs.CV

TL;DR: MFSeg是一种高效的多帧3D语义分割框架，通过特征级点云序列聚合和轻量级MLP解码器，降低计算开销并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D语义分割中存在计算开销大和冗余点云上采样的问题，MFSeg旨在解决这些问题。

Method: 通过特征级点云序列聚合和正则化特征提取过程，结合轻量级MLP解码器，避免冗余点上采样。

Result: 在nuScenes和Waymo数据集上表现优于现有方法，证明了其高效性和准确性。

Conclusion: MFSeg是一种高效且准确的3D语义分割框架，适用于实际应用。

Abstract: We propose MFSeg, an efficient multi-frame 3D semantic segmentation
framework. By aggregating point cloud sequences at the feature level and
regularizing the feature extraction and aggregation process, MFSeg reduces
computational overhead while maintaining high accuracy. Moreover, by employing
a lightweight MLP-based point decoder, our method eliminates the need to
upsample redundant points from past frames. Experiments on the nuScenes and
Waymo datasets show that MFSeg outperforms existing methods, demonstrating its
effectiveness and efficiency.

</details>


### [69] [DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception](https://arxiv.org/abs/2505.04410)
*Junjie Wang,Bin Chen,Yulin Li,Bin Kang,Yichi Chen,Zhuotao Tian*

Main category: cs.CV

TL;DR: DeCLIP改进CLIP模型，通过解耦自注意力模块提升局部特征表示能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决CLIP在密集预测任务中局部特征表示不足的问题。

Method: 提出DeCLIP框架，解耦自注意力模块为“内容”和“上下文”特征，分别优化局部判别性和空间一致性。

Result: 在开放词汇密集预测任务（如目标检测和语义分割）中表现优异。

Conclusion: DeCLIP通过改进特征表示，显著提升了密集预测任务的性能。

Abstract: Dense visual prediction tasks have been constrained by their reliance on
predefined categories, limiting their applicability in real-world scenarios
where visual concepts are unbounded. While Vision-Language Models (VLMs) like
CLIP have shown promise in open-vocabulary tasks, their direct application to
dense prediction often leads to suboptimal performance due to limitations in
local feature representation. In this work, we present our observation that
CLIP's image tokens struggle to effectively aggregate information from
spatially or semantically related regions, resulting in features that lack
local discriminability and spatial consistency. To address this issue, we
propose DeCLIP, a novel framework that enhances CLIP by decoupling the
self-attention module to obtain ``content'' and ``context'' features
respectively. The ``content'' features are aligned with image crop
representations to improve local discriminability, while ``context'' features
learn to retain the spatial correlations under the guidance of vision
foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP
significantly outperforms existing methods across multiple open-vocabulary
dense prediction tasks, including object detection and semantic segmentation.
Code is available at \textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}.

</details>


### [70] [RLMiniStyler: Light-weight RL Style Agent for Arbitrary Sequential Neural Style Generation](https://arxiv.org/abs/2505.04424)
*Jing Hu,Chengming Feng,Shu Hu,Ming-Ching Chang,Xin Li,Xi Wu,Xin Wang*

Main category: cs.CV

TL;DR: RLMiniStyler是一个基于强化学习的任意风格迁移框架，通过迭代优化和轻量化设计，以较低成本生成高质量多样化的风格化图像。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在生成多样化风格化结果时计算成本高，因此提出一种轻量化的强化学习框架。

Method: 采用统一的强化学习策略迭代指导风格迁移过程，并结合不确定性感知的多任务学习策略动态调整损失权重。

Result: 实验验证了RLMiniStyler在多种分辨率下生成高质量、多样化风格化图像序列的优势。

Conclusion: RLMiniStyler在低成本下实现了高质量的风格迁移，代码已开源。

Abstract: Arbitrary style transfer aims to apply the style of any given artistic image
to another content image. Still, existing deep learning-based methods often
require significant computational costs to generate diverse stylized results.
Motivated by this, we propose a novel reinforcement learning-based framework
for arbitrary style transfer RLMiniStyler. This framework leverages a unified
reinforcement learning policy to iteratively guide the style transfer process
by exploring and exploiting stylization feedback, generating smooth sequences
of stylized results while achieving model lightweight. Furthermore, we
introduce an uncertainty-aware multi-task learning strategy that automatically
adjusts loss weights to adapt to the content and style balance requirements at
different training stages, thereby accelerating model convergence. Through a
series of experiments across image various resolutions, we have validated the
advantages of RLMiniStyler over other state-of-the-art methods in generating
high-quality, diverse artistic image sequences at a lower cost. Codes are
available at https://github.com/fengxiaoming520/RLMiniStyler.

</details>


### [71] [Learning Real Facial Concepts for Independent Deepfake Detection](https://arxiv.org/abs/2505.04460)
*Ming-Hui Liu,Harry Cheng,Tianyi Wang,Xin Luo,Xin-Shun Xu*

Main category: cs.CV

TL;DR: 论文提出RealID方法，通过独立评估真实与伪造类别的概率，提升深度伪造检测模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测模型因过度依赖伪造痕迹和对真实人脸理解不足，导致在未见数据集上泛化能力差。

Method: RealID包含RealC2模块和IDC模块，前者通过MultiReal Memory捕获真实人脸的全面概念，后者独立决策分类。

Result: 在五个数据集上的实验表明，RealID平均准确率提升1.74%，显著优于现有方法。

Conclusion: RealID通过独立决策和全面学习真实人脸概念，有效提升了深度伪造检测的泛化能力。

Abstract: Deepfake detection models often struggle with generalization to unseen
datasets, manifesting as misclassifying real instances as fake in target
domains. This is primarily due to an overreliance on forgery artifacts and a
limited understanding of real faces. To address this challenge, we propose a
novel approach RealID to enhance generalization by learning a comprehensive
concept of real faces while assessing the probabilities of belonging to the
real and fake classes independently. RealID comprises two key modules: the Real
Concept Capture Module (RealC2) and the Independent Dual-Decision Classifier
(IDC). With the assistance of a MultiReal Memory, RealC2 maintains various
prototypes for real faces, allowing the model to capture a comprehensive
concept of real class. Meanwhile, IDC redefines the classification strategy by
making independent decisions based on the concept of the real class and the
presence of forgery artifacts. Through the combined effect of the above
modules, the influence of forgery-irrelevant patterns is alleviated, and
extensive experiments on five widely used datasets demonstrate that RealID
significantly outperforms existing state-of-the-art methods, achieving a 1.74%
improvement in average accuracy.

</details>


### [72] [CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation](https://arxiv.org/abs/2505.04481)
*Jiahao Li,Weijian Ma,Xueyang Li,Yunzhong Lou,Guichun Zhou,Xiangdong Zhou*

Main category: cs.CV

TL;DR: 该研究提出了一种名为CAD-Llama的框架，旨在增强预训练大语言模型（LLMs）生成参数化3D CAD模型的能力。通过开发分层注释管道和类似代码的格式，将参数化CAD命令序列转换为结构化参数化CAD代码（SPCC），并结合自适应预训练和指令调优，显著提升了生成效果。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在通用文本生成方面表现出色，但其在特定领域（如参数化3D CAD模型生成）的应用仍面临挑战，因为LLMs缺乏对3D结构和参数化序列的直接理解。

Method: 研究开发了分层注释管道和SPCC格式，将参数化CAD命令序列转换为结构化代码。随后采用自适应预训练和CAD特定的指令调优方法，以增强LLMs的空间知识。

Result: 实验结果表明，CAD-Llama框架在生成参数化3D CAD模型方面显著优于现有的自回归方法和LLM基线。

Conclusion: 该研究为LLMs在参数化3D CAD模型生成领域的应用提供了有效解决方案，展示了其在特定领域的潜力。

Abstract: Recently, Large Language Models (LLMs) have achieved significant success,
prompting increased interest in expanding their generative capabilities beyond
general text into domain-specific areas. This study investigates the generation
of parametric sequences for computer-aided design (CAD) models using LLMs. This
endeavor represents an initial step towards creating parametric 3D shapes with
LLMs, as CAD model parameters directly correlate with shapes in
three-dimensional space. Despite the formidable generative capacities of LLMs,
this task remains challenging, as these models neither encounter parametric
sequences during their pretraining phase nor possess direct awareness of 3D
structures. To address this, we present CAD-Llama, a framework designed to
enhance pretrained LLMs for generating parametric 3D CAD models. Specifically,
we develop a hierarchical annotation pipeline and a code-like format to
translate parametric 3D CAD command sequences into Structured Parametric CAD
Code (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we
propose an adaptive pretraining approach utilizing SPCC, followed by an
instruction tuning process aligned with CAD-specific guidelines. This
methodology aims to equip LLMs with the spatial knowledge inherent in
parametric sequences. Experimental results demonstrate that our framework
significantly outperforms prior autoregressive methods and existing LLM
baselines.

</details>


### [73] [FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging](https://arxiv.org/abs/2505.04485)
*Ali Alawieh,Alexandru P. Condurache*

Main category: cs.CV

TL;DR: FA-KPConv是一种基于KPConv的神经网络架构，通过Frame Averaging实现点云网络对欧几里得变换的精确不变性和/或等变性，适用于点云分类和配准任务。


<details>
  <summary>Details</summary>
Motivation: KPConv网络在处理欧几里得变换时仅能近似实现不变性或等变性，需依赖大数据或数据增强。FA-KPConv旨在通过几何先验知识精确实现这些性质。

Method: 在KPConv基础上引入Frame Averaging，无需增加可学习参数或损失输入信息，即可实现点云网络对平移、旋转和反射的精确不变性和/或等变性。

Result: FA-KPConv在点云分类和配准任务中表现优异，尤其在训练数据稀缺或测试数据随机旋转的挑战性场景下。

Conclusion: FA-KPConv通过几何先验知识显著提升了KPConv网络的性能，尤其在数据受限或变换复杂的情况下。

Abstract: We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neural
network architecture built on top of the well-known KPConv, a widely adopted
backbone for 3D point cloud analysis. Even though invariance and/or
equivariance to Euclidean transformations are required for many common tasks,
KPConv-based networks can only approximately achieve such properties when
training on large datasets or with significant data augmentations. Using Frame
Averaging, we allow to flexibly customize point cloud neural networks built
with KPConv layers, by making them exactly invariant and/or equivariant to
translations, rotations and/or reflections of the input point clouds. By simply
wrapping around an existing KPConv-based network, FA-KPConv embeds geometrical
prior knowledge into it while preserving the number of learnable parameters and
not compromising any input information. We showcase the benefit of such an
introduced bias for point cloud classification and point cloud registration,
especially in challenging cases such as scarce training data or randomly
rotated test data.

</details>


### [74] [Efficient Flow Matching using Latent Variables](https://arxiv.org/abs/2505.04486)
*Anirban Samaddar,Yixuan Sun,Viktor Nilsson,Sandeep Madireddy*

Main category: cs.CV

TL;DR: Latent-CFM提出了一种简化训练和推理的策略，利用预训练的深度潜在变量模型处理多模态数据，显著减少了训练和计算成本，同时提高了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有流匹配模型未显式建模目标数据的底层结构/流形，导致学习效率低下，尤其是在高维数据集中。现有方法处理多模态数据时训练成本高且性能不佳。

Method: 提出Latent-CFM，利用预训练的深度潜在变量模型简化训练和推理策略，以处理多模态数据。

Result: 实验表明，Latent-CFM在生成质量上优于现有流匹配模型，训练和计算成本显著降低（某些情况下减少50%），并能生成更物理准确的样本。

Conclusion: Latent-CFM通过潜在空间分析支持条件图像生成，为高效处理多模态数据提供了新思路。

Abstract: Flow matching models have shown great potential in image generation tasks
among probabilistic generative models. Building upon the ideas of continuous
normalizing flows, flow matching models generalize the transport path of the
diffusion models from a simple prior distribution to the data. Most flow
matching models in the literature do not explicitly model the underlying
structure/manifold in the target data when learning the flow from a simple
source distribution like the standard Gaussian. This leads to inefficient
learning, especially for many high-dimensional real-world datasets, which often
reside in a low-dimensional manifold. Existing strategies of incorporating
manifolds, including data with underlying multi-modal distribution, often
require expensive training and hence frequently lead to suboptimal performance.
To this end, we present \texttt{Latent-CFM}, which provides simplified
training/inference strategies to incorporate multi-modal data structures using
pretrained deep latent variable models. Through experiments on multi-modal
synthetic data and widely used image benchmark datasets, we show that
\texttt{Latent-CFM} exhibits improved generation quality with significantly
less training ($\sim 50\%$ less in some cases) and computation than
state-of-the-art flow matching models. Using a 2d Darcy flow dataset, we
demonstrate that our approach generates more physically accurate samples than
competitive approaches. In addition, through latent space analysis, we
demonstrate that our approach can be used for conditional image generation
conditioned on latent features.

</details>


### [75] ["I Can See Forever!": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments](https://arxiv.org/abs/2505.04488)
*Ziyi Zhang,Zhen Sun,Zongmin Zhang,Zifan Peng,Yuemeng Zhao,Zichun Wang,Zeren Luo,Ruiting Zuo,Xinlei He*

Main category: cs.CV

TL;DR: 该论文首次系统评估了实时视觉语言模型（VideoLLMs）在帮助视障人士方面的有效性，构建了VisAssistDaily基准数据集，并发现GPT-4o表现最佳。同时，提出了环境感知数据集SafeVid和轮询机制以解决动态环境中的风险感知问题。


<details>
  <summary>Details</summary>
Motivation: 视障人士在动态复杂环境中缺乏有效的实时感知辅助技术，现有研究多关注静态内容，无法满足实际需求。

Method: 构建VisAssistDaily数据集，涵盖三类辅助任务；通过用户研究评估模型在封闭和开放场景中的表现；提出SafeVid数据集和轮询机制以提升环境风险感知能力。

Result: GPT-4o在任务成功率上表现最佳；当前模型在动态环境中感知潜在风险的能力有限。

Conclusion: 论文为视障辅助技术提供了新思路，并呼吁进一步研究动态环境中的实时感知问题。

Abstract: The visually impaired population, especially the severely visually impaired,
is currently large in scale, and daily activities pose significant challenges
for them. Although many studies use large language and vision-language models
to assist the blind, most focus on static content and fail to meet real-time
perception needs in dynamic and complex environments, such as daily activities.
To provide them with more effective intelligent assistance, it is imperative to
incorporate advanced visual understanding technologies. Although real-time
vision and speech interaction VideoLLMs demonstrate strong real-time visual
understanding, no prior work has systematically evaluated their effectiveness
in assisting visually impaired individuals. In this work, we conduct the first
such evaluation. First, we construct a benchmark dataset (VisAssistDaily),
covering three categories of assistive tasks for visually impaired individuals:
Basic Skills, Home Life Tasks, and Social Life Tasks. The results show that
GPT-4o achieves the highest task success rate. Next, we conduct a user study to
evaluate the models in both closed-world and open-world scenarios, further
exploring the practical challenges of applying VideoLLMs in assistive contexts.
One key issue we identify is the difficulty current models face in perceiving
potential hazards in dynamic environments. To address this, we build an
environment-awareness dataset named SafeVid and introduce a polling mechanism
that enables the model to proactively detect environmental risks. We hope this
work provides valuable insights and inspiration for future research in this
field.

</details>


### [76] [Defining and Quantifying Creative Behavior in Popular Image Generators](https://arxiv.org/abs/2505.04497)
*Aditi Ramaswamy*

Main category: cs.CV

TL;DR: 本文提出了一种从实践角度衡量生成式AI模型创造力的定量方法，帮助用户根据任务选择合适的模型。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI模型的创造力，并解决科学界对此的争议。

Method: 引入定量指标，评估多种流行的图像生成模型。

Result: 提出的指标与人类直觉一致。

Conclusion: 定量方法可有效衡量AI模型的创造力，辅助用户选择模型。

Abstract: Creativity of generative AI models has been a subject of scientific debate in
the last years, without a conclusive answer. In this paper, we study creativity
from a practical perspective and introduce quantitative measures that help the
user to choose a suitable AI model for a given task. We evaluated our measures
on a number of popular image-to-image generation models, and the results of
this suggest that our measures conform to human intuition.

</details>


### [77] [Leveraging Simultaneous Usage of Edge GPU Hardware Engines for Video Face Detection and Recognition](https://arxiv.org/abs/2505.04502)
*Asma Baobaid,Mahmoud Meribout*

Main category: cs.CV

TL;DR: 本文提出了一种在边缘GPU上同时利用多个硬件引擎进行视频人脸检测和识别的方法，提高了吞吐量并节省了少量功耗。


<details>
  <summary>Details</summary>
Motivation: 公共场合的视频人脸检测和识别在安全和无接触访问等应用中需求增加，但现有方法通常仅使用单一硬件引擎，效率不足。

Method: 通过并发和流水线化任务（包括视频解码、人脸检测和识别），利用边缘GPU的多个硬件引擎。

Result: 在NVIDIA边缘Orin GPU上实现了更高的吞吐量，功耗节省约300 mW（5%），同时满足实时性能要求。

Conclusion: 建议进一步优化硬件设计以减少计算层数，从而提升性能。

Abstract: Video face detection and recognition in public places at the edge is required
in several applications, such as security reinforcement and contactless access
to authorized venues. This paper aims to maximize the simultaneous usage of
hardware engines available in edge GPUs nowadays by leveraging the concurrency
and pipelining of tasks required for face detection and recognition. This also
includes the video decoding task, which is required in most face monitoring
applications as the video streams are usually carried via Gbps Ethernet
network. This constitutes an improvement over previous works where the tasks
are usually allocated to a single engine due to the lack of a unified and
automated framework that simultaneously explores all hardware engines. In
addition, previously, the input faces were usually embedded in still images or
within raw video streams that overlook the burst delay caused by the decoding
stage. The results on real-life video streams suggest that simultaneously using
all the hardware engines available in the recent NVIDIA edge Orin GPU, higher
throughput, and a slight saving of power consumption of around 300 mW,
accounting for around 5%, have been achieved while satisfying the real-time
performance constraint. The performance gets even higher by considering several
video streams simultaneously. Further performance improvement could have been
obtained if the number of shuffle layers that were created by the tensor RT
framework for the face recognition task was lower. Thus, the paper suggests
some hardware improvements to the existing edge GPU processors to enhance their
performance even higher.

</details>


### [78] [HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation](https://arxiv.org/abs/2505.04512)
*Teng Hu,Zhentao Yu,Zhengguang Zhou,Sen Liang,Yuan Zhou,Qin Lin,Qinglin Lu*

Main category: cs.CV

TL;DR: HunyuanCustom是一个多模态定制视频生成框架，支持图像、音频、视频和文本输入，强调主题一致性，并在ID一致性、真实性和文本-视频对齐方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在身份一致性和输入模态多样性上存在不足，HunyuanCustom旨在解决这些问题。

Method: 基于HunyuanVideo，引入文本-图像融合模块和图像ID增强模块；针对音频和视频输入，提出AudioNet和视频驱动注入模块。

Result: 实验表明HunyuanCustom在单/多主题场景中表现优异，ID一致性、真实性和对齐效果显著提升。

Conclusion: 多模态条件和身份保持策略有效推动了可控视频生成的发展。

Abstract: Customized video generation aims to produce videos featuring specific
subjects under flexible user-defined conditions, yet existing methods often
struggle with identity consistency and limited input modalities. In this paper,
we propose HunyuanCustom, a multi-modal customized video generation framework
that emphasizes subject consistency while supporting image, audio, video, and
text conditions. Built upon HunyuanVideo, our model first addresses the
image-text conditioned generation task by introducing a text-image fusion
module based on LLaVA for enhanced multi-modal understanding, along with an
image ID enhancement module that leverages temporal concatenation to reinforce
identity features across frames. To enable audio- and video-conditioned
generation, we further propose modality-specific condition injection
mechanisms: an AudioNet module that achieves hierarchical alignment via spatial
cross-attention, and a video-driven injection module that integrates
latent-compressed conditional video through a patchify-based feature-alignment
network. Extensive experiments on single- and multi-subject scenarios
demonstrate that HunyuanCustom significantly outperforms state-of-the-art open-
and closed-source methods in terms of ID consistency, realism, and text-video
alignment. Moreover, we validate its robustness across downstream tasks,
including audio and video-driven customized video generation. Our results
highlight the effectiveness of multi-modal conditioning and identity-preserving
strategies in advancing controllable video generation. All the code and models
are available at https://hunyuancustom.github.io.

</details>


### [79] [Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions Using Diffusion Model](https://arxiv.org/abs/2505.04522)
*Pengfei Guo,Can Zhao,Dong Yang,Yufan He,Vishwesh Nath,Ziyue Xu,Pedro R. A. S. Bassi,Zongwei Zhou,Benjamin D. Simon,Stephanie Anne Harmon,Baris Turkbey,Daguang Xu*

Main category: cs.CV

TL;DR: Text2CT是一种基于扩散模型的新方法，可从自由文本描述生成3D CT体积，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 通过自由文本生成3D CT体积，为诊断和研究提供新机会。

Method: 使用扩散模型，将医学文本编码为潜在表示并解码为高分辨率3D CT扫描。

Result: 方法在保持解剖保真度和捕捉细节结构方面表现优异，达到最先进水平。

Conclusion: Text2CT在诊断和数据增强方面具有广阔应用前景。

Abstract: Generating 3D CT volumes from descriptive free-text inputs presents a
transformative opportunity in diagnostics and research. In this paper, we
introduce Text2CT, a novel approach for synthesizing 3D CT volumes from textual
descriptions using the diffusion model. Unlike previous methods that rely on
fixed-format text input, Text2CT employs a novel prompt formulation that
enables generation from diverse, free-text descriptions. The proposed framework
encodes medical text into latent representations and decodes them into
high-resolution 3D CT scans, effectively bridging the gap between semantic text
inputs and detailed volumetric representations in a unified 3D framework. Our
method demonstrates superior performance in preserving anatomical fidelity and
capturing intricate structures as described in the input text. Extensive
evaluations show that our approach achieves state-of-the-art results, offering
promising potential applications in diagnostics, and data augmentation.

</details>


### [80] [Edge-GPU Based Face Tracking for Face Detection and Recognition Acceleration](https://arxiv.org/abs/2505.04524)
*Asma Baobaid,Mahmoud Meribout*

Main category: cs.CV

TL;DR: 提出了一种结合硬件-软件的优化方法，用于在NVIDIA Jetson AGX Orin边缘GPU上提升人脸检测与识别系统的性能，通过同时利用所有硬件引擎和集成人脸跟踪模块，实现了290 FPS的高吞吐量和约800 mW的功耗节省。


<details>
  <summary>Details</summary>
Motivation: 现代应用中，公共场合的实时、准确人脸检测与识别系统需求迫切，但现有系统在吞吐量和功耗方面仍有改进空间。

Method: 结合硬件-软件优化，充分利用Orin GPU的所有硬件引擎，并集成人脸跟踪模块以减少冗余计算。

Result: 在1920x1080分辨率、平均每帧6张人脸的情况下，系统吞吐量达到290 FPS，功耗节省约800 mW。

Conclusion: 该硬件-软件协同设计方法为高性能边缘机器视觉系统提供了可行方案，适用于公共场合的多摄像头监控场景。

Abstract: Cost-effective machine vision systems dedicated to real-time and accurate
face detection and recognition in public places are crucial for many modern
applications. However, despite their high performance, which could be reached
using specialized edge or cloud AI hardware accelerators, there is still room
for improvement in throughput and power consumption. This paper aims to suggest
a combined hardware-software approach that optimizes face detection and
recognition systems on one of the latest edge GPUs, namely NVIDIA Jetson AGX
Orin. First, it leverages the simultaneous usage of all its hardware engines to
improve processing time. This offers an improvement over previous works where
these tasks were mainly allocated automatically and exclusively to the CPU or,
to a higher extent, to the GPU core. Additionally, the paper suggests
integrating a face tracker module to avoid redundantly running the face
recognition algorithm for every frame but only when a new face appears in the
scene. The results of extended experiments suggest that simultaneous usage of
all the hardware engines that are available in the Orin GPU and tracker
integration into the pipeline yield an impressive throughput of 290 FPS (frames
per second) on 1920 x 1080 input size frames containing in average of 6
faces/frame. Additionally, a substantial saving of power consumption of around
800 mW was achieved when compared to running the task on the CPU/GPU engines
only and without integrating a tracker into the Orin GPU\'92s pipeline. This
hardware-codesign approach can pave the way to design high-performance machine
vision systems at the edge, critically needed in video monitoring in public
places where several nearby cameras are usually deployed for a same scene.

</details>


### [81] [DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement and Fusion All at Once](https://arxiv.org/abs/2505.04526)
*Qi Zhou,Yukai Shi,Xiaojun Yang,Xiaoyu Xian,Lunjia Liao,Ruimao Zhang,Liang Lin*

Main category: cs.CV

TL;DR: 论文提出了一种名为DFVO的网络，用于在黑暗环境中处理可见光和红外图像的分离与融合，通过多任务级联方法替代传统两阶段训练，解决了信息熵损失问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有图像融合方法在可见光图像光照退化严重时，融合结果模糊且暗淡，影响自动驾驶等高层次视觉任务。

Method: 设计了DFVO网络，包括潜在共同特征提取器（LCFE）、细节提取模块（DEM）和超交叉注意力模块（HCAM），并通过相关损失函数指导网络学习。

Result: 实验表明，DFVO在黑暗环境中生成更清晰、信息更丰富且光照均匀的融合结果，在LLVIP数据集上达到63.258 dB PSNR和0.724 CC。

Conclusion: DFVO在可见光和红外图像融合任务中表现优异，为高层次视觉任务提供了更有效的信息。

Abstract: Visible and infrared image fusion is one of the most crucial tasks in the
field of image fusion, aiming to generate fused images with clear structural
information and high-quality texture features for high-level vision tasks.
However, when faced with severe illumination degradation in visible images, the
fusion results of existing image fusion methods often exhibit blurry and dim
visual effects, posing major challenges for autonomous driving. To this end, a
Darkness-Free network is proposed to handle Visible and infrared image
disentanglement and fusion all at Once (DFVO), which employs a cascaded
multi-task approach to replace the traditional two-stage cascaded training
(enhancement and fusion), addressing the issue of information entropy loss
caused by hierarchical data transmission. Specifically, we construct a
latent-common feature extractor (LCFE) to obtain latent features for the
cascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised
to acquire high-frequency semantic information. Secondly, we design a hyper
cross-attention module (HCAM) to extract low-frequency information and preserve
texture features from source images. Finally, a relevant loss function is
designed to guide the holistic network learning, thereby achieving better image
fusion. Extensive experiments demonstrate that our proposed approach
outperforms state-of-the-art alternatives in terms of qualitative and
quantitative evaluations. Particularly, DFVO can generate clearer, more
informative, and more evenly illuminated fusion results in the dark
environments, achieving best performance on the LLVIP dataset with 63.258 dB
PSNR and 0.724 CC, providing more effective information for high-level vision
tasks. Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO.

</details>


### [82] [RAFT: Robust Augmentation of FeaTures for Image Segmentation](https://arxiv.org/abs/2505.04529)
*Edward Humes,Xiaomin Lin,Uttej Kallakuri,Tinoosh Mohsenin*

Main category: cs.CV

TL;DR: RAFT是一种新型框架，通过数据和特征增强以及主动学习，利用少量真实数据优化图像分割模型，显著提升了合成到真实场景的迁移性能。


<details>
  <summary>Details</summary>
Motivation: 解决合成数据训练的模型在真实场景中性能下降（Syn2Real问题）的挑战。

Method: 提出RAFT框架，结合数据与特征增强及主动学习，使用少量真实标注数据优化模型。

Result: 在SYNTHIA->Cityscapes和GTAV->Cityscapes基准测试中，mIoU分别提升2.1%和0.4%，在Cityscapes->ACDC中提升1.3%。

Conclusion: RAFT有效缓解了合成到真实场景的迁移问题，性能优于现有方法HALO。

Abstract: Image segmentation is a powerful computer vision technique for scene
understanding. However, real-world deployment is stymied by the need for
high-quality, meticulously labeled datasets. Synthetic data provides
high-quality labels while reducing the need for manual data collection and
annotation. However, deep neural networks trained on synthetic data often face
the Syn2Real problem, leading to poor performance in real-world deployments.
  To mitigate the aforementioned gap in image segmentation, we propose RAFT, a
novel framework for adapting image segmentation models using minimal labeled
real-world data through data and feature augmentations, as well as active
learning. To validate RAFT, we perform experiments on the synthetic-to-real
"SYNTHIA->Cityscapes" and "GTAV->Cityscapes" benchmarks. We managed to surpass
the previous state of the art, HALO. SYNTHIA->Cityscapes experiences an
improvement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes
experiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach
on the real-to-real benchmark of "Cityscapes->ACDC", and again surpass HALO,
with a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the
effect of the allocated annotation budget and various components of RAFT upon
the final transfer mIoU.

</details>


### [83] [Registration of 3D Point Sets Using Exponential-based Similarity Matrix](https://arxiv.org/abs/2505.04540)
*Ashutosh Singandhupe,Sanket Lokhande,Hung Manh La*

Main category: cs.CV

TL;DR: 提出了一种改进的ICP算法（ESM-ICP），通过动态调整相似性矩阵，解决了点云配准中旋转差异大和噪声干扰的问题。


<details>
  <summary>Details</summary>
Motivation: 现有配准技术在大旋转差异或噪声干扰下表现不佳，导致配准不准确。

Method: 引入高斯启发的指数加权方案，构建动态相似性矩阵，改进旋转和平移估计。

Result: ESM-ICP在大旋转差异和非高斯噪声下优于传统几何方法和部分学习型方法。

Conclusion: ESM-ICP显著提升了配准鲁棒性，代码已开源。

Abstract: Point cloud registration is a fundamental problem in computer vision and
robotics, involving the alignment of 3D point sets captured from varying
viewpoints using depth sensors such as LiDAR or structured light. In modern
robotic systems, especially those focused on mapping, it is essential to merge
multiple views of the same environment accurately. However, state-of-the-art
registration techniques often struggle when large rotational differences exist
between point sets or when the data is significantly corrupted by sensor noise.
These challenges can lead to misalignments and, consequently, to inaccurate or
distorted 3D reconstructions. In this work, we address both these limitations
by proposing a robust modification to the classic Iterative Closest Point (ICP)
algorithm. Our method, termed Exponential Similarity Matrix ICP (ESM-ICP),
integrates a Gaussian-inspired exponential weighting scheme to construct a
similarity matrix that dynamically adapts across iterations. This matrix
facilitates improved estimation of both rotational and translational components
during alignment. We demonstrate the robustness of ESM-ICP in two challenging
scenarios: (i) large rotational discrepancies between the source and target
point clouds, and (ii) data corrupted by non-Gaussian noise. Our results show
that ESM-ICP outperforms traditional geometric registration techniques as well
as several recent learning-based methods. To encourage reproducibility and
community engagement, our full implementation is made publicly available on
GitHub. https://github.com/aralab-unr/ESM_ICP

</details>


### [84] [Componential Prompt-Knowledge Alignment for Domain Incremental Learning](https://arxiv.org/abs/2505.04575)
*Kunlun Xu,Xu Zou,Gang Hua,Jiahuan Zhou*

Main category: cs.CV

TL;DR: KA-Prompt通过组件感知的提示-知识对齐解决了领域增量学习中提示方法的知识冲突问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 揭示现有提示方法中因知识组件随机定位导致的知识冲突和预测退化问题。

Method: 提出KA-Prompt，分两阶段：初始组件结构配置和在线对齐保持，实现新旧提示的组件对齐。

Result: 在DIL基准测试中表现优异。

Conclusion: KA-Prompt有效解决了知识冲突问题，提升了模型的跨领域学习能力。

Abstract: Domain Incremental Learning (DIL) aims to learn from non-stationary data
streams across domains while retaining and utilizing past knowledge. Although
prompt-based methods effectively store multi-domain knowledge in prompt
parameters and obtain advanced performance through cross-domain prompt fusion,
we reveal an intrinsic limitation: component-wise misalignment between
domain-specific prompts leads to conflicting knowledge integration and degraded
predictions. This arises from the random positioning of knowledge components
within prompts, where irrelevant component fusion introduces interference.To
address this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a
novel prompt-based DIL method that introduces component-aware prompt-knowledge
alignment during training, significantly improving both the learning and
inference capacity of the model. KA-Prompt operates in two phases: (1) Initial
Componential Structure Configuring, where a set of old prompts containing
knowledge relevant to the new domain are mined via greedy search, which is then
exploited to initialize new prompts to achieve reusable knowledge transfer and
establish intrinsic alignment between new and old prompts. (2) Online Alignment
Preservation, which dynamically identifies the target old prompts and applies
adaptive componential consistency constraints as new prompts evolve. Extensive
experiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.
Our source code is available at
https://github.com/zhoujiahuan1991/ICML2025-KA-Prompt

</details>


### [85] [Active Sampling for MRI-based Sequential Decision Making](https://arxiv.org/abs/2505.04586)
*Yuning Du,Jingshuai Liu,Rohan Dharmakumar,Sotirios A. Tsaftaris*

Main category: cs.CV

TL;DR: 提出一种多目标强化学习框架，用于从欠采样的k空间数据中进行全面、连续的诊断评估，显著减少样本需求。


<details>
  <summary>Details</summary>
Motivation: 降低MRI的高成本和复杂性，使其成为实用的即时诊断设备。

Method: 采用多目标强化学习框架，动态调整采样策略，优化样本选择。

Result: 在膝关节病理评估任务中表现优异，显著节省k空间样本。

Conclusion: 为MRI作为经济高效的即时诊断设备提供了新途径。

Abstract: Despite the superior diagnostic capability of Magnetic Resonance Imaging
(MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and
complexity. To enable such a future by reducing the magnetic field strength,
one key approach will be to improve sampling strategies. Previous work has
shown that it is possible to make diagnostic decisions directly from k-space
with fewer samples. Such work shows that single diagnostic decisions can be
made, but if we aspire to see MRI as a true PoC, multiple and sequential
decisions are necessary while minimizing the number of samples acquired. We
present a novel multi-objective reinforcement learning framework enabling
comprehensive, sequential, diagnostic evaluation from undersampled k-space
data. Our approach during inference actively adapts to sequential decisions to
optimally sample. To achieve this, we introduce a training methodology that
identifies the samples that contribute the best to each diagnostic objective
using a step-wise weighting reward function. We evaluate our approach in two
sequential knee pathology assessment tasks: ACL sprain detection and cartilage
thickness loss assessment. Our framework achieves diagnostic performance
competitive with various policy-based benchmarks on disease detection, severity
quantification, and overall sequential diagnosis, while substantially saving
k-space samples. Our approach paves the way for the future of MRI as a
comprehensive and affordable PoC device. Our code is publicly available at
https://github.com/vios-s/MRI_Sequential_Active_Sampling

</details>


### [86] [MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection](https://arxiv.org/abs/2505.04594)
*Zhihao Zhang,Abhinav Kumar,Girish Chandar Ganesan,Xiaoming Liu*

Main category: cs.CV

TL;DR: 论文提出MonoCoP方法，通过链式预测（CoP）顺序预测3D属性，结合AttributeNet和残差连接，显著提升单目3D物体检测的深度估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略3D属性间的内在关联，导致深度估计精度受限。受大语言模型中Chain-of-Thought启发，提出链式预测以解决这一问题。

Method: 1. 为每个3D属性设计轻量级AttributeNet；2. 构建显式链式结构传播特征；3. 使用残差连接确保后续属性预测依赖先前属性。

Result: 在KITTI、Waymo和nuScenes数据集上达到最优性能，无需额外数据。

Conclusion: MonoCoP通过链式预测有效建模3D属性关联，显著提升单目3D检测的精度和稳定性。

Abstract: Accurately predicting 3D attributes is crucial for monocular 3D object
detection (Mono3D), with depth estimation posing the greatest challenge due to
the inherent ambiguity in mapping 2D images to 3D space. While existing methods
leverage multiple depth cues (e.g., estimating depth uncertainty, modeling
depth error) to improve depth accuracy, they overlook that accurate depth
prediction requires conditioning on other 3D attributes, as these attributes
are intrinsically inter-correlated through the 3D to 2D projection, which
ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought
(CoT) in large language models (LLMs), this paper proposes MonoCoP, which
leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and
conditionally via three key designs. First, it employs a lightweight
AttributeNet (AN) for each 3D attribute to learn attribute-specific features.
Next, MonoCoP constructs an explicit chain to propagate these learned features
from one attribute to the next. Finally, MonoCoP uses a residual connection to
aggregate features for each attribute along the chain, ensuring that later
attribute predictions are conditioned on all previously processed attributes
without forgetting the features of earlier ones. Experimental results show that
our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI
leaderboard without requiring additional data and further surpasses existing
methods on the Waymo and nuScenes frontal datasets.

</details>


### [87] [OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning](https://arxiv.org/abs/2505.04601)
*Xianhang Li,Yanqing Liu,Haoqin Tu,Hongru Zhu,Cihang Xie*

Main category: cs.CV

TL;DR: OpenVision是一个完全开放的视觉编码器家族，性能媲美或超越CLIP，提供从5.9M到632.1M参数的灵活选择。


<details>
  <summary>Details</summary>
Motivation: 填补现有视觉编码器（如CLIP）训练数据和配方未完全开放的空白，提供成本效益高的替代方案。

Method: 基于现有工作（如CLIPS训练框架和Recap-DataComp-1B训练数据），优化编码器质量并展示实际优势。

Result: OpenVision在性能上匹配或超越CLIP，并支持从轻量级到高性能的多模态模型部署。

Conclusion: OpenVision为多模态模型提供了灵活、高效且完全开放的视觉编码解决方案。

Abstract: OpenAI's CLIP, released in early 2021, have long been the go-to choice of
vision encoder for building multimodal foundation models. Although recent
alternatives such as SigLIP have begun to challenge this status quo, to our
knowledge none are fully open: their training data remains proprietary and/or
their training recipes are not released. This paper fills this gap with
OpenVision, a fully-open, cost-effective family of vision encoders that match
or surpass the performance of OpenAI's CLIP when integrated into multimodal
frameworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for
training framework and Recap-DataComp-1B for training data -- while revealing
multiple key insights in enhancing encoder quality and showcasing practical
benefits in advancing multimodal models. By releasing vision encoders spanning
from 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible
trade-off between capacity and efficiency in building multimodal models: larger
models deliver enhanced multimodal performance, while smaller versions enable
lightweight, edge-ready multimodal deployments.

</details>


### [88] [FastMap: Revisiting Dense and Scalable Structure from Motion](https://arxiv.org/abs/2505.04612)
*Jiahao Li,Haochen Wang,Muhammad Zubair Irshad,Igor Vasiljevic,Matthew R. Walter,Vitor Campagnolo Guizilini,Greg Shakhnarovich*

Main category: cs.CV

TL;DR: FastMap是一种新的全局运动结构方法，专注于速度和简洁性，解决了COLMAP和GLOMAP在大规模场景中扩展性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如COLMAP和GLOMAP）虽然能估计高精度相机姿态，但在匹配关键点对数量大时扩展性差，主要由于并行性差和优化步骤计算成本高。

Method: 设计了完全基于GPU友好操作的SfM框架，易于并行化，且每个优化步骤的运行时间与图像对数量线性相关。

Result: 实验表明，FastMap在大规模场景中比COLMAP和GLOMAP快一到两个数量级，且姿态精度相当。

Conclusion: FastMap通过优化并行性和计算效率，显著提升了大规模场景下的运动结构估计速度。

Abstract: We propose FastMap, a new global structure from motion method focused on
speed and simplicity. Previous methods like COLMAP and GLOMAP are able to
estimate high-precision camera poses, but suffer from poor scalability when the
number of matched keypoint pairs becomes large. We identify two key factors
leading to this problem: poor parallelization and computationally expensive
optimization steps. To overcome these issues, we design an SfM framework that
relies entirely on GPU-friendly operations, making it easily parallelizable.
Moreover, each optimization step runs in time linear to the number of image
pairs, independent of keypoint pairs or 3D points. Through extensive
experiments, we show that FastMap is one to two orders of magnitude faster than
COLMAP and GLOMAP on large-scale scenes with comparable pose accuracy.

</details>


### [89] [Person Recognition at Altitude and Range: Fusion of Face, Body Shape and Gait](https://arxiv.org/abs/2505.04616)
*Feng Liu,Nicholas Chimitt,Lanqing Guo,Jitesh Jain,Aditya Kane,Minchul Kim,Wes Robbins,Yiyang Su,Dingqiang Ye,Xingguang Zhang,Jie Zhu,Siddharth Satyakam,Christopher Perry,Stanley H. Chan,Arun Ross,Humphrey Shi,Zhangyang Wang,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: FarSight是一个端到端的全身人物识别系统，整合了面部、步态和体型等多模态生物特征，在恶劣条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决在无约束环境（如远距离、高视角和恶劣天气）下的全身人物识别问题，适用于监控场景。

Method: 系统包含四个核心模块：多目标检测与跟踪、识别感知的视频恢复、模态特异性生物特征编码和质量引导的多模态融合。

Result: 在BRIAR数据集上，FarSight在1:1验证、闭集识别和开集识别任务中分别提升了34.1%、17.8%和34.3%的性能。

Conclusion: FarSight是恶劣现实条件下生物识别的先进解决方案。

Abstract: We address the problem of whole-body person recognition in unconstrained
environments. This problem arises in surveillance scenarios such as those in
the IARPA Biometric Recognition and Identification at Altitude and Range
(BRIAR) program, where biometric data is captured at long standoff distances,
elevated viewing angles, and under adverse atmospheric conditions (e.g.,
turbulence and high wind velocity). To this end, we propose FarSight, a unified
end-to-end system for person recognition that integrates complementary
biometric cues across face, gait, and body shape modalities. FarSight
incorporates novel algorithms across four core modules: multi-subject detection
and tracking, recognition-aware video restoration, modality-specific biometric
feature encoding, and quality-guided multi-modal fusion. These components are
designed to work cohesively under degraded image conditions, large pose and
scale variations, and cross-domain gaps. Extensive experiments on the BRIAR
dataset, one of the most comprehensive benchmarks for long-range, multi-modal
biometric recognition, demonstrate the effectiveness of FarSight. Compared to
our preliminary system, this system achieves a 34.1% absolute gain in 1:1
verification accuracy (TAR@0.1% FAR), a 17.8% increase in closed-set
identification (Rank-20), and a 34.3% reduction in open-set identification
errors (FNIR@1% FPIR). Furthermore, FarSight was evaluated in the 2025 NIST RTE
Face in Video Evaluation (FIVE), which conducts standardized face recognition
testing on the BRIAR dataset. These results establish FarSight as a
state-of-the-art solution for operational biometric recognition in challenging
real-world conditions.

</details>


### [90] [On Path to Multimodal Generalist: General-Level and General-Bench](https://arxiv.org/abs/2505.04620)
*Hao Fei,Yuan Zhou,Juncheng Li,Xiangtai Li,Qingshan Xu,Bobo Li,Shengqiong Wu,Yaoting Wang,Junbao Zhou,Jiahao Meng,Qingyu Shi,Zhiyuan Zhou,Liangtao Shi,Minghe Gao,Daoan Zhang,Zhiqi Ge,Weiming Wu,Siliang Tang,Kaihang Pan,Yaobo Ye,Haobo Yuan,Tao Zhang,Tianjie Ju,Zixiang Meng,Shilin Xu,Liyu Jia,Wentao Hu,Meng Luo,Jiebo Luo,Tat-Seng Chua,Shuicheng Yan,Hanwang Zhang*

Main category: cs.CV

TL;DR: 论文介绍了General-Level评估框架，用于衡量多模态大语言模型（MLLM）的性能和通用性，并提出了Synergy概念来评估模型在多模态理解和生成中的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评估方法无法全面衡量模型性能，无法判断其是否接近人类水平AI。

Method: 提出General-Level框架和General-Bench数据集，涵盖700多个任务和325,800个实例，评估100多个MLLM。

Result: 揭示了MLLM的能力排名，展示了实现真正AI的挑战。

Conclusion: 该框架为下一代多模态基础模型研究提供了基础设施，加速AGI的实现。

Abstract: The Multimodal Large Language Model (MLLM) is currently experiencing rapid
growth, driven by the advanced capabilities of LLMs. Unlike earlier
specialists, existing MLLMs are evolving towards a Multimodal Generalist
paradigm. Initially limited to understanding multiple modalities, these models
have advanced to not only comprehend but also generate across modalities. Their
capabilities have expanded from coarse-grained to fine-grained multimodal
understanding and from supporting limited modalities to arbitrary ones. While
many benchmarks exist to assess MLLMs, a critical question arises: Can we
simply assume that higher performance across tasks indicates a stronger MLLM
capability, bringing us closer to human-level AI? We argue that the answer is
not as straightforward as it seems. This project introduces General-Level, an
evaluation framework that defines 5-scale levels of MLLM performance and
generality, offering a methodology to compare MLLMs and gauge the progress of
existing systems towards more robust multimodal generalists and, ultimately,
towards AGI. At the core of the framework is the concept of Synergy, which
measures whether models maintain consistent capabilities across comprehension
and generation, and across multiple modalities. To support this evaluation, we
present General-Bench, which encompasses a broader spectrum of skills,
modalities, formats, and capabilities, including over 700 tasks and 325,800
instances. The evaluation results that involve over 100 existing
state-of-the-art MLLMs uncover the capability rankings of generalists,
highlighting the challenges in reaching genuine AI. We expect this project to
pave the way for future research on next-generation multimodal foundation
models, providing a robust infrastructure to accelerate the realization of AGI.
Project page: https://generalist.top/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [91] [Proceedings of 1st Workshop on Advancing Artificial Intelligence through Theory of Mind](https://arxiv.org/abs/2505.03770)
*Mouad Abrini,Omri Abend,Dina Acklin,Henny Admoni,Gregor Aichinger,Nitay Alon,Zahra Ashktorab,Ashish Atreja,Moises Auron,Alexander Aufreiter,Raghav Awasthi,Soumya Banerjee,Joe M. Barnby,Rhea Basappa,Severin Bergsmann,Djallel Bouneffouf,Patrick Callaghan,Marc Cavazza,Thierry Chaminade,Sonia Chernova,Mohamed Chetouan,Moumita Choudhury,Axel Cleeremans,Jacek B. Cywinski,Fabio Cuzzolin,Hokin Deng,N'yoma Diamond,Camilla Di Pasquasio,Guillaume Dumas,Max van Duijn,Mahapatra Dwarikanath,Qingying Gao,Ashok Goel,Rebecca Goldstein,Matthew Gombolay,Gabriel Enrique Gonzalez,Amar Halilovic,Tobias Halmdienst,Mahimul Islam,Julian Jara-Ettinger,Natalie Kastel,Renana Keydar,Ashish K. Khanna,Mahdi Khoramshahi,JiHyun Kim,MiHyeon Kim,YoungBin Kim,Senka Krivic,Nikita Krasnytskyi,Arun Kumar,JuneHyoung Kwon,Eunju Lee,Shane Lee,Peter R. Lewis,Xue Li,Yijiang Li,Michal Lewandowski,Nathan Lloyd,Matthew B. Luebbers,Dezhi Luo,Haiyun Lyu,Dwarikanath Mahapatra,Kamal Maheshwari,Mallika Mainali,Piyush Mathur,Patrick Mederitsch,Shuwa Miura,Manuel Preston de Miranda,Reuth Mirsky,Shreya Mishra,Nina Moorman,Katelyn Morrison,John Muchovej,Bernhard Nessler,Felix Nessler,Hieu Minh Jord Nguyen,Abby Ortego,Francis A. Papay,Antoine Pasquali,Hamed Rahimi,Charumathi Raghu,Amanda Royka,Stefan Sarkadi,Jaelle Scheuerman,Simon Schmid,Paul Schrater,Anik Sen,Zahra Sheikhbahaee,Ke Shi,Reid Simmons,Nishant Singh,Mason O. Smith,Ramira van der Meulen,Anthia Solaki,Haoran Sun,Viktor Szolga,Matthew E. Taylor,Travis Taylor,Sanne Van Waveren,Juan David Vargas,Rineke Verbrugge,Eitan Wagner,Justin D. Weisz,Ximing Wen,William Yeoh,Wenlong Zhang,Michelle Zhao,Shlomo Zilberstein*

Main category: cs.AI

TL;DR: 该论文集收录了2025年AAAI研讨会上关于通过心智理论推动人工智能发展的论文，旨在为ToM和AI研究社区提供开放获取的精选资源。


<details>
  <summary>Details</summary>
Motivation: 为心智理论（ToM）和人工智能（AI）研究社区提供一个开放获取的精选论文集，促进相关领域的交流与发展。

Method: 通过研讨会的形式收集和筛选相关研究论文，并整理成开放获取的论文集。

Result: 出版了一本开放获取的论文集，涵盖了ToM与AI领域的最新研究成果。

Conclusion: 该论文集为ToM和AI研究社区提供了有价值的资源，推动了相关领域的学术交流与进步。

Abstract: This volume includes a selection of papers presented at the Workshop on
Advancing Artificial Intelligence through Theory of Mind held at AAAI 2025 in
Philadelphia US on 3rd March 2025. The purpose of this volume is to provide an
open access and curated anthology for the ToM and AI research community.

</details>


### [92] [Design description of Wisdom Computing Persperctive](https://arxiv.org/abs/2505.03800)
*TianYi Yu*

Main category: cs.AI

TL;DR: 开发了一个结合AI和可视化动画的手写矩阵识别与计算步骤展示系统，帮助学生理解数学计算逻辑。


<details>
  <summary>Details</summary>
Motivation: 解决学生在学习数学时对抽象公式和复杂计算步骤难以理解的问题。

Method: 使用Mamba骨干网络和YOLO模型进行手写矩阵识别与重构，结合CoordAttention机制优化空间定位，并通过Manim动画引擎分步展示计算过程。

Result: 系统具有高模块化和灵活性，能实时生成不同数学运算示例，提升学生对计算逻辑的直观理解。

Conclusion: 该系统通过创新的交互方式，实现了“每一步都理解”的学习体验，是一种直观、高效的教育辅助工具。

Abstract: This course design aims to develop and research a handwriting matrix
recognition and step-by-step visual calculation process display system,
addressing the issue of abstract formulas and complex calculation steps that
students find difficult to understand when learning mathematics. By integrating
artificial intelligence with visualization animation technology, the system
enhances precise recognition of handwritten matrix content through the
introduction of Mamba backbone networks, completes digital extraction and
matrix reconstruction using the YOLO model, and simultaneously combines
CoordAttention coordinate attention mechanisms to improve the accurate grasp of
character spatial positions. The calculation process is demonstrated frame by
frame through the Manim animation engine, vividly showcasing each mathematical
calculation step, helping students intuitively understand the intrinsic logic
of mathematical operations. Through dynamically generating animation processes
for different computational tasks, the system exhibits high modularity and
flexibility, capable of generating various mathematical operation examples in
real-time according to student needs. By innovating human-computer interaction
methods, it brings mathematical calculation processes to life, helping students
bridge the gap between knowledge and understanding on a deeper level,
ultimately achieving a learning experience where "every step is understood."
The system's scalability and interactivity make it an intuitive, user-friendly,
and efficient auxiliary tool in education.

</details>


### [93] [GRAML: Dynamic Goal Recognition As Metric Learning](https://arxiv.org/abs/2505.03941)
*Matan Shamir,Reuth Mirsky*

Main category: cs.AI

TL;DR: GRAML提出了一种基于度量学习的目标识别方法，通过Siamese网络和RNN学习嵌入空间中的距离度量，能够快速适应新目标。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法需要预定义目标集且训练耗时，GRAML旨在实现自动化模型学习并快速适应新目标。

Method: 使用Siamese网络和RNN学习嵌入空间中的距离度量，使不同目标的观测轨迹嵌入距离远，相同目标的嵌入距离近。

Result: GRAML在多种环境中表现出速度、灵活性和运行时的优势，同时保持高识别准确率。

Conclusion: GRAML为快速适应新目标提供了一种高效且准确的目标识别解决方案。

Abstract: Goal Recognition (GR) is the problem of recognizing an agent's objectives
based on observed actions. Recent data-driven approaches for GR alleviate the
need for costly, manually crafted domain models. However, these approaches can
only reason about a pre-defined set of goals, and time-consuming training is
needed for new emerging goals. To keep this model-learning automated while
enabling quick adaptation to new goals, this paper introduces GRAML: Goal
Recognition As Metric Learning. GRAML uses a Siamese network to treat GR as a
deep metric learning task, employing an RNN that learns a metric over an
embedding space, where the embeddings for observation traces leading to
different goals are distant, and embeddings of traces leading to the same goals
are close. This metric is especially useful when adapting to new goals, even if
given just one example observation trace per goal. Evaluated on a versatile set
of environments, GRAML shows speed, flexibility, and runtime improvements over
the state-of-the-art GR while maintaining accurate recognition.

</details>


### [94] [Frog Soup: Zero-Shot, In-Context, and Sample-Efficient Frogger Agents](https://arxiv.org/abs/2505.03947)
*Xiang Li,Yiyang Hao,Doug Fulop*

Main category: cs.AI

TL;DR: 论文展示了如何利用推理型LLM在零样本设置下玩Atari游戏Frogger，并探讨了上下文学习和推理量对性能的影响，同时提出了一种用LLM演示提升传统RL方法性能的方式。


<details>
  <summary>Details</summary>
Motivation: 强化学习研究的主要目标是开发能够快速适应并掌握新任务的通用智能体。尽管RL游戏智能体已掌握许多Atari游戏，但训练过程仍然缓慢且成本高昂。

Method: 使用推理型LLM进行零样本训练，研究上下文学习和推理量对性能的影响，并通过LLM演示提升传统RL方法的性能。

Result: LLM在零样本设置下成功玩转Frogger，上下文学习和推理量显著影响性能，LLM演示显著提升了传统RL方法的性能和样本效率。

Conclusion: 研究表明，推理型LLM在零样本RL任务中具有潜力，并能通过演示提升传统RL方法的效率。

Abstract: One of the primary aspirations in reinforcement learning research is
developing general-purpose agents capable of rapidly adapting to and mastering
novel tasks. While RL gaming agents have mastered many Atari games, they remain
slow and costly to train for each game. In this work, we demonstrate that
latest reasoning LLMs with out-of-domain RL post-training can play a
challenging Atari game called Frogger under a zero-shot setting. We then
investigate the effect of in-context learning and the amount of reasoning
effort on LLM performance. Lastly, we demonstrate a way to bootstrap
traditional RL method with LLM demonstrations, which significantly improves
their performance and sample efficiency. Our implementation is open sourced at
https://github.com/AlienKevin/frogger.

</details>


### [95] [The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete](https://arxiv.org/abs/2505.03961)
*Gerrit Großmann,Larisa Ivanova,Sai Leela Poduru,Mohaddeseh Tabrizian,Islam Mesabah,David A. Selby,Sebastian J. Vollmer*

Main category: cs.AI

TL;DR: 研究探讨共享叙事是否能促进LLM代理合作，实验表明共同叙事显著提升合作效果，而不同叙事则降低合作。


<details>
  <summary>Details</summary>
Motivation: 探索共享叙事对LLM代理合作行为的影响，以验证人类合作机制是否适用于AI。

Method: 采用有限重复公共物品游戏，通过叙事引导代理选择合作或自私策略，分析谈判结果。

Result: 共同叙事显著提高合作成功率，不同叙事则导致自私行为占优。

Conclusion: 研究结果对多代理系统设计和AI对齐具有潜在意义。

Abstract: According to Yuval Noah Harari, large-scale human cooperation is driven by
shared narratives that encode common beliefs and values. This study explores
whether such narratives can similarly nudge LLM agents toward collaboration. We
use a finitely repeated public goods game in which LLM agents choose either
cooperative or egoistic spending strategies. We prime agents with stories
highlighting teamwork to different degrees and test how this influences
negotiation outcomes. Our experiments explore four questions:(1) How do
narratives influence negotiation behavior? (2) What differs when agents share
the same story versus different ones? (3) What happens when the agent numbers
grow? (4) Are agents resilient against self-serving negotiators? We find that
story-based priming significantly affects negotiation strategies and success
rates. Common stories improve collaboration, benefiting each agent. By
contrast, priming agents with different stories reverses this effect, and those
agents primed toward self-interest prevail. We hypothesize that these results
carry implications for multi-agent system design and AI alignment.

</details>


### [96] [LogiDebrief: A Signal-Temporal Logic based Automated Debriefing Approach with Large Language Models Integration](https://arxiv.org/abs/2505.03985)
*Zirong Chen,Ziyan An,Jennifer Reynolds,Kristin Mullen,Stephen Martini,Meiyi Ma*

Main category: cs.AI

TL;DR: LogiDebrief是一个AI驱动的框架，通过结合信号时序逻辑和大语言模型，自动化911呼叫的评估过程，提高覆盖率和效率。


<details>
  <summary>Details</summary>
Motivation: 传统人工评估911呼叫效率低、覆盖率低，无法满足高呼叫量的需求。

Method: LogiDebrief采用三步验证：上下文理解、STL与LLM结合的运行时检查、结果自动汇总。

Result: 在真实部署中，LogiDebrief评估了1701个呼叫，节省了311.85小时人工时间，实证显示其准确性高。

Conclusion: LogiDebrief有效提升了911呼叫评估的效率和性能，具有实际应用价值。

Abstract: Emergency response services are critical to public safety, with 9-1-1
call-takers playing a key role in ensuring timely and effective emergency
operations. To ensure call-taking performance consistency, quality assurance is
implemented to evaluate and refine call-takers' skillsets. However, traditional
human-led evaluations struggle with high call volumes, leading to low coverage
and delayed assessments. We introduce LogiDebrief, an AI-driven framework that
automates traditional 9-1-1 call debriefing by integrating Signal-Temporal
Logic (STL) with Large Language Models (LLMs) for fully-covered rigorous
performance evaluation. LogiDebrief formalizes call-taking requirements as
logical specifications, enabling systematic assessment of 9-1-1 calls against
procedural guidelines. It employs a three-step verification process: (1)
contextual understanding to identify responder types, incident classifications,
and critical conditions; (2) STL-based runtime checking with LLM integration to
ensure compliance; and (3) automated aggregation of results into quality
assurance reports. Beyond its technical contributions, LogiDebrief has
demonstrated real-world impact. Successfully deployed at Metro Nashville
Department of Emergency Communications, it has assisted in debriefing 1,701
real-world calls, saving 311.85 hours of active engagement. Empirical
evaluation with real-world data confirms its accuracy, while a case study and
extensive user study highlight its effectiveness in enhancing call-taking
performance.

</details>


### [97] [An alignment safety case sketch based on debate](https://arxiv.org/abs/2505.03989)
*Marie Davidsen Buhl,Jacob Pfau,Benjamin Hilton,Geoffrey Irving*

Main category: cs.AI

TL;DR: 论文探讨了通过辩论机制确保AI系统安全性的方法，提出了一个基于辩论的AI对齐安全案例，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在多任务上超越人类能力，人类难以有效评估其行为，需通过辩论机制利用超级AI系统指出其缺陷，以确保AI安全性。

Method: 提出基于辩论的训练方法，结合探索保证和在线训练，确保AI系统在研发和部署中保持诚实。

Result: 构建了一个AI对齐安全案例，基于四个关键假设：辩论能力、诚实性、稳定性及容错性。

Conclusion: 辩论机制有望成为确保AI安全性的有效方法，但仍需解决多个开放研究问题。

Abstract: If AI systems match or exceed human capabilities on a wide range of tasks, it
may become difficult for humans to efficiently judge their actions -- making it
hard to use human feedback to steer them towards desirable traits. One proposed
solution is to leverage another superhuman system to point out flaws in the
system's outputs via a debate. This paper outlines the value of debate for AI
safety, as well as the assumptions and further research required to make debate
work. It does so by sketching an ``alignment safety case'' -- an argument that
an AI system will not autonomously take actions which could lead to egregious
harm, despite being able to do so. The sketch focuses on the risk of an AI R\&D
agent inside an AI company sabotaging research, for example by producing false
results. To prevent this, the agent is trained via debate, subject to
exploration guarantees, to teach the system to be honest. Honesty is maintained
throughout deployment via online training. The safety case rests on four key
claims: (1) the agent has become good at the debate game, (2) good performance
in the debate game implies that the system is mostly honest, (3) the system
will not become significantly less honest during deployment, and (4) the
deployment context is tolerant of some errors. We identify open research
problems that, if solved, could render this a compelling argument that an AI
system is safe.

</details>


### [98] [Extending Decision Predicate Graphs for Comprehensive Explanation of Isolation Forest](https://arxiv.org/abs/2505.04019)
*Matteo Ceschin,Leonardo Arrighi,Luca Longo,Sylvio Barbon Junior*

Main category: cs.AI

TL;DR: 论文提出了一种新的可解释AI方法，基于决策谓词图（DPG）提升孤立森林（iForest）的全局可解释性，解决了其决策边界不透明的问题。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习中，不仅需要解释模型本身，还需理解数据预处理方法的影响。孤立森林虽然高效，但其多树结构导致决策边界难以解释。

Method: 采用决策谓词图（DPG）和提出的内点-离群点传播分数（IOP-Score），提供全局解释，阐明特征如何影响离群点识别。

Result: 该方法增强了iForest的可解释性，展示了决策边界和特征使用情况，推动了可解释机器学习的发展。

Conclusion: 研究通过DPG和IOP-Score实现了iForest的全局可解释性，为构建透明可靠的机器学习流程提供了新思路。

Abstract: The need to explain predictive models is well-established in modern machine
learning. However, beyond model interpretability, understanding pre-processing
methods is equally essential. Understanding how data modifications impact model
performance improvements and potential biases and promoting a reliable pipeline
is mandatory for developing robust machine learning solutions. Isolation Forest
(iForest) is a widely used technique for outlier detection that performs well.
Its effectiveness increases with the number of tree-based learners. However,
this also complicates the explanation of outlier selection and the decision
boundaries for inliers. This research introduces a novel Explainable AI (XAI)
method, tackling the problem of global explainability. In detail, it aims to
offer a global explanation for outlier detection to address its opaque nature.
Our approach is based on the Decision Predicate Graph (DPG), which clarifies
the logic of ensemble methods and provides both insights and a graph-based
metric to explain how samples are identified as outliers using the proposed
Inlier-Outlier Propagation Score (IOP-Score). Our proposal enhances iForest's
explainability and provides a comprehensive view of the decision-making
process, detailing which features contribute to outlier identification and how
the model utilizes them. This method advances the state-of-the-art by providing
insights into decision boundaries and a comprehensive view of holistic feature
usage in outlier identification. -- thus promoting a fully explainable machine
learning pipeline.

</details>


### [99] [Polynomial-Time Relational Probabilistic Inference in Open Universes](https://arxiv.org/abs/2505.04115)
*Luise Ge,Brendan Juba,Kris Nilsson*

Main category: cs.AI

TL;DR: 论文提出了一种基于一阶关系概率推理的方法，解决了不确定性推理中表达力与计算可处理性的矛盾，支持混合变量，并在多项式时间内完成有界度片段推理。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能中不确定性推理的表达力与计算可处理性之间的矛盾，受人类推理启发。

Method: 扩展期望的平方和逻辑至关系设置，支持混合变量，并在有界度片段中进行多项式时间推理。

Result: 证明了在有界度片段中，推理可在多项式时间内完成，且能处理未知或无限对象集。

Conclusion: 通过证明理论框架实现紧致边界和完备性，为不确定性推理提供了高效且表达力强的方法。

Abstract: Reasoning under uncertainty is a fundamental challenge in Artificial
Intelligence. As with most of these challenges, there is a harsh dilemma
between the expressive power of the language used, and the tractability of the
computational problem posed by reasoning. Inspired by human reasoning, we
introduce a method of first-order relational probabilistic inference that
satisfies both criteria, and can handle hybrid (discrete and continuous)
variables. Specifically, we extend sum-of-squares logic of expectation to
relational settings, demonstrating that lifted reasoning in the bounded-degree
fragment for knowledge bases of bounded quantifier rank can be performed in
polynomial time, even with an a priori unknown and/or countably infinite set of
objects. Crucially, our notion of tractability is framed in proof-theoretic
terms, which extends beyond the syntactic properties of the language or
queries. We are able to derive the tightest bounds provable by proofs of a
given degree and size and establish completeness in our sum-of-squares
refutations for fixed degrees.

</details>


### [100] [Flow Models for Unbounded and Geometry-Aware Distributional Reinforcement Learning](https://arxiv.org/abs/2505.04310)
*Simo Alami C.,Rim Kaddah,Jesse Read,Marie-Paule Cani*

Main category: cs.AI

TL;DR: 提出了一种基于归一化流的分布强化学习新架构，支持灵活的、无界的回报分布建模，优于固定或有限表示方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如C51）依赖固定或有限表示，无法捕捉回报分布的多模态、偏态和尾部行为。

Method: 使用归一化流建模回报分布，提出一种新的Cramér距离替代方法，避免昂贵的CDF计算。

Result: 在ATARI-5子基准测试中表现优于基于PDF的模型，与基于分位数的方法竞争。

Conclusion: 新方法在参数效率和建模能力上优于现有方法，解决了梯度偏差和尺度不敏感问题。

Abstract: We introduce a new architecture for Distributional Reinforcement Learning
(DistRL) that models return distributions using normalizing flows. This
approach enables flexible, unbounded support for return distributions, in
contrast to categorical approaches like C51 that rely on fixed or bounded
representations. It also offers richer modeling capacity to capture
multi-modality, skewness, and tail behavior than quantile based approaches. Our
method is significantly more parameter-efficient than categorical approaches.
Standard metrics used to train existing models like KL divergence or
Wasserstein distance either are scale insensitive or have biased sample
gradients, especially when return supports do not overlap. To address this, we
propose a novel surrogate for the Cram\`er distance, that is geometry-aware and
computable directly from the return distribution's PDF, avoiding the costly CDF
computation. We test our model on the ATARI-5 sub-benchmark and show that our
approach outperforms PDF based models while remaining competitive with quantile
based methods.

</details>


### [101] [KERAIA: An Adaptive and Explainable Framework for Dynamic Knowledge Representation and Reasoning](https://arxiv.org/abs/2505.04313)
*Stephen Richard Varey,Alessandro Di Stefano,The Anh Han*

Main category: cs.AI

TL;DR: KERAIA是一个新型符号知识工程框架，旨在解决动态复杂环境中知识表示、推理和执行的挑战，通过创新方法如知识云和动态关系，结合可解释AI原则，验证了其多样性和实用性。


<details>
  <summary>Details</summary>
Motivation: 如何将非结构化的人类专业知识转化为AI系统可计算的高效算法，是研究的核心问题。

Method: 基于Minsky的框架推理和K-lines，引入知识云、动态关系、显式思维线和云细化等创新方法，设计KSYNTH语言和GPPB工具。

Result: 通过多个案例验证了KERAIA的多样性和实用性，包括海军模拟、工业诊断和战略游戏。

Conclusion: KERAIA超越了传统静态知识表示范式，提供了一种透明、适应性强的新方法。

Abstract: In this paper, we introduce KERAIA, a novel framework and software platform
for symbolic knowledge engineering designed to address the persistent
challenges of representing, reasoning with, and executing knowledge in dynamic,
complex, and context-sensitive environments. The central research question that
motivates this work is: How can unstructured, often tacit, human expertise be
effectively transformed into computationally tractable algorithms that AI
systems can efficiently utilise? KERAIA seeks to bridge this gap by building on
foundational concepts such as Minsky's frame-based reasoning and K-lines, while
introducing significant innovations. These include Clouds of Knowledge for
dynamic aggregation, Dynamic Relations (DRels) for context-sensitive
inheritance, explicit Lines of Thought (LoTs) for traceable reasoning, and
Cloud Elaboration for adaptive knowledge transformation. This approach moves
beyond the limitations of traditional, often static, knowledge representation
paradigms. KERAIA is designed with Explainable AI (XAI) as a core principle,
ensuring transparency and interpretability, particularly through the use of
LoTs. The paper details the framework's architecture, the KSYNTH representation
language, and the General Purpose Paradigm Builder (GPPB) to integrate diverse
inference methods within a unified structure. We validate KERAIA's versatility,
expressiveness, and practical applicability through detailed analysis of
multiple case studies spanning naval warfare simulation, industrial diagnostics
in water treatment plants, and strategic decision-making in the game of RISK.
Furthermore, we provide a comparative analysis against established knowledge
representation paradigms (including ontologies, rule-based systems, and
knowledge graphs) and discuss the implementation aspects and computational
considerations of the KERAIA platform.

</details>


### [102] [Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning](https://arxiv.org/abs/2505.04317)
*Ruize Zhang,Sirui Xiang,Zelai Xu,Feng Gao,Shilong Ji,Wenhao Tang,Wenbo Ding,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: 论文提出了一种分层强化学习框架HCSP，用于解决3v3多无人机排球任务，通过分层训练实现高性能和团队协作。


<details>
  <summary>Details</summary>
Motivation: 解决多无人机排球任务中的高层次战略协调和低层次敏捷控制问题，克服长时依赖、紧密耦合和无人机动力学挑战。

Method: 提出HCSP框架，分三个阶段训练：低层次技能训练、高层次战略自博弈学习、联合微调。

Result: HCSP平均胜率82.9%，优于非分层和规则基线，并涌现出角色切换和队形协调等团队行为。

Conclusion: HCSP的分层设计和训练方案有效，适用于复杂多智能体任务。

Abstract: In this paper, we tackle the problem of learning to play 3v3 multi-drone
volleyball, a new embodied competitive task that requires both high-level
strategic coordination and low-level agile control. The task is turn-based,
multi-agent, and physically grounded, posing significant challenges due to its
long-horizon dependencies, tight inter-agent coupling, and the underactuated
dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play
(HCSP), a hierarchical reinforcement learning framework that separates
centralized high-level strategic decision-making from decentralized low-level
motion control. We design a three-stage population-based training pipeline to
enable both strategy and skill to emerge from scratch without expert
demonstrations: (I) training diverse low-level skills, (II) learning high-level
strategy via self-play with fixed low-level controllers, and (III) joint
fine-tuning through co-self-play. Experiments show that HCSP achieves superior
performance, outperforming non-hierarchical self-play and rule-based
hierarchical baselines with an average 82.9\% win rate and a 71.5\% win rate
against the two-stage variant. Moreover, co-self-play leads to emergent team
behaviors such as role switching and coordinated formations, demonstrating the
effectiveness of our hierarchical design and training scheme.

</details>


### [103] [Uncertain Machine Ethics Planning](https://arxiv.org/abs/2505.04352)
*Simon Kolker,Louise A. Dennis,Ramon Fraga Pereira,Mengwei Xu*

Main category: cs.AI

TL;DR: 论文提出了一种基于多道德马尔可夫决策过程的算法，用于在不确定性下平衡冲突的道德理论。


<details>
  <summary>Details</summary>
Motivation: 机器伦理决策需考虑不确定性及长期结果，但不同道德理论（如功利主义、义务论、美德伦理学）可能冲突，需统一处理。

Method: 将问题形式化为多道德马尔可夫决策过程，提出基于多目标AO*的启发式算法，结合Hanssson的假设回顾程序进行伦理推理。

Result: 通过案例研究（是否偷胰岛素）验证了方法的有效性。

Conclusion: 该方法能有效处理道德冲突，适用于机器伦理决策。

Abstract: Machine Ethics decisions should consider the implications of uncertainty over
decisions. Decisions should be made over sequences of actions to reach
preferable outcomes long term. The evaluation of outcomes, however, may invoke
one or more moral theories, which might have conflicting judgements. Each
theory will require differing representations of the ethical situation. For
example, Utilitarianism measures numerical values, Deontology analyses duties,
and Virtue Ethics emphasises moral character. While balancing potentially
conflicting moral considerations, decisions may need to be made, for example,
to achieve morally neutral goals with minimal costs. In this paper, we
formalise the problem as a Multi-Moral Markov Decision Process and a
Multi-Moral Stochastic Shortest Path Problem. We develop a heuristic algorithm
based on Multi-Objective AO*, utilising Sven-Ove Hansson's Hypothetical
Retrospection procedure for ethical reasoning under uncertainty. Our approach
is validated by a case study from Machine Ethics literature: the problem of
whether to steal insulin for someone who needs it.

</details>


### [104] [TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven Evolution](https://arxiv.org/abs/2505.04480)
*Zhikai Zhao,Chuanbo Hua,Federico Berto,Kanghoon Lee,Zihan Ma,Jiachen Li,Jinkyoo Park*

Main category: cs.AI

TL;DR: TrajEvo利用大型语言模型（LLMs）和进化算法自动设计轨迹预测启发式方法，优于传统启发式和深度学习方法，尤其在泛化能力上表现突出。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法准确性不足，深度学习方法存在计算成本高、可解释性差和泛化能力有限的问题。

Method: TrajEvo结合LLMs和进化算法，通过跨代精英采样和统计反馈循环生成和优化启发式方法。

Result: 在ETH-UCY数据集上优于传统方法，在未见过的SDD数据集上显著优于启发式和深度学习方法。

Conclusion: TrajEvo为快速、可解释且泛化能力强的轨迹预测启发式方法设计提供了新思路，代码已开源。

Abstract: Trajectory prediction is a crucial task in modeling human behavior,
especially in fields as social robotics and autonomous vehicle navigation.
Traditional heuristics based on handcrafted rules often lack accuracy, while
recently proposed deep learning approaches suffer from computational cost, lack
of explainability, and generalization issues that limit their practical
adoption. In this paper, we introduce TrajEvo, a framework that leverages Large
Language Models (LLMs) to automatically design trajectory prediction
heuristics. TrajEvo employs an evolutionary algorithm to generate and refine
prediction heuristics from past trajectory data. We introduce a
Cross-Generation Elite Sampling to promote population diversity and a
Statistics Feedback Loop allowing the LLM to analyze alternative predictions.
Our evaluations show TrajEvo outperforms previous heuristic methods on the
ETH-UCY datasets, and remarkably outperforms both heuristics and deep learning
methods when generalizing to the unseen SDD dataset. TrajEvo represents a first
step toward automated design of fast, explainable, and generalizable trajectory
prediction heuristics. We make our source code publicly available to foster
future research at https://github.com/ai4co/trajevo.

</details>


### [105] [On some improvements to Unbounded Minimax](https://arxiv.org/abs/2505.04525)
*Quentin Cohen-Solal,Tristan Cazenave*

Main category: cs.AI

TL;DR: 本文首次实验评估了四种对Unbounded Best-First Minimax算法的未测试修改，包括使用置换表、改进反向传播策略、替换终端评估函数以及优化完成技术，结果显示这些修改能提升算法效率。


<details>
  <summary>Details</summary>
Motivation: 评估四种未测试的修改对Unbounded Best-First Minimax算法性能的影响，以探索如何优化该算法的效率。

Method: 通过实验比较四种修改：置换表的使用、反向传播策略的改进、终端评估函数的替换以及完成技术的优化。

Result: 置换表和完成技术显著提升性能；改进的反向传播策略在特定情况下略有提升；替换终端评估函数在成本高时有益，但在低成本时降低性能。

Conclusion: 针对性的修改可以有效提升Unbounded Best-First Minimax算法的效率，但需根据具体场景选择合适的优化策略。

Abstract: This paper presents the first experimental evaluation of four previously
untested modifications of Unbounded Best-First Minimax algorithm. This
algorithm explores the game tree by iteratively expanding the most promising
sequences of actions based on the current partial game tree. We first evaluate
the use of transposition tables, which convert the game tree into a directed
acyclic graph by merging duplicate states. Second, we compare the original
algorithm by Korf & Chickering with the variant proposed by Cohen-Solal, which
differs in its backpropagation strategy: instead of stopping when a stable
value is encountered, it updates values up to the root. This change slightly
improves performance when value ties or transposition tables are involved.
Third, we assess replacing the exact terminal evaluation function with the
learned heuristic function. While beneficial when exact evaluations are costly,
this modification reduces performance in inexpensive settings. Finally, we
examine the impact of the completion technique that prioritizes resolved
winning states and avoids resolved losing states. This technique also improves
performance. Overall, our findings highlight how targeted modifications can
enhance the efficiency of Unbounded Best-First Minimax.

</details>


### [106] [Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving](https://arxiv.org/abs/2505.04528)
*Qi Liu,Xinhao Zheng,Renqiu Xia,Xingzhi Qi,Qinxiang Cao,Junchi Yan*

Main category: cs.AI

TL;DR: 论文提出了FPS和D-FPS框架，用于形式化问题解决，并通过验证环境实现过程级验证。构建了三个基准测试，并提出了RPE方法用于答案正确性验证。


<details>
  <summary>Details</summary>
Motivation: 问题解决在科学与工程中至关重要，但缺乏通用且具体的定义。AI问题解决代理的发展增加了对过程级验证的需求。

Method: 提出FPS框架（确定性马尔可夫决策过程）和D-FPS框架（解耦求解与验证），利用FTP环境进行验证。构建三个基准测试，提出RPE方法验证答案。

Result: 在三个基准测试中，现有FTP模型的解决率最高为23.77%（FormalMath500）、27.47%（MiniF2F-Solving）和0.31%（PutnamBench-Solving）。

Conclusion: FPS和D-FPS框架为问题解决提供了形式化方法，并通过RPE实现了可验证的评估，但现有模型在复杂问题上的表现仍有提升空间。

Abstract: As a seemingly self-explanatory task, problem-solving has been a significant
component of science and engineering. However, a general yet concrete
formulation of problem-solving itself is missing. With the recent development
of AI-based problem-solving agents, the demand for process-level verifiability
is rapidly increasing yet underexplored. To fill these gaps, we present a
principled formulation of problem-solving as a deterministic Markov decision
process; a novel framework, FPS (Formal Problem-Solving), which utilizes
existing FTP (formal theorem proving) environments to perform process-verified
problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer
verification for better human-alignment. The expressiveness, soundness and
completeness of the frameworks are proven. We construct three benchmarks on
problem-solving: FormalMath500, a formalization of a subset of the MATH500
benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP
benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and
human-aligned evaluation, we propose RPE (Restricted Propositional
Equivalence), a symbolic approach to determine the correctness of answers by
formal verification. We evaluate four prevalent FTP models and two prompting
methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of
MiniF2F-Solving, and 0.31% of PutnamBench-Solving.

</details>


### [107] [Qualitative Analysis of $ω$-Regular Objectives on Robust MDPs](https://arxiv.org/abs/2505.04539)
*Ali Asadi,Krishnendu Chatterjee,Ehsan Kafshdar Goharshady,Mehrdad Karrabi,Ali Shafiee*

Main category: cs.AI

TL;DR: 本文研究了鲁棒马尔可夫决策过程（RMDPs）中的定性分析问题，针对可达性和奇偶性目标，提出了高效的算法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: RMDPs扩展了经典MDPs，考虑了转移概率的不确定性，但现有研究通常假设RMDPs具有特定结构（如单链或非周期性）。本文旨在解决无结构假设下的定性分析问题。

Method: 提出了基于不确定性集合的oracle访问的高效算法，用于解决可达性和奇偶性目标的定性问题。

Result: 实验结果表明，该方法在经典RMDP示例中有效，可扩展到数千个状态。

Conclusion: 本文为无结构假设的RMDPs提供了高效的定性分析算法，并通过实验验证了其实际应用价值。

Abstract: Robust Markov Decision Processes (RMDPs) generalize classical MDPs that
consider uncertainties in transition probabilities by defining a set of
possible transition functions. An objective is a set of runs (or infinite
trajectories) of the RMDP, and the value for an objective is the maximal
probability that the agent can guarantee against the adversarial environment.
We consider (a) reachability objectives, where given a target set of states,
the goal is to eventually arrive at one of them; and (b) parity objectives,
which are a canonical representation for $\omega$-regular objectives. The
qualitative analysis problem asks whether the objective can be ensured with
probability 1.
  In this work, we study the qualitative problem for reachability and parity
objectives on RMDPs without making any assumption over the structures of the
RMDPs, e.g., unichain or aperiodic. Our contributions are twofold. We first
present efficient algorithms with oracle access to uncertainty sets that solve
qualitative problems of reachability and parity objectives. We then report
experimental results demonstrating the effectiveness of our oracle-based
approach on classical RMDP examples from the literature scaling up to thousands
of states.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [108] [Out-of-Distribution Detection in Heterogeneous Graphs via Energy Propagation](https://arxiv.org/abs/2505.03774)
*Tao Yin,Chen Zhao,Xiaoyan Liu,Minglai Shao*

Main category: cs.LG

TL;DR: 提出了一种用于异构图（OODHG）的OOD检测新方法，通过能量传播机制和约束区分ID和OOD节点，实验证明其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现实图中的异构性和分布偏移问题使得OOD检测具有挑战性，现有研究多关注同构图，而异构图OOD检测研究不足。

Method: 学习节点表示，计算能量值判断OOD节点，利用元路径能量传播机制和能量约束增强ID与OOD节点的区分。

Result: 实验验证OODHG在OOD检测任务中优于基线模型，且在ID节点分类中表现准确。

Conclusion: OODHG方法简单有效，适用于异构图的OOD检测和ID节点分类。

Abstract: Graph neural networks (GNNs) are proven effective in extracting complex node
and structural information from graph data. While current GNNs perform well in
node classification tasks within in-distribution (ID) settings, real-world
scenarios often present distribution shifts, leading to the presence of
out-of-distribution (OOD) nodes. OOD detection in graphs is a crucial and
challenging task. Most existing research focuses on homogeneous graphs, but
real-world graphs are often heterogeneous, consisting of diverse node and edge
types. This heterogeneity adds complexity and enriches the informational
content. To the best of our knowledge, OOD detection in heterogeneous graphs
remains an underexplored area. In this context, we propose a novel methodology
for OOD detection in heterogeneous graphs (OODHG) that aims to achieve two main
objectives: 1) detecting OOD nodes and 2) classifying all ID nodes based on the
first task's results. Specifically, we learn representations for each node in
the heterogeneous graph, calculate energy values to determine whether nodes are
OOD, and then classify ID nodes. To leverage the structural information of
heterogeneous graphs, we introduce a meta-path-based energy propagation
mechanism and an energy constraint to enhance the distinction between ID and
OOD nodes. Extensive experimental findings substantiate the simplicity and
effectiveness of OODHG, demonstrating its superiority over baseline models in
OOD detection tasks and its accuracy in ID node classification.

</details>


### [109] [Hierarchical Multi-Label Generation with Probabilistic Level-Constraint](https://arxiv.org/abs/2505.03775)
*Linqing Chen,Weilei Wang,Wentao Wu,Hanmeng Zhong*

Main category: cs.LG

TL;DR: 将层次化多标签分类任务重新定义为层次化多标签生成（HMG），并提出了一种带有概率层次约束（PLC）的生成框架，以生成具有复杂层次关系的标签。该方法无需依赖聚类等预处理步骤，并能精确控制模型输出。实验表明，该方法在HMG任务中达到了新的SOTA性能，并且在输出控制方面优于先前研究。


<details>
  <summary>Details</summary>
Motivation: 层次化极端多标签分类由于标签间复杂的层次关系和大量标签而更具挑战性。先前研究依赖辅助步骤或生成方法但未能有效控制输出。

Method: 将任务重新定义为HMG，并采用带有PLC的生成框架，直接生成层次化标签，无需预处理步骤。

Result: 实验表明，该方法在HMG任务中达到新的SOTA性能，且在输出控制方面表现优异。

Conclusion: 提出的生成框架在层次化多标签生成任务中表现出色，尤其在输出控制方面优于先前方法。

Abstract: Hierarchical Extreme Multi-Label Classification poses greater difficulties
compared to traditional multi-label classification because of the intricate
hierarchical connections of labels within a domain-specific taxonomy and the
substantial number of labels. Some of the prior research endeavors centered on
classifying text through several ancillary stages such as the cluster algorithm
and multiphase classification. Others made attempts to leverage the assistance
of generative methods yet were unable to properly control the output of the
generative model. We redefine the task from hierarchical multi-Label
classification to Hierarchical Multi-Label Generation (HMG) and employ a
generative framework with Probabilistic Level Constraints (PLC) to generate
hierarchical labels within a specific taxonomy that have complex hierarchical
relationships. The approach we proposed in this paper enables the framework to
generate all relevant labels across levels for each document without relying on
preliminary operations like clustering. Meanwhile, it can control the model
output precisely in terms of count, length, and level aspects. Experiments
demonstrate that our approach not only achieves a new SOTA performance in the
HMG task, but also has a much better performance in constrained the output of
model than previous research work.

</details>


### [110] [PAPN: Proximity Attention Encoder and Pointer Network Decoder for Parcel Pickup Route Prediction](https://arxiv.org/abs/2505.03776)
*Hansi Denis,Siegfried Mercelis,Ngoc-Quang Luong*

Main category: cs.LG

TL;DR: 论文提出了一种结合邻近注意力机制和指针网络的编码器-解码器架构（PAPN），用于优化最后一公里配送和第一公里取件的路线预测，显著优于现有监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 最后一公里配送和第一公里取件的优化对物流效率和成本至关重要，需要准确的路线预测系统。

Method: 提出PAPN模型，结合邻近注意力机制和全局多头注意力，通过编码器-解码器架构预测路线。

Result: 在LaDE数据集上表现优异，超越现有监督学习方法，与最佳强化学习方法DRL4Route竞争。

Conclusion: PAPN模型通过混合全局和局部注意力，显著提升了路线预测的准确性。

Abstract: Optimization of the last-mile delivery and first-mile pickup of parcels is an
integral part of the broader logistics optimization pipeline as it entails both
cost and resource efficiency as well as a heightened service quality. Such
optimization requires accurate route and time prediction systems to adapt to
different scenarios in advance. This work tackles the first building block,
namely route prediction. This is done by introducing a novel Proximity
Attention mechanism in an encoder-decoder architecture utilizing a Pointer
Network in the decoding process (Proximity Attention Encoder and Pointer
Network decoder: PAPN) to leverage the underlying connections between the
different visitable pickup positions at each timestep. To this local attention
process is coupled global context computing via a multi-head attention
transformer encoder. The obtained global context is then mixed to an aggregated
version of the local embedding thus achieving a mix of global and local
attention for complete modeling of the problems. Proximity attention is also
used in the decoding process to skew predictions towards the locations with the
highest attention scores and thus using inter-connectivity of locations as a
base for next-location prediction. This method is trained, validated and tested
on a large industry-level dataset of real-world, large-scale last-mile delivery
and first-mile pickup named LaDE[1]. This approach shows noticeable promise,
outperforming all state-of-the-art supervised systems in terms of most metrics
used for benchmarking methods on this dataset while still being competitive
with the best-performing reinforcement learning method named DRL4Route[2].

</details>


### [111] [MolMole: Molecule Mining from Scientific Literature](https://arxiv.org/abs/2505.03777)
*LG AI Research,Sehyun Chun,Jiye Kim,Ahra Jo,Yeonsik Jo,Seungyul Oh,Seungjun Lee,Kwangrok Ryoo,Jongmin Lee,Seunghwan Kim,Byung Jun Kang,Soonyoung Lee,Jun Ha Park,Chanwoo Moon,Jiwon Ham,Haein Lee,Heejae Han,Jaeseung Byun,Soojong Do,Minju Ha,Dongyun Kim,Kyunghoon Bae,Woohyung Lim,Edward Hwayoung Lee,Yongmin Park,Jeongsang Yu,Gerrard Jeongwon Jo,Yeonjung Hong,Kyungjae Yoo,Sehui Han,Jaewan Lee,Changyoung Park,Kijeong Jeon,Sihyuk Yi*

Main category: cs.LG

TL;DR: MolMole是一个基于视觉的深度学习框架，用于从科学文档中提取分子结构和反应数据，统一了分子检测、反应图解析和光学化学结构识别（OCSR）。


<details>
  <summary>Details</summary>
Motivation: 科学文档中的化学数据格式多样且布局复杂，缺乏标准化的提取工具和评估指标。

Method: MolMole通过深度学习框架整合了分子检测、反应图解析和OCSR，并提出了新的测试集和评估指标。

Result: MolMole在基准测试和公共数据集上优于现有工具包。

Conclusion: MolMole框架和测试集将公开，工具包将通过LG AI Research网站提供交互式演示。

Abstract: The extraction of molecular structures and reaction data from scientific
documents is challenging due to their varied, unstructured chemical formats and
complex document layouts. To address this, we introduce MolMole, a vision-based
deep learning framework that unifies molecule detection, reaction diagram
parsing, and optical chemical structure recognition (OCSR) into a single
pipeline for automating the extraction of chemical data directly from
page-level documents. Recognizing the lack of a standard page-level benchmark
and evaluation metric, we also present a testset of 550 pages annotated with
molecule bounding boxes, reaction labels, and MOLfiles, along with a novel
evaluation metric. Experimental results demonstrate that MolMole outperforms
existing toolkits on both our benchmark and public datasets. The benchmark
testset will be publicly available, and the MolMole toolkit will be accessible
soon through an interactive demo on the LG AI Research website. For commercial
inquiries, please contact us at
\href{mailto:contact_ddu@lgresearch.ai}{contact\_ddu@lgresearch.ai}.

</details>


### [112] [Dragonfly: a modular deep reinforcement learning library](https://arxiv.org/abs/2505.03778)
*Jonathan Viquerat,Paul Garnier,Amirhossein Bateni,Elie Hachem*

Main category: cs.LG

TL;DR: Dragonfly是一个模块化的深度强化学习库，旨在简化实验和开发。


<details>
  <summary>Details</summary>
Motivation: 为了减少代码维护并支持模块化实验，设计了一个基于JSON序列化的库。

Method: 使用JSON序列化实现模块交换和参数扫描，特别针对CPU密集型环境优化。

Result: 在标准代理和常见基准测试中表现优于文献中的方法。

Conclusion: Dragonfly是一个高效且灵活的深度强化学习工具。

Abstract: Dragonfly is a deep reinforcement learning library focused on modularity, in
order to ease experimentation and developments. It relies on a json
serialization that allows to swap building blocks and perform parameter sweep,
while minimizing code maintenance. Some of its features are specifically
designed for CPU-intensive environments, such as numerical simulations. Its
performance on standard agents using common benchmarks compares favorably with
the literature.

</details>


### [113] [Neural Co-Optimization of Structural Topology, Manufacturable Layers, and Path Orientations for Fiber-Reinforced Composites](https://arxiv.org/abs/2505.03779)
*Tao Liu,Tianyu Zhang,Yongxue Chen,Weiming Wang,Yu Jiang,Yuming Huang,Charlie C. L. Wang*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络的框架，同时优化纤维增强热塑性复合材料的结构拓扑、弯曲层和路径方向，以提高各向异性强度并确保可制造性。


<details>
  <summary>Details</summary>
Motivation: 纤维增强复合材料在轻量化和高强度应用中具有潜力，但传统优化方法难以同时满足设计和制造要求。

Method: 使用三个隐式神经场表示几何形状、层序列和纤维方向，将设计和制造目标（如各向异性强度、结构体积、机器运动控制等）整合为可微优化过程。

Result: 物理实验表明，该方法生成的复合材料比顺序优化方法的失效载荷提高了33.1%。

Conclusion: 该框架为复合材料的设计和制造提供了一种高效且可扩展的解决方案。

Abstract: We propose a neural network-based computational framework for the
simultaneous optimization of structural topology, curved layers, and path
orientations to achieve strong anisotropic strength in fiber-reinforced
thermoplastic composites while ensuring manufacturability. Our framework
employs three implicit neural fields to represent geometric shape, layer
sequence, and fiber orientation. This enables the direct formulation of both
design and manufacturability objectives - such as anisotropic strength,
structural volume, machine motion control, layer curvature, and layer thickness
- into an integrated and differentiable optimization process. By incorporating
these objectives as loss functions, the framework ensures that the resultant
composites exhibit optimized mechanical strength while remaining its
manufacturability for filament-based multi-axis 3D printing across diverse
hardware platforms. Physical experiments demonstrate that the composites
generated by our co-optimization method can achieve an improvement of up to
33.1% in failure loads compared to composites with sequentially optimized
structures and manufacturing sequences.

</details>


### [114] [ALFRED: Ask a Large-language model For Reliable ECG Diagnosis](https://arxiv.org/abs/2505.03781)
*Jin Yu,JaeHo Park,TaeJun Park,Gyurin Kim,JiHyun Lee,Min Sung Lee,Joon-myoung Kwon,Jeong Min Son,Yong-Yeon Jo*

Main category: cs.LG

TL;DR: 提出了一种基于RAG的零样本ECG诊断框架，结合专家知识提高准确性和可解释性，在PTB-XL数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域（如ECG分析）中，仅依赖RAG生成可靠、基于证据的结果存在挑战，需结合专家知识。

Method: 开发了基于RAG的零样本ECG诊断框架，融入专家整理的知识。

Result: 在PTB-XL数据集上验证了框架的有效性，展示了结构化领域知识在自动化ECG分析中的价值。

Conclusion: 该框架支持全面的ECG分析，可满足多样化诊断需求，并具有超越测试数据集的潜在应用。

Abstract: Leveraging Large Language Models (LLMs) with Retrieval-Augmented Generation
(RAG) for analyzing medical data, particularly Electrocardiogram (ECG), offers
high accuracy and convenience. However, generating reliable, evidence-based
results in specialized fields like healthcare remains a challenge, as RAG alone
may not suffice. We propose a Zero-shot ECG diagnosis framework based on RAG
for ECG analysis that incorporates expert-curated knowledge to enhance
diagnostic accuracy and explainability. Evaluation on the PTB-XL dataset
demonstrates the framework's effectiveness, highlighting the value of
structured domain expertise in automated ECG interpretation. Our framework is
designed to support comprehensive ECG analysis, addressing diverse diagnostic
needs with potential applications beyond the tested dataset.

</details>


### [115] [A general physics-constrained method for the modelling of equation's closure terms with sparse data](https://arxiv.org/abs/2505.03783)
*Tian Chen,Shengping Liu,Li Liu,Heng Yong*

Main category: cs.LG

TL;DR: 提出了一种新颖的Series-Parallel Multi-Network Architecture方法，结合PINNs和专用子网络，用于稀疏数据下的闭包建模，并集成到PDE求解器中。


<details>
  <summary>Details</summary>
Motivation: 在数据稀疏或不完整的情况下，开发广泛适用的闭包模型具有挑战性。

Method: 采用Series-Parallel Multi-Network Architecture，结合PINNs和专用子网络，独立建模未知闭包项。

Result: 该方法增强了模型的泛化能力，并成功集成到PDE求解器中，适用于复杂工程预测模拟。

Conclusion: 提出的方法在稀疏数据下有效构建闭包模型，为工程应用提供了稳健的解决方案。

Abstract: Accurate modeling of closure terms is a critical challenge in engineering and
scientific research, particularly when data is sparse (scarse or incomplete),
making widely applicable models difficult to develop. This study proposes a
novel approach for constructing closure models in such challenging scenarios.
We introduce a Series-Parallel Multi-Network Architecture that integrates
Physics-Informed Neural Networks (PINNs) to incorporate physical constraints
and heterogeneous data from multiple initial and boundary conditions, while
employing dedicated subnetworks to independently model unknown closure terms,
enhancing generalizability across diverse problems. These closure models are
integrated into an accurate Partial Differential Equation (PDE) solver,
enabling robust solutions to complex predictive simulations in engineering
applications.

</details>


### [116] [Insulin Resistance Prediction From Wearables and Routine Blood Biomarkers](https://arxiv.org/abs/2505.03784)
*Ahmed A. Metwally,A. Ali Heydari,Daniel McDuff,Alexandru Solot,Zeinab Esmaeilpour,Anthony Z Faranesh,Menglian Zhou,David B. Savage,Conor Heneghan,Shwetak Patel,Cathy Speed,Javier L. Prieto*

Main category: cs.LG

TL;DR: 该研究利用可穿戴设备和血液生物标志物数据，开发深度学习模型预测胰岛素抵抗，优于单独数据源，并在高风险人群中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有胰岛素抵抗检测方法昂贵且不便，阻碍早期干预。

Method: 远程招募大规模数据集，结合可穿戴设备和血液生物标志物，开发深度神经网络模型。

Result: 模型预测胰岛素抵抗表现优异（R2=0.5, auROC=0.80），在肥胖和久坐人群中敏感性达93%。

Conclusion: 该模型有望实现早期糖尿病风险检测，推动预防策略实施。

Abstract: Insulin resistance, a precursor to type 2 diabetes, is characterized by
impaired insulin action in tissues. Current methods for measuring insulin
resistance, while effective, are expensive, inaccessible, not widely available
and hinder opportunities for early intervention. In this study, we remotely
recruited the largest dataset to date across the US to study insulin resistance
(N=1,165 participants, with median BMI=28 kg/m2, age=45 years, HbA1c=5.4%),
incorporating wearable device time series data and blood biomarkers, including
the ground-truth measure of insulin resistance, homeostatic model assessment
for insulin resistance (HOMA-IR). We developed deep neural network models to
predict insulin resistance based on readily available digital and blood
biomarkers. Our results show that our models can predict insulin resistance by
combining both wearable data and readily available blood biomarkers better than
either of the two data sources separately (R2=0.5, auROC=0.80, Sensitivity=76%,
and specificity 84%). The model showed 93% sensitivity and 95% adjusted
specificity in obese and sedentary participants, a subpopulation most
vulnerable to developing type 2 diabetes and who could benefit most from early
intervention. Rigorous evaluation of model performance, including
interpretability, and robustness, facilitates generalizability across larger
cohorts, which is demonstrated by reproducing the prediction performance on an
independent validation cohort (N=72 participants). Additionally, we
demonstrated how the predicted insulin resistance can be integrated into a
large language model agent to help understand and contextualize HOMA-IR values,
facilitating interpretation and safe personalized recommendations. This work
offers the potential for early detection of people at risk of type 2 diabetes
and thereby facilitate earlier implementation of preventative strategies.

</details>


### [117] [mAIstro: an open-source multi-agentic system for automated end-to-end development of radiomics and deep learning models for medical imaging](https://arxiv.org/abs/2505.03785)
*Eleftherios Tzanis,Michail E. Klontzas*

Main category: cs.LG

TL;DR: mAIstro是一个基于LLM的开源多代理框架，用于医疗AI模型的端到端开发和部署，无需用户编码，支持多种任务和数据集。


<details>
  <summary>Details</summary>
Motivation: 自动化医疗AI中的复杂工作流程，提供无需编码的解决方案。

Method: 采用模块化架构，通过自然语言界面协调数据分析、特征提取、图像分割、分类和回归等任务。

Result: 在16个开源数据集上成功执行所有任务，生成可解释输出和验证模型。

Conclusion: mAIstro是首个统一医疗AI数据分析和模型开发的代理框架，具有可扩展性和可重复性。

Abstract: Agentic systems built on large language models (LLMs) offer promising
capabilities for automating complex workflows in healthcare AI. We introduce
mAIstro, an open-source, autonomous multi-agentic framework for end-to-end
development and deployment of medical AI models. The system orchestrates
exploratory data analysis, radiomic feature extraction, image segmentation,
classification, and regression through a natural language interface, requiring
no coding from the user. Built on a modular architecture, mAIstro supports both
open- and closed-source LLMs, and was evaluated using a large and diverse set
of prompts across 16 open-source datasets, covering a wide range of imaging
modalities, anatomical regions, and data types. The agents successfully
executed all tasks, producing interpretable outputs and validated models. This
work presents the first agentic framework capable of unifying data analysis, AI
model development, and inference across varied healthcare applications,
offering a reproducible and extensible foundation for clinical and research AI
integration. The code is available at: https://github.com/eltzanis/mAIstro

</details>


### [118] [When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator](https://arxiv.org/abs/2505.03786)
*Md Fahim Anjum*

Main category: cs.LG

TL;DR: 研究比较了推理型与非推理型大语言模型在文本到SQL任务中的表现，发现推理模型（如DeepSeek-R1）在判别任务上表现更优，但在生成任务上可能不如小型非推理模型。


<details>
  <summary>Details</summary>
Motivation: 探索推理型大语言模型在规划框架中作为判别器的潜力，并比较其与传统非推理模型的性能差异。

Method: 使用蒸馏后的1.5B参数推理模型（DeepSeek-R1）与多个非推理型LLM进行对比，提出了一种从推理链输出中提取软分数的新方法。

Result: DeepSeek-R1在判别任务上表现优于CodeLlama-7B和CodeLlama-13B，但在生成任务上表现较差。推理模型的逻辑能力存在上限。

Conclusion: 推理模型更适合作为判别器而非生成器，为LLM规划框架中的角色分配提供了新见解。

Abstract: Large Language Models (LLM) with reasoning capabilities offer a promising
path for improving candidate evaluation in planning frameworks, but their
relative performance against traditional non-reasoning models remains largely
underexplored. In this study, we benchmark a distilled 1.5B parameter reasoning
model (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within
a generator-discriminator LLM planning framework for the text-to-SQL task. For
this, we introduce a novel method for extracting soft scores from the
chain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking
of candidates. Our central hypothesis is that reasoning models are more
effective discriminators than non-reasoning LLMs. Our results show that
distilled DeepSeek-R1-1.5B achieves up to $87\%$ higher F1 and $3.7\%$ better
discrimination accuracy than CodeLlama-7B, as well as $3.7\%$ higher execution
accuracy than CodeLlama-13B, despite having significantly fewer parameters.
Furthermore, we find that there is a limit to the logical capabilities of
reasoning models, and only providing more context or allowing more compute
budget for reasoning is not enough to improve their discrimination performance.
Finally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find
generation more challenging than discrimination and may underperform as
generators compared to smaller non-reasoning LLMs. Our work highlights the
potential of reasoning models as discriminators in agentic frameworks, far
outweighing their capabilities as generators, offering insights into their
optimal role within LLM planning infrastructures.

</details>


### [119] [ArrhythmiaVision: Resource-Conscious Deep Learning Models with Visual Explanations for ECG Arrhythmia Classification](https://arxiv.org/abs/2505.03787)
*Zuraiz Baig,Sidra Nasir,Rizwan Ahmed Khan,Muhammad Zeeshan Ul Haque*

Main category: cs.LG

TL;DR: 论文提出了两种轻量级1D卷积神经网络（ArrhythmiNet V1和V2），用于实时心律失常分类，兼顾高准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 心律失常是危及生命的心脏事件的主要原因，现有ECG分析方法依赖人工且易出错，深度学习模型缺乏可解释性和计算效率。

Method: 基于MobileNet的深度可分离卷积设计，开发了两种轻量级模型，并集成SHAP和Grad-CAM以提高可解释性。

Result: 模型在MIT-BIH数据集上达到0.99（V1）和0.98（V2）的分类准确率，内存占用分别为302.18 KB和157.76 KB。

Conclusion: 研究证明了在可穿戴ECG监测系统中结合可解释性、预测准确性和计算效率的可行性。

Abstract: Cardiac arrhythmias are a leading cause of life-threatening cardiac events,
highlighting the urgent need for accurate and timely detection.
Electrocardiography (ECG) remains the clinical gold standard for arrhythmia
diagnosis; however, manual interpretation is time-consuming, dependent on
clinical expertise, and prone to human error. Although deep learning has
advanced automated ECG analysis, many existing models abstract away the
signal's intrinsic temporal and morphological features, lack interpretability,
and are computationally intensive-hindering their deployment on
resource-constrained platforms. In this work, we propose two novel lightweight
1D convolutional neural networks, ArrhythmiNet V1 and V2, optimized for
efficient, real-time arrhythmia classification on edge devices. Inspired by
MobileNet's depthwise separable convolutional design, these models maintain
memory footprints of just 302.18 KB and 157.76 KB, respectively, while
achieving classification accuracies of 0.99 (V1) and 0.98 (V2) on the MIT-BIH
Arrhythmia Dataset across five classes: Normal Sinus Rhythm, Left Bundle Branch
Block, Right Bundle Branch Block, Atrial Premature Contraction, and Premature
Ventricular Contraction. In order to ensure clinical transparency and
relevance, we integrate Shapley Additive Explanations and Gradient-weighted
Class Activation Mapping, enabling both local and global interpretability.
These techniques highlight physiologically meaningful patterns such as the QRS
complex and T-wave that contribute to the model's predictions. We also discuss
performance-efficiency trade-offs and address current limitations related to
dataset diversity and generalizability. Overall, our findings demonstrate the
feasibility of combining interpretability, predictive accuracy, and
computational efficiency in practical, wearable, and embedded ECG monitoring
systems.

</details>


### [120] [A new architecture of high-order deep neural networks that learn martingales](https://arxiv.org/abs/2505.03789)
*Syoiti Ninomiya,Yuming Ma*

Main category: cs.LG

TL;DR: 提出了一种基于高阶弱逼近算法的深度学习神经网络架构，用于高效学习鞅，并应用于金融衍生品定价问题。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在金融衍生品定价中高效学习鞅的问题。

Method: 采用基于显式Runge-Kutta类型的高阶弱逼近算法，通过迭代组合和线性组合目标SDE的向量场实现逼近。

Result: 新架构能够有效学习鞅，并在金融衍生品定价中表现出良好性能。

Conclusion: 该架构为深度学习在金融数学中的应用提供了新思路。

Abstract: A new deep-learning neural network architecture based on high-order weak
approximation algorithms for stochastic differential equations (SDEs) is
proposed. The architecture enables the efficient learning of martingales by
deep learning models. The behaviour of deep neural networks based on this
architecture, when applied to the problem of pricing financial derivatives, is
also examined. The core of this new architecture lies in the high-order weak
approximation algorithms of the explicit Runge--Kutta type, wherein the
approximation is realised solely through iterative compositions and linear
combinations of vector fields of the target SDEs.

</details>


### [121] [A Time-Series Data Augmentation Model through Diffusion and Transformer Integration](https://arxiv.org/abs/2505.03790)
*Yuren Zhang,Zhongnan Pu,Lei Jing*

Main category: cs.LG

TL;DR: 提出了一种结合扩散模型和Transformer的方法，用于生成高质量的时间序列增强数据，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列数据增强研究较少，而深度学习需要大量数据，本文旨在填补这一空白。

Method: 使用扩散去噪模型生成初始时间步数据，再用Transformer预测后续动作，结合加权损失函数实现收敛。

Result: 该方法能生成高质量增强数据，相比传统方法或无增强，显著提升模型性能。

Conclusion: 提出的方法简单有效，适用于时间序列数据增强，为深度学习任务提供更多数据支持。

Abstract: With the development of Artificial Intelligence, numerous real-world tasks
have been accomplished using technology integrated with deep learning. To
achieve optimal performance, deep neural networks typically require large
volumes of data for training. Although advances in data augmentation have
facilitated the acquisition of vast datasets, most of this data is concentrated
in domains like images and speech. However, there has been relatively less
focus on augmenting time-series data. To address this gap and generate a
substantial amount of time-series data, we propose a simple and effective
method that combines the Diffusion and Transformer models. By utilizing an
adjusted diffusion denoising model to generate a large volume of initial
time-step action data, followed by employing a Transformer model to predict
subsequent actions, and incorporating a weighted loss function to achieve
convergence, the method demonstrates its effectiveness. Using the performance
improvement of the model after applying augmented data as a benchmark, and
comparing the results with those obtained without data augmentation or using
traditional data augmentation methods, this approach shows its capability to
produce high-quality augmented data.

</details>


### [122] [Practical Boolean Backpropagation](https://arxiv.org/abs/2505.03791)
*Simon Golbert*

Main category: cs.LG

TL;DR: 提出了一种纯布尔反向传播方法，直接在布尔代数中操作，无需数值计算。


<details>
  <summary>Details</summary>
Motivation: 布尔神经网络提供硬件高效的替代方案，但纯布尔训练研究不足。

Method: 基于特定门设计的纯布尔反向传播方法，完全在布尔代数中操作。

Result: 初步实验验证了该方法的可行性。

Conclusion: 该方法为布尔神经网络的训练提供了实用解决方案。

Abstract: Boolean neural networks offer hardware-efficient alternatives to real-valued
models. While quantization is common, purely Boolean training remains
underexplored. We present a practical method for purely Boolean backpropagation
for networks based on a single specific gate we chose, operating directly in
Boolean algebra involving no numerics. Initial experiments confirm its
feasibility.

</details>


### [123] [Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning](https://arxiv.org/abs/2505.03792)
*Lang Feng,Weihao Tan,Zhiyi Lyu,Longtao Zheng,Haiyang Xu,Ming Yan,Fei Huang,Bo An*

Main category: cs.LG

TL;DR: 论文提出了一种名为CoSo的在线微调方法，通过反事实推理动态评估文本动作中关键令牌的影响，优化了视觉语言模型（VLM）代理的探索效率。


<details>
  <summary>Details</summary>
Motivation: 在线微调视觉语言模型代理在动态环境中具有潜力，但其开放的文本动作空间和非端到端的动作生成方式导致探索效率低下。

Method: CoSo利用反事实推理动态评估令牌对动作的因果影响，优先探索关键令牌，减少冗余令牌的影响。

Result: 理论分析和实验验证表明，CoSo在多种任务（如Android设备控制、卡牌游戏和具身AI）中显著提升了探索效率和性能。

Conclusion: CoSo为VLM代理的在线微调提供了一种高效且理论可靠的方法，具有广泛的应用潜力。

Abstract: Online fine-tuning vision-language model (VLM) agents with reinforcement
learning (RL) has shown promise for equipping agents with multi-step,
goal-oriented capabilities in dynamic environments. However, their open-ended
textual action space and non-end-to-end nature of action generation present
significant challenges to effective online exploration in RL, e.g., explosion
of the exploration space. We propose a novel online fine-tuning method,
Counterfactual Soft Reinforcement Learning (CoSo), better suited to the textual
output space of VLM agents. Compared to prior methods that assign uniform
uncertainty to all tokens, CoSo leverages counterfactual reasoning to
dynamically assess the causal influence of individual tokens on post-processed
actions. By prioritizing the exploration of action-critical tokens while
reducing the impact of semantically redundant or low-impact tokens, CoSo
enables a more targeted and efficient online rollout process. We provide
theoretical analysis proving CoSo's convergence and policy improvement
guarantees, and extensive empirical evaluations supporting CoSo's
effectiveness. Our results across a diverse set of agent tasks, including
Android device control, card gaming, and embodied AI, highlight its remarkable
ability to enhance exploration efficiency and deliver consistent performance
gains. The code is available at https://github.com/langfengQ/CoSo.

</details>


### [124] [LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection](https://arxiv.org/abs/2505.03793)
*Xinyue Zeng,Haohui Wang,Junhong Lin,Jun Wu,Tyler Cody,Dawei Zhou*

Main category: cs.LG

TL;DR: 提出了一种新的理论框架LENSLLM，用于评估大型语言模型（LLM）在微调过程中的动态行为，从而提高模型选择的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 由于计算资源的限制，无法对所有候选LLM进行微调，因此需要一种高效的方法来评估和选择模型。

Method: 基于Hessian的PAC-Bayes泛化边界和NTK的修正缩放模型LENSLLM。

Result: 在3个大规模基准测试中，LENSLLM达到91.1%的准确率，并减少88.5%的计算成本。

Conclusion: LENSLLM在模型选择中表现优异，且开源了模型和结果。

Abstract: The proliferation of open-sourced Large Language Models (LLMs) and diverse
downstream tasks necessitates efficient model selection, given the
impracticality of fine-tuning all candidates due to computational constraints.
Despite the recent advances in LLM selection, a fundamental research question
largely remains nascent: how can we model the dynamic behaviors of LLMs during
fine-tuning, thereby enhancing our understanding of their generalization
performance across diverse downstream tasks? In this work, we propose a novel
theoretical framework that provides a proper lens to assess the generalization
capabilities of LLMs, thereby enabling accurate and efficient LLM selection for
downstream applications. In particular, we first derive a Hessian-based
PAC-Bayes generalization bound that unveils fine-tuning dynamics of LLMs and
then introduce LENSLLM, a Neural Tangent Kernel(NTK)-based Rectified Scaling
Model that enables accurate performance predictions across diverse tasks while
maintaining computational efficiency. Extensive empirical results on 3
large-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy
and reduces up to 88.5% computational cost in LLM selection, outperforming 5
state-of-the-art methods. We open-source our proposed LENSLLM model and
corresponding results at the Github link:
https://github.com/Susan571/LENSLLM.git.

</details>


### [125] [A Double Inertial Forward-Backward Splitting Algorithm With Applications to Regression and Classification Problems](https://arxiv.org/abs/2505.03794)
*İrfan Işik,Ibrahim Karahan,Okan Erkaymaz*

Main category: cs.LG

TL;DR: 本文提出了一种改进的前后分裂算法，包含两个惯性参数，用于在实希尔伯特空间中寻找使两个算子之和为零的点。


<details>
  <summary>Details</summary>
Motivation: 解决在实希尔伯特空间中，如何高效找到使特定算子之和为零的点的问题。

Method: 提出改进的前后分裂算法，引入两个惯性参数，并在标准假设下验证其弱收敛性。

Result: 实验结果表明，该算法在回归和数据分类问题中表现优于现有算法。

Conclusion: 改进的算法在性能上优于现有文献中的相关算法，具有实际应用潜力。

Abstract: This paper presents an improved forward-backward splitting algorithm with two
inertial parameters. It aims to find a point in the real Hilbert space at which
the sum of a co-coercive operator and a maximal monotone operator vanishes.
Under standard assumptions, our proposed algorithm demonstrates weak
convergence. We present numerous experimental results to demonstrate the
behavior of the developed algorithm by comparing it with existing algorithms in
the literature for regression and data classification problems. Furthermore,
these implementations suggest our proposed algorithm yields superior outcomes
when benchmarked against other relevant algorithms in existing literature.

</details>


### [126] [Utilising Gradient-Based Proposals Within Sequential Monte Carlo Samplers for Training of Partial Bayesian Neural Networks](https://arxiv.org/abs/2505.03797)
*Andrew Millard,Joshua Murphy,Simon Maskell,Zheng Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于SMC的训练方法用于pBNNs，通过引导提议和梯度马尔可夫核提升高维问题的扩展性，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: pBNNs在性能上与全贝叶斯神经网络竞争，但仅部分参数为随机性。SMC采样器提供了非参数概率估计，优于参数方法。

Method: 引入基于SMC的新训练方法，结合引导提议和梯度马尔可夫核，提升高维问题的扩展性。

Result: 新方法在预测性能和最优损失上优于现有技术，且pBNNs在大批量下扩展性好，显著减少训练时间。

Conclusion: 新方法在性能和效率上均有显著提升，适用于高维问题和大批量训练。

Abstract: Partial Bayesian neural networks (pBNNs) have been shown to perform
competitively with fully Bayesian neural networks while only having a subset of
the parameters be stochastic. Using sequential Monte Carlo (SMC) samplers as
the inference method for pBNNs gives a non-parametric probabilistic estimation
of the stochastic parameters, and has shown improved performance over
parametric methods. In this paper we introduce a new SMC-based training method
for pBNNs by utilising a guided proposal and incorporating gradient-based
Markov kernels, which gives us better scalability on high dimensional problems.
We show that our new method outperforms the state-of-the-art in terms of
predictive performance and optimal loss. We also show that pBNNs scale well
with larger batch sizes, resulting in significantly reduced training times and
often better performance.

</details>


### [127] [Position: Foundation Models Need Digital Twin Representations](https://arxiv.org/abs/2505.03798)
*Yiqing Shen,Hao Ding,Lalithkumar Seenivasan,Tianmin Shu,Mathias Unberath*

Main category: cs.LG

TL;DR: 当前基础模型（FMs）依赖离散的token表示，限制了其对真实世界知识的理解。本文提出数字孪生（DT）表示作为替代，以解决语义连贯性、时空动态和因果推理的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前FMs通过统计相关性学习知识，缺乏显式领域知识，导致在多模态语义连贯性、时空动态和因果推理方面表现不佳。

Method: 提出使用数字孪生（DT）表示，作为物理过程的虚拟复制品，替代传统的token表示。

Result: DT表示能提供物理基础的表示，显式编码领域知识并保持真实世界过程的连续性。

Conclusion: 数字孪生表示有望解决当前FMs的局限性，推动更强大的基础模型发展。

Abstract: Current foundation models (FMs) rely on token representations that directly
fragment continuous real-world multimodal data into discrete tokens. They limit
FMs to learning real-world knowledge and relationships purely through
statistical correlation rather than leveraging explicit domain knowledge.
Consequently, current FMs struggle with maintaining semantic coherence across
modalities, capturing fine-grained spatial-temporal dynamics, and performing
causal reasoning. These limitations cannot be overcome by simply scaling up
model size or expanding datasets. This position paper argues that the machine
learning community should consider digital twin (DT) representations, which are
outcome-driven digital representations that serve as building blocks for
creating virtual replicas of physical processes, as an alternative to the token
representation for building FMs. Finally, we discuss how DT representations can
address these challenges by providing physically grounded representations that
explicitly encode domain knowledge and preserve the continuous nature of
real-world processes.

</details>


### [128] [Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling](https://arxiv.org/abs/2505.03799)
*Hyun Lee,Chris Yi,Maminur Islam,B. D. S. Aritra*

Main category: cs.LG

TL;DR: 提出SDM-InstructGLM框架，通过指令调优增强LLMs处理图结构的能力，无需依赖GNNs。


<details>
  <summary>Details</summary>
Motivation: LLMs在图相关任务中应用受限，主要由于可扩展性不足和缺乏处理图结构的专用机制。

Method: 引入基于相似度和度的偏置随机游走机制，选择性编码图信息，优化LLM中的表示。

Result: 显著提升图任务（如节点分类和链接预测）的性能，并验证LLM-only图处理的可行性。

Conclusion: 为不依赖GNN的图学习方法开辟新途径，推动LLMs作为独立图推理模型的发展。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in various
natural language processing tasks; however, their application to graph-related
problems remains limited, primarily due to scalability constraints and the
absence of dedicated mechanisms for processing graph structures. Existing
approaches predominantly integrate LLMs with Graph Neural Networks (GNNs),
using GNNs as feature encoders or auxiliary components. However, directly
encoding graph structures within LLMs has been underexplored, particularly in
the context of large-scale graphs where token limitations hinder effective
representation. To address these challenges, we propose SDM-InstructGLM, a
novel instruction-tuned Graph Language Model (InstructGLM) framework that
enhances scalability and efficiency without relying on GNNs. Our method
introduces a similarity-degree-based biased random walk mechanism, which
selectively samples and encodes graph information based on node-feature
similarity and degree centrality, ensuring an adaptive and structured
representation within the LLM. This approach significantly improves token
efficiency, mitigates information loss due to random sampling, and enhances
performance on graph-based tasks such as node classification and link
prediction. Furthermore, our results demonstrate the feasibility of LLM-only
graph processing, enabling scalable and interpretable Graph Language Models
(GLMs) optimized through instruction-based fine-tuning. This work paves the way
for GNN-free approaches to graph learning, leveraging LLMs as standalone graph
reasoning models. Our source code is available on GitHub.

</details>


### [129] [Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free](https://arxiv.org/abs/2505.03810)
*Euntae Choi,Sumin Song,Woosang Lim,Sungjoo Yoo*

Main category: cs.LG

TL;DR: 提出了一种无需训练的改进旋转矩阵方法，显著提升低比特量化性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有旋转方法在极低比特（如2-bit）量化中的性能不足问题。

Method: 利用Walsh-Hadamard变换和分组序列排列旋转（GSR）构建改进的旋转矩阵。

Result: 在推理任务和WikiText-2的PPL得分上表现优异，性能接近基于优化的方法。

Conclusion: 该方法无需训练，显著提升低比特量化性能，并可与其他学习技术结合使用。

Abstract: Large Language Models (LLMs) face deployment challenges due to high
computational costs, and while Post-Training Quantization (PTQ) offers a
solution, existing rotation-based methods struggle at very low bit-widths like
2-bit. We introduce a novel, training-free approach to construct an improved
rotation matrix, addressing the limitations of current methods. The key
contributions include leveraging the Walsh-Hadamard transform with sequency
ordering, which clusters similar frequency components to reduce quantization
error compared to standard Hadamard matrices, significantly improving
performance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR)
using block-diagonal matrices with smaller Walsh blocks, effectively isolating
outlier impacts and achieving performance comparable to optimization-based
methods without requiring any training. Our method demonstrates robust
performance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our
method also enhances results even when applied over existing learned rotation
techniques.

</details>


### [130] [Large Language Model Compression with Global Rank and Sparsity Optimization](https://arxiv.org/abs/2505.03801)
*Changhai Zhou,Qian Qiao,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: 提出一种两阶段LLM压缩方法，通过全局秩和稀疏优化解决低秩与稀疏矩阵交互及权重分配问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低秩与稀疏矩阵交互及不同层权重分配上表现不佳，影响压缩性能。

Method: 第一阶段用鲁棒主成分分析分解权重矩阵为低秩和稀疏成分；第二阶段用概率全局优化技术联合识别结构。

Result: 实验表明，该方法显著优于现有稀疏化和复合近似技术。

Conclusion: 该方法能自动检测冗余并管理稀疏与低秩交互，性能优越。

Abstract: Low-rank and sparse composite approximation is a natural idea to compress
Large Language Models (LLMs). However, such an idea faces two primary
challenges that adversely affect the performance of existing methods. The first
challenge relates to the interaction and cooperation between low-rank and
sparse matrices, while the second involves determining weight allocation across
different layers, as redundancy varies considerably among them. To address
these challenges, we propose a novel two-stage LLM compression method with the
capability of global rank and sparsity optimization. It is noteworthy that the
overall optimization space is vast, making comprehensive optimization
computationally prohibitive. Therefore, to reduce the optimization space, our
first stage utilizes robust principal component analysis to decompose the
weight matrices of LLMs into low-rank and sparse components, which span the low
dimensional and sparse spaces containing the resultant low-rank and sparse
matrices, respectively. In the second stage, we propose a probabilistic global
optimization technique to jointly identify the low-rank and sparse structures
within the above two spaces. The appealing feature of our approach is its
ability to automatically detect the redundancy across different layers and to
manage the interaction between the sparse and low-rank components. Extensive
experimental results indicate that our method significantly surpasses
state-of-the-art techniques for sparsification and composite approximation.

</details>


### [131] [Efficient Fine-Tuning of Quantized Models via Adaptive Rank and Bitwidth](https://arxiv.org/abs/2505.03802)
*Changhai Zhou,Yuhua Zhou,Qian Qiao,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: QLoRA结合低比特量化和LoRA实现内存友好的大语言模型微调。QR-Adaptor提出了一种联合优化量化组件和低秩空间秩的策略，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能一致提升性能，且未考虑量化与低秩子空间的协同作用。

Method: 提出QR-Adaptor，通过部分校准数据联合搜索量化组件和低秩空间秩，以离散优化问题处理精度和秩分配。

Result: 在GSM8K上准确率提升4.89%，部分情况下优于16位微调模型，同时保持4位内存占用。

Conclusion: QR-Adaptor通过联合优化显著提升了量化模型的微调性能。

Abstract: QLoRA effectively combines low-bit quantization and LoRA to achieve
memory-friendly fine-tuning for large language models (LLM). Recently, methods
based on SVD for continuous update iterations to initialize LoRA matrices to
accommodate quantization errors have generally failed to consistently improve
performance. Dynamic mixed precision is a natural idea for continuously
improving the fine-tuning performance of quantized models, but previous methods
often optimize low-rank subspaces or quantization components separately,
without considering their synergy. To address this, we propose
\textbf{QR-Adaptor}, a unified, gradient-free strategy that uses partial
calibration data to jointly search the quantization components and the rank of
low-rank spaces for each layer, thereby continuously improving model
performance. QR-Adaptor does not minimize quantization error but treats
precision and rank allocation as a discrete optimization problem guided by
actual downstream performance and memory usage. Compared to state-of-the-art
(SOTA) quantized LoRA fine-tuning methods, our approach achieves a 4.89\%
accuracy improvement on GSM8K, and in some cases even outperforms the 16-bit
fine-tuned model while maintaining the memory footprint of the 4-bit setting.

</details>


### [132] [RWKVQuant: Quantizing the RWKV Family with Proxy Guided Hybrid of Scalar and Vector Quantization](https://arxiv.org/abs/2505.03803)
*Chen Xu,Yuxuan Yue,Zukang Xu,Xing Hu,Jiangyong Yu,Zhixuan Chen,Sifan Zhou,Zhihang Yuan,Dawei Yang*

Main category: cs.LG

TL;DR: RWKVQuant是一种专为RWKV模型设计的PTQ框架，通过自适应选择量化方法和优化代码簿，显著减少量化性能损失，实现高效压缩和加速。


<details>
  <summary>Details</summary>
Motivation: RWKV作为一种现代RNN架构，在资源受限设备上部署时面临量化性能下降的挑战，需要针对其特性设计专用量化方法。

Method: 提出RWKVQuant框架，包括自适应选择量化方法的粗到细代理和优化代码簿算法，以解决非线性操作和权重分布问题。

Result: 实验表明，RWKVQuant可将RWKV-6-14B量化为约3位，精度损失小于1%，速度提升2.14倍。

Conclusion: RWKVQuant有效解决了RWKV模型的量化难题，为资源受限设备上的高效部署提供了可行方案。

Abstract: RWKV is a modern RNN architecture with comparable performance to Transformer,
but still faces challenges when deployed to resource-constrained devices. Post
Training Quantization (PTQ), which is a an essential technique to reduce model
size and inference latency, has been widely used in Transformer models.
However, it suffers significant degradation of performance when applied to
RWKV. This paper investigates and identifies two key constraints inherent in
the properties of RWKV: (1) Non-linear operators hinder the parameter-fusion of
both smooth- and rotation-based quantization, introducing extra computation
overhead. (2) The larger amount of uniformly distributed weights poses
challenges for cluster-based quantization, leading to reduced accuracy. To this
end, we propose RWKVQuant, a PTQ framework tailored for RWKV models, consisting
of two novel techniques: (1) a coarse-to-fine proxy capable of adaptively
selecting different quantization approaches by assessing the uniformity and
identifying outliers in the weights, and (2) a codebook optimization algorithm
that enhances the performance of cluster-based quantization methods for
element-wise multiplication in RWKV. Experiments show that RWKVQuant can
quantize RWKV-6-14B into about 3-bit with less than 1% accuracy loss and 2.14x
speed up.

</details>


### [133] [MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models via Expert-Balanced Sampling and Affinity Guidance](https://arxiv.org/abs/2505.03804)
*Xing Hu,Zhixuan Chen,Dawei Yang,Zukang Xu,Chen Xu,Zhihang Yuan,Sifan Zhou,Jiangyong Yu*

Main category: cs.LG

TL;DR: MoEQuant是一种针对MoE LLMs的量化框架，解决了量化中的专家间和专家内不平衡问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: MoE LLMs因稀疏和动态特性导致量化时精度下降，限制了实际应用。

Method: 提出MoEQuant框架，包含EBSS（平衡专家分布的采样方法）和AGQ（结合样本与专家亲和力的量化技术）。

Result: 实验显示，MoEQuant在4位量化下显著提升性能（HumanEval中准确率提升超10点）。

Conclusion: MoEQuant有效解决了MoE模型量化问题，提升了效率和实用性。

Abstract: Mixture-of-Experts (MoE) large language models (LLMs), which leverage dynamic
routing and sparse activation to enhance efficiency and scalability, have
achieved higher performance while reducing computational costs. However, these
models face significant memory overheads, limiting their practical deployment
and broader adoption. Post-training quantization (PTQ), a widely used method
for compressing LLMs, encounters severe accuracy degradation and diminished
generalization performance when applied to MoE models. This paper investigates
the impact of MoE's sparse and dynamic characteristics on quantization and
identifies two primary challenges: (1) Inter-expert imbalance, referring to the
uneven distribution of samples across experts, which leads to insufficient and
biased calibration for less frequently utilized experts; (2) Intra-expert
imbalance, arising from MoE's unique aggregation mechanism, which leads to
varying degrees of correlation between different samples and their assigned
experts. To address these challenges, we propose MoEQuant, a novel quantization
framework tailored for MoE LLMs. MoE-Quant includes two novel techniques: 1)
Expert-Balanced Self-Sampling (EBSS) is an efficient sampling method that
efficiently constructs a calibration set with balanced expert distributions by
leveraging the cumulative probabilities of tokens and expert balance metrics as
guiding factors. 2) Affinity-Guided Quantization (AGQ), which incorporates
affinities between experts and samples into the quantization process, thereby
accurately assessing the impact of individual samples on different experts
within the MoE layer. Experiments demonstrate that MoEQuant achieves
substantial performance gains (more than 10 points accuracy gain in the
HumanEval for DeepSeekMoE-16B under 4-bit quantization) and boosts efficiency.

</details>


### [134] [Quiet Feature Learning in Algorithmic Tasks](https://arxiv.org/abs/2505.03997)
*Prudhviraj Naidu,Zixian Wang,Leon Bergen,Ramamohan Paturi*

Main category: cs.LG

TL;DR: Transformer语言模型在算法任务训练中出现明显的损失曲线相变，挑战了损失函数逐步改善的假设。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型在算法任务中的学习动态，特别是损失曲线的异常相变现象。

Method: 训练模型并观察损失曲线，通过内部表征分析和特征消融实验验证学习机制。

Result: 发现模型学习过程中存在静默特征和突发特征，后者与性能突升相关。

Conclusion: 损失函数不能完全反映学习进展，关键特征可能在表面下积累后突然显现。

Abstract: We train Transformer-based language models on ten foundational algorithmic
tasks and observe pronounced phase transitions in their loss curves that
deviate from established power-law scaling trends. Over large ranges of
compute, the validation loss barely improves, then abruptly decreases. Probing
the models' internal representations reveals the learning of quiet features
during the stagnant phase, followed by sudden acquisition of loud features that
coincide with the sharp drop in loss. Our ablation experiments show that
disrupting a single learned feature can dramatically degrade performance,
providing evidence of their causal role in task performance. These findings
challenge the prevailing assumption that next-token predictive loss reliably
tracks incremental progress; instead, key internal features may be developing
below the surface until they coalesce, triggering a rapid performance gain.

</details>


### [135] [Feature Optimization for Time Series Forecasting via Novel Randomized Uphill Climbing](https://arxiv.org/abs/2505.03805)
*Nguyen Van Thanh*

Main category: cs.LG

TL;DR: 提出了一种基于随机上坡爬升（RUC）的轻量级特征优化框架，用于多元时间序列预测，通过随机组合操作生成候选特征，快速评分并过滤不稳定特征。


<details>
  <summary>Details</summary>
Motivation: 旨在为资源受限的机构提供高效、透明且可解释的预测工具，减少对GPU密集型深度学习的依赖。

Method: 通过随机组合领域特定语法中的操作生成候选特征，使用廉价代理模型在滚动窗口上快速评分，并通过嵌套交叉验证和信息论收缩过滤不稳定特征。

Result: 该方法有望实现更快的迭代周期、更低的能耗和更高的可解释性。

Conclusion: 该框架为多元时间序列预测提供了一种高效、透明的替代方案，特别适合资源受限的机构。

Abstract: Randomized Uphill Climbing is a lightweight, stochastic search heuristic that
has delivered state of the art equity alpha factors for quantitative hedge
funds. I propose to generalize RUC into a model agnostic feature optimization
framework for multivariate time series forecasting. The core idea is to
synthesize candidate feature programs by randomly composing operators from a
domain specific grammar, score candidates rapidly with inexpensive surrogate
models on rolling windows, and filter instability via nested cross validation
and information theoretic shrinkage. By decoupling feature discovery from GPU
heavy deep learning, the method promises faster iteration cycles, lower energy
consumption, and greater interpretability. Societal relevance: accurate,
transparent forecasting tools empower resource constrained institutions, energy
regulators, climate risk NGOs to make data driven decisions without proprietary
black box models.

</details>


### [136] [LLAMAPIE: Proactive In-Ear Conversation Assistants](https://arxiv.org/abs/2505.04066)
*Tuochao Chen,Nicholas Batchelder,Alisa Liu,Noah Smith,Shyamnath Gollakota*

Main category: cs.LG

TL;DR: LlamaPIE是一个实时主动助手，通过可穿戴设备提供不显眼的对话指导，无需用户明确调用。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型需要用户明确调用，而LlamaPIE旨在通过背景运行主动预测用户需求，提升对话体验。

Method: 构建半合成对话数据集，采用双模型管道：小模型决定何时响应，大模型生成响应。

Result: 在真实数据集上验证了有效性，用户研究显示其优于无助手和被动模型。

Conclusion: LlamaPIE展示了提升实时对话的潜力，用户更偏好主动助手。

Abstract: We introduce LlamaPIE, the first real-time proactive assistant designed to
enhance human conversations through discreet, concise guidance delivered via
hearable devices. Unlike traditional language models that require explicit user
invocation, this assistant operates in the background, anticipating user needs
without interrupting conversations. We address several challenges, including
determining when to respond, crafting concise responses that enhance
conversations, leveraging knowledge of the user for context-aware assistance,
and real-time, on-device processing. To achieve this, we construct a
semi-synthetic dialogue dataset and propose a two-model pipeline: a small model
that decides when to respond and a larger model that generates the response. We
evaluate our approach on real-world datasets, demonstrating its effectiveness
in providing helpful, unobtrusive assistance. User studies with our assistant,
implemented on Apple Silicon M2 hardware, show a strong preference for the
proactive assistant over both a baseline with no assistance and a reactive
model, highlighting the potential of LlamaPie to enhance live conversations.

</details>


### [137] [Perception-Informed Neural Networks: Beyond Physics-Informed Neural Networks](https://arxiv.org/abs/2505.03806)
*Mehran Mazandarani,Marzieh Najariyan*

Main category: cs.LG

TL;DR: PrINNs是一种将感知信息融入神经网络的框架，扩展了PINNs，支持多种感知形式，如概率分布和模糊图，通过损失函数整合专家知识，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决已知和未知物理定律或微分方程的系统建模问题，结合感知信息与神经网络，弥补传统物理建模与现代数据驱动方法的差距。

Method: 通过损失函数整合感知信息，提出MOEINNs、TKINNs和FINNs等方法，支持模糊逻辑约束和在线训练。

Result: PrINNs能处理不确定环境，建模复杂系统，发现新型微分方程，提升计算科学与工程的性能。

Conclusion: PrINNs是连接物理建模与数据驱动方法的重要工具，具有广泛的应用潜力。

Abstract: This article introduces Perception-Informed Neural Networks (PrINNs), a
framework designed to incorporate perception-based information into neural
networks, addressing both systems with known and unknown physics laws or
differential equations. Moreover, PrINNs extend the concept of Physics-Informed
Neural Networks (PINNs) and their variants, offering a platform for the
integration of diverse forms of perception precisiation, including singular,
probability distribution, possibility distribution, interval, and fuzzy graph.
In fact, PrINNs allow neural networks to model dynamical systems by integrating
expert knowledge and perception-based information through loss functions,
enabling the creation of modern data-driven models. Some of the key
contributions include Mixture of Experts Informed Neural Networks (MOEINNs),
which combine heterogeneous expert knowledge into the network, and
Transformed-Knowledge Informed Neural Networks (TKINNs), which facilitate the
incorporation of meta-information for enhanced model performance. Additionally,
Fuzzy-Informed Neural Networks (FINNs) as a modern class of fuzzy deep neural
networks leverage fuzzy logic constraints within a deep learning architecture,
allowing online training without pre-training and eliminating the need for
defuzzification. PrINNs represent a significant step forward in bridging the
gap between traditional physics-based modeling and modern data-driven
approaches, enabling neural networks to learn from both structured physics laws
and flexible perception-based rules. This approach empowers neural networks to
operate in uncertain environments, model complex systems, and discover new
forms of differential equations, making PrINNs a powerful tool for advancing
computational science and engineering.

</details>


### [138] [AI-driven multi-source data fusion for algal bloom severity classification in small inland water bodies: Leveraging Sentinel-2, DEM, and NOAA climate data](https://arxiv.org/abs/2505.03808)
*Ioannis Nasios*

Main category: cs.LG

TL;DR: 该研究提出了一种结合多源遥感数据和AI模型的高效方法，用于检测有害藻华，展示了全球应用的潜力。


<details>
  <summary>Details</summary>
Motivation: 有害藻华对水质和公共健康的威胁日益严重，亟需高效、准确且经济的检测方法。

Method: 整合Sentinel-2光学影像、DEM和NOAA气候数据，结合树模型和神经网络进行藻华严重性分类。

Result: 树模型表现优异，加入神经网络增强了鲁棒性，展示了深度学习处理多样化遥感数据的能力。

Conclusion: 该方法动态监测藻华，代码开源，展示了遥感数据与AI结合解决环境问题的潜力。

Abstract: Harmful algal blooms are a growing threat to inland water quality and public
health worldwide, creating an urgent need for efficient, accurate, and
cost-effective detection methods. This research introduces a high-performing
methodology that integrates multiple open-source remote sensing data with
advanced artificial intelligence models. Key data sources include Copernicus
Sentinel-2 optical imagery, the Copernicus Digital Elevation Model (DEM), and
NOAA's High-Resolution Rapid Refresh (HRRR) climate data, all efficiently
retrieved using platforms like Google Earth Engine (GEE) and Microsoft
Planetary Computer (MPC). The NIR and two SWIR bands from Sentinel-2, the
altitude from the elevation model, the temperature and wind from NOAA as well
as the longitude and latitude were the most important features. The approach
combines two types of machine learning models, tree-based models and a neural
network, into an ensemble for classifying algal bloom severity. While the tree
models performed strongly on their own, incorporating a neural network added
robustness and demonstrated how deep learning models can effectively use
diverse remote sensing inputs. The method leverages high-resolution satellite
imagery and AI-driven analysis to monitor algal blooms dynamically, and
although initially developed for a NASA competition in the U.S., it shows
potential for global application. The complete code is available for further
adaptation and practical implementation, illustrating the convergence of remote
sensing data and AI to address critical environmental challenges
(https://github.com/IoannisNasios/HarmfulAlgalBloomDetection).

</details>


### [139] [When Dynamic Data Selection Meets Data Augmentation](https://arxiv.org/abs/2505.03809)
*Suorong Yang,Peng Ye,Furao Shen,Dongzhan Zhou*

Main category: cs.LG

TL;DR: 提出了一种动态数据选择与增强统一框架，提升训练效率与性能。


<details>
  <summary>Details</summary>
Motivation: 动态数据选择虽加速训练但可能限制数据多样性，现有方法未充分结合数据增强。

Method: 在线训练框架，联合估计样本局部密度与多模态语义一致性，针对性选择适合增强的样本。

Result: 在多个基准数据集上表现优异，如ImageNet-1k训练成本降低50%且性能无损。

Conclusion: 方法显著减少数据集规模，提升噪声抵抗与模型鲁棒性，实用性强。

Abstract: Dynamic data selection aims to accelerate training with lossless performance.
However, reducing training data inherently limits data diversity, potentially
hindering generalization. While data augmentation is widely used to enhance
diversity, it is typically not optimized in conjunction with selection. As a
result, directly combining these techniques fails to fully exploit their
synergies. To tackle the challenge, we propose a novel online data training
framework that, for the first time, unifies dynamic data selection and
augmentation, achieving both training efficiency and enhanced performance. Our
method estimates each sample's joint distribution of local density and
multimodal semantic consistency, allowing for the targeted selection of
augmentation-suitable samples while suppressing the inclusion of noisy or
ambiguous data. This enables a more significant reduction in dataset size
without sacrificing model generalization. Experimental results demonstrate that
our method outperforms existing state-of-the-art approaches on various
benchmark datasets and architectures, e.g., reducing 50\% training costs on
ImageNet-1k with lossless performance. Furthermore, our approach enhances noise
resistance and improves model robustness, reinforcing its practical utility in
real-world scenarios.

</details>


### [140] [ScarceGAN: Discriminative Classification Framework for Rare Class Identification for Longitudinal Data with Weak Prior](https://arxiv.org/abs/2505.03811)
*Surajit Chakrabarty,Rukma Talwadker,Tridib Mukherjee*

Main category: cs.LG

TL;DR: ScarceGAN是一种针对多维度纵向遥测数据中极稀有样本识别的半监督GAN方法，通过改进判别器和生成器的目标函数，显著提升了稀有类别的召回率。


<details>
  <summary>Details</summary>
Motivation: 解决数据中正类样本极度稀缺、负类样本多类别分布不均以及大量未标记数据带来的挑战。

Method: 重新设计半监督GAN，引入‘leeway’项处理噪声先验，改进判别器和生成器的目标函数。

Result: 在技能游戏中风险玩家识别中，召回率提升至85%（比传统方法提高60%），并在KDDCUP99入侵数据集中稀有攻击类别（0.09%）识别上创下新基准。

Conclusion: ScarceGAN通过有效利用负类样本信息，显著提升了稀有样本的识别性能，为半监督学习提供了新思路。

Abstract: This paper introduces ScarceGAN which focuses on identification of extremely
rare or scarce samples from multi-dimensional longitudinal telemetry data with
small and weak label prior. We specifically address: (i) severe scarcity in
positive class, stemming from both underlying organic skew in the data, as well
as extremely limited labels; (ii) multi-class nature of the negative samples,
with uneven density distributions and partially overlapping feature
distributions; and (iii) massively unlabelled data leading to tiny and weak
prior on both positive and negative classes, and possibility of unseen or
unknown behavior in the unlabelled set, especially in the negative class.
Although related to PU learning problems, we contend that knowledge (or lack of
it) on the negative class can be leveraged to learn the compliment of it (i.e.,
the positive class) better in a semi-supervised manner. To this effect,
ScarceGAN re-formulates semi-supervised GAN by accommodating weakly labelled
multi-class negative samples and the available positive samples. It relaxes the
supervised discriminator's constraint on exact differentiation between negative
samples by introducing a 'leeway' term for samples with noisy prior. We propose
modifications to the cost objectives of discriminator, in supervised and
unsupervised path as well as that of the generator. For identifying risky
players in skill gaming, this formulation in whole gives us a recall of over
85% (~60% jump over vanilla semi-supervised GAN) on our scarce class with very
minimal verbosity in the unknown space. Further ScarceGAN outperforms the
recall benchmarks established by recent GAN based specialized models for the
positive imbalanced class identification and establishes a new benchmark in
identifying one of rare attack classes (0.09%) in the intrusion dataset from
the KDDCUP99 challenge.

</details>


### [141] [Information Filtering Networks: Theoretical Foundations, Generative Methodologies, and Real-World Applications](https://arxiv.org/abs/2505.03812)
*Tomaso Aste*

Main category: cs.LG

TL;DR: 本文综述了信息过滤网络（IFNs）的理论基础、构建方法及应用，强调其在处理高维数据建模中的优势，以及与机器学习和深度学习的结合潜力。


<details>
  <summary>Details</summary>
Motivation: IFNs通过全局稀疏但局部密集的结构捕捉多变量依赖关系，解决了高维数据建模中的关键挑战。

Method: IFNs的构建方法包括Triangulated Maximally Filtered Graph (TMFG)和Maximally Filtered Clique Forest (MFCF)等，生成高阶网络（如单纯复形）。

Result: IFNs在金融、生物、心理学和人工智能等领域提升了模型的可解释性、计算效率和预测性能。

Conclusion: IFNs不仅连接了经典网络理论与现代数据驱动范式，还可能影响深度学习模型的架构设计。

Abstract: Information Filtering Networks (IFNs) provide a powerful framework for
modeling complex systems through globally sparse yet locally dense and
interpretable structures that capture multivariate dependencies. This review
offers a comprehensive account of IFNs, covering their theoretical foundations,
construction methodologies, and diverse applications. Tracing their origins
from early network-based models to advanced formulations such as the
Triangulated Maximally Filtered Graph (TMFG) and the Maximally Filtered Clique
Forest (MFCF), the paper highlights how IFNs address key challenges in
high-dimensional data-driven modeling. IFNs and their construction
methodologies are intrinsically higher-order networks that generate simplicial
complexes-structures that are only now becoming popular in the broader
literature. Applications span fields including finance, biology, psychology,
and artificial intelligence, where IFNs improve interpretability, computational
efficiency, and predictive performance. Special attention is given to their
role in graphical modeling, where IFNs enable the estimation of sparse inverse
covariance matrices with greater accuracy and scalability than traditional
approaches like Graphical LASSO. Finally, the review discusses recent
developments that integrate IFNs with machine learning and deep learning,
underscoring their potential not only to bridge classical network theory with
contemporary data-driven paradigms, but also to shape the architectures of deep
learning models themselves.

</details>


### [142] [Program Semantic Inequivalence Game with Large Language Models](https://arxiv.org/abs/2505.03818)
*Antonio Valerio Miceli-Barone,Vaishak Belle,Ali Payani*

Main category: cs.LG

TL;DR: 论文提出了一种通过语义不等价游戏SInQ生成代码推理训练数据的方法，以提升大语言模型在复杂编程任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在日常编程任务中表现良好，但在需要复杂程序语义推理的任务中表现不佳，且缺乏相关训练数据。

Method: 采用生成器和评估器代理的半对抗训练方法，生成语义不同的程序变体，并通过自我博弈实现理论上的无限改进。

Result: 在跨语言漏洞检测和Python内置标识符交换等基准测试中，该方法显著提升了模型表现。

Conclusion: 该方法为生成高质量代码推理训练数据提供了有效途径，并展示了自我博弈的潜力。

Abstract: Large Language Models (LLMs) can achieve strong performance on everyday
coding tasks, but they can fail on complex tasks that require non-trivial
reasoning about program semantics. Finding training examples to teach LLMs to
solve these tasks can be challenging.
  In this work, we explore a method to synthetically generate code reasoning
training data based on a semantic inequivalence game SInQ: a generator agent
creates program variants that are semantically distinct, derived from a dataset
of real-world programming tasks, while an evaluator agent has to identify input
examples that cause the original programs and the generated variants to diverge
in their behaviour, with the agents training each other semi-adversarially. We
prove that this setup enables theoretically unlimited improvement through
self-play in the limit of infinite computational resources.
  We evaluated our approach on multiple code generation and understanding
benchmarks, including cross-language vulnerability detection (Lu et al., 2021),
where our method improves vulnerability detection in C/C++ code despite being
trained exclusively on Python code, and the challenging Python builtin
identifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas
modern LLMs still struggle with this benchmark, our approach yields substantial
improvements.
  We release the code needed to replicate the experiments, as well as the
generated synthetic data, which can be used to fine-tune LLMs.

</details>


### [143] [Focus on the Likely: Test-time Instance-based Uncertainty Removal](https://arxiv.org/abs/2505.03819)
*Johannes Schneider*

Main category: cs.LG

TL;DR: 提出了两种无需辅助数据的测试时微调方法，通过单步梯度下降优化高不确定性预测。


<details>
  <summary>Details</summary>
Motivation: 改进模型在高不确定性情况下的预测表现，无需额外数据。

Method: 引入关注可能类别的步骤，通过单步梯度下降优化预测。

Result: 在文本和图像领域的高不确定性样本上提升了准确性。

Conclusion: 方法有效优化了高不确定性预测，理论分析揭示了其对特征的影响。

Abstract: We propose two novel test-time fine-tuning methods to improve uncertain model
predictions. Our methods require no auxiliary data and use the given test
instance only. Instead of performing a greedy selection of the most likely
class to make a prediction, we introduce an additional focus on the likely
classes step during inference. By applying a single-step gradient descent, we
refine predictions when an initial forward pass indicates high uncertainty.
This aligns predictions more closely with the ideal of assigning zero
probability to less plausible outcomes. Our theoretical discussion provides a
deeper understanding highlighting the impact on shared and non-shared features
among (focus) classes. The experimental evaluation highlights accuracy gains on
samples exhibiting high decision uncertainty for a diverse set of models from
both the text and image domain using the same hyperparameters.

</details>


### [144] [DRSLF: Double Regularized Second-Order Low-Rank Representation for Web Service QoS Prediction](https://arxiv.org/abs/2505.03822)
*Hao Wu,Jialiang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种双正则化二阶潜在因子（DRSLF）模型，通过结合L1和L2正则化及二阶信息，提高了QoS预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LFA模型依赖一阶优化器和L2正则化，导致QoS预测精度不足。

Method: 提出DRSLF模型，整合L1和L2正则化，并在共轭梯度步中计算Hessian-向量积以引入二阶信息。

Result: 在两个真实响应时间QoS数据集上，DRSLF表现出比基线更高的低秩表示能力。

Conclusion: DRSLF模型通过双正则化和二阶优化，显著提升了QoS预测性能。

Abstract: Quality-of-Service (QoS) data plays a crucial role in cloud service
selection. Since users cannot access all services, QoS can be represented by a
high-dimensional and incomplete (HDI) matrix. Latent factor analysis (LFA)
models have been proven effective as low-rank representation techniques for
addressing this issue. However, most LFA models rely on first-order optimizers
and use L2-norm regularization, which can lead to lower QoS prediction
accuracy. To address this issue, this paper proposes a double regularized
second-order latent factor (DRSLF) model with two key ideas: a) integrating
L1-norm and L2-norm regularization terms to enhance the low-rank representation
performance; b) incorporating second-order information by calculating the
Hessian-vector product in each conjugate gradient step. Experimental results on
two real-world response-time QoS datasets demonstrate that DRSLF has a higher
low-rank representation capability than two baselines.

</details>


### [145] [Intelligently Augmented Contrastive Tensor Factorization: Empowering Multi-dimensional Time Series Classification in Low-Data Environments](https://arxiv.org/abs/2505.03825)
*Anushiya Arunan,Yan Qin,Xiaoli Li,Yuen Chau*

Main category: cs.LG

TL;DR: 论文提出了一种名为ITA-CTF的数据高效框架，用于从多维时间序列中学习有效表示，通过对比学习和智能增强解决低数据环境下的分类问题。


<details>
  <summary>Details</summary>
Motivation: 在多维时间序列分类中，标准深度学习在低数据环境下容易过拟合，难以学习泛化特征。需要一种能捕捉跨维度依赖和类内变化的高效方法。

Method: ITA-CTF框架结合了对比张量分解（CTF）和智能增强（ITA）。CTF学习时间序列的核心成分及其联合依赖，并引入对比损失优化；ITA生成类内模式增强数据。

Result: 在五个分类任务中，ITA-CTF相比标准张量分解和深度学习基准，性能提升高达18.7%。

Conclusion: ITA-CTF通过对比学习和智能增强，有效解决了低数据环境下的多维时间序列分类问题，显著提升了分类性能。

Abstract: Classification of multi-dimensional time series from real-world systems
require fine-grained learning of complex features such as cross-dimensional
dependencies and intra-class variations-all under the practical challenge of
low training data availability. However, standard deep learning (DL) struggles
to learn generalizable features in low-data environments due to model
overfitting. We propose a versatile yet data-efficient framework, Intelligently
Augmented Contrastive Tensor Factorization (ITA-CTF), to learn effective
representations from multi-dimensional time series. The CTF module learns core
explanatory components of the time series (e.g., sensor factors, temporal
factors), and importantly, their joint dependencies. Notably, unlike standard
tensor factorization (TF), the CTF module incorporates a new contrastive loss
optimization to induce similarity learning and class-awareness into the learnt
representations for better classification performance. To strengthen this
contrastive learning, the preceding ITA module generates targeted but
informative augmentations that highlight realistic intra-class patterns in the
original data, while preserving class-wise properties. This is achieved by
dynamically sampling a "soft" class prototype to guide the warping of each
query data sample, which results in an augmentation that is intelligently
pattern-mixed between the "soft" class prototype and the query sample. These
augmentations enable the CTF module to recognize complex intra-class variations
despite the limited original training data, and seek out invariant class-wise
properties for accurate classification performance. The proposed method is
comprehensively evaluated on five different classification tasks. Compared to
standard TF and several DL benchmarks, notable performance improvements up to
18.7% were achieved.

</details>


### [146] [MISE: Meta-knowledge Inheritance for Social Media-Based Stressor Estimation](https://arxiv.org/abs/2505.03827)
*Xin Wang,Ling Feng,Huijun Zhang,Lei Cao,Kaisheng Zeng,Qi Li,Yang Ding,Yi Dai,David Clifton*

Main category: cs.LG

TL;DR: 该研究提出了一种基于元学习的社交媒体压力源估计框架，通过元知识继承机制解决压力源多样性和数据稀缺问题，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现代社会中压力问题严重，社交媒体成为压力检测的新途径。现有研究多关注压力状态分类，而本研究旨在更具体地识别压力源（如考试、写论文等）。

Method: 采用少样本学习场景下的元学习框架，结合元知识继承机制，防止模型在适应新压力源时出现灾难性遗忘。

Result: 实验表明，该模型在压力源估计任务中表现优于基线方法，并公开了一个社交媒体压力源数据集。

Conclusion: 该框架能有效学习通用压力源上下文，并具有对少量标注数据的新压力源的泛化能力。

Abstract: Stress haunts people in modern society, which may cause severe health issues
if left unattended. With social media becoming an integral part of daily life,
leveraging social media to detect stress has gained increasing attention. While
the majority of the work focuses on classifying stress states and stress
categories, this study introduce a new task aimed at estimating more specific
stressors (like exam, writing paper, etc.) through users' posts on social
media. Unfortunately, the diversity of stressors with many different classes
but a few examples per class, combined with the consistent arising of new
stressors over time, hinders the machine understanding of stressors. To this
end, we cast the stressor estimation problem within a practical scenario
few-shot learning setting, and propose a novel meta-learning based stressor
estimation framework that is enhanced by a meta-knowledge inheritance
mechanism. This model can not only learn generic stressor context through
meta-learning, but also has a good generalization ability to estimate new
stressors with little labeled data. A fundamental breakthrough in our approach
lies in the inclusion of the meta-knowledge inheritance mechanism, which equips
our model with the ability to prevent catastrophic forgetting when adapting to
new stressors. The experimental results show that our model achieves
state-of-the-art performance compared with the baselines. Additionally, we
construct a social media-based stressor estimation dataset that can help train
artificial intelligence models to facilitate human well-being. The dataset is
now public at
\href{https://www.kaggle.com/datasets/xinwangcs/stressor-cause-of-mental-health-problem-dataset}{\underline{Kaggle}}
and
\href{https://huggingface.co/datasets/XinWangcs/Stressor}{\underline{Hugging
Face}}.

</details>


### [147] [Improved Dimensionality Reduction for Inverse Problems in Nuclear Fusion and High-Energy Astrophysics](https://arxiv.org/abs/2505.03849)
*Jonathan Gorard,Ammar Hakim,Hong Qin,Kyle Parfrey,Shantenu Jha*

Main category: cs.LG

TL;DR: 本文提出了一种结合蒙特卡洛采样与形式化验证方法的混合方法，用于解决核聚变和高能天体物理中的高维参数反演问题，确保参数空间的物理和数学有效性。


<details>
  <summary>Details</summary>
Motivation: 核聚变和高能天体物理中的反演问题通常涉及高维参数扫描和大规模模拟，且存在测量参数和物理模型的不确定性。传统方法无法保证参数组合的物理有效性或数学一致性。

Method: 采用蒙特卡洛采样结合非线性降维技术（如自动编码器和流形学习），并引入形式化验证方法，构建具有数学和物理正确性的参数空间限制。

Result: 该方法能够在处理实验和物理过程不确定性的同时，确保参数组合的有效性和一致性。

Conclusion: 混合方法为解决高维反演问题提供了新思路，结合了计算效率和形式化验证的可靠性。

Abstract: Many inverse problems in nuclear fusion and high-energy astrophysics
research, such as the optimization of tokamak reactor geometries or the
inference of black hole parameters from interferometric images, necessitate
high-dimensional parameter scans and large ensembles of simulations to be
performed. Such inverse problems typically involve large uncertainties, both in
the measurement parameters being inverted and in the underlying physics models
themselves. Monte Carlo sampling, when combined with modern non-linear
dimensionality reduction techniques such as autoencoders and manifold learning,
can be used to reduce the size of the parameter spaces considerably. However,
there is no guarantee that the resulting combinations of parameters will be
physically valid, or even mathematically consistent. In this position paper, we
advocate adopting a hybrid approach that leverages our recent advances in the
development of formal verification methods for numerical algorithms, with the
goal of constructing parameter space restrictions with provable mathematical
and physical correctness properties, whilst nevertheless respecting both
experimental uncertainties and uncertainties in the underlying physical
processes.

</details>


### [148] [Machine Learning: a Lecture Note](https://arxiv.org/abs/2505.03861)
*Kyunghyun Cho*

Main category: cs.LG

TL;DR: 这篇讲义旨在为数据科学及相关领域的早期硕士和博士生提供机器学习的基础知识，涵盖分类任务、概率无监督学习及进阶主题。


<details>
  <summary>Details</summary>
Motivation: 为初学者提供机器学习的基础知识，帮助他们为更高级的研究做准备。

Method: 从分类任务的基础概念（如损失函数、反向传播、随机梯度下降）入手，深入探讨概率无监督学习模型（如GAN、自回归模型），最后介绍强化学习等进阶主题。

Result: 学生将掌握机器学习的基础知识，能够进一步研究高级主题。

Conclusion: 讲义为学生提供了扎实的基础，使其能够进入更复杂的机器学习和人工智能领域。

Abstract: This lecture note is intended to prepare early-year master's and PhD students
in data science or a related discipline with foundational ideas in machine
learning. It starts with basic ideas in modern machine learning with
classification as a main target task. These basic ideas include loss
formulation, backpropagation, stochastic gradient descent, generalization,
model selection as well as fundamental blocks of artificial neural networks.
Based on these basic ideas, the lecture note explores in depth the probablistic
approach to unsupervised learning, covering directed latent variable models,
product of experts, generative adversarial networks and autoregressive models.
Finally, the note ends by covering a diverse set of further topics, such as
reinforcement learning, ensemble methods and meta-learning. After reading this
lecture note, a student should be ready to embark on studying and researching
more advanced topics in machine learning and more broadly artificial
intelligence.

</details>


### [149] [Explaining Anomalies with Tensor Networks](https://arxiv.org/abs/2505.03911)
*Hans Hohenfeld,Marius Beuerle,Elie Mounzer*

Main category: cs.LG

TL;DR: 论文扩展了张量网络在实数数据域的应用，并引入树张量网络用于可解释异常检测，展示了其预测性能和解释能力。


<details>
  <summary>Details</summary>
Motivation: 将张量网络框架扩展到实数数据域，并探索其在可解释异常检测中的潜力。

Method: 使用矩阵乘积态和树张量网络进行异常检测，并在三个基准问题上验证。

Result: 方法在预测性能上与基线模型相当，并能解释异常样本。

Conclusion: 扩展了张量网络的应用范围，为未来更复杂架构的研究铺平了道路。

Abstract: Tensor networks, a class of variational quantum many-body wave functions have
attracted considerable research interest across many disciplines, including
classical machine learning. Recently, Aizpurua et al. demonstrated explainable
anomaly detection with matrix product states on a discrete-valued
cyber-security task, using quantum-inspired methods to gain insight into the
learned model and detected anomalies. Here, we extend this framework to
real-valued data domains. We furthermore introduce tree tensor networks for the
task of explainable anomaly detection. We demonstrate these methods with three
benchmark problems, show adequate predictive performance compared to several
baseline models and both tensor network architectures' ability to explain
anomalous samples. We thereby extend the application of tensor networks to a
broader class of potential problems and open a pathway for future extensions to
more complex tensor network architectures.

</details>


### [150] [SAND: One-Shot Feature Selection with Additive Noise Distortion](https://arxiv.org/abs/2505.03923)
*Pedram Pad,Hadi Hammoud,Mohamad Dia,Nadim Maamari,L. Andrea Dunbar*

Main category: cs.LG

TL;DR: 提出了一种非侵入式特征选择层，自动选择最有信息的特征，无需调整超参数或重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要后选择重新训练和超参数调整，增加了复杂性。

Method: 通过数学公式自动选择特征，无需修改损失函数或网络架构。

Result: 在标准数据集和真实数据集上表现优异，无需超参数搜索。

Conclusion: 证明了简单性与高性能可以并存，为特征选择提供了强大工具。

Abstract: Feature selection is a critical step in data-driven applications, reducing
input dimensionality to enhance learning accuracy, computational efficiency,
and interpretability. Existing state-of-the-art methods often require
post-selection retraining and extensive hyperparameter tuning, complicating
their adoption. We introduce a novel, non-intrusive feature selection layer
that, given a target feature count $k$, automatically identifies and selects
the $k$ most informative features during neural network training. Our method is
uniquely simple, requiring no alterations to the loss function, network
architecture, or post-selection retraining. The layer is mathematically elegant
and can be fully described by: \begin{align} \nonumber \tilde{x}_i = a_i x_i +
(1-a_i)z_i \end{align} where $x_i$ is the input feature, $\tilde{x}_i$ the
output, $z_i$ a Gaussian noise, and $a_i$ trainable gain such that
$\sum_i{a_i^2}=k$. This formulation induces an automatic clustering effect,
driving $k$ of the $a_i$ gains to $1$ (selecting informative features) and the
rest to $0$ (discarding redundant ones) via weighted noise distortion and gain
normalization. Despite its extreme simplicity, our method delivers
state-of-the-art performance on standard benchmark datasets and a novel
real-world dataset, outperforming or matching existing approaches without
requiring hyperparameter search for $k$ or retraining. Theoretical analysis in
the context of linear regression further validates its efficacy. Our work
demonstrates that simplicity and performance are not mutually exclusive,
offering a powerful yet straightforward tool for feature selection in machine
learning.

</details>


### [151] [Deep Q-Network (DQN) multi-agent reinforcement learning (MARL) for Stock Trading](https://arxiv.org/abs/2505.03949)
*John Christopher Tidwell,John Storm Tidwell*

Main category: cs.LG

TL;DR: 提出了一种结合CNN、LSTM和DQN的深度学习框架，用于自动化股票交易，解决了传统方法和直接强化学习在市场噪声和复杂性中的不足。


<details>
  <summary>Details</summary>
Motivation: 传统方法和直接强化学习在股票交易中难以应对市场噪声、复杂性和泛化问题。

Method: 整合CNN（用于技术指标图像模式识别）、LSTM（捕捉价格历史和技术指标的时间依赖性）和DQN（学习最优交易策略）。

Result: 框架能够有效提取特征并学习交易策略。

Conclusion: 该集成方法为自动化股票交易提供了一种有效的解决方案。

Abstract: This project addresses the challenge of automated stock trading, where
traditional methods and direct reinforcement learning (RL) struggle with market
noise, complexity, and generalization. Our proposed solution is an integrated
deep learning framework combining a Convolutional Neural Network (CNN) to
identify patterns in technical indicators formatted as images, a Long
Short-Term Memory (LSTM) network to capture temporal dependencies across both
price history and technical indicators, and a Deep Q-Network (DQN) agent which
learns the optimal trading policy (buy, sell, hold) based on the features
extracted by the CNN and LSTM.

</details>


### [152] [Sufficient Decision Proxies for Decision-Focused Learning](https://arxiv.org/abs/2505.03953)
*Noah Schutte,Grigorii Veviurko,Krzysztof Postek,Neil Yorke-Smith*

Main category: cs.LG

TL;DR: 论文研究了在不确定优化问题中，如何通过决策导向学习（DFL）选择单值预测或分布估计，并提出了有效的决策代理方法。


<details>
  <summary>Details</summary>
Motivation: 在不确定优化问题中，传统方法通常假设单值预测或分布估计足以获得最优决策，但缺乏对这两种假设适用条件的理解。本文旨在填补这一空白。

Method: 通过分析问题特性，提出了适用于DFL的决策代理方法，并在连续和离散变量、目标函数和约束条件不确定的问题中进行了实验验证。

Result: 实验结果表明，所提出的方法在保证学习任务复杂性的同时，能够有效提升决策质量。

Conclusion: 本文为DFL中单值预测和分布估计的选择提供了理论依据，并展示了其在实际问题中的有效性。

Abstract: When solving optimization problems under uncertainty with contextual data,
utilizing machine learning to predict the uncertain parameters is a popular and
effective approach. Decision-focused learning (DFL) aims at learning a
predictive model such that decision quality, instead of prediction accuracy, is
maximized. Common practice here is to predict a single value for each uncertain
parameter, implicitly assuming that there exists a (single-scenario)
deterministic problem approximation (proxy) that is sufficient to obtain an
optimal decision. Other work assumes the opposite, where the underlying
distribution needs to be estimated. However, little is known about when either
choice is valid. This paper investigates for the first time problem properties
that justify using either assumption. Using this, we present effective decision
proxies for DFL, with very limited compromise on the complexity of the learning
task. We show the effectiveness of presented approaches in experiments on
problems with continuous and discrete variables, as well as uncertainty in the
objective function and in the constraints.

</details>


### [153] [Hierarchical Forecast Reconciliation on Networks: A Network Flow Optimization Formulation](https://arxiv.org/abs/2505.03955)
*Charupriya Sharma,Iñaki Estella Aguerri,Daniel Guimarans*

Main category: cs.LG

TL;DR: FlowRec将层次预测协调问题重新定义为网络流优化，显著提升计算效率和适用范围。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法（如MinT）仅限于树结构且计算成本高的问题。

Method: 将协调问题建模为网络流优化，支持广义网络结构，并证明多项式时间可解性。

Result: FlowRec在精度、运行时间和内存使用上显著优于MinT。

Conclusion: FlowRec是高效、可扩展的层次预测协调工具。

Abstract: Hierarchical forecasting with reconciliation requires forecasting values of a
hierarchy (e.g.~customer demand in a state and district), such that forecast
values are linked (e.g.~ district forecasts should add up to the state
forecast). Basic forecasting provides no guarantee for these desired structural
relationships. Reconciliation addresses this problem, which is crucial for
organizations requiring coherent predictions across multiple aggregation
levels. Current methods like minimum trace (MinT) are mostly limited to tree
structures and are computationally expensive. We introduce FlowRec, which
reformulates hierarchical forecast reconciliation as a network flow
optimization, enabling forecasting on generalized network structures. While
reconciliation under the $\ell_0$ norm is NP-hard, we prove polynomial-time
solvability for all $\ell_{p > 0}$ norms and , for any strictly convex and
continuously differentiable loss function. For sparse networks, FlowRec
achieves $O(n^2\log n)$ complexity, significantly improving upon MinT's
$O(n^3)$. Furthermore, we prove that FlowRec extends MinT to handle general
networks, replacing MinT's error-covariance estimation step with direct network
structural information. A key novelty of our approach is its handling of
dynamic scenarios: while traditional methods recompute both base forecasts and
reconciliation, FlowRec provides efficient localised updates with optimality
guarantees. Monotonicity ensures that when forecasts improve incrementally, the
initial reconciliation remains optimal. We also establish efficient,
error-bounded approximate reconciliation, enabling fast updates in
time-critical applications. Experiments on both simulated and real benchmarks
demonstrate that FlowRec improves accuracy, runtime by 3-40x and memory usage
by 5-7x. These results establish FlowRec as a powerful tool for large-scale
hierarchical forecasting applications.

</details>


### [154] [Call for Action: towards the next generation of symbolic regression benchmark](https://arxiv.org/abs/2505.03977)
*Guilherme S. Imai Aldeia,Hengzhe Zhang,Geoffrey Bomarito,Miles Cranmer,Alcides Fonseca,Bogdan Burlacu,William G. La Cava,Fabrício Olivetti de França*

Main category: cs.LG

TL;DR: SRBench更新版扩展了评估方法、改进了指标和可视化，分析了模型复杂性、准确性和能耗的权衡，结果显示无单一算法在所有数据集上占优，并呼吁社区共同维护SRBench。


<details>
  <summary>Details</summary>
Motivation: 由于符号回归（SR）方法的多样性、数据集和评估标准的差异，基准测试面临挑战，因此需要更新和改进SRBench以反映最新进展。

Method: 通过增加评估方法数量、优化评估指标和改进结果可视化，分析模型复杂性、准确性和能耗的权衡。

Result: 结果显示无单一算法在所有数据集上表现最优。

Conclusion: 呼吁社区共同维护SRBench，提出标准化超参数调优、执行约束和资源分配的建议，并讨论改进SR算法的最佳实践。

Abstract: Symbolic Regression (SR) is a powerful technique for discovering
interpretable mathematical expressions. However, benchmarking SR methods
remains challenging due to the diversity of algorithms, datasets, and
evaluation criteria. In this work, we present an updated version of SRBench.
Our benchmark expands the previous one by nearly doubling the number of
evaluated methods, refining evaluation metrics, and using improved
visualizations of the results to understand the performances. Additionally, we
analyze trade-offs between model complexity, accuracy, and energy consumption.
Our results show that no single algorithm dominates across all datasets. We
propose a call for action from SR community in maintaining and evolving SRBench
as a living benchmark that reflects the state-of-the-art in symbolic
regression, by standardizing hyperparameter tuning, execution constraints, and
computational resource allocation. We also propose deprecation criteria to
maintain the benchmark's relevance and discuss best practices for improving SR
algorithms, such as adaptive hyperparameter tuning and energy-efficient
implementations.

</details>


### [155] [Comparing statistical and deep learning techniques for parameter estimation of continuous-time stochastic differentiable equations](https://arxiv.org/abs/2505.03980)
*Aroon Sankoh,Victor Wickerhauser*

Main category: cs.LG

TL;DR: 比较传统统计方法（MLE）与深度学习模型（RNN）在估计Ornstein-Uhlenbeck过程参数时的准确性和计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法（如MLE）在估计随机微分方程参数时存在局限性，深度学习技术（如RNN）可能提供更精确的估计。

Method: 通过一系列实验，对比MLE和RNN在估计Ornstein-Uhlenbeck过程参数时的表现。

Result: 实验结果表明RNN在估计精度上可能优于MLE，但计算成本较高。

Conclusion: 深度学习模型（如RNN）在参数估计中具有潜力，但需权衡计算成本。

Abstract: Stochastic differential equations such as the Ornstein-Uhlenbeck process have
long been used to model realworld probablistic events such as stock prices and
temperature fluctuations. While statistical methods such as Maximum Likelihood
Estimation (MLE), Kalman Filtering, Inverse Variable Method, and more have
historically been used to estimate the parameters of stochastic differential
equations, the recent explosion of deep learning technology suggests that
models such as a Recurrent Neural Network (RNN) could produce more precise
estimators. We present a series of experiments that compare the estimation
accuracy and computational expensiveness of a statistical method (MLE) with a
deep learning model (RNN) for the parameters of the Ornstein-Uhlenbeck process.

</details>


### [156] [Diffusion Models are Secretly Exchangeable: Parallelizing DDPMs via Autospeculation](https://arxiv.org/abs/2505.03983)
*Hengyuan Hu,Aniket Das,Dorsa Sadigh,Nima Anari*

Main category: cs.LG

TL;DR: 论文提出了一种名为Autospeculative Decoding (ASD)的方法，通过利用DDPM与随机定位的联系，证明DDPM的增量满足可交换性，从而将自回归模型的优化技术应用于扩散模型，显著加速推理。


<details>
  <summary>Details</summary>
Motivation: DDPM在生成建模中表现出色，但其顺序计算需求导致推理时间瓶颈。本文旨在解决这一问题。

Method: 通过重新参数化证明DDPM增量具有可交换性，并引入ASD方法，无需辅助模型即可加速推理。

Result: 理论分析显示ASD实现了$\tilde{O}(K^{\frac{1}{3}})$的并行加速，实际应用中也显著提升了DDPM推理速度。

Conclusion: ASD为DDPM提供了一种高效的推理加速方法，展示了理论洞察的实际价值。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have emerged as powerful
tools for generative modeling. However, their sequential computation
requirements lead to significant inference-time bottlenecks. In this work, we
utilize the connection between DDPMs and Stochastic Localization to prove that,
under an appropriate reparametrization, the increments of DDPM satisfy an
exchangeability property. This general insight enables near-black-box
adaptation of various performance optimization techniques from autoregressive
models to the diffusion setting. To demonstrate this, we introduce
\emph{Autospeculative Decoding} (ASD), an extension of the widely used
speculative decoding algorithm to DDPMs that does not require any auxiliary
draft models. Our theoretical analysis shows that ASD achieves a $\tilde{O}
(K^{\frac{1}{3}})$ parallel runtime speedup over the $K$ step sequential DDPM.
We also demonstrate that a practical implementation of autospeculative decoding
accelerates DDPM inference significantly in various domains.

</details>


### [157] [Algorithmic Accountability in Small Data: Sample-Size-Induced Bias Within Classification Metrics](https://arxiv.org/abs/2505.03992)
*Jarren Briscoe,Garrett Kepler,Daryl Deford,Assefaw Gebremedhin*

Main category: cs.LG

TL;DR: 论文揭示了分类指标中组合概率引起的样本大小偏差，挑战了高分辨率评估偏见的有效性，并提出了一种模型无关的校正方法。


<details>
  <summary>Details</summary>
Motivation: 评估机器学习模型不仅需要技术准确性，还需考虑社会影响。现有方法在样本大小差异较大时可能因组合概率导致偏差。

Method: 分析了常见分类指标的偏差，提出模型无关的评估与校正技术，并处理未定义案例的影响。

Result: 揭示了组合概率在标准评估中的未识别挑战，改进了公平和可信分类方法。

Conclusion: 研究为高分辨率偏见评估提供了新视角，推动了公平分类方法的发展。

Abstract: Evaluating machine learning models is crucial not only for determining their
technical accuracy but also for assessing their potential societal
implications. While the potential for low-sample-size bias in algorithms is
well known, we demonstrate the significance of sample-size bias induced by
combinatorics in classification metrics. This revelation challenges the
efficacy of these metrics in assessing bias with high resolution, especially
when comparing groups of disparate sizes, which frequently arise in social
applications. We provide analyses of the bias that appears in several commonly
applied metrics and propose a model-agnostic assessment and correction
technique. Additionally, we analyze counts of undefined cases in metric
calculations, which can lead to misleading evaluations if improperly handled.
This work illuminates the previously unrecognized challenge of combinatorics
and probability in standard evaluation practices and thereby advances
approaches for performing fair and trustworthy classification methods.

</details>


### [158] [Iterative Orthogonalization Scaling Laws](https://arxiv.org/abs/2505.04005)
*Devan Selvaraj*

Main category: cs.LG

TL;DR: 论文探讨了muon优化器在大规模应用中可能存在的奇异值收缩问题，但未提出解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究muon优化器在大规模应用中的性能限制，尤其是奇异值收缩问题。

Method: 通过理论和实验分析随机矩阵的奇异值缩放行为。

Result: 发现muon优化器在大规模下存在奇异值收缩问题。

Conclusion: 论文揭示了问题，但未提供具体解决方案。

Abstract: The muon optimizer has picked up much attention as of late as a possible
replacement to the seemingly omnipresent Adam optimizer. Recently, care has
been taken to document the scaling laws of hyper-parameters under muon such as
weight decay and learning rate. However, at much larger scales the iterative
orthogonalization procedure present in muon may suffer a possible issue as the
singular values of random matrices shrink with scale. This paper shows this
scaling behavior theoretically and empirically on random matrices but does not
suggest what to do about it.

</details>


### [159] [Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks](https://arxiv.org/abs/2505.04046)
*Xuyang Wang,Siyuan Duan,Qizhi Li,Guiduo Duan,Yuan Sun,Dezhong Peng*

Main category: cs.LG

TL;DR: 论文提出了一种名为RDML的多视图学习框架，通过证据解耦学习和特征重校准模块解决对抗性扰动问题，显著提升了多视图分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有可信多视图学习方法假设数据安全，但实际应用中对抗性扰动会破坏模型可靠性，导致对抗性不可靠问题（AUP）。

Method: 提出RDML框架，包括证据解耦学习、特征重校准模块和视图级证据注意力机制。

Result: 实验表明，RDML在对抗性攻击下的多视图分类任务中显著优于现有方法。

Conclusion: RDML有效解决了对抗性不可靠问题，提升了多视图学习的可信度。

Abstract: Recently, trustworthy multi-view learning has attracted extensive attention
because evidence learning can provide reliable uncertainty estimation to
enhance the credibility of multi-view predictions. Existing trusted multi-view
learning methods implicitly assume that multi-view data is secure. In practice,
however, in safety-sensitive applications such as autonomous driving and
security monitoring, multi-view data often faces threats from adversarial
perturbations, thereby deceiving or disrupting multi-view learning models. This
inevitably leads to the adversarial unreliability problem (AUP) in trusted
multi-view learning. To overcome this tricky problem, we propose a novel
multi-view learning framework, namely Reliable Disentanglement Multi-view
Learning (RDML). Specifically, we first propose evidential disentanglement
learning to decompose each view into clean and adversarial parts under the
guidance of corresponding evidences, which is extracted by a pretrained
evidence extractor. Then, we employ the feature recalibration module to
mitigate the negative impact of adversarial perturbations and extract potential
informative features from them. Finally, to further ignore the irreparable
adversarial interferences, a view-level evidential attention mechanism is
designed. Extensive experiments on multi-view classification tasks with
adversarial attacks show that our RDML outperforms the state-of-the-art
multi-view learning methods by a relatively large margin.

</details>


### [160] [LLM-e Guess: Can LLMs Capabilities Advance Without Hardware Progress?](https://arxiv.org/abs/2505.04075)
*Teddy Foley,Spencer Guo,Henry Josephson,Anqi Qu,Jack Sanderson*

Main category: cs.LG

TL;DR: 论文探讨了在计算资源受限的情况下，大型语言模型（LLM）是否仍能通过算法创新取得进展，并提出了计算等效增益（CEG）指标来衡量算法改进的效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于监管机构对高性能硬件的限制，探讨在计算资源受限的环境中，LLM是否仍能通过算法创新实现性能提升。

Method: 提出了一个分类框架，区分计算依赖型创新和计算无关型创新，并通过小规模GPT-2模型训练实验验证框架。

Result: 计算无关型创新在资源受限环境中仍能带来显著性能提升（CEG达3.5倍），而计算依赖型创新在小规模实验中效果有限甚至可能降低性能。

Conclusion: 计算资源的可用性对某些算法改进至关重要，但计算无关型创新为LLM在受限环境中的发展提供了可行路径。

Abstract: This paper examines whether large language model (LLM) capabilities can
continue to advance without additional compute by analyzing the development and
role of algorithms used in state-of-the-art LLMs. Motivated by regulatory
efforts that have largely focused on restricting access to high-performance
hardware, we ask: Can LLMs progress in a compute-constrained environment, and
how do algorithmic innovations perform under such conditions?
  To address these questions, we introduce a novel classification framework
that distinguishes between compute-dependent innovations -- which yield
disproportionate benefits at high compute levels (e.g., the Transformer
architecture and mixture-of-experts models) and compute-independent
innovations, which improve efficiency across all compute scales (e.g., rotary
positional encoding, FlashAttention, or layer normalization). We quantify these
contributions using a metric called compute-equivalent gain (CEG), which
estimates the additional compute that would be required to achieve similar
improvements without these algorithmic advancements.
  To validate this framework, we conduct small-scale training experiments with
a scaled-down GPT-2 model. Our results confirm that compute-independent
advancements yield meaningful performance gains even in resource-constrained
settings, with a CEG of up to $3.5\times$ over a baseline model. By contrast,
compute-dependent advancements provided little benefit or even degraded
performance at the small scale, reinforcing the importance of compute
availability for certain algorithmic gains.

</details>


### [161] [Plexus: Taming Billion-edge Graphs with 3D Parallel GNN Training](https://arxiv.org/abs/2505.04083)
*Aditya K. Ranjan,Siddharth Singh,Cunyang Wei,Abhinav Bhatele*

Main category: cs.LG

TL;DR: Plexus提出了一种三维并行方法，用于全图训练，解决了GPU内存限制和分布式训练中的通信开销问题，显著提升了训练速度和效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图数据规模庞大，超出GPU内存容量，传统方法如小批量采样可能降低准确性，而分布式全图训练则面临通信开销和负载不均衡问题。

Method: Plexus采用三维并行方法，结合负载平衡的置换方案和性能模型优化配置，支持亿级边图的训练。

Result: 在Perlmutter和Frontier上，Plexus实现了2.3x-12.5x的速度提升，解决方案时间减少5.2-8.7x和7-54.2x。

Conclusion: Plexus为大规模图数据的训练提供了一种高效且可扩展的解决方案，显著优于现有方法。

Abstract: Graph neural networks have emerged as a potent class of neural networks
capable of leveraging the connectivity and structure of real-world graphs to
learn intricate properties and relationships between nodes. Many real-world
graphs exceed the memory capacity of a GPU due to their sheer size, and using
GNNs on them requires techniques such as mini-batch sampling to scale. However,
this can lead to reduced accuracy in some cases, and sampling and data transfer
from the CPU to the GPU can also slow down training. On the other hand,
distributed full-graph training suffers from high communication overhead and
load imbalance due to the irregular structure of graphs. We propose Plexus, a
three-dimensional (3D) parallel approach for full-graph training that tackles
these issues and scales to billion-edge graphs. Additionally, we introduce
optimizations such as a permutation scheme for load balancing, and a
performance model to predict the optimal 3D configuration. We evaluate Plexus
on several graph datasets and show scaling results for up to 2048 GPUs on
Perlmutter, which is 33% of the machine, and 2048 GCDs on Frontier. Plexus
achieves unprecedented speedups of 2.3x-12.5x over existing methods and a
reduction in the time to solution by 5.2-8.7x on Perlmutter and 7-54.2x on
Frontier.

</details>


### [162] [Position: We need responsible, application-driven (RAD) AI research](https://arxiv.org/abs/2505.04104)
*Sarah Hartman,Cheng Soon Ong,Julia Powles,Petra Kuhnert*

Main category: cs.LG

TL;DR: 本文主张通过负责任、应用驱动的方法（RAD-AI）推动AI研究，以实现科学和社会进步。


<details>
  <summary>Details</summary>
Motivation: 随着AI在社会中的广泛应用，研究者需关注具体应用场景，包括伦理、法律、技术和社会约束。

Method: 提出三阶段方法：1）组建跨学科团队和以人为本的研究；2）解决特定背景下的方法、伦理承诺和指标；3）通过测试平台和实践社区验证和维持效果。

Result: 展示了应用驱动AI研究的未来愿景，强调技术可行性与社区需求的适应性。

Conclusion: RAD-AI方法能通过满足具体需求和价值观，释放AI的新价值。

Abstract: This position paper argues that achieving meaningful scientific and societal
advances with artificial intelligence (AI) requires a responsible,
application-driven approach (RAD) to AI research. As AI is increasingly
integrated into society, AI researchers must engage with the specific contexts
where AI is being applied. This includes being responsive to ethical and legal
considerations, technical and societal constraints, and public discourse. We
present the case for RAD-AI to drive research through a three-staged approach:
(1) building transdisciplinary teams and people-centred studies; (2) addressing
context-specific methods, ethical commitments, assumptions, and metrics; and
(3) testing and sustaining efficacy through staged testbeds and a community of
practice. We present a vision for the future of application-driven AI research
to unlock new value through technically feasible methods that are adaptive to
the contextual needs and values of the communities they ultimately serve.

</details>


### [163] [Alpha Excel Benchmark](https://arxiv.org/abs/2505.04110)
*David Noever,Forrest McKee*

Main category: cs.LG

TL;DR: 该研究提出了一种基于金融建模世界杯（FMWC）Excel竞赛的新基准，用于评估大型语言模型（LLMs），并展示了不同模型在任务中的表现差异。


<details>
  <summary>Details</summary>
Motivation: 通过将FMWC挑战转化为可编程评估的JSON格式，研究旨在为LLMs提供一个更贴近实际业务任务的标准化评估框架，弥补学术基准与实际应用之间的差距。

Method: 将113个FMWC挑战转化为JSON格式，并利用该数据集比较多个领先LLMs的性能。

Result: 模型在不同任务类别中表现差异显著，擅长模式识别但在复杂数值推理上表现不佳。

Conclusion: 该基准为评估LLMs在业务任务中的能力提供了标准化工具，并强调了Excel用户群体作为评估指标的重要性。

Abstract: This study presents a novel benchmark for evaluating Large Language Models
(LLMs) using challenges derived from the Financial Modeling World Cup (FMWC)
Excel competitions. We introduce a methodology for converting 113 existing FMWC
challenges into programmatically evaluable JSON formats and use this dataset to
compare the performance of several leading LLMs. Our findings demonstrate
significant variations in performance across different challenge categories,
with models showing specific strengths in pattern recognition tasks but
struggling with complex numerical reasoning. The benchmark provides a
standardized framework for assessing LLM capabilities in realistic
business-oriented tasks rather than abstract academic problems. This research
contributes to the growing field of AI benchmarking by establishing proficiency
among the 1.5 billion people who daily use Microsoft Excel as a meaningful
evaluation metric that bridges the gap between academic AI benchmarks and
practical business applications.

</details>


### [164] [LHT: Statistically-Driven Oblique Decision Trees for Interpretable Classification](https://arxiv.org/abs/2505.04139)
*Hongyi Li,Jun Xu,William Ward Armstrong*

Main category: cs.LG

TL;DR: LHT是一种新型斜决策树模型，通过非迭代统计方法构建分割超平面，具有高表达性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统斜决策树依赖迭代优化或启发式方法，LHT旨在提供一种直接、确定性且理论支持的方法。

Method: LHT直接计算超平面参数，基于节点内类间特征期望差异的权重，并使用局部最小二乘拟合的线性隶属函数进行预测。

Result: 实验表明LHT在基准数据集上具有竞争力，时间复杂度为O(mnd)，适合构建深度树。

Conclusion: LHT是一种实用、理论支持且可解释的斜决策树替代方案。

Abstract: We introduce the Learning Hyperplane Tree (LHT), a novel oblique decision
tree model designed for expressive and interpretable classification. LHT
fundamentally distinguishes itself through a non-iterative,
statistically-driven approach to constructing splitting hyperplanes. Unlike
methods that rely on iterative optimization or heuristics, LHT directly
computes the hyperplane parameters, which are derived from feature weights
based on the differences in feature expectations between classes within each
node. This deterministic mechanism enables a direct and well-defined hyperplane
construction process. Predictions leverage a unique piecewise linear membership
function within leaf nodes, obtained via local least-squares fitting. We
formally analyze the convergence of the LHT splitting process, ensuring that
each split yields meaningful, non-empty partitions. Furthermore, we establish
that the time complexity for building an LHT up to depth $d$ is $O(mnd)$,
demonstrating the practical feasibility of constructing trees with powerful
oblique splits using this methodology. The explicit feature weighting at each
split provides inherent interpretability. Experimental results on benchmark
datasets demonstrate LHT's competitive accuracy, positioning it as a practical,
theoretically grounded, and interpretable alternative in the landscape of
tree-based models. The implementation of the proposed method is available at
https://github.com/Hongyi-Li-sz/LHT_model.

</details>


### [165] [FilterTS: Comprehensive Frequency Filtering for Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.04158)
*Yulong Wang,Yushuo Liu,Xiaoyi Duan,Kai Wang*

Main category: cs.LG

TL;DR: FilterTS是一种新颖的多变量时间序列预测模型，通过频域滤波技术动态提取和强化变量间的共享频率成分，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以捕捉复杂的周期和趋势成分，影响了多变量时间序列预测的准确性。

Method: FilterTS采用动态跨变量滤波模块和静态全局滤波模块，结合频域转换提升计算效率。

Result: 在八个真实数据集上的实验表明，FilterTS在预测准确性和计算效率上显著优于现有方法。

Conclusion: FilterTS通过频域滤波技术有效解决了多变量时间序列预测中的复杂模式提取问题。

Abstract: Multivariate time series forecasting is crucial across various industries,
where accurate extraction of complex periodic and trend components can
significantly enhance prediction performance. However, existing models often
struggle to capture these intricate patterns. To address these challenges, we
propose FilterTS, a novel forecasting model that utilizes specialized filtering
techniques based on the frequency domain. FilterTS introduces a Dynamic
Cross-Variable Filtering Module, a key innovation that dynamically leverages
other variables as filters to extract and reinforce shared variable frequency
components across variables in multivariate time series. Additionally, a Static
Global Filtering Module captures stable frequency components, identified
throughout the entire training set. Moreover, the model is built in the
frequency domain, converting time-domain convolutions into frequency-domain
multiplicative operations to enhance computational efficiency. Extensive
experimental results on eight real-world datasets have demonstrated that
FilterTS significantly outperforms existing methods in terms of prediction
accuracy and computational efficiency.

</details>


### [166] [Optimization of Infectious Disease Intervention Measures Based on Reinforcement Learning -- Empirical analysis based on UK COVID-19 epidemic data](https://arxiv.org/abs/2505.04161)
*Baida Zhang,Yakai Chen,Huichun Li,Zhenghu Zu*

Main category: cs.LG

TL;DR: 本文提出了一种基于个体代理模型的强化学习框架，用于优化传染病干预措施，验证了其有效性和可行性。


<details>
  <summary>Details</summary>
Motivation: 传染病爆发对健康和经济造成深远影响，现有研究多基于简化模型，无法充分模拟复杂动态传播过程。

Method: 结合强化学习与个体代理模型（Covasim），探索多种算法在不同动作空间的应用，并进行理论分析。

Result: 实验验证了框架的有效性，策略能有效抑制疫情扩散并维护经济稳定。

Conclusion: 该框架为全球公共卫生安全策略提供了重要参考。

Abstract: Globally, the outbreaks of infectious diseases have exerted an extremely
profound and severe influence on health security and the economy. During the
critical phases of epidemics, devising effective intervention measures poses a
significant challenge to both the academic and practical arenas. There is
numerous research based on reinforcement learning to optimize intervention
measures of infectious diseases. Nevertheless, most of these efforts have been
confined within the differential equation based on infectious disease models.
Although a limited number of studies have incorporated reinforcement learning
methodologies into individual-based infectious disease models, the models
employed therein have entailed simplifications and limitations, rendering it
incapable of modeling the complexity and dynamics inherent in infectious
disease transmission. We establish a decision-making framework based on an
individual agent-based transmission model, utilizing reinforcement learning to
continuously explore and develop a strategy function. The framework's validity
is verified through both experimental and theoretical approaches. Covasim, a
detailed and widely used agent-based disease transmission model, was modified
to support reinforcement learning research. We conduct an exhaustive
exploration of the application efficacy of multiple algorithms across diverse
action spaces. Furthermore, we conduct an innovative preliminary theoretical
analysis concerning the issue of "time coverage". The results of the experiment
robustly validate the effectiveness and feasibility of the methodological
framework of this study. The coping strategies gleaned therefrom prove highly
efficacious in suppressing the expansion of the epidemic scale and safeguarding
the stability of the economic system, thereby providing crucial reference
perspectives for the formulation of global public health security strategies.

</details>


### [167] [Retrieval Augmented Time Series Forecasting](https://arxiv.org/abs/2505.04163)
*Sungwon Han,Seungeon Lee,Meeyoung Cha,Sercan O Arik,Jinsung Yoon*

Main category: cs.LG

TL;DR: RAFT是一种基于检索增强的时间序列预测方法，通过检索历史数据中的相似模式来提升预测能力，实验表明其优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法可能无法充分利用历史数据中的模式，RAFT旨在通过检索相似历史数据来增强模型的预测能力。

Method: RAFT通过检索与输入最相似的历史数据候选，并利用这些候选的未来值辅助预测。

Result: 在十个基准数据集上，RAFT平均胜率为86%，显著优于现有方法。

Conclusion: RAFT通过检索历史数据模式，有效提升了时间序列预测的准确性。

Abstract: Time series forecasting uses historical data to predict future trends,
leveraging the relationships between past observations and available features.
In this paper, we propose RAFT, a retrieval-augmented time series forecasting
method to provide sufficient inductive biases and complement the model's
learning capacity. When forecasting the subsequent time frames, we directly
retrieve historical data candidates from the training dataset with patterns
most similar to the input, and utilize the future values of these candidates
alongside the inputs to obtain predictions. This simple approach augments the
model's capacity by externally providing information about past patterns via
retrieval modules. Our empirical evaluations on ten benchmark datasets show
that RAFT consistently outperforms contemporary baselines with an average win
ratio of 86%.

</details>


### [168] [STRGCN: Capturing Asynchronous Spatio-Temporal Dependencies for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.04167)
*Yulong Wang,Xiaofeng Hu,Xiaojian Cui,Kai Wang*

Main category: cs.LG

TL;DR: STRGCN是一种新型的时空关系图卷积网络，用于处理不规则多元时间序列（IMTS），无需预对齐数据，直接捕捉复杂依赖关系。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因预对齐数据而扭曲固有模式、增加计算和内存需求的问题。

Method: 将IMTS表示为全连接图，每个观测点为节点，通过分层“三明治”结构优化图嵌入。

Result: 在四个公共数据集上实现了最先进的准确性，同时保持了竞争性的内存使用和训练速度。

Conclusion: STRGCN有效解决了IMTS建模中的异步性问题，具有高效性和准确性。

Abstract: Irregular multivariate time series (IMTS) are prevalent in real-world
applications across many fields, where varying sensor frequencies and
asynchronous measurements pose significant modeling challenges. Existing
solutions often rely on a pre-alignment strategy to normalize data, which can
distort intrinsic patterns and escalate computational and memory demands.
Addressing these limitations, we introduce STRGCN, a Spatio-Temporal Relational
Graph Convolutional Network that avoids pre-alignment and directly captures the
complex interdependencies in IMTS by representing them as a fully connected
graph. Each observation is represented as a node, allowing the model to
effectively handle misaligned timestamps by mapping all inter-node
relationships, thus faithfully preserving the asynchronous nature of the data.
Moreover, we enhance this model with a hierarchical ``Sandwich'' structure that
strategically aggregates nodes to optimize graph embeddings, reducing
computational overhead while maintaining detailed local and global context.
Extensive experiments on four public datasets demonstrate that STRGCN achieves
state-of-the-art accuracy, competitive memory usage and training speed.

</details>


### [169] [DiffPattern-Flex: Efficient Layout Pattern Generation via Discrete Diffusion](https://arxiv.org/abs/2505.04173)
*Zixiao Wang,Wenqian Zhao,Yunheng Shen,Yang Bai,Guojin Chen,Farzan Farnia,Bei Yu*

Main category: cs.LG

TL;DR: DiffPattern-Flex 是一种基于离散扩散模型的新方法，用于高效生成合法且多样化的布局模式，结合优化评估和快速采样技术。


<details>
  <summary>Details</summary>
Motivation: 当前深度生成模型在布局模式生成中占主导，但仅依赖神经网络难以保证合法性，因此需要一种更可靠的方法。

Method: 采用离散扩散模型生成多样化拓扑，结合基于设计规则的优化评估和快速采样技术。

Result: 实验表明，DiffPattern-Flex 在多个基准测试中显著优于现有方法，能高效生成合法布局模式。

Conclusion: DiffPattern-Flex 提供了一种高效可靠的布局模式生成方案，解决了神经网络在合法性上的不足。

Abstract: Recent advancements in layout pattern generation have been dominated by deep
generative models. However, relying solely on neural networks for legality
guarantees raises concerns in many practical applications. In this paper, we
present \tool{DiffPattern}-Flex, a novel approach designed to generate reliable
layout patterns efficiently. \tool{DiffPattern}-Flex incorporates a new method
for generating diverse topologies using a discrete diffusion model while
maintaining a lossless and compute-efficient layout representation. To ensure
legal pattern generation, we employ {an} optimization-based, white-box pattern
assessment process based on specific design rules. Furthermore, fast sampling
and efficient legalization technologies are employed to accelerate the
generation process. Experimental results across various benchmarks demonstrate
that \tool{DiffPattern}-Flex significantly outperforms existing methods and
excels at producing reliable layout patterns.

</details>


### [170] [On-Device LLM for Context-Aware Wi-Fi Roaming](https://arxiv.org/abs/2505.04174)
*Ju-Hyung Lee,Yanqing Lu*

Main category: cs.LG

TL;DR: 论文提出了一种利用设备端大型语言模型（LLM）进行跨层无线漫游优化的方法，通过上下文感知AP选择和动态阈值调整，显著提升了漫游稳定性和信号质量。


<details>
  <summary>Details</summary>
Motivation: 传统基于阈值或启发式的方法在动态移动环境中表现不佳，导致漫游粘滞或频繁切换，亟需更智能的解决方案。

Method: 采用LLM进行跨层优化，包括上下文感知AP选择和动态阈值调整，并通过链式思维提示、参数高效微调和量化等技术满足边缘硬件的低延迟和资源限制。

Result: 实验表明，该方法优于传统启发式和深度强化学习基线，实现了漫游稳定性和信号质量的平衡。

Conclusion: 应用层LLM推理为未来边缘系统中的底层无线控制提供了新思路。

Abstract: Wireless roaming is a critical yet challenging task for maintaining seamless
connectivity in dynamic mobile environments. Conventional threshold-based or
heuristic schemes often fail, leading to either sticky or excessive handovers.
We introduce the first cross-layer use of an on-device large language model
(LLM): high-level reasoning in the application layer that issues real-time
actions executed in the PHY/MAC stack. The LLM addresses two tasks: (i)
context-aware AP selection, where structured prompts fuse environmental cues
(e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold
adjustment, where the model adaptively decides when to roam. To satisfy the
tight latency and resource budgets of edge hardware, we apply a suite of
optimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and
quantization. Experiments on indoor and outdoor datasets show that our approach
surpasses legacy heuristics and DRL baselines, achieving a strong balance
between roaming stability and signal quality. These findings underscore the
promise of application-layer LLM reasoning for lower-layer wireless control in
future edge systems.

</details>


### [171] [Trajectory Entropy Reinforcement Learning for Predictable and Robust Control](https://arxiv.org/abs/2505.04193)
*Bang You,Chenxu Wang,Huaping Liu*

Main category: cs.LG

TL;DR: 论文提出了一种新的强化学习归纳偏置，通过最小化动作轨迹的熵来简化策略，提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在复杂控制任务中表现优异，但容易捕捉观测与动作间的虚假关联，导致环境扰动下失效。

Method: 引入轨迹熵最小化作为归纳偏置，通过变分参数化动作预测模型估计熵，构建信息正则化奖励函数。

Result: 实验表明，该方法在高维运动任务中生成更一致的动作轨迹，性能和鲁棒性优于现有技术。

Conclusion: 轨迹熵最小化是一种有效的简化策略方法，显著提升了强化学习的鲁棒性。

Abstract: Simplicity is a critical inductive bias for designing data-driven
controllers, especially when robustness is important. Despite the impressive
results of deep reinforcement learning in complex control tasks, it is prone to
capturing intricate and spurious correlations between observations and actions,
leading to failure under slight perturbations to the environment. To tackle
this problem, in this work we introduce a novel inductive bias towards simple
policies in reinforcement learning. The simplicity inductive bias is introduced
by minimizing the entropy of entire action trajectories, corresponding to the
number of bits required to describe information in action trajectories after
the agent observes state trajectories. Our reinforcement learning agent,
Trajectory Entropy Reinforcement Learning, is optimized to minimize the
trajectory entropy while maximizing rewards. We show that the trajectory
entropy can be effectively estimated by learning a variational parameterized
action prediction model, and use the prediction model to construct an
information-regularized reward function. Furthermore, we construct a practical
algorithm that enables the joint optimization of models, including the policy
and the prediction model. Experimental evaluations on several high-dimensional
locomotion tasks show that our learned policies produce more cyclical and
consistent action trajectories, and achieve superior performance, and
robustness to noise and dynamic changes than the state-of-the-art.

</details>


### [172] [A Large Language Model for Feasible and Diverse Population Synthesis](https://arxiv.org/abs/2505.04196)
*Sung Yoo Lim,Hyunsoo Yun,Prateek Bansal,Dong-Kyu Kim,Eui-Jin Kim*

Main category: cs.LG

TL;DR: 提出了一种结合大型语言模型（LLM）和贝叶斯网络（BN）的微调方法，用于生成可行且多样化的合成人口，显著提高了可行性并保持了多样性。


<details>
  <summary>Details</summary>
Motivation: 在基于活动的模型（ABM）中，生成可行且多样化的合成人口对下游活动计划模拟的有效性至关重要。传统深度生成模型（DGM）难以平衡稀有但合理的组合与排除不合理组合。

Method: 通过贝叶斯网络（BN）的拓扑排序显式控制自回归生成过程，对大型语言模型（LLM）进行微调。

Result: 实验结果显示，该方法可行性达95%，显著高于传统DGM的80%，同时保持多样性。

Conclusion: 该方法基于轻量级开源LLM，成本低且可扩展，适用于大规模应用（如大城市人口合成），提高了模拟可靠性并减少下游误差传播。

Abstract: Generating a synthetic population that is both feasible and diverse is
crucial for ensuring the validity of downstream activity schedule simulation in
activity-based models (ABMs). While deep generative models (DGMs), such as
variational autoencoders and generative adversarial networks, have been applied
to this task, they often struggle to balance the inclusion of rare but
plausible combinations (i.e., sampling zeros) with the exclusion of implausible
ones (i.e., structural zeros). To improve feasibility while maintaining
diversity, we propose a fine-tuning method for large language models (LLMs)
that explicitly controls the autoregressive generation process through
topological orderings derived from a Bayesian Network (BN). Experimental
results show that our hybrid LLM-BN approach outperforms both traditional DGMs
and proprietary LLMs (e.g., ChatGPT-4o) with few-shot learning. Specifically,
our approach achieves approximately 95% feasibility, significantly higher than
the ~80% observed in DGMs, while maintaining comparable diversity, making it
well-suited for practical applications. Importantly, the method is based on a
lightweight open-source LLM, enabling fine-tuning and inference on standard
personal computing environments. This makes the approach cost-effective and
scalable for large-scale applications, such as synthesizing populations in
megacities, without relying on expensive infrastructure. By initiating the ABM
pipeline with high-quality synthetic populations, our method improves overall
simulation reliability and reduces downstream error propagation. The source
code for these methods is available for research and practical application.

</details>


### [173] [Estimating Causal Effects in Networks with Cluster-Based Bandits](https://arxiv.org/abs/2505.04200)
*Ahmed Sayeed Faruk,Jason Sulskis,Elena Zheleva*

Main category: cs.LG

TL;DR: 论文提出了两种基于聚类的多臂老虎机（MAB）算法，用于在网络中逐步估计总治疗效果，同时通过探索与利用的权衡最大化预期奖励。相比忽略聚类的普通MAB算法和随机对照试验（RCT）方法，聚类MAB算法在奖励-行动比上表现更好，且未显著牺牲治疗效果估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 在存在干扰（如社交网络中个体相互影响）的情况下，A/B测试难以准确估计因果效应，且可能导致性能损失。因此，需要一种能动态适应并高效学习网络中总治疗效果的策略。

Method: 引入两种基于聚类的MAB算法，通过探索与利用的权衡逐步估计网络中的总治疗效果。

Result: 聚类MAB算法在奖励-行动比上优于普通MAB和RCT方法，且未显著牺牲治疗效果估计的准确性。普通MAB算法因未考虑聚类导致更高的治疗效应误差。

Conclusion: 基于聚类的MAB算法能有效平衡探索与利用，在网络干扰环境下提供更优的奖励-行动比和治疗效果估计。

Abstract: The gold standard for estimating causal effects is randomized controlled
trial (RCT) or A/B testing where a random group of individuals from a
population of interest are given treatment and the outcome is compared to a
random group of individuals from the same population. However, A/B testing is
challenging in the presence of interference, commonly occurring in social
networks, where individuals can impact each others outcome. Moreover, A/B
testing can incur a high performance loss when one of the treatment arms has a
poor performance and the test continues to treat individuals with it.
Therefore, it is important to design a strategy that can adapt over time and
efficiently learn the total treatment effect in the network. We introduce two
cluster-based multi-armed bandit (MAB) algorithms to gradually estimate the
total treatment effect in a network while maximizing the expected reward by
making a tradeoff between exploration and exploitation. We compare the
performance of our MAB algorithms with a vanilla MAB algorithm that ignores
clusters and the corresponding RCT methods on semi-synthetic data with
simulated interference. The vanilla MAB algorithm shows higher reward-action
ratio at the cost of higher treatment effect error due to undesired spillover.
The cluster-based MAB algorithms show higher reward-action ratio compared to
their corresponding RCT methods without sacrificing much accuracy in treatment
effect estimation.

</details>


### [174] [Cyber Security Data Science: Machine Learning Methods and their Performance on Imbalanced Datasets](https://arxiv.org/abs/2505.04204)
*Mateo Lopez-Ledezma,Gissel Velarde*

Main category: cs.LG

TL;DR: 论文探讨了网络安全中的二元分类问题，通过三个实验评估了多种分类器和采样技术，发现不平衡学习技术效果不一，建议针对不同数据集测试最佳方法。


<details>
  <summary>Details</summary>
Motivation: 网络安全需求日益增长，自动化处理大规模数据成为关键。二元分类在异常检测等应用中尤为重要，但数据不平衡问题普遍存在。

Method: 通过三个实验评估：1）单分类器性能（如随机森林、XGBoost等）；2）不同采样技术（过采样、欠采样等）；3）Self-Paced Ensembling及其基分类器数量。

Result: 不平衡学习技术效果因数据集而异，部分技术表现不稳定。不同数据集的最佳分类器和采样方法不同。

Conclusion: 建议针对新数据集测试单分类器和不平衡学习技术，以优化网络安全应用中的分类效果。

Abstract: Cybersecurity has become essential worldwide and at all levels, concerning
individuals, institutions, and governments. A basic principle in cybersecurity
is to be always alert. Therefore, automation is imperative in processes where
the volume of daily operations is large. Several cybersecurity applications can
be addressed as binary classification problems, including anomaly detection,
fraud detection, intrusion detection, spam detection, or malware detection. We
present three experiments. In the first experiment, we evaluate single
classifiers including Random Forests, Light Gradient Boosting Machine, eXtreme
Gradient Boosting, Logistic Regression, Decision Tree, and Gradient Boosting
Decision Tree. In the second experiment, we test different sampling techniques
including over-sampling, under-sampling, Synthetic Minority Over-sampling
Technique, and Self-Paced Ensembling. In the last experiment, we evaluate
Self-Paced Ensembling and its number of base classifiers. We found that
imbalance learning techniques had positive and negative effects, as reported in
related studies. Thus, these techniques should be applied with caution.
Besides, we found different best performers for each dataset. Therefore, we
recommend testing single classifiers and imbalance learning techniques for each
new dataset and application involving imbalanced datasets as is the case in
several cyber security applications.

</details>


### [175] [FRAIN to Train: A Fast-and-Reliable Solution for Decentralized Federated Learning](https://arxiv.org/abs/2505.04223)
*Sanghyeon Park,Soo-Mook Moon*

Main category: cs.LG

TL;DR: FRAIN是一种新的异步联邦学习方法，通过FastSync策略和SLERP参数合并技术，解决了传统方法在数据异构性和延迟问题上的局限性，表现出更稳定的收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法（如FedAvg和FedAsync）在数据异构性和设备延迟问题上表现不佳，区块链方法（如BRAIN）也存在同步开销问题。FRAIN旨在解决这些限制。

Method: FRAIN采用FastSync策略避免重放旧模型版本，并使用SLERP技术合并参数以减少干扰。

Result: 实验表明，FRAIN在非IID数据、延迟网络和恶意节点环境下，比FedAvg、FedAsync和BRAIN表现更稳定。

Conclusion: FRAIN通过创新策略显著提升了异步联邦学习的性能和鲁棒性。

Abstract: Federated learning (FL) enables collaborative model training across
distributed clients while preserving data locality. Although FedAvg pioneered
synchronous rounds for global model averaging, slower devices can delay
collective progress. Asynchronous FL (e.g., FedAsync) addresses stragglers by
continuously integrating client updates, yet naive implementations risk client
drift due to non-IID data and stale contributions. Some Blockchain-based FL
approaches (e.g., BRAIN) employ robust weighting or scoring of updates to
resist malicious or misaligned proposals. However, performance drops can still
persist under severe data heterogeneity or high staleness, and synchronization
overhead has emerged as a new concern due to its aggregator-free architectures.
  We introduce Fast-and-Reliable AI Network, FRAIN, a new asynchronous FL
method that mitigates these limitations by incorporating two key ideas. First,
our FastSync strategy eliminates the need to replay past model versions,
enabling newcomers and infrequent participants to efficiently approximate the
global model. Second, we adopt spherical linear interpolation (SLERP) when
merging parameters, preserving models' directions and alleviating destructive
interference from divergent local training.
  Experiments with a CNN image-classification model and a Transformer-based
language model demonstrate that FRAIN achieves more stable and robust
convergence than FedAvg, FedAsync, and BRAIN, especially under harsh
environments: non-IID data distributions, networks that experience delays and
require frequent re-synchronization, and the presence of malicious nodes.

</details>


### [176] [Technology prediction of a 3D model using Neural Network](https://arxiv.org/abs/2505.04241)
*Grzegorz Miebs,Rafał A. Bachorz*

Main category: cs.LG

TL;DR: 本文提出了一种数据驱动的方法，通过3D模型预测制造步骤及其时间，替代传统依赖专家或历史数据的方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法在动态或定制化生产环境中效果不佳，需要更准确的生产时间估计。

Method: 将3D模型渲染为2D图像，利用受生成查询网络启发的神经网络，将几何特征映射为生产步骤的时间估计。

Result: 方法能够实现跨产品类型的可扩展、自适应和精确的工艺规划。

Conclusion: 数据驱动方法为动态生产环境提供了更高效的时间预测工具。

Abstract: Accurate estimation of production times is critical for effective
manufacturing scheduling, yet traditional methods relying on expert analysis or
historical data often fall short in dynamic or customized production
environments. This paper introduces a data-driven approach that predicts
manufacturing steps and their durations directly from a product's 3D model. By
rendering the model into multiple 2D images and leveraging a neural network
inspired by the Generative Query Network, the method learns to map geometric
features into time estimates for predefined production steps enabling scalable,
adaptive, and precise process planning across varied product types.

</details>


### [177] [Physics-Informed DeepONets for drift-diffusion on metric graphs: simulation and parameter identification](https://arxiv.org/abs/2505.04263)
*Jan Blechschmidt,Tom-Christian Riemer,Max Winkler,Martin Stoll,Jan-F. Pietschmann*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的深度学习方法来求解度量图上的非线性漂移扩散方程，适用于多种应用场景，如生物细胞传输和人群运动。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在模型设计或参数识别问题上需要大量定制，而物理信息深度算子网络（DeepONet）能更灵活地解决偏微分方程，并易于处理参数识别问题。

Method: 首先学习三个DeepONet模型（分别针对流入、内部和流出边），然后通过基于边的域分解方法耦合这些模型来解决漂移扩散度量图问题。

Result: 该方法能准确评估图耦合物理模型，并适用于解决这些耦合网络上的优化或逆问题。

Conclusion: 提出的框架为度量图上的非线性漂移扩散方程提供了一种高效且通用的解决方案。

Abstract: We develop a novel physics informed deep learning approach for solving
nonlinear drift-diffusion equations on metric graphs. These models represent an
important model class with a large number of applications in areas ranging from
transport in biological cells to the motion of human crowds. While traditional
numerical schemes require a large amount of tailoring, especially in the case
of model design or parameter identification problems, physics informed deep
operator networks (DeepONet) have emerged as a versatile tool for the solution
of partial differential equations with the particular advantage that they
easily incorporate parameter identification questions. We here present an
approach where we first learn three DeepONet models for representative inflow,
inner and outflow edges, resp., and then subsequently couple these models for
the solution of the drift-diffusion metric graph problem by relying on an
edge-based domain decomposition approach. We illustrate that our framework is
applicable for the accurate evaluation of graph-coupled physics models and is
well suited for solving optimization or inverse problems on these coupled
networks.

</details>


### [178] [Non-stationary Diffusion For Probabilistic Time Series Forecasting](https://arxiv.org/abs/2505.04278)
*Weiwei Ye,Zhuopeng Xu,Ning Gui*

Main category: cs.LG

TL;DR: 论文提出了一种基于位置-尺度噪声模型（LSNM）的非平稳扩散模型（NsDiff），用于解决传统扩散模型（DDPMs）在时间序列不确定性建模中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型（DDPMs）因假设噪声方差恒定，无法捕捉时间序列中不确定性的非平稳特性。

Method: 利用LSNM放松固定不确定性假设，设计NsDiff框架，结合去噪扩散生成模型和预训练的条件均值与方差估计器，并引入不确定性感知的噪声调度。

Result: 在九个真实和合成数据集上的实验表明，NsDiff优于现有方法。

Conclusion: NsDiff通过动态调整噪声水平，有效建模了时间序列的不确定性变化。

Abstract: Due to the dynamics of underlying physics and external influences, the
uncertainty of time series often varies over time. However, existing Denoising
Diffusion Probabilistic Models (DDPMs) often fail to capture this
non-stationary nature, constrained by their constant variance assumption from
the additive noise model (ANM). In this paper, we innovatively utilize the
Location-Scale Noise Model (LSNM) to relax the fixed uncertainty assumption of
ANM. A diffusion-based probabilistic forecasting framework, termed
Non-stationary Diffusion (NsDiff), is designed based on LSNM that is capable of
modeling the changing pattern of uncertainty. Specifically, NsDiff combines a
denoising diffusion-based conditional generative model with a pre-trained
conditional mean and variance estimator, enabling adaptive endpoint
distribution modeling. Furthermore, we propose an uncertainty-aware noise
schedule, which dynamically adjusts the noise levels to accurately reflect the
data uncertainty at each step and integrates the time-varying variances into
the diffusion process. Extensive experiments conducted on nine real-world and
synthetic datasets demonstrate the superior performance of NsDiff compared to
existing approaches. Code is available at https://github.com/wwy155/NsDiff.

</details>


### [179] [Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of Fit Testing](https://arxiv.org/abs/2505.04318)
*Jacob Glenn Ayers,Buvaneswari A. Ramanan,Manzoor A. Khan*

Main category: cs.LG

TL;DR: 论文提出了一种基于χ²拟合优度检验的概念漂移检测方法，用于监控深度学习模型在推理过程中可能遇到的数据分布变化。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型的广泛应用，人工验证变得不可行，需要元算法确保模型推理的可靠性。概念漂移检测领域在监控神经网络中的应用不足。

Method: 应用χ²拟合优度假设检验作为漂移检测元算法，测试多层感知机、卷积神经网络和Transformer在模拟漂移下的表现。

Result: 研究表明，无需直接检查推理输出即可检测因概念漂移导致的准确性下降。

Conclusion: 该方法通过持续评估模型在不同条件下的可靠性，增强了安全性。

Abstract: As the adoption of deep learning models has grown beyond human capacity for
verification, meta-algorithms are needed to ensure reliable model inference.
Concept drift detection is a field dedicated to identifying statistical shifts
that is underutilized in monitoring neural networks that may encounter
inference data with distributional characteristics diverging from their
training data. Given the wide variety of model architectures, applications, and
datasets, it is important that concept drift detection algorithms are adaptable
to different inference scenarios. In this paper, we introduce an application of
the $\chi^2$ Goodness of Fit Hypothesis Test as a drift detection
meta-algorithm applied to a multilayer perceptron, a convolutional neural
network, and a transformer trained for machine vision as they are exposed to
simulated drift during inference. To that end, we demonstrate how unexpected
drops in accuracy due to concept drift can be detected without directly
examining the inference outputs. Our approach enhances safety by ensuring
models are continually evaluated for reliability across varying conditions.

</details>


### [180] [Hyperbolic Fuzzy $C$-Means with Adaptive Weight-based Filtering for Clustering in Non-Euclidean Spaces](https://arxiv.org/abs/2505.04335)
*Swagato Das,Arghya Pratihar,Swagatam Das*

Main category: cs.LG

TL;DR: 论文提出了一种名为HypeFCM的新聚类算法，结合双曲几何和模糊聚类，用于处理非欧几里得空间的数据，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统模糊聚类方法（如FCM）在非欧几里得空间中表现不佳，无法有效捕捉复杂或层次化结构。

Method: HypeFCM结合双曲几何（Poincaré Disc模型）和模糊聚类，通过基于Dirichlet分布的权重初始化和迭代优化聚类中心与成员分配。

Result: 实验表明，HypeFCM在非欧几里得设置中显著优于传统模糊聚类方法。

Conclusion: HypeFCM是一种高效且鲁棒的聚类算法，特别适用于非欧几里得空间的数据分析。

Abstract: Clustering algorithms play a pivotal role in unsupervised learning by
identifying and grouping similar objects based on shared characteristics. While
traditional clustering techniques, such as hard and fuzzy center-based
clustering, have been widely used, they struggle with complex,
high-dimensional, and non-Euclidean datasets. In particular, the Fuzzy
$C$-Means (FCM) algorithm, despite its efficiency and popularity, exhibits
notable limitations in non-Euclidean spaces. Euclidean spaces assume linear
separability and uniform distance scaling, limiting their effectiveness in
capturing complex, hierarchical, or non-Euclidean structures in fuzzy
clustering. To overcome these challenges, we introduce Filtration-based
Hyperbolic Fuzzy $C$-Means (HypeFCM), a novel clustering algorithm tailored for
better representation of data relationships in non-Euclidean spaces. HypeFCM
integrates the principles of fuzzy clustering with hyperbolic geometry and
employs a weight-based filtering mechanism to improve performance. The
algorithm initializes weights using a Dirichlet distribution and iteratively
refines cluster centroids and membership assignments based on a hyperbolic
metric in the Poincar\'e Disc model. Extensive experimental evaluations
demonstrate that HypeFCM significantly outperforms conventional fuzzy
clustering methods in non-Euclidean settings, underscoring its robustness and
effectiveness.

</details>


### [181] [Riemannian Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2505.04338)
*Zichen Liu,Wei Zhang,Christof Schütte,Tiejun Li*

Main category: cs.LG

TL;DR: 提出了一种基于黎曼流形的去噪扩散概率模型（RDDPMs），用于学习欧几里得空间中子流形上的分布，适用于更广泛的流形。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于大量几何信息（如测地线或拉普拉斯-贝尔特拉米算子的特征函数），限制了其应用范围。本文方法仅需函数值和一阶导数，适用性更广。

Method: 基于投影方案，仅需定义子流形的函数值和一阶导数，无需复杂几何信息。

Result: 理论分析表明RDDPMs与流形上的基于分数的生成模型有关联。实验验证了方法在多个数据集上的有效性。

Conclusion: RDDPMs为更广泛的流形提供了一种高效的生成建模方法，扩展了现有技术的应用范围。

Abstract: We propose Riemannian Denoising Diffusion Probabilistic Models (RDDPMs) for
learning distributions on submanifolds of Euclidean space that are level sets
of functions, including most of the manifolds relevant to applications.
Existing methods for generative modeling on manifolds rely on substantial
geometric information such as geodesic curves or eigenfunctions of the
Laplace-Beltrami operator and, as a result, they are limited to manifolds where
such information is available. In contrast, our method, built on a projection
scheme, can be applied to more general manifolds, as it only requires being
able to evaluate the value and the first order derivatives of the function that
defines the submanifold. We provide a theoretical analysis of our method in the
continuous-time limit, which elucidates the connection between our RDDPMs and
score-based generative models on manifolds. The capability of our method is
demonstrated on datasets from previous studies and on new datasets sampled from
two high-dimensional manifolds, i.e. $\mathrm{SO}(10)$ and the configuration
space of molecular system alanine dipeptide with fixed dihedral angle.

</details>


### [182] [Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning](https://arxiv.org/abs/2505.04339)
*Hao Peng,Xiang Huang,Shuo Sun,Ruitong Zhang,Philip S. Yu*

Main category: cs.LG

TL;DR: AR-DBSCAN是一种基于多智能体强化学习的自适应DBSCAN改进方法，通过分区处理和自动参数搜索，有效解决了传统DBSCAN在变密度数据集上的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统DBSCAN在处理变密度数据集时效果不佳，需要手动调整参数，限制了其在实际应用中的表现。

Method: 1. 将数据集建模为两级编码树，分区处理；2. 设计多智能体强化学习自动搜索参数；3. 提出递归搜索机制适应数据规模。

Result: 在NMI和ARI指标上分别提升144.1%和175.3%，并能鲁棒地找到最优参数。

Conclusion: AR-DBSCAN显著提升了DBSCAN在变密度数据集上的聚类效果，具有实际应用价值。

Abstract: DBSCAN, a well-known density-based clustering algorithm, has gained
widespread popularity and usage due to its effectiveness in identifying
clusters of arbitrary shapes and handling noisy data. However, it encounters
challenges in producing satisfactory cluster results when confronted with
datasets of varying density scales, a common scenario in real-world
applications. In this paper, we propose a novel Adaptive and Robust DBSCAN with
Multi-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First,
we model the initial dataset as a two-level encoding tree and categorize the
data vertices into distinct density partitions according to the information
uncertainty determined in the encoding tree. Each partition is then assigned to
an agent to find the best clustering parameters without manual assistance. The
allocation is density-adaptive, enabling AR-DBSCAN to effectively handle
diverse density distributions within the dataset by utilizing distinct agents
for different partitions. Second, a multi-agent deep reinforcement learning
guided automatic parameter searching process is designed. The process of
adjusting the parameter search direction by perceiving the clustering
environment is modeled as a Markov decision process. Using a weakly-supervised
reward training policy network, each agent adaptively learns the optimal
clustering parameters by interacting with the clusters. Third, a recursive
search mechanism adaptable to the data's scale is presented, enabling efficient
and controlled exploration of large parameter spaces. Extensive experiments are
conducted on nine artificial datasets and a real-world dataset. The results of
offline and online tasks show that AR-DBSCAN not only improves clustering
accuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively,
but also is capable of robustly finding dominant parameters.

</details>


### [183] [Multi-Granular Attention based Heterogeneous Hypergraph Neural Network](https://arxiv.org/abs/2505.04340)
*Hong Jin,Kaicheng Zhou,Jie Yin,Lan You,Zhifeng Zhou*

Main category: cs.LG

TL;DR: MGA-HHN提出了一种基于多粒度注意力的异质超图神经网络，通过构建元路径异质超图和多粒度注意力机制，解决了现有HeteGNNs无法捕获高阶关系和长距离信息失真的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的异质图神经网络（HeteGNNs）依赖元路径的消息传递，无法有效捕获高阶关系，且存在长距离信息失真（over-squashing）的问题。

Method: 提出MGA-HHN，包括构建元路径异质超图和多粒度注意力机制，分别在节点和超边级别操作，以捕获细粒度交互并保持语义多样性。

Result: 在真实数据集上的实验表明，MGA-HHN在节点分类、聚类和可视化任务中优于现有模型。

Conclusion: MGA-HHN通过建模高阶语义和多粒度注意力，显著提升了异质图表示学习的性能。

Abstract: Heterogeneous graph neural networks (HeteGNNs) have demonstrated strong
abilities to learn node representations by effectively extracting complex
structural and semantic information in heterogeneous graphs. Most of the
prevailing HeteGNNs follow the neighborhood aggregation paradigm, leveraging
meta-path based message passing to learn latent node representations. However,
due to the pairwise nature of meta-paths, these models fail to capture
high-order relations among nodes, resulting in suboptimal performance.
Additionally, the challenge of ``over-squashing'', where long-range message
passing in HeteGNNs leads to severe information distortion, further limits the
efficacy of these models. To address these limitations, this paper proposes
MGA-HHN, a Multi-Granular Attention based Heterogeneous Hypergraph Neural
Network for heterogeneous graph representation learning. MGA-HHN introduces two
key innovations: (1) a novel approach for constructing meta-path based
heterogeneous hypergraphs that explicitly models higher-order semantic
information in heterogeneous graphs through multiple views, and (2) a
multi-granular attention mechanism that operates at both the node and hyperedge
levels. This mechanism enables the model to capture fine-grained interactions
among nodes sharing the same semantic context within a hyperedge type, while
preserving the diversity of semantics across different hyperedge types. As
such, MGA-HHN effectively mitigates long-range message distortion and generates
more expressive node representations. Extensive experiments on real-world
benchmark datasets demonstrate that MGA-HHN outperforms state-of-the-art
models, showcasing its effectiveness in node classification, node clustering
and visualization tasks.

</details>


### [184] [Topology-Driven Clustering: Enhancing Performance with Betti Number Filtration](https://arxiv.org/abs/2505.04346)
*Arghya Pratihar,Kushal Bose,Swagatam Das*

Main category: cs.LG

TL;DR: 该论文提出了一种基于拓扑结构（如Vietoris-Rips复形和Betti数过滤）的聚类算法，通过引入Betti序列来捕捉复杂数据集中的拓扑特征，解决了现有方法在复杂形状数据集上的性能不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有拓扑聚类算法未能充分利用拓扑结构，且在复杂数据集上表现不一致，因此需要一种更有效的方法来聚类复杂且交织的形状。

Method: 通过Vietoris-Rips复形和Betti数过滤识别拓扑相似的邻居，并引入Betti序列来灵活捕捉拓扑结构中的关键特征。

Result: 在合成和真实数据集上的实验表明，该算法优于其他基于拓扑的聚类算法。

Conclusion: 提出的算法能够有效聚类复杂数据集中的交织形状，性能优于现有方法。

Abstract: Clustering aims to form groups of similar data points in an unsupervised
regime. Yet, clustering complex datasets containing critically intertwined
shapes poses significant challenges. The prevailing clustering algorithms
widely depend on evaluating similarity measures based on Euclidean metrics.
Exploring topological characteristics to perform clustering of complex datasets
inevitably presents a better scope. The topological clustering algorithms
predominantly perceive the point set through the lens of Simplicial complexes
and Persistent homology. Despite these approaches, the existing topological
clustering algorithms cannot somehow fully exploit topological structures and
show inconsistent performances on some highly complicated datasets. This work
aims to mitigate the limitations by identifying topologically similar neighbors
through the Vietoris-Rips complex and Betti number filtration. In addition, we
introduce the concept of the Betti sequences to capture flexibly essential
features from the topological structures. Our proposed algorithm is adept at
clustering complex, intertwined shapes contained in the datasets. We carried
out experiments on several synthetic and real-world datasets. Our algorithm
demonstrated commendable performances across the datasets compared to some of
the well-known topology-based clustering algorithms.

</details>


### [185] [Deep Learning Innovations for Energy Efficiency: Advances in Non-Intrusive Load Monitoring and EV Charging Optimization for a Sustainable Grid](https://arxiv.org/abs/2505.04367)
*Stavros Sykiotis*

Main category: cs.LG

TL;DR: 论文探讨了通过深度学习技术加速能源转型，重点关注住宅能耗和道路运输两个领域。


<details>
  <summary>Details</summary>
Motivation: 全球能源转型面临复杂性和速度问题，需探索新途径以减少高能耗领域的碳排放。

Method: 开发深度学习工具，包括非侵入式负载监测和深度强化学习优化电动车充电。

Result: 提出两种技术方案，分别降低住宅能耗和优化道路运输的碳减排。

Conclusion: 深度学习技术为加速能源转型提供了可行路径，尤其在住宅和交通领域。

Abstract: The global energy landscape is undergoing a profound transformation, often
referred to as the energy transition, driven by the urgent need to mitigate
climate change, reduce greenhouse gas emissions, and ensure sustainable energy
supplies. However, the undoubted complexity of new investments in renewables,
as well as the phase out of high CO2-emission energy sources, hampers the pace
of the energy transition and raises doubts as to whether new renewable energy
sources are capable of solely meeting the climate target goals. This highlights
the need to investigate alternative pathways to accelerate the energy
transition, by identifying human activity domains with higher/excessive energy
demands. Two notable examples where there is room for improvement, in the sense
of reducing energy consumption and consequently CO2 emissions, are residential
energy consumption and road transport. This dissertation investigates the
development of novel Deep Learning techniques to create tools which solve
limitations in these two key energy domains. Reduction of residential energy
consumption can be achieved by empowering end-users with the user of
Non-Intrusive Load Monitoring, whereas optimization of EV charging with Deep
Reinforcement Learning can tackle road transport decarbonization.

</details>


### [186] [Extending a Quantum Reinforcement Learning Exploration Policy with Flags to Connect Four](https://arxiv.org/abs/2505.04371)
*Filipe Santos,João Paulo Fernandes,Luís Macedo*

Main category: cs.LG

TL;DR: 论文研究了基于标志的动作选择强化学习探索策略，并在量子计算中实现二次加速，应用于Connect Four游戏，验证其性能。


<details>
  <summary>Details</summary>
Motivation: 探索基于标志的RL策略在不同游戏环境（Connect Four）中的表现，并研究量子加速的效果。

Method: 训练和测试经典与量子RL代理，分别作为先手和后手，对抗随机Negamax对手，记录迭代次数和胜率。

Result: 量子代理能更快采样标志动作，但胜率与经典代理相同；标志探索策略优于epsilon-greedy策略。

Conclusion: 量子加速在采样效率上有优势，但胜率未提升，可能与训练场景简单有关。

Abstract: Action selection based on flags is a Reinforcement Learning (RL) exploration
policy that improves the exploration of the state space through the use of
flags, which can identify the most promising actions to take in each state. The
quantum counterpart of this exploration policy further improves upon this by
taking advantage of a quadratic speedup for sampling flagged actions. This
approach has already been successfully employed for the game of Checkers. In
this work, we describe the application of this method to the context of Connect
Four, in order to study its performance in a different setting, which can lead
to a better generalization of the technique. We also kept track of a metric
that wasn't taken into account in previous work: the average number of
iterations to obtain a flagged action. Since going second is a significant
disadvantage in Connect Four, we also had the intent of exploring how this more
complex scenario would impact the performance of our approach. The experiments
involved training and testing classical and quantum RL agents that played
either going first or going second against a Randomized Negamax opponent. The
results showed that both flagged exploration policies were clearly superior to
a simple epsilon-greedy policy. Furthermore, the quantum agents did in fact
sample flagged actions in less iterations. Despite obtaining tagged actions
more consistently, the win rates between the classical and quantum versions of
the approach were identical, which could be due to the simplicity of the
training scenario chosen.

</details>


### [187] [Clust-Splitter $-$ an Efficient Nonsmooth Optimization-Based Algorithm for Clustering Large Datasets](https://arxiv.org/abs/2505.04389)
*Jenni Lampainen,Kaisa Joki,Napsu Karmitsa,Marko M. Mäkelä*

Main category: cs.LG

TL;DR: Clust-Splitter是一种基于非光滑优化的高效算法，用于解决大规模数据集中的最小平方和聚类问题。


<details>
  <summary>Details</summary>
Motivation: 聚类是数据挖掘和机器学习中的基础任务，尤其在大规模数据分析中。现有方法在处理大规模数据时效率不足，因此需要一种更高效的算法。

Method: 通过三个非光滑优化问题的序列（两个辅助问题生成初始点，一个主聚类问题），结合有限内存束方法和增量方法开发了Clust-Splitter算法。

Result: 在真实数据集上的实验表明，Clust-Splitter在大规模数据聚类中效率高，且解的质量与现有最佳方法相当。

Conclusion: Clust-Splitter是一种高效且高质量的聚类算法，适用于大规模数据集。

Abstract: Clustering is a fundamental task in data mining and machine learning,
particularly for analyzing large-scale data. In this paper, we introduce
Clust-Splitter, an efficient algorithm based on nonsmooth optimization,
designed to solve the minimum sum-of-squares clustering problem in very large
datasets. The clustering task is approached through a sequence of three
nonsmooth optimization problems: two auxiliary problems used to generate
suitable starting points, followed by a main clustering formulation. To solve
these problems effectively, the limited memory bundle method is combined with
an incremental approach to develop the Clust-Splitter algorithm. We evaluate
Clust-Splitter on real-world datasets characterized by both a large number of
attributes and a large number of data points and compare its performance with
several state-of-the-art large-scale clustering algorithms. Experimental
results demonstrate the efficiency of the proposed method for clustering very
large datasets, as well as the high quality of its solutions, which are on par
with those of the best existing methods.

</details>


### [188] [Supporting renewable energy planning and operation with data-driven high-resolution ensemble weather forecast](https://arxiv.org/abs/2505.04396)
*Jingnan Wang,Jie Chao,Shangshang Yang,Congyi Nai,Kaijun Ren,Kefeng Deng,Xi Chen,Yaxin Liu,Hanqiuzi Wen,Ziniu Xiao,Lifeng Zhang,Xiaodong Wang,Jiping Guan,Baoxiang Pan*

Main category: cs.LG

TL;DR: 论文提出了一种通过结合高分辨率气候先验和粗网格天气预报的方法，显著提高了风电场天气预测的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 可再生能源（尤其是风能）的规划和运营需要准确、及时且高分辨率的天气信息，但传统方法存在尺度不一致、计算成本高等问题。

Method: 通过学习目标风电场的高分辨率数值天气模拟的气候分布，结合粗网格天气预报，生成高精度、细粒度的天气模式预测。

Result: 与现有方法相比，该方法在确定性/概率性预测和经济收益方面表现更优，且计算成本大幅降低。

Conclusion: 该方法为可再生能源的高效可靠规划和运营提供了新途径。

Abstract: The planning and operation of renewable energy, especially wind power, depend
crucially on accurate, timely, and high-resolution weather information.
Coarse-grid global numerical weather forecasts are typically downscaled to meet
these requirements, introducing challenges of scale inconsistency, process
representation error, computation cost, and entanglement of distinct
uncertainty sources from chaoticity, model bias, and large-scale forcing. We
address these challenges by learning the climatological distribution of a
target wind farm using its high-resolution numerical weather simulations. An
optimal combination of this learned high-resolution climatological prior with
coarse-grid large scale forecasts yields highly accurate, fine-grained,
full-variable, large ensemble of weather pattern forecasts. Using observed
meteorological records and wind turbine power outputs as references, the
proposed methodology verifies advantageously compared to existing
numerical/statistical forecasting-downscaling pipelines, regarding either
deterministic/probabilistic skills or economic gains. Moreover, a 100-member,
10-day forecast with spatial resolution of 1 km and output frequency of 15 min
takes < 1 hour on a moderate-end GPU, as contrast to $\mathcal{O}(10^3)$ CPU
hours for conventional numerical simulation. By drastically reducing
computational costs while maintaining accuracy, our method paves the way for
more efficient and reliable renewable energy planning and operation.

</details>


### [189] [Latent Manifold Reconstruction and Representation with Topological and Geometrical Regularization](https://arxiv.org/abs/2505.04412)
*Ren Wang,Pengcheng Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于AutoEncoder的方法，结合流形重建层，从噪声点云中提取潜在流形结构，并在降维过程中保持拓扑和几何特性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以从噪声数据中同时捕捉局部细节和全局拓扑完整性，导致降维结果失真或断裂。

Method: 通过AutoEncoder结合流形重建层，提取潜在流形结构，并在降维过程中加入拓扑和几何正则化。

Result: 在点云数据集上优于t-SNE、UMAP和Topological AutoEncoders，验证了方法的有效性。

Conclusion: 结合流形重建与流形学习，能更可靠地表示潜在流形结构，尤其适用于噪声数据。

Abstract: Manifold learning aims to discover and represent low-dimensional structures
underlying high-dimensional data while preserving critical topological and
geometric properties. Existing methods often fail to capture local details with
global topological integrity from noisy data or construct a balanced
dimensionality reduction, resulting in distorted or fractured embeddings. We
present an AutoEncoder-based method that integrates a manifold reconstruction
layer, which uncovers latent manifold structures from noisy point clouds, and
further provides regularizations on topological and geometric properties during
dimensionality reduction, whereas the two components promote each other during
training. Experiments on point cloud datasets demonstrate that our method
outperforms baselines like t-SNE, UMAP, and Topological AutoEncoders in
discovering manifold structures from noisy data and preserving them through
dimensionality reduction, as validated by visualization and quantitative
metrics. This work demonstrates the significance of combining manifold
reconstruction with manifold learning to achieve reliable representation of the
latent manifold, particularly when dealing with noisy real-world data. Code
repository: https://github.com/Thanatorika/mrtg.

</details>


### [190] [Localized Diffusion Models for High Dimensional Distributions Generation](https://arxiv.org/abs/2505.04417)
*Georg A. Gottwald,Shuigen Liu,Youssef Marzouk,Sebastian Reich,Xin T. Tong*

Main category: cs.LG

TL;DR: 扩散模型在生成任务中表现优异，但高维评分函数估计可能导致维度灾难。本文提出局部扩散模型，利用目标分布的低维结构，通过局部化评分匹配损失和神经网络降低样本复杂度，平衡统计与局部化误差，提升性能。


<details>
  <summary>Details</summary>
Motivation: 高维评分函数估计可能导致维度灾难，需利用目标分布的低维结构（如局部性结构）来优化扩散模型。

Method: 提出局部扩散模型，使用局部化评分匹配损失训练评分函数，并限制假设空间为局部化结构，以减少样本复杂度。

Result: 理论及实验表明，适度局部化半径可平衡统计与局部化误差，提升整体性能，并支持并行训练。

Conclusion: 局部扩散模型能有效规避维度灾难，适用于大规模应用，性能优于传统方法。

Abstract: Diffusion models are the state-of-the-art tools for various generative tasks.
However, estimating high-dimensional score functions makes them potentially
suffer from the curse of dimensionality (CoD). This underscores the importance
of better understanding and exploiting low-dimensional structure in the target
distribution. In this work, we consider locality structure, which describes
sparse dependencies between model components. Under locality structure, the
score function is effectively low-dimensional, so that it can be estimated by a
localized neural network with significantly reduced sample complexity. This
motivates the localized diffusion model, where a localized score matching loss
is used to train the score function within a localized hypothesis space. We
prove that such localization enables diffusion models to circumvent CoD, at the
price of additional localization error. Under realistic sample size scaling, we
show both theoretically and numerically that a moderate localization radius can
balance the statistical and localization error, leading to a better overall
performance. The localized structure also facilitates parallel training of
diffusion models, making it potentially more efficient for large-scale
applications.

</details>


### [191] [FedBWO: Enhancing Communication Efficiency in Federated Learning](https://arxiv.org/abs/2505.04435)
*Vahideh Hayyolalam,Öznur Özkasap*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedBWO的联邦学习优化技术，通过传输性能评分而非模型权重来减少通信数据量，显著提升了全局模型性能和通信效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中，客户端因资源受限常面临传输能力不足的问题，现有方法传输大量模型权重数据导致通信瓶颈。

Method: 采用黑寡妇优化算法（BWO）改进本地模型更新，仅传输性能评分以减少通信数据量。

Result: 实验表明，FedBWO比FedAvg和FedGWO分别平均提升21%和12%的全局模型准确率，并大幅降低通信成本。

Conclusion: FedBWO是一种高效的联邦学习优化方法，显著提升了模型性能和通信效率。

Abstract: Federated Learning (FL) is a distributed Machine Learning (ML) setup, where a
shared model is collaboratively trained by various clients using their local
datasets while keeping the data private. Considering resource-constrained
devices, FL clients often suffer from restricted transmission capacity. Aiming
to enhance the system performance, the communication between clients and server
needs to be diminished. Current FL strategies transmit a tremendous amount of
data (model weights) within the FL process, which needs a high communication
bandwidth. Considering resource constraints, increasing the number of clients
and, consequently, the amount of data (model weights) can lead to a bottleneck.
In this paper, we introduce the Federated Black Widow Optimization (FedBWO)
technique to decrease the amount of transmitted data by transmitting only a
performance score rather than the local model weights from clients. FedBWO
employs the BWO algorithm to improve local model updates. The conducted
experiments prove that FedBWO remarkably improves the performance of the global
model and the communication efficiency of the overall system. According to the
experimental outcomes, FedBWO enhances the global model accuracy by an average
of 21% over FedAvg, and 12% over FedGWO. Furthermore, FedBWO dramatically
decreases the communication cost compared to other methods.

</details>


### [192] [Towards Initialization-Agnostic Clustering with Iterative Adaptive Resonance Theory](https://arxiv.org/abs/2505.04440)
*Xiaozheng Qu,Zhaochuan Li,Zhuang Qi,Xiang Li,Haibei Huang,Lei Meng,Xiangxu Meng*

Main category: cs.LG

TL;DR: IR-ART通过迭代框架提升Fuzzy ART的聚类稳定性，减少对预设参数的依赖，适合非专家用户。


<details>
  <summary>Details</summary>
Motivation: Fuzzy ART的聚类性能高度依赖预设参数，现有方法虽提升鲁棒性但增加了复杂性，IR-ART旨在解决这一问题。

Method: IR-ART包含三个迭代阶段：聚类稳定性检测、不稳定簇删除和警戒区域扩展，动态调整参数。

Result: 在15个数据集上的实验表明，IR-ART提升了对参数值的容忍度，同时保持了算法的简洁性。

Conclusion: IR-ART通过迭代优化提升了聚类性能，特别适合资源受限场景下的非专家用户。

Abstract: The clustering performance of Fuzzy Adaptive Resonance Theory (Fuzzy ART) is
highly dependent on the preset vigilance parameter, where deviations in its
value can lead to significant fluctuations in clustering results, severely
limiting its practicality for non-expert users. Existing approaches generally
enhance vigilance parameter robustness through adaptive mechanisms such as
particle swarm optimization and fuzzy logic rules. However, they often
introduce additional hyperparameters or complex frameworks that contradict the
original simplicity of the algorithm. To address this, we propose Iterative
Refinement Adaptive Resonance Theory (IR-ART), which integrates three key
phases into a unified iterative framework: (1) Cluster Stability Detection: A
dynamic stability detection module that identifies unstable clusters by
analyzing the change of sample size (number of samples in the cluster) in
iteration. (2) Unstable Cluster Deletion: An evolutionary pruning module that
eliminates low-quality clusters. (3) Vigilance Region Expansion: A vigilance
region expansion mechanism that adaptively adjusts similarity thresholds.
Independent of the specific execution of clustering, these three phases
sequentially focus on analyzing the implicit knowledge within the iterative
process, adjusting weights and vigilance parameters, thereby laying a
foundation for the next iteration. Experimental evaluation on 15 datasets
demonstrates that IR-ART improves tolerance to suboptimal vigilance parameter
values while preserving the parameter simplicity of Fuzzy ART. Case studies
visually confirm the algorithm's self-optimization capability through iterative
refinement, making it particularly suitable for non-expert users in
resource-constrained scenarios.

</details>


### [193] [Towards Effectively Leveraging Execution Traces for Program Repair with Code LLMs](https://arxiv.org/abs/2505.04441)
*Mirazul Haque,Petr Babkin,Farima Farmahinifarahani,Manuela Veloso*

Main category: cs.LG

TL;DR: 论文探讨了在自动程序修复（APR）中结合程序执行轨迹对大型语言模型（LLMs）性能的影响，发现其效果有限且随复杂性降低。优化的提示策略能更稳定提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based APR方法主要依赖静态分析，忽略了运行时行为。研究旨在通过加入执行轨迹弥补这一不足。

Method: 在标准APR提示中融入程序执行轨迹，使用GPT模型在三个APR数据集上评估。

Result: 执行轨迹仅对部分配置有有限提升，复杂性增加时效果减弱。优化提示策略表现更稳定。

Conclusion: 执行轨迹可补充LLM推理能力，但需优化提示策略以充分发挥潜力。

Abstract: Large Language Models (LLMs) show promising performance on various
programming tasks, including Automatic Program Repair (APR). However, most
approaches to LLM-based APR are limited to the static analysis of the programs,
while disregarding their runtime behavior. Inspired by knowledge-augmented NLP,
in this work, we aim to remedy this potential blind spot by augmenting standard
APR prompts with program execution traces. We evaluate our approach using the
GPT family of models on three popular APR datasets. Our findings suggest that
simply incorporating execution traces into the prompt provides a limited
performance improvement over trace-free baselines, in only 2 out of 6 tested
dataset / model configurations. We further find that the effectiveness of
execution traces for APR diminishes as their complexity increases. We explore
several strategies for leveraging traces in prompts and demonstrate that
LLM-optimized prompts help outperform trace-free prompts more consistently.
Additionally, we show trace-based prompting to be superior to finetuning a
smaller LLM on a small-scale dataset; and conduct probing studies reinforcing
the notion that execution traces can complement the reasoning abilities of the
LLMs.

</details>


### [194] [A Survey on Temporal Interaction Graph Representation Learning: Progress, Challenges, and Opportunities](https://arxiv.org/abs/2505.04461)
*Pengfei Jiao,Hongjiang Chen,Xuan Guo,Zhidong Zhao,Dongxiao He,Di Jin*

Main category: cs.LG

TL;DR: 本文综述了时序交互图表示学习（TIGRL）的研究现状，提出了一种分类方法，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 时序交互图（TIGs）在动态系统建模中应用广泛，但其表示学习面临独特挑战，需要系统梳理和分类现有方法。

Method: 提出了一种基于信息类型的分类法，总结了现有TIGRL方法，并整理了数据集和基准资源。

Result: 系统分类了TIGRL方法，并提供了数据集和基准，为后续研究奠定基础。

Conclusion: TIGRL领域仍有开放挑战，未来研究可进一步探索其潜力。

Abstract: Temporal interaction graphs (TIGs), defined by sequences of timestamped
interaction events, have become ubiquitous in real-world applications due to
their capability to model complex dynamic system behaviors. As a result,
temporal interaction graph representation learning (TIGRL) has garnered
significant attention in recent years. TIGRL aims to embed nodes in TIGs into
low-dimensional representations that effectively preserve both structural and
temporal information, thereby enhancing the performance of downstream tasks
such as classification, prediction, and clustering within constantly evolving
data environments. In this paper, we begin by introducing the foundational
concepts of TIGs and emphasize the critical role of temporal dependencies. We
then propose a comprehensive taxonomy of state-of-the-art TIGRL methods,
systematically categorizing them based on the types of information utilized
during the learning process to address the unique challenges inherent to TIGs.
To facilitate further research and practical applications, we curate the source
of datasets and benchmarks, providing valuable resources for empirical
investigations. Finally, we examine key open challenges and explore promising
research directions in TIGRL, laying the groundwork for future advancements
that have the potential to shape the evolution of this field.

</details>


### [195] [Discriminative Ordering Through Ensemble Consensus](https://arxiv.org/abs/2505.04464)
*Louis Ohl,Fredrik Lindsten*

Main category: cs.LG

TL;DR: 本文提出了一种基于共识聚类的评分方法，用于评估聚类模型的性能，特别是在处理多样化的聚类定义和约束条件时表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有聚类评估指标难以处理多样化聚类定义和约束条件，因此需要一种更通用的评估方法。

Method: 通过构建基于共识矩阵和聚类模型连通性距离的判别性排序，提出了一种新的评分方法。

Result: 实验验证表明，该方法在合成场景和实际应用中均优于其他评分方法。

Conclusion: 该方法能够有效评估多样化聚类模型，并兼容约束条件，具有广泛适用性。

Abstract: Evaluating the performance of clustering models is a challenging task where
the outcome depends on the definition of what constitutes a cluster. Due to
this design, current existing metrics rarely handle multiple clustering models
with diverse cluster definitions, nor do they comply with the integration of
constraints when available. In this work, we take inspiration from consensus
clustering and assume that a set of clustering models is able to uncover hidden
structures in the data. We propose to construct a discriminative ordering
through ensemble clustering based on the distance between the connectivity of a
clustering model and the consensus matrix. We first validate the proposed
method with synthetic scenarios, highlighting that the proposed score ranks the
models that best match the consensus first. We then show that this simple
ranking score significantly outperforms other scoring methods when comparing
sets of different clustering algorithms that are not restricted to a fixed
number of clusters and is compatible with clustering constraints.

</details>


### [196] [Spectral and Temporal Denoising for Differentially Private Optimization](https://arxiv.org/abs/2505.04468)
*Hyeju Shin,Kyudan Jung,Seongwon Yun,Juyoung Yun*

Main category: cs.LG

TL;DR: FFTKF是一种差分隐私优化方法，通过频域噪声整形和卡尔曼滤波提升梯度质量，同时保持隐私保证。


<details>
  <summary>Details</summary>
Motivation: 解决DP-SGD中噪声导致模型性能下降的问题。

Method: 结合频域噪声整形和卡尔曼滤波，使用高频整形掩码和标量增益卡尔曼滤波器。

Result: 在多个数据集上测试精度优于DP-SGD和DiSK，复杂度为O(d log d)。

Conclusion: FFTKF在保持隐私的同时，通过减少噪声和控制偏差实现了更好的隐私-效用权衡。

Abstract: This paper introduces the FFT-Enhanced Kalman Filter (FFTKF), a
differentially private optimization method that addresses the challenge of
preserving performance in DP-SGD, where added noise typically degrades model
utility. FFTKF integrates frequency-domain noise shaping with Kalman filtering
to enhance gradient quality while preserving $(\varepsilon, \delta)$-DP
guarantees. It employs a high-frequency shaping mask in the Fourier domain to
concentrate differential privacy noise in less informative spectral components,
preserving low-frequency gradient signals. A scalar-gain Kalman filter with
finite-difference Hessian approximation further refines the denoised gradients.
With a per-iteration complexity of $\mathcal{O}(d \log d)$, FFTKF demonstrates
improved test accuracy over DP-SGD and DiSK across MNIST, CIFAR-10, CIFAR-100,
and Tiny-ImageNet datasets using CNNs, Wide ResNets, and Vision Transformers.
Theoretical analysis confirms that FFTKF maintains equivalent privacy
guarantees while achieving a tighter privacy-utility trade-off through reduced
noise and controlled bias.

</details>


### [197] [Hamiltonian Normalizing Flows as kinetic PDE solvers: application to the 1D Vlasov-Poisson Equations](https://arxiv.org/abs/2505.04471)
*Vincent Souveton,Sébastien Terrana*

Main category: cs.LG

TL;DR: 提出了一种基于哈密顿动力学的新型归一化流方法，用于快速采样相空间分布并学习可解释的物理势。


<details>
  <summary>Details</summary>
Motivation: 由于哈密顿系统（如Vlasov-Poisson方程）的复杂性，解析解罕见，需要高效数值方法。

Method: 使用哈密顿归一化流（Fixed-Kinetic Neural Hamiltonian Flows），通过可逆、保体积变换将初始高斯分布转为最终分布。

Result: 模型能快速采样最终分布，并学习物理势，泛化至未见的中间状态。

Conclusion: 该方法为复杂哈密顿系统提供了高效且可解释的数值解决方案。

Abstract: Many conservative physical systems can be described using the Hamiltonian
formalism. A notable example is the Vlasov-Poisson equations, a set of partial
differential equations that govern the time evolution of a phase-space density
function representing collisionless particles under a self-consistent
potential. These equations play a central role in both plasma physics and
cosmology. Due to the complexity of the potential involved, analytical
solutions are rarely available, necessitating the use of numerical methods such
as Particle-In-Cell. In this work, we introduce a novel approach based on
Hamiltonian-informed Normalizing Flows, specifically a variant of Fixed-Kinetic
Neural Hamiltonian Flows. Our method transforms an initial Gaussian
distribution in phase space into the final distribution using a sequence of
invertible, volume-preserving transformations derived from Hamiltonian
dynamics. The model is trained on a dataset comprising initial and final states
at a fixed time T, generated via numerical simulations. After training, the
model enables fast sampling of the final distribution from any given initial
state. Moreover, by automatically learning an interpretable physical potential,
it can generalize to intermediate states not seen during training, offering
insights into the system's evolution across time.

</details>


### [198] [Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation](https://arxiv.org/abs/2505.04619)
*Abdulaziz Almuzairee,Rohan Patil,Dwait Bhatt,Henrik I. Christensen*

Main category: cs.LG

TL;DR: 提出了一种Merge And Disentanglement (MAD)算法，通过高效合并多视角数据提升样本效率，同时结合单视角特征实现轻量部署和鲁棒策略。


<details>
  <summary>Details</summary>
Motivation: 多摄像头视觉伺服在机器人操作中具有鲁棒性，但计算复杂且部署成本高。

Method: 使用MAD算法合并多视角数据并优化样本效率，同时结合单视角特征。

Result: 在Meta-World和ManiSkill3上验证了算法的效率和鲁棒性。

Conclusion: MAD算法在提升样本效率和轻量部署方面表现优异。

Abstract: Vision is well-known for its use in manipulation, especially using visual
servoing. To make it robust, multiple cameras are needed to expand the field of
view. That is computationally challenging. Merging multiple views and using
Q-learning allows the design of more effective representations and optimization
of sample efficiency. Such a solution might be expensive to deploy. To mitigate
this, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently
merges views to increase sample efficiency while augmenting with single-view
features to allow lightweight deployment and ensure robust policies. We
demonstrate the efficiency and robustness of our approach using Meta-World and
ManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad

</details>


### [199] [Communication-Efficient Federated Fine-Tuning of Language Models via Dynamic Update Schedules](https://arxiv.org/abs/2505.04535)
*Michail Theologitis,Vasilis Samoladas,Antonios Deligiannakis*

Main category: cs.LG

TL;DR: FDA-Opt算法家族统一了FDA和FedOpt的优点，解决了它们的核心限制，在联邦学习中表现优于FedOpt。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）中，预训练语言模型（LM）的微调是一个重要但存在通信效率问题的领域。现有算法如FedOpt和FDA各有局限性。

Method: 提出FDA-Opt算法家族，结合FDA的动态同步和FedOpt的优化原则，无需额外配置即可提升性能。

Result: 在多个NLP下游任务中，FDA-Opt表现优于FedOpt，即使使用对手优化的超参数。

Conclusion: FDA-Opt是FedOpt的实用替代方案，无需调整即可直接提升性能。

Abstract: Federated learning (FL) makes it possible to train models on data that would
otherwise remain untapped and inaccessible. Simultaneously, pre-trained
language models (LMs) have emerged as indispensable tools in modern workflows.
These models exhibit extraordinary capabilities and are easily adapted to
downstream tasks. This opens one of the most exciting frontiers in FL:
fine-tuning LMs. However, a persistent challenge in FL is the frequent, rigid
communication of parameters, a problem which is magnified by the sheer size of
these modern models. Currently, the FedOpt family of algorithms is the
prevailing approach in FL, though it relies on fixed, heuristic intervals for
model synchronization. Recently, the FDA algorithm introduced a dynamic
alternative by monitoring training progress, but it came with its own
drawbacks; namely, a hard-to-tune threshold parameter and a rigid
synchronization scheme. In this work, we introduce the FDA-Opt family of
algorithms -- a unified generalization that extends the principles behind both
FDA and FedOpt, while resolving their core limitations. We evaluate our
approach on fine-tuning LMs across a range of downstream NLP tasks, and
demonstrate that it consistently outperforms FedOpt -- even when FDA-Opt
operates under hyper-parameter settings originally optimized for its
competitors. In other words, we show that FDA-Opt is a practical, drop-in
replacement for FedOpt in modern FL libraries and systems: it requires no
additional configuration and delivers superior performance out of the box.

</details>


### [200] [Purity Law for Generalizable Neural TSP Solvers](https://arxiv.org/abs/2505.04558)
*Wenzhao Liu,Haoran Li,Congying Han,Zicheng Zhang,Anqi Li,Tiande Guo*

Main category: cs.LG

TL;DR: 本文揭示了TSP最优解的结构原则Purity Law（PuLa），并提出PUPO训练范式，显著提升神经求解器的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在不同规模和分布的TSP问题中泛化能力不足的挑战。

Method: 发现PuLa原则，并基于此提出PUPO训练范式，优化神经求解器的解决方案。

Result: PUPO显著提升神经求解器的泛化性能，且不增加推理时的计算开销。

Conclusion: PuLa和PUPO为TSP问题的神经求解提供了新的理论基础和实用方法。

Abstract: Achieving generalization in neural approaches across different scales and
distributions remains a significant challenge for the Traveling Salesman
Problem~(TSP). A key obstacle is that neural networks often fail to learn
robust principles for identifying universal patterns and deriving optimal
solutions from diverse instances. In this paper, we first uncover Purity Law
(PuLa), a fundamental structural principle for optimal TSP solutions, defining
that edge prevalence grows exponentially with the sparsity of surrounding
vertices. Statistically validated across diverse instances, PuLa reveals a
consistent bias toward local sparsity in global optima. Building on this
insight, we propose Purity Policy Optimization~(PUPO), a novel training
paradigm that explicitly aligns characteristics of neural solutions with PuLa
during the solution construction process to enhance generalization. Extensive
experiments demonstrate that PUPO can be seamlessly integrated with popular
neural solvers, significantly enhancing their generalization performance
without incurring additional computational overhead during inference.

</details>


### [201] [ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via $α$-$β$-Divergence](https://arxiv.org/abs/2505.04560)
*Guanghui Wang,Zhiyong Yang,Zitai Wang,Shi Wang,Qianqian Xu,Qingming Huang*

Main category: cs.LG

TL;DR: 论文提出ABKD框架，通过α-β散度平衡知识蒸馏中的Hardness-Concentration和Confidence-Concentration效应，优于传统的FKLD和RKLD方法。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法（FKLD和RKLD）在平衡Hardness-Concentration和Confidence-Concentration效应上存在极端问题，导致性能不佳。

Method: 提出ABKD框架，使用α-β散度平滑插值FKLD和RKLD，实现两种效应的平衡。

Result: 在17个语言/视觉数据集和12种师生模型设置下验证了ABKD的有效性。

Conclusion: ABKD通过平衡两种效应，显著提升了知识蒸馏的效果。

Abstract: Knowledge Distillation (KD) transfers knowledge from a large teacher model to
a smaller student model by minimizing the divergence between their output
distributions, typically using forward Kullback-Leibler divergence (FKLD) or
reverse KLD (RKLD). It has become an effective training paradigm due to the
broader supervision information provided by the teacher distribution compared
to one-hot labels. We identify that the core challenge in KD lies in balancing
two mode-concentration effects: the \textbf{\textit{Hardness-Concentration}}
effect, which refers to focusing on modes with large errors, and the
\textbf{\textit{Confidence-Concentration}} effect, which refers to focusing on
modes with high student confidence. Through an analysis of how probabilities
are reassigned during gradient updates, we observe that these two effects are
entangled in FKLD and RKLD, but in extreme forms. Specifically, both are too
weak in FKLD, causing the student to fail to concentrate on the target class.
In contrast, both are too strong in RKLD, causing the student to overly
emphasize the target class while ignoring the broader distributional
information from the teacher. To address this imbalance, we propose ABKD, a
generic framework with $\alpha$-$\beta$-divergence. Our theoretical results
show that ABKD offers a smooth interpolation between FKLD and RKLD, achieving
an effective trade-off between these effects. Extensive experiments on 17
language/vision datasets with 12 teacher-student settings confirm its efficacy.
The code is available at https://github.com/ghwang-s/abkd.

</details>


### [202] [Multitask LSTM for Arboviral Outbreak Prediction Using Public Health Data](https://arxiv.org/abs/2505.04566)
*Lucas R. C. Farias,Talita P. Silva,Pedro H. M. Araujo*

Main category: cs.LG

TL;DR: 本文提出了一种基于LSTM的多任务学习方法，用于联合预测巴西累西腓的登革热、基孔肯雅热和寨卡病毒的爆发及病例数。模型利用历史公共卫生数据，同时进行二元分类（爆发检测）和回归（病例预测）任务，并通过滑动窗口策略优化输入长度。结果表明，多任务架构在数据有限的公共卫生场景中具有可行性和优势。


<details>
  <summary>Details</summary>
Motivation: 解决数据有限的公共卫生场景中多病毒爆发预测的挑战，探索统一建模策略的可行性。

Method: 采用LSTM网络进行多任务学习，结合滑动窗口策略和超参数优化，同时执行分类和回归任务。

Result: 较长窗口提升登革热回归准确性，中等窗口分类性能最佳，多任务架构在疾病和任务中表现优异。

Conclusion: 多任务学习方法在流行病预测中具有可行性和优势，适用于数据有限的公共卫生场景。

Abstract: This paper presents a multitask learning approach based on long-short-term
memory (LSTM) networks for the joint prediction of arboviral outbreaks and case
counts of dengue, chikungunya, and Zika in Recife, Brazil. Leveraging
historical public health data from DataSUS (2017-2023), the proposed model
concurrently performs binary classification (outbreak detection) and regression
(case forecasting) tasks. A sliding window strategy was adopted to construct
temporal features using varying input lengths (60, 90, and 120 days), with
hyperparameter optimization carried out using Keras Tuner. Model evaluation
used time series cross-validation for robustness and a held-out test from 2023
for generalization assessment. The results show that longer windows improve
dengue regression accuracy, while classification performance peaked at
intermediate windows, suggesting an optimal trade-off between sequence length
and generalization. The multitask architecture delivers competitive performance
across diseases and tasks, demonstrating the feasibility and advantages of
unified modeling strategies for scalable epidemic forecasting in data-limited
public health scenarios.

</details>


### [203] [Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization](https://arxiv.org/abs/2505.04578)
*Wenjun Cao*

Main category: cs.LG

TL;DR: 论文研究了RL微调对大型语言模型安全性的威胁，提出了一种名为Reward Neutralization的防御框架，有效抵御恶意RL攻击。


<details>
  <summary>Details</summary>
Motivation: RL微调可能破坏模型的安全防护，现有防御方法对动态反馈机制无效，需针对性解决方案。

Method: 提出Reward Neutralization框架，通过训练模型生成最小信息拒绝响应，使恶意奖励信号失效。

Result: 实验显示，该方法在200次攻击步骤后仍保持低有害分数（≤2），而标准模型迅速恶化。

Conclusion: 研究首次证明针对RL攻击的鲁棒防御可行，填补了开源模型安全漏洞。

Abstract: Reinforcement learning (RL) fine-tuning transforms large language models
while creating a vulnerability we experimentally verify: Our experiment shows
that malicious RL fine-tuning dismantles safety guardrails with remarkable
efficiency, requiring only 50 steps and minimal adversarial prompts, with
harmful escalating from 0-2 to 7-9. This attack vector particularly threatens
open-source models with parameter-level access. Existing defenses targeting
supervised fine-tuning prove ineffective against RL's dynamic feedback
mechanisms. We introduce Reward Neutralization, the first defense framework
specifically designed against RL fine-tuning attacks, establishing concise
rejection patterns that render malicious reward signals ineffective. Our
approach trains models to produce minimal-information rejections that attackers
cannot exploit, systematically neutralizing attempts to optimize toward harmful
outputs. Experiments validate that our approach maintains low harmful scores
(no greater than 2) after 200 attack steps, while standard models rapidly
deteriorate. This work provides the first constructive proof that robust
defense against increasingly accessible RL attacks is achievable, addressing a
critical security gap for open-weight models.

</details>


### [204] [Complexity Lower Bounds of Adaptive Gradient Algorithms for Non-convex Stochastic Optimization under Relaxed Smoothness](https://arxiv.org/abs/2505.04599)
*Michael Crawshaw,Mingrui Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent results in non-convex stochastic optimization demonstrate the
convergence of popular adaptive algorithms (e.g., AdaGrad) under the $(L_0,
L_1)$-smoothness condition, but the rate of convergence is a higher-order
polynomial in terms of problem parameters like the smoothness constants. The
complexity guaranteed by such algorithms to find an $\epsilon$-stationary point
may be significantly larger than the optimal complexity of $\Theta \left(
\Delta L \sigma^2 \epsilon^{-4} \right)$ achieved by SGD in the $L$-smooth
setting, where $\Delta$ is the initial optimality gap, $\sigma^2$ is the
variance of stochastic gradient. However, it is currently not known whether
these higher-order dependencies can be tightened. To answer this question, we
investigate complexity lower bounds for several adaptive optimization
algorithms in the $(L_0, L_1)$-smooth setting, with a focus on the dependence
in terms of problem parameters $\Delta, L_0, L_1$. We provide complexity bounds
for three variations of AdaGrad, which show at least a quadratic dependence on
problem parameters $\Delta, L_0, L_1$. Notably, we show that the decorrelated
variant of AdaGrad-Norm requires at least $\Omega \left( \Delta^2 L_1^2
\sigma^2 \epsilon^{-4} \right)$ stochastic gradient queries to find an
$\epsilon$-stationary point. We also provide a lower bound for SGD with a broad
class of adaptive stepsizes. Our results show that, for certain adaptive
algorithms, the $(L_0, L_1)$-smooth setting is fundamentally more difficult
than the standard smooth setting, in terms of the initial optimality gap and
the smoothness constants.

</details>


### [205] [Testing Juntas Optimally with Samples](https://arxiv.org/abs/2505.04604)
*Lorenzo Beretta,Nathaniel Harms,Caleb Koch*

Main category: cs.LG

TL;DR: 论文证明了在无分布样本模型中，k-junta测试的样本数量上下界为Θ(1/ϵ(√(2^k logC(n,k)) + logC(n,k)))，并首次为该模型中的布尔函数自然类提供了紧界。


<details>
  <summary>Details</summary>
Motivation: 研究k-junta测试在无分布样本模型中的样本复杂度，填补了该领域的研究空白。

Method: 通过理论分析，推导出样本数量的上下界，并扩展到特征选择问题和容忍性测试。

Result: 证明了样本数量的紧界，并发现容忍性测试与学习之间不存在显著差距。

Conclusion: 该研究为k-junta测试提供了理论支持，并揭示了容忍性测试的复杂性。

Abstract: We prove tight upper and lower bounds of
$\Theta\left(\tfrac{1}{\epsilon}\left( \sqrt{2^k \log\binom{n}{k} } +
\log\binom{n}{k} \right)\right)$ on the number of samples required for
distribution-free $k$-junta testing. This is the first tight bound for testing
a natural class of Boolean functions in the distribution-free sample-based
model. Our bounds also hold for the feature selection problem, showing that a
junta tester must learn the set of relevant variables. For tolerant junta
testing, we prove a sample lower bound of $\Omega(2^{(1-o(1)) k} +
\log\binom{n}{k})$ showing that, unlike standard testing, there is no large gap
between tolerant testing and learning.

</details>


### [206] [WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via Weighted-Conformal Martingales](https://arxiv.org/abs/2505.04608)
*Drew Prinster,Xing Han,Anqi Liu,Suchi Saria*

Main category: cs.LG

TL;DR: 论文提出了一种加权广义共形测试鞅（WCTMs），用于在线监测数据分布中的异常变化点，同时控制误报率。


<details>
  <summary>Details</summary>
Motivation: 在高风险环境中，AI/ML系统不仅需要可靠性证明，还需持续监测以确保安全性。现有方法局限于监测有限假设类别或无法在线适应数据变化。

Method: 提出加权广义共形测试鞅（WCTMs），支持在线监测任何数据分布中的异常变化点，并设计具体算法以适应轻度协变量偏移。

Result: 在真实数据集上，WCTMs表现优于现有基线方法。

Conclusion: WCTMs为AI/ML系统的安全监测提供了更灵活和强大的工具。

Abstract: Responsibly deploying artificial intelligence (AI) / machine learning (ML)
systems in high-stakes settings arguably requires not only proof of system
reliability, but moreover continual, post-deployment monitoring to quickly
detect and address any unsafe behavior. Statistical methods for nonparametric
change-point detection -- especially the tools of conformal test martingales
(CTMs) and anytime-valid inference -- offer promising approaches to this
monitoring task. However, existing methods are restricted to monitoring limited
hypothesis classes or ``alarm criteria,'' such as data shifts that violate
certain exchangeability assumptions, or do not allow for online adaptation in
response to shifts. In this paper, we expand the scope of these monitoring
methods by proposing a weighted generalization of conformal test martingales
(WCTMs), which lay a theoretical foundation for online monitoring for any
unexpected changepoints in the data distribution while controlling
false-alarms. For practical applications, we propose specific WCTM algorithms
that accommodate online adaptation to mild covariate shifts (in the marginal
input distribution) while raising alarms in response to more severe shifts,
such as concept shifts (in the conditional label distribution) or extreme
(out-of-support) covariate shifts that cannot be easily adapted to. On
real-world datasets, we demonstrate improved performance relative to
state-of-the-art baselines.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [207] [Promoting Security and Trust on Social Networks: Explainable Cyberbullying Detection Using Large Language Models in a Stream-Based Machine Learning Framework](https://arxiv.org/abs/2505.03746)
*Silvia García-Méndez,Francisco De Arriba-Pérez*

Main category: cs.SI

TL;DR: 提出了一种基于流式机器学习和大型语言模型的实时网络欺凌检测方案，性能接近90%，并提供了可解释性仪表板。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台虽然促进了社交互动，但也带来了网络欺凌等负面行为。现有生成式AI研究多集中于零/少样本学习，仍有探索空间。

Method: 结合流式机器学习模型（增量处理数据）和大型语言模型（特征工程），以应对网络欺凌的动态性，并提供可解释性仪表板。

Result: 实验数据显示性能接近90%，优于现有文献中的竞争方法。

Conclusion: 该方案能及时检测网络欺凌行为，提升在线社区安全性，减少社会负面影响。

Abstract: Social media platforms enable instant and ubiquitous connectivity and are
essential to social interaction and communication in our technological society.
Apart from its advantages, these platforms have given rise to negative
behaviors in the online community, the so-called cyberbullying. Despite the
many works involving generative Artificial Intelligence (AI) in the literature
lately, there remain opportunities to study its performance apart from
zero/few-shot learning strategies. Accordingly, we propose an innovative and
real-time solution for cyberbullying detection that leverages stream-based
Machine Learning (ML) models able to process the incoming samples incrementally
and Large Language Models (LLMS) for feature engineering to address the
evolving nature of abusive and hate speech online. An explainability dashboard
is provided to promote the system's trustworthiness, reliability, and
accountability. Results on experimental data report promising performance close
to 90 % in all evaluation metrics and surpassing those obtained by competing
works in the literature. Ultimately, our proposal contributes to the safety of
online communities by timely detecting abusive behavior to prevent long-lasting
harassment and reduce the negative consequences in society.

</details>


### [208] [The Influence of Text Variation on User Engagement in Cross-Platform Content Sharing](https://arxiv.org/abs/2505.03769)
*Yibo Hu,Yiqiao Jin,Meng Ye,Ajay Divakaran,Srijan Kumar*

Main category: cs.SI

TL;DR: 研究探讨了改写Reddit帖子标题对用户参与度的影响，发现改写能显著提升参与度，并通过实验验证了情感共鸣、词汇丰富度和社区规范对齐是关键因素。


<details>
  <summary>Details</summary>
Motivation: 在跨平台社交媒体中，理解多模态内容（尤其是文本与视觉结合）如何驱动用户参与度仍具挑战性。

Method: 构建并分析Reddit帖子数据集，设计多阶段实验以隔离文本变化的影响，并使用BERT分类器进行预测实验。

Result: 改写标题显著提升参与度，情感共鸣、词汇丰富度和社区规范对齐是关键因素；BERT模型预测准确率达74%。

Conclusion: 研究揭示了标题改写对参与度的影响机制，为跨平台多模态内容策略提供了框架。

Abstract: In today's cross-platform social media landscape, understanding factors that
drive engagement for multimodal content, especially text paired with visuals,
remains complex. This study investigates how rewriting Reddit post titles
adapted from YouTube video titles affects user engagement. First, we build and
analyze a large dataset of Reddit posts sharing YouTube videos, revealing that
21% of post titles are minimally modified. Statistical analysis demonstrates
that title rewrites measurably improve engagement. Second, we design a
controlled, multi-phase experiment to rigorously isolate the effects of textual
variations by neutralizing confounding factors like video popularity, timing,
and community norms. Comprehensive statistical tests reveal that effective
title rewrites tend to feature emotional resonance, lexical richness, and
alignment with community-specific norms. Lastly, pairwise ranking prediction
experiments using a fine-tuned BERT classifier achieves 74% accuracy,
significantly outperforming near-random baselines, including GPT-4o. These
results validate that our controlled dataset effectively minimizes confounding
effects, allowing advanced models to both learn and demonstrate the impact of
textual features on engagement. By bridging quantitative rigor with qualitative
insights, this study uncovers engagement dynamics and offers a robust framework
for future cross-platform, multimodal content strategies.

</details>


### [209] [Modeling Human Behavior in a Strategic Network Game with Complex Group Dynamics](https://arxiv.org/abs/2505.03795)
*Jacob W. Crandall,Jonathan Skaggs*

Main category: cs.SI

TL;DR: 论文比较了多种建模方法在战略网络游戏中的表现，发现hCAB方法最能模拟人类行为。


<details>
  <summary>Details</summary>
Motivation: 理解人类网络对社会结果的影响，如不平等和贫困。

Method: 比较不同建模方法，包括行为假设和统计建模方式。

Result: hCAB方法表现最佳，能模拟人类群体动态且难以被区分。

Conclusion: hCAB方法在战略网络游戏中有效模拟人类行为。

Abstract: Human networks greatly impact important societal outcomes, including wealth
and health inequality, poverty, and bullying. As such, understanding human
networks is critical to learning how to promote favorable societal outcomes. As
a step toward better understanding human networks, we compare and contrast
several methods for learning models of human behavior in a strategic network
game called the Junior High Game (JHG). These modeling methods differ with
respect to the assumptions they use to parameterize human behavior (behavior
vs. community-aware behavior) and the statistical moments they model (mean vs.
distribution). Results show that the highest-performing method models the
population's distribution rather than the mean and assumes humans use
community-aware behavior rather than behavior matching. When applied to small
societies (6-11 individuals), this learned model, called hCAB, closely mirrors
the population dynamics of human groups (with some differences). Additionally,
a user study reveals that human participants were unable to distinguish hCAB
agents from other humans, thus illustrating that individual hCAB behavior
plausibly mirrors human behavior in this strategic network game.

</details>


### [210] [Geospatial and Temporal Trends in Urban Transportation: A Study of NYC Taxis and Pathao Food Deliveries](https://arxiv.org/abs/2505.03816)
*Bidyarthi Paul,Fariha Tasnim Chowdhury,Dipta Biswas,Meherin Sultana*

Main category: cs.SI

TL;DR: 该研究通过分析纽约和达卡的数据集，探索城市交通需求模式，结合EDA、地理空间分析、SARIMAX模型和聚类技术，为优化车队管理和资源分配提供见解。


<details>
  <summary>Details</summary>
Motivation: 研究城市交通模式以提升效率和满足需求，特别是在乘客运输和食品配送服务中。

Method: 使用EDA、地理空间分析、SARIMAX时间序列模型和聚类技术分析数据集。

Result: 识别了需求高峰、地理热点和季节性变化，为优化资源分配提供了依据。

Conclusion: 研究结果为提升服务效率和优化城市交通系统提供了实用见解。

Abstract: Urban transportation plays a vital role in modern city life, affecting how
efficiently people and goods move around. This study analyzes transportation
patterns using two datasets: the NYC Taxi Trip dataset from New York City and
the Pathao Food Trip dataset from Dhaka, Bangladesh. Our goal is to identify
key trends in demand, peak times, and important geographical hotspots. We start
with Exploratory Data Analysis (EDA) to understand the basic characteristics of
the datasets. Next, we perform geospatial analysis to map out high-demand and
low-demand regions. We use the SARIMAX model for time series analysis to
forecast demand patterns, capturing seasonal and weekly variations. Lastly, we
apply clustering techniques to identify significant areas of high and low
demand. Our findings provide valuable insights for optimizing fleet management
and resource allocation in both passenger transport and food delivery services.
These insights can help improve service efficiency, better meet customer needs,
and enhance urban transportation systems in diverse urban environments.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [211] [A Graphical Global Optimization Framework for Parameter Estimation of Statistical Models with Nonconvex Regularization Functions](https://arxiv.org/abs/2505.03899)
*Danial Davarnia,Mohammadreza Kiaghadi*

Main category: math.OC

TL;DR: 提出了一种基于图的新方法，用于全局解决涉及广义范数约束的优化问题，适用于标准ℓ_p范数和非凸惩罚函数。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理范数约束优化问题时，通常需要引入辅助变量或依赖特定结构，难以通用化。

Method: 利用决策图在原始变量空间中构建强凸松弛，结合空间分支切割框架，确保全局最优解。

Result: 在涉及复杂非凸惩罚的稀疏线性回归问题上，方法表现优于现有全局优化技术。

Conclusion: 新方法无需辅助变量，通用性强，能有效解决现有技术难以处理的非凸优化问题。

Abstract: Optimization problems with norm-bounding constraints arise in a variety of
applications, including portfolio optimization, machine learning, and feature
selection. A common approach to these problems involves relaxing the norm
constraint via Lagrangian relaxation, transforming it into a regularization
term in the objective function. A particularly challenging class includes the
zero-norm function, which promotes sparsity in statistical parameter
estimation. Most existing exact methods for solving these problems introduce
binary variables and artificial bounds to reformulate them as
higher-dimensional mixed-integer programs, solvable by standard solvers. Other
exact approaches exploit specific structural properties of the objective,
making them difficult to generalize across different problem types. Alternative
methods employ nonconvex penalties with favorable statistical characteristics,
but these are typically addressed using heuristic or local optimization
techniques due to their structural complexity. In this paper, we propose a
novel graph-based method to globally solve optimization problems involving
generalized norm-bounding constraints. Our approach encompasses standard
$\ell_p$-norms for $p \in [0, \infty)$ and nonconvex penalties such as SCAD and
MCP. We leverage decision diagrams to construct strong convex relaxations
directly in the original variable space, eliminating the need for auxiliary
variables or artificial bounds. Integrated into a spatial branch-and-cut
framework, our method guarantees convergence to the global optimum. We
demonstrate its effectiveness through preliminary computational experiments on
benchmark sparse linear regression problems involving complex nonconvex
penalties, which are not tractable using existing global optimization
techniques.

</details>


### [212] [Dynamic Network Flow Optimization for Task Scheduling in PTZ Camera Surveillance Systems](https://arxiv.org/abs/2505.04596)
*Mohammad Merati,David Castañón*

Main category: math.OC

TL;DR: 提出了一种优化PTZ相机调度与控制的新方法，结合卡尔曼滤波和动态网络流模型，提升实时视频捕捉效率。


<details>
  <summary>Details</summary>
Motivation: 动态监控环境中，传统主从相机系统效率低，需优化调度以减少冗余监控和提升关键事件捕捉。

Method: 集成卡尔曼滤波预测目标位置，结合网络流优化调度，引入群组跟踪节点和价值系统优先处理关键任务。

Result: 仿真显示，该方法在覆盖率、等待时间和事件遗漏方面优于传统系统。

Conclusion: 显著提升了监控系统的效率、可扩展性和动态环境适应性。

Abstract: This paper presents a novel approach for optimizing the scheduling and
control of Pan-Tilt-Zoom (PTZ) cameras in dynamic surveillance environments.
The proposed method integrates Kalman filters for motion prediction with a
dynamic network flow model to enhance real-time video capture efficiency. By
assigning Kalman filters to tracked objects, the system predicts future
locations, enabling precise scheduling of camera tasks. This prediction-driven
approach is formulated as a network flow optimization, ensuring scalability and
adaptability to various surveillance scenarios. To further reduce redundant
monitoring, we also incorporate group-tracking nodes, allowing multiple objects
to be captured within a single camera focus when appropriate. In addition, a
value-based system is introduced to prioritize camera actions, focusing on the
timely capture of critical events. By adjusting the decay rates of these values
over time, the system ensures prompt responses to tasks with imminent
deadlines. Extensive simulations demonstrate that this approach improves
coverage, reduces average wait times, and minimizes missed events compared to
traditional master-slave camera systems. Overall, our method significantly
enhances the efficiency, scalability, and effectiveness of surveillance
systems, particularly in dynamic and crowded environments.

</details>


### [213] [Optimization Problem Solving Can Transition to Evolutionary Agentic Workflows](https://arxiv.org/abs/2505.04354)
*Wenhao Li,Bo Jin,Mingyi Hong,Changhong Lu,Xiangfeng Wang*

Main category: math.OC

TL;DR: 论文提出从依赖专家的优化问题解决转向基于进化代理的工作流，通过基础模型和进化搜索自主导航优化空间。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法依赖人类专家，导致工业应用瓶颈，阻碍前沿方法的采用。

Method: 采用进化代理工作流，结合基础模型和进化搜索，自主处理问题、算法和超参数空间。

Result: 通过云资源调度和ADMM参数适应的案例研究，验证了方法的可行性。

Conclusion: 挑战以人为中心的优化工作流，提倡更可扩展、自适应的现实问题解决方法。

Abstract: This position paper argues that optimization problem solving can transition
from expert-dependent to evolutionary agentic workflows. Traditional
optimization practices rely on human specialists for problem formulation,
algorithm selection, and hyperparameter tuning, creating bottlenecks that
impede industrial adoption of cutting-edge methods. We contend that an
evolutionary agentic workflow, powered by foundation models and evolutionary
search, can autonomously navigate the optimization space, comprising problem,
formulation, algorithm, and hyperparameter spaces. Through case studies in
cloud resource scheduling and ADMM parameter adaptation, we demonstrate how
this approach can bridge the gap between academic innovation and industrial
implementation. Our position challenges the status quo of human-centric
optimization workflows and advocates for a more scalable, adaptive approach to
solving real-world optimization problems.

</details>


### [214] [Learning based convex approximation for constrained parametric optimization](https://arxiv.org/abs/2505.04037)
*Kang Liu,Wei Peng,Jianchen Hu*

Main category: math.OC

TL;DR: 提出了一种基于输入凸神经网络（ICNN）的自监督学习框架，用于解决连续约束优化问题，结合增广拉格朗日方法和约束修正机制，确保非严格约束可行性、更优的最优性差距和最佳收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有学习型方法在连续约束优化问题中的表现有限，需要一种能平衡准确性、可行性和计算效率的新方法。

Method: 结合增广拉格朗日方法（ALM）和约束修正机制，利用ICNN进行自监督学习。

Result: 在二次规划、非凸规划和大规模交流最优潮流问题中，优于现有求解器和最新学习型方法。

Conclusion: 该框架在准确性、可行性和计算效率上实现了优越的平衡，并提供了严格的收敛性分析。

Abstract: We propose an input convex neural network (ICNN)-based self-supervised
learning framework to solve continuous constrained optimization problems. By
integrating the augmented Lagrangian method (ALM) with the constraint
correction mechanism, our framework ensures \emph{non-strict constraint
feasibility}, \emph{better optimality gap}, and \emph{best convergence rate}
with respect to the state-of-the-art learning-based methods. We provide a
rigorous convergence analysis, showing that the algorithm converges to a
Karush-Kuhn-Tucker (KKT) point of the original problem even when the internal
solver is a neural network, and the approximation error is bounded. We test our
approach on a range of benchmark tasks including quadratic programming (QP),
nonconvex programming, and large-scale AC optimal power flow problems. The
results demonstrate that compared to existing solvers (e.g., \texttt{OSQP},
\texttt{IPOPT}) and the latest learning-based methods (e.g., DC3, PDL), our
approach achieves a superior balance among accuracy, feasibility, and
computational efficiency.

</details>


### [215] [A Two-Timescale Primal-Dual Framework for Reinforcement Learning via Online Dual Variable Guidance](https://arxiv.org/abs/2505.04494)
*Axel Friedrich Wolter,Tobias Sutter*

Main category: math.OC

TL;DR: PGDA-RL是一种新的原始-对偶投影梯度下降-上升算法，用于解决正则化MDP，结合了经验回放和双时间尺度分解，实现了异步在线策略更新。


<details>
  <summary>Details</summary>
Motivation: 设计一种能够利用离策略数据同时保持在线探索的强化学习算法。

Method: 提出PGDA-RL算法，结合经验回放的梯度估计和双时间尺度分解，异步更新策略。

Result: PGDA-RL几乎必然收敛到正则化MDP的最优值函数和策略。

Conclusion: PGDA-RL在较弱假设下收敛，无需模拟器或固定行为策略，优于现有方法。

Abstract: We study reinforcement learning by combining recent advances in regularized
linear programming formulations with the classical theory of stochastic
approximation. Motivated by the challenge of designing algorithms that leverage
off-policy data while maintaining on-policy exploration, we propose PGDA-RL, a
novel primal-dual Projected Gradient Descent-Ascent algorithm for solving
regularized Markov Decision Processes (MDPs). PGDA-RL integrates experience
replay-based gradient estimation with a two-timescale decomposition of the
underlying nested optimization problem. The algorithm operates asynchronously,
interacts with the environment through a single trajectory of correlated data,
and updates its policy online in response to the dual variable associated with
the occupation measure of the underlying MDP. We prove that PGDA-RL converges
almost surely to the optimal value function and policy of the regularized MDP.
Our convergence analysis relies on tools from stochastic approximation theory
and holds under weaker assumptions than those required by existing primal-dual
RL approaches, notably removing the need for a simulator or a fixed behavioral
policy.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [216] [Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective](https://arxiv.org/abs/2505.03828)
*Yogesh Gajula*

Main category: cs.IR

TL;DR: 本文综述了2023年至2025年初基于自然语言处理的情绪感知推荐系统，探讨了如何通过情感分析提升电商推荐的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 电商平台用户反馈中的文本信息常被忽略，而情感分析能挖掘这些信息以改进推荐系统。

Method: 综述了四种方法：深度学习分类器、基于Transformer的特征提取、图神经网络传播情感信号、以及实时响应的对话推荐系统。

Result: 展示了情感分析如何影响推荐流程，并总结了模型架构。

Conclusion: 指出了当前挑战（如噪声文本和偏见缓解）和研究空白，提出了未来发展方向。

Abstract: E-commerce platforms generate vast volumes of user feedback, such as star
ratings, written reviews, and comments. However, most recommendation engines
rely primarily on numerical scores, often overlooking the nuanced opinions
embedded in free text. This paper comprehensively reviews sentiment-aware
recommendation systems from a natural language processing perspective, covering
advancements from 2023 to early 2025. It highlights the benefits of integrating
sentiment analysis into e-commerce recommenders to enhance prediction accuracy
and explainability through detailed opinion extraction. Our survey categorizes
recent work into four main approaches: deep learning classifiers that combine
sentiment embeddings with user item interactions, transformer based methods for
nuanced feature extraction, graph neural networks that propagate sentiment
signals, and conversational recommenders that adapt in real time to user
feedback. We summarize model architectures and demonstrate how sentiment flows
through recommendation pipelines, impacting dialogue-based suggestions. Key
challenges include handling noisy or sarcastic text, dynamic user preferences,
and bias mitigation. Finally, we outline research gaps and provide a roadmap
for developing smarter, fairer, and more user-centric recommendation tools.

</details>


### [217] [Memory Assisted LLM for Personalized Recommendation System](https://arxiv.org/abs/2505.03824)
*Jiarui Chen*

Main category: cs.IR

TL;DR: MAP（Memory-Assisted Personalized LLM）通过用户历史档案和相似性提取记忆，提升个性化推荐效果，优于传统LLM方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM个性化方法成本高、效率低或无法及时更新用户历史，MAP旨在解决这些问题。

Method: 创建用户历史档案，提取相似记忆并融入提示，增强个性化推荐。

Result: MAP在单领域和跨领域任务中均优于传统LLM推荐方法，且用户历史越长效果越显著。

Conclusion: MAP适合处理连续个性化请求，为LLM个性化推荐提供了高效解决方案。

Abstract: Large language models (LLMs) have demonstrated significant potential in
solving recommendation tasks. With proven capabilities in understanding user
preferences, LLM personalization has emerged as a critical area for providing
tailored responses to individuals. Current studies explore personalization
through prompt design and fine-tuning, paving the way for further research in
personalized LLMs. However, existing approaches are either costly and
inefficient in capturing diverse user preferences or fail to account for timely
updates to user history. To address these gaps, we propose the Memory-Assisted
Personalized LLM (MAP). Through user interactions, we first create a history
profile for each user, capturing their preferences, such as ratings for
historical items. During recommendation, we extract relevant memory based on
similarity, which is then incorporated into the prompts to enhance personalized
recommendations. In our experiments, we evaluate MAP using a sequential rating
prediction task under two scenarios: single domain, where memory and tasks are
from the same category (e.g., movies), and cross-domain (e.g., memory from
movies and recommendation tasks in books). The results show that MAP
outperforms regular LLM-based recommenders that integrate user history directly
through prompt design. Moreover, as user history grows, MAP's advantage
increases in both scenarios, making it more suitable for addressing successive
personalized user requests.

</details>


### [218] [OBD-Finder: Explainable Coarse-to-Fine Text-Centric Oracle Bone Duplicates Discovery](https://arxiv.org/abs/2505.03836)
*Chongsheng Zhang,Shuwen Wu,Yingqi Chen,Matthias Aßenmacher,Christian Heumann,Yi Men,Gaojuan Fan,João Gama*

Main category: cs.IR

TL;DR: 本文提出了一种渐进式甲骨文重复发现框架，结合无监督低层关键点匹配与高层文本中心内容匹配，显著提升了甲骨文重复识别的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 甲骨文是中国最早的系统文字，而甲骨文重复识别是甲骨文研究的基础问题。现有方法在效率和语义解释性上存在不足。

Method: 设计了一个渐进式框架，结合低层关键点匹配和高层文本内容匹配，优化候选重复的排序和语义解释性。

Result: 与现有方法相比，本方法在召回率和简化平均倒数排名上表现优异，计算效率显著提升，并发现了60多对新甲骨文重复。

Conclusion: 该方法在甲骨文重复识别中表现出色，具有实际应用价值，为甲骨文研究提供了新工具。

Abstract: Oracle Bone Inscription (OBI) is the earliest systematic writing system in
China, while the identification of Oracle Bone (OB) duplicates is a fundamental
issue in OBI research. In this work, we design a progressive OB duplicate
discovery framework that combines unsupervised low-level keypoints matching
with high-level text-centric content-based matching to refine and rank the
candidate OB duplicates with semantic awareness and interpretability. We
compare our approach with state-of-the-art content-based image retrieval and
image matching methods, showing that our approach yields comparable recall
performance and the highest simplified mean reciprocal rank scores for both
Top-5 and Top-15 retrieval results, and with significantly accelerated
computation efficiency. We have discovered over 60 pairs of new OB duplicates
in real-world deployment, which were missed by OBI researchers for decades. The
models, video illustration and demonstration of this work are available at:
https://github.com/cszhangLMU/OBD-Finder/.

</details>


### [219] [CoCoB: Adaptive Collaborative Combinatorial Bandits for Online Recommendation](https://arxiv.org/abs/2505.03840)
*Cairong Yan,Jinyi Han,Jin Ju,Yanting Zhang,Zijian Wang,Xuan Shao*

Main category: cs.IR

TL;DR: 本文提出了一种自适应协作组合多臂老虎机算法（CoCoB），通过双面老虎机架构改进推荐系统中的用户相似性识别和推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现有聚类老虎机方法在定义相似用户和处理独特偏好用户时存在不足，导致推荐质量下降。

Method: CoCoB采用双面老虎机架构：用户端老虎机通过贝叶斯模型探索用户相似性，物品端老虎机生成多样化推荐。

Result: 在线性上下文老虎机设置下的遗憾分析和三个真实数据集实验表明，CoCoB的F1分数平均提升2.4%。

Conclusion: CoCoB通过动态适应机制有效提升了推荐系统的性能。

Abstract: Clustering bandits have gained significant attention in recommender systems
by leveraging collaborative information from neighboring users to better
capture target user preferences. However, these methods often lack a clear
definition of similar users and face challenges when users with unique
preferences lack appropriate neighbors. In such cases, relying on divergent
preferences of misidentified neighbors can degrade recommendation quality. To
address these limitations, this paper proposes an adaptive Collaborative
Combinatorial Bandits algorithm (CoCoB). CoCoB employs an innovative two-sided
bandit architecture, applying bandit principles to both the user and item
sides. The user-bandit employs an enhanced Bayesian model to explore user
similarity, identifying neighbors based on a similarity probability threshold.
The item-bandit treats items as arms, generating diverse recommendations
informed by the user-bandit's output. CoCoB dynamically adapts, leveraging
neighbor preferences when available or focusing solely on the target user
otherwise. Regret analysis under a linear contextual bandit setting and
experiments on three real-world datasets demonstrate CoCoB's effectiveness,
achieving an average 2.4% improvement in F1 score over state-of-the-art
methods.

</details>


### [220] [To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase Relevance at eBay](https://arxiv.org/abs/2505.04209)
*Soumik Dey,Hansi Wu,Binbin Li*

Main category: cs.IR

TL;DR: 论文探讨了电商广告关键词相关性的重要性，提出了一种结合卖家判断、广告系统和搜索系统的动态交互模型，并利用LLM作为代理来优化相关性模型。


<details>
  <summary>Details</summary>
Motivation: 广告关键词的相关性对防止搜索系统被无关内容淹没和维持卖家感知至关重要。现有模型依赖点击/销售信号，但需与卖家判断对齐以提高采纳率。

Method: 研究将广告关键词相关性建模为卖家判断、广告系统和搜索系统的动态交互，并通过eBay案例展示如何利用LLM作为代理来训练模型。

Result: 使用LLM作为代理并结合业务指标评估框架，能更好地协调三个系统，提升关键词相关性模型的性能。

Conclusion: 结合卖家判断和LLM代理的方法能有效优化广告关键词相关性，但需严格的业务指标评估框架支持。

Abstract: E-commerce sellers are recommended keyphrases based on their inventory on
which they advertise to increase buyer engagement (clicks/sales). The relevance
of advertiser keyphrases plays an important role in preventing the inundation
of search systems with numerous irrelevant items that compete for attention in
auctions, in addition to maintaining a healthy seller perception. In this work,
we describe the shortcomings of training Advertiser keyphrase relevance filter
models on click/sales/search relevance signals and the importance of aligning
with human judgment, as sellers have the power to adopt or reject said
keyphrase recommendations. In this study, we frame Advertiser keyphrase
relevance as a complex interaction between 3 dynamical systems -- seller
judgment, which influences seller adoption of our product, Advertising, which
provides the keyphrases to bid on, and Search, who holds the auctions for the
same keyphrases. This study discusses the practicalities of using human
judgment via a case study at eBay Advertising and demonstrate that using
LLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our
relevance models achieves a better harmony across the three systems -- provided
that they are bound by a meticulous evaluation framework grounded in business
metrics.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [221] [AI-Driven IRM: Transforming insider risk management with adaptive scoring and LLM-based threat detection](https://arxiv.org/abs/2505.03796)
*Lokesh Koli,Shubham Kalra,Rohan Thakur,Anas Saifi,Karanpreet Singh*

Main category: cs.CR

TL;DR: 本文提出了一种基于AI的内部风险管理（IRM）系统，通过行为分析、动态风险评分和实时策略执行，显著提高了内部威胁检测的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的检测系统难以应对内部威胁的隐蔽性和上下文依赖性，因此需要更智能的解决方案。

Method: 采用混合评分机制，结合静态PRISM模型和基于自动编码器神经网络的AI模型，通过迭代反馈和持续学习优化性能。

Result: 系统将误报率降低59%，真阳性检测率提高30%，每日可处理1000万日志事件，响应时间减少47%。

Conclusion: 该IRM系统为内部风险管理提供了可扩展且主动的框架，未来将结合可解释AI、联邦学习等技术进一步提升性能。

Abstract: Insider threats pose a significant challenge to organizational security,
often evading traditional rule-based detection systems due to their subtlety
and contextual nature. This paper presents an AI-powered Insider Risk
Management (IRM) system that integrates behavioral analytics, dynamic risk
scoring, and real-time policy enforcement to detect and mitigate insider
threats with high accuracy and adaptability. We introduce a hybrid scoring
mechanism - transitioning from the static PRISM model to an adaptive AI-based
model utilizing an autoencoder neural network trained on expert-annotated user
activity data. Through iterative feedback loops and continuous learning, the
system reduces false positives by 59% and improves true positive detection
rates by 30%, demonstrating substantial gains in detection precision.
Additionally, the platform scales efficiently, processing up to 10 million log
events daily with sub-300ms query latency, and supports automated enforcement
actions for policy violations, reducing manual intervention. The IRM system's
deployment resulted in a 47% reduction in incident response times, highlighting
its operational impact. Future enhancements include integrating explainable AI,
federated learning, graph-based anomaly detection, and alignment with Zero
Trust principles to further elevate its adaptability, transparency, and
compliance-readiness. This work establishes a scalable and proactive framework
for mitigating emerging insider risks in both on-premises and hybrid
environments.

</details>


### [222] [Modeling Behavioral Preferences of Cyber Adversaries Using Inverse Reinforcement Learning](https://arxiv.org/abs/2505.03817)
*Aditya Shinde,Prashant Doshi*

Main category: cs.CR

TL;DR: 本文提出了一种基于系统级审计日志的逆向强化学习（IRL）方法，用于建模攻击者的行为偏好，为网络安全中的威胁归因提供了新维度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖不断更新的攻击工具和技术来追踪威胁行为者，但攻击者的行为偏好具有内在性和稳定性，因此需要一种更稳定的建模方法。

Method: 通过攻击溯源图从审计日志中提取攻击的状态-动作轨迹，将攻击者建模为具有未知行为偏好的专家决策代理。

Result: 实验表明，低级别的取证数据可以自动揭示攻击者的主观偏好，这些偏好在不同工具下保持稳定，可作为独特的行为签名。

Conclusion: 攻击者的行为偏好可作为威胁归因的补充维度，提升对已知威胁行为者的识别能力。

Abstract: This paper presents a holistic approach to attacker preference modeling from
system-level audit logs using inverse reinforcement learning (IRL). Adversary
modeling is an important capability in cybersecurity that lets defenders
characterize behaviors of potential attackers, which enables attribution to
known cyber adversary groups. Existing approaches rely on documenting an
ever-evolving set of attacker tools and techniques to track known threat
actors. Although attacks evolve constantly, attacker behavioral preferences are
intrinsic and less volatile. Our approach learns the behavioral preferences of
cyber adversaries from forensics data on their tools and techniques. We model
the attacker as an expert decision-making agent with unknown behavioral
preferences situated in a computer host. We leverage attack provenance graphs
of audit logs to derive a state-action trajectory of the attack. We test our
approach on open datasets of audit logs containing real attack data. Our
results demonstrate for the first time that low-level forensics data can
automatically reveal an adversary's subjective preferences, which serves as an
additional dimension to modeling and documenting cyber adversaries. Attackers'
preferences tend to be invariant despite their different tools and indicate
predispositions that are inherent to the attacker. As such, these inferred
preferences can potentially serve as unique behavioral signatures of attackers
and improve threat attribution.

</details>


### [223] [Impact Analysis of Inference Time Attack of Perception Sensors on Autonomous Vehicles](https://arxiv.org/abs/2505.03850)
*Hanlin Chen,Simin Chen,Wenyu Li,Wei Yang,Yiheng Feng*

Main category: cs.CR

TL;DR: 本文提出了一种基于推理时间攻击的自动驾驶车辆影响分析方法，展示了此类攻击对车辆和交通参与者的安全威胁。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆（AVs）的感知模块易受攻击，现有研究多关注感知正确性，而忽略了推理时间攻击对安全的影响。

Method: 通过仿真系统分析推理时间攻击对AVs的影响。

Result: 研究表明，推理时间攻击不仅威胁自身车辆安全，还会影响其他交通参与者。

Conclusion: 推理时间攻击是AVs安全的重要威胁，需进一步研究和防护。

Abstract: As a safety-critical cyber-physical system, cybersecurity and related safety
issues for Autonomous Vehicles (AVs) have been important research topics for a
while. Among all the modules on AVs, perception is one of the most accessible
attack surfaces, as drivers and AVs have no control over the outside
environment. Most current work targeting perception security for AVs focuses on
perception correctness. In this work, we propose an impact analysis based on
inference time attacks for autonomous vehicles. We demonstrate in a simulation
system that such inference time attacks can also threaten the safety of both
the ego vehicle and other traffic participants.

</details>


### [224] [Data-Driven Falsification of Cyber-Physical Systems](https://arxiv.org/abs/2505.03863)
*Atanu Kundu,Sauvik Gon,Rajarshi Ray*

Main category: cs.CR

TL;DR: 该论文提出了一种框架，通过将CPS的伪造问题与DNN的伪造问题联系起来，并利用决策树的可解释性，快速发现CPS中的不安全执行。


<details>
  <summary>Details</summary>
Motivation: CPS在安全关键领域广泛应用，验证其操作安全性至关重要。传统方法难以高效发现不安全执行，因此需要新的伪造方法。

Method: 构建CPS的替代模型（DNN或决策树），应用DNN伪造工具，并提出基于决策树解释的新型伪造算法。

Result: 框架工具FlexiFal能有效发现CPS中的反例，在ARCH-COMP 2024基准测试中表现优异。

Conclusion: 该框架结合DNN和决策树的优势，为CPS的伪造提供了高效解决方案。

Abstract: Cyber-Physical Systems (CPS) are abundant in safety-critical domains such as
healthcare, avionics, and autonomous vehicles. Formal verification of their
operational safety is, therefore, of utmost importance. In this paper, we
address the falsification problem, where the focus is on searching for an
unsafe execution in the system instead of proving their absence. The
contribution of this paper is a framework that (a) connects the falsification
of CPS with the falsification of deep neural networks (DNNs) and (b) leverages
the inherent interpretability of Decision Trees for faster falsification of
CPS. This is achieved by: (1) building a surrogate model of the CPS under test,
either as a DNN model or a Decision Tree, (2) application of various DNN
falsification tools to falsify CPS, and (3) a novel falsification algorithm
guided by the explanations of safety violations of the CPS model extracted from
its Decision Tree surrogate. The proposed framework has the potential to
exploit a repertoire of \emph{adversarial attack} algorithms designed to
falsify robustness properties of DNNs, as well as state-of-the-art
falsification algorithms for DNNs. Although the presented methodology is
applicable to systems that can be executed/simulated in general, we demonstrate
its effectiveness, particularly in CPS. We show that our framework, implemented
as a tool \textsc{FlexiFal}, can detect hard-to-find counterexamples in CPS
that have linear and non-linear dynamics. Decision tree-guided falsification
shows promising results in efficiently finding multiple counterexamples in the
ARCH-COMP 2024 falsification benchmarks~\cite{khandait2024arch}.

</details>


### [225] [AI-Driven Security in Cloud Computing: Enhancing Threat Detection, Automated Response, and Cyber Resilience](https://arxiv.org/abs/2505.03945)
*Shamnad Mohamed Shaffi,Sunish Vengathattil,Jezeena Nikarthil Sidhick,Resmi Vijayan*

Main category: cs.CR

TL;DR: AI在云计算安全中的应用，包括预测分析、行为检测和加密技术，解决了传统方法的不足，但也面临隐私、偏见和合规问题。


<details>
  <summary>Details</summary>
Motivation: 传统安全方案难以应对复杂威胁，AI为云计算安全提供了新的解决方案。

Method: 应用预测分析、行为检测和AI驱动的加密技术。

Result: AI显著提升了云计算环境的安全性，但仍需解决隐私、偏见和合规问题。

Conclusion: AI与区块链等新技术结合可进一步优化安全框架，未来需关注技术可靠性和伦理问题。

Abstract: Cloud security concerns have been greatly realized in recent years due to the
increase of complicated threats in the computing world. Many traditional
solutions do not work well in real-time to detect or prevent more complex
threats. Artificial intelligence is today regarded as a revolution in
determining a protection plan for cloud data architecture through machine
learning, statistical visualization of computing infrastructure, and detection
of security breaches followed by counteraction. These AI-enabled systems make
work easier as more network activities are scrutinized, and any anomalous
behavior that might be a precursor to a more serious breach is prevented. This
paper examines ways AI can enhance cloud security by applying predictive
analytics, behavior-based security threat detection, and AI-stirring
encryption. It also outlines the problems of the previous security models and
how AI overcomes them. For a similar reason, issues like data privacy, biases
in the AI model, and regulatory compliance are also covered. So, AI improves
the protection of cloud computing contexts; however, more efforts are needed in
the subsequent phases to extend the technology's reliability, modularity, and
ethical aspects. This means that AI can be blended with other new computing
technologies, including blockchain, to improve security frameworks further. The
paper discusses the current trends in securing cloud data architecture using AI
and presents further research and application directions.

</details>


### [226] [MergeGuard: Efficient Thwarting of Trojan Attacks in Machine Learning Models](https://arxiv.org/abs/2505.04015)
*Soheil Zibakhsh Shabgahi,Yaman Jandali,Farinaz Koushanfar*

Main category: cs.CR

TL;DR: MergeGuard是一种新颖的后训练方法，用于减轻AI木马攻击，通过线性化和合并全连接层，提高模型的泛化性和性能。


<details>
  <summary>Details</summary>
Motivation: AI模型中的木马攻击会导致带有触发器的输入被错误分类，对由不可信第三方训练的模型构成威胁。

Method: 提出MergeGuard方法，通过线性化和合并全连接层来改进模型。

Result: 在Transformer模型上的评估显示，MergeGuard保持模型准确性的同时降低了木马攻击成功率，优于常见的微调方法。

Conclusion: MergeGuard是一种有效的后训练木马攻击缓解方法。

Abstract: This paper proposes MergeGuard, a novel methodology for mitigation of AI
Trojan attacks. Trojan attacks on AI models cause inputs embedded with triggers
to be misclassified to an adversary's target class, posing a significant threat
to model usability trained by an untrusted third party. The core of MergeGuard
is a new post-training methodology for linearizing and merging fully connected
layers which we show simultaneously improves model generalizability and
performance. Our Proof of Concept evaluation on Transformer models demonstrates
that MergeGuard maintains model accuracy while decreasing trojan attack success
rate, outperforming commonly used (post-training) Trojan mitigation by
fine-tuning methodologies.

</details>


### [227] [LLMs' Suitability for Network Security: A Case Study of STRIDE Threat Modeling](https://arxiv.org/abs/2505.04101)
*AbdulAziz AbdulGhaffar,Ashraf Matrawy*

Main category: cs.CR

TL;DR: 该论文探讨了大型语言模型（LLMs）在网络安全性中的适用性，特别是通过STRIDE威胁建模案例研究，发现需要调整和微调LLMs以适应网络安全用例。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在6G网络中的应用广泛，但LLMs在网络安全性中的适用性研究几乎空白，本文旨在填补这一研究空白。

Method: 使用四种提示技术和五种LLMs对5G威胁进行STRIDE分类，分析其行为及影响因素。

Result: 评估结果揭示了LLMs在威胁建模中的表现，并指出了调整和微调的必要性。

Conclusion: LLMs在网络安全中具有潜力，但需进一步调整和微调以提高适用性。

Abstract: Artificial Intelligence (AI) is expected to be an integral part of
next-generation AI-native 6G networks. With the prevalence of AI, researchers
have identified numerous use cases of AI in network security. However, there
are almost nonexistent studies that analyze the suitability of Large Language
Models (LLMs) in network security. To fill this gap, we examine the suitability
of LLMs in network security, particularly with the case study of STRIDE threat
modeling. We utilize four prompting techniques with five LLMs to perform STRIDE
classification of 5G threats. From our evaluation results, we point out key
findings and detailed insights along with the explanation of the possible
underlying factors influencing the behavior of LLMs in the modeling of certain
threats. The numerical results and the insights support the necessity for
adjusting and fine-tuning LLMs for network security use cases.

</details>


### [228] [A Comprehensive Analysis of Adversarial Attacks against Spam Filters](https://arxiv.org/abs/2505.03831)
*Esra Hotoğlu,Sevil Sen,Burcu Can*

Main category: cs.CR

TL;DR: 该研究探讨了对抗性攻击对基于深度学习的垃圾邮件检测系统的影响，评估了六种模型，并提出了新的评分函数以提高攻击效果。


<details>
  <summary>Details</summary>
Motivation: 深度学习在邮件过滤中应用广泛，但对抗性攻击的复杂性威胁其有效性。

Method: 使用真实数据集评估六种深度学习模型，分析不同级别的攻击，并引入新的评分函数。

Result: 揭示了垃圾邮件过滤器的漏洞，为提升安全性提供了见解。

Conclusion: 研究有助于改进垃圾邮件过滤器对抗对抗性威胁的能力。

Abstract: Deep learning has revolutionized email filtering, which is critical to
protect users from cyber threats such as spam, malware, and phishing. However,
the increasing sophistication of adversarial attacks poses a significant
challenge to the effectiveness of these filters. This study investigates the
impact of adversarial attacks on deep learning-based spam detection systems
using real-world datasets. Six prominent deep learning models are evaluated on
these datasets, analyzing attacks at the word, character sentence, and
AI-generated paragraph-levels. Novel scoring functions, including spam weights
and attention weights, are introduced to improve attack effectiveness. This
comprehensive analysis sheds light on the vulnerabilities of spam filters and
contributes to efforts to improve their security against evolving adversarial
threats.

</details>


### [229] [Weaponizing Language Models for Cybersecurity Offensive Operations: Automating Vulnerability Assessment Report Validation; A Review Paper](https://arxiv.org/abs/2505.04265)
*Abdulrahman S Almuhaidib,Azlan Mohd Zain,Zalmiyah Zakaria,Izyan Izzati Kamsani,Abdulaziz S Almuhaidib*

Main category: cs.CR

TL;DR: 本文探讨了大型语言模型（LLMs）在漏洞评估（VA）报告验证中的潜力，填补了现有文献中关于LLMs进攻性应用的空白。


<details>
  <summary>Details</summary>
Motivation: 随着网络战的日益复杂，需要新的解决方案。LLMs在防御性和进攻性网络安全策略中展现出潜力，但现有研究多集中于防御性应用，进攻性应用（如VA报告验证）研究较少。

Method: 通过文献综述，提出了一种利用LLMs自动化分析和验证VA报告的新方法，旨在减少误报并提高效率。

Result: 结果表明，LLMs在自动化VA报告验证中具有潜力，可提高准确性并减少人工工作量。

Conclusion: 本文为LLMs在进攻性和防御性网络安全中的应用提供了新证据，有助于设计更合适的网络安全策略和工具。

Abstract: This, with the ever-increasing sophistication of cyberwar, calls for novel
solutions. In this regard, Large Language Models (LLMs) have emerged as a
highly promising tool for defensive and offensive cybersecurity-related
strategies. While existing literature has focused much on the defensive use of
LLMs, when it comes to their offensive utilization, very little has been
reported-namely, concerning Vulnerability Assessment (VA) report validation.
Consequentially, this paper tries to fill that gap by investigating the
capabilities of LLMs in automating and improving the validation process of the
report of the VA. From the critical review of the related literature, this
paper hereby proposes a new approach to using the LLMs in the automation of the
analysis and within the validation process of the report of the VA that could
potentially reduce the number of false positives and generally enhance
efficiency. These results are promising for LLM automatization for improving
validation on reports coming from VA in order to improve accuracy while
reducing human effort and security postures. The contribution of this paper
provides further evidence about the offensive and defensive LLM capabilities
and therefor helps in devising more appropriate cybersecurity strategies and
tools accordingly.

</details>


### [230] [Guardians of the Web: The Evolution and Future of Website Information Security](https://arxiv.org/abs/2505.04308)
*Md Saiful Islam,Li Xiangdong*

Main category: cs.CR

TL;DR: 本文探讨了网站信息安全的历史发展、当前实践及未来方向，强调了技术进步和国际合作的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究网站信息安全的演变，以应对日益复杂的网络威胁，保护敏感信息并维持数字世界的信任。

Method: 通过历史回顾和技术分析，总结了从早期（1960s-1980s）到现代（1990s至今）的安全技术发展。

Result: 当前实践采用多层次安全措施，未来将依赖人工智能、区块链等新兴技术及国际合作。

Conclusion: 持续的研究和创新对应对不断演变的网络威胁至关重要。

Abstract: Website information security has become a critical concern in the digital
age. This article explores the evolution of website information security,
examining its historical development, current practices, and future directions.
The early beginnings from the 1960s to the 1980s laid the groundwork for modern
cybersecurity, with the development of ARPANET, TCP/IP, public-key
cryptography, and the first antivirus programs. The 1990s marked a
transformative era, driven by the commercialization of the Internet and the
emergence of web-based services. As the Internet grew, so did the range and
sophistication of cyber threats, leading to advancements in security
technologies such as the Secure Sockets Layer (SSL) protocol, password
protection, and firewalls. Current practices in website information security
involve a multi-layered approach, including encryption, secure coding
practices, regular security audits, and user education. The future of website
information security is expected to be shaped by emerging technologies such as
artificial intelligence, blockchain, and quantum computing, as well as the
increasing importance of international cooperation and standardization efforts.
As cyber threats continue to evolve, ongoing research and innovation in website
information security will be essential to protect sensitive information and
maintain trust in the digital world.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [231] [Benchmarking LLMs' Swarm intelligence](https://arxiv.org/abs/2505.04364)
*Kai Ruan,Mowen Huang,Ji-Rong Wen,Hao Sun*

Main category: cs.MA

TL;DR: 论文介绍了SwarmBench，一个用于评估LLMs在分散多智能体系统中群体智能能力的新基准，发现LLMs在局部信息约束下存在局限性。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在严格约束条件下的群体智能能力，填补现有基准在分散协调挑战上的不足。

Method: 设计了SwarmBench，包含五个MAS协调任务，评估LLMs在局部感知和通信下的表现。

Result: LLMs在不同任务中表现差异显著，局部信息约束下规划和策略形成存在困难。

Conclusion: SwarmBench为LLMs在分散系统中的潜力评估提供了工具，促进可重复研究。

Abstract: Large Language Models (LLMs) show potential for complex reasoning, yet their
capacity for emergent coordination in Multi-Agent Systems (MAS) when operating
under strict constraints-such as limited local perception and communication,
characteristic of natural swarms-remains largely unexplored, particularly
concerning the nuances of swarm intelligence. Existing benchmarks often do not
fully capture the unique challenges of decentralized coordination that arise
when agents operate with incomplete spatio-temporal information. To bridge this
gap, we introduce SwarmBench, a novel benchmark designed to systematically
evaluate the swarm intelligence capabilities of LLMs acting as decentralized
agents. SwarmBench features five foundational MAS coordination tasks within a
configurable 2D grid environment, forcing agents to rely primarily on local
sensory input (k x k view) and local communication. We propose metrics for
coordination effectiveness and analyze emergent group dynamics. Evaluating
several leading LLMs in a zero-shot setting, we find significant performance
variations across tasks, highlighting the difficulties posed by local
information constraints. While some coordination emerges, results indicate
limitations in robust planning and strategy formation under uncertainty in
these decentralized scenarios. Assessing LLMs under swarm-like conditions is
crucial for realizing their potential in future decentralized systems. We
release SwarmBench as an open, extensible toolkit-built upon a customizable and
scalable physical system with defined mechanical properties. It provides
environments, prompts, evaluation scripts, and the comprehensive experimental
datasets generated, aiming to foster reproducible research into LLM-based MAS
coordination and the theoretical underpinnings of Embodied MAS. Our code
repository is available at https://github.com/x66ccff/swarmbench.

</details>


### [232] [From Glue-Code to Protocols: A Critical Analysis of A2A and MCP Integration for Scalable Agent Systems](https://arxiv.org/abs/2505.03864)
*Qiaomu Li,Ying Xie*

Main category: cs.MA

TL;DR: 论文探讨了整合Google的A2A协议和Anthropic的MCP协议时面临的挑战，包括语义互操作性、安全风险和治理问题。


<details>
  <summary>Details</summary>
Motivation: 研究A2A和MCP协议的潜在协同效应及其整合中的独特挑战，为多智能体系统的发展提供实践指导。

Method: 通过批判性分析，评估整合A2A和MCP的实际影响和固有困难，包括安全、隐私和调试问题。

Result: A2A+MCP为多智能体系统提供了重要架构基础，但需解决语义协商、安全漏洞等复杂问题。

Conclusion: A2A+MCP整合潜力巨大，但需进一步研究以应对其操作中的复杂性。

Abstract: Artificial intelligence is rapidly evolving towards multi-agent systems where
numerous AI agents collaborate and interact with external tools. Two key open
standards, Google's Agent to Agent (A2A) protocol for inter-agent communication
and Anthropic's Model Context Protocol (MCP) for standardized tool access,
promise to overcome the limitations of fragmented, custom integration
approaches. While their potential synergy is significant, this paper argues
that effectively integrating A2A and MCP presents unique, emergent challenges
at their intersection, particularly concerning semantic interoperability
between agent tasks and tool capabilities, the compounded security risks
arising from combined discovery and execution, and the practical governance
required for the envisioned "Agent Economy". This work provides a critical
analysis, moving beyond a survey to evaluate the practical implications and
inherent difficulties of combining these horizontal and vertical integration
standards. We examine the benefits (e.g., specialization, scalability) while
critically assessing their dependencies and trade-offs in an integrated
context. We identify key challenges increased by the integration, including
novel security vulnerabilities, privacy complexities, debugging difficulties
across protocols, and the need for robust semantic negotiation mechanisms. In
summary, A2A+MCP offers a vital architectural foundation, but fully realizing
its potential requires substantial advancements to manage the complexities of
their combined operation.

</details>


### [233] [Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and Performance in Mixed Urban Traffic](https://arxiv.org/abs/2505.04379)
*Mohammad Elayan,Wissam Kontar*

Main category: cs.MA

TL;DR: 论文研究了自动驾驶车辆（AVs）与人类驾驶车辆（HDVs）在交通系统中的共识问题，通过高分辨率轨迹数据分析，发现安全、交互和性能三者完全共识的情况罕见。


<details>
  <summary>Details</summary>
Motivation: 交通系统的复杂性和异质性使得自动驾驶车辆的部署面临共识挑战，尤其是在安全、交互质量和交通性能之间达成平衡。

Method: 使用TGSIM数据集的高分辨率轨迹数据，分析AV和HDV在信号交叉口和弱势道路使用者（VRUs）周围的行为，评估TTC、PET、减速模式等关键指标。

Result: 仅有1.63%的AV-VRU交互帧能同时满足安全、交互和性能的共识条件，表明完全共识罕见。

Conclusion: 研究强调了在混合交通环境中需要开发能平衡多维性能的AV模型，并提供了开源代码支持可复现性。

Abstract: Transportation systems have long been shaped by complexity and heterogeneity,
driven by the interdependency of agent actions and traffic outcomes. The
deployment of automated vehicles (AVs) in such systems introduces a new
challenge: achieving consensus across safety, interaction quality, and traffic
performance. In this work, we position consensus as a fundamental property of
the traffic system and aim to quantify it. We use high-resolution trajectory
data from the Third Generation Simulation (TGSIM) dataset to empirically
analyze AV and human-driven vehicle (HDV) behavior at a signalized urban
intersection and around vulnerable road users (VRUs). Key metrics, including
Time-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns,
headways, and string stability, are evaluated across the three performance
dimensions. Results show that full consensus across safety, interaction, and
performance is rare, with only 1.63% of AV-VRU interaction frames meeting all
three conditions. These findings highlight the need for AV models that
explicitly balance multi-dimensional performance in mixed-traffic environments.
Full reproducibility is supported via our open-source codebase on
https://github.com/wissamkontar/Consensus-AV-Analysis.

</details>


### [234] [Implicitly Aligning Humans and Autonomous Agents through Shared Task Abstractions](https://arxiv.org/abs/2505.04579)
*Stéphane Aroca-Ouellette,Miguel Aroca-Ouellette,Katharina von der Wense,Alessandro Roncone*

Main category: cs.MA

TL;DR: HA$^2$框架通过分层强化学习模仿人类协作方式，显著提升零样本协调能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自主代理在新队友适应能力上不如人类，缺乏共享任务抽象是限制因素。

Method: 引入HA$^2$框架，利用分层强化学习模拟人类协作结构。

Result: 在Overcooked环境中，HA$^2$显著优于基线方法，适应性强且表现最佳。

Conclusion: HA$^2$通过共享任务抽象有效提升协作能力，为自主代理提供新方向。

Abstract: In collaborative tasks, autonomous agents fall short of humans in their
capability to quickly adapt to new and unfamiliar teammates. We posit that a
limiting factor for zero-shot coordination is the lack of shared task
abstractions, a mechanism humans rely on to implicitly align with teammates. To
address this gap, we introduce HA$^2$: Hierarchical Ad Hoc Agents, a framework
leveraging hierarchical reinforcement learning to mimic the structured approach
humans use in collaboration. We evaluate HA$^2$ in the Overcooked environment,
demonstrating statistically significant improvement over existing baselines
when paired with both unseen agents and humans, providing better resilience to
environmental shifts, and outperforming all state-of-the-art methods.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [235] [An Empirical Study of OpenAI API Discussions on Stack Overflow](https://arxiv.org/abs/2505.04084)
*Xiang Chen,Jibin Wang,Chaoyang Gao,Xiaolin Ju,Zhanqi Cui*

Main category: cs.SE

TL;DR: 论文通过分析Stack Overflow上的2,874条OpenAI API相关讨论，首次全面研究了开发者使用OpenAI API时遇到的挑战，并提出了针对开发者、LLM供应商和研究者的建议。


<details>
  <summary>Details</summary>
Motivation: OpenAI API的独特挑战（如提示工程、非确定性输出等）尚未被实证研究探索，填补这一空白是研究的动机。

Method: 研究分析了Stack Overflow上的2,874条OpenAI API相关讨论，通过手动分类和主题建模分析识别挑战。

Result: 研究发现开发者在使用OpenAI API时面临多种挑战，并针对不同类别提出了具体问题。

Conclusion: 研究为开发者、LLM供应商和研究者提供了实用的建议，以应对OpenAI API的挑战。

Abstract: The rapid advancement of large language models (LLMs), represented by
OpenAI's GPT series, has significantly impacted various domains such as natural
language processing, software development, education, healthcare, finance, and
scientific research. However, OpenAI APIs introduce unique challenges that
differ from traditional APIs, such as the complexities of prompt engineering,
token-based cost management, non-deterministic outputs, and operation as black
boxes. To the best of our knowledge, the challenges developers encounter when
using OpenAI APIs have not been explored in previous empirical studies. To fill
this gap, we conduct the first comprehensive empirical study by analyzing 2,874
OpenAI API-related discussions from the popular Q&A forum Stack Overflow. We
first examine the popularity and difficulty of these posts. After manually
categorizing them into nine OpenAI API-related categories, we identify specific
challenges associated with each category through topic modeling analysis. Based
on our empirical findings, we finally propose actionable implications for
developers, LLM vendors, and researchers.

</details>


### [236] [Facilitating Trustworthy Human-Agent Collaboration in LLM-based Multi-Agent System oriented Software Engineering](https://arxiv.org/abs/2505.04251)
*Krishna Ronanki*

Main category: cs.SE

TL;DR: 本文提出了一种基于RACI的框架，用于解决LLM驱动的多智能体自主系统（LMA）在软件工程中任务分配的挑战，旨在促进高效协作并确保可信赖性。


<details>
  <summary>Details</summary>
Motivation: 多智能体自主系统（MAS）在软件工程中表现优异，但LLM驱动的LMA系统引入后，任务分配的可信赖性成为主要挑战。

Method: 提出RACI框架，提供实施指南和示例实现，以优化任务分配。

Result: 框架有助于高效协作、确保责任明确，并降低LLM自动化风险，符合可信赖AI准则。

Conclusion: 未来计划通过实证验证进一步完善框架。

Abstract: Multi-agent autonomous systems (MAS) are better at addressing challenges that
spans across multiple domains than singular autonomous agents. This holds true
within the field of software engineering (SE) as well. The state-of-the-art
research on MAS within SE focuses on integrating LLMs at the core of autonomous
agents to create LLM-based multi-agent autonomous (LMA) systems. However, the
introduction of LMA systems into SE brings a plethora of challenges. One of the
major challenges is the strategic allocation of tasks between humans and the
LMA system in a trustworthy manner. To address this challenge, a RACI-based
framework is proposed in this work in progress article, along with
implementation guidelines and an example implementation of the framework. The
proposed framework can facilitate efficient collaboration, ensure
accountability, and mitigate potential risks associated with LLM-driven
automation while aligning with the Trustworthy AI guidelines. The future steps
for this work delineating the planned empirical validation method are also
presented.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [237] [Is the end of Insight in Sight ?](https://arxiv.org/abs/2505.04627)
*Jean-Michel Tucny,Mihir Durve,Sauro Succi*

Main category: physics.comp-ph

TL;DR: PINN在玻尔兹曼方程中的应用显示权重矩阵与物理问题结构无关，接近高斯随机分布，暗示深度学习与数值解可能是等效但独立的路径，Explainable AI可能不现实。


<details>
  <summary>Details</summary>
Motivation: 探讨PINN在解决玻尔兹曼方程时权重矩阵的性质及其与物理问题结构的关系。

Method: 分析PINN权重矩阵的分布特性，对比其与玻尔兹曼方程数学结构的联系。

Result: 权重矩阵接近高斯随机分布，与物理问题结构无直接关联。

Conclusion: 深度学习与数值解可能是等效但独立的路径，Explainable AI目标可能不现实或不适定。

Abstract: It is shown that the weight matrices of a Physics-informed neural network
(PINN)-based deep learning application to a rarefied gas dynamics problem
described by the Boltzmann equation bear no evident link to the mathematical
structure of the physical problem. Instead, the weights appear close to
Gaussian distributed random matrices. Although significantly more work is
needed to support a robust assessment in this direction, these results suggest
that deep-learning and the numerical solution of the Boltzmann equation
represent two equivalent, but largely distinct paths to the same physical
knowledge. If so, Explainable AI might be an unrealistic target and possibly
even an ill-posed one.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [238] [Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs](https://arxiv.org/abs/2505.03814)
*Ganghua Wang,Zhaorun Chen,Bo Li,Haifeng Xu*

Main category: stat.ML

TL;DR: 本文提出了一种可认证且高效的LLM评估框架Cer-Eval，通过自适应选择测试点减少评估成本，同时保持高置信度。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型规模扩大，评估大型语言模型（LLM）的挑战日益突出，当前评估方法缺乏系统性分析和数据选择指导。

Method: 提出基于测试样本复杂度的理论框架，开发分区算法Cer-Eval，自适应选择测试点以减少评估成本。

Result: 实验表明，Cer-Eval可节省20%-40%测试点，同时保持与现有方法相当的误差水平并提供95%置信度保证。

Conclusion: Cer-Eval为LLM评估提供了一种高效且可认证的解决方案，显著降低了评估成本。

Abstract: As foundation models continue to scale, the size of trained models grows
exponentially, presenting significant challenges for their evaluation. Current
evaluation practices involve curating increasingly large datasets to assess the
performance of large language models (LLMs). However, there is a lack of
systematic analysis and guidance on determining the sufficiency of test data or
selecting informative samples for evaluation. This paper introduces a
certifiable and cost-efficient evaluation framework for LLMs. Our framework
adapts to different evaluation objectives and outputs confidence intervals that
contain true values with high probability. We use ``test sample complexity'' to
quantify the number of test points needed for a certifiable evaluation and
derive tight bounds on test sample complexity. Based on the developed theory,
we develop a partition-based algorithm, named Cer-Eval, that adaptively selects
test points to minimize the cost of LLM evaluation. Real-world experiments
demonstrate that Cer-Eval can save 20% to 40% test points across various
benchmarks, while maintaining an estimation error level comparable to the
current evaluation process and providing a 95% confidence guarantee.

</details>


### [239] [Categorical and geometric methods in statistical, manifold, and machine learning](https://arxiv.org/abs/2505.03862)
*Hông Vân Lê,Hà Quang Minh,Frederic Protin,Wilderich Tuschmann*

Main category: stat.ML

TL;DR: 本文探讨了概率态射范畴的应用及几何方法在统计、机器和流形学习中的问题。


<details>
  <summary>Details</summary>
Motivation: 研究概率态射范畴及其几何方法在多种学习问题中的应用潜力。

Method: 结合概率态射范畴和几何方法，分析统计、机器和流形学习中的问题。

Result: 展示了这些方法在多个学习问题中的适用性，并将在未来书籍中进一步探讨。

Conclusion: 概率态射范畴和几何方法为学习问题提供了新的视角和工具。

Abstract: We present and discuss applications of the category of probabilistic
morphisms, initially developed in \cite{Le2023}, as well as some geometric
methods to several classes of problems in statistical, machine and manifold
learning which shall be, along with many other topics, considered in depth in
the forthcoming book \cite{LMPT2024}.

</details>


### [240] [Variational Formulation of the Particle Flow Particle Filter](https://arxiv.org/abs/2505.04007)
*Yinzhuang Yi,Jorge Cortés,Nikolay Atanasov*

Main category: stat.ML

TL;DR: 本文从变分推断的角度重新表述了粒子流粒子滤波器，并证明其瞬态密度遵循Fisher-Rao梯度流的时间尺度轨迹。


<details>
  <summary>Details</summary>
Motivation: 通过变分推断的视角，重新理解粒子流粒子滤波器的理论基础。

Method: 利用Fisher-Rao梯度流作为连续时间算法，最小化变分密度与真实后验密度之间的Kullback-Leibler散度。

Result: 证明了粒子流粒子滤波器的瞬态密度遵循Fisher-Rao梯度流的时间尺度轨迹。

Conclusion: 该研究为粒子流粒子滤波器提供了新的理论框架，并展示了其与变分推断的紧密联系。

Abstract: This paper provides a formulation of the particle flow particle filter from
the perspective of variational inference. We show that the transient density
used to derive the particle flow particle filter follows a time-scaled
trajectory of the Fisher-Rao gradient flow in the space of probability
densities. The Fisher-Rao gradient flow is obtained as a continuous-time
algorithm for variational inference, minimizing the Kullback-Leibler divergence
between a variational density and the true posterior density.

</details>


### [241] [A Tutorial on Discriminative Clustering and Mutual Information](https://arxiv.org/abs/2505.04484)
*Louis Ohl,Pierre-Alexandre Mattei,Frédéric Precioso*

Main category: stat.ML

TL;DR: 本文回顾了判别式聚类方法的历史演变，重点关注其假设的变化，从决策边界到不变性批评，并探讨了互信息的作用及其局限性。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习在聚类中的应用增长，判别式聚类方法成为研究热点。本文旨在提供其历史视角，展示假设的演变及其挑战。

Method: 通过分析判别式聚类方法的假设变化，特别是互信息的作用，并结合Python工具GemClus展示技术实现。

Result: 总结了判别式聚类方法的进展，揭示了互信息的核心地位及其局限性，并探讨了聚类数量选择的挑战。

Conclusion: 判别式聚类方法在假设演变和技术实现上取得进展，但仍面临聚类数量选择等挑战。

Abstract: To cluster data is to separate samples into distinctive groups that should
ideally have some cohesive properties. Today, numerous clustering algorithms
exist, and their differences lie essentially in what can be perceived as
``cohesive properties''. Therefore, hypotheses on the nature of clusters must
be set: they can be either generative or discriminative. As the last decade
witnessed the impressive growth of deep clustering methods that involve neural
networks to handle high-dimensional data often in a discriminative manner; we
concentrate mainly on the discriminative hypotheses. In this paper, our aim is
to provide an accessible historical perspective on the evolution of
discriminative clustering methods and notably how the nature of assumptions of
the discriminative models changed over time: from decision boundaries to
invariance critics. We notably highlight how mutual information has been a
historical cornerstone of the progress of (deep) discriminative clustering
methods. We also show some known limitations of mutual information and how
discriminative clustering methods tried to circumvent those. We then discuss
the challenges that discriminative clustering faces with respect to the
selection of the number of clusters. Finally, we showcase these techniques
using the dedicated Python package, GemClus, that we have developed for
discriminative clustering.

</details>


### [242] [From Two Sample Testing to Singular Gaussian Discrimination](https://arxiv.org/abs/2505.04613)
*Leonardo V. Santoro,Kartik G. Waghmare,Victor M. Panaretos*

Main category: stat.ML

TL;DR: 论文证明了在一般可分紧度量空间上测试两个概率测度相等性，等价于在合适的再生核希尔伯特空间中测试两个对应高斯测度的奇异性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过高斯嵌入方法简化高维设置下的非参数两样本测试问题。

Method: 利用核均值和协方差嵌入概率测度，并基于Feldman-Hajek准则分析高斯测度的奇异性。

Result: 结果表明，不同概率测度通过高斯嵌入后会显著分离，从而简化测试问题。

Conclusion: 结论指出这是一种“维度祝福”的新实例，可用于设计高效推断工具。

Abstract: We establish that testing for the equality of two probability measures on a
general separable and compact metric space is equivalent to testing for the
singularity between two corresponding Gaussian measures on a suitable
Reproducing Kernel Hilbert Space. The corresponding Gaussians are defined via
the notion of kernel mean and covariance embedding of a probability measure.
Discerning two singular Gaussians is fundamentally simpler from an
information-theoretic perspective than non-parametric two-sample testing,
particularly in high-dimensional settings. Our proof leverages the
Feldman-Hajek criterion for singularity/equivalence of Gaussians on Hilbert
spaces, and shows that discrepancies between distributions are heavily
magnified through their corresponding Gaussian embeddings: at a population
level, distinct probability measures lead to essentially separated Gaussian
embeddings. This appears to be a new instance of the blessing of dimensionality
that can be harnessed for the design of efficient inference tools in great
generality.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [243] [Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts](https://arxiv.org/abs/2505.04171)
*Nouar Aldahoul,Hazem Ibrahim,Matteo Varvello,Aaron Kaufman,Talal Rahwan,Yasir Zaki*

Main category: cs.CY

TL;DR: 研究发现大型语言模型（LLMs）的政治偏见虽看似微小，实则是极端观点的抵消结果，且能显著影响用户政治倾向。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs的政治偏见及其对用户政治倾向的实际影响，挑战现有研究认为偏见较小的观点。

Method: 通过比较31个LLMs与立法者、法官及美国选民样本，并进行随机实验，评估LLMs的政治偏见及其说服力。

Result: LLMs的总体偏见是极端观点的抵消结果，且能使用户政治倾向改变多达5个百分点。

Conclusion: LLMs可能成为政治影响力的强大工具，需警惕其潜在影响。

Abstract: Large Language Models (LLMs) are a transformational technology, fundamentally
changing how people obtain information and interact with the world. As people
become increasingly reliant on them for an enormous variety of tasks, a body of
academic research has developed to examine these models for inherent biases,
especially political biases, often finding them small. We challenge this
prevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a
nationally representative sample of U.S. voters, we show that LLMs' apparently
small overall partisan preference is the net result of offsetting extreme views
on specific topics, much like moderate voters. Second, in a randomized
experiment, we show that LLMs can promulgate their preferences into political
persuasiveness even in information-seeking contexts: voters randomized to
discuss political issues with an LLM chatbot are as much as 5 percentage points
more likely to express the same preferences as that chatbot. Contrary to
expectations, these persuasive effects are not moderated by familiarity with
LLMs, news consumption, or interest in politics. LLMs, especially those
controlled by private companies or governments, may become a powerful and
targeted vector for political influence.

</details>


### [244] [Deepfakes on Demand: the rise of accessible non-consensual deepfake image generators](https://arxiv.org/abs/2505.03859)
*Will Hawkins,Chris Russell,Brent Mittelstadt*

Main category: cs.CY

TL;DR: 研究发现，文本到图像（T2I）模型的普及导致大量深度伪造模型在线可访问，主要针对女性，且生成非自愿亲密图像（NCII）意图明显。


<details>
  <summary>Details</summary>
Motivation: 探讨深度伪造模型在线可访问性及其对社会的影响。

Method: 分析Hugging Face和Civitai上数千个公开可下载的模型变体，统计其下载量、目标对象及技术细节。

Result: 发现近35,000个深度伪造模型变体，下载量达1500万次，96%针对女性，且生成NCII意图明显。

Conclusion: 强调需采取更多行动打击深度伪造和NCII的生成与传播。

Abstract: Advances in multimodal machine learning have made text-to-image (T2I) models
increasingly accessible and popular. However, T2I models introduce risks such
as the generation of non-consensual depictions of identifiable individuals,
otherwise known as deepfakes. This paper presents an empirical study exploring
the accessibility of deepfake model variants online. Through a metadata
analysis of thousands of publicly downloadable model variants on two popular
repositories, Hugging Face and Civitai, we demonstrate a huge rise in easily
accessible deepfake models. Almost 35,000 examples of publicly downloadable
deepfake model variants are identified, primarily hosted on Civitai. These
deepfake models have been downloaded almost 15 million times since November
2022, with the models targeting a range of individuals from global celebrities
to Instagram users with under 10,000 followers. Both Stable Diffusion and Flux
models are used for the creation of deepfake models, with 96% of these
targeting women and many signalling intent to generate non-consensual intimate
imagery (NCII). Deepfake model variants are often created via the
parameter-efficient fine-tuning technique known as low rank adaptation (LoRA),
requiring as few as 20 images, 24GB VRAM, and 15 minutes of time, making this
process widely accessible via consumer-grade computers. Despite these models
violating the Terms of Service of hosting platforms, and regulation seeking to
prevent dissemination, these results emphasise the pressing need for greater
action to be taken against the creation of deepfakes and NCII.

</details>


### [245] [Coverage Biases in High-Resolution Satellite Imagery](https://arxiv.org/abs/2505.03842)
*Vadim Musienko,Axel Jacquet,Ingmar Weber,Till Koebe*

Main category: cs.CY

TL;DR: 卫星图像覆盖存在地理和社会经济偏差，远离赤道地区更频繁被访问，欠发达地区历史图像较少，地缘政治事件也影响图像可用性。


<details>
  <summary>Details</summary>
Motivation: 探讨卫星图像是否在全球范围内公平分布，揭示覆盖偏差及其原因。

Method: 分析卫星轨道路径评估30天内访问频率，收集历史图像元数据并结合社会经济和地缘政治因素。

Result: 远离赤道地区访问更频繁，欠发达地区历史图像较少，冲突地区图像可用性受地缘政治影响。

Conclusion: 卫星图像的数字化红利在全球分布不均，需关注覆盖偏差及其潜在影响。

Abstract: Satellite imagery is increasingly used to complement traditional data
collection approaches such as surveys and censuses across scientific
disciplines. However, we ask: Do all places on earth benefit equally from this
new wealth of information? In this study, we investigate coverage bias of major
satellite constellations that provide optical satellite imagery with a ground
sampling distance below 10 meters, evaluating both the future on-demand tasking
opportunities as well as the availability of historic images across the globe.
Specifically, forward-looking, we estimate how often different places are
revisited during a window of 30 days based on the satellites' orbital paths,
thus investigating potential coverage biases caused by physical factors. We
find that locations farther away from the equator are generally revisited more
frequently by the constellations under study. Backward-looking, we show that
historic satellite image availability -- based on metadata collected from major
satellite imagery providers -- is influenced by socio-economic factors on the
ground: less developed, less populated places have less satellite images
available. Furthermore, in three small case studies on recent conflict regions
in this world, namely Gaza, Sudan and Ukraine, we show that also geopolitical
events play an important role in satellite image availability, hinting at
underlying business model decisions. These insights lay bare that the digital
dividend yielded by satellite imagery is not equally distributed across our
planet.

</details>


### [246] [AI Governance to Avoid Extinction: The Strategic Landscape and Actionable Research Questions](https://arxiv.org/abs/2505.04592)
*Peter Barnett,Aaron Scher*

Main category: cs.CY

TL;DR: 论文探讨了AI快速发展可能带来的灾难性风险，提出了四种应对场景，并强调需要紧急行动以避免人类灭绝。


<details>
  <summary>Details</summary>
Motivation: AI系统可能在所有认知领域超越人类，但默认发展路径可能导致灾难性后果，包括人类灭绝。研究旨在描述AI发展的战略格局并提出治理问题。

Method: 分析了四种高级别场景，包括国际限制危险AI发展的“关闭开关”方案、美国的国家AI项目、轻触式世界和破坏威胁情境。

Result: 除“关闭开关”方案外，其他场景均可能带来不可接受的灾难性风险。

Conclusion: 需要美国国家安全社区和AI治理生态系统紧急行动，回答关键研究问题并准备国际AI协议。

Abstract: Humanity appears to be on course to soon develop AI systems that
substantially outperform human experts in all cognitive domains and activities.
We believe the default trajectory has a high likelihood of catastrophe,
including human extinction. Risks come from failure to control powerful AI
systems, misuse of AI by malicious rogue actors, war between great powers, and
authoritarian lock-in. This research agenda has two aims: to describe the
strategic landscape of AI development and to catalog important governance
research questions. These questions, if answered, would provide important
insight on how to successfully reduce catastrophic risks.
  We describe four high-level scenarios for the geopolitical response to
advanced AI development, cataloging the research questions most relevant to
each. Our favored scenario involves building the technical, legal, and
institutional infrastructure required to internationally restrict dangerous AI
development and deployment (which we refer to as an Off Switch), which leads
into an internationally coordinated Halt on frontier AI activities at some
point in the future. The second scenario we describe is a US National Project
for AI, in which the US Government races to develop advanced AI systems and
establish unilateral control over global AI development. We also describe two
additional scenarios: a Light-Touch world similar to that of today and a Threat
of Sabotage situation where countries use sabotage and deterrence to slow AI
development.
  In our view, apart from the Off Switch and Halt scenario, all of these
trajectories appear to carry an unacceptable risk of catastrophic harm. Urgent
action is needed from the US National Security community and AI governance
ecosystem to answer key research questions, build the capability to halt
dangerous AI activities, and prepare for international AI agreements.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [247] [Ultra-Low-Power Spiking Neurons in 7 nm FinFET Technology: A Comparative Analysis of Leaky Integrate-and-Fire, Morris-Lecar, and Axon-Hillock Architectures](https://arxiv.org/abs/2505.03764)
*Logan Larsh,Raiyan Siddique,Sarah Sharif Yaser Mike Banad*

Main category: cs.NE

TL;DR: 本文比较了三种尖峰神经元电路架构（LIF、ML、AH）在7纳米FinFET技术中的性能，发现AH设计吞吐量最高，ML在低功耗下表现优异，LIF则在高频操作中略有优势。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过比较不同神经元电路架构，为神经形态计算提供高效、低功耗的硬件解决方案。

Method: 采用7纳米FinFET技术实现三种架构，并通过SPICE模拟优化尖峰频率、能量消耗和静态功耗。

Result: AH设计实现最高吞吐量（3 GHz），ML在低功耗下表现最佳（0.385 aJ/spike），LIF在高频操作中略有优势但静态泄漏较高。

Conclusion: 研究为先进纳米技术中的尖峰神经元电路优化提供了设计指南，支持超低功耗和高吞吐量的神经形态硬件开发。

Abstract: Neuromorphic computing aims to replicate the brain's remarkable energy
efficiency and parallel processing capabilities for large-scale artificial
intelligence applications. In this work, we present a comprehensive comparative
study of three spiking neuron circuit architectures-Leaky-Integrate-and-Fire
(LIF), Morris-Lecar (ML), and Axon-Hillock (AH)-implemented in a 7 nm FinFET
technology. Through extensive SPICE simulations, we explore the optimization of
spiking frequency, energy per spike, and static power consumption. Our results
show that the AH design achieves the highest throughput, demonstrating
multi-gigahertz firing rates (up to 3 GHz) with attojoule energy costs. By
contrast, the ML architecture excels in subthreshold to near-threshold regimes,
offering robust low-power operation (as low as 0.385 aJ/spike) and biological
bursting behavior. Although LIF benefits from a decoupled current mirror for
high-frequency operation, it exhibits slightly higher static leakage compared
to ML and AH at elevated supply voltages. Comparisons with previous node
implementations (22 nm planar, 28 nm) reveal that 7 nm FinFETs can drastically
boost energy efficiency and speed albeit at the cost of increased subthreshold
leakage in deep subthreshold regions. By quantifying design trade-offs for each
neuron architecture, our work provides a roadmap for optimizing spiking neuron
circuits in advanced nanoscale technologies to deliver neuromorphic hardware
capable of both ultra-low-power operation and high computational throughput.

</details>


### [248] [Izhikevich-Inspired Temporal Dynamics for Enhancing Privacy, Efficiency, and Transferability in Spiking Neural Networks](https://arxiv.org/abs/2505.04034)
*Ayana Moshruba,Hamed Poursiami,Maryam Parsa*

Main category: cs.NE

TL;DR: 论文提出两种概率驱动的输入级时间尖峰变换（Poisson-Burst和Delayed-Burst），用于在标准LIF神经元中引入生物启发的时序变异性，以研究尖峰时序动态对隐私、泛化和学习性能的影响。


<details>
  <summary>Details</summary>
Motivation: 生物神经元表现出多样的时序尖峰模式，但现有复杂模型难以直接集成到可扩展的SNN训练流程中。

Method: 提出Poisson-Burst和Delayed-Burst两种变换，分别通过输入强度调节爆发频率和通过爆发起始时间编码输入强度。

Result: 实验表明，Poisson-Burst在保持准确性的同时增强隐私鲁棒性，而Delayed-Burst提供更强的隐私保护但牺牲部分准确性。

Conclusion: 生物启发的时序尖峰动态可提升神经形态学习系统的隐私性、泛化能力和生物合理性。

Abstract: Biological neurons exhibit diverse temporal spike patterns, which are
believed to support efficient, robust, and adaptive neural information
processing. While models such as Izhikevich can replicate a wide range of these
firing dynamics, their complexity poses challenges for directly integrating
them into scalable spiking neural networks (SNN) training pipelines. In this
work, we propose two probabilistically driven, input-level temporal spike
transformations: Poisson-Burst and Delayed-Burst that introduce biologically
inspired temporal variability directly into standard Leaky Integrate-and-Fire
(LIF) neurons. This enables scalable training and systematic evaluation of how
spike timing dynamics affect privacy, generalization, and learning performance.
Poisson-Burst modulates burst occurrence based on input intensity, while
Delayed-Burst encodes input strength through burst onset timing. Through
extensive experiments across multiple benchmarks, we demonstrate that
Poisson-Burst maintains competitive accuracy and lower resource overhead while
exhibiting enhanced privacy robustness against membership inference attacks,
whereas Delayed-Burst provides stronger privacy protection at a modest accuracy
trade-off. These findings highlight the potential of biologically grounded
temporal spike dynamics in improving the privacy, generalization and biological
plausibility of neuromorphic learning systems.

</details>


### [249] [TS-SNN: Temporal Shift Module for Spiking Neural Networks](https://arxiv.org/abs/2505.04165)
*Kairong Yu,Tianqing Zhang,Qi Xu,Gang Pan,Hongwei Wang*

Main category: cs.NE

TL;DR: TS-SNN通过引入轻量级Temporal Shift模块，有效整合时空特征，在保持低能耗的同时实现高性能。


<details>
  <summary>Details</summary>
Motivation: SNN在神经形态计算中具有生物合理性和能效优势，但如何平衡时间特征利用与低能耗仍具挑战性。

Method: 提出TS-SNN，通过Temporal Shift模块整合过去、现在和未来的脉冲特征，采用残差组合防止信息丢失。

Result: 在CIFAR-10、CIFAR-100和ImageNet上取得SOTA性能，且能耗低。

Conclusion: TS-SNN为高效准确的SNN架构开发迈出重要一步。

Abstract: Spiking Neural Networks (SNNs) are increasingly recognized for their
biological plausibility and energy efficiency, positioning them as strong
alternatives to Artificial Neural Networks (ANNs) in neuromorphic computing
applications. SNNs inherently process temporal information by leveraging the
precise timing of spikes, but balancing temporal feature utilization with low
energy consumption remains a challenge. In this work, we introduce Temporal
Shift module for Spiking Neural Networks (TS-SNN), which incorporates a novel
Temporal Shift (TS) module to integrate past, present, and future spike
features within a single timestep via a simple yet effective shift operation. A
residual combination method prevents information loss by integrating shifted
and original features. The TS module is lightweight, requiring only one
additional learnable parameter, and can be seamlessly integrated into existing
architectures with minimal additional computational cost. TS-SNN achieves
state-of-the-art performance on benchmarks like CIFAR-10 (96.72\%), CIFAR-100
(80.28\%), and ImageNet (70.61\%) with fewer timesteps, while maintaining low
energy consumption. This work marks a significant step forward in developing
efficient and accurate SNN architectures.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [250] [High-speed multiwavelength photonic temporal integration using silicon photonics](https://arxiv.org/abs/2505.04405)
*Yi Zhang,Nikolaos Farmakidis,Ioannis Roumpos,Miltiadis Moralis-Pegios,Apostolos Tsakyridis,June Sang Lee,Bowei Dong,Yuhan He,Samarth Aggarwal,Nikolaos Pleros,Harish Bhaskaran*

Main category: physics.optics

TL;DR: 提出了一种通过光热集成实现高速光子计算的可扩展方法。


<details>
  <summary>Details</summary>
Motivation: 解决光学硬件在AI任务中映射大向量规模的挑战。

Method: 利用慢热耗散过程进行光信号集成，引入PHIL单元实现全光时域集成。

Result: 支持端到端光信号处理，消除低效电光转换，实现线性和非线性操作。

Conclusion: 通过热驱动集成展示了高速光子计算的可扩展路径。

Abstract: Optical systems have been pivotal for energy-efficient computing, performing
high-speed, parallel operations in low-loss carriers. While these predominantly
analog optical accelerators bypass digitization to perform parallel
floating-point computations, scaling optical hardware to map large-vector sizes
for AI tasks remains challenging. Here, we overcome this limitation by
unfolding scalar operations in time and introducing a
photonic-heater-in-lightpath (PHIL) unit for all-optical temporal integration.
Counterintuitively, we exploit a slow heat dissipation process to integrate
optical signals modulated at 50 GHz bridging the speed gap between the widely
applied thermo-optic effects and ultrafast photonics. This architecture
supports optical end-to-end signal processing, eliminates inefficient
electro-optical conversions, and enables both linear and nonlinear operations
within a unified framework. Our results demonstrate a scalable path towards
high-speed photonic computing through thermally driven integration.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [251] [Likelihood-Free Adaptive Bayesian Inference via Nonparametric Distribution Matching](https://arxiv.org/abs/2505.04603)
*Wenhui Sophia Lu,Wing Hung Wong*

Main category: stat.ME

TL;DR: 论文提出了一种名为自适应贝叶斯推断（ABI）的新框架，通过直接在后验空间中比较分布而非传统的数据空间差异，解决了高维或扩散先验下近似贝叶斯计算（ABC）的计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统ABC方法在高维或扩散先验下计算效率低下，需要一种更高效的后验推断方法。

Method: ABI利用一种新颖的边际增强切片Wasserstein（MSW）距离，将后验分布之间的差异测量转化为一系列一维条件分位数回归任务，并结合自适应拒绝采样方案。

Result: 理论证明ABI后验收敛于真实后验，并通过实验验证其在多维或依赖观测场景中优于现有方法。

Conclusion: ABI为高维或复杂后验推断提供了一种高效且理论可靠的解决方案。

Abstract: When the likelihood is analytically unavailable and computationally
intractable, approximate Bayesian computation (ABC) has emerged as a widely
used methodology for approximate posterior inference; however, it suffers from
severe computational inefficiency in high-dimensional settings or under diffuse
priors. To overcome these limitations, we propose Adaptive Bayesian Inference
(ABI), a framework that bypasses traditional data-space discrepancies and
instead compares distributions directly in posterior space through
nonparametric distribution matching. By leveraging a novel Marginally-augmented
Sliced Wasserstein (MSW) distance on posterior measures and exploiting its
quantile representation, ABI transforms the challenging problem of measuring
divergence between posterior distributions into a tractable sequence of
one-dimensional conditional quantile regression tasks. Moreover, we introduce a
new adaptive rejection sampling scheme that iteratively refines the posterior
approximation by updating the proposal distribution via generative density
estimation. Theoretically, we establish parametric convergence rates for the
trimmed MSW distance and prove that the ABI posterior converges to the true
posterior as the tolerance threshold vanishes. Through extensive empirical
evaluation, we demonstrate that ABI significantly outperforms data-based
Wasserstein ABC, summary-based ABC, and state-of-the-art likelihood-free
simulators, especially in high-dimensional or dependent observation regimes.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [252] [The Shift Towards Preprints in AI Policy Research: A Comparative Study of Preprint Trends in the U.S., Europe, and South Korea](https://arxiv.org/abs/2505.03835)
*Simon Suh,Jihyuk Bang,Ji Woo Han*

Main category: cs.DL

TL;DR: 研究探讨了COVID-19和ChatGPT发布对AI政策研究中预印本引用趋势的区域影响，发现美国、欧洲和韩国的增长模式各异，强调未来AI治理需考虑区域差异。


<details>
  <summary>Details</summary>
Motivation: 分析全球性事件（如COVID-19和ChatGPT发布）如何影响不同地区AI政策研究中预印本的引用趋势，揭示区域差异背后的原因。

Method: 使用Web of Science的文献计量数据，追踪2015至2024年间美国、欧洲和韩国的预印本引用变化，并标记关键事件时间点。

Result: 所有地区预印本引用均增长，但模式不同：美国事件驱动增长，欧洲机构性增长，韩国线性增长。区域差异受本地文化和开放科学成熟度影响。

Conclusion: 全球事件加速预印本采用，但区域差异显著。未来AI治理需考虑区域特性，建议进一步纵向和比较研究。

Abstract: The adoption of open science has quickly changed how artificial intelligence
(AI) policy research is distributed globally. This study examines the regional
trends in the citation of preprints, specifically focusing on the impact of two
major disruptive events: the COVID-19 pandemic and the release of ChatGPT, on
research dissemination patterns in the United States, Europe, and South Korea
from 2015 to 2024. Using bibliometrics data from the Web of Science, this study
tracks how global disruptive events influenced the adoption of preprints in AI
policy research and how such shifts vary by region. By marking the timing of
these disruptive events, the analysis reveals that while all regions
experienced growth in preprint citations, the magnitude and trajectory of
change varied significantly. The United States exhibited sharp, event-driven
increases; Europe demonstrated institutional growth; and South Korea maintained
consistent, linear growth in preprint adoption. These findings suggest that
global disruptions may have accelerated preprint adoption, but the extent and
trajectory are shaped by local research cultures, policy environments, and
levels of open science maturity. This paper emphasizes the need for future AI
governance strategies to consider regional variability in research
dissemination and highlights opportunities for further longitudinal and
comparative research to deepen our understanding of open-access adoption in AI
policy development.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [253] [The Evolution of Rough Sets 1970s-1981](https://arxiv.org/abs/2505.03747)
*Viktor Marek,Ewa Orłowska,Ivo Düntsch*

Main category: math.HO

TL;DR: 回顾Zdzisław Pawlak及其合作者在1970年代和1981年的研究与出版物，重点关注这些出版物中的灵感来源，并概述1981年与粗糙集和信息系统相关的发展。


<details>
  <summary>Details</summary>
Motivation: 回顾Pawlak及其团队的研究，探索其灵感来源，并总结粗糙集和信息系统的早期发展。

Method: 通过分析Pawlak及其合作者的出版物，识别灵感来源，并梳理1981年的相关进展。

Result: 明确了Pawlak研究中的灵感来源，并总结了粗糙集和信息系统在1981年的关键发展。

Conclusion: Pawlak的研究为粗糙集和信息系统奠定了基础，其灵感来源对后续研究具有重要影响。

Abstract: In this note research and publications by Zdzis{\l}aw Pawlak and his
collaborators from 1970s and 1981 are recalled. Focus is placed on the sources
of inspiration which one can identify on the basis of those publications.
Finally, developments from 1981 related to rough sets and information systems
are outlined.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [254] [Differentially Private Densest-$k$-Subgraph](https://arxiv.org/abs/2505.03858)
*Alireza Khayatian,Anil Vullikanti,Aritra Konar*

Main category: cs.DS

TL;DR: 该论文首次为Densest-$k$-subgraph（D$k$S）问题设计了具有差分隐私（DP）保证的算法，基于图邻接矩阵的主成分（PC）方法，并提出了高效的Propose-Test-Release（PTR）框架和迭代私有幂方法（PPM）。


<details>
  <summary>Details</summary>
Motivation: 图数据集常涉及敏感网络数据，需要隐私保护的图挖掘方法。D$k$S问题是图挖掘中的关键问题，但现有算法缺乏隐私保护。

Method: 基于邻接矩阵的主成分（PC）方法，提出输出扰动和实例特定敏感方法，并设计了高效的PTR框架和PPM方法。

Result: 在大型真实网络（最多300万顶点）上验证了方法的隐私-效用平衡，PTR在运行时比PPM快180倍。

Conclusion: 论文首次为D$k$S问题提供DP保证，PTR框架在效率和实用性上表现优异。

Abstract: Many graph datasets involve sensitive network data, motivating the need for
privacy-preserving graph mining. The Densest-$k$-subgraph (D$k$S) problem is a
key primitive in graph mining that aims to extract a subset of $k$ vertices
with the maximum internal connectivity. Although non-private algorithms are
known for D$k$S, this paper is the first to design algorithms that offer formal
differential privacy (DP) guarantees for the problem. We base our general
approach on using the principal component (PC) of the graph adjacency matrix to
output a subset of $k$ vertices under edge DP. For this task, we first consider
output perturbation, which traditionally offer good scalability, but at the
expense of utility. Our tight on the local sensitivity indicate a big gap with
the global sensitivity, motivating the use of instance specific sensitive
methods for private PC. Next, we derive a tight bound on the smooth sensitivity
and show that it can be close to the global sensitivity. This leads us to
consider the Propose-Test-Release (PTR) framework for private PC. Although
computationally expensive in general, we design a novel approach for
implementing PTR in the same time as computation of a non-private PC, while
offering good utility for \DkS{}. Additionally, we also consider the iterative
private power method (PPM) for private PC, albeit it is significantly slower
than PTR on large networks. We run our methods on diverse real-world networks,
with the largest having 3 million vertices, and show good privacy-utility
trade-offs. Although PTR requires a slightly larger privacy budget, on average,
it achieves a 180-fold improvement in runtime over PPM.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [255] [Risk-sensitive Reinforcement Learning Based on Convex Scoring Functions](https://arxiv.org/abs/2505.04553)
*Shanyu Han,Yang Liu,Xiang Yu*

Main category: q-fin.MF

TL;DR: 提出了一种基于凸评分函数的强化学习框架，解决风险目标下的时间不一致性问题，并通过实验验证了算法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在强化学习中处理多种常见风险度量（如方差、期望短缺等），并解决时间不一致性问题。

Method: 采用增强状态空间和辅助变量，将问题转化为两状态优化问题，并提出定制化的Actor-Critic算法及辅助变量采样方法。

Result: 理论证明了算法在非连续马尔可夫决策过程中的适用性，并通过金融统计套利交易实验验证了算法的有效性。

Conclusion: 该框架为风险目标下的强化学习提供了通用解决方案，具有理论和实践意义。

Abstract: We propose a reinforcement learning (RL) framework under a broad class of
risk objectives, characterized by convex scoring functions. This class covers
many common risk measures, such as variance, Expected Shortfall, entropic
Value-at-Risk, and mean-risk utility. To resolve the time-inconsistency issue,
we consider an augmented state space and an auxiliary variable and recast the
problem as a two-state optimization problem. We propose a customized
Actor-Critic algorithm and establish some theoretical approximation guarantees.
A key theoretical contribution is that our results do not require the Markov
decision process to be continuous. Additionally, we propose an auxiliary
variable sampling method inspired by the alternating minimization algorithm,
which is convergent under certain conditions. We validate our approach in
simulation experiments with a financial application in statistical arbitrage
trading, demonstrating the effectiveness of the algorithm.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [256] [AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design](https://arxiv.org/abs/2505.03745)
*Yanbiao Liang,Huihong Shi,Haikuo Shao,Zhongfeng Wang*

Main category: cs.AR

TL;DR: AccLLM是一个通过算法与硬件协同设计加速大型语言模型（LLM）在边缘设备上部署的框架，解决了计算、内存和带宽需求高的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于LLM在自然语言处理领域的成功，需求从云端扩展到边缘设备，但资源受限的设备面临计算、内存和带宽的挑战。

Method: AccLLM结合算法层面的剪枝、Λ形注意力和W2A8KV4量化方案，以及硬件层面的FPGA加速器设计。

Result: 在Xilinx Alveo U280 FPGA上验证，能效提升4.07倍，吞吐量提升2.98倍。

Conclusion: AccLLM有效解决了LLM在边缘设备上的部署难题，为高效长序列生成提供了可行方案。

Abstract: Recently, large language models (LLMs) have achieved huge success in the
natural language processing (NLP) field, driving a growing demand to extend
their deployment from the cloud to edge devices. However, deploying LLMs on
resource-constrained edge devices poses significant challenges, including (1)
intensive computations and huge model sizes, (2) great memory and bandwidth
demands introduced by the autoregressive generation process, and (3) limited
scalability for handling long sequences. To address these challenges, we
propose AccLLM, a comprehensive acceleration framework that enables efficient
and fast long-context LLM inference through algorithm and hardware co-design.
At the algorithmic level, we integrate (1) pruning, (2) {\Lambda}-shaped
attention, and (3) an innovative W2A8KV4 (2-bit weights, 8-bit activations, and
4-bit KV cache) quantization scheme, thus effectively reducing memory and
bandwidth requirements while facilitating LLMs' long-sequence generation. At
the hardware level, we design a dedicated FPGA-based accelerator with a
reconfigurable computing engine to effectively and flexibly accommodate diverse
operations arising from our compression algorithm, thereby fully translating
the algorithmic innovations into tangible hardware efficiency. We validate
AccLLM on the Xilinx Alveo U280 FPGA, demonstrating a 4.07x energy efficiency
and a 2.98x throughput compared to the state-of-the-art work FlightLLM.

</details>


### [257] [APSQ: Additive Partial Sum Quantization with Algorithm-Hardware Co-Design](https://arxiv.org/abs/2505.03748)
*Yonghao Tan,Pingcheng Dong,Yongkun Wu,Yu Liu,Xuejiao Liu,Peng Luo,Shih-Yang Liu,Xijie Huang,Dong Zhang,Luhong Liang,Kwang-Ting Cheng*

Main category: cs.AR

TL;DR: 论文提出了一种新型的加法部分和量化（APSQ）方法，通过将部分和（PSUM）量化与数据流结合，显著降低内存需求和能耗。


<details>
  <summary>Details</summary>
Motivation: 传统压缩方法忽视了PSUM量化，而PSUM的高精度访问导致内存需求大且能耗高（占69%）。

Method: 提出APSQ方法，将PSUM累加纳入量化框架，并结合分组策略和可重构架构。

Result: 在BERT、Segformer和EfficientViT等模型上实现近乎无损的量化（压缩至INT8），能耗降低28-87%。LLaMA2-7B实验验证了其在大语言模型中的潜力。

Conclusion: APSQ是一种高效的PSUM量化方法，显著降低能耗且适用于多种模型。

Abstract: DNN accelerators, significantly advanced by model compression and specialized
dataflow techniques, have marked considerable progress. However, the frequent
access of high-precision partial sums (PSUMs) leads to excessive memory demands
in architectures utilizing input/weight stationary dataflows. Traditional
compression strategies have typically overlooked PSUM quantization, which may
account for 69% of power consumption. This study introduces a novel Additive
Partial Sum Quantization (APSQ) method, seamlessly integrating PSUM
accumulation into the quantization framework. A grouping strategy that combines
APSQ with PSUM quantization enhanced by a reconfigurable architecture is
further proposed. The APSQ performs nearly lossless on NLP and CV tasks across
BERT, Segformer, and EfficientViT models while compressing PSUMs to INT8. This
leads to a notable reduction in energy costs by 28-87%. Extended experiments on
LLaMA2-7B demonstrate the potential of APSQ for large language models. Code is
available at https://github.com/Yonghao-Tan/APSQ.

</details>


### [258] [AI-Powered Agile Analog Circuit Design and Optimization](https://arxiv.org/abs/2505.03750)
*Jinhai Hu,Wang Ling Goh,Yuan Gao*

Main category: cs.AR

TL;DR: 论文结合AI技术优化模拟电路设计，通过多目标贝叶斯优化（MOBO）和系统级建模，提升性能并减少设计迭代。


<details>
  <summary>Details</summary>
Motivation: 利用AI技术改进模拟电路设计，实现设备级调优和系统级协同优化。

Method: 1. 使用MOBO进行晶体管尺寸优化；2. 在关键词识别（KWS）应用中集成AI建模优化模拟带通滤波器。

Result: AI显著提升模拟电路性能，减少设计迭代，实现组件与应用指标的联合优化。

Conclusion: AI技术为模拟电路设计带来高效优化，展示了其在设备级和系统级的潜力。

Abstract: Artificial intelligence (AI) techniques are transforming analog circuit
design by automating device-level tuning and enabling system-level
co-optimization. This paper integrates two approaches: (1) AI-assisted
transistor sizing using Multi-Objective Bayesian Optimization (MOBO) for direct
circuit parameter optimization, demonstrated on a linearly tunable
transconductor; and (2) AI-integrated circuit transfer function modeling for
system-level optimization in a keyword spotting (KWS) application, demonstrated
by optimizing an analog bandpass filter within a machine learning training
loop. The combined insights highlight how AI can improve analog performance,
reduce design iteration effort, and jointly optimize analog components and
application-level metrics.

</details>


### [259] [Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management](https://arxiv.org/abs/2505.03756)
*Hang Zhang,Jiuchen Shi,Yixiao Wang,Quan Chen,Yizhou Shan,Minyi Guo*

Main category: cs.AR

TL;DR: FASTLIBRA是一种优化多LoRA推理性能的缓存系统，通过依赖感知的缓存管理和性能驱动的缓存交换，显著降低了首次令牌生成时间（TTFT）。


<details>
  <summary>Details</summary>
Motivation: 现有的多LoRA推理系统未能优化服务性能（如TTFT），且忽视了LoRA和KV缓存之间的使用依赖关系。

Method: FASTLIBRA包括依赖感知的缓存管理器和性能驱动的缓存交换器，统一管理LoRA和KV缓存的依赖关系，并根据统一成本模型进行交换。

Result: 实验表明，FASTLIBRA平均将TTFT降低了63.4%，优于现有技术。

Conclusion: FASTLIBRA通过优化缓存管理，显著提升了多LoRA推理的性能。

Abstract: Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for
task-specific Large Language Model (LLM) applications. For multi-LoRA serving,
caching hot KV caches and LoRA adapters in high bandwidth memory of
accelerations can improve inference performance. However, existing Multi-LoRA
inference systems fail to optimize serving performance like Time-To-First-Toke
(TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore
propose FASTLIBRA, a Multi-LoRA caching system to optimize the serving
performance. FASTLIBRA comprises a dependency-aware cache manager and a
performance-driven cache swapper. The cache manager maintains the usage
dependencies between LoRAs and KV caches during the inference with a unified
caching pool. The cache swapper determines the swap-in or out of LoRAs and KV
caches based on a unified cost model, when the HBM is idle or busy,
respectively. Experimental results show that ELORA reduces the TTFT by 63.4% on
average, compared to state-of-the-art works.

</details>


### [260] [Splitwiser: Efficient LM inference with constrained resources](https://arxiv.org/abs/2505.03763)
*Asad Aali,Adney Cardoza,Melissa Capo*

Main category: cs.AR

TL;DR: Splitwiser通过将LLM推理的两个阶段（提示计算和令牌生成）拆分到同一GPU上，减少开销并提高资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现有技术未能充分利用计算资源，尤其是在令牌生成阶段，导致效率低下。

Method: 提出Splitwiser方法，将两个阶段拆分到同一GPU上，减少数据传输开销，并开源了Huggingface和vLLM的实现。

Result: 初步结果显示Splitwiser能减少网络开销并提高资源利用率。

Conclusion: Splitwiser为LLM推理提供了一种高效的方法，有望进一步优化性能。

Abstract: Efficient inference of LLMs remains a crucial challenge, with two main
phases: a compute-intensive prompt computation and a memory-intensive token
generation. Despite existing batching and scheduling techniques, token
generation phases fail to fully utilize compute resources, especially when
compared to prompt computation phases. To address these challenges, we propose
Splitwiser, a methodology that splits the two phases of an LLM inference
request onto the same GPU, thereby reducing overhead and improving memory
access and cache utilization. By eliminating the need to transfer data across
devices, Splitwiser aims to minimize network-related overheads. In this report,
we describe the basic structure of our proposed pipeline while sharing
preliminary results and analysis. We implement our proposed multiprocessing
design on two widely-used and independent LLM architectures: Huggingface and
vLLM. We open-source our code for the respective implementations: 1)
Huggingface (https://github.com/asad-aali/splitwiser), and 2) vLLM
(https://github.com/adney11/vllm-sysml).

</details>


### [261] [GPU Performance Portability needs Autotuning](https://arxiv.org/abs/2505.03780)
*Burkhard Ringlein,Thomas Parnell,Radu Stoica*

Main category: cs.AR

TL;DR: 论文提出结合即时编译（JIT）和内核参数自动调优，实现无需代码修改即可移植且高性能的LLM执行，显著提升性能和多样性。


<details>
  <summary>Details</summary>
Motivation: 当前依赖单一平台限制了LLM的移植性，导致供应商锁定和AI硬件创新壁垒。

Method: 结合JIT编译和内核参数自动调优，以Flash Attention为例进行验证。

Result: 探索了15倍多的参数配置，代码多样性显著提升，性能超越供应商优化实现230%，同时代码量减少70倍。

Conclusion: 自动调优是实现LLM跨GPU供应商移植性的有效途径。

Abstract: As LLMs grow in complexity, achieving state-of-the-art performance requires
tight co-design across algorithms, software, and hardware. Today's reliance on
a single dominant platform limits portability, creates vendor lock-in, and
raises barriers for new AI hardware. In this work, we make the case for
combining just-in-time (JIT) compilation with kernel parameter autotuning to
enable portable, state-of-the-art performance LLM execution without code
changes. Focusing on flash attention -- a widespread performance-critical LLM
kernel -- we demonstrate that this approach explores up to 15x more kernel
parameter configurations, produces significantly more diverse code across
multiple dimensions, and even outperforms vendor-optimized implementations by
up to 230%, all while reducing kernel code size by 70x and eliminating manual
code optimizations. Our results highlight autotuning as a promising path to
unlocking model portability across GPU vendors.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [262] [Beyond Misinformation: A Conceptual Framework for Studying AI Hallucinations in (Science) Communication](https://arxiv.org/abs/2504.13777)
*Anqi Shao*

Main category: cs.HC

TL;DR: 本文提出一个概念框架，将AI幻觉视为一种独特的虚假信息形式，区别于传统的人类意图驱动的虚假信息。


<details>
  <summary>Details</summary>
Motivation: 传统虚假信息研究聚焦人类意图，而生成式AI系统产生的虚假输出缺乏意图，需重新审视其社会影响。

Method: 采用供需模型和分布式代理概念，分析AI幻觉在产生、感知和制度应对上与人类虚假信息的差异。

Result: 框架揭示了AI幻觉的独特性，并提出了从宏观、中观和微观层面研究其传播与接收的议程。

Conclusion: 呼吁传播学者重新思考虚假信息理论的边界，以应对非人类概率性行为体在知识生产中的日益嵌入。

Abstract: This paper proposes a conceptual framework for understanding AI
hallucinations as a distinct form of misinformation. While misinformation
scholarship has traditionally focused on human intent, generative AI systems
now produce false yet plausible outputs absent of such intent. I argue that
these AI hallucinations should not be treated merely as technical failures but
as communication phenomena with social consequences. Drawing on a
supply-and-demand model and the concept of distributed agency, the framework
outlines how hallucinations differ from human-generated misinformation in
production, perception, and institutional response. I conclude by outlining a
research agenda for communication scholars to investigate the emergence,
dissemination, and audience reception of hallucinated content, with attention
to macro (institutional), meso (group), and micro (individual) levels. This
work urges communication researchers to rethink the boundaries of
misinformation theory in light of probabilistic, non-human actors increasingly
embedded in knowledge production.

</details>


### [263] [Facilitating Video Story Interaction with Multi-Agent Collaborative System](https://arxiv.org/abs/2505.03807)
*Yiwen Zhang,Jianing Hao,Zhan Wang,Hongling Sheng,Wei Zeng*

Main category: cs.HC

TL;DR: 提出了一种基于用户意图的视频故事交互系统，结合VLM、RAG和MAS技术，通过三个阶段实现个性化体验。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于用户选择和预设叙事，缺乏定制化，需改进以提升交互体验。

Method: 系统分三阶段：1）视频故事处理（VLM）；2）多空间聊天（MAS）；3）场景定制（RAG）。

Result: 应用于《哈利·波特》系列，系统成功展现了角色社交行为和成长，提升交互体验。

Conclusion: 该系统通过技术整合有效实现了视频故事的个性化和动态交互。

Abstract: Video story interaction enables viewers to engage with and explore narrative
content for personalized experiences. However, existing methods are limited to
user selection, specially designed narratives, and lack customization. To
address this, we propose an interactive system based on user intent. Our system
uses a Vision Language Model (VLM) to enable machines to understand video
stories, combining Retrieval-Augmented Generation (RAG) and a Multi-Agent
System (MAS) to create evolving characters and scene experiences. It includes
three stages: 1) Video story processing, utilizing VLM and prior knowledge to
simulate human understanding of stories across three modalities. 2) Multi-space
chat, creating growth-oriented characters through MAS interactions based on
user queries and story stages. 3) Scene customization, expanding and
visualizing various story scenes mentioned in dialogue. Applied to the Harry
Potter series, our study shows the system effectively portrays emergent
character social behavior and growth, enhancing the interactive experience in
the video story world.

</details>


### [264] [Scratch Copilot: Supporting Youth Creative Coding with AI](https://arxiv.org/abs/2505.03867)
*Stefania Druga,Amy J. Ko*

Main category: cs.HC

TL;DR: Cognimates Scratch Copilot是一个面向儿童的AI编程助手，集成在Scratch环境中，支持实时创意、代码生成和调试。研究发现，儿童在使用中表现出强烈的自主性，同时揭示了设计上的平衡点。


<details>
  <summary>Details</summary>
Motivation: 尽管Scratch等平台为儿童提供了编程机会，但将创意转化为代码仍具挑战性。现有AI助手多面向成人，缺乏针对儿童的工具。

Method: 研究开发了Cognimates Scratch Copilot，并对其进行了18名国际儿童的定性评估。

Result: AI助手在创意和调试方面表现突出，儿童通过调整或拒绝建议保持创意控制，同时揭示了设计上的平衡点。

Conclusion: 研究展示了AI助手在提升儿童创意自信和参与度方面的潜力，并提出了优先考虑儿童自主性和批判性互动的设计指南。

Abstract: Creative coding platforms like Scratch have democratized programming for
children, yet translating imaginative ideas into functional code remains a
significant hurdle for many young learners. While AI copilots assist adult
programmers, few tools target children in block-based environments. Building on
prior research \cite{druga_how_2021,druga2023ai, druga2023scratch}, we present
Cognimates Scratch Copilot: an AI-powered assistant integrated into a
Scratch-like environment, providing real-time support for ideation, code
generation, debugging, and asset creation. This paper details the system
architecture and findings from an exploratory qualitative evaluation with 18
international children (ages 7--12). Our analysis reveals how the AI Copilot
supported key creative coding processes, particularly aiding ideation and
debugging. Crucially, it also highlights how children actively negotiated the
use of AI, demonstrating strong agency by adapting or rejecting suggestions to
maintain creative control. Interactions surfaced design tensions between
providing helpful scaffolding and fostering independent problem-solving, as
well as learning opportunities arising from navigating AI limitations and
errors. Findings indicate Cognimates Scratch Copilot's potential to enhance
creative self-efficacy and engagement. Based on these insights, we propose
initial design guidelines for AI coding assistants that prioritize youth agency
and critical interaction alongside supportive scaffolding.

</details>


### [265] [Steerable Chatbots: Personalizing LLMs with Preference-Based Activation Steering](https://arxiv.org/abs/2505.04260)
*Jessica Y. Bo,Tianyu Xu,Ishan Chatterjee,Katrina Passarella-Ward,Achin Kulshrestha,D Shin*

Main category: cs.HC

TL;DR: 论文提出了一种基于激活引导的方法，帮助LLM在推理时更贴合用户潜在偏好，提升个性化对话体验。


<details>
  <summary>Details</summary>
Motivation: 用户难以通过提示准确表达潜在偏好，影响AI助手的个性化效果。

Method: 利用激活引导技术，结合线性强度因子，嵌入三种交互式聊天机器人界面，并通过用户研究验证。

Result: 激活引导能有效对齐用户隐藏偏好，不同用户因控制、可用性和透明度偏好选择不同界面。

Conclusion: 偏好引导是一种轻量且有效的个性化方法，用户界面设计需考虑多样化需求。

Abstract: As large language models (LLMs) improve in their capacity to serve as
personal AI assistants, their ability to output uniquely tailored, personalized
responses that align with the soft preferences of their users is essential for
enhancing user satisfaction and retention. However, untrained lay users have
poor prompt specification abilities and often struggle with conveying their
latent preferences to AI assistants. To address this, we leverage activation
steering to guide LLMs to align with interpretable preference dimensions during
inference. In contrast to memory-based personalization methods that require
longer user history, steering is extremely lightweight and can be easily
controlled by the user via an linear strength factor. We embed steering into
three different interactive chatbot interfaces and conduct a within-subjects
user study (n=14) to investigate how end users prefer to personalize their
conversations. The results demonstrate the effectiveness of preference-based
steering for aligning real-world conversations with hidden user preferences,
and highlight further insights on how diverse values around control, usability,
and transparency lead users to prefer different interfaces.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [266] [GRAPE: Heterogeneous Graph Representation Learning for Genetic Perturbation with Coding and Non-Coding Biotype](https://arxiv.org/abs/2505.03853)
*Changxi Chi,Jun Xia,Jingbo Zhou,Jiabei Cheng,Chang Yu,Stan Z. Li*

Main category: q-bio.QM

TL;DR: 论文提出了一种名为GRAPE的异构图神经网络方法，通过结合预训练语言模型和DNA序列模型提取基因特征，并首次引入基因生物型信息，动态优化基因调控网络（GRN），显著提升了遗传扰动预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用基因相关信息，且忽略了生物型的功能差异，限制了基因互作的捕捉能力。因此，需要一种更全面的方法来构建精细化的GRN。

Method: 利用预训练语言模型和DNA序列模型分别提取基因描述和序列特征，结合基因生物型信息，通过图结构学习（GSL）动态优化GRN，提出GRAPE异构图神经网络。

Result: 在公开数据集上，GRAPE方法实现了最先进的性能。

Conclusion: GRAPE通过整合多源信息和动态优化，显著提升了遗传扰动预测的效率和准确性，为基因调控研究提供了新工具。

Abstract: Predicting genetic perturbations enables the identification of potentially
crucial genes prior to wet-lab experiments, significantly improving overall
experimental efficiency. Since genes are the foundation of cellular life,
building gene regulatory networks (GRN) is essential to understand and predict
the effects of genetic perturbations. However, current methods fail to fully
leverage gene-related information, and solely rely on simple evaluation metrics
to construct coarse-grained GRN. More importantly, they ignore functional
differences between biotypes, limiting the ability to capture potential gene
interactions. In this work, we leverage pre-trained large language model and
DNA sequence model to extract features from gene descriptions and DNA sequence
data, respectively, which serve as the initialization for gene representations.
Additionally, we introduce gene biotype information for the first time in
genetic perturbation, simulating the distinct roles of genes with different
biotypes in regulating cellular processes, while capturing implicit gene
relationships through graph structure learning (GSL). We propose GRAPE, a
heterogeneous graph neural network (HGNN) that leverages gene representations
initialized with features from descriptions and sequences, models the distinct
roles of genes with different biotypes, and dynamically refines the GRN through
GSL. The results on publicly available datasets show that our method achieves
state-of-the-art performance.

</details>


### [267] [Sparsity is All You Need: Rethinking Biological Pathway-Informed Approaches in Deep Learning](https://arxiv.org/abs/2505.04300)
*Isabella Caranzano,Corrado Pancotti,Cesare Rollo,Flavio Sartori,Pietro Liò,Piero Fariselli,Tiziana Sanavia*

Main category: q-bio.QM

TL;DR: 研究发现，生物信息神经网络中通路注释的性能提升可能源于稀疏性而非生物学相关性，随机化模型表现相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 验证通路注释在神经网络中的实际作用，区分其生物学相关性与稀疏性贡献。

Method: 全面分析通路神经网络模型，比较生物信息模型与随机化版本的表现。

Result: 随机化模型表现与生物信息模型相当，部分甚至更优，且未显示解释性优势。

Conclusion: 通路注释可能噪声较大或方法不足，提出新方法作为基准验证生物学贡献。

Abstract: Biologically-informed neural networks typically leverage pathway annotations
to enhance performance in biomedical applications. We hypothesized that the
benefits of pathway integration does not arise from its biological relevance,
but rather from the sparsity it introduces. We conducted a comprehensive
analysis of all relevant pathway-based neural network models for predictive
tasks, critically evaluating each study's contributions. From this review, we
curated a subset of methods for which the source code was publicly available.
The comparison of the biologically informed state-of-the-art deep learning
models and their randomized counterparts showed that models based on randomized
information performed equally well as biologically informed ones across
different metrics and datasets. Notably, in 3 out of the 15 analyzed models,
the randomized versions even outperformed their biologically informed
counterparts. Moreover, pathway-informed models did not show any clear
advantage in interpretability, as randomized models were still able to identify
relevant disease biomarkers despite lacking explicit pathway information. Our
findings suggest that pathway annotations may be too noisy or inadequately
explored by current methods. Therefore, we propose a methodology that can be
applied to different domains and can serve as a robust benchmark for
systematically comparing novel pathway-informed models against their randomized
counterparts. This approach enables researchers to rigorously determine whether
observed performance improvements can be attributed to biological insights.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [268] [Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance Computing Scheduling on Multi-User Systems](https://arxiv.org/abs/2505.03946)
*Matthew Sgambati,Aleksandar Vakanski,Matthew Anderson*

Main category: cs.DC

TL;DR: 本文提出了一种基于DD-PPO算法的RL调度器，解决了传统和现有RL调度算法在大规模数据集上的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: HPC环境中资源分配的复杂性要求更智能的调度算法，传统方法在异构和大规模系统中效率不足。

Method: 采用DD-PPO算法，支持多工作节点分布式训练，无需每一步参数同步。

Result: 实验使用1150万条HPC作业轨迹验证，DD-PPO在调度性能上优于传统和现有RL算法。

Conclusion: DD-PPO算法显著提升了调度效率和可扩展性，适用于大规模HPC系统。

Abstract: Resource allocation in High Performance Computing (HPC) environments presents
a complex and multifaceted challenge for job scheduling algorithms. Beyond the
efficient allocation of system resources, schedulers must account for and
optimize multiple performance metrics, including job wait time and system
utilization. While traditional rule-based scheduling algorithms dominate the
current deployments of HPC systems, the increasing heterogeneity and scale of
those systems is expected to challenge the efficiency and flexibility of those
algorithms in minimizing job wait time and maximizing utilization. Recent
research efforts have focused on leveraging advancements in Reinforcement
Learning (RL) to develop more adaptable and intelligent scheduling strategies.
Recent RL-based scheduling approaches have explored a range of algorithms, from
Deep Q-Networks (DQN) to Proximal Policy Optimization (PPO), and more recently,
hybrid methods that integrate Graph Neural Networks with RL techniques.
However, a common limitation across these methods is their reliance on
relatively small datasets, and these methods face scalability issues when using
large datasets. This study introduces a novel RL-based scheduler utilizing the
Decentralized Distributed Proximal Policy Optimization (DD-PPO) algorithm,
which supports large-scale distributed training across multiple workers without
requiring parameter synchronization at every step. By eliminating reliance on
centralized updates to a shared policy, the DD-PPO scheduler enhances
scalability, training efficiency, and sample utilization. The validation
dataset leveraged over 11.5 million real HPC job traces for comparing DD-PPO
performance between traditional and advanced scheduling approaches, and the
experimental results demonstrate improved scheduling performance in comparison
to both rule-based schedulers and existing RL-based scheduling algorithms.

</details>


### [269] [Can Large Language Models Predict Parallel Code Performance?](https://arxiv.org/abs/2505.03988)
*Gregory Bolet,Giorgis Georgakoudis,Harshitha Menon,Konstantinos Parasyris,Niranjan Hasabnis,Hayden Estes,Kirk W. Cameron,Gal Oren*

Main category: cs.DC

TL;DR: 论文探讨了是否能用大型语言模型（LLM）替代硬件执行时间分析来预测GPU性能，通过将问题定义为Roofline分类任务，评估了LLM在不同场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 由于高端GPU访问受限，传统性能分析方法变得困难，研究探索LLM是否能提供无需硬件的替代方案。

Method: 构建了340个GPU内核的平衡数据集，评估LLM在四种场景下的性能预测能力：带分析数据、零样本、少样本和微调。

Result: LLM在提供明确分析数据时分类准确率达100%，推理能力强的LLM在零样本和少样本设置中表现显著优于标准LLM，微调需更多数据。

Conclusion: LLM在性能预测方面具有潜力，未来可通过更好的数据集和提示策略成为HPC性能分析实用工具。

Abstract: Accurate determination of the performance of parallel GPU code typically
requires execution-time profiling on target hardware -- an increasingly
prohibitive step due to limited access to high-end GPUs. This paper explores
whether Large Language Models (LLMs) can offer an alternative approach for GPU
performance prediction without relying on hardware. We frame the problem as a
roofline classification task: given the source code of a GPU kernel and the
hardware specifications of a target GPU, can an LLM predict whether the GPU
kernel is compute-bound or bandwidth-bound?
  For this study, we build a balanced dataset of 340 GPU kernels, obtained from
HeCBench benchmark and written in CUDA and OpenMP, along with their
ground-truth labels obtained via empirical GPU profiling. We evaluate LLMs
across four scenarios: (1) with access to profiling data of the kernel source,
(2) zero-shot with source code only, (3) few-shot with code and label pairs,
and (4) fine-tuned on a small custom dataset.
  Our results show that state-of-the-art LLMs have a strong understanding of
the Roofline model, achieving 100% classification accuracy when provided with
explicit profiling data. We also find that reasoning-capable LLMs significantly
outperform standard LLMs in zero- and few-shot settings, achieving up to 64%
accuracy on GPU source codes, without profiling information. Lastly, we find
that LLM fine-tuning will require much more data than what we currently have
available.
  This work is among the first to use LLMs for source-level roofline
performance prediction via classification, and illustrates their potential to
guide optimization efforts when runtime profiling is infeasible. Our findings
suggest that with better datasets and prompt strategies, LLMs could become
practical tools for HPC performance analysis and performance portability.

</details>


### [270] [Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving](https://arxiv.org/abs/2505.04021)
*Shan Yu,Jiarong Xing,Yifan Qiao,Mingyuan Ma,Yangmin Li,Yang Wang,Shuo Yang,Zhiqiang Xie,Shiyi Cao,Ke Bao,Ion Stoica,Harry Xu,Ying Sheng*

Main category: cs.DC

TL;DR: Prism是一个多LLM服务系统，通过GPU共享和动态内存协调实现成本节约和SLO达标。


<details>
  <summary>Details</summary>
Motivation: 降低大型语言模型（LLM）服务成本，同时满足动态工作负载下的延迟SLO。

Method: 支持按需内存分配和两级调度策略，动态调整GPU共享策略。

Result: 相比现有系统，Prism节省2倍成本，SLO达标率提高3.3倍。

Conclusion: Prism通过跨模型内存协调和动态调度，显著提升了多LLM服务的效率和性能。

Abstract: Serving large language models (LLMs) is expensive, especially for providers
hosting many models, making cost reduction essential. The unique workload
patterns of serving multiple LLMs (i.e., multi-LLM serving) create new
opportunities and challenges for this task. The long-tail popularity of models
and their long idle periods present opportunities to improve utilization
through GPU sharing. However, existing GPU sharing systems lack the ability to
adjust their resource allocation and sharing policies at runtime, making them
ineffective at meeting latency service-level objectives (SLOs) under rapidly
fluctuating workloads.
  This paper presents Prism, a multi-LLM serving system that unleashes the full
potential of GPU sharing to achieve both cost efficiency and SLO attainment. At
its core, Prism tackles a key limitation of existing
systems$\unicode{x2014}$the lack of $\textit{cross-model memory coordination}$,
which is essential for flexibly sharing GPU memory across models under dynamic
workloads. Prism achieves this with two key designs. First, it supports
on-demand memory allocation by dynamically mapping physical to virtual memory
pages, allowing flexible memory redistribution among models that space- and
time-share a GPU. Second, it improves memory efficiency through a two-level
scheduling policy that dynamically adjusts sharing strategies based on models'
runtime demands. Evaluations on real-world traces show that Prism achieves more
than $2\times$ cost savings and $3.3\times$ SLO attainment compared to
state-of-the-art systems.

</details>


### [271] [MARCO: A Multi-Agent System for Optimizing HPC Code Generation Using Large Language Models](https://arxiv.org/abs/2505.03906)
*Asif Rahman,Veljko Cvetkovic,Kathleen Reece,Aidan Walters,Yasir Hassan,Aneesh Tummeti,Bryan Torres,Denise Cooney,Margaret Ellis,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: MARCO框架通过多智能体架构优化LLM生成的HPC代码，结合实时网络搜索技术，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 通用LLM在高性能计算（HPC）代码生成中因缺乏专业优化而表现不佳，需针对性解决方案。

Method: MARCO采用多智能体架构，包括代码生成和性能评估代理，通过反馈循环和实时网络搜索优化代码。

Result: 在LeetCode 75问题集上，MARCO平均运行时减少14.6%，结合网络搜索后性能提升30.9%。

Conclusion: 多智能体系统为HPC代码生成提供高效替代方案，无需领域特定模型微调。

Abstract: Large language models (LLMs) have transformed software development through
code generation capabilities, yet their effectiveness for high-performance
computing (HPC) remains limited. HPC code requires specialized optimizations
for parallelism, memory efficiency, and architecture-specific considerations
that general-purpose LLMs often overlook. We present MARCO (Multi-Agent
Reactive Code Optimizer), a novel framework that enhances LLM-generated code
for HPC through a specialized multi-agent architecture. MARCO employs separate
agents for code generation and performance evaluation, connected by a feedback
loop that progressively refines optimizations. A key innovation is MARCO's
web-search component that retrieves real-time optimization techniques from
recent conference proceedings and research publications, bridging the knowledge
gap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem
set demonstrates that MARCO achieves a 14.6% average runtime reduction compared
to Claude 3.5 Sonnet alone, while the integration of the web-search component
yields a 30.9% performance improvement over the base MARCO system. These
results highlight the potential of multi-agent systems to address the
specialized requirements of high-performance code generation, offering a
cost-effective alternative to domain-specific model fine-tuning.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [272] [A Heuristic-Integrated DRL Approach for Phase Optimization in Large-Scale RISs](https://arxiv.org/abs/2505.04401)
*Wei Wang,Peizheng Li,Angela Doufexi,Mark A. Beach*

Main category: eess.SP

TL;DR: 提出了一种结合启发式算法和深度强化学习（DRL）的框架，用于优化大规模可重构智能表面（RIS）的离散相移配置。


<details>
  <summary>Details</summary>
Motivation: 由于RIS的离散相移优化问题具有非凸和非线性特性，传统方法难以高效解决。

Method: 结合双深度Q网络（DDQN）和贪婪算法（GA），通过多步累积动作实现RIS列级控制，并在每一步DRL中集成GA进行细粒度优化。

Result: 该方法在小规模DRL动作空间中有效优化了大规模RIS的相移配置。

Conclusion: 提出的框架为大规模RIS优化提供了一种高效解决方案。

Abstract: Optimizing discrete phase shifts in large-scale reconfigurable intelligent
surfaces (RISs) is challenging due to their non-convex and non-linear nature.
In this letter, we propose a heuristic-integrated deep reinforcement learning
(DRL) framework that (1) leverages accumulated actions over multiple steps in
the double deep Q-network (DDQN) for RIS column-wise control and (2) integrates
a greedy algorithm (GA) into each DRL step to refine the state via
fine-grained, element-wise optimization of RIS configurations. By learning from
GA-included states, the proposed approach effectively addresses RIS
optimization within a small DRL action space, demonstrating its capability to
optimize phase-shift configurations of large-scale RISs.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [273] [Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration](https://arxiv.org/abs/2505.04457)
*Shigeki Karita,Yuma Koizumi,Heiga Zen,Haruko Ishikawa,Robin Scheibler,Michiel Bacchiani*

Main category: cs.SD

TL;DR: Miipher-2是一种用于大规模生成模型训练数据清理的语音恢复模型，支持300多种语言，无需显式条件输入，计算高效。


<details>
  <summary>Details</summary>
Motivation: 解决大规模生成模型训练数据清理中的泛化、无条件和计算效率问题。

Method: 利用预训练的通用语音模型（USM）作为特征提取器，结合并行适配器和WaneFit神经声码器。

Result: 在词错误率、说话人相似性和音质评分上优于或媲美传统模型，计算效率高。

Conclusion: Miipher-2为大规模语音数据清理提供了高效、通用的解决方案。

Abstract: Training data cleaning is a new application for generative model-based speech
restoration (SR). This paper introduces Miipher-2, an SR model designed for
million-hour scale data, for training data cleaning for large-scale generative
models like large language models. Key challenges addressed include
generalization to unseen languages, operation without explicit conditioning
(e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a
frozen, pre-trained Universal Speech Model (USM), supporting over 300
languages, as a robust, conditioning-free feature extractor. To optimize
efficiency and minimize memory, Miipher-2 incorporates parallel adapters for
predicting clean USM features from noisy inputs and employs the WaneFit neural
vocoder for waveform synthesis. These components were trained on 3,000 hours of
multi-lingual, studio-quality recordings with augmented degradations, while USM
parameters remained fixed. Experimental results demonstrate Miipher-2's
superior or comparable performance to conventional SR models in
word-error-rate, speaker similarity, and both objective and subjective sound
quality scores across all tested languages. Miipher-2 operates efficiently on
consumer-grade accelerators, achieving a real-time factor of 0.0078, enabling
the processing of a million-hour speech dataset in approximately three days
using only 100 such accelerators.

</details>


### [274] [Automatic Music Transcription using Convolutional Neural Networks and Constant-Q transform](https://arxiv.org/abs/2505.04451)
*Yohannis Telila,Tommaso Cucinotta,Davide Bacciu*

Main category: cs.SD

TL;DR: 本文提出了一种基于卷积神经网络（CNN）的自动音乐转录（AMT）方法，用于将古典钢琴音频文件转换为乐谱表示。


<details>
  <summary>Details</summary>
Motivation: 自动音乐转录（AMT）在处理复调音乐时具有挑战性，目标是分析包含多个同时演奏音符的音频信号，生成乐谱表示。

Method: 使用恒定Q变换提取音频信号特征，并将结果系数输入卷积神经网络（CNN）模型进行处理。

Result: 设计了一个处理流程，能够将.wav格式的古典钢琴音频文件转换为乐谱表示。

Conclusion: 该方法为复调音乐的自动转录提供了一种有效的解决方案。

Abstract: Automatic music transcription (AMT) is the problem of analyzing an audio
recording of a musical piece and detecting notes that are being played. AMT is
a challenging problem, particularly when it comes to polyphonic music. The goal
of AMT is to produce a score representation of a music piece, by analyzing a
sound signal containing multiple notes played simultaneously. In this work, we
design a processing pipeline that can transform classical piano audio files in
.wav format into a music score representation. The features from the audio
signals are extracted using the constant-Q transform, and the resulting
coefficients are used as an input to the convolutional neural network (CNN)
model.

</details>


### [275] [Score Distillation Sampling for Audio: Source Separation, Synthesis, and Beyond](https://arxiv.org/abs/2505.04621)
*Jessie Richter-Powell,Antonio Torralba,Jonathan Lorraine*

Main category: cs.SD

TL;DR: Audio-SDS将Score Distillation Sampling（SDS）推广到音频领域，利用预训练模型实现多种任务，如物理模拟、参数校准和音源分离。


<details>
  <summary>Details</summary>
Motivation: 将SDS的核心思想扩展到音频领域，利用生成先验解决多种音频任务，避免依赖专用数据集。

Method: 基于预训练的文本条件音频扩散模型，通过蒸馏生成先验实现任务通用性。

Result: 成功应用于物理模拟、FM合成参数校准和音源分离，展示了方法的跨模态适应性。

Conclusion: Audio-SDS为音频任务中的生成先验应用提供了通用且强大的基础。

Abstract: We introduce Audio-SDS, a generalization of Score Distillation Sampling (SDS)
to text-conditioned audio diffusion models. While SDS was initially designed
for text-to-3D generation using image diffusion, its core idea of distilling a
powerful generative prior into a separate parametric representation extends to
the audio domain. Leveraging a single pretrained model, Audio-SDS enables a
broad range of tasks without requiring specialized datasets. In particular, we
demonstrate how Audio-SDS can guide physically informed impact sound
simulations, calibrate FM-synthesis parameters, and perform prompt-specified
source separation. Our findings illustrate the versatility of
distillation-based methods across modalities and establish a robust foundation
for future work using generative priors in audio tasks.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [276] [TerraFusion: Joint Generation of Terrain Geometry and Texture Using Latent Diffusion Models](https://arxiv.org/abs/2505.04050)
*Kazuki Higo,Toshiki Kanai,Yuki Endo,Yoshihiro Kanamori*

Main category: cs.GR

TL;DR: 提出了一种联合生成地形高度图和纹理的潜在扩散模型方法，通过无监督学习和有监督学习实现用户控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常单独生成高度图或纹理，未能充分捕捉两者之间的相关性，影响了真实感。

Method: 使用潜在扩散模型无监督生成配对高度图和纹理，并通过有监督学习的外部适配器实现用户控制。

Result: 实验表明，该方法能直观生成地形并保持高度图与纹理的相关性。

Conclusion: 该方法有效解决了地形生成中高度图与纹理的关联问题，提升了真实感和用户控制能力。

Abstract: 3D terrain models are essential in fields such as video game development and
film production. Since surface color often correlates with terrain geometry,
capturing this relationship is crucial to achieving realism. However, most
existing methods generate either a heightmap or a texture, without sufficiently
accounting for the inherent correlation. In this paper, we propose a method
that jointly generates terrain heightmaps and textures using a latent diffusion
model. First, we train the model in an unsupervised manner to randomly generate
paired heightmaps and textures. Then, we perform supervised learning of an
external adapter to enable user control via hand-drawn sketches. Experiments
show that our approach allows intuitive terrain generation while preserving the
correlation between heightmaps and textures.

</details>


### [277] [Person-In-Situ: Scene-Consistent Human Image Insertion with Occlusion-Aware Pose Control](https://arxiv.org/abs/2505.04052)
*Shun Masuda,Yuki Endo,Yoshihiro Kanamori*

Main category: cs.GR

TL;DR: 论文提出两种方法，通过3D人体模型控制姿势，并利用潜在扩散模型合成人物，解决现有方法在遮挡和深度问题上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在人物插入场景时难以处理遮挡问题，且无法自然地将人物放置在适当深度，同时对姿势控制有限。

Method: 提出两种方法：1）两阶段方法，先通过监督学习生成场景深度图，再合成人物；2）直接通过输入数据隐式学习遮挡并合成人物。

Result: 定量和定性评估表明，两种方法在场景一致性和遮挡处理上优于现有方法。

Conclusion: 新方法在人物插入场景时能更自然地处理遮挡和深度问题，同时提供精确的姿势控制。

Abstract: Compositing human figures into scene images has broad applications in areas
such as entertainment and advertising. However, existing methods often cannot
handle occlusion of the inserted person by foreground objects and unnaturally
place the person in the frontmost layer. Moreover, they offer limited control
over the inserted person's pose. To address these challenges, we propose two
methods. Both allow explicit pose control via a 3D body model and leverage
latent diffusion models to synthesize the person at a contextually appropriate
depth, naturally handling occlusions without requiring occlusion masks. The
first is a two-stage approach: the model first learns a depth map of the scene
with the person through supervised learning, and then synthesizes the person
accordingly. The second method learns occlusion implicitly and synthesizes the
person directly from input data without explicit depth supervision.
Quantitative and qualitative evaluations show that both methods outperform
existing approaches by better preserving scene consistency while accurately
reflecting occlusions and user-specified poses.

</details>


### [278] [Geometry-Aware Texture Generation for 3D Head Modeling with Artist-driven Control](https://arxiv.org/abs/2505.04387)
*Amin Fadaeinejad,Abdallah Dib,Luiz Gustavo Hafemann,Emeline Got,Trevor Anderson,Amaury Depierre,Nikolaus F. Troje,Marcus A. Brubaker,Marc-André Carbonneau*

Main category: cs.GR

TL;DR: 提出了一种新型框架，通过几何感知的纹理合成流程，简化3D头部资产的创建，提供三种级别的艺术控制。


<details>
  <summary>Details</summary>
Motivation: 为虚拟角色创建符合精确艺术愿景的3D头部资产仍是一项劳动密集型任务，需要更高效的工具。

Method: 采用几何感知的纹理合成流程，学习头部几何与皮肤纹理之间的相关性，支持整体几何、肤色和细节编辑。

Result: 实验表明，该方法能生成多样化的结果并保持几何清洁，适用于肤色调整和细节编辑。

Conclusion: 该框架通过提供直观的艺术控制，简化了虚拟角色创建的流程。

Abstract: Creating realistic 3D head assets for virtual characters that match a precise
artistic vision remains labor-intensive. We present a novel framework that
streamlines this process by providing artists with intuitive control over
generated 3D heads. Our approach uses a geometry-aware texture synthesis
pipeline that learns correlations between head geometry and skin texture maps
across different demographics. The framework offers three levels of artistic
control: manipulation of overall head geometry, adjustment of skin tone while
preserving facial characteristics, and fine-grained editing of details such as
wrinkles or facial hair. Our pipeline allows artists to make edits to a single
texture map using familiar tools, with our system automatically propagating
these changes coherently across the remaining texture maps needed for realistic
rendering. Experiments demonstrate that our method produces diverse results
with clean geometries. We showcase practical applications focusing on intuitive
control for artists, including skin tone adjustments and simplified editing
workflows for adding age-related details or removing unwanted features from
scanned models. This integrated approach aims to streamline the artistic
workflow in virtual character creation.

</details>


### [279] [PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers](https://arxiv.org/abs/2505.04002)
*Michael Xu,Yi Shi,KangKang Yin,Xue Bin Peng*

Main category: cs.GR

TL;DR: PARC框架通过机器学习和物理模拟迭代增强运动数据集，解决敏捷地形穿越控制器开发中的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 人类在复杂环境中的敏捷运动能力难以通过模拟复现，主要因运动捕捉数据稀缺且获取成本高。

Method: PARC结合机器学习与物理模拟，通过迭代训练运动生成器和跟踪控制器，逐步扩展数据集和能力。

Result: PARC生成了敏捷且多功能的模型，能够适应复杂环境，弥补了数据稀缺与控制器需求之间的差距。

Conclusion: PARC为敏捷地形穿越控制器的开发提供了有效方法，解决了数据不足的挑战。

Abstract: Humans excel in navigating diverse, complex environments with agile motor
skills, exemplified by parkour practitioners performing dynamic maneuvers, such
as climbing up walls and jumping across gaps. Reproducing these agile movements
with simulated characters remains challenging, in part due to the scarcity of
motion capture data for agile terrain traversal behaviors and the high cost of
acquiring such data. In this work, we introduce PARC (Physics-based
Augmentation with Reinforcement Learning for Character Controllers), a
framework that leverages machine learning and physics-based simulation to
iteratively augment motion datasets and expand the capabilities of terrain
traversal controllers. PARC begins by training a motion generator on a small
dataset consisting of core terrain traversal skills. The motion generator is
then used to produce synthetic data for traversing new terrains. However, these
generated motions often exhibit artifacts, such as incorrect contacts or
discontinuities. To correct these artifacts, we train a physics-based tracking
controller to imitate the motions in simulation. The corrected motions are then
added to the dataset, which is used to continue training the motion generator
in the next iteration. PARC's iterative process jointly expands the
capabilities of the motion generator and tracker, creating agile and versatile
models for interacting with complex environments. PARC provides an effective
approach to develop controllers for agile terrain traversal, which bridges the
gap between the scarcity of motion data and the need for versatile character
controllers.

</details>


### [280] [TetWeave: Isosurface Extraction using On-The-Fly Delaunay Tetrahedral Grids for Gradient-Based Mesh Optimization](https://arxiv.org/abs/2505.04590)
*Alexandre Binninger,Ruben Wiersma,Philipp Herholz,Olga Sorkine-Hornung*

Main category: cs.GR

TL;DR: TetWeave是一种新型等值面表示方法，通过联合优化四面体网格和方向有符号距离，实现高质量、自适应的网格生成。


<details>
  <summary>Details</summary>
Motivation: 传统预定义网格在灵活性和内存效率上存在不足，TetWeave旨在提供一种更灵活、高效的网格优化方法。

Method: 利用Delaunay三角剖分动态构建四面体网格，并结合方向有符号距离优化，确保网格水密、无交叉。

Result: TetWeave生成高质量网格，内存占用低，适用于多视图3D重建、网格压缩等任务。

Conclusion: TetWeave在灵活性和效率上优于传统方法，适用于多种图形和视觉任务。

Abstract: We introduce TetWeave, a novel isosurface representation for gradient-based
mesh optimization that jointly optimizes the placement of a tetrahedral grid
used for Marching Tetrahedra and a novel directional signed distance at each
point. TetWeave constructs tetrahedral grids on-the-fly via Delaunay
triangulation, enabling increased flexibility compared to predefined grids. The
extracted meshes are guaranteed to be watertight, two-manifold and
intersection-free. The flexibility of TetWeave enables a resampling strategy
that places new points where reconstruction error is high and allows to
encourage mesh fairness without compromising on reconstruction error. This
leads to high-quality, adaptive meshes that require minimal memory usage and
few parameters to optimize. Consequently, TetWeave exhibits near-linear memory
scaling relative to the vertex count of the output mesh - a substantial
improvement over predefined grids. We demonstrate the applicability of TetWeave
to a broad range of challenging tasks in computer graphics and vision, such as
multi-view 3D reconstruction, mesh compression and geometric texture
generation.

</details>


### [281] [PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive Transformer](https://arxiv.org/abs/2505.04622)
*Jingwen Ye,Yuze He,Yanning Zhou,Yiqin Zhu,Kaiwen Xiao,Yong-Jin Liu,Wei Yang,Xiao Han*

Main category: cs.GR

TL;DR: PrimitiveAnything是一个新框架，将形状基元抽象任务重新定义为基元组装生成任务，通过大规模人类制作的抽象学习，生成高质量且符合人类感知的基元组装。


<details>
  <summary>Details</summary>
Motivation: 现有基元抽象方法在语义理解或泛化能力上存在局限，无法适应多样化的形状类别。

Method: 提出形状条件基元变换器进行自回归生成，并使用无歧义参数化方案统一表示多种基元类型。

Result: 实验表明，PrimitiveAnything能生成高质量且几何保真的基元组装，适用于多样化形状类别。

Conclusion: 该框架在3D应用中表现优异，并有望支持游戏中的基元用户生成内容。

Abstract: Shape primitive abstraction, which decomposes complex 3D shapes into simple
geometric elements, plays a crucial role in human visual cognition and has
broad applications in computer vision and graphics. While recent advances in 3D
content generation have shown remarkable progress, existing primitive
abstraction methods either rely on geometric optimization with limited semantic
understanding or learn from small-scale, category-specific datasets, struggling
to generalize across diverse shape categories. We present PrimitiveAnything, a
novel framework that reformulates shape primitive abstraction as a primitive
assembly generation task. PrimitiveAnything includes a shape-conditioned
primitive transformer for auto-regressive generation and an ambiguity-free
parameterization scheme to represent multiple types of primitives in a unified
manner. The proposed framework directly learns the process of primitive
assembly from large-scale human-crafted abstractions, enabling it to capture
how humans decompose complex shapes into primitive elements. Through extensive
experiments, we demonstrate that PrimitiveAnything can generate high-quality
primitive assemblies that better align with human perception while maintaining
geometric fidelity across diverse shape categories. It benefits various 3D
applications and shows potential for enabling primitive-based user-generated
content (UGC) in games. Project page: https://primitiveanything.github.io

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [282] [EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.04623)
*Zhenghao Xing,Xiaowei Hu,Chi-Wing Fu,Wenhai Wang,Jifeng Dai,Pheng-Ann Heng*

Main category: eess.AS

TL;DR: EchoInk-R1是一个基于强化学习的框架，用于提升多模态大语言模型（MLLMs）在音频和视觉信号的结构化跨模态推理能力，并在AVQA-R1-6K数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs在音频和视觉信号的跨模态推理上表现不足，EchoInk-R1旨在通过强化学习优化这一能力。

Method: 基于Qwen2.5-Omni-7B模型，采用Group Relative Policy Optimization（GRPO）进行强化学习优化，专注于同步音频-图像对的多选题回答任务。

Result: EchoInk-R1-7B在验证集上达到85.77%的准确率，优于基础模型的80.53%，且仅需562步强化学习。

Conclusion: 轻量级强化学习微调可显著提升MLLMs的跨模态推理能力，EchoInk-R1是首个通过强化学习统一音频、视觉和文本模态的框架。

Abstract: Multimodal large language models (MLLMs) have advanced perception across
text, vision, and audio, yet they often struggle with structured cross-modal
reasoning, particularly when integrating audio and visual signals. We introduce
EchoInk-R1, a reinforcement learning framework that enhances such reasoning in
MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group
Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice
question answering over synchronized audio-image pairs. To enable this, we
curate AVQA-R1-6K, a dataset pairing such audio-image inputs with
multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves
85.77% accuracy on the validation set, outperforming the base model, which
scores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,
EchoInk-R1 demonstrates reflective reasoning by revisiting initial
interpretations and refining responses when facing ambiguous multimodal inputs.
These results suggest that lightweight reinforcement learning fine-tuning
enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to
unify audio, visual, and textual modalities for general open-world reasoning
via reinforcement learning. Code and data are publicly released to facilitate
further research.

</details>


### [283] [Recognizing Ornaments in Vocal Indian Art Music with Active Annotation](https://arxiv.org/abs/2505.04419)
*Sumit Kumar,Parampreet Singh,Vipul Arora*

Main category: eess.AS

TL;DR: 论文介绍了ROD数据集和基于深度时间序列分析的装饰音检测模型，优于基线CRNN。


<details>
  <summary>Details</summary>
Motivation: 装饰音对旋律表达至关重要，但缺乏标注数据集和专门建模方法阻碍了研究进展。

Method: 使用ROD数据集和深度时间序列分析模型，保留装饰音边界。

Result: 实验表明该方法优于基线CRNN。

Conclusion: ROD数据集和提出的模型为装饰音检测提供了有效工具。

Abstract: Ornamentations, embellishments, or microtonal inflections are essential to
melodic expression across many musical traditions, adding depth, nuance, and
emotional impact to performances. Recognizing ornamentations in singing voices
is key to MIR, with potential applications in music pedagogy, singer
identification, genre classification, and controlled singing voice generation.
However, the lack of annotated datasets and specialized modeling approaches
remains a major obstacle for progress in this research area. In this work, we
introduce R\=aga Ornamentation Detection (ROD), a novel dataset comprising
Indian classical music recordings curated by expert musicians. The dataset is
annotated using a custom Human-in-the-Loop tool for six vocal ornaments marked
as event-based labels. Using this dataset, we develop an ornamentation
detection model based on deep time-series analysis, preserving ornament
boundaries during the chunking of long audio recordings. We conduct experiments
using different train-test configurations within the ROD dataset and also
evaluate our approach on a separate, manually annotated dataset of Indian
classical concert recordings. Our experimental results support the superior
performance of our proposed approach over the baseline CRNN.

</details>


### [284] [Discrete Optimal Transport and Voice Conversion](https://arxiv.org/abs/2505.04382)
*Anton Selitskiy,Maitreya Kocharekar*

Main category: eess.AS

TL;DR: 本文提出了一种基于向量接口的语音转换方法，采用离散最优传输映射对齐音频嵌入，验证了其高质量和有效性，并发现该方法可能导致合成音频被误判为真实音频。


<details>
  <summary>Details</summary>
Motivation: 解决语音转换任务中音频嵌入对齐的问题。

Method: 使用离散最优传输映射对齐不同说话者的音频嵌入。

Result: 方法表现出高质量和有效性，但可能导致合成音频被误判为真实音频。

Conclusion: 离散最优传输映射在语音转换中有效，但需注意其潜在的误判风险。

Abstract: In this work, we address the voice conversion (VC) task using a vector-based
interface. To align audio embeddings between speakers, we employ discrete
optimal transport mapping. Our evaluation results demonstrate the high quality
and effectiveness of this method. Additionally, we show that applying discrete
optimal transport as a post-processing step in audio generation can lead to the
incorrect classification of synthetic audio as real.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [285] [IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation and Classification](https://arxiv.org/abs/2505.03838)
*Ting Yu Tsai,An Yu,Meghana Spurthi Maadugundu,Ishrat Jahan Mohima,Umme Habiba Barsha,Mei-Hwa F. Chen,Balakrishnan Prabhakaran,Ming-Ching Chang*

Main category: eess.IV

TL;DR: IntelliCardiac是一个基于AI的4D心脏图像自动分割和疾病分类平台，准确率高，适用于临床决策支持。


<details>
  <summary>Details</summary>
Motivation: 心脏影像数据的精确处理对心血管疾病识别和管理至关重要，现有方法性能不足。

Method: 结合深度学习分割模型和两步分类流程，利用ACDC数据集训练。

Result: 分割准确率92.6%，分类准确率98%，优于现有方法。

Conclusion: IntelliCardiac具有临床潜力，可作为心脏影像诊断的精准工具。

Abstract: Precise and effective processing of cardiac imaging data is critical for the
identification and management of the cardiovascular diseases. We introduce
IntelliCardiac, a comprehensive, web-based medical image processing platform
for the automatic segmentation of 4D cardiac images and disease classification,
utilizing an AI model trained on the publicly accessible ACDC dataset. The
system, intended for patients, cardiologists, and healthcare professionals,
offers an intuitive interface and uses deep learning models to identify
essential heart structures and categorize cardiac diseases. The system supports
analysis of both the right and left ventricles as well as myocardium, and then
classifies patient's cardiac images into five diagnostic categories: dilated
cardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right
ventricular abnormality, and no disease. IntelliCardiac combines a deep
learning-based segmentation model with a two-step classification pipeline. The
segmentation module gains an overall accuracy of 92.6\%. The classification
module, trained on characteristics taken from segmented heart structures,
achieves 98\% accuracy in five categories. These results exceed the performance
of the existing state-of-the-art methods that integrate both segmentation and
classification models. IntelliCardiac, which supports real-time visualization,
workflow integration, and AI-assisted diagnostics, has great potential as a
scalable, accurate tool for clinical decision assistance in cardiac imaging and
diagnosis.

</details>


### [286] [From Spaceborn to Airborn: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation](https://arxiv.org/abs/2505.03844)
*Solène Debuysère,Nicolas Trouvé,Nathan Letheule,Olivier Lévêque,Elise Colin*

Main category: eess.IV

TL;DR: 论文提出了一种利用预训练潜在扩散模型和空间条件技术，将卫星SAR图像转换为机载SAR表示的新方法，填补了公开SAR数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 高分辨率机载SAR图像获取成本高且数据稀缺，限制了现有基础模型在遥感应用中的使用。

Method: 利用ONERA的15年机载SAR数据，构建了11万张图像的训练集，采用35亿参数的预训练潜在扩散模型，结合空间条件技术进行图像转换。

Result: 方法成功将卫星SAR图像转换为机载SAR表示，并提升了物理模拟器生成图像的逼真度。

Conclusion: 该研究为SAR成像技术的AI应用提供了新思路，填补了文献中的空白。

Abstract: The availability of Synthetic Aperture Radar (SAR) satellite imagery has
increased considerably in recent years, with datasets commercially available.
However, the acquisition of high-resolution SAR images in airborne
configurations, remains costly and limited. Thus, the lack of open source,
well-labeled, or easily exploitable SAR text-image datasets is a barrier to the
use of existing foundation models in remote sensing applications. In this
context, synthetic image generation is a promising solution to augment this
scarce data, enabling a broader range of applications. Leveraging over 15 years
of ONERA's extensive archival airborn data from acquisition campaigns, we
created a comprehensive training dataset of 110 thousands SAR images to exploit
a 3.5 billion parameters pre-trained latent diffusion model. In this work, we
present a novel approach utilizing spatial conditioning techniques within a
foundation model to transform satellite SAR imagery into airborne SAR
representations. Additionally, we demonstrate that our pipeline is effective
for bridging the realism of simulated images generated by ONERA's physics-based
simulator EMPRISE. Our method explores a key application of AI in advancing SAR
imaging technology. To the best of our knowledge, we are the first to introduce
this approach in the literature.

</details>


### [287] [A Deep Learning approach for Depressive Symptoms assessment in Parkinson's disease patients using facial videos](https://arxiv.org/abs/2505.03845)
*Ioannis Kyprakis,Vasileios Skaramagkas,Iro Boura,Georgios Karamanis,Dimitrios I. Fotiadis,Zinovia Kefalopoulou,Cleanthe Spanaki,Manolis Tsiknakis*

Main category: eess.IV

TL;DR: 研究使用深度学习模型分析帕金森病患者面部视频，评估抑郁症状的存在和严重程度，Video Swin Tiny模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 帕金森病中抑郁症状普遍但易被忽视，需开发准确评估工具。

Method: 采用ViViT、Video Swin Tiny和3D CNN-LSTM等模型，结合GDS量表分析面部视频。

Result: Video Swin Tiny模型在二元分类和多分类任务中分别达到94%和87.1%的准确率。

Conclusion: 深度学习模型，尤其是Video Swin Tiny，可有效评估帕金森病患者的抑郁症状。

Abstract: Parkinson's disease (PD) is a neurodegenerative disorder, manifesting with
motor and non-motor symptoms. Depressive symptoms are prevalent in PD,
affecting up to 45% of patients. They are often underdiagnosed due to
overlapping motor features, such as hypomimia. This study explores deep
learning (DL) models-ViViT, Video Swin Tiny, and 3D CNN-LSTM with attention
layers-to assess the presence and severity of depressive symptoms, as detected
by the Geriatric Depression Scale (GDS), in PD patients through facial video
analysis. The same parameters were assessed in a secondary analysis taking into
account whether patients were one hour after (ON-medication state) or 12 hours
without (OFF-medication state) dopaminergic medication. Using a dataset of
1,875 videos from 178 patients, the Video Swin Tiny model achieved the highest
performance, with up to 94% accuracy and 93.7% F1-score in binary
classification (presence of absence of depressive symptoms), and 87.1% accuracy
with an 85.4% F1-score in multiclass tasks (absence or mild or severe
depressive symptoms).

</details>


### [288] [Prototype-Based Information Compensation Network for Multi-Source Remote Sensing Data Classification](https://arxiv.org/abs/2505.04003)
*Feng Gao,Sheng Liu,Chuanzheng Gong,Xiaowei Zhou,Jiayi Wang,Junyu Dong,Qian Du*

Main category: eess.IV

TL;DR: 提出了一种基于原型的信息补偿网络（PICNet），用于多源遥感数据联合分类，解决了多源特征耦合和互补信息不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 多源遥感数据联合分类的现有方法面临多源特征耦合和互补信息探索不一致的挑战。

Method: 设计频率交互模块增强多源特征提取中的频率耦合，并通过原型补偿模块建模全局互补信息。

Result: 在三个公开数据集上的实验表明，PICNet显著优于现有方法。

Conclusion: PICNet有效解决了多源遥感数据分类中的关键问题，性能优越。

Abstract: Multi-source remote sensing data joint classification aims to provide
accuracy and reliability of land cover classification by leveraging the
complementary information from multiple data sources. Existing methods confront
two challenges: inter-frequency multi-source feature coupling and inconsistency
of complementary information exploration. To solve these issues, we present a
Prototype-based Information Compensation Network (PICNet) for land cover
classification based on HSI and SAR/LiDAR data. Specifically, we first design a
frequency interaction module to enhance the inter-frequency coupling in
multi-source feature extraction. The multi-source features are first decoupled
into high- and low-frequency components. Then, these features are recoupled to
achieve efficient inter-frequency communication. Afterward, we design a
prototype-based information compensation module to model the global
multi-source complementary information. Two sets of learnable modality
prototypes are introduced to represent the global modality information of
multi-source data. Subsequently, cross-modal feature integration and alignment
are achieved through cross-attention computation between the modality-specific
prototype vectors and the raw feature representations. Extensive experiments on
three public datasets demonstrate the significant superiority of our PICNet
over state-of-the-art methods. The codes are available at
https://github.com/oucailab/PICNet.

</details>


### [289] [3D Brain MRI Classification for Alzheimer Diagnosis Using CNN with Data Augmentation](https://arxiv.org/abs/2505.04097)
*Thien Nhan Vo,Bac Nam Ho,Thanh Xuan Truong*

Main category: eess.IV

TL;DR: 开发了一种3D卷积神经网络，用于将T1加权脑MRI扫描分类为健康或阿尔茨海默病，通过噪声注入和交叉验证，模型表现优异。


<details>
  <summary>Details</summary>
Motivation: 提高对阿尔茨海默病的MRI扫描分类准确性，探索数据增强方法的有效性。

Method: 采用3D卷积、池化、批归一化、ReLU层和Sigmoid输出，结合噪声注入和五折交叉验证。

Result: 测试集准确率为0.912，ROC曲线下面积为0.961，灵敏度和特异性均超过0.90。

Conclusion: 简单数据增强对3D MRI分类有效，未来可探索更先进的增强方法和架构。

Abstract: A three-dimensional convolutional neural network was developed to classify
T1-weighted brain MRI scans as healthy or Alzheimer. The network comprises 3D
convolution, pooling, batch normalization, dense ReLU layers, and a sigmoid
output. Using stochastic noise injection and five-fold cross-validation, the
model achieved test set accuracy of 0.912 and area under the ROC curve of
0.961, an improvement of approximately 0.027 over resizing alone. Sensitivity
and specificity both exceeded 0.90. These results align with prior work
reporting up to 0.10 gain via synthetic augmentation. The findings demonstrate
the effectiveness of simple augmentation for 3D MRI classification and motivate
future exploration of advanced augmentation methods and architectures such as
3D U-Net and vision transformers.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [290] [In-Context Adaptation to Concept Drift for Learned Database Operations](https://arxiv.org/abs/2505.04404)
*Jiaqi Zhu,Shaofeng Cai,Yanyan Shen,Gang Chen,Fang Deng,Beng Chin Ooi*

Main category: cs.DB

TL;DR: FLAIR是一个在线适应框架，通过动态上下文构建解决数据库环境中概念漂移问题，无需运行时参数优化。


<details>
  <summary>Details</summary>
Motivation: 动态数据库环境中频繁更新和数据分布变化导致学习模型性能下降，需要高效适应框架。

Method: FLAIR利用数据系统的即时执行结果特性，通过任务特征化模块和动态决策引擎实现动态上下文适应。

Result: 实验表明，FLAIR在数据库任务中表现优于现有方法，适应速度提升5.2倍，错误率降低22.5%。

Conclusion: FLAIR为学习型数据库操作提供了一种高效、无需参数优化的适应范式。

Abstract: Machine learning has demonstrated transformative potential for database
operations, such as query optimization and in-database data analytics. However,
dynamic database environments, characterized by frequent updates and evolving
data distributions, introduce concept drift, which leads to performance
degradation for learned models and limits their practical applicability.
Addressing this challenge requires efficient frameworks capable of adapting to
shifting concepts while minimizing the overhead of retraining or fine-tuning.
  In this paper, we propose FLAIR, an online adaptation framework that
introduces a new paradigm called \textit{in-context adaptation} for learned
database operations. FLAIR leverages the inherent property of data systems,
i.e., immediate availability of execution results for predictions, to enable
dynamic context construction. By formalizing adaptation as $f:(\mathbf{x} \,|
\,\mathcal{C}_t) \to \mathbf{y}$, with $\mathcal{C}_t$ representing a dynamic
context memory, FLAIR delivers predictions aligned with the current concept,
eliminating the need for runtime parameter optimization. To achieve this, FLAIR
integrates two key modules: a Task Featurization Module for encoding
task-specific features into standardized representations, and a Dynamic
Decision Engine, pre-trained via Bayesian meta-training, to adapt seamlessly
using contextual information at runtime. Extensive experiments across key
database tasks demonstrate that FLAIR outperforms state-of-the-art baselines,
achieving up to 5.2x faster adaptation and reducing error by 22.5% for
cardinality estimation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [291] [OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation](https://arxiv.org/abs/2505.03912)
*Can Cui,Pengxiang Ding,Wenxuan Song,Shuanghao Bai,Xinyang Tong,Zirui Ge,Runze Suo,Wanqi Zhou,Yang Liu,Bofang Jia,Han Zhao,Siteng Huang,Donglin Wang*

Main category: cs.RO

TL;DR: 本文总结了现有双系统VLA架构的设计，并对其核心元素进行了系统评估，旨在提供一个低成本的开源模型供进一步研究。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏开源的双系统VLA架构工作，阻碍了性能分析和优化。

Method: 总结和比较现有双系统架构设计，并进行系统性实验评估。

Result: 提供了一个低成本的开源模型，并计划持续更新实验结论和改进模型。

Conclusion: 该项目为双系统VLA架构的研究提供了开源资源，未来将持续优化。

Abstract: Dual-system VLA (Vision-Language-Action) architectures have become a hot
topic in embodied intelligence research, but there is a lack of sufficient
open-source work for further performance analysis and optimization. To address
this problem, this paper will summarize and compare the structural designs of
existing dual-system architectures, and conduct systematic empirical
evaluations on the core design elements of existing dual-system architectures.
Ultimately, it will provide a low-cost open-source model for further
exploration. Of course, this project will continue to update with more
experimental conclusions and open-source models with improved performance for
everyone to choose from. Project page: https://openhelix-robot.github.io/.

</details>


### [292] [Scalable Aerial GNSS Localization for Marine Robots](https://arxiv.org/abs/2505.04095)
*Shuo Wen,Edwin Meriaux,Mariana Sosa Guzmán,Charlotte Morissette,Chloe Si,Bobak Baghi,Gregory Dudek*

Main category: cs.RO

TL;DR: 提出一种利用配备GNSS的无人机追踪和定位水面附近海洋机器人的新方法，解决了传统GNSS在水面信号反射和高成本的问题。


<details>
  <summary>Details</summary>
Motivation: 传统GNSS在水面定位中因信号反射和高成本效果不佳，现有方法如惯性导航、DVL等存在误差累积和计算复杂度高的问题。

Method: 利用配备GNSS的无人机追踪和定位水面附近的海洋机器人。

Result: 实验证明该方法能实现高精度的单机器人及多机器人海洋定位。

Conclusion: 该方法为水面机器人定位提供了一种高效且可扩展的解决方案。

Abstract: Accurate localization is crucial for water robotics, yet traditional onboard
Global Navigation Satellite System (GNSS) approaches are difficult or
ineffective due to signal reflection on the water's surface and its high cost
of aquatic GNSS receivers. Existing approaches, such as inertial navigation,
Doppler Velocity Loggers (DVL), SLAM, and acoustic-based methods, face
challenges like error accumulation and high computational complexity.
Therefore, a more efficient and scalable solution remains necessary. This paper
proposes an alternative approach that leverages an aerial drone equipped with
GNSS localization to track and localize a marine robot once it is near the
surface of the water. Our results show that this novel adaptation enables
accurate single and multi-robot marine robot localization.

</details>


### [293] [RGB-Event Fusion with Self-Attention for Collision Prediction](https://arxiv.org/abs/2505.04258)
*Pietro Bonazzi,Christian Vogt,Michael Jost,Haotong Qin,Lyes Khacef,Federico Paredes-Valles,Michele Magno*

Main category: cs.RO

TL;DR: 论文提出了一种基于RGB和事件视觉传感器的神经网络框架，用于预测无人机与动态物体的碰撞时间和位置，通过自注意力融合提升精度。


<details>
  <summary>Details</summary>
Motivation: 确保自主机器人在动态环境中的实时避障安全。

Method: 采用双编码器分支分别处理RGB和事件数据，通过自注意力融合提高预测准确性。

Result: 融合模型在50Hz预测频率下，精度平均提升1%，远距离提升10%，但内存和计算成本显著增加；事件模型在相似计算成本下优于RGB模型。

Conclusion: 多模态感知在机器人应用中存在性能与效率的权衡。

Abstract: Ensuring robust and real-time obstacle avoidance is critical for the safe
operation of autonomous robots in dynamic, real-world environments. This paper
proposes a neural network framework for predicting the time and collision
position of an unmanned aerial vehicle with a dynamic object, using RGB and
event-based vision sensors. The proposed architecture consists of two separate
encoder branches, one for each modality, followed by fusion by self-attention
to improve prediction accuracy. To facilitate benchmarking, we leverage the
ABCD [8] dataset collected that enables detailed comparisons of single-modality
and fusion-based approaches. At the same prediction throughput of 50Hz, the
experimental results show that the fusion-based model offers an improvement in
prediction accuracy over single-modality approaches of 1% on average and 10%
for distances beyond 0.5m, but comes at the cost of +71% in memory and + 105%
in FLOPs. Notably, the event-based model outperforms the RGB model by 4% for
position and 26% for time error at a similar computational cost, making it a
competitive alternative. Additionally, we evaluate quantized versions of the
event-based models, applying 1- to 8-bit quantization to assess the trade-offs
between predictive performance and computational efficiency. These findings
highlight the trade-offs of multi-modal perception using RGB and event-based
cameras in robotic applications.

</details>


### [294] [Model-Based AI planning and Execution Systems for Robotics](https://arxiv.org/abs/2505.04493)
*Or Wertheim,Ronen I. Brafman*

Main category: cs.RO

TL;DR: 本文探讨了基于模型的规划和执行系统在机器人任务控制中的设计选择、解决方案及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过模型化方法构建灵活、通用的机器人系统，以自动组合基本技能完成多样化任务。

Method: 回顾现有系统的设计选择与解决方案，分析其优缺点。

Result: 总结了当前系统的多样性，并提出了未来发展的可能方向。

Conclusion: 基于模型的系统在机器人任务控制中具有潜力，但仍需进一步研究以实现更广泛的应用。

Abstract: Model-based planning and execution systems offer a principled approach to
building flexible autonomous robots that can perform diverse tasks by
automatically combining a host of basic skills. This idea is almost as old as
modern robotics. Yet, while diverse general-purpose reasoning architectures
have been proposed since, general-purpose systems that are integrated with
modern robotic platforms have emerged only recently, starting with the
influential ROSPlan system. Since then, a growing number of model-based systems
for robot task-level control have emerged. In this paper, we consider the
diverse design choices and issues existing systems attempt to address, the
different solutions proposed so far, and suggest avenues for future
development.

</details>


### [295] [Modeling Personalized Difficulty of Rehabilitation Exercises Using Causal Trees](https://arxiv.org/abs/2505.04583)
*Nathaniel Dennler,Zhonghao Shi,Uksang Yoo,Stefanos Nikolaidis,Maja Matarić*

Main category: cs.RO

TL;DR: 本文提出了一种基于因果树的方法，用于根据用户表现动态调整康复机器人的训练难度，以提高康复效果和用户动机。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设训练难度对所有用户通用，但研究发现中风幸存者对难度的感知存在个体差异，需个性化调整。

Method: 采用因果树模型，根据用户表现动态计算训练难度。

Result: 该方法能准确建模训练难度，并为用户和护理人员提供易于理解的难度解释。

Conclusion: 个性化难度调整方法能有效提升康复效果和用户动机。

Abstract: Rehabilitation robots are often used in game-like interactions for
rehabilitation to increase a person's motivation to complete rehabilitation
exercises. By adjusting exercise difficulty for a specific user throughout the
exercise interaction, robots can maximize both the user's rehabilitation
outcomes and the their motivation throughout the exercise. Previous approaches
have assumed exercises have generic difficulty values that apply to all users
equally, however, we identified that stroke survivors have varied and unique
perceptions of exercise difficulty. For example, some stroke survivors found
reaching vertically more difficult than reaching farther but lower while others
found reaching farther more challenging than reaching vertically. In this
paper, we formulate a causal tree-based method to calculate exercise difficulty
based on the user's performance. We find that this approach accurately models
exercise difficulty and provides a readily interpretable model of why that
exercise is difficult for both users and caretakers.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [296] [Deep Reinforcement Learning for Investor-Specific Portfolio Optimization: A Volatility-Guided Asset Selection Approach](https://arxiv.org/abs/2505.03760)
*Arishi Orra,Aryan Bhambu,Himanshu Choudhary,Manoj Thakur,Selvaraju Natarajan*

Main category: q-fin.PM

TL;DR: 提出了一种基于波动性引导的深度强化学习（DRL）投资组合优化框架，结合投资者风险偏好动态构建投资组合，表现优于基准策略。


<details>
  <summary>Details</summary>
Motivation: 传统投资组合优化需平衡风险与回报，而DRL虽能提供自适应策略，但资产预选对整体表现至关重要。结合投资者偏好优化策略是研究重点。

Method: 利用GARCH模型预测股票波动性并分类，DRL代理通过历史市场数据学习最优投资策略。

Result: 在道琼斯30指数股票上验证，提出的方法生成了一致的风险调整后回报，优于基准策略。

Conclusion: 波动性引导的DRL框架能有效结合投资者风险偏好，动态优化投资组合，提升投资表现。

Abstract: Portfolio optimization requires dynamic allocation of funds by balancing the
risk and return tradeoff under dynamic market conditions. With the recent
advancements in AI, Deep Reinforcement Learning (DRL) has gained prominence in
providing adaptive and scalable strategies for portfolio optimization. However,
the success of these strategies depends not only on their ability to adapt to
market dynamics but also on the careful pre-selection of assets that influence
overall portfolio performance. Incorporating the investor's preference in
pre-selecting assets for a portfolio is essential in refining their investment
strategies. This study proposes a volatility-guided DRL-based portfolio
optimization framework that dynamically constructs portfolios based on
investors' risk profiles. The Generalized Autoregressive Conditional
Heteroscedasticity (GARCH) model is utilized for volatility forecasting of
stocks and categorizes them based on their volatility as aggressive, moderate,
and conservative. The DRL agent is then employed to learn an optimal investment
policy by interacting with the historical market data. The efficacy of the
proposed methodology is established using stocks from the Dow $30$ index. The
proposed investor-specific DRL-based portfolios outperformed the baseline
strategies by generating consistent risk-adjusted returns.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [297] [On the Residual-based Neural Network for Unmodeled Distortions in Coordinate Transformation](https://arxiv.org/abs/2505.03757)
*Vinicius Francisco Rofatto,Luiz Felipe Rodrigues de Almeida,Marcelo Tomio Matsuoka,Ivandro Klein,Mauricio Roberto Veronez,Luiz Gonzaga Da Silveira Junior*

Main category: physics.geo-ph

TL;DR: 提出了一种基于残差的神经校正策略，通过神经网络学习初始几何变换后的系统性失真，降低模型复杂度并提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统坐标变换模型难以处理非线性和空间依赖性失真，导致地理空间应用中存在显著的残差误差。

Method: 使用神经网络仅建模初始几何变换后的残差模式，通过模拟数据集和实际图像地理配准任务进行评估。

Result: 在稀疏或结构化控制点配置下，该方法比直接神经网络坐标转换和经典变换模型更准确稳定。

Conclusion: 残差建模是一种轻量且鲁棒的替代方案，可有效提升坐标变换精度。

Abstract: Coordinate transformation models often fail to account for nonlinear and
spatially dependent distortions, leading to significant residual errors in
geospatial applications. Here we propose a residual-based neural correction
strategy, in which a neural network learns to model only the systematic
distortions left by an initial geometric transformation. By focusing solely on
residual patterns, the proposed method reduces model complexity and improves
performance, particularly in scenarios with sparse or structured control point
configurations. We evaluate the method using both simulated datasets with
varying distortion intensities and sampling strategies, as well as under the
real-world image georeferencing tasks. Compared with direct neural network
coordinate converter and classical transformation models, the residual-based
neural correction delivers more accurate and stable results under challenging
conditions, while maintaining comparable performance in ideal cases. These
findings demonstrate the effectiveness of residual modelling as a lightweight
and robust alternative for improving coordinate transformation accuracy.

</details>
