<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 25]
- [cs.CV](#cs.CV) [Total: 52]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.LG](#cs.LG) [Total: 74]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.SC](#cs.SC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.CE](#cs.CE) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.OH](#cs.OH) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.NE](#cs.NE) [Total: 2]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.RO](#cs.RO) [Total: 10]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.MM](#cs.MM) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [math.OC](#math.OC) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models](https://arxiv.org/abs/2505.00725)
*Bithiah Yuan*

Main category: cs.CL

TL;DR: 提出了一种基于BERT的金融QA系统，用于非事实性答案选择，通过检索和重排序提高效率，FinBERT-QA模型在FiQA数据集上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 金融行业对自动分析大规模非结构化数据的需求日益增长，QA系统可为金融顾问提供决策支持。

Method: 结合BM25检索和BERT模型重排序，研究多种BERT学习、预训练和微调方法。

Result: FinBERT-QA模型在FiQA数据集任务2上，MRR提升16%，NDCG提升17%，Precision@1提升21%。

Conclusion: FinBERT-QA在金融QA任务中表现优异，显著提升现有技术水平。

Abstract: Motivated by the emerging demand in the financial industry for the automatic
analysis of unstructured and structured data at scale, Question Answering (QA)
systems can provide lucrative and competitive advantages to companies by
facilitating the decision making of financial advisers. Consequently, we
propose a novel financial QA system using the transformer-based pre-trained
BERT language model to address the limitations of data scarcity and language
specificity in the financial domain. Our system focuses on financial
non-factoid answer selection, which retrieves a set of passage-level texts and
selects the most relevant as the answer. To increase efficiency, we formulate
the answer selection task as a re-ranking problem, in which our system consists
of an Answer Retriever using BM25, a simple information retrieval approach, to
first return a list of candidate answers, and an Answer Re-ranker built with
variants of pre-trained BERT language models to re-rank and select the most
relevant answers. We investigate various learning, further pre-training, and
fine-tuning approaches for BERT. Our experiments suggest that FinBERT-QA, a
model built from applying the Transfer and Adapt further fine-tuning and
pointwise learning approach, is the most effective, improving the
state-of-the-art results of task 2 of the FiQA dataset by 16% on MRR, 17% on
NDCG, and 21% on Precision@1.

</details>


### [2] [A Survey on Large Language Model based Human-Agent Systems](https://arxiv.org/abs/2505.00753)
*Henry Peng Zou,Wei-Chieh Huang,Yaozu Wu,Yankai Chen,Chunyu Miao,Hoang Nguyen,Yue Zhou,Weizhi Zhang,Liancheng Fang,Langzhou He,Yangning Li,Yuwei Cao,Dongyuan Li,Renhe Jiang,Philip S. Yu*

Main category: cs.CL

TL;DR: 该论文综述了基于大型语言模型（LLM）的人机协作系统（LLM-HAS），旨在解决完全自主LLM代理的局限性，如幻觉、复杂任务处理能力不足及安全风险。


<details>
  <summary>Details</summary>
Motivation: 完全自主的LLM代理存在可靠性、复杂任务处理和安全问题，LLM-HAS通过引入人类反馈和控制来提升系统性能、可靠性和安全性。

Method: 论文首次对LLM-HAS进行了系统化综述，涵盖环境与配置、人类反馈、交互类型、编排与通信等核心组件。

Result: 提供了LLM-HAS的结构化概述，总结了当前知识，并探讨了新兴应用、挑战与机遇。

Conclusion: 通过整合现有知识，论文旨在推动这一快速发展的跨学科领域的进一步研究与创新。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building fully autonomous agents. However, fully autonomous LLM-based agents
still face significant challenges, including limited reliability due to
hallucinations, difficulty in handling complex tasks, and substantial safety
and ethical risks, all of which limit their feasibility and trustworthiness in
real-world applications. To overcome these limitations, LLM-based human-agent
systems (LLM-HAS) incorporate human-provided information, feedback, or control
into the agent system to enhance system performance, reliability and safety.
This paper provides the first comprehensive and structured survey of LLM-HAS.
It clarifies fundamental concepts, systematically presents core components
shaping these systems, including environment & profiling, human feedback,
interaction types, orchestration and communication, explores emerging
applications, and discusses unique challenges and opportunities. By
consolidating current knowledge and offering a structured overview, we aim to
foster further research and innovation in this rapidly evolving
interdisciplinary field. Paper lists and resources are available at
https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers.

</details>


### [3] [Reasoning Capabilities and Invariability of Large Language Models](https://arxiv.org/abs/2505.00776)
*Alessandro Raganato,Rafael Peñaloza,Marco Viviani,Gabriella Pasi*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型（LLMs）在简单推理任务中的表现，重点关注其对提示的依赖性，并引入了一个新的基准数据集进行测试。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自然语言处理中表现出色，但其在简单推理任务中的能力仍受质疑，因此需要深入研究其推理能力。

Method: 通过零样本、少样本和思维链提示，测试了24种不同规模的LLMs在几何图形推理任务中的表现。

Result: 超过700亿参数的LLMs在零样本设置中表现较好，但仍有改进空间；思维链提示的效果取决于提示的时机。

Conclusion: LLMs在简单推理任务中仍有局限性，提示方式对其表现有显著影响。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in
manipulating natural language across multiple applications, but their ability
to handle simple reasoning tasks is often questioned. In this work, we aim to
provide a comprehensive analysis of LLMs' reasoning competence, specifically
focusing on their prompt dependency. In particular, we introduce a new
benchmark dataset with a series of simple reasoning questions demanding shallow
logical reasoning. Aligned with cognitive psychology standards, the questions
are confined to a basic domain revolving around geometric figures, ensuring
that responses are independent of any pre-existing intuition about the world
and rely solely on deduction. An empirical analysis involving zero-shot and
few-shot prompting across 24 LLMs of different sizes reveals that, while LLMs
with over 70 billion parameters perform better in the zero-shot setting, there
is still a large room for improvement. An additional test with chain-of-thought
prompting over 22 LLMs shows that this additional prompt can aid or damage the
performance of models, depending on whether the rationale is required before or
after the answer.

</details>


### [4] [Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction](https://arxiv.org/abs/2505.00814)
*Mario Sänger,Ulf Leser*

Main category: cs.CL

TL;DR: 研究评估了预训练语言模型（PLMs）在生物医学关系抽取（RE）中的表现，重点探讨了上下文信息和超参数优化的影响。


<details>
  <summary>Details</summary>
Motivation: 生物医学文献中关系抽取的需求日益增长，但现有研究因模型、数据和评估方法的差异难以直接比较。本研究旨在填补这一空白。

Method: 在五个数据集上评估三种基线PLMs，进行超参数优化，并加入上下文信息（如实体描述、知识图谱关系和分子结构编码）增强模型。

Result: 研究发现模型选择和超参数优化对性能至关重要，上下文信息对小模型有显著提升，但对整体改进有限。

Conclusion: 研究强调了模型选择和超参数优化的重要性，上下文信息对小模型有显著帮助。

Abstract: Automatic relationship extraction (RE) from biomedical literature is critical
for managing the vast amount of scientific knowledge produced each year. In
recent years, utilizing pre-trained language models (PLMs) has become the
prevalent approach in RE. Several studies report improved performance when
incorporating additional context information while fine-tuning PLMs for RE.
However, variations in the PLMs applied, the databases used for augmentation,
hyper-parameter optimization, and evaluation methods complicate direct
comparisons between studies and raise questions about the generalizability of
these findings. Our study addresses this research gap by evaluating PLMs
enhanced with contextual information on five datasets spanning four relation
scenarios within a consistent evaluation framework. We evaluate three baseline
PLMs and first conduct extensive hyperparameter optimization. After selecting
the top-performing model, we enhance it with additional data, including textual
entity descriptions, relational information from knowledge graphs, and
molecular structure encodings. Our findings illustrate the importance of i) the
choice of the underlying language model and ii) a comprehensive hyperparameter
optimization for achieving strong extraction performance. Although inclusion of
context information yield only minor overall improvements, an ablation study
reveals substantial benefits for smaller PLMs when such external data was
included during fine-tuning.

</details>


### [5] [Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing](https://arxiv.org/abs/2505.00931)
*Timur Jaganov,John Blake,Julián Villegas,Nicholas Carr*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在动态评估（DA）中的扩展潜力，开发了DynaWrite应用测试21种LLMs，发现GPT-4o和neural chat表现最佳，GPT-4o在反馈质量上更优。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs如何扩展动态评估，以支持更大规模的语言学习。

Method: 开发DynaWrite应用，测试21种LLMs，重点评估GPT-4o和neural chat的反馈能力。

Result: GPT-4o在反馈质量和实时性上优于neural chat，验证了LLMs扩展DA的可行性。

Conclusion: LLMs可有效扩展动态评估，适用于大规模语言学习场景。

Abstract: This study investigates the potential for Large Language Models (LLMs) to
scale-up Dynamic Assessment (DA). To facilitate such an investigation, we first
developed DynaWrite-a modular, microservices-based grammatical tutoring
application which supports multiple LLMs to generate dynamic feedback to
learners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural
chat to have the most potential to scale-up DA in the language learning
classroom. Further testing of these two candidates found both models performed
similarly in their ability to accurately identify grammatical errors in user
sentences. However, GPT-4o consistently outperformed neural chat in the quality
of its DA by generating clear, consistent, and progressively explicit hints.
Real-time responsiveness and system stability were also confirmed through
detailed performance testing, with GPT-4o exhibiting sufficient speed and
stability. This study shows that LLMs can be used to scale-up dynamic
assessment and thus enable dynamic assessment to be delivered to larger groups
than possible in traditional teacher-learner settings.

</details>


### [6] [Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/abs/2505.00949)
*Akhiad Bercovich,Itay Levy,Izik Golan,Mohammad Dabbah,Ran El-Yaniv,Omri Puny,Ido Galil,Zach Moshe,Tomer Ronen,Najeeb Nabwani,Ido Shahaf,Oren Tropp,Ehud Karpas,Ran Zilberstein,Jiaqi Zeng,Soumye Singhal,Alexander Bukharin,Yian Zhang,Tugrul Konuk,Gerald Shen,Ameya Sunil Mahabaleshwarkar,Bilal Kartal,Yoshi Suhara,Olivier Delalleau,Zijia Chen,Zhilin Wang,David Mosallanezhad,Adi Renduchintala,Haifeng Qian,Dima Rekesh,Fei Jia,Somshubra Majumdar,Vahid Noroozi,Wasi Uddin Ahmad,Sean Narenthiran,Aleksander Ficek,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Igor Gitman,Ivan Moshkov,Wei Du,Shubham Toshniwal,George Armstrong,Branislav Kisacanin,Matvei Novikov,Daria Gitman,Evelina Bakhturina,Jane Polak Scowcroft,John Kamalu,Dan Su,Kezhi Kong,Markus Kliegl,Rabeeh Karimi,Ying Lin,Sanjeev Satheesh,Jupinder Parmar,Pritam Gundecha,Brandon Norick,Joseph Jennings,Shrimai Prabhumoye,Syeda Nahida Akter,Mostofa Patwary,Abhinav Khattar,Deepak Narayanan,Roger Waleffe,Jimmy Zhang,Bor-Yiing Su,Guyue Huang,Terry Kong,Parth Chadha,Sahil Jain,Christine Harvey,Elad Segal,Jining Huang,Sergey Kashirsky,Robert McQueen,Izzy Putterman,George Lam,Arun Venkatesan,Sherry Wu,Vinh Nguyen,Manoj Kilaru,Andrew Wang,Anna Warno,Abhilash Somasamudramath,Sandip Bhaskar,Maka Dong,Nave Assaf,Shahar Mor,Omer Ullman Argov,Scot Junkin,Oleksandr Romanenko,Pedro Larroy,Monika Katariya,Marco Rovinelli,Viji Balas,Nicholas Edelman,Anahita Bhiwandiwalla,Muthu Subramaniam,Smita Ithape,Karthik Ramamoorthy,Yuting Wu,Suguna Varshini Velury,Omri Almog,Joyjit Daw,Denys Fridman,Erick Galinkin,Michael Evans,Katherine Luna,Leon Derczynski,Nikki Pope,Eileen Long,Seth Schneider,Guillermo Siman,Tomasz Grzegorzek,Pablo Ribalta,Monika Katariya,Joey Conway,Trisha Saar,Ann Guan,Krzysztof Pawelec,Shyamala Prayaga,Oleksii Kuchaiev,Boris Ginsburg,Oluwatobi Olabiyi,Kari Briski,Jonathan Cohen,Bryan Catanzaro,Jonah Alben,Yonatan Geifman,Eric Chung*

Main category: cs.CL

TL;DR: Llama-Nemotron系列模型是一个开源的异构推理模型家族，提供卓越的推理能力、高效的推理速度和商用许可。


<details>
  <summary>Details</summary>
Motivation: 推动开源推理模型的发展，提供高性能且高效的解决方案，支持企业使用和研究。

Method: 通过神经架构搜索、知识蒸馏和持续预训练优化模型，随后进行监督微调和大规模强化学习的后训练阶段。

Result: 模型在推理性能和效率上表现优异，支持动态推理切换，并开源了模型、数据集和代码。

Conclusion: Llama-Nemotron系列为开源社区和企业提供了强大的推理工具，推动了相关研究和应用的发展。

Abstract: We introduce the Llama-Nemotron series of models, an open family of
heterogeneous reasoning models that deliver exceptional reasoning capabilities,
inference efficiency, and an open license for enterprise use. The family comes
in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs
competitively with state-of-the-art reasoning models such as DeepSeek-R1 while
offering superior inference throughput and memory efficiency. In this report,
we discuss the training procedure for these models, which entails using neural
architecture search from Llama 3 models for accelerated inference, knowledge
distillation, and continued pretraining, followed by a reasoning-focused
post-training stage consisting of two main parts: supervised fine-tuning and
large scale reinforcement learning. Llama-Nemotron models are the first
open-source models to support a dynamic reasoning toggle, allowing users to
switch between standard chat and reasoning modes during inference. To further
support open research and facilitate model development, we provide the
following resources: 1. We release the Llama-Nemotron reasoning models --
LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA
Open Model License Agreement. 2. We release the complete post-training dataset:
Llama-Nemotron-Post-Training-Dataset. 3. We also release our training
codebases: NeMo, NeMo-Aligner, and Megatron-LM.

</details>


### [7] [A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts](https://arxiv.org/abs/2505.00977)
*Yingquan Chen,Qianmu Li,Xiaocong Wu,Huifeng Li,Qing Chang*

Main category: cs.CL

TL;DR: 本文提出了一种新的嵌入算法CDEA，结合XLNet模型，显著提升了隐写文本的质量，特别是在感知不可察觉性方面。


<details>
  <summary>Details</summary>
Motivation: 现有模型在文本生成能力上的局限性，以及嵌入算法未能有效缓解敏感信息属性（如语义内容或随机性）的负面影响，导致隐写文本质量下降。

Method: 提出基于字符的扩散嵌入算法（CDEA），利用敏感信息的属性，通过字符级统计特性和幂律分布分组方法，优化候选词的选择频率。同时引入XLNet模型处理长序列。

Result: 实验结果表明，CDEA与XLNet的结合显著提升了隐写文本的质量，尤其是在感知不可察觉性方面。

Conclusion: CDEA算法和XLNet模型的有效结合为解决隐写文本生成中的质量问题提供了新思路。

Abstract: Generating high-quality steganographic text is a fundamental challenge in the
field of generative linguistic steganography. This challenge arises primarily
from two aspects: firstly, the capabilities of existing models in text
generation are limited; secondly, embedding algorithms fail to effectively
mitigate the negative impacts of sensitive information's properties, such as
semantic content or randomness. Specifically, to ensure that the recipient can
accurately extract hidden information, embedding algorithms often have to
consider selecting candidate words with relatively low probabilities. This
phenomenon leads to a decrease in the number of high-probability candidate
words and an increase in low-probability candidate words, thereby compromising
the semantic coherence and logical fluency of the steganographic text and
diminishing the overall quality of the generated steganographic material. To
address this issue, this paper proposes a novel embedding algorithm,
character-based diffusion embedding algorithm (CDEA). Unlike existing embedding
algorithms that strive to eliminate the impact of sensitive information's
properties on the generation process, CDEA leverages sensitive information's
properties. It enhances the selection frequency of high-probability candidate
words in the candidate pool based on general statistical properties at the
character level and grouping methods based on power-law distributions, while
reducing the selection frequency of low-probability candidate words in the
candidate pool. Furthermore, to ensure the effective transformation of
sensitive information in long sequences, we also introduce the XLNet model.
Experimental results demonstrate that the combination of CDEA and XLNet
significantly improves the quality of generated steganographic text,
particularly in terms of perceptual-imperceptibility.

</details>


### [8] [Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models](https://arxiv.org/abs/2505.00979)
*Xuhui Jiang,Shengjie Ma,Chengjin Xu,Cehao Yang,Liyu Zhang,Jian Guo*

Main category: cs.CL

TL;DR: SoG框架通过构建上下文图整合跨文档知识关联，提升合成数据的多样性和连贯性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在小规模专业语料上数据效率低的问题，现有方法忽视跨文档知识关联。

Method: SoG框架提取实体和概念构建上下文图，结合图游走策略生成知识关联的合成数据，并整合CoT和CC提升质量。

Result: SoG在多跳文档问答数据集上优于SOTA方法，在阅读理解任务中表现相当，泛化能力更强。

Conclusion: SoG推动了合成数据生成，为数据有限领域的高效知识获取提供了实用解决方案。

Abstract: Large Language Models (LLMs) have achieved remarkable success but remain
data-inefficient, especially when learning from small, specialized corpora with
limited and proprietary data. Existing synthetic data generation methods for
continue pre-training focus on intra-document content and overlook
cross-document knowledge associations, limiting content diversity and depth. We
propose Synthetic-on-Graph (SoG), a synthetic data generation framework that
incorporates cross-document knowledge associations for efficient corpus
expansion. SoG constructs a context graph by extracting entities and concepts
from the original corpus, representing cross-document associations, and
employing a graph walk strategy for knowledge-associated sampling. This
enhances synthetic data diversity and coherence, enabling models to learn
complex knowledge structures and handle rare knowledge. To further improve
synthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive
Clarifying (CC) synthetic, enhancing reasoning processes and discriminative
power. Experiments show that SoG outperforms the state-of-the-art (SOTA) method
in a multi-hop document Q&A dataset while performing comparably to the SOTA
method on the reading comprehension task datasets, which also underscores the
better generalization capability of SoG. Our work advances synthetic data
generation and provides practical solutions for efficient knowledge acquisition
in LLMs, especially in domains with limited data availability.

</details>


### [9] [Position: Enough of Scaling LLMs! Lets Focus on Downscaling](https://arxiv.org/abs/2505.00985)
*Ayan Sengupta,Yash Goel,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文主张从神经扩展定律转向缩小模型规模，提出一种更可持续、高效且资源需求低的LLM开发框架。


<details>
  <summary>Details</summary>
Motivation: 传统扩展方法在计算效率、环境影响和部署限制方面存在显著问题，需要更可持续的替代方案。

Method: 提出一个全面的缩小LLM规模的框架，旨在保持性能同时大幅减少资源需求。

Result: 通过缩小规模，可以在减少资源消耗的同时维持模型性能。

Conclusion: 缩小规模是LLM开发中更可持续、高效且可访问的方向。

Abstract: We challenge the dominant focus on neural scaling laws and advocate for a
paradigm shift toward downscaling in the development of large language models
(LLMs). While scaling laws have provided critical insights into performance
improvements through increasing model and dataset size, we emphasize the
significant limitations of this approach, particularly in terms of
computational inefficiency, environmental impact, and deployment constraints.
To address these challenges, we propose a holistic framework for downscaling
LLMs that seeks to maintain performance while drastically reducing resource
demands. This paper outlines practical strategies for transitioning away from
traditional scaling paradigms, advocating for a more sustainable, efficient,
and accessible approach to LLM development.

</details>


### [10] [VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language](https://arxiv.org/abs/2505.00989)
*Sijin Sun,Liangbin Zhao,Ming Deng,Xiuju Fu*

Main category: cs.CL

TL;DR: 提出VTS-LLM Agent，首个针对VTS操作的交互式决策支持领域自适应大语言模型代理，通过知识增强的Text-to-SQL任务识别高风险船舶，并在多语言风格查询中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有VTS系统在时空推理和直观人机交互方面存在局限，需应对日益复杂的交通和多模态数据。

Method: 结合结构化船舶数据库与外部海事知识，构建定制数据集，采用NER关系推理、领域知识注入、语义代数中间表示和查询重思机制。

Result: VTS-LLM在命令式、操作式和正式自然语言查询中均优于通用和SQL专用基线模型。

Conclusion: 为VTS自然语言接口奠定基础，推动LLM驱动的主动式实时海事交通管理。

Abstract: Vessel Traffic Services (VTS) are essential for maritime safety and
regulatory compliance through real-time traffic management. However, with
increasing traffic complexity and the prevalence of heterogeneous, multimodal
data, existing VTS systems face limitations in spatiotemporal reasoning and
intuitive human interaction. In this work, we propose VTS-LLM Agent, the first
domain-adaptive large LLM agent tailored for interactive decision support in
VTS operations. We formalize risk-prone vessel identification as a
knowledge-augmented Text-to-SQL task, combining structured vessel databases
with external maritime knowledge. To support this, we construct a curated
benchmark dataset consisting of a custom schema, domain-specific corpus, and a
query-SQL test set in multiple linguistic styles. Our framework incorporates
NER-based relational reasoning, agent-based domain knowledge injection,
semantic algebra intermediate representation, and query rethink mechanisms to
enhance domain grounding and context-aware understanding. Experimental results
show that VTS-LLM outperforms both general-purpose and SQL-focused baselines
under command-style, operational-style, and formal natural language queries,
respectively. Moreover, our analysis provides the first empirical evidence that
linguistic style variation introduces systematic performance challenges in
Text-to-SQL modeling. This work lays the foundation for natural language
interfaces in vessel traffic services and opens new opportunities for
proactive, LLM-driven maritime real-time traffic management.

</details>


### [11] [Token-free Models for Sarcasm Detection](https://arxiv.org/abs/2505.01006)
*Sumit Mamtani,Maitreya Sonawane,Kanika Agarwal,Nishanth Sanjeev*

Main category: cs.CL

TL;DR: 论文评估了两种无标记模型（ByT5和CANINE）在社交媒体和非社交媒体领域的讽刺检测任务中的表现，结果显示它们优于基于标记的模型，并实现了新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 标记化在自然语言处理中引入词汇不匹配和词汇表外问题，无标记模型可能解决这些限制。

Method: 通过微调和基准测试比较ByT5和CANINE与基于标记的模型在讽刺检测任务中的表现。

Result: ByT5-small和CANINE在新闻标题和Twitter讽刺数据集上的准确率分别提高了0.77%和0.49%。

Conclusion: 无标记模型在嘈杂和非正式领域（如社交媒体）中具有潜力。

Abstract: Tokenization is a foundational step in most natural language processing (NLP)
pipelines, yet it introduces challenges such as vocabulary mismatch and
out-of-vocabulary issues. Recent work has shown that models operating directly
on raw text at the byte or character level can mitigate these limitations. In
this paper, we evaluate two token-free models, ByT5 and CANINE, on the task of
sarcasm detection in both social media (Twitter) and non-social media (news
headlines) domains. We fine-tune and benchmark these models against token-based
baselines and state-of-the-art approaches. Our results show that ByT5-small and
CANINE outperform token-based counterparts and achieve new state-of-the-art
performance, improving accuracy by 0.77% and 0.49% on the News Headlines and
Twitter Sarcasm datasets, respectively. These findings underscore the potential
of token-free models for robust NLP in noisy and informal domains such as
social media.

</details>


### [12] [Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark](https://arxiv.org/abs/2505.01015)
*Jongwook Han,Dongmin Choi,Woojung Song,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 提出了Value Portrait基准，用于评估语言模型的价值取向，通过真实用户交互和心理测量验证，发现模型更关注Benevolence、Security和Self-Direction，而忽视Tradition、Power和Achievement。


<details>
  <summary>Details</summary>
Motivation: 现有基准易受价值偏见影响且与真实场景脱节，需更可靠的评估框架。

Method: 设计包含真实用户交互的基准项，通过人类评分与心理测量验证相关性。

Result: 评估27个LLM，发现其价值取向偏向Benevolence等，且存在对人口群体的偏见。

Conclusion: Value Portrait基准提供了生态效度和心理测量验证的价值评估方法，揭示了LLM的价值偏差。

Abstract: The importance of benchmarks for assessing the values of language models has
been pronounced due to the growing need of more authentic, human-aligned
responses. However, existing benchmarks rely on human or machine annotations
that are vulnerable to value-related biases. Furthermore, the tested scenarios
often diverge from real-world contexts in which models are commonly used to
generate text and express values. To address these issues, we propose the Value
Portrait benchmark, a reliable framework for evaluating LLMs' value
orientations with two key characteristics. First, the benchmark consists of
items that capture real-life user-LLM interactions, enhancing the relevance of
assessment results to real-world LLM usage and thus ecological validity.
Second, each item is rated by human subjects based on its similarity to their
own thoughts, and correlations between these ratings and the subjects' actual
value scores are derived. This psychometrically validated approach ensures that
items strongly correlated with specific values serve as reliable items for
assessing those values. Through evaluating 27 LLMs with our benchmark, we find
that these models prioritize Benevolence, Security, and Self-Direction values
while placing less emphasis on Tradition, Power, and Achievement values. Also,
our analysis reveals biases in how LLMs perceive various demographic groups,
deviating from real human data.

</details>


### [13] [Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?](https://arxiv.org/abs/2505.01035)
*Lui Yoshida*

Main category: cs.CL

TL;DR: 研究探讨了在基于大型语言模型（LLM）的自动作文评分（AES）中详细评分标准的必要性和影响，发现简化标准在多数情况下不影响评分准确性且更高效。


<details>
  <summary>Details</summary>
Motivation: 详细评分标准在LLM-based AES中虽常见，但制作耗时且增加计算资源消耗，研究旨在验证简化标准的可行性。

Method: 使用TOEFL11数据集，比较了三种评分标准（完整、简化、无标准）对四种LLM（Claude 3.5 Haiku等）评分准确性的影响。

Result: 三种LLM在简化标准下保持准确性且减少资源消耗，但Gemini 1.5 Flash性能下降。

Conclusion: 简化标准对多数LLM-based AES足够高效，但需针对不同模型评估性能差异。

Abstract: This study investigates the necessity and impact of a detailed rubric in
automated essay scoring (AES) using large language models (LLMs). While using
rubrics are standard in LLM-based AES, creating detailed rubrics requires
substantial ef-fort and increases token usage. We examined how different levels
of rubric detail affect scoring accuracy across multiple LLMs using the TOEFL11
dataset. Our experiments compared three conditions: a full rubric, a simplified
rubric, and no rubric, using four different LLMs (Claude 3.5 Haiku, Gemini 1.5
Flash, GPT-4o-mini, and Llama 3 70B Instruct). Results showed that three out of
four models maintained similar scoring accuracy with the simplified rubric
compared to the detailed one, while significantly reducing token usage.
However, one model (Gemini 1.5 Flash) showed decreased performance with more
detailed rubrics. The findings suggest that simplified rubrics may be
sufficient for most LLM-based AES applications, offering a more efficient
alternative without compromis-ing scoring accuracy. However, model-specific
evaluation remains crucial as per-formance patterns vary across different LLMs.

</details>


### [14] [Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs](https://arxiv.org/abs/2505.01068)
*Yijie Jin,Junjie Peng,Xuanchao Lin,Haochen Yuan,Lan Wang,Cangzhi Zheng*

Main category: cs.CL

TL;DR: 论文提出了一种基于图结构的GsiT模型，通过Interlaced Mask机制优化多模态Transformer的效率，参数减少2/3且性能提升。


<details>
  <summary>Details</summary>
Motivation: 多模态Transformer（MulTs）在多模态情感分析中效率低下，需要优化。

Method: 将MulTs建模为层次化模态异构图（HMHGs），提出GsiT模型，采用Interlaced Mask机制实现参数共享。

Result: GsiT参数仅为传统MulTs的1/3，性能显著提升，并在多个SOTA模型中验证了有效性。

Conclusion: GsiT和HMHG概念在多模态情感分析中高效且有效，为未来研究提供了新思路。

Abstract: Multimodal Sentiment Analysis (MSA) is a rapidly developing field that
integrates multimodal information to recognize sentiments, and existing models
have made significant progress in this area. The central challenge in MSA is
multimodal fusion, which is predominantly addressed by Multimodal Transformers
(MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns.
In this work, from the perspective of efficiency optimization, we propose and
prove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and
we introduce the graph-structured representation pattern of MulTs. Based on
this pattern, we propose an Interlaced Mask (IM) mechanism to design the
Graph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is
formally equivalent to MulTs which achieves an efficient weight-sharing
mechanism without information disorder through IM, enabling All-Modal-In-One
fusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called
Decomposition is implemented to ensure avoiding additional computational
overhead. Moreover, it achieves significantly higher performance than
traditional MulTs. To further validate the effectiveness of GsiT itself and the
HMHG concept, we integrate them into multiple state-of-the-art models and
demonstrate notable performance improvements and parameter reduction on widely
used MSA datasets.

</details>


### [15] [MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning](https://arxiv.org/abs/2505.01110)
*Murtadha Ahmed,Wenbo,Liu yunfeng*

Main category: cs.CL

TL;DR: MateICL通过分窗和注意力权重调整，解决了大语言模型在上下文扩展中的注意力分散问题，提升了In-Context Learning性能。


<details>
  <summary>Details</summary>
Motivation: 预训练模型的固定位置长度限制和注意力分散问题限制了演示示例的数量和效果。

Method: 将上下文分窗处理并引入额外层重新校准注意力权重，优先处理查询标记。

Result: MateICL能有效利用更大上下文提升性能，优于基于检索的基线方法。

Conclusion: MateICL在计算资源受限环境下仍具优势，代码已开源。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
In-Context Learning (ICL). However, the fixed position length constraints in
pre-trained models limit the number of demonstration examples. Recent efforts
to extend context suffer from attention dispersion as the number of
demonstrations increases. In this paper, we introduce Mitigating Attention
Dispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective
self-attention as the context size grows. We first split the context into
multiple windows, each filled to the model's context capacity, which are
processed separately. Then, we introduce an additional layer to recalibrate the
attention weights, prioritizing the query tokens as the number of
demonstrations increases. Our empirical results show that MateICL can
effectively leverage larger contexts to improve ICL performance. Compared to
retrieval-based baselines, MateICL consistently achieves better performance
without requiring an externally trained retrieval model. Despite recent
advances in inference strategies (e.g., 32k token contexts), our results
demonstrate that MateICL remains beneficial in computationally
resource-constrained settings. The code is publicly available at
https://github.com/amurtadha/MateICL.

</details>


### [16] [On the Limitations of Steering in Language Model Alignment](https://arxiv.org/abs/2505.01162)
*Chebrolu Niranjan,Kokil Jaidka,Gerard Christopher Yeo*

Main category: cs.CL

TL;DR: 本文评估了导向向量在语言模型对齐中的局限性，发现其在特定任务（如价值观对齐）中有效，但在复杂场景中可能不足。


<details>
  <summary>Details</summary>
Motivation: 导向向量是一种有前景的语言模型对齐方法，但其局限性尚不明确，需要系统评估。

Method: 通过变压器钩干预和反义词功能向量框架，分析提示结构和上下文复杂性对导向效果的影响。

Result: 导向向量在特定对齐任务中表现良好，但在复杂场景中缺乏鲁棒性。

Conclusion: 导向向量适用于特定对齐任务，但需进一步研究其在通用对齐中的潜力。

Abstract: Steering vectors are a promising approach to aligning language model behavior
at inference time. In this paper, we propose a framework to assess the
limitations of steering vectors as alignment mechanisms. Using a framework of
transformer hook interventions and antonym-based function vectors, we evaluate
the role of prompt structure and context complexity in steering effectiveness.
Our findings indicate that steering vectors are promising for specific
alignment tasks, such as value alignment, but may not provide a robust
foundation for general-purpose alignment in LLMs, particularly in complex
scenarios. We establish a methodological foundation for future investigations
into steering capabilities of reasoning models.

</details>


### [17] [Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods](https://arxiv.org/abs/2505.01198)
*Mahdi Dhaini,Ege Erdogan,Nils Feldhus,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 研究发现，广泛使用的后验特征归因方法在性别上存在显著差异，影响解释的忠实性、鲁棒性和复杂性，且与训练数据无关。


<details>
  <summary>Details</summary>
Motivation: 探讨解释方法在公平性方面的不足，尤其是不同子群间的性能差异。

Method: 分析三个任务和五个语言模型中后验特征归因方法的性别差异。

Result: 发现性别差异普遍存在，且不受训练数据偏见的直接影响。

Conclusion: 强调在开发解释方法时需关注公平性，并将其纳入监管框架。

Abstract: While research on applications and evaluations of explanation methods
continues to expand, fairness of the explanation methods concerning disparities
in their performance across subgroups remains an often overlooked aspect. In
this paper, we address this gap by showing that, across three tasks and five
language models, widely used post-hoc feature attribution methods exhibit
significant gender disparity with respect to their faithfulness, robustness,
and complexity. These disparities persist even when the models are pre-trained
or fine-tuned on particularly unbiased datasets, indicating that the
disparities we observe are not merely consequences of biased training data. Our
results highlight the importance of addressing disparities in explanations when
developing and applying explainability methods, as these can lead to biased
outcomes against certain subgroups, with particularly critical implications in
high-stakes contexts. Furthermore, our findings underscore the importance of
incorporating the fairness of explanations, alongside overall model fairness
and explainability, as a requirement in regulatory frameworks.

</details>


### [18] [EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models](https://arxiv.org/abs/2505.01238)
*Mahdi Dhaini,Kafaite Zahra Hussain,Efstratios Zaradoukas,Gjergji Kasneci*

Main category: cs.CL

TL;DR: EvalxNLP是一个Python框架，用于评估NLP模型的特征归因方法，支持多种解释技术并提供交互式解释。


<details>
  <summary>Details</summary>
Motivation: 随着NLP模型在高风险应用中的普及，解释性成为关键挑战，需要为不同用户提供定制化解释框架。

Method: EvalxNLP整合了八种XAI技术，支持基于忠实性、合理性和复杂性的解释生成与评估，并提供LLM驱动的交互式解释。

Result: 用户评估显示高满意度，表明EvalxNLP是一个有前景的框架。

Conclusion: EvalxNLP旨在普及解释工具，支持XAI技术的系统比较与进步。

Abstract: As Natural Language Processing (NLP) models continue to evolve and become
integral to high-stakes applications, ensuring their interpretability remains a
critical challenge. Given the growing variety of explainability methods and
diverse stakeholder requirements, frameworks that help stakeholders select
appropriate explanations tailored to their specific use cases are increasingly
important. To address this need, we introduce EvalxNLP, a Python framework for
benchmarking state-of-the-art feature attribution methods for transformer-based
NLP models. EvalxNLP integrates eight widely recognized explainability
techniques from the Explainable AI (XAI) literature, enabling users to generate
and evaluate explanations based on key properties such as faithfulness,
plausibility, and complexity. Our framework also provides interactive,
LLM-based textual explanations, facilitating user understanding of the
generated explanations and evaluation outcomes. Human evaluation results
indicate high user satisfaction with EvalxNLP, suggesting it is a promising
framework for benchmarking explanation methods across diverse user groups. By
offering a user-friendly and extensible platform, EvalxNLP aims at
democratizing explainability tools and supporting the systematic comparison and
advancement of XAI techniques in NLP.

</details>


### [19] [PREMISE: Matching-based Prediction for Accurate Review Recommendation](https://arxiv.org/abs/2505.01255)
*Wei Han,Hui Chen,Soujanya Poria*

Main category: cs.CL

TL;DR: PREMISE是一种新的多模态匹配学习架构，用于多模态评论有用性任务，通过多尺度、多领域表示和匹配分数提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统融合方法在多模态任务中因重复语义和低效表示导致性能不足的问题。

Method: 计算多尺度、多领域表示，过滤重复语义，生成匹配分数作为特征向量。

Result: 在公开数据集上表现优异，计算成本更低。

Conclusion: PREMISE在多模态任务中优于现有融合方法，尤其适用于上下文匹配与任务目标高度相关的场景。

Abstract: We present PREMISE (PREdict with Matching ScorEs), a new architecture for the
matching-based learning in the multimodal fields for the multimodal review
helpfulness (MRHP) task. Distinct to previous fusion-based methods which
obtains multimodal representations via cross-modal attention for downstream
tasks, PREMISE computes the multi-scale and multi-field representations,
filters duplicated semantics, and then obtained a set of matching scores as
feature vectors for the downstream recommendation task. This new architecture
significantly boosts the performance for such multimodal tasks whose context
matching content are highly correlated to the targets of that task, compared to
the state-of-the-art fusion-based methods. Experimental results on two publicly
available datasets show that PREMISE achieves promising performance with less
computational cost.

</details>


### [20] [Anti-adversarial Learning: Desensitizing Prompts for Large Language Models](https://arxiv.org/abs/2505.01273)
*Xuan Li,Zhe Yin,Xiaodong Gu,Beijun Shen*

Main category: cs.CL

TL;DR: PromptObfus是一种通过反对抗学习扰动隐私词的方法，用于保护LLM提示中的隐私，同时保持模型预测稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用，用户提示中的隐私保护变得至关重要，传统方法因计算成本高和用户参与需求难以适用。

Method: 将提示脱敏任务建模为掩码语言建模，用[MASK]替换隐私词，并通过梯度反馈选择候选替换词。

Result: 在三个NLP任务中，PromptObfus有效防止了隐私泄露，同时保持了任务性能。

Conclusion: PromptObfus为LLM提示隐私保护提供了一种高效且实用的解决方案。

Abstract: With the widespread use of LLMs, preserving privacy in user prompts has
become crucial, as prompts risk exposing privacy and sensitive data to the
cloud LLMs. Traditional techniques like homomorphic encryption, secure
multi-party computation, and federated learning face challenges due to heavy
computational costs and user participation requirements, limiting their
applicability in LLM scenarios. In this paper, we propose PromptObfus, a novel
method for desensitizing LLM prompts. The core idea of PromptObfus is
"anti-adversarial" learning, which perturbs privacy words in the prompt to
obscure sensitive information while retaining the stability of model
predictions. Specifically, PromptObfus frames prompt desensitization as a
masked language modeling task, replacing privacy-sensitive terms with a [MASK]
token. A desensitization model is trained to generate candidate replacements
for each masked position. These candidates are subsequently selected based on
gradient feedback from a surrogate model, ensuring minimal disruption to the
task output. We demonstrate the effectiveness of our approach on three NLP
tasks. Results show that PromptObfus effectively prevents privacy inference
from remote LLMs while preserving task performance.

</details>


### [21] [TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models](https://arxiv.org/abs/2504.20605)
*Mihai Nadas,Laura Diosan,Andrei Piscoran,Andreea Tomescu*

Main category: cs.CL

TL;DR: 论文介绍了TF1-EN-3M，一个由指令调优模型生成的300万英语寓言数据集，填补了现代NLP在道德故事结构化数据上的空白。


<details>
  <summary>Details</summary>
Motivation: 现代NLP缺乏结合连贯叙事与明确道德教训的大规模结构化数据集，本文旨在填补这一空白。

Method: 使用不超过8B参数的指令调优模型生成寓言，采用六槽模板（角色->特质->场景->冲突->解决->道德），并通过混合评估流程（GPT评分与无参考多样性指标）验证质量。

Result: 8B参数的Llama-3变体在质量与速度上表现最佳，单块消费级GPU即可高效生成高质量寓言。

Conclusion: TF1-EN-3M为指令遵循、叙事智能等领域提供了开源资源，证明大规模道德故事生成无需依赖专有巨型模型。

Abstract: Moral stories are a time-tested vehicle for transmitting values, yet modern
NLP lacks a large, structured corpus that couples coherent narratives with
explicit ethical lessons. We close this gap with TF1-EN-3M, the first open
dataset of three million English-language fables generated exclusively by
instruction-tuned models no larger than 8B parameters. Each story follows a
six-slot scaffold (character -> trait -> setting -> conflict -> resolution ->
moral), produced through a combinatorial prompt engine that guarantees genre
fidelity while covering a broad thematic space.
  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores
grammar, creativity, moral clarity, and template adherence with (ii)
reference-free diversity and readability metrics. Among ten open-weight
candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed
trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)
at approximately 13.5 cents per 1,000 fables.
  We release the dataset, generation code, evaluation scripts, and full
metadata under a permissive license, enabling exact reproducibility and cost
benchmarking. TF1-EN-3M opens avenues for research in instruction following,
narrative intelligence, value alignment, and child-friendly educational AI,
demonstrating that large-scale moral storytelling no longer requires
proprietary giant models.

</details>


### [22] [A Factorized Probabilistic Model of the Semantics of Vague Temporal Adverbials Relative to Different Event Types](https://arxiv.org/abs/2505.01311)
*Svenja Kenneweg,Jörg Deigmöller,Julian Eggert,Philipp Cimiano*

Main category: cs.CL

TL;DR: 论文提出了一种因子化模型，用于捕捉模糊时间副词的语义，并将其与事件特定分布结合，生成上下文意义。与非因子化模型相比，该模型在预测能力相似的情况下更简洁且扩展性更好。


<details>
  <summary>Details</summary>
Motivation: 模糊时间副词（如“recently”）描述事件与当前时间的时间距离，但未明确具体时长。研究旨在通过概率分布捕捉这些副词的语义。

Method: 引入因子化模型，将时间副词的语义建模为概率分布，并与事件特定分布结合。使用母语者对副词适用性的判断数据拟合模型参数。

Result: 与非因子化模型（基于高斯分布）相比，因子化模型预测能力相当，但更简洁且扩展性更好。

Conclusion: 因子化模型在捕捉模糊时间副词语义方面更优，符合奥卡姆剃刀原则。

Abstract: Vague temporal adverbials, such as recently, just, and a long time ago,
describe the temporal distance between a past event and the utterance time but
leave the exact duration underspecified. In this paper, we introduce a
factorized model that captures the semantics of these adverbials as
probabilistic distributions. These distributions are composed with
event-specific distributions to yield a contextualized meaning for an adverbial
applied to a specific event. We fit the model's parameters using existing data
capturing judgments of native speakers regarding the applicability of these
vague temporal adverbials to events that took place a given time ago. Comparing
our approach to a non-factorized model based on a single Gaussian distribution
for each pair of event and temporal adverbial, we find that while both models
have similar predictive power, our model is preferable in terms of Occam's
razor, as it is simpler and has better extendability.

</details>


### [23] [A Transformer-based Neural Architecture Search Method](https://arxiv.org/abs/2505.01314)
*Shang Wang,Huanrong Tang,Jianquan Ouyang*

Main category: cs.CL

TL;DR: 提出了一种基于Transformer架构的神经架构搜索方法，通过多目标遗传算法优化网络结构，结合BLEU和困惑度作为评价指标，实验表明其性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 为了找到翻译效果更好的神经网络结构，需要一种更全面的评价方法，而不仅仅是依赖BLEU分数。

Method: 基于Transformer架构，通过多目标遗传算法搜索多头注意力的计算方式，结合BLEU和困惑度作为评价指标。

Result: 搜索到的网络结构优于所有基线模型，且引入困惑度作为辅助评价指标比仅用BLEU分数效果更好。

Conclusion: 多目标遗传算法结合多种评价指标能有效提升神经架构搜索的性能。

Abstract: This paper presents a neural architecture search method based on Transformer
architecture, searching cross multihead attention computation ways for
different number of encoder and decoder combinations. In order to search for
neural network structures with better translation results, we considered
perplexity as an auxiliary evaluation metric for the algorithm in addition to
BLEU scores and iteratively improved each individual neural network within the
population by a multi-objective genetic algorithm. Experimental results show
that the neural network structures searched by the algorithm outperform all the
baseline models, and that the introduction of the auxiliary evaluation metric
can find better models than considering only the BLEU score as an evaluation
metric.

</details>


### [24] [Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System](https://arxiv.org/abs/2505.01315)
*Sheikh Samit Muhaimin,Spyridon Mastorakis*

Main category: cs.CL

TL;DR: 该论文提出了一种无需重新训练或微调的新型防御框架，帮助大型语言模型（LLM）自主识别和防御对抗性或恶意输入。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，其易受对抗性攻击和恶意输入的影响，而现有防御方法通常需要重新训练模型，成本高昂且不切实际。

Method: 框架包含两个模块：(1) 提示过滤模块，利用零样本分类、关键词分析和编码内容检测等技术识别有害输入；(2) 摘要模块，通过总结对抗性研究文献为LLM提供上下文防御知识。

Result: 实验显示，该方法在识别有害模式、操纵性语言结构和编码提示方面的成功率达98.71%，显著提高了模型的抗攻击能力。

Conclusion: 该框架为LLM提供了一种高效且无需重新训练的防御方案，显著提升了其对抗恶意输入的能力，同时保持了响应质量。

Abstract: The recent growth in the use of Large Language Models has made them
vulnerable to sophisticated adversarial assaults, manipulative prompts, and
encoded malicious inputs. Existing countermeasures frequently necessitate
retraining models, which is computationally costly and impracticable for
deployment. Without the need for retraining or fine-tuning, this study presents
a unique defense paradigm that allows LLMs to recognize, filter, and defend
against adversarial or malicious inputs on their own. There are two main parts
to the suggested framework: (1) A prompt filtering module that uses
sophisticated Natural Language Processing (NLP) techniques, including zero-shot
classification, keyword analysis, and encoded content detection (e.g. base64,
hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and
(2) A summarization module that processes and summarizes adversarial research
literature to give the LLM context-aware defense knowledge. This approach
strengthens LLMs' resistance to adversarial exploitation by fusing text
extraction, summarization, and harmful prompt analysis. According to
experimental results, this integrated technique has a 98.71% success rate in
identifying harmful patterns, manipulative language structures, and encoded
prompts. By employing a modest amount of adversarial research literature as
context, the methodology also allows the model to react correctly to harmful
inputs with a larger percentage of jailbreak resistance and refusal rate. While
maintaining the quality of LLM responses, the framework dramatically increases
LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and
easy substitute for time-consuming, retraining-based defenses.

</details>


### [25] [TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References](https://arxiv.org/abs/2505.01325)
*Svenja Kenneweg,Jörg Deigmöller,Philipp Cimiano,Julian Eggert*

Main category: cs.CL

TL;DR: TRAVELER是一个新的合成基准数据集，用于评估模型在解决时间引用问题上的能力，涵盖显式、隐式和模糊时间引用。


<details>
  <summary>Details</summary>
Motivation: 现有基准对时间引用的系统评估有限，TRAVELER旨在填补这一空白。

Method: 通过问答范式构建数据集，包含3300个问题，并利用人类调查确定模糊时间引用的正确答案。

Result: 测试的四个LLM在显式时间引用和少量事件上表现良好，但随着事件数量增加或引用模糊，性能显著下降。

Conclusion: TRAVELER为评估LLM在时间引用处理上的能力提供了有效工具，尤其在模糊引用上表现较差。

Abstract: Understanding and resolving temporal references is essential in Natural
Language Understanding as we often refer to the past or future in daily
communication. Although existing benchmarks address a system's ability to
reason about and resolve temporal references, systematic evaluation of specific
temporal references remains limited. Towards closing this gap, we introduce
TRAVELER, a novel synthetic benchmark dataset that follows a Question Answering
paradigm and consists of questions involving temporal references with the
corresponding correct answers. TRAVELER assesses models' abilities to resolve
explicit, implicit relative to speech time, and vague temporal references.
Beyond investigating the performance of state-of-the-art LLMs depending on the
type of temporal reference, our benchmark also allows evaluation of performance
in relation to the length of the set of events. For the category of vague
temporal references, ground-truth answers were established via human surveys on
Prolific, following a procedure similar to the one from Kenneweg et al. To
demonstrate the benchmark's applicability, we evaluate four state-of-the-art
LLMs using a question-answering task encompassing 3,300 questions. Our findings
show that while the benchmarked LLMs can answer questions over event sets with
a handful of events and explicit temporal references successfully, performance
clearly deteriorates with larger event set length and when temporal references
get less explicit. Notably, the vague question category exhibits the lowest
performance across all models.
  The benchmark is publicly available at:
https://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [26] [Unconstrained Large-scale 3D Reconstruction and Rendering across Altitudes](https://arxiv.org/abs/2505.00734)
*Neil Joshi,Joshua Carney,Nathanael Kuo,Homer Li,Cheng Peng,Myron Brown*

Main category: cs.CV

TL;DR: 论文提出了一个公开基准数据集，用于解决3D重建和新视角合成中的现实挑战，如图像数量有限、相机异构、光照不一致和极端视角差异。


<details>
  <summary>Details</summary>
Motivation: 为灾难救援或执法等场景中缺乏高质量图像的问题提供研究支持。

Method: 开发基于多校准地面、安防和空中相机的数据集，独立评估未校准相机和新视角渲染质量。

Result: 展示了现有方法的基线性能，并指出了进一步研究的挑战。

Conclusion: 该数据集为3D重建和新视角合成研究提供了重要资源，推动了相关技术的发展。

Abstract: Production of photorealistic, navigable 3D site models requires a large
volume of carefully collected images that are often unavailable to first
responders for disaster relief or law enforcement. Real-world challenges
include limited numbers of images, heterogeneous unposed cameras, inconsistent
lighting, and extreme viewpoint differences for images collected from varying
altitudes. To promote research aimed at addressing these challenges, we have
developed the first public benchmark dataset for 3D reconstruction and novel
view synthesis based on multiple calibrated ground-level, security-level, and
airborne cameras. We present datasets that pose real-world challenges,
independently evaluate calibration of unposed cameras and quality of novel
rendered views, demonstrate baseline performance using recent state-of-practice
methods, and identify challenges for further research.

</details>


### [27] [MoSAM: Motion-Guided Segment Anything Model with Spatial-Temporal Memory Selection](https://arxiv.org/abs/2505.00739)
*Qiushi Yang,Yuan Yao,Miaomiao Cui,Liefeng Bo*

Main category: cs.CV

TL;DR: MoSAM通过引入运动引导提示和时空记忆选择机制，解决了SAM2在视频分割中依赖固定帧记忆的问题，提升了对象跟踪能力和分割准确性。


<details>
  <summary>Details</summary>
Motivation: SAM2在视频分割中仅依赖过去六帧的记忆，忽视了对象运动信息，导致长程跟踪能力受限和记忆不准确的问题。

Method: 提出MoSAM，结合运动引导提示（MGP）和时空记忆选择（ST-MS）机制，动态整合运动信息和优化记忆特征。

Result: 在多个视频对象分割和实例分割基准测试中，MoSAM取得了最先进的性能。

Conclusion: MoSAM通过运动信息和动态记忆优化，显著提升了视频分割的准确性和鲁棒性。

Abstract: The recent Segment Anything Model 2 (SAM2) has demonstrated exceptional
capabilities in interactive object segmentation for both images and videos.
However, as a foundational model on interactive segmentation, SAM2 performs
segmentation directly based on mask memory from the past six frames, leading to
two significant challenges. Firstly, during inference in videos, objects may
disappear since SAM2 relies solely on memory without accounting for object
motion information, which limits its long-range object tracking capabilities.
Secondly, its memory is constructed from fixed past frames, making it
susceptible to challenges associated with object disappearance or occlusion,
due to potentially inaccurate segmentation results in memory. To address these
problems, we present MoSAM, incorporating two key strategies to integrate
object motion cues into the model and establish more reliable feature memory.
Firstly, we propose Motion-Guided Prompting (MGP), which represents the object
motion in both sparse and dense manners, then injects them into SAM2 through a
set of motion-guided prompts. MGP enables the model to adjust its focus towards
the direction of motion, thereby enhancing the object tracking capabilities.
Furthermore, acknowledging that past segmentation results may be inaccurate, we
devise a Spatial-Temporal Memory Selection (ST-MS) mechanism that dynamically
identifies frames likely to contain accurate segmentation in both pixel- and
frame-level. By eliminating potentially inaccurate mask predictions from
memory, we can leverage more reliable memory features to exploit similar
regions for improving segmentation results. Extensive experiments on various
benchmarks of video object segmentation and video instance segmentation
demonstrate that our MoSAM achieves state-of-the-art results compared to other
competitors.

</details>


### [28] [Fast2comm:Collaborative perception combined with prior knowledge](https://arxiv.org/abs/2505.00740)
*Zhengbin Zhang,Yan Wu,Hongkun Zhang*

Main category: cs.CV

TL;DR: Fast2comm是一个基于先验知识的协作感知框架，通过生成高区分度的置信特征和优化特征选择策略，解决了协作感知中的性能与带宽平衡问题。


<details>
  <summary>Details</summary>
Motivation: 协作感知通过共享互补信息提升感知准确性，但面临性能与带宽平衡以及定位误差的挑战。

Method: 提出先验监督的置信特征生成方法、基于GT边界框的空间先验特征选择策略，并解耦训练与测试阶段的特征融合策略。

Result: 在真实和模拟数据集上的实验验证了模型的优越性能。

Conclusion: Fast2comm有效解决了协作感知中的关键问题，并展示了方法的必要性。

Abstract: Collaborative perception has the potential to significantly enhance
perceptual accuracy through the sharing of complementary information among
agents. However, real-world collaborative perception faces persistent
challenges, particularly in balancing perception performance and bandwidth
limitations, as well as coping with localization errors. To address these
challenges, we propose Fast2comm, a prior knowledge-based collaborative
perception framework. Specifically, (1)we propose a prior-supervised confidence
feature generation method, that effectively distinguishes foreground from
background by producing highly discriminative confidence features; (2)we
propose GT Bounding Box-based spatial prior feature selection strategy to
ensure that only the most informative prior-knowledge features are selected and
shared, thereby minimizing background noise and optimizing bandwidth efficiency
while enhancing adaptability to localization inaccuracies; (3)we decouple the
feature fusion strategies between model training and testing phases, enabling
dynamic bandwidth adaptation. To comprehensively validate our framework, we
conduct extensive experiments on both real-world and simulated datasets. The
results demonstrate the superior performance of our model and highlight the
necessity of the proposed methods. Our code is available at
https://github.com/Zhangzhengbin-TJ/Fast2comm.

</details>


### [29] [Detection and Classification of Diseases in Multi-Crop Leaves using LSTM and CNN Models](https://arxiv.org/abs/2505.00741)
*Srinivas Kanakala,Sneha Ningappa*

Main category: cs.CV

TL;DR: 该研究利用CNN和LSTM模型对植物叶片疾病进行分类，CNN模型表现最佳，验证准确率达96.4%。


<details>
  <summary>Details</summary>
Motivation: 植物疾病严重影响农业产量和食品质量，早期检测和分类对减少损失至关重要。

Method: 使用CNN和LSTM模型，数据集包含70,295张训练图像和17,572张验证图像，共38种疾病类别。CNN采用Adam优化器和分类交叉熵损失函数。

Result: CNN模型训练准确率99.1%，验证准确率96.4%；LSTM验证准确率93.43%。评估指标包括精确率、召回率、F1分数和混淆矩阵。

Conclusion: 深度学习模型（尤其是CNN）为植物疾病分类提供了高效、可扩展的解决方案，适用于农业监测。

Abstract: Plant diseases pose a serious challenge to agriculture by reducing crop yield
and affecting food quality. Early detection and classification of these
diseases are essential for minimising losses and improving crop management
practices. This study applies Convolutional Neural Networks (CNN) and Long
Short-Term Memory (LSTM) models to classify plant leaf diseases using a dataset
containing 70,295 training images and 17,572 validation images across 38
disease classes. The CNN model was trained using the Adam optimiser with a
learning rate of 0.0001 and categorical cross-entropy as the loss function.
After 10 training epochs, the model achieved a training accuracy of 99.1% and a
validation accuracy of 96.4%. The LSTM model reached a validation accuracy of
93.43%. Performance was evaluated using precision, recall, F1-score, and
confusion matrix, confirming the reliability of the CNN-based approach. The
results suggest that deep learning models, particularly CNN, enable an
effective solution for accurate and scalable plant disease classification,
supporting practical applications in agricultural monitoring.

</details>


### [30] [Zoomer: Adaptive Image Focus Optimization for Black-box MLLM](https://arxiv.org/abs/2505.00742)
*Jiaxu Qian,Chendong Wang,Yifan Yang,Chaoyun Zhang,Huiqiang Jiang,Xufang Luo,Yu Kang,Qingwei Lin,Anlan Zhang,Shiqi Jiang,Ting Cao,Tianjun Mao,Suman Banerjee,Guyue Liu,Saravan Rajmohan,Dongmei Zhang,Yuqing Yang,Qi Zhang,Lili Qiu*

Main category: cs.CV

TL;DR: 论文提出了一种名为\SysName的新型视觉提示机制，旨在提升多模态大语言模型（MLLMs）在视觉任务中的性能，同时保留关键视觉细节。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs在精确处理视觉数据（如对象识别和细节捕捉）方面表现不佳，且严格的token限制常导致关键信息丢失。

Method: \SysName包含三个创新：动态突出相关图像区域的提示感知策略、保持对象完整性的空间保留编排模式，以及平衡全局上下文与关键视觉细节的预算感知提示方法。

Result: 在多个数据集上的评估显示，\SysName显著优于基线方法，准确率提升高达26.9%，同时显著减少token消耗。

Conclusion: \SysName通过创新的视觉提示机制有效解决了MLLMs在视觉任务中的局限性，提升了性能并优化了资源使用。

Abstract: Recent advancements in multimodal large language models (MLLMs) have
broadened the scope of vision-language tasks, excelling in applications like
image captioning and interactive question-answering. However, these models
struggle with accurately processing visual data, particularly in tasks
requiring precise object recognition and fine visual details. Stringent token
limits often result in the omission of critical information, hampering
performance. To address these limitations, we introduce \SysName, a novel
visual prompting mechanism designed to enhance MLLM performance while
preserving essential visual details within token limits. \SysName features
three key innovations: a prompt-aware strategy that dynamically highlights
relevant image regions, a spatial-preserving orchestration schema that
maintains object integrity, and a budget-aware prompting method that balances
global context with crucial visual details. Comprehensive evaluations across
multiple datasets demonstrate that \SysName consistently outperforms baseline
methods, achieving up to a $26.9\%$ improvement in accuracy while significantly
reducing token consumption.

</details>


### [31] [DOPE: Dual Object Perception-Enhancement Network for Vision-and-Language Navigation](https://arxiv.org/abs/2505.00743)
*Yinfeng Yu,Dongsheng Yang*

Main category: cs.CV

TL;DR: 论文提出DOPE网络，通过文本语义提取和图像对象感知增强，提升视觉语言导航任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用语言指令的细节信息，且忽视跨模态对象关系建模，限制了导航的准确性和鲁棒性。

Method: 设计TSE提取关键文本短语，结合TOPA和IOPA增强对象感知，优化语言理解和跨模态对象关系建模。

Result: 在R2R和REVERIE数据集上的实验验证了DOPE的有效性。

Conclusion: DOPE通过增强对象感知和跨模态建模，显著提升了导航性能。

Abstract: Vision-and-Language Navigation (VLN) is a challenging task where an agent
must understand language instructions and navigate unfamiliar environments
using visual cues. The agent must accurately locate the target based on visual
information from the environment and complete tasks through interaction with
the surroundings. Despite significant advancements in this field, two major
limitations persist: (1) Many existing methods input complete language
instructions directly into multi-layer Transformer networks without fully
exploiting the detailed information within the instructions, thereby limiting
the agent's language understanding capabilities during task execution; (2)
Current approaches often overlook the modeling of object relationships across
different modalities, failing to effectively utilize latent clues between
objects, which affects the accuracy and robustness of navigation decisions. We
propose a Dual Object Perception-Enhancement Network (DOPE) to address these
issues to improve navigation performance. First, we design a Text Semantic
Extraction (TSE) to extract relatively essential phrases from the text and
input them into the Text Object Perception-Augmentation (TOPA) to fully
leverage details such as objects and actions within the instructions. Second,
we introduce an Image Object Perception-Augmentation (IOPA), which performs
additional modeling of object information across different modalities, enabling
the model to more effectively utilize latent clues between objects in images
and text, enhancing decision-making accuracy. Extensive experiments on the R2R
and REVERIE datasets validate the efficacy of the proposed approach.

</details>


### [32] [Localizing Before Answering: A Benchmark for Grounded Medical Visual Question Answering](https://arxiv.org/abs/2505.00744)
*Dung Nguyen,Minh Khoi Ho,Huy Ta,Thanh Tam Nguyen,Qi Chen,Kumar Rav,Quy Duong Dang,Satwik Ramchandre,Son Lam Phung,Zhibin Liao,Minh-Son To,Johan Verjans,Phi Le Nguyen,Vu Minh Hieu Phan*

Main category: cs.CV

TL;DR: 论文提出HEAL-MedVQA基准和LobA框架，解决医学LMMs在定位病理区域时的幻觉问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前医学LMMs在回答疾病相关问题时，常依赖语言模式或关注无关图像区域，导致幻觉生成。

Method: 引入HEAL-MedVQA基准评估定位能力，并提出LobA框架，通过定位目标区域并自我提示生成可靠答案。

Result: 实验表明LobA框架在HEAL-MedVQA上显著优于现有医学LMMs。

Conclusion: LobA框架提升了医学VQA的鲁棒性，为未来研究提供了新方向。

Abstract: Medical Large Multi-modal Models (LMMs) have demonstrated remarkable
capabilities in medical data interpretation. However, these models frequently
generate hallucinations contradicting source evidence, particularly due to
inadequate localization reasoning. This work reveals a critical limitation in
current medical LMMs: instead of analyzing relevant pathological regions, they
often rely on linguistic patterns or attend to irrelevant image areas when
responding to disease-related queries. To address this, we introduce
HEAL-MedVQA (Hallucination Evaluation via Localization MedVQA), a comprehensive
benchmark designed to evaluate LMMs' localization abilities and hallucination
robustness. HEAL-MedVQA features (i) two innovative evaluation protocols to
assess visual and textual shortcut learning, and (ii) a dataset of 67K VQA
pairs, with doctor-annotated anatomical segmentation masks for pathological
regions. To improve visual reasoning, we propose the Localize-before-Answer
(LobA) framework, which trains LMMs to localize target regions of interest and
self-prompt to emphasize segmented pathological areas, generating grounded and
reliable answers. Experimental results demonstrate that our approach
significantly outperforms state-of-the-art biomedical LMMs on the challenging
HEAL-MedVQA benchmark, advancing robustness in medical VQA.

</details>


### [33] [Responsive DNN Adaptation for Video Analytics against Environment Shift via Hierarchical Mobile-Cloud Collaborations](https://arxiv.org/abs/2505.00745)
*Maozhe Zhao,Shengzhong Liu,Fan Wu,Guihai Chen*

Main category: cs.CV

TL;DR: MOCHA框架通过移动与云资源的层次协作优化连续模型适应的响应速度，提升模型精度并减少延迟。


<details>
  <summary>Details</summary>
Motivation: 移动视频分析系统在环境变化时需要快速适应，现有云中心框架性能下降且反应延迟。

Method: MOCHA通过设备端模型重用、快速微调、结构化分类和本地模型缓存优化响应速度。

Result: 实验表明MOCHA在模型精度上提升6.8%，响应延迟减少35.5倍，重训练时间缩短3倍。

Conclusion: MOCHA通过层次协作显著提升了模型适应性和响应效率。

Abstract: Mobile video analysis systems often encounter various deploying environments,
where environment shifts present greater demands for responsiveness in
adaptations of deployed "expert DNN models". Existing model adaptation
frameworks primarily operate in a cloud-centric way, exhibiting degraded
performance during adaptation and delayed reactions to environment shifts.
Instead, this paper proposes MOCHA, a novel framework optimizing the
responsiveness of continuous model adaptation through hierarchical
collaborations between mobile and cloud resources. Specifically, MOCHA (1)
reduces adaptation response delays by performing on-device model reuse and fast
fine-tuning before requesting cloud model retrieval and end-to-end retraining;
(2) accelerates history expert model retrieval by organizing them into a
structured taxonomy utilizing domain semantics analyzed by a cloud foundation
model as indices; (3) enables efficient local model reuse by maintaining
onboard expert model caches for frequent scenes, which proactively prefetch
model weights from the cloud model database. Extensive evaluations with
real-world videos on three DNN tasks show MOCHA improves the model accuracy
during adaptation by up to 6.8% while saving the response delay and retraining
time by up to 35.5x and 3.0x respectively.

</details>


### [34] [Entropy Heat-Mapping: Localizing GPT-Based OCR Errors with Sliding-Window Shannon Analysis](https://arxiv.org/abs/2505.00746)
*Alexei Kaltchenko*

Main category: cs.CV

TL;DR: 论文提出了一种基于熵热图的方法，通过滑动窗口分析GPT-4o的令牌级置信度，定位OCR错误。实验表明，高熵区域确实集中了大部分实际错误。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（如GPT-4o）在数学文档转录中，令牌级置信度信号未被充分利用以定位识别错误。

Method: 使用熵热图将每个令牌的香农熵转化为视觉化的“不确定性景观”，并通过滑动窗口扫描熵序列，定位可能的OCR错误。

Result: 实验表明，大多数实际错误集中在高熵区域。

Conclusion: 滑动窗口熵分析是一种轻量级的后编辑辅助工具，可用于改进GPT-based OCR的准确性。

Abstract: Vision-language models such as OpenAI GPT-4o can transcribe mathematical
documents directly from images, yet their token-level confidence signals are
seldom used to pinpoint local recognition mistakes. We present an
entropy-heat-mapping proof-of-concept that turns per-token Shannon entropy into
a visual ''uncertainty landscape''. By scanning the entropy sequence with a
fixed-length sliding window, we obtain hotspots that are likely to contain OCR
errors such as missing symbols, mismatched braces, or garbled prose. Using a
small, curated set of scanned research pages rendered at several resolutions,
we compare the highlighted hotspots with the actual transcription errors
produced by GPT-4o. Our analysis shows that the vast majority of true errors
are indeed concentrated inside the high-entropy regions. This study
demonstrates--in a minimally engineered setting--that sliding-window entropy
can serve as a practical, lightweight aid for post-editing GPT-based OCR. All
code, sample data, and annotation guidelines are released to encourage
replication and further research.

</details>


### [35] [InstructAttribute: Fine-grained Object Attributes editing with Instruction](https://arxiv.org/abs/2505.00751)
*Xingxi Yin,Jingfeng Zhang,Zhi Li,Yicheng Li,Yin Zhang*

Main category: cs.CV

TL;DR: SPAA方法通过编辑自注意力和跨注意力图，实现了对物体颜色和材质的精确控制，同时保持图像结构和其他区域的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑技术难以精确控制细粒度属性或保持图像结构一致性，SPAA旨在解决这一问题。

Method: 提出SPAA方法，通过编辑自注意力和跨注意力图实现精确控制；构建Attribute Dataset，利用MLLM自动过滤和标注数据；训练InstructAttribute模型。

Result: 实验表明，SPAA在物体级颜色和材质编辑上优于现有方法。

Conclusion: SPAA为细粒度图像编辑提供了高效且无需训练的解决方案。

Abstract: Text-to-image (T2I) diffusion models, renowned for their advanced generative
abilities, are extensively utilized in image editing applications,
demonstrating remarkable effectiveness. However, achieving precise control over
fine-grained attributes still presents considerable challenges. Existing image
editing techniques either fail to modify the attributes of an object or
struggle to preserve its structure and maintain consistency in other areas of
the image. To address these challenges, we propose the Structure-Preserving and
Attribute Amplification (SPAA), a training-free method which enables precise
control over the color and material transformations of objects by editing the
self-attention maps and cross-attention values. Furthermore, we constructed the
Attribute Dataset, which encompasses nearly all colors and materials associated
with various objects, by integrating multimodal large language models (MLLM) to
develop an automated pipeline for data filtering and instruction labeling.
Training on this dataset, we present our InstructAttribute, an
instruction-based model designed to facilitate fine-grained editing of color
and material attributes. Extensive experiments demonstrate that our method
achieves superior performance in object-level color and material editing,
outperforming existing instruction-based image editing approaches.

</details>


### [36] [DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking](https://arxiv.org/abs/2505.00752)
*Xuzhao Li,Xuchen Li,Shiyu Hu*

Main category: cs.CV

TL;DR: DARTer是一种用于夜间无人机跟踪的端到端框架，通过动态特征融合和激活机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 夜间无人机跟踪因光照变化和视角变化导致性能下降，现有方法计算成本高或未能充分利用动态特征。

Method: 提出DARTer框架，结合动态特征混合器（DFB）和动态特征激活器（DFA），优化特征融合与计算效率。

Result: 在多个夜间无人机跟踪基准测试中表现优于现有方法，平衡了准确性和效率。

Conclusion: DARTer为实际夜间无人机跟踪应用提供了高效且准确的解决方案。

Abstract: Nighttime UAV tracking presents significant challenges due to extreme
illumination variations and viewpoint changes, which severely degrade tracking
performance. Existing approaches either rely on light enhancers with high
computational costs or introduce redundant domain adaptation mechanisms,
failing to fully utilize the dynamic features in varying perspectives. To
address these issues, we propose \textbf{DARTer} (\textbf{D}ynamic
\textbf{A}daptive \textbf{R}epresentation \textbf{T}racker), an end-to-end
tracking framework designed for nighttime UAV scenarios. DARTer leverages a
Dynamic Feature Blender (DFB) to effectively fuse multi-perspective nighttime
features from static and dynamic templates, enhancing representation
robustness. Meanwhile, a Dynamic Feature Activator (DFA) adaptively activates
Vision Transformer layers based on extracted features, significantly improving
efficiency by reducing redundant computations. Our model eliminates the need
for complex multi-task loss functions, enabling a streamlined training process.
Extensive experiments on multiple nighttime UAV tracking benchmarks demonstrate
the superiority of DARTer over state-of-the-art trackers. These results confirm
that DARTer effectively balances tracking accuracy and efficiency, making it a
promising solution for real-world nighttime UAV tracking applications.

</details>


### [37] [P2P-Insole: Human Pose Estimation Using Foot Pressure Distribution and Motion Sensors](https://arxiv.org/abs/2505.00755)
*Atsuya Watanabe,Ratna Aisuwarya,Lei Jing*

Main category: cs.CV

TL;DR: P2P-Insole是一种低成本方法，通过集成IMU的鞋垫传感器估计和可视化3D人体骨骼数据，适用于大规模生产。


<details>
  <summary>Details</summary>
Motivation: 解决现有商业解决方案成本高、侵入性强的问题，提供轻量、隐私友好的解决方案。

Method: 使用鞋垫压力分布、加速度和旋转数据，结合Transformer模型提取时间特征，并利用多模态信息提高识别精度。

Result: 实验证明该方法在复杂运动模式识别和姿态估计任务中具有鲁棒性。

Conclusion: P2P-Insole为康复、伤害预防和健康监测提供了低成本实用基础，未来可通过传感器优化和数据集扩展进一步发展。

Abstract: This work presents P2P-Insole, a low-cost approach for estimating and
visualizing 3D human skeletal data using insole-type sensors integrated with
IMUs. Each insole, fabricated with e-textile garment techniques, costs under
USD 1, making it significantly cheaper than commercial alternatives and ideal
for large-scale production. Our approach uses foot pressure distribution,
acceleration, and rotation data to overcome limitations, providing a
lightweight, minimally intrusive, and privacy-aware solution. The system
employs a Transformer model for efficient temporal feature extraction, enriched
by first and second derivatives in the input stream. Including multimodal
information, such as accelerometers and rotational measurements, improves the
accuracy of complex motion pattern recognition. These facts are demonstrated
experimentally, while error metrics show the robustness of the approach in
various posture estimation tasks. This work could be the foundation for a
low-cost, practical application in rehabilitation, injury prevention, and
health monitoring while enabling further development through sensor
optimization and expanded datasets.

</details>


### [38] [Efficient On-Chip Implementation of 4D Radar-Based 3D Object Detection on Hailo-8L](https://arxiv.org/abs/2505.00757)
*Woong-Chan Byun,Dong-Hee Paek,Seung-Hyun Song,Seung-Hyun Kong*

Main category: cs.CV

TL;DR: 本文提出了一种在Hailo-8L AI加速器上实现4D雷达3D目标检测的芯片级方案，通过张量变换方法解决5D输入与4D支持的兼容性问题，实现了实时处理并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 4D雷达在恶劣天气下仍能实现鲁棒的3D目标检测，但需在低功耗嵌入式环境中实现实时处理。

Method: 提出张量变换方法，将5D输入重塑为4D格式，兼容Hailo-8L的4D张量支持，无需修改模型结构。

Result: 系统达到46.47% AP_3D和52.75% AP_BEV，推理速度为13.76 Hz，精度与GPU模型相当。

Conclusion: 该技术证明了4D雷达感知在自动驾驶系统中的实用性。

Abstract: 4D radar has attracted attention in autonomous driving due to its ability to
enable robust 3D object detection even under adverse weather conditions. To
practically deploy such technologies, it is essential to achieve real-time
processing within low-power embedded environments. Addressing this, we present
the first on-chip implementation of a 4D radar-based 3D object detection model
on the Hailo-8L AI accelerator. Although conventional 3D convolutional neural
network (CNN) architectures require 5D inputs, the Hailo-8L only supports 4D
tensors, posing a significant challenge. To overcome this limitation, we
introduce a tensor transformation method that reshapes 5D inputs into 4D
formats during the compilation process, enabling direct deployment without
altering the model structure. The proposed system achieves 46.47% AP_3D and
52.75% AP_BEV, maintaining comparable accuracy to GPU-based models while
achieving an inference speed of 13.76 Hz. These results demonstrate the
applicability of 4D radar-based perception technologies to autonomous driving
systems.

</details>


### [39] [Multi-Modal Language Models as Text-to-Image Model Evaluators](https://arxiv.org/abs/2505.00759)
*Jiahui Chen,Candace Ross,Reyhane Askari-Hemmat,Koustuv Sinha,Melissa Hall,Michal Drozdzal,Adriana Romero-Soriano*

Main category: cs.CV

TL;DR: MT2IE框架利用多模态大语言模型评估文本到图像生成模型的性能，显著减少所需提示数量，并与人类判断更相关。


<details>
  <summary>Details</summary>
Motivation: 由于静态数据集评估方法逐渐过时，研究探索多模态大语言模型作为评估代理，以提升评估效率和相关性。

Method: 提出MT2IE框架，通过迭代生成提示、评分图像，并与现有基准匹配，仅需少量提示即可完成评估。

Result: MT2IE的提示生成一致性评分与人类判断相关性更高，且仅需1/80的提示数量即可达到相同模型排名效果。

Conclusion: MT2IE为文本到图像模型评估提供了一种高效且相关性更高的新方法。

Abstract: The steady improvements of text-to-image (T2I) generative models lead to slow
deprecation of automatic evaluation benchmarks that rely on static datasets,
motivating researchers to seek alternative ways to evaluate the T2I progress.
In this paper, we explore the potential of multi-modal large language models
(MLLMs) as evaluator agents that interact with a T2I model, with the objective
of assessing prompt-generation consistency and image aesthetics. We present
Multimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively
generates prompts for evaluation, scores generated images and matches T2I
evaluation of existing benchmarks with a fraction of the prompts used in
existing static benchmarks. Moreover, we show that MT2IE's prompt-generation
consistency scores have higher correlation with human judgment than scores
previously introduced in the literature. MT2IE generates prompts that are
efficient at probing T2I model performance, producing the same relative T2I
model rankings as existing benchmarks while using only 1/80th the number of
prompts for evaluation.

</details>


### [40] [Person detection and re-identification in open-world settings of retail stores and public spaces](https://arxiv.org/abs/2505.00772)
*Branko Brkljač,Milan Brkljač*

Main category: cs.CV

TL;DR: 论文探讨了智能城市中计算机视觉的实际应用，特别是开放环境下的人员重识别任务，分析了系统设计挑战、解决方案及其在零售和公共空间的应用。


<details>
  <summary>Details</summary>
Motivation: 智能城市中的人员重识别任务面临开放环境的复杂性和多摄像头、多人员场景的挑战，需高效解决方案。

Method: 讨论了系统设计架构的挑战，提出了基于计算机视觉技术的解决方案，并测试了一种近实时方案在不同视频和实时摄像头中的性能。

Result: 通过实验验证了方案的性能，并展示了其在零售和公共空间中的应用潜力。

Conclusion: 总结了当前研究的局限性，并指出了未来改进方向和研究重点。

Abstract: Practical applications of computer vision in smart cities usually assume
system integration and operation in challenging open-world environments. In the
case of person re-identification task the main goal is to retrieve information
whether the specific person has appeared in another place at a different time
instance of the same video, or over multiple camera feeds. This typically
assumes collecting raw data from video surveillance cameras in different places
and under varying illumination conditions. In the considered open-world setting
it also requires detection and localization of the person inside the analyzed
video frame before the main re-identification step. With multi-person and
multi-camera setups the system complexity becomes higher, requiring
sophisticated tracking solutions and re-identification models. In this work we
will discuss existing challenges in system design architectures, consider
possible solutions based on different computer vision techniques, and describe
applications of such systems in retail stores and public spaces for improved
marketing analytics. In order to analyse sensitivity of person
re-identification task under different open-world environments, a performance
of one close to real-time solution will be demonstrated over several video
captures and live camera feeds. Finally, based on conducted experiments we will
indicate further research directions and possible system improvements.

</details>


### [41] [AI-ready Snow Radar Echogram Dataset (SRED) for climate change monitoring](https://arxiv.org/abs/2505.00786)
*Oluwanisola Ibikunle,Hara Talasila,Debvrat Varshney,Jilu Li,John Paden,Maryam Rahnemoonfar*

Main category: cs.CV

TL;DR: 该研究首次提供了一个标准化的雷达回波图数据集，用于深度学习算法测试和比较，并评估了五种模型的性能。


<details>
  <summary>Details</summary>
Motivation: 高精度追踪雷达回波图中的内部冰层对理解冰盖动态和全球气候变暖的影响至关重要，但缺乏标准化数据集限制了算法的发展。

Method: 研究基于NASA OIB任务的Snow Radar数据，构建了包含标记和弱标记回波图的数据集，并测试了五种深度学习模型。

Result: 结果表明，现有计算机视觉算法能追踪冰层像素，但需要更先进的端到端模型以减少后处理。

Conclusion: 该数据集和基准框架为冰层追踪和积雪估算提供了宝贵资源，有助于理解极地冰盖对气候变暖的响应。

Abstract: Tracking internal layers in radar echograms with high accuracy is essential
for understanding ice sheet dynamics and quantifying the impact of accelerated
ice discharge in Greenland and other polar regions due to contemporary global
climate warming. Deep learning algorithms have become the leading approach for
automating this task, but the absence of a standardized and well-annotated
echogram dataset has hindered the ability to test and compare algorithms
reliably, limiting the advancement of state-of-the-art methods for the radar
echogram layer tracking problem. This study introduces the first comprehensive
``deep learning ready'' radar echogram dataset derived from Snow Radar airborne
data collected during the National Aeronautics and Space Administration
Operation Ice Bridge (OIB) mission in 2012. The dataset contains 13,717 labeled
and 57,815 weakly-labeled echograms covering diverse snow zones (dry, ablation,
wet) with varying along-track resolutions. To demonstrate its utility, we
evaluated the performance of five deep learning models on the dataset. Our
results show that while current computer vision segmentation algorithms can
identify and track snow layer pixels in echogram images, advanced end-to-end
models are needed to directly extract snow depth and annual accumulation from
echograms, reducing or eliminating post-processing. The dataset and
accompanying benchmarking framework provide a valuable resource for advancing
radar echogram layer tracking and snow accumulation estimation, advancing our
understanding of polar ice sheets response to climate warming.

</details>


### [42] [SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models](https://arxiv.org/abs/2505.00788)
*Wufei Ma,Luoxin Ye,Nessa McWeeney,Celso M de Melo,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: 论文提出SpatialLLM模型，通过3D数据增强和架构设计提升多模态模型的3D空间推理能力，性能超越GPT-4o 8.7%。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型缺乏3D空间推理能力，主要因数据稀缺和设计偏向2D数据。

Method: 开发3D感知的训练数据集（3D位置和方向数据、复杂空间关系对话数据），并系统整合到模型架构和训练中。

Result: SpatialLLM在3D推理能力上表现优异，性能超过GPT-4o 8.7%。

Conclusion: 研究为未来3D推理模型设计提供了系统化方法和数据支持。

Abstract: Humans naturally understand 3D spatial relationships, enabling complex
reasoning like predicting collisions of vehicles from different directions.
Current large multimodal models (LMMs), however, lack of this capability of 3D
spatial reasoning. This limitation stems from the scarcity of 3D training data
and the bias in current model designs toward 2D data. In this paper, we
systematically study the impact of 3D-informed data, architecture, and training
setups, introducing SpatialLLM, a large multi-modal model with advanced 3D
spatial reasoning abilities. To address data limitations, we develop two types
of 3D-informed training datasets: (1) 3D-informed probing data focused on
object's 3D location and orientation, and (2) 3D-informed conversation data for
complex spatial relationships. Notably, we are the first to curate VQA data
that incorporate 3D orientation relationships on real images. Furthermore, we
systematically integrate these two types of training data with the
architectural and training designs of LMMs, providing a roadmap for optimal
design aimed at achieving superior 3D reasoning capabilities. Our SpatialLLM
advances machines toward highly capable 3D-informed reasoning, surpassing
GPT-4o performance by 8.7%. Our systematic empirical design and the resulting
findings offer valuable insights for future research in this direction.

</details>


### [43] [Advancing Wheat Crop Analysis: A Survey of Deep Learning Approaches Using Hyperspectral Imaging](https://arxiv.org/abs/2505.00805)
*Fadi Abdeladhim Zidi,Abdelkrim Ouafi,Fares Bougourzi,Cosimo Distante,Abdelmalik Taleb-Ahmed*

Main category: cs.CV

TL;DR: 综述探讨了深度学习在高光谱成像（HSI）小麦作物分析中的应用，填补了该领域缺乏全面调查的空白。


<details>
  <summary>Details</summary>
Motivation: 小麦生产面临病虫害、气候变化等挑战，传统监测方法效率低。HSI结合深度学习有望解决这些问题。

Method: 总结了基准数据集，追踪了深度学习方法进展，并分析了品种分类、病害检测和产量估计等应用。

Result: 综述展示了深度学习在HSI小麦分析中的潜力，同时指出了现有方法的局限性和未来机会。

Conclusion: 深度学习为HSI小麦分析提供了高效工具，未来需进一步优化方法和数据可用性。

Abstract: As one of the most widely cultivated and consumed crops, wheat is essential
to global food security. However, wheat production is increasingly challenged
by pests, diseases, climate change, and water scarcity, threatening yields.
Traditional crop monitoring methods are labor-intensive and often ineffective
for early issue detection. Hyperspectral imaging (HSI) has emerged as a
non-destructive and efficient technology for remote crop health assessment.
However, the high dimensionality of HSI data and limited availability of
labeled samples present notable challenges. In recent years, deep learning has
shown great promise in addressing these challenges due to its ability to
extract and analysis complex structures. Despite advancements in applying deep
learning methods to HSI data for wheat crop analysis, no comprehensive survey
currently exists in this field. This review addresses this gap by summarizing
benchmark datasets, tracking advancements in deep learning methods, and
analyzing key applications such as variety classification, disease detection,
and yield estimation. It also highlights the strengths, limitations, and future
opportunities in leveraging deep learning methods for HSI-based wheat crop
analysis. We have listed the current state-of-the-art papers and will continue
tracking updating them in the following
https://github.com/fadi-07/Awesome-Wheat-HSI-DeepLearning.

</details>


### [44] [The Comparability of Model Fusion to Measured Data in Confuser Rejection](https://arxiv.org/abs/2505.00836)
*Conor Flynn,Christopher Ebersole,Edmund Zelnio*

Main category: cs.CV

TL;DR: 论文提出通过集成多个基于合成数据训练的模型，以弥补合成孔径雷达（SAR）数据不足的问题，并引入干扰物拒绝技术处理未知目标。


<details>
  <summary>Details</summary>
Motivation: 解决SAR数据收集成本高、样本不足的问题，以及合成数据与实测数据不完全匹配的挑战。

Method: 使用合成数据训练多个模型，并通过集成技术和干扰物拒绝技术处理未知目标。

Result: 通过集成模型和干扰物拒绝技术，提高了模型在实测数据上的泛化能力。

Conclusion: 集成合成数据训练的模型和干扰物拒绝技术是解决SAR数据不足的有效方法。

Abstract: Data collection has always been a major issue in the modeling and training of
large deep learning networks, as no dataset can account for every slight
deviation we might see in live usage. Collecting samples can be especially
costly for Synthetic Aperture Radar (SAR), limiting the amount of unique
targets and operating conditions we are able to observe from. To counter this
lack of data, simulators have been developed utilizing the shooting and
bouncing ray method to allow for the generation of synthetic SAR data on 3D
models. While effective, the synthetically generated data does not perfectly
correlate to the measured data leading to issues when training models solely on
synthetic data. We aim to use computational power as a substitution for this
lack of quality measured data, by ensembling many models trained on synthetic
data. Synthetic data is also not complete, as we do not know what targets might
be present in a live environment. Therefore we need to have our ensembling
techniques account for these unknown targets by applying confuser rejection in
which our models will reject unknown targets it is presented with, and only
classify those it has been trained on.

</details>


### [45] [Are Minimal Radial Distortion Solvers Really Necessary for Relative Pose Estimation?](https://arxiv.org/abs/2505.00866)
*Viktor Kocur,Charalambos Tzamos,Yaqing Ding,Zuzana Berger Haladova,Torsten Sattler,Zuzana Kukelova*

Main category: cs.CV

TL;DR: 论文比较了两种简单实现方法（采样径向畸变参数和基于学习的参数估计）与复杂的最小径向畸变求解器，发现后者在实践中并非必要。


<details>
  <summary>Details</summary>
Motivation: 解决径向畸变对相机相对位姿估计的影响，避免复杂求解器的高实现和运行成本。

Method: 1. 结合高效针孔求解器与采样径向畸变参数；2. 使用神经网络估计畸变参数。

Result: 实验表明，复杂的最小径向畸变求解器在实践中不必要，简单方法效果相当。

Conclusion: 在特定条件下，采样径向畸变参数或基于学习的方法优于复杂求解器。

Abstract: Estimating the relative pose between two cameras is a fundamental step in
many applications such as Structure-from-Motion. The common approach to
relative pose estimation is to apply a minimal solver inside a RANSAC loop.
Highly efficient solvers exist for pinhole cameras. Yet, (nearly) all cameras
exhibit radial distortion. Not modeling radial distortion leads to
(significantly) worse results. However, minimal radial distortion solvers are
significantly more complex than pinhole solvers, both in terms of run-time and
implementation efforts. This paper compares radial distortion solvers with two
simple-to-implement approaches that do not use minimal radial distortion
solvers: The first approach combines an efficient pinhole solver with sampled
radial undistortion parameters, where the sampled parameters are used for
undistortion prior to applying the pinhole solver. The second approach uses a
state-of-the-art neural network to estimate the distortion parameters rather
than sampling them from a set of potential values. Extensive experiments on
multiple datasets, and different camera setups, show that complex minimal
radial distortion solvers are not necessary in practice. We discuss under which
conditions a simple sampling of radial undistortion parameters is preferable
over calibrating cameras using a learning-based prior approach. Code and newly
created benchmark for relative pose estimation under radial distortion are
available at https://github.com/kocurvik/rdnet.

</details>


### [46] [CDFormer: Cross-Domain Few-Shot Object Detection Transformer Against Feature Confusion](https://arxiv.org/abs/2505.00938)
*Boyuan Meng,Xiaohan Zhang,Peilin Li,Zhe Wu,Yiming Li,Wenkai Zhao,Beinan Yu,Hui-Liang Shen*

Main category: cs.CV

TL;DR: CDFormer是一种针对跨域少样本目标检测中特征混淆问题的Transformer方法，通过OBD和OOD模块显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 跨域少样本目标检测中，特征混淆（如目标-背景混淆和目标间混淆）是主要挑战。

Method: 提出CDFormer，包含OBD模块（区分目标与背景）和OOD模块（区分不同类别目标）。

Result: 实验显示，CDFormer在1/5/10 shot设置下分别提升12.9%、11.0%和10.4%的mAP。

Conclusion: CDFormer有效解决了特征混淆问题，性能优于现有方法。

Abstract: Cross-domain few-shot object detection (CD-FSOD) aims to detect novel objects
across different domains with limited class instances. Feature confusion,
including object-background confusion and object-object confusion, presents
significant challenges in both cross-domain and few-shot settings. In this
work, we introduce CDFormer, a cross-domain few-shot object detection
transformer against feature confusion, to address these challenges. The method
specifically tackles feature confusion through two key modules:
object-background distinguishing (OBD) and object-object distinguishing (OOD).
The OBD module leverages a learnable background token to differentiate between
objects and background, while the OOD module enhances the distinction between
objects of different classes. Experimental results demonstrate that CDFormer
outperforms previous state-of-the-art approaches, achieving 12.9% mAP, 11.0%
mAP, and 10.4% mAP improvements under the 1/5/10 shot settings, respectively,
when fine-tuned.

</details>


### [47] [Generating Animated Layouts as Structured Text Representations](https://arxiv.org/abs/2505.00975)
*Yeonsang Shin,Jihwan Kim,Yumin Song,Kyungseung Lee,Hyunhee Chung,Taeyoung Na*

Main category: cs.CV

TL;DR: 论文提出了一种名为Animated Layout Generation的新方法，通过结构化文本表示实现细粒度视频控制，并开发了VAKER工具，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频模型在控制文本元素和动态图形方面存在不足，尤其是在视频广告应用中。

Method: 提出Animated Layout Generation方法，结合结构化文本表示和三阶段生成流程，开发了VAKER工具。

Result: VAKER在视频广告生成中显著优于现有方法。

Conclusion: 该方法为视频广告生成提供了高效且自动化的解决方案。

Abstract: Despite the remarkable progress in text-to-video models, achieving precise
control over text elements and animated graphics remains a significant
challenge, especially in applications such as video advertisements. To address
this limitation, we introduce Animated Layout Generation, a novel approach to
extend static graphic layouts with temporal dynamics. We propose a Structured
Text Representation for fine-grained video control through hierarchical visual
elements. To demonstrate the effectiveness of our approach, we present VAKER
(Video Ad maKER), a text-to-video advertisement generation pipeline that
combines a three-stage generation process with Unstructured Text Reasoning for
seamless integration with LLMs. VAKER fully automates video advertisement
generation by incorporating dynamic layout trajectories for objects and
graphics across specific video frames. Through extensive evaluations, we
demonstrate that VAKER significantly outperforms existing methods in generating
video advertisements. Project Page:
https://yeonsangshin.github.io/projects/Vaker

</details>


### [48] [LMDepth: Lightweight Mamba-based Monocular Depth Estimation for Real-World Deployment](https://arxiv.org/abs/2505.00980)
*Jiahuan Long,Xin Zhou*

Main category: cs.CV

TL;DR: LMDepth是一种基于Mamba的轻量级单目深度估计网络，旨在平衡性能与计算效率，适用于资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 现有深度估计算法难以平衡性能与计算效率，限制了在资源受限设备上的部署。

Method: 提出改进的金字塔空间池化模块和多深度Mamba块，以线性计算高效解码全局特征。

Result: 在NYUDv2和KITTI数据集上表现优异，参数和计算复杂度更低。

Conclusion: LMDepth在边缘设备上实用性强，验证了其实际应用潜力。

Abstract: Monocular depth estimation provides an additional depth dimension to RGB
images, making it widely applicable in various fields such as virtual reality,
autonomous driving and robotic navigation. However, existing depth estimation
algorithms often struggle to effectively balance performance and computational
efficiency, which poses challenges for deployment on resource-constrained
devices. To address this, we propose LMDepth, a lightweight Mamba-based
monocular depth estimation network, designed to reconstruct high-precision
depth information while maintaining low computational overhead. Specifically,
we propose a modified pyramid spatial pooling module that serves as a
multi-scale feature aggregator and context extractor, ensuring global spatial
information for accurate depth estimation. Moreover, we integrate multiple
depth Mamba blocks into the decoder. Designed with linear computations, the
Mamba Blocks enable LMDepth to efficiently decode depth information from global
features, providing a lightweight alternative to Transformer-based
architectures that depend on complex attention mechanisms. Extensive
experiments on the NYUDv2 and KITTI datasets demonstrate the effectiveness of
our proposed LMDepth. Compared to previous lightweight depth estimation
methods, LMDepth achieves higher performance with fewer parameters and lower
computational complexity (measured by GFLOPs). We further deploy LMDepth on an
embedded platform with INT8 quantization, validating its practicality for
real-world edge applications.

</details>


### [49] [Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis](https://arxiv.org/abs/2505.00998)
*Yu Hua,Weiming Liu,Gui Xu,Yaqing Hou,Yew-Soon Ong,Qiang Zhang*

Main category: cs.CV

TL;DR: 提出了一种确定性到随机性的多样化潜在特征映射（DSDFM）方法，用于人体运动合成，解决了基于分数生成模型（SGMs）训练不稳定的问题，并提升了生成运动的多样性和准确性。


<details>
  <summary>Details</summary>
Motivation: 基于分数生成模型（SGMs）在人体运动合成中表现出色，但其训练过程复杂且不稳定，需要一种更稳定且能增强多样性的方法。

Method: DSDFM分为两个阶段：1）人体运动重建阶段，学习潜在空间分布；2）多样化运动生成阶段，通过确定性特征映射（DerODE）和随机多样化输出生成（DivSDE）连接高斯分布与潜在空间分布。

Result: DSDFM在训练上更简单，无需额外参数即可增强多样性，实验表明其在人体运动合成中达到了最先进的性能。

Conclusion: DSDFM是一种高效且稳定的方法，显著提升了人体运动合成的多样性和准确性。

Abstract: Human motion synthesis aims to generate plausible human motion sequences,
which has raised widespread attention in computer animation. Recent score-based
generative models (SGMs) have demonstrated impressive results on this task.
However, their training process involves complex curvature trajectories,
leading to unstable training process. In this paper, we propose a
Deterministic-to-Stochastic Diverse Latent Feature Mapping (DSDFM) method for
human motion synthesis. DSDFM consists of two stages. The first human motion
reconstruction stage aims to learn the latent space distribution of human
motions. The second diverse motion generation stage aims to build connections
between the Gaussian distribution and the latent space distribution of human
motions, thereby enhancing the diversity and accuracy of the generated human
motions. This stage is achieved by the designed deterministic feature mapping
procedure with DerODE and stochastic diverse output generation procedure with
DivSDE.DSDFM is easy to train compared to previous SGMs-based methods and can
enhance diversity without introducing additional training parameters.Through
qualitative and quantitative experiments, DSDFM achieves state-of-the-art
results surpassing the latest methods, validating its superiority in human
motion synthesis.

</details>


### [50] [3D Human Pose Estimation via Spatial Graph Order Attention and Temporal Body Aware Transformer](https://arxiv.org/abs/2505.01003)
*Kamel Aouaidjia,Aofan Li,Wenhao Zhang,Chongsheng Zhang*

Main category: cs.CV

TL;DR: 提出了一种结合GCN和Transformer的新方法，通过多阶图表示骨架并动态关注代表性阶数，同时利用改进的Transformer建模时空特征。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D人体姿态估计中忽视了空间邻域关系或局部时间模式，GCN方法也缺乏姿态特定表示。

Method: 使用GCN的多阶图表示骨架，引入Graph Order Attention模块动态选择阶数，结合改进的Body Aware Transformer建模时空特征。

Result: 在Human3.6m等数据集上验证了方法的有效性。

Conclusion: 新方法通过结合GCN和Transformer的优势，显著提升了3D姿态估计性能。

Abstract: Nowadays, Transformers and Graph Convolutional Networks (GCNs) are the
prevailing techniques for 3D human pose estimation. However, Transformer-based
methods either ignore the spatial neighborhood relationships between the joints
when used for skeleton representations or disregard the local temporal patterns
of the local joint movements in skeleton sequence modeling, while GCN-based
methods often neglect the need for pose-specific representations. To address
these problems, we propose a new method that exploits the graph modeling
capability of GCN to represent each skeleton with multiple graphs of different
orders, incorporated with a newly introduced Graph Order Attention module that
dynamically emphasizes the most representative orders for each joint. The
resulting spatial features of the sequence are further processed using a
proposed temporal Body Aware Transformer that models the global body feature
dependencies in the sequence with awareness of the local inter-skeleton feature
dependencies of joints. Given that our 3D pose output aligns with the central
2D pose in the sequence, we improve the self-attention mechanism to be aware of
the central pose while diminishing its focus gradually towards the first and
the last poses. Extensive experiments on Human3.6m, MPIINF-3DHP, and HumanEva-I
datasets demonstrate the effectiveness of the proposed method. Code and models
are made available on Github.

</details>


### [51] [Fine-Tuning Without Forgetting: Adaptation of YOLOv8 Preserves COCO Performance](https://arxiv.org/abs/2505.01016)
*Vishal Gandhi,Sagar Gandhi*

Main category: cs.CV

TL;DR: 研究表明，深度微调预训练目标检测模型（如YOLOv8n）在细粒度任务中表现更优，且不会显著影响原始通用性能。


<details>
  <summary>Details</summary>
Motivation: 探讨预训练模型在细粒度任务中的微调深度，以优化性能同时避免灾难性遗忘。

Method: 通过逐步解冻YOLOv8n的骨干网络层（22、15、10层），在细粒度水果检测数据集上微调，并评估其在目标数据集和原始COCO验证集上的表现。

Result: 深度微调（解冻至第10层）显著提升细粒度任务性能（如mAP50提高10%），且对COCO基准的影响极小（<0.1% mAP差异）。

Conclusion: 深度微调中后期骨干网络层对细粒度任务有效且安全，为复杂领域的高性能优化提供了新思路。

Abstract: The success of large pre-trained object detectors hinges on their
adaptability to diverse downstream tasks. While fine-tuning is the standard
adaptation method, specializing these models for challenging fine-grained
domains necessitates careful consideration of feature granularity. The critical
question remains: how deeply should the pre-trained backbone be fine-tuned to
optimize for the specialized task without incurring catastrophic forgetting of
the original general capabilities? Addressing this, we present a systematic
empirical study evaluating the impact of fine-tuning depth. We adapt a standard
YOLOv8n model to a custom, fine-grained fruit detection dataset by
progressively unfreezing backbone layers (freeze points at layers 22, 15, and
10) and training. Performance was rigorously evaluated on both the target fruit
dataset and, using a dual-head evaluation architecture, on the original COCO
validation set. Our results demonstrate unequivocally that deeper fine-tuning
(unfreezing down to layer 10) yields substantial performance gains (e.g., +10\%
absolute mAP50) on the fine-grained fruit task compared to only training the
head. Strikingly, this significant adaptation and specialization resulted in
negligible performance degradation (<0.1\% absolute mAP difference) on the COCO
benchmark across all tested freeze levels. We conclude that adapting
mid-to-late backbone features is highly effective for fine-grained
specialization. Critically, our results demonstrate this adaptation can be
achieved without the commonly expected penalty of catastrophic forgetting,
presenting a compelling case for exploring deeper fine-tuning strategies,
particularly when targeting complex domains or when maximizing specialized
performance is paramount.

</details>


### [52] [Edge-preserving Image Denoising via Multi-scale Adaptive Statistical Independence Testing](https://arxiv.org/abs/2505.01032)
*Ruyu Yan,Da-Qing Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于多尺度自适应独立性测试的边缘检测与去噪方法（EDD-MAIT），通过动态调整窗口大小和结合通道注意力机制，显著提升了边缘检测的清晰度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有边缘检测方法常因过度细节或固定窗口统计测试的局限性（如尺度不匹配和计算冗余）导致效果不佳。

Method: 结合多尺度自适应统计测试和通道注意力机制，采用梯度驱动的自适应窗口策略动态调整窗口大小。

Result: 在BSDS500和BIPED数据集上表现优于传统和基于学习的方法，F-score、MSE、PSNR等指标均有提升，且运行时间更短。对高斯噪声也表现出强鲁棒性。

Conclusion: EDD-MAIT方法在边缘检测和去噪方面具有显著优势，适用于噪声环境下的高精度边缘提取。

Abstract: Edge detection is crucial in image processing, but existing methods often
produce overly detailed edge maps, affecting clarity. Fixed-window statistical
testing faces issues like scale mismatch and computational redundancy. To
address these, we propose a novel Multi-scale Adaptive Independence
Testing-based Edge Detection and Denoising (EDD-MAIT), a Multi-scale Adaptive
Statistical Testing-based edge detection and denoising method that integrates a
channel attention mechanism with independence testing. A gradient-driven
adaptive window strategy adjusts window sizes dynamically, improving detail
preservation and noise suppression. EDD-MAIT achieves better robustness,
accuracy, and efficiency, outperforming traditional and learning-based methods
on BSDS500 and BIPED datasets, with improvements in F-score, MSE, PSNR, and
reduced runtime. It also shows robustness against Gaussian noise, generating
accurate and clean edge maps in noisy environments.

</details>


### [53] [Edge Detection based on Channel Attention and Inter-region Independence Test](https://arxiv.org/abs/2505.01040)
*Ru-yu Yan,Da-Qing Zhang*

Main category: cs.CV

TL;DR: CAM-EDIT结合通道注意力机制和独立性测试的边缘检测框架，显著提升了边缘检测性能，适用于高精度工业场景。


<details>
  <summary>Details</summary>
Motivation: 现有边缘检测方法存在噪声放大和非显著细节保留过多的问题，限制了其在工业场景中的应用。

Method: 提出CAM-EDIT框架，结合CAM模块（多通道融合增强边缘特征）和EDIT模块（统计独立性分析抑制噪声）。

Result: 在BSDS500和NYUDv2数据集上表现优异，F-measure分别提升19.2%至26.5%，噪声鲁棒性PSNR提高2.2%。

Conclusion: CAM-EDIT在边缘检测中表现出色，适用于高精度工业应用。

Abstract: Existing edge detection methods often suffer from noise amplification and
excessive retention of non-salient details, limiting their applicability in
high-precision industrial scenarios. To address these challenges, we propose
CAM-EDIT, a novel framework that integrates Channel Attention Mechanism (CAM)
and Edge Detection via Independence Testing (EDIT). The CAM module adaptively
enhances discriminative edge features through multi-channel fusion, while the
EDIT module employs region-wise statistical independence analysis (using
Fisher's exact test and chi-square test) to suppress uncorrelated
noise.Extensive experiments on BSDS500 and NYUDv2 datasets demonstrate
state-of-the-art performance. Among the nine comparison algorithms, the
F-measure scores of CAM-EDIT are 0.635 and 0.460, representing improvements of
19.2\% to 26.5\% over traditional methods (Canny, CannySR), and better than the
latest learning based methods (TIP2020, MSCNGP). Noise robustness evaluations
further reveal a 2.2\% PSNR improvement under Gaussian noise compared to
baseline methods. Qualitative results exhibit cleaner edge maps with reduced
artifacts, demonstrating its potential for high-precision industrial
applications.

</details>


### [54] [Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages](https://arxiv.org/abs/2505.01096)
*Marco Salmè,Rosa Sicilia,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 研究评估了指令调优的视觉语言模型（VLMs）在低资源语言（意大利语、德语、西班牙语）中生成放射学报告的性能，发现语言和领域特定训练对提升报告质量至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在医疗领域有广泛应用，但在低资源语言中生成准确且上下文相关的放射学报告仍具挑战性。

Method: 采用LLaVA架构，系统评估了预训练模型在通用、领域特定和低资源语言特定数据集上的表现，并分析了不同适应的效果。

Result: 语言特定模型表现最优，医学术语微调进一步提升了性能；温度参数对报告连贯性有影响。

Conclusion: 语言和领域特定训练对多语言放射学报告生成至关重要，为未来模型调优和语言适应研究提供了方向。

Abstract: The integration of artificial intelligence in healthcare has opened new
horizons for improving medical diagnostics and patient care. However,
challenges persist in developing systems capable of generating accurate and
contextually relevant radiology reports, particularly in low-resource
languages. In this study, we present a comprehensive benchmark to evaluate the
performance of instruction-tuned Vision-Language Models (VLMs) in the
specialized task of radiology report generation across three low-resource
languages: Italian, German, and Spanish. Employing the LLaVA architectural
framework, we conducted a systematic evaluation of pre-trained models utilizing
general datasets, domain-specific datasets, and low-resource language-specific
datasets. In light of the unavailability of models that possess prior knowledge
of both the medical domain and low-resource languages, we analyzed various
adaptations to determine the most effective approach for these contexts. The
results revealed that language-specific models substantially outperformed both
general and domain-specific models in generating radiology reports, emphasizing
the critical role of linguistic adaptation. Additionally, models fine-tuned
with medical terminology exhibited enhanced performance across all languages
compared to models with generic knowledge, highlighting the importance of
domain-specific training. We also explored the influence of the temperature
parameter on the coherence of report generation, providing insights for optimal
model settings. Our findings highlight the importance of tailored language and
domain-specific training for improving the quality and accuracy of radiological
reports in multilingual settings. This research not only advances our
understanding of VLMs adaptability in healthcare but also points to significant
avenues for future investigations into model tuning and language-specific
adaptations.

</details>


### [55] [Transferable Adversarial Attacks on Black-Box Vision-Language Models](https://arxiv.org/abs/2505.01050)
*Kai Hu,Weichen Yu,Li Zhang,Alexander Robey,Andy Zou,Chengming Xu,Haoqi Hu,Matt Fredrikson*

Main category: cs.CV

TL;DR: 研究表明，针对开源模型的对抗攻击可以转移到专有的视觉大语言模型（VLLMs）上，导致模型对视觉信息的错误解读，需要加强安全性。


<details>
  <summary>Details</summary>
Motivation: 探索视觉大语言模型（VLLMs）在对抗攻击下的脆弱性，填补现有研究的空白。

Method: 通过生成目标对抗样本和通用扰动，测试其在专有VLLMs（如GPT-4o、Claude和Gemini）上的可转移性和效果。

Result: 实验表明，对抗攻击能有效诱导模型错误解读视觉信息，且通用扰动在多模型上表现一致。

Conclusion: 当前VLLMs普遍存在对抗攻击漏洞，亟需开发鲁棒的防御措施以确保安全部署。

Abstract: Vision Large Language Models (VLLMs) are increasingly deployed to offer
advanced capabilities on inputs comprising both text and images. While prior
research has shown that adversarial attacks can transfer from open-source to
proprietary black-box models in text-only and vision-only contexts, the extent
and effectiveness of such vulnerabilities remain underexplored for VLLMs. We
present a comprehensive analysis demonstrating that targeted adversarial
examples are highly transferable to widely-used proprietary VLLMs such as
GPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to
induce specific attacker-chosen interpretations of visual information, such as
misinterpreting hazardous content as safe, overlooking sensitive or restricted
material, or generating detailed incorrect responses aligned with the
attacker's intent. Furthermore, we discover that universal perturbations --
modifications applicable to a wide set of images -- can consistently induce
these misinterpretations across multiple proprietary VLLMs. Our experimental
results on object recognition, visual question answering, and image captioning
show that this vulnerability is common across current state-of-the-art models,
and underscore an urgent need for robust mitigations to ensure the safe and
secure deployment of VLLMs.

</details>


### [56] [GeloVec: Higher Dimensional Geometric Smoothing for Coherent Visual Feature Extraction in Image Segmentation](https://arxiv.org/abs/2505.01057)
*Boris Kriuk,Matey Yordanov*

Main category: cs.CV

TL;DR: GeloVec是一种基于CNN的注意力平滑框架，用于语义分割，通过高维几何平滑方法解决传统方法的边界不稳定和上下文不连续问题。


<details>
  <summary>Details</summary>
Motivation: 现有注意力分割方法在特征映射中存在边界不稳定和上下文不连续问题，GeloVec旨在通过几何平滑方法建立鲁棒的流形关系。

Method: 结合改进的Chebyshev距离度量和多空间变换，通过自适应采样权重系统计算n维特征空间中的几何距离，增强分割准确性。

Result: 在多个基准数据集上表现优异，mIoU提升2.1%-2.7%，同时保持计算效率。

Conclusion: GeloVec通过Riemannian几何理论保证分割稳定性，具有强泛化能力和高效计算性能。

Abstract: This paper introduces GeloVec, a new CNN-based attention smoothing framework
for semantic segmentation that addresses critical limitations in conventional
approaches. While existing attention-backed segmentation methods suffer from
boundary instability and contextual discontinuities during feature mapping, our
framework implements a higher-dimensional geometric smoothing method to
establish a robust manifold relationships between visually coherent regions.
GeloVec combines modified Chebyshev distance metrics with multispatial
transformations to enhance segmentation accuracy through stabilized feature
extraction. The core innovation lies in the adaptive sampling weights system
that calculates geometric distances in n-dimensional feature space, achieving
superior edge preservation while maintaining intra-class homogeneity. The
multispatial transformation matrix incorporates tensorial projections with
orthogonal basis vectors, creating more discriminative feature representations
without sacrificing computational efficiency. Experimental validation across
multiple benchmark datasets demonstrates significant improvements in
segmentation performance, with mean Intersection over Union (mIoU) gains of
2.1%, 2.7%, and 2.4% on Caltech Birds-200, LSDSC, and FSSD datasets
respectively compared to state-of-the-art methods. GeloVec's mathematical
foundation in Riemannian geometry provides theoretical guarantees on
segmentation stability. Importantly, our framework maintains computational
efficiency through parallelized implementation of geodesic transformations and
exhibits strong generalization capabilities across disciplines due to the
absence of information loss during transformations.

</details>


### [57] [Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs](https://arxiv.org/abs/2505.01064)
*Hari Chandana Kuchibhotla,Sai Srinivas Kancheti,Abbavaram Gowtham Reddy,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: 论文提出了一种名为NeaR的新方法，用于解决无标签数据下的细粒度视觉识别问题，通过利用多模态大语言模型生成标签并优化下游模型。


<details>
  <summary>Details</summary>
Motivation: 细粒度视觉识别（FGVR）在缺乏标注数据的情况下难以实现，尤其是在隐私敏感或标注成本高的领域。传统的FGVR方法依赖于预定义的标签空间，而VF-FGVR任务需要在无约束的输出空间中进行预测。

Method: 提出NeaR方法，通过多模态大语言模型（MLLM）生成弱监督标签，并利用这些标签微调下游CLIP模型，以处理标签噪声和开放性问题。

Result: NeaR方法在无标签数据下实现了高效的细粒度视觉识别，为VF-FGVR任务设立了新的基准。

Conclusion: NeaR通过结合MLLM和下游模型优化，解决了无标签数据下的细粒度视觉识别问题，具有实际应用潜力。

Abstract: Fine-grained Visual Recognition (FGVR) involves distinguishing between
visually similar categories, which is inherently challenging due to subtle
inter-class differences and the need for large, expert-annotated datasets. In
domains like medical imaging, such curated datasets are unavailable due to
issues like privacy concerns and high annotation costs. In such scenarios
lacking labeled data, an FGVR model cannot rely on a predefined set of training
labels, and hence has an unconstrained output space for predictions. We refer
to this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict
labels from an unconstrained output space without prior label information.
While recent Multimodal Large Language Models (MLLMs) show potential for
VF-FGVR, querying these models for each test input is impractical because of
high costs and prohibitive inference times. To address these limitations, we
introduce \textbf{Nea}rest-Neighbor Label \textbf{R}efinement (NeaR), a novel
approach that fine-tunes a downstream CLIP model using labels generated by an
MLLM. Our approach constructs a weakly supervised dataset from a small,
unlabeled training set, leveraging MLLMs for label generation. NeaR is designed
to handle the noise, stochasticity, and open-endedness inherent in labels
generated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.

</details>


### [58] [Improving Editability in Image Generation with Layer-wise Memory](https://arxiv.org/abs/2505.01079)
*Daneul Kim,Jaeah Lee,Jaesik Park*

Main category: cs.CV

TL;DR: 论文提出了一种支持多步图像编辑的框架，通过层记忆和一致性引导技术，解决了现有方法在连续编辑中难以保持上下文一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法主要针对单对象修改，难以处理多步编辑任务，尤其是在保持先前编辑内容的同时自然融入新对象。

Method: 提出基于层记忆的框架，存储先前编辑的潜在表示和提示嵌入；引入背景一致性引导和多查询解耦技术，确保场景连贯性和自然适应。

Result: 通过新基准数据集和实验验证，该方法在多步编辑任务中表现优异，仅需粗略掩码即可保持高质量结果。

Conclusion: 该框架显著提升了多步图像编辑的效率和一致性，为复杂编辑场景提供了实用解决方案。

Abstract: Most real-world image editing tasks require multiple sequential edits to
achieve desired results. Current editing approaches, primarily designed for
single-object modifications, struggle with sequential editing: especially with
maintaining previous edits along with adapting new objects naturally into the
existing content. These limitations significantly hinder complex editing
scenarios where multiple objects need to be modified while preserving their
contextual relationships. We address this fundamental challenge through two key
proposals: enabling rough mask inputs that preserve existing content while
naturally integrating new elements and supporting consistent editing across
multiple modifications. Our framework achieves this through layer-wise memory,
which stores latent representations and prompt embeddings from previous edits.
We propose Background Consistency Guidance that leverages memorized latents to
maintain scene coherence and Multi-Query Disentanglement in cross-attention
that ensures natural adaptation to existing content. To evaluate our method, we
present a new benchmark dataset incorporating semantic alignment metrics and
interactive editing scenarios. Through comprehensive experiments, we
demonstrate superior performance in iterative image editing tasks with minimal
user effort, requiring only rough masks while maintaining high-quality results
throughout multiple editing steps.

</details>


### [59] [Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation](https://arxiv.org/abs/2505.01091)
*Daniele Molino,Francesco di Feola,Linlin Shen,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 提出了一种针对多模态医学数据生成的框架，能够生成多视角胸部X光及其临床报告，填补了通用视觉语言模型与医疗领域需求之间的差距。


<details>
  <summary>Details</summary>
Motivation: 生成模型在AI中表现突出，但医学领域的复杂性和临床准确性需求使其应用面临挑战。

Method: 利用MIMIC-CXR数据集，设计了一个框架，用于生成高质量图像和语义连贯的报告。

Result: 生成的图像和报告在FID和BLEU评分上表现优异，甚至在下游疾病分类任务中优于真实数据。

Conclusion: 该框架展示了生成模型在医学领域的潜力，为未来合成多模态医学数据的研究奠定了基础。

Abstract: Generative models have revolutionized Artificial Intelligence (AI),
particularly in multimodal applications. However, adapting these models to the
medical domain poses unique challenges due to the complexity of medical data
and the stringent need for clinical accuracy. In this work, we introduce a
framework specifically designed for multimodal medical data generation. By
enabling the generation of multi-view chest X-rays and their associated
clinical report, it bridges the gap between general-purpose vision-language
models and the specialized requirements of healthcare. Leveraging the MIMIC-CXR
dataset, the proposed framework shows superior performance in generating
high-fidelity images and semantically coherent reports. Our quantitative
evaluation reveals significant results in terms of FID and BLEU scores,
showcasing the quality of the generated data. Notably, our framework achieves
comparable or even superior performance compared to real data on downstream
disease classification tasks, underlining its potential as a tool for medical
research and diagnostics. This study highlights the importance of
domain-specific adaptations in enhancing the relevance and utility of
generative models for clinical applications, paving the way for future
advancements in synthetic multimodal medical data generation.

</details>


### [60] [VSC: Visual Search Compositional Text-to-Image Diffusion Model](https://arxiv.org/abs/2505.01104)
*Do Huu Dat,Nam Hyeonu,Po-Yuan Mao,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 本文提出了一种新的组合生成方法，通过成对图像嵌入改善属性-对象绑定，解决了文本到图像扩散模型中多属性-对象对绑定的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有文本编码器（如CLIP）难以有效编码复杂语言关系和修饰词，导致多属性-对象对绑定不准确。

Method: 分解复杂提示为子提示，生成对应图像并计算视觉原型，结合文本嵌入增强表示；通过基于分割的定位训练解决交叉注意力错位。

Result: 在T2I CompBench上优于现有组合模型，图像质量更高，且在提示中绑定对数增加时表现更稳健。

Conclusion: 该方法显著提升了多属性-对象对绑定的准确性，为复杂提示生成高质量图像提供了有效解决方案。

Abstract: Text-to-image diffusion models have shown impressive capabilities in
generating realistic visuals from natural-language prompts, yet they often
struggle with accurately binding attributes to corresponding objects,
especially in prompts containing multiple attribute-object pairs. This
challenge primarily arises from the limitations of commonly used text encoders,
such as CLIP, which can fail to encode complex linguistic relationships and
modifiers effectively. Existing approaches have attempted to mitigate these
issues through attention map control during inference and the use of layout
information or fine-tuning during training, yet they face performance drops
with increased prompt complexity. In this work, we introduce a novel
compositional generation method that leverages pairwise image embeddings to
improve attribute-object binding. Our approach decomposes complex prompts into
sub-prompts, generates corresponding images, and computes visual prototypes
that fuse with text embeddings to enhance representation. By applying
segmentation-based localization training, we address cross-attention
misalignment, achieving improved accuracy in binding multiple attributes to
objects. Our approaches outperform existing compositional text-to-image
diffusion models on the benchmark T2I CompBench, achieving better image
quality, evaluated by humans, and emerging robustness under scaling number of
binding pairs in the prompt.

</details>


### [61] [Self-Supervision Enhances Instance-based Multiple Instance Learning Methods in Digital Pathology: A Benchmark Study](https://arxiv.org/abs/2505.01109)
*Ali Mammadov,Loic Le Folgoc,Julien Adam,Anne Buronfosse,Gilles Hayem,Guillaume Hocquet,Pietro Gori*

Main category: cs.CV

TL;DR: 研究表明，使用自监督学习（SSL）提取高质量特征后，简单的基于实例的多实例学习（MIL）方法在性能上优于复杂的基于嵌入的MIL方法，且更具可解释性。


<details>
  <summary>Details</summary>
Motivation: 探讨在特征提取质量提升后，基于实例的MIL方法是否仍优于基于嵌入的方法，并验证其在病理学领域的适用性。

Method: 通过710次实验，比较10种MIL策略、6种SSL方法、4种基础模型及多种病理学适应技术，并引入4种新的基于实例的MIL方法。

Result: 实验显示，简单的基于实例的MIL方法在BRACS和Camelyon16数据集上达到或超越现有最佳性能。

Conclusion: 建议将更多精力投入适应病理学的SSL方法，而非复杂的基于嵌入的MIL方法，以提升可解释性和性能。

Abstract: Multiple Instance Learning (MIL) has emerged as the best solution for Whole
Slide Image (WSI) classification. It consists of dividing each slide into
patches, which are treated as a bag of instances labeled with a global label.
MIL includes two main approaches: instance-based and embedding-based. In the
former, each patch is classified independently, and then the patch scores are
aggregated to predict the bag label. In the latter, bag classification is
performed after aggregating patch embeddings. Even if instance-based methods
are naturally more interpretable, embedding-based MILs have usually been
preferred in the past due to their robustness to poor feature extractors.
However, recently, the quality of feature embeddings has drastically increased
using self-supervised learning (SSL). Nevertheless, many authors continue to
endorse the superiority of embedding-based MIL. To investigate this further, we
conduct 710 experiments across 4 datasets, comparing 10 MIL strategies, 6
self-supervised methods with 4 backbones, 4 foundation models, and various
pathology-adapted techniques. Furthermore, we introduce 4 instance-based MIL
methods never used before in the pathology domain. Through these extensive
experiments, we show that with a good SSL feature extractor, simple
instance-based MILs, with very few parameters, obtain similar or better
performance than complex, state-of-the-art (SOTA) embedding-based MIL methods,
setting new SOTA results on the BRACS and Camelyon16 datasets. Since simple
instance-based MIL methods are naturally more interpretable and explainable to
clinicians, our results suggest that more effort should be put into
well-adapted SSL methods for WSI rather than into complex embedding-based MIL
methods.

</details>


### [62] [FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis](https://arxiv.org/abs/2505.01172)
*Jiangtong Tan,Hu Yu,Jie Huang,Jie Xiao,Feng Zhao*

Main category: cs.CV

TL;DR: FreePCA是一种基于PCA的无训练长视频生成方法，通过解耦全局一致性和局部质量，显著提升视频生成效果。


<details>
  <summary>Details</summary>
Motivation: 长视频生成因帧数变化导致分布偏移，现有方法难以兼顾全局一致性和局部质量。

Method: 利用PCA将全局和局部信息解耦为一致外观和运动强度特征，并通过余弦相似度测量和渐进集成实现高质量生成。

Result: 实验表明，FreePCA无需训练即可应用于多种视频扩散模型，显著提升一致性和质量。

Conclusion: FreePCA为长视频生成提供了一种高效且无需训练的解决方案。

Abstract: Long video generation involves generating extended videos using models
trained on short videos, suffering from distribution shifts due to varying
frame counts. It necessitates the use of local information from the original
short frames to enhance visual and motion quality, and global information from
the entire long frames to ensure appearance consistency. Existing training-free
methods struggle to effectively integrate the benefits of both, as appearance
and motion in videos are closely coupled, leading to motion inconsistency and
visual quality. In this paper, we reveal that global and local information can
be precisely decoupled into consistent appearance and motion intensity
information by applying Principal Component Analysis (PCA), allowing for
refined complementary integration of global consistency and local quality. With
this insight, we propose FreePCA, a training-free long video generation
paradigm based on PCA that simultaneously achieves high consistency and
quality. Concretely, we decouple consistent appearance and motion intensity
features by measuring cosine similarity in the principal component space.
Critically, we progressively integrate these features to preserve original
quality and ensure smooth transitions, while further enhancing consistency by
reusing the mean statistics of the initial noise. Experiments demonstrate that
FreePCA can be applied to various video diffusion models without requiring
training, leading to substantial improvements. Code is available at
https://github.com/JosephTiTan/FreePCA.

</details>


### [63] [TSTMotion: Training-free Scene-awarenText-to-motion Generation](https://arxiv.org/abs/2505.01182)
*Ziyan Guo,Haoxuan Qu,Hossein Rahmani,Dewen Soh,Ping Hu,Qiuhong Ke,Jun Liu*

Main category: cs.CV

TL;DR: 提出了一种无需训练的场景感知文本到动作生成框架TSTMotion，利用预训练的空白背景动作生成器和基础模型，实现高效场景感知动作生成。


<details>
  <summary>Details</summary>
Motivation: 现有场景感知方法依赖大规模真实动作序列，成本高昂，因此提出无需训练的解决方案。

Method: 结合基础模型预测场景感知动作指导，并通过修改预训练生成器实现场景感知动作生成。

Result: 实验证明框架高效且通用，代码已开源。

Conclusion: TSTMotion为场景感知文本到动作生成提供了一种低成本、高效的解决方案。

Abstract: Text-to-motion generation has recently garnered significant research
interest, primarily focusing on generating human motion sequences in blank
backgrounds. However, human motions commonly occur within diverse 3D scenes,
which has prompted exploration into scene-aware text-to-motion generation
methods. Yet, existing scene-aware methods often rely on large-scale
ground-truth motion sequences in diverse 3D scenes, which poses practical
challenges due to the expensive cost. To mitigate this challenge, we are the
first to propose a \textbf{T}raining-free \textbf{S}cene-aware
\textbf{T}ext-to-\textbf{Motion} framework, dubbed as \textbf{TSTMotion}, that
efficiently empowers pre-trained blank-background motion generators with the
scene-aware capability. Specifically, conditioned on the given 3D scene and
text description, we adopt foundation models together to reason, predict and
validate a scene-aware motion guidance. Then, the motion guidance is
incorporated into the blank-background motion generators with two
modifications, resulting in scene-aware text-driven motion sequences. Extensive
experiments demonstrate the efficacy and generalizability of our proposed
framework. We release our code in \href{https://tstmotion.github.io/}{Project
Page}.

</details>


### [64] [Efficient Vision-based Vehicle Speed Estimation](https://arxiv.org/abs/2505.01203)
*Andrej Macko,Lukáš Gajdošech,Viktor Kocur*

Main category: cs.CV

TL;DR: 提出了一种高效计算车辆速度估计方法，通过改进3D边界框和消失点几何技术，显著提升实时性能。


<details>
  <summary>Details</summary>
Motivation: 提升交通摄像头视频中车辆速度估计的计算效率和实时性能。

Method: 基于2D检测和消失点几何的3D边界框技术，引入改进以优化实时性能，并在不同硬件平台上评估。

Result: 在BrnoCompSpeed数据集上，速度估计误差中位数（0.58 km/h vs. 0.60 km/h）、检测精度（91.02% vs 87.08%）和召回率（91.14% vs. 83.32%）均优于现有方法，且速度快5.5倍。

Conclusion: 通过量化后训练的小模型，在计算成本和精度之间取得最佳平衡，适合实际部署。

Abstract: This paper presents a computationally efficient method for vehicle speed
estimation from traffic camera footage. Building upon previous work that
utilizes 3D bounding boxes derived from 2D detections and vanishing point
geometry, we introduce several improvements to enhance real-time performance.
We evaluate our method in several variants on the BrnoCompSpeed dataset in
terms of vehicle detection and speed estimation accuracy. Our extensive
evaluation across various hardware platforms, including edge devices,
demonstrates significant gains in frames per second (FPS) compared to the prior
state-of-the-art, while maintaining comparable or improved speed estimation
accuracy. We analyze the trade-off between accuracy and computational cost,
showing that smaller models utilizing post-training quantization offer the best
balance for real-world deployment. Our best performing model beats previous
state-of-the-art in terms of median vehicle speed estimation error (0.58 km/h
vs. 0.60 km/h), detection precision (91.02% vs 87.08%) and recall (91.14% vs.
83.32%) while also being 5.5 times faster.

</details>


### [65] [T-Graph: Enhancing Sparse-view Camera Pose Estimation by Pairwise Translation Graph](https://arxiv.org/abs/2505.01207)
*Qingyu Xian,Weiqin Jiao,Hao Cheng,Berend Jan van der Zwaag,Yanqiu Huang*

Main category: cs.CV

TL;DR: 论文提出了一种名为T-Graph的轻量级模块，用于提升稀疏视角下的相机姿态估计性能，通过构建翻译图和多层感知机处理成对图像特征，显著提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角相机姿态估计在遥感应用中至关重要，但现有方法常忽略视角间的平移信息，导致性能不佳。

Method: T-Graph通过MLP处理成对图像特征，构建全连接的翻译图，并引入两种平移表示（relative-t和pair-t）以增强适应性。

Result: 在RelPose++和Forge方法上的实验表明，T-Graph显著提升了相机中心精度（2到8视角下提升1%至6%）。

Conclusion: T-Graph是一个高效且通用的模块，能无缝集成到现有模型中，显著提升稀疏视角相机姿态估计的性能。

Abstract: Sparse-view camera pose estimation, which aims to estimate the
6-Degree-of-Freedom (6-DoF) poses from a limited number of images captured from
different viewpoints, is a fundamental yet challenging problem in remote
sensing applications. Existing methods often overlook the translation
information between each pair of viewpoints, leading to suboptimal performance
in sparse-view scenarios. To address this limitation, we introduce T-Graph, a
lightweight, plug-and-play module to enhance camera pose estimation in
sparse-view settings. T-graph takes paired image features as input and maps
them through a Multilayer Perceptron (MLP). It then constructs a fully
connected translation graph, where nodes represent cameras and edges encode
their translation relationships. It can be seamlessly integrated into existing
models as an additional branch in parallel with the original prediction,
maintaining efficiency and ease of use. Furthermore, we introduce two pairwise
translation representations, relative-t and pair-t, formulated under different
local coordinate systems. While relative-t captures intuitive spatial
relationships, pair-t offers a rotation-disentangled alternative. The two
representations contribute to enhanced adaptability across diverse application
scenarios, further improving our module's robustness. Extensive experiments on
two state-of-the-art methods (RelPose++ and Forge) using public datasets (C03D
and IMC PhotoTourism) validate both the effectiveness and generalizability of
T-Graph. The results demonstrate consistent improvements across various
metrics, notably camera center accuracy, which improves by 1% to 6% from 2 to 8
viewpoints.

</details>


### [66] [High Dynamic Range Novel View Synthesis with Single Exposure](https://arxiv.org/abs/2505.01212)
*Kaixuan Zhang,Hu Wang,Minxian Li,Mingwu Ren,Mao Ye,Xiatian Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种单曝光HDR-NVS方法Mono-HDR-3D，解决了多曝光HDR-NVS的局限性，如运动伪影和高成本。


<details>
  <summary>Details</summary>
Motivation: 多曝光HDR-NVS存在运动伪影和高成本问题，需一种仅依赖单曝光LDR图像的方法。

Method: 提出Mono-HDR-3D，包含两个模块：LDR转HDR和HDR转LDR，实现闭环无监督学习。

Result: 实验表明Mono-HDR-3D显著优于现有方法。

Conclusion: Mono-HDR-3D为单曝光HDR-NVS提供了高效解决方案，可无缝集成现有NVS模型。

Abstract: High Dynamic Range Novel View Synthesis (HDR-NVS) aims to establish a 3D
scene HDR model from Low Dynamic Range (LDR) imagery. Typically,
multiple-exposure LDR images are employed to capture a wider range of
brightness levels in a scene, as a single LDR image cannot represent both the
brightest and darkest regions simultaneously. While effective, this
multiple-exposure HDR-NVS approach has significant limitations, including
susceptibility to motion artifacts (e.g., ghosting and blurring), high capture
and storage costs. To overcome these challenges, we introduce, for the first
time, the single-exposure HDR-NVS problem, where only single exposure LDR
images are available during training. We further introduce a novel approach,
Mono-HDR-3D, featuring two dedicated modules formulated by the LDR image
formation principles, one for converting LDR colors to HDR counterparts, and
the other for transforming HDR images to LDR format so that unsupervised
learning is enabled in a closed loop. Designed as a meta-algorithm, our
approach can be seamlessly integrated with existing NVS models. Extensive
experiments show that Mono-HDR-3D significantly outperforms previous methods.
Source code will be released.

</details>


### [67] [RD-UIE: Relation-Driven State Space Modeling for Underwater Image Enhancement](https://arxiv.org/abs/2505.01224)
*Kui Jiang,Yan Luo,Junjun Jiang,Xin Xu,Fei Ma,Fei Yu*

Main category: cs.CV

TL;DR: 论文提出了一种基于关系驱动的Mamba框架（RD-UIE），通过动态排序扫描机制和视觉自适应状态块（VSSB）改进水下图像增强，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 水下图像因波长衰减导致内容退化和颜色失真，现有Mamba模型因固定扫描路径无法适应复杂水下环境，需改进。

Method: 引入动态排序扫描机制和VSSB，结合跨特征桥（CFB）融合多尺度特征，构建RD-UIE框架。

Result: 在多个基准测试中，RD-UIE优于现有方法WMamba，平均性能提升0.55 dB。

Conclusion: RD-UIE通过动态扫描和自适应特征融合，有效提升了水下图像增强的性能和视觉保真度。

Abstract: Underwater image enhancement (UIE) is a critical preprocessing step for
marine vision applications, where wavelength-dependent attenuation causes
severe content degradation and color distortion. While recent state space
models like Mamba show potential for long-range dependency modeling, their
unfolding operations and fixed scan paths on 1D sequences fail to adapt to
local object semantics and global relation modeling, limiting their efficacy in
complex underwater environments. To address this, we enhance conventional Mamba
with the sorting-based scanning mechanism that dynamically reorders scanning
sequences based on statistical distribution of spatial correlation of all
pixels. In this way, it encourages the network to prioritize the most
informative components--structural and semantic features. Upon building this
mechanism, we devise a Visually Self-adaptive State Block (VSSB) that
harmonizes dynamic sorting of Mamba with input-dependent dynamic convolution,
enabling coherent integration of global context and local relational cues. This
exquisite design helps eliminate global focus bias, especially for widely
distributed contents, which greatly weakens the statistical frequency. For
robust feature extraction and refinement, we design a cross-feature bridge
(CFB) to adaptively fuse multi-scale representations. These efforts compose the
novel relation-driven Mamba framework for effective UIE (RD-UIE). Extensive
experiments on underwater enhancement benchmarks demonstrate RD-UIE outperforms
the state-of-the-art approach WMamba in both quantitative metrics and visual
fidelity, averagely achieving 0.55 dB performance gain on the three benchmarks.
Our code is available at https://github.com/kkoucy/RD-UIE/tree/main

</details>


### [68] [Core-Set Selection for Data-efficient Land Cover Segmentation](https://arxiv.org/abs/2505.01225)
*Keiller Nogueira,Akram Zaytar,Wanli Ma,Ribana Roscher,Ronny Hänsch,Caleb Robinson,Anthony Ortiz,Simone Nsutezo,Rahul Dodhia,Juan M. Lavista Ferres,Oktay Karakuş,Paul L. Rosin*

Main category: cs.CV

TL;DR: 论文提出六种核心集选择方法，用于从遥感图像分割数据集中选择重要子集，并在三个常用土地分类数据集上验证其优于随机选择基线。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型依赖大数据集训练，但大数据的复杂性、偏差和噪声问题常被忽视，需同时关注数据数量和质量。

Method: 提出六种核心集选择方法，基于图像、标签或其组合选择样本子集，并在DFC2022、Vaihingen和Potsdam数据集上测试。

Result: 实验表明，训练子集优于随机基线，部分方法甚至优于全数据训练，验证了数据为中心学习的重要性。

Conclusion: 数据为中心的学习在遥感领域具有潜力，代码已开源。

Abstract: The increasing accessibility of remotely sensed data and the potential of
such data to inform large-scale decision-making has driven the development of
deep learning models for many Earth Observation tasks. Traditionally, such
models must be trained on large datasets. However, the common assumption that
broadly larger datasets lead to better outcomes tends to overlook the
complexities of the data distribution, the potential for introducing biases and
noise, and the computational resources required for processing and storing vast
datasets. Therefore, effective solutions should consider both the quantity and
quality of data. In this paper, we propose six novel core-set selection methods
for selecting important subsets of samples from remote sensing image
segmentation datasets that rely on imagery only, labels only, and a combination
of each. We benchmark these approaches against a random-selection baseline on
three commonly used land cover classification datasets: DFC2022, Vaihingen, and
Potsdam. In each of the datasets, we demonstrate that training on a subset of
samples outperforms the random baseline, and some approaches outperform
training on all available data. This result shows the importance and potential
of data-centric learning for the remote sensing domain. The code is available
at https://github.com/keillernogueira/data-centric-rs-classification/.

</details>


### [69] [Compensating Spatiotemporally Inconsistent Observations for Online Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2505.01235)
*Youngsik Yun,Jeongmin Bae,Hyunseung Son,Seoha Kim,Hahyun Lee,Gun Bang,Youngjung Uh*

Main category: cs.CV

TL;DR: 本文提出了一种在线动态场景重建方法，通过消除学习到的误差来增强时间一致性，显著提升了渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有在线重建方法注重效率和渲染质量，但忽略了时间一致性，导致静态区域出现明显伪影。本文旨在解决这一问题。

Method: 通过从观测中减去学习到的误差，恢复理想观测，从而增强时间一致性。

Result: 实验表明，该方法显著提升了时间一致性和渲染质量。

Conclusion: 该方法有效解决了在线动态场景重建中的时间一致性问题，提升了整体性能。

Abstract: Online reconstruction of dynamic scenes is significant as it enables learning
scenes from live-streaming video inputs, while existing offline dynamic
reconstruction methods rely on recorded video inputs. However, previous online
reconstruction approaches have primarily focused on efficiency and rendering
quality, overlooking the temporal consistency of their results, which often
contain noticeable artifacts in static regions. This paper identifies that
errors such as noise in real-world recordings affect temporal inconsistency in
online reconstruction. We propose a method that enhances temporal consistency
in online reconstruction from observations with temporal inconsistency which is
inevitable in cameras. We show that our method restores the ideal observation
by subtracting the learned error. We demonstrate that applying our method to
various baselines significantly enhances both temporal consistency and
rendering quality across datasets. Code, video results, and checkpoints are
available at https://bbangsik13.github.io/OR2.

</details>


### [70] [Fusing Foveal Fixations Using Linear Retinal Transformations and Bayesian Experimental Design](https://arxiv.org/abs/2505.01249)
*Christopher K. I. Williams*

Main category: cs.CV

TL;DR: 论文提出了一种基于线性下采样的视网膜变换方法，用于融合多个注视点的场景表示，并通过贝叶斯实验设计优化注视点选择。


<details>
  <summary>Details</summary>
Motivation: 解决人类和脊椎动物如何通过多个注视点融合高分辨率场景表示的问题。

Method: 利用已知几何关系，将视网膜变换表示为高分辨率潜在图像的线性下采样，并在因子分析（FA）和FA混合模型中进行精确推断。

Result: 在Frey人脸和MNIST数据集上的实验验证了模型的有效性。

Conclusion: 提出的方法能够有效融合多个注视点的信息，并通过贝叶斯实验设计优化注视点选择。

Abstract: Humans (and many vertebrates) face the problem of fusing together multiple
fixations of a scene in order to obtain a representation of the whole, where
each fixation uses a high-resolution fovea and decreasing resolution in the
periphery. In this paper we explicitly represent the retinal transformation of
a fixation as a linear downsampling of a high-resolution latent image of the
scene, exploiting the known geometry. This linear transformation allows us to
carry out exact inference for the latent variables in factor analysis (FA) and
mixtures of FA models of the scene. Further, this allows us to formulate and
solve the choice of "where to look next" as a Bayesian experimental design
problem using the Expected Information Gain criterion. Experiments on the Frey
faces and MNIST datasets demonstrate the effectiveness of our models.

</details>


### [71] [CAMELTrack: Context-Aware Multi-cue ExpLoitation for Online Multi-Object Tracking](https://arxiv.org/abs/2505.01257)
*Vladimir Somers,Baptiste Standaert,Victor Joos,Alexandre Alahi,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: CAMEL是一种新型的关联模块，通过学习数据驱动的关联策略，摆脱了传统手工启发式方法的限制，同时保持了模块化设计的优势。


<details>
  <summary>Details</summary>
Motivation: 现有的跟踪方法依赖手工规则进行目标关联，难以捕捉复杂的跟踪线索交互。

Method: CAMEL采用两个基于Transformer的模块和一种新的关联中心训练方案，建模目标与关联线索的复杂交互。

Result: CAMELTrack在多个跟踪基准上实现了最先进的性能。

Conclusion: CAMEL提供了一种轻量且高效的数据驱动关联方法，优于传统启发式方法。

Abstract: Online multi-object tracking has been recently dominated by
tracking-by-detection (TbD) methods, where recent advances rely on increasingly
sophisticated heuristics for tracklet representation, feature fusion, and
multi-stage matching. The key strength of TbD lies in its modular design,
enabling the integration of specialized off-the-shelf models like motion
predictors and re-identification. However, the extensive usage of human-crafted
rules for temporal associations makes these methods inherently limited in their
ability to capture the complex interplay between various tracking cues. In this
work, we introduce CAMEL, a novel association module for Context-Aware
Multi-Cue ExpLoitation, that learns resilient association strategies directly
from data, breaking free from hand-crafted heuristics while maintaining TbD's
valuable modularity. At its core, CAMEL employs two transformer-based modules
and relies on a novel association-centric training scheme to effectively model
the complex interactions between tracked targets and their various association
cues. Unlike end-to-end detection-by-tracking approaches, our method remains
lightweight and fast to train while being able to leverage external
off-the-shelf models. Our proposed online tracking pipeline, CAMELTrack,
achieves state-of-the-art performance on multiple tracking benchmarks. Our code
is available at https://github.com/TrackingLaboratory/CAMELTrack.

</details>


### [72] [Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain](https://arxiv.org/abs/2505.01267)
*Gaozheng Pei,Ke Ma,Yingfei Sun,Qianqian Xu,Qingming Huang*

Main category: cs.CV

TL;DR: 论文提出了一种基于频率域的对抗净化方法，通过替换低频振幅谱和投影相位谱，有效消除对抗扰动并保留图像内容。


<details>
  <summary>Details</summary>
Motivation: 现有对抗净化方法因缺乏对抗扰动的分布信息，常破坏正常语义，因此转向频率域视角以更精准处理。

Method: 在反向过程中替换低频振幅谱，并将相位谱投影到指定范围，专注于低频部分以减少对图像的损害。

Result: 实验表明，该方法显著优于当前大多数防御方法。

Conclusion: 频率域视角的对抗净化方法能更有效地消除扰动并保留图像内容。

Abstract: The diffusion-based adversarial purification methods attempt to drown
adversarial perturbations into a part of isotropic noise through the forward
process, and then recover the clean images through the reverse process. Due to
the lack of distribution information about adversarial perturbations in the
pixel domain, it is often unavoidable to damage normal semantics. We turn to
the frequency domain perspective, decomposing the image into amplitude spectrum
and phase spectrum. We find that for both spectra, the damage caused by
adversarial perturbations tends to increase monotonically with frequency. This
means that we can extract the content and structural information of the
original clean sample from the frequency components that are less damaged.
Meanwhile, theoretical analysis indicates that existing purification methods
indiscriminately damage all frequency components, leading to excessive damage
to the image. Therefore, we propose a purification method that can eliminate
adversarial perturbations while maximizing the preservation of the content and
structure of the original image. Specifically, at each time step during the
reverse process, for the amplitude spectrum, we replace the low-frequency
components of the estimated image's amplitude spectrum with the corresponding
parts of the adversarial image. For the phase spectrum, we project the phase of
the estimated image into a designated range of the adversarial image's phase
spectrum, focusing on the low frequencies. Empirical evidence from extensive
experiments demonstrates that our method significantly outperforms most current
defense methods.

</details>


### [73] [FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian Scene without Spatial Priors](https://arxiv.org/abs/2505.01322)
*Chenxi Li,Weijie Wang,Qiang Li,Bruno Lepri,Nicu Sebe,Weizhi Nie*

Main category: cs.CV

TL;DR: FreeInsert是一个无需空间先验的3D场景文本驱动对象插入框架，利用基础模型实现灵活编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖空间先验（如2D掩码或3D边界框），限制了灵活性和一致性。

Method: 结合MLLM、LGM和扩散模型，通过语义解析和分层细化实现对象生成与空间放置的解耦。

Result: 实验表明，FreeInsert实现了语义一致、空间精确且视觉逼真的3D插入。

Conclusion: FreeInsert提供了一种无需先验、用户友好的灵活编辑方案。

Abstract: Text-driven object insertion in 3D scenes is an emerging task that enables
intuitive scene editing through natural language. However, existing 2D
editing-based methods often rely on spatial priors such as 2D masks or 3D
bounding boxes, and they struggle to ensure consistency of the inserted object.
These limitations hinder flexibility and scalability in real-world
applications. In this paper, we propose FreeInsert, a novel framework that
leverages foundation models including MLLMs, LGMs, and diffusion models to
disentangle object generation from spatial placement. This enables unsupervised
and flexible object insertion in 3D scenes without spatial priors. FreeInsert
starts with an MLLM-based parser that extracts structured semantics, including
object types, spatial relationships, and attachment regions, from user
instructions. These semantics guide both the reconstruction of the inserted
object for 3D consistency and the learning of its degrees of freedom. We
leverage the spatial reasoning capabilities of MLLMs to initialize object pose
and scale. A hierarchical, spatially aware refinement stage further integrates
spatial semantics and MLLM-inferred priors to enhance placement. Finally, the
appearance of the object is improved using the inserted-object image to enhance
visual fidelity. Experimental results demonstrate that FreeInsert achieves
semantically coherent, spatially precise, and visually realistic 3D insertions
without relying on spatial priors, offering a user-friendly and flexible
editing experience.

</details>


### [74] [Monitoring morphometric drift in lifelong learning segmentation of the spinal cord](https://arxiv.org/abs/2505.01364)
*Enamundram Naga Karthik,Sandrine Bédard,Jan Valošek,Christoph S. Aigner,Elise Bannier,Josef Bednařík,Virginie Callot,Anna Combes,Armin Curt,Gergely David,Falk Eippert,Lynn Farner,Michael G Fehlings,Patrick Freund,Tobias Granberg,Cristina Granziera,RHSCIR Network Imaging Group,Ulrike Horn,Tomáš Horák,Suzanne Humphreys,Markus Hupp,Anne Kerbrat,Nawal Kinany,Shannon Kolind,Petr Kudlička,Anna Lebret,Lisa Eunyoung Lee,Caterina Mainero,Allan R. Martin,Megan McGrath,Govind Nair,Kristin P. O'Grady,Jiwon Oh,Russell Ouellette,Nikolai Pfender,Dario Pfyffer,Pierre-François Pradat,Alexandre Prat,Emanuele Pravatà,Daniel S. Reich,Ilaria Ricchi,Naama Rotem-Kohavi,Simon Schading-Sassenhausen,Maryam Seif,Andrew Smith,Seth A Smith,Grace Sweeney,Roger Tam,Anthony Traboulsee,Constantina Andrada Treaba,Charidimos Tsagkas,Zachary Vavasour,Dimitri Van De Ville,Kenneth Arnold Weber II,Sarath Chandar,Julien Cohen-Adad*

Main category: cs.CV

TL;DR: 该研究提出了一种脊髓分割模型和终身学习框架，用于监测模型更新时的形态学漂移，并更新健康参与者的规范数据库。


<details>
  <summary>Details</summary>
Motivation: 评估自动分割模型在更新时的稳定性，特别是在从健康参与者中获取规范值时。

Method: 训练多站点数据集上的脊髓分割模型，并引入终身学习框架和自动GitHub Actions工作流监测形态学漂移。

Result: 模型在腰椎脊髓病例中表现优于先前版本，平均Dice得分为0.95±0.03；形态学漂移监测提供了快速反馈；规范数据库更新所需的缩放因子几乎恒定。

Conclusion: 该模型和框架为未来分割模型的开发和规范数据库的更新提供了有效工具。

Abstract: Morphometric measures derived from spinal cord segmentations can serve as
diagnostic and prognostic biomarkers in neurological diseases and injuries
affecting the spinal cord. While robust, automatic segmentation methods to a
wide variety of contrasts and pathologies have been developed over the past few
years, whether their predictions are stable as the model is updated using new
datasets has not been assessed. This is particularly important for deriving
normative values from healthy participants. In this study, we present a spinal
cord segmentation model trained on a multisite $(n=75)$ dataset, including 9
different MRI contrasts and several spinal cord pathologies. We also introduce
a lifelong learning framework to automatically monitor the morphometric drift
as the model is updated using additional datasets. The framework is triggered
by an automatic GitHub Actions workflow every time a new model is created,
recording the morphometric values derived from the model's predictions over
time. As a real-world application of the proposed framework, we employed the
spinal cord segmentation model to update a recently-introduced normative
database of healthy participants containing commonly used measures of spinal
cord morphometry. Results showed that: (i) our model outperforms previous
versions and pathology-specific models on challenging lumbar spinal cord cases,
achieving an average Dice score of $0.95 \pm 0.03$; (ii) the automatic workflow
for monitoring morphometric drift provides a quick feedback loop for developing
future segmentation models; and (iii) the scaling factor required to update the
database of morphometric measures is nearly constant among slices across the
given vertebral levels, showing minimum drift between the current and previous
versions of the model monitored by the framework. The model is freely available
in Spinal Cord Toolbox v7.0.

</details>


### [75] [Global Collinearity-aware Polygonizer for Polygonal Building Mapping in Remote Sensing](https://arxiv.org/abs/2505.01385)
*Fahong Zhang,Yilei Shi,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种名为GCP的新算法，用于从遥感图像中映射多边形建筑物。GCP通过优化轮廓拟合和简化多边形，显著提升了准确性。


<details>
  <summary>Details</summary>
Motivation: 解决从遥感图像中准确映射多边形建筑物的挑战。

Method: 基于实例分割框架，通过轮廓采样、Transformer回归模块和动态编程简化多边形。

Result: 在公共基准测试中验证了GCP的有效性，其简化模块优于传统方法。

Conclusion: GCP算法具有广泛适用性，代码将开源。

Abstract: This paper addresses the challenge of mapping polygonal buildings from remote
sensing images and introduces a novel algorithm, the Global Collinearity-aware
Polygonizer (GCP). GCP, built upon an instance segmentation framework,
processes binary masks produced by any instance segmentation model. The
algorithm begins by collecting polylines sampled along the contours of the
binary masks. These polylines undergo a refinement process using a
transformer-based regression module to ensure they accurately fit the contours
of the targeted building instances. Subsequently, a collinearity-aware polygon
simplification module simplifies these refined polylines and generate the final
polygon representation. This module employs dynamic programming technique to
optimize an objective function that balances the simplicity and fidelity of the
polygons, achieving globally optimal solutions. Furthermore, the optimized
collinearity-aware objective is seamlessly integrated into network training,
enhancing the cohesiveness of the entire pipeline. The effectiveness of GCP has
been validated on two public benchmarks for polygonal building mapping. Further
experiments reveal that applying the collinearity-aware polygon simplification
module to arbitrary polylines, without prior knowledge, enhances accuracy over
traditional methods such as the Douglas-Peucker algorithm. This finding
underscores the broad applicability of GCP. The code for the proposed method
will be made available at https://github.com/zhu-xlab.

</details>


### [76] [Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer](https://arxiv.org/abs/2505.01390)
*Alice Natalina Caragliano,Claudia Tacconi,Carlo Greco,Lorenzo Nibid,Edy Ippolito,Michele Fiore,Giuseppe Perrone,Sara Ramella,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 提出了一种结合多模态深度学习和可解释AI的新方法，用于预测非小细胞肺癌患者对新辅助治疗的反应，通过中间融合策略整合影像和临床数据，并引入医生参与循环机制提升临床相关性。


<details>
  <summary>Details</summary>
Motivation: 现有放射组学和单模态深度学习方法存在局限性，无法高效整合多模态数据，且缺乏临床可解释性。

Method: 采用中间融合策略整合影像和临床数据，结合医生参与循环机制，逐步引导模型从广泛肺区域聚焦到特定病灶。

Result: 结果显示预测准确性和可解释性均有提升，为临床数据整合提供了优化策略。

Conclusion: 该方法在多模态数据整合和临床可解释性方面具有优势，为肺癌治疗预测提供了新思路。

Abstract: This study proposes a novel approach combining Multimodal Deep Learning with
intrinsic eXplainable Artificial Intelligence techniques to predict
pathological response in non-small cell lung cancer patients undergoing
neoadjuvant therapy. Due to the limitations of existing radiomics and unimodal
deep learning approaches, we introduce an intermediate fusion strategy that
integrates imaging and clinical data, enabling efficient interaction between
data modalities. The proposed Multimodal Doctor-in-the-Loop method further
enhances clinical relevance by embedding clinicians' domain knowledge directly
into the training process, guiding the model's focus gradually from broader
lung regions to specific lesions. Results demonstrate improved predictive
accuracy and explainability, providing insights into optimal data integration
strategies for clinical applications.

</details>


### [77] [VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models](https://arxiv.org/abs/2505.01406)
*Mohammadreza Teymoorianfard,Shiqing Ma,Amir Houmansadr*

Main category: cs.CV

TL;DR: VIDSTAMP是一种水印框架，专为视频扩散模型设计，通过两阶段微调在潜在空间中嵌入高容量水印，保持视觉质量并抵御常见篡改。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法难以应对视频特定操作（如帧插入、删除或重排序），且通常影响视觉质量，需要一种更鲁棒且不影响感知质量的解决方案。

Method: 通过两阶段微调模型解码器：先在静态图像数据集上训练以分离空间信息，再在合成视频序列上恢复时间一致性，利用3D卷积和时间注意力机制。

Result: VIDSTAMP嵌入768比特/视频（48比特/帧），比特准确率95.0%，视频质量得分0.836（接近无水印的0.838），优于现有方法。

Conclusion: VIDSTAMP在保持高视觉质量的同时，提供了高容量和鲁棒性的水印嵌入，适用于视频扩散模型。

Abstract: The rapid rise of video diffusion models has enabled the generation of highly
realistic and temporally coherent videos, raising critical concerns about
content authenticity, provenance, and misuse. Existing watermarking approaches,
whether passive, post-hoc, or adapted from image-based techniques, often
struggle to withstand video-specific manipulations such as frame insertion,
dropping, or reordering, and typically degrade visual quality. In this work, we
introduce VIDSTAMP, a watermarking framework that embeds per-frame or
per-segment messages directly into the latent space of temporally-aware video
diffusion models. By fine-tuning the model's decoder through a two-stage
pipeline, first on static image datasets to promote spatial message separation,
and then on synthesized video sequences to restore temporal consistency,
VIDSTAMP learns to embed high-capacity, flexible watermarks with minimal
perceptual impact. Leveraging architectural components such as 3D convolutions
and temporal attention, our method imposes no additional inference cost and
offers better perceptual quality than prior methods, while maintaining
comparable robustness against common distortions and tampering. VIDSTAMP embeds
768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a
log P-value of -166.65 (lower is better), and maintains a video quality score
of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior
methods in capacity-quality tradeoffs. Code: Code:
\url{https://github.com/SPIN-UMass/VidStamp}

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [78] [ROSA: A Knowledge-based Solution for Robot Self-Adaptation](https://arxiv.org/abs/2505.00733)
*Gustavo Rezende Silva,Juliane Päßler,S. Lizeth Tapia Tarifa,Einar Broch Johnsen,Carlos Hernández Corbato*

Main category: cs.AI

TL;DR: ROSA是一个基于知识的机器人自适应性框架，支持任务与架构协同适应（TACA），通过运行时推理实现动态调整。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在多样化环境中因不确定性导致的任务逻辑和架构配置需求变化问题。

Method: 提出ROSA框架，包括知识模型和运行时推理机制，并基于ROS 2实现开源参考。

Result: 实验验证了ROSA在可重用性和开发效率上的优势，适用于水下机器人应用。

Conclusion: ROSA为设计自适应性机器人系统提供了有效解决方案。

Abstract: Autonomous robots must operate in diverse environments and handle multiple
tasks despite uncertainties. This creates challenges in designing software
architectures and task decision-making algorithms, as different contexts may
require distinct task logic and architectural configurations. To address this,
robotic systems can be designed as self-adaptive systems capable of adapting
their task execution and software architecture at runtime based on their
context.This paper introduces ROSA, a novel knowledge-based framework for RObot
Self-Adaptation, which enables task-and-architecture co-adaptation (TACA) in
robotic systems. ROSA achieves this by providing a knowledge model that
captures all application-specific knowledge required for adaptation and by
reasoning over this knowledge at runtime to determine when and how adaptation
should occur. In addition to a conceptual framework, this work provides an
open-source ROS 2-based reference implementation of ROSA and evaluates its
feasibility and performance in an underwater robotics application. Experimental
results highlight ROSA's advantages in reusability and development effort for
designing self-adaptive robotic systems.

</details>


### [79] [Howard's Policy Iteration is Subexponential for Deterministic Markov Decision Problems with Rewards of Fixed Bit-size and Arbitrary Discount Factor](https://arxiv.org/abs/2505.00795)
*Dibyangshu Mukherjee,Shivaram Kalyanakrishnan*

Main category: cs.AI

TL;DR: 本文改进了Howard策略迭代（HPI）在确定性MDP（DMDP）上的运行时间上界，提出了一种次指数上界，且与折扣因子无关。


<details>
  <summary>Details</summary>
Motivation: 尽管HPI算法已有60多年历史，但其在确定性MDP上的运行时间上界仍为指数级，而现有下界仅为线性。本文旨在填补这一理论空白。

Method: 通过参数化奖励的比特大小，提出了一种新的分析方法，改进了HPI在DMDP上的运行时间上界。

Result: 证明了HPI在DMDP上的次指数上界，且该上界适用于仅含两种奖励的DMDP。

Conclusion: 本文显著提升了HPI在DMDP上的理论性能，为未来研究提供了新的方向。

Abstract: Howard's Policy Iteration (HPI) is a classic algorithm for solving Markov
Decision Problems (MDPs). HPI uses a "greedy" switching rule to update from any
non-optimal policy to a dominating one, iterating until an optimal policy is
found. Despite its introduction over 60 years ago, the best-known upper bounds
on HPI's running time remain exponential in the number of states -- indeed even
on the restricted class of MDPs with only deterministic transitions (DMDPs).
Meanwhile, the tightest lower bound for HPI for MDPs with a constant number of
actions per state is only linear. In this paper, we report a significant
improvement: a subexponential upper bound for HPI on DMDPs, which is
parameterised by the bit-size of the rewards, while independent of the discount
factor. The same upper bound also applies to DMDPs with only two possible
rewards (which may be of arbitrary size).

</details>


### [80] [Explanations as Bias Detectors: A Critical Study of Local Post-hoc XAI Methods for Fairness Exploration](https://arxiv.org/abs/2505.00802)
*Vasiliki Papanikou,Danae Pla Karidi,Evaggelia Pitoura,Emmanouil Panagiotou,Eirini Ntoutsi*

Main category: cs.AI

TL;DR: 论文探讨如何利用可解释性方法检测和解释AI中的不公平现象，提出了一种结合局部事后解释方法的流程，并分析了其关键问题和效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI在影响人类生活的领域广泛应用，公平性和透明性问题日益突出，尤其是对受保护群体的影响。可解释性与公平性的结合成为推动负责任AI的重要方向。

Method: 提出了一种整合局部事后解释方法的流程，用于检测和解释不公平现象，并解决了使用解释作为偏见检测器时的关键问题。

Result: 结果显示可解释性方法在公平性方面具有潜力，但需谨慎考虑解释方法的可靠性、一致性及其对群体公平评估的影响。

Conclusion: 可解释性方法可用于公平性检测，但需注意解释方法的局限性及其对公平性评估的影响。

Abstract: As Artificial Intelligence (AI) is increasingly used in areas that
significantly impact human lives, concerns about fairness and transparency have
grown, especially regarding their impact on protected groups. Recently, the
intersection of explainability and fairness has emerged as an important area to
promote responsible AI systems. This paper explores how explainability methods
can be leveraged to detect and interpret unfairness. We propose a pipeline that
integrates local post-hoc explanation methods to derive fairness-related
insights. During the pipeline design, we identify and address critical
questions arising from the use of explanations as bias detectors such as the
relationship between distributive and procedural fairness, the effect of
removing the protected attribute, the consistency and quality of results across
different explanation methods, the impact of various aggregation strategies of
local explanations on group fairness evaluations, and the overall
trustworthiness of explanations as bias detectors. Our results show the
potential of explanation methods used for fairness while highlighting the need
to carefully consider the aforementioned critical aspects.

</details>


### [81] [MIMIC-\RNum{4}-Ext-22MCTS: A 22 Millions-Event Temporal Clinical Time-Series Dataset with Relative Timestamp for Risk Prediction](https://arxiv.org/abs/2505.00827)
*Jing Wang,Xing Niu,Juyong Kim,Jie Shen,Tong Zhang,Jeremy C. Weiss*

Main category: cs.AI

TL;DR: 论文介绍了MIMIC-4-Ext-22MCTS数据集，通过从MIMIC-IV-Note中提取临床事件和时间戳，解决了非结构化数据的处理难题，并展示了其在医疗应用中的显著效果。


<details>
  <summary>Details</summary>
Motivation: 现代医疗中基于机器学习的临床风险预测需要高质量的时间序列数据，但现有数据（如MIMIC-IV-Note）存在非结构化和时间信息缺失的问题。

Method: 提出新框架：1）将出院摘要分块；2）使用BM25和语义搜索筛选含临床事件的块；3）设计提示词引导Llama-3.1-8B模型识别或推断时间信息。

Result: BERT和GPT-2模型在医疗问答和临床试验匹配任务中分别提升了10%和3%的准确率，生成结果更可靠。

Conclusion: MIMIC-4-Ext-22MCTS数据集为医疗预测模型提供了高质量数据，显著提升了模型性能。

Abstract: Clinical risk prediction based on machine learning algorithms plays a vital
role in modern healthcare. A crucial component in developing a reliable
prediction model is collecting high-quality time series clinical events. In
this work, we release such a dataset that consists of 22,588,586 Clinical Time
Series events, which we term MIMIC-\RNum{4}-Ext-22MCTS. Our source data are
discharge summaries selected from the well-known yet unstructured MIMIC-IV-Note
\cite{Johnson2023-pg}. We then extract clinical events as short text span from
the discharge summaries, along with the timestamps of these events as temporal
information. The general-purpose MIMIC-IV-Note pose specific challenges for our
work: it turns out that the discharge summaries are too lengthy for typical
natural language models to process, and the clinical events of interest often
are not accompanied with explicit timestamps. Therefore, we propose a new
framework that works as follows: 1) we break each discharge summary into
manageably small text chunks; 2) we apply contextual BM25 and contextual
semantic search to retrieve chunks that have a high potential of containing
clinical events; and 3) we carefully design prompts to teach the recently
released Llama-3.1-8B \cite{touvron2023llama} model to identify or infer
temporal information of the chunks. We show that the obtained dataset is so
informative and transparent that standard models fine-tuned on our dataset are
achieving significant improvements in healthcare applications. In particular,
the BERT model fine-tuned based on our dataset achieves 10\% improvement in
accuracy on medical question answering task, and 3\% improvement in clinical
trial matching task compared with the classic BERT. The GPT-2 model, fine-tuned
on our dataset, produces more clinically reliable results for clinical
questions.

</details>


### [82] [Thoughts without Thinking: Reconsidering the Explanatory Value of Chain-of-Thought Reasoning in LLMs through Agentic Pipelines](https://arxiv.org/abs/2505.00875)
*Ramesh Manuvinakurike,Emanuel Moss,Elizabeth Anne Watkins,Saurav Sahay,Giuseppe Raffa,Lama Nachman*

Main category: cs.AI

TL;DR: 研究发现，在多LLM协作的Agentic管道中，Chain-of-Thought（CoT）推理并不能提升输出质量或提供有效的可解释性。


<details>
  <summary>Details</summary>
Motivation: 探索如何使LLM的内部运作在Agentic管道中对用户透明且可操作，解决HCXAI社区面临的挑战。

Method: 通过定量和定性分析，研究CoT推理在Agentic管道中的表现。

Result: CoT推理未能提升输出质量，其生成的解释对用户理解系统或达成目标无帮助。

Conclusion: CoT推理在Agentic管道中无法满足可解释性需求，需进一步研究其他方法。

Abstract: Agentic pipelines present novel challenges and opportunities for
human-centered explainability. The HCXAI community is still grappling with how
best to make the inner workings of LLMs transparent in actionable ways. Agentic
pipelines consist of multiple LLMs working in cooperation with minimal human
control. In this research paper, we present early findings from an agentic
pipeline implementation of a perceptive task guidance system. Through
quantitative and qualitative analysis, we analyze how Chain-of-Thought (CoT)
reasoning, a common vehicle for explainability in LLMs, operates within agentic
pipelines. We demonstrate that CoT reasoning alone does not lead to better
outputs, nor does it offer explainability, as it tends to produce explanations
without explainability, in that they do not improve the ability of end users to
better understand systems or achieve their goals.

</details>


### [83] [Car Sensors Health Monitoring by Verification Based on Autoencoder and Random Forest Regression](https://arxiv.org/abs/2505.00876)
*Sahar Torkhesari,Behnam Yousefimehr,Mehdi Ghatee*

Main category: cs.AI

TL;DR: 论文提出了一种创新的汽车传感器健康监测系统，利用机器学习和深度学习技术分析传感器数据，结合自编码器和随机森林回归实现高精度故障检测与估计。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够实时监测汽车传感器健康状况的系统，以提前发现故障并确保行车安全。

Method: 通过机器学习和深度学习分析传感器数据，使用自编码器检测故障，随机森林回归估计传感器值，并基于正态分布模型进行故障预测。

Result: 系统在Saipa Quick车辆的20个关键传感器上测试，准确率达到99%。

Conclusion: 该系统能有效提前检测传感器故障，并通过估计值替代故障值，显著提升车辆安全性和维护效率。

Abstract: Driver assistance systems provide a wide range of crucial services, including
closely monitoring the condition of vehicles. This paper showcases a
groundbreaking sensor health monitoring system designed for the automotive
industry. The ingenious system leverages cutting-edge techniques to process
data collected from various vehicle sensors. It compares their outputs within
the Electronic Control Unit (ECU) to evaluate the health of each sensor. To
unravel the intricate correlations between sensor data, an extensive
exploration of machine learning and deep learning methodologies was conducted.
Through meticulous analysis, the most correlated sensor data were identified.
These valuable insights were then utilized to provide accurate estimations of
sensor values. Among the diverse learning methods examined, the combination of
autoencoders for detecting sensor failures and random forest regression for
estimating sensor values proved to yield the most impressive outcomes. A
statistical model using the normal distribution has been developed to identify
possible sensor failures proactively. By comparing the actual values of the
sensors with their estimated values based on correlated sensors, faulty sensors
can be detected early. When a defective sensor is detected, both the driver and
the maintenance department are promptly alerted. Additionally, the system
replaces the value of the faulty sensor with the estimated value obtained
through analysis. This proactive approach was evaluated using data from twenty
essential sensors in the Saipa's Quick vehicle's ECU, resulting in an
impressive accuracy rate of 99\%.

</details>


### [84] [Seeking to Collide: Online Safety-Critical Scenario Generation for Autonomous Driving with Retrieval Augmented Large Language Models](https://arxiv.org/abs/2505.00972)
*Yuewen Mei,Tong Nie,Jian Sun,Ye Tian*

Main category: cs.AI

TL;DR: 提出了一种基于检索增强的大型语言模型框架，用于生成安全关键的驾驶场景，显著提高了碰撞检测率。


<details>
  <summary>Details</summary>
Motivation: 现有场景生成方法容易过拟合或无法暴露罕见的安全关键情况，需要一种在线、交互式的方法。

Method: 使用LLM推断背景车辆的危险意图，并通过查询其他LLM代理合成对抗轨迹，结合动态记忆库加速适应。

Result: 在Waymo数据集上，模型将平均最小碰撞时间从1.62秒降至1.08秒，碰撞率提高75%。

Conclusion: 该方法有效生成安全关键场景，显著优于基线方法。

Abstract: Simulation-based testing is crucial for validating autonomous vehicles (AVs),
yet existing scenario generation methods either overfit to common driving
patterns or operate in an offline, non-interactive manner that fails to expose
rare, safety-critical corner cases. In this paper, we introduce an online,
retrieval-augmented large language model (LLM) framework for generating
safety-critical driving scenarios. Our method first employs an LLM-based
behavior analyzer to infer the most dangerous intent of the background vehicle
from the observed state, then queries additional LLM agents to synthesize
feasible adversarial trajectories. To mitigate catastrophic forgetting and
accelerate adaptation, we augment the framework with a dynamic memorization and
retrieval bank of intent-planner pairs, automatically expanding its behavioral
library when novel intents arise. Evaluations using the Waymo Open Motion
Dataset demonstrate that our model reduces the mean minimum time-to-collision
from 1.62 to 1.08 s and incurs a 75% collision rate, substantially
outperforming baselines.

</details>


### [85] [Improving Large Language Model Planning with Action Sequence Similarity](https://arxiv.org/abs/2505.01009)
*Xinran Zhao,Hanie Sedghi,Bernd Bohnet,Dale Schuurmans,Azade Nova*

Main category: cs.AI

TL;DR: 论文探讨了如何通过上下文学习（ICL）提升大型语言模型（LLM）的规划能力，提出了一种基于动作序列相似性（AS）的样本选择和过滤方法GRASE-DC，显著提高了规划任务的性能。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在规划任务中的表现，并探索上下文中的哪些信号能有效提升模型性能。

Method: 提出GRASE-DC方法，通过两阶段流程（重采样高AS样本和动态聚类）优化样本选择，平衡相关性与多样性。

Result: GRASE-DC在多种规划任务中显著提升性能（最高提升11-40点准确率，平均减少27.3%样本需求），并验证了其在分布外问题上的泛化能力。

Conclusion: GRASE-DC通过优化样本选择显著提升了LLM的规划能力，尤其在复杂任务中表现突出。

Abstract: Planning is essential for artificial intelligence systems to look ahead and
proactively determine a course of actions to reach objectives in the virtual
and real world. Recent work on large language models (LLMs) sheds light on
their planning capability in various tasks. However, it remains unclear what
signals in the context influence the model performance. In this work, we
explore how to improve the model planning capability through in-context
learning (ICL), specifically, what signals can help select the exemplars.
Through extensive experiments, we observe that commonly used problem similarity
may result in false positives with drastically different plans, which can
mislead the model. In response, we propose to sample and filter exemplars
leveraging plan side action sequence similarity (AS). We propose GRASE-DC: a
two-stage pipeline that first re-samples high AS exemplars and then curates the
selected exemplars with dynamic clustering on AS to achieve a balance of
relevance and diversity. Our experimental result confirms that GRASE-DC
achieves significant performance improvement on various planning tasks (up to
~11-40 point absolute accuracy improvement with 27.3% fewer exemplars needed on
average). With GRASE-DC* + VAL, where we iteratively apply GRASE-DC with a
validator, we are able to even boost the performance by 18.9% more.
  Extensive analysis validates the consistent performance improvement of
GRASE-DC with various backbone LLMs and on both classical planning and natural
language planning benchmarks. GRASE-DC can further boost the planning accuracy
by ~24 absolute points on harder problems using simpler problems as exemplars
over a random baseline. This demonstrates its ability to generalize to
out-of-distribution problems.

</details>


### [86] [Adaptive Wizard for Removing Cross-Tier Misconfigurations in Active Directory](https://arxiv.org/abs/2505.01028)
*Huy Q. Ngo,Mingyu Guo,Hung Nguyen*

Main category: cs.AI

TL;DR: 论文提出了一种优化Windows AD系统安全漏洞修复流程的方法，通过最小化IT管理员与安全向导的交互次数来减少人工工作量。


<details>
  <summary>Details</summary>
Motivation: 现有的AD系统漏洞修复流程需要大量人工审核，效率低下。研究旨在减少这一过程中的手动工作量。

Method: 提出自适应路径移除问题模型，设计精确算法、近似算法和启发式方法（如DPR）来优化交互次数。

Result: 实验证明DPR启发式方法在大规模图上表现优异，优于近似算法。

Conclusion: 该方法有效减少了AD系统漏洞修复中的人工工作量，适用于实际场景。

Abstract: Security vulnerabilities in Windows Active Directory (AD) systems are
typically modeled using an attack graph and hardening AD systems involves an
iterative workflow: security teams propose an edge to remove, and IT operations
teams manually review these fixes before implementing the removal. As
verification requires significant manual effort, we formulate an Adaptive Path
Removal Problem to minimize the number of steps in this iterative removal
process. In our model, a wizard proposes an attack path in each step and
presents it as a set of multiple-choice options to the IT admin. The IT admin
then selects one edge from the proposed set to remove. This process continues
until the target $t$ is disconnected from source $s$ or the number of proposed
paths reaches $B$. The model aims to optimize the human effort by minimizing
the expected number of interactions between the IT admin and the security
wizard. We first prove that the problem is $\mathcal{\#P}$-hard. We then
propose a set of solutions including an exact algorithm, an approximate
algorithm, and several scalable heuristics. Our best heuristic, called DPR, can
operate effectively on larger-scale graphs compared to the exact algorithm and
consistently outperforms the approximate algorithm across all graphs. We verify
the effectiveness of our algorithms on several synthetic AD graphs and an AD
attack graph collected from a real organization.

</details>


### [87] [Retrieval Augmented Learning: A Retrial-based Large Language Model Self-Supervised Learning and Autonomous Knowledge Generation](https://arxiv.org/abs/2505.01073)
*Zongyuan Li,Pengfei Li,Runnan Qi,Yanan Ni,Lumin Jiang,Hui Wu,Xuebo Zhang,Kuihua Huang,Xian Guo*

Main category: cs.AI

TL;DR: 本文提出了一种无需训练的自监督学习框架RAL，通过检索增强生成（RAG）组织中间数据，实现自主知识生成，显著降低幻觉并提升决策性能。


<details>
  <summary>Details</summary>
Motivation: 预训练大型语言模型（LLMs）在专业领域数据不足，限制了其在专业应用中的表现，而后续训练又需大量计算资源。

Method: 开发RAG模块，实现假设提出、验证和知识生成的三阶段自主知识生成，并在LLM-PySC2环境中评估。

Result: 实验表明，该方法有效减少幻觉，提升决策性能，且在OOD任务、鲁棒性和可迁移性方面表现潜力。

Conclusion: RAL是一种低成本高效的决策和自主知识生成解决方案。

Abstract: The lack of domain-specific data in the pre-training of Large Language Models
(LLMs) severely limits LLM-based decision systems in specialized applications,
while post-training a model in the scenarios requires significant computational
resources. In this paper, we present Retrial-Augmented Learning (RAL), a
reward-free self-supervised learning framework for LLMs that operates without
model training. By developing Retrieval-Augmented Generation (RAG) into a
module for organizing intermediate data, we realized a three-stage autonomous
knowledge generation of proposing a hypothesis, validating the hypothesis, and
generating the knowledge. The method is evaluated in the LLM-PySC2 environment,
a representative decision-making platform that combines sufficient complexity
with domain-specific knowledge requirements. Experiments demonstrate that the
proposed method effectively reduces hallucination by generating and utilizing
validated knowledge, and increases decision-making performance at an extremely
low cost. Meanwhile, the approach exhibits potential in
out-of-distribution(OOD) tasks, robustness, and transferability, making it a
cost-friendly but effective solution for decision-making problems and
autonomous knowledge generation.

</details>


### [88] [MADIL: An MDL-based Framework for Efficient Program Synthesis in the ARC Benchmark](https://arxiv.org/abs/2505.01081)
*Sébastien Ferré*

Main category: cs.AI

TL;DR: MADIL（基于MDL的AI）是一种利用最小描述长度原则的高效归纳学习方法，用于解决AI在技能获取和泛化上的挑战，尽管性能低于LLM方法，但更高效且可解释。


<details>
  <summary>Details</summary>
Motivation: AI在专业任务上表现出色，但在技能获取和泛化方面效率不足，ARC基准测试为此提供了评估标准。

Method: MADIL采用基于最小描述长度（MDL）原则的模式分解方法，实现结构化泛化。

Result: MADIL在ArcPrize 2024中的表现（7%）低于基于LLM的方法，但具有更高的效率和可解释性。

Conclusion: MADIL为高效且可解释的AI学习提供了新思路，尽管性能有待提升。

Abstract: Artificial Intelligence (AI) has achieved remarkable success in specialized
tasks but struggles with efficient skill acquisition and generalization. The
Abstraction and Reasoning Corpus (ARC) benchmark evaluates intelligence based
on minimal training requirements. While Large Language Models (LLMs) have
recently improved ARC performance, they rely on extensive pre-training and high
computational costs. We introduce MADIL (MDL-based AI), a novel approach
leveraging the Minimum Description Length (MDL) principle for efficient
inductive learning. MADIL performs pattern-based decomposition, enabling
structured generalization. While its performance (7% at ArcPrize 2024) remains
below LLM-based methods, it offers greater efficiency and interpretability.
This paper details MADIL's methodology, its application to ARC, and
experimental evaluations.

</details>


### [89] [Explainable AI Based Diagnosis of Poisoning Attacks in Evolutionary Swarms](https://arxiv.org/abs/2505.01181)
*Mehrdad Asadi,Roxana Rădulescu,Ann Nowé*

Main category: cs.AI

TL;DR: 论文研究了多无人机网络中数据投毒攻击对团队协作策略的影响，并提出了一种基于可解释AI的框架来诊断和量化攻击效果。


<details>
  <summary>Details</summary>
Motivation: 多无人机网络在关键环境中的协作任务（如监测、灾害援助）中表现优异，但团队级策略易受数据投毒攻击，导致协作失效或对抗行为。

Method: 通过进化智能建模代理交互，形成最优联盟执行任务，并系统性地投毒攻击模型，利用可解释AI方法量化攻击效果。

Result: 研究发现，当投毒比例超过10%时，会导致非最优策略和低效协作。

Conclusion: 可解释AI方法能有效诊断数据投毒攻击对团队策略的影响，为未来防御措施提供依据。

Abstract: Swarming systems, such as for example multi-drone networks, excel at
cooperative tasks like monitoring, surveillance, or disaster assistance in
critical environments, where autonomous agents make decentralized decisions in
order to fulfill team-level objectives in a robust and efficient manner.
Unfortunately, team-level coordinated strategies in the wild are vulnerable to
data poisoning attacks, resulting in either inaccurate coordination or
adversarial behavior among the agents. To address this challenge, we contribute
a framework that investigates the effects of such data poisoning attacks, using
explainable AI methods. We model the interaction among agents using
evolutionary intelligence, where an optimal coalition strategically emerges to
perform coordinated tasks. Then, through a rigorous evaluation, the swarm model
is systematically poisoned using data manipulation attacks. We showcase the
applicability of explainable AI methods to quantify the effects of poisoning on
the team strategy and extract footprint characterizations that enable
diagnosing. Our findings indicate that when the model is poisoned above 10%,
non-optimal strategies resulting in inefficient cooperation can be identified.

</details>


### [90] [Exploring the Impact of Explainable AI and Cognitive Capabilities on Users' Decisions](https://arxiv.org/abs/2505.01192)
*Federico Maria Cau,Lucio Davide Spano*

Main category: cs.AI

TL;DR: 研究探讨了不同解释风格（如示例、特征、规则和反事实）和AI信息对贷款申请决策的影响，发现高AI信心增加依赖并降低认知负荷，反事实解释提高准确性但理解性较差。


<details>
  <summary>Details</summary>
Motivation: 探讨AI决策中不同解释风格和个性特质（如NFC）对用户决策的影响，以优化人机协作。

Method: 在贷款申请场景中测试不同解释风格和AI信息对准确性、依赖性和认知负荷的影响，并比较高低NFC个体的差异。

Result: 高AI信心增加依赖并降低认知负荷；反事实解释提高准确性但理解性较差；高低NFC个体在优先级上无显著差异。

Conclusion: 需个性化XAI界面，结合多种解释风格和用户特质以优化人机协作。

Abstract: Artificial Intelligence (AI) systems are increasingly used for
decision-making across domains, raising debates over the information and
explanations they should provide. Most research on Explainable AI (XAI) has
focused on feature-based explanations, with less attention on alternative
styles. Personality traits like the Need for Cognition (NFC) can also lead to
different decision-making outcomes among low and high NFC individuals. We
investigated how presenting AI information (prediction, confidence, and
accuracy) and different explanation styles (example-based, feature-based,
rule-based, and counterfactual) affect accuracy, reliance on AI, and cognitive
load in a loan application scenario. We also examined low and high NFC
individuals' differences in prioritizing XAI interface elements (loan
attributes, AI information, and explanations), accuracy, and cognitive load.
Our findings show that high AI confidence significantly increases reliance on
AI while reducing cognitive load. Feature-based explanations did not enhance
accuracy compared to other conditions. Although counterfactual explanations
were less understandable, they enhanced overall accuracy, increasing reliance
on AI and reducing cognitive load when AI predictions were correct. Both low
and high NFC individuals prioritized explanations after loan attributes,
leaving AI information as the least important. However, we found no significant
differences between low and high NFC groups in accuracy or cognitive load,
raising questions about the role of personality traits in AI-assisted
decision-making. These findings highlight the need for user-centric
personalization in XAI interfaces, incorporating diverse explanation styles and
exploring multiple personality traits and other user characteristics to
optimize human-AI collaboration.

</details>


### [91] [Early Detection of Patient Deterioration from Real-Time Wearable Monitoring System](https://arxiv.org/abs/2505.01305)
*Lo Pang-Yun Ting,Hong-Pei Chen,An-Shan Liu,Chun-Yin Yeh,Po-Lin Chen,Kun-Ta Chuang*

Main category: cs.AI

TL;DR: TARL是一种创新方法，通过建模心率时间序列中的代表性子序列（shapelets）的结构关系，构建知识图谱以预测病情进展，并引入嵌入技术处理缺失值，实现早期病情检测。


<details>
  <summary>Details</summary>
Motivation: 早期检测患者病情恶化对降低死亡率至关重要，但心率数据的多样性和缺失值处理是主要挑战。

Method: TARL通过建模shapelet动态关系构建知识图谱，并利用过渡感知知识嵌入强化关系，量化缺失值影响。

Result: 实验表明TARL在ICU数据中具有高可靠性和早期检测能力，案例研究展示了其可解释性。

Conclusion: TARL可作为AI工具辅助临床医生早期识别患者病情恶化。

Abstract: Early detection of patient deterioration is crucial for reducing mortality
rates. Heart rate data has shown promise in assessing patient health, and
wearable devices offer a cost-effective solution for real-time monitoring.
However, extracting meaningful insights from diverse heart rate data and
handling missing values in wearable device data remain key challenges. To
address these challenges, we propose TARL, an innovative approach that models
the structural relationships of representative subsequences, known as
shapelets, in heart rate time series. TARL creates a shapelet-transition
knowledge graph to model shapelet dynamics in heart rate time series,
indicating illness progression and potential future changes. We further
introduce a transition-aware knowledge embedding to reinforce relationships
among shapelets and quantify the impact of missing values, enabling the
formulation of comprehensive heart rate representations. These representations
capture explanatory structures and predict future heart rate trends, aiding
early illness detection. We collaborate with physicians and nurses to gather
ICU patient heart rate data from wearables and diagnostic metrics assessing
illness severity for evaluating deterioration. Experiments on real-world ICU
data demonstrate that TARL achieves both high reliability and early detection.
A case study further showcases TARL's explainable detection process,
highlighting its potential as an AI-driven tool to assist clinicians in
recognizing early signs of patient deterioration.

</details>


### [92] [BalancEdit: Dynamically Balancing the Generality-Locality Trade-off in Multi-modal Model Editing](https://arxiv.org/abs/2505.01343)
*Dongliang Guo,Mengxuan Hu,Zihan Guan,Thomas Hartvigsen,Sheng Li*

Main category: cs.AI

TL;DR: 论文提出BalancEdit方法，解决多模态模型编辑中的通用性与局部性权衡问题，通过动态平衡和局部化编辑实现高效更新。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法不适用于大型多模态模型更新，现有编辑技术忽视事实影响范围，导致性能下降。

Method: 提出BalancEdit方法，生成正负样本确定影响范围，利用局部化编辑码本动态平衡通用性与局部性。

Result: BalancEdit在保持编辑能力的同时，最小化通用性与局部性的权衡，效果显著。

Conclusion: BalancEdit是首个明确解决多模态模型编辑中通用性与局部性权衡的方法，实验证明其有效性。

Abstract: Large multi-modal models inevitably decay over time as facts change and
previously learned information becomes outdated. Traditional approaches such as
fine-tuning are often impractical for updating these models due to their size
and complexity. Instead, direct knowledge editing within the models presents a
more viable solution. Current model editing techniques, however, typically
overlook the unique influence ranges of different facts, leading to compromised
model performance in terms of both generality and locality. To address this
issue, we introduce the concept of the generality-locality trade-off in
multi-modal model editing. We develop a new model editing dataset named OKEDIT,
specifically designed to effectively evaluate this trade-off. Building on this
foundation, we propose BalancEdit, a novel method for balanced model editing
that dynamically achieves an optimal balance between generality and locality.
BalancEdit utilizes a unique mechanism that generates both positive and
negative samples for each fact to accurately determine its influence scope and
incorporates these insights into the model's latent space using a discrete,
localized codebook of edits, without modifying the underlying model weights. To
our knowledge, this is the first approach explicitly addressing the
generality-locality trade-off in multi-modal model editing. Our comprehensive
results confirm the effectiveness of BalancEdit, demonstrating minimal
trade-offs while maintaining robust editing capabilities. Our code and dataset
will be available.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [93] [Constructing an Optimal Behavior Basis for the Option Keyboard](https://arxiv.org/abs/2505.00787)
*Lucas N. Alegre,Ana L. C. Bazzan,André Barreto,Bruno C. da Silva*

Main category: cs.LG

TL;DR: 论文提出了一种高效构建最优行为基的新方法，显著减少了确保新任务最优性所需的基策略数量，并在复杂任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习的目标是快速为新任务找到解决方案，而无需额外环境交互。现有方法如GPI和OK虽有效，但依赖基策略选择且计算成本高。

Method: 引入了一种新方法，高效构建最优行为基，证明其比CCS更具表达力，并能处理非线性任务。

Result: 实验表明，该方法在复杂任务中显著优于现有技术，且随着任务复杂度增加，优势更明显。

Conclusion: 该方法解决了构建最优行为基的开源问题，为多任务强化学习提供了更高效的解决方案。

Abstract: Multi-task reinforcement learning aims to quickly identify solutions for new
tasks with minimal or no additional interaction with the environment.
Generalized Policy Improvement (GPI) addresses this by combining a set of base
policies to produce a new one that is at least as good -- though not
necessarily optimal -- as any individual base policy. Optimality can be
ensured, particularly in the linear-reward case, via techniques that compute a
Convex Coverage Set (CCS). However, these are computationally expensive and do
not scale to complex domains. The Option Keyboard (OK) improves upon GPI by
producing policies that are at least as good -- and often better. It achieves
this through a learned meta-policy that dynamically combines base policies.
However, its performance critically depends on the choice of base policies.
This raises a key question: is there an optimal set of base policies -- an
optimal behavior basis -- that enables zero-shot identification of optimal
solutions for any linear tasks? We solve this open problem by introducing a
novel method that efficiently constructs such an optimal behavior basis. We
show that it significantly reduces the number of base policies needed to ensure
optimality in new tasks. We also prove that it is strictly more expressive than
a CCS, enabling particular classes of non-linear tasks to be solved optimally.
We empirically evaluate our technique in challenging domains and show that it
outperforms state-of-the-art approaches, increasingly so as task complexity
increases.

</details>


### [94] [Improving Routing in Sparse Mixture of Experts with Graph of Tokens](https://arxiv.org/abs/2505.00792)
*Tam Nguyen,Ngoc N. Tran,Khai Nguyen,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 论文提出了一种新型的Similarity/Attention-Aware (S)MoE方法，通过考虑token间的交互和注意力矩阵来减少路由波动，提高模型鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 稀疏混合专家（SMoE）在深度学习中的扩展性显著，但存在路由波动问题，导致模型非鲁棒性。本文从概率图模型（PGM）角度揭示其局限性，并提出改进方法。

Method: 提出Similarity-Aware (S)MoE和Attention-Aware (S)MoE，利用token相似性和注意力矩阵优化路由选择，减少专家选择的熵。

Result: 实验验证表明，新方法显著减少了路由波动，提高了准确性和模型鲁棒性。

Conclusion: 通过引入token交互和注意力机制，新方法有效解决了SMoE的路由波动问题，提升了模型性能。

Abstract: Sparse Mixture of Experts (SMoE) has emerged as a key to achieving
unprecedented scalability in deep learning. By activating only a small subset
of parameters per sample, SMoE achieves an exponential increase in parameter
counts while maintaining a constant computational overhead. However, SMoE
models are susceptible to routing fluctuations--changes in the routing of a
given input to its target expert--at the late stage of model training, leading
to model non-robustness. In this work, we unveil the limitation of SMoE through
the perspective of the probabilistic graphical model (PGM). Through this PGM
framework, we highlight the independence in the expert-selection of tokens,
which exposes the model to routing fluctuation and non-robustness. Alleviating
this independence, we propose the novel Similarity-Aware (S)MoE, which
considers interactions between tokens during expert selection. We then derive a
new PGM underlying an (S)MoE-Attention block, going beyond just a single (S)MoE
layer. Leveraging the token similarities captured by the attention matrix, we
propose the innovative Attention-Aware (S)MoE, which employs the attention
matrix to guide the routing of tokens to appropriate experts in (S)MoE. We
theoretically prove that Similarity/Attention-Aware routing help reduce the
entropy of expert selection, resulting in more stable token routing mechanisms.
We empirically validate our models on various tasks and domains, showing
significant improvements in reducing routing fluctuations, enhancing accuracy,
and increasing model robustness over the baseline MoE-Transformer with token
routing via softmax gating.

</details>


### [95] [Scalable Meta-Learning via Mixed-Mode Differentiation](https://arxiv.org/abs/2505.00793)
*Iurii Kemaev,Dan A Calian,Luisa M Zintgraf,Gregory Farquhar,Hado van Hasselt*

Main category: cs.LG

TL;DR: 论文提出MixFlow-MG算法，通过混合模式微分优化梯度计算，显著提升内存和计算效率。


<details>
  <summary>Details</summary>
Motivation: 梯度双层优化在超参数优化、任务适应等领域广泛应用，但现有方法计算效率低。

Method: 提出Mixed-Flow Meta-Gradients（MixFlow-MG），利用混合模式微分构建高效计算图。

Result: 在元学习场景中，内存节省10倍以上，计算时间减少25%。

Conclusion: MixFlow-MG为梯度双层优化提供了更高效、可扩展的解决方案。

Abstract: Gradient-based bilevel optimisation is a powerful technique with applications
in hyperparameter optimisation, task adaptation, algorithm discovery,
meta-learning more broadly, and beyond. It often requires differentiating
through the gradient-based optimisation process itself, leading to
"gradient-of-a-gradient" calculations with computationally expensive
second-order and mixed derivatives. While modern automatic differentiation
libraries provide a convenient way to write programs for calculating these
derivatives, they oftentimes cannot fully exploit the specific structure of
these problems out-of-the-box, leading to suboptimal performance. In this
paper, we analyse such cases and propose Mixed-Flow Meta-Gradients, or
MixFlow-MG -- a practical algorithm that uses mixed-mode differentiation to
construct more efficient and scalable computational graphs yielding over 10x
memory and up to 25% wall-clock time improvements over standard implementations
in modern meta-learning setups.

</details>


### [96] [A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i](https://arxiv.org/abs/2505.00808)
*Kola Ayonrinde,Louis Jaburi*

Main category: cs.LG

TL;DR: 论文提出解释性观点假说，认为机制可解释性研究是一种理解神经网络的系统性方法，并定义了机制可解释性（MI）及其评估标准。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过因果解释理解神经网络，提出机制可解释性研究的重要性。

Method: 提出解释性忠实度的定义，并详细阐述机制可解释性的框架及其限制。

Result: 定义了机制可解释性，并提出了解释性乐观原则作为其成功的前提条件。

Conclusion: 机制可解释性是一种系统性的解释方法，但其成功依赖于解释性乐观原则。

Abstract: Mechanistic Interpretability aims to understand neural networks through
causal explanations. We argue for the Explanatory View Hypothesis: that
Mechanistic Interpretability research is a principled approach to understanding
models because neural networks contain implicit explanations which can be
extracted and understood. We hence show that Explanatory Faithfulness, an
assessment of how well an explanation fits a model, is well-defined. We propose
a definition of Mechanistic Interpretability (MI) as the practice of producing
Model-level, Ontic, Causal-Mechanistic, and Falsifiable explanations of neural
networks, allowing us to distinguish MI from other interpretability paradigms
and detail MI's inherent limits. We formulate the Principle of Explanatory
Optimism, a conjecture which we argue is a necessary precondition for the
success of Mechanistic Interpretability.

</details>


### [97] [Scalable Unit Harmonization in Medical Informatics Using Bi-directional Transformers and Bayesian-Optimized BM25 and Sentence Embedding Retrieval](https://arxiv.org/abs/2505.00810)
*Jordi de la Torre*

Main category: cs.LG

TL;DR: 论文提出了一种结合BM25、句子嵌入和双向Transformer的方法，用于大规模临床数据中的单位标准化，显著提高了检索和匹配的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决临床数据中单位不一致的问题，提升数据互操作性。

Method: 设计了一个多阶段流程，包括过滤、识别、标准化提议生成、自动重新排序和手动验证，结合BM25、句子嵌入和双向Transformer分类器。

Result: 混合检索方法（MRR: 0.8833）优于纯词法或纯嵌入方法，Transformer重新排序进一步提升了性能（最终MRR: 0.9833）。

Conclusion: 该框架为临床数据单位标准化提供了高效、可扩展的解决方案，减少了人工干预并提高了准确性，支持跨机构研究和元分析。

Abstract: Objective: To develop and evaluate a scalable methodology for harmonizing
inconsistent units in large-scale clinical datasets, addressing a key barrier
to data interoperability.
  Materials and Methods: We designed a novel unit harmonization system
combining BM25, sentence embeddings, Bayesian optimization, and a bidirectional
transformer based binary classifier for retrieving and matching laboratory test
entries. The system was evaluated using the Optum Clinformatics Datamart
dataset (7.5 billion entries). We implemented a multi-stage pipeline:
filtering, identification, harmonization proposal generation, automated
re-ranking, and manual validation. Performance was assessed using Mean
Reciprocal Rank (MRR) and other standard information retrieval metrics.
  Results: Our hybrid retrieval approach combining BM25 and sentence embeddings
(MRR: 0.8833) significantly outperformed both lexical-only (MRR: 0.7985) and
embedding-only (MRR: 0.5277) approaches. The transformer-based reranker further
improved performance (absolute MRR improvement: 0.10), bringing the final
system MRR to 0.9833. The system achieved 83.39\% precision at rank 1 and
94.66\% recall at rank 5.
  Discussion: The hybrid architecture effectively leverages the complementary
strengths of lexical and semantic approaches. The reranker addresses cases
where initial retrieval components make errors due to complex semantic
relationships in medical terminology.
  Conclusion: Our framework provides an efficient, scalable solution for unit
harmonization in clinical datasets, reducing manual effort while improving
accuracy. Once harmonized, data can be reused seamlessly in different analyses,
ensuring consistency across healthcare systems and enabling more reliable
multi-institutional studies and meta-analyses.

</details>


### [98] [Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic Optimization](https://arxiv.org/abs/2505.00812)
*Kuan Zhang,Chengliang Chai,Jingzhe Xu,Chi Zhang,Ye Yuan,Guoren Wang,Lei Cao*

Main category: cs.LG

TL;DR: 提出了一种新颖的两阶段噪声学习框架，通过动态加权损失函数实现实例级优化，避免了超参数调优，显著提升了模型性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在噪声监督下存在计算成本高、超参数调优复杂和粗粒度优化等问题，需要一种更高效的解决方案。

Method: 采用动态加权损失函数和简单有效的错误事件指标，分两阶段进行噪声建模和鲁棒训练。

Result: 在五个合成和真实世界的LNL基准测试中，性能优于现有方法，计算时间减少75%，模型可扩展性提升。

Conclusion: 该框架在噪声学习任务中表现出色，具有高效性和可扩展性。

Abstract: Recent studies indicate that deep neural networks degrade in generalization
performance under noisy supervision. Existing methods focus on isolating clean
subsets or correcting noisy labels, facing limitations such as high
computational costs, heavy hyperparameter tuning process, and coarse-grained
optimization. To address these challenges, we propose a novel two-stage noisy
learning framework that enables instance-level optimization through a
dynamically weighted loss function, avoiding hyperparameter tuning. To obtain
stable and accurate information about noise modeling, we introduce a simple yet
effective metric, termed wrong event, which dynamically models the cleanliness
and difficulty of individual samples while maintaining computational costs. Our
framework first collects wrong event information and builds a strong base
model. Then we perform noise-robust training on the base model, using a
probabilistic model to handle the wrong event information of samples.
Experiments on five synthetic and real-world LNL benchmarks demonstrate our
method surpasses state-of-the-art methods in performance, achieves a nearly 75%
reduction in computational time and improves model scalability.

</details>


### [99] [Dual Filter: A Mathematical Framework for Inference using Transformer-like Architectures](https://arxiv.org/abs/2505.00818)
*Heng-Sheng Chang,Prashant G. Mehta*

Main category: cs.LG

TL;DR: 本文提出了一种用于因果非线性预测的数学框架，基于隐马尔可夫模型（HMM）生成观测数据，并借鉴了解码器-仅Transformer架构的思想。


<details>
  <summary>Details</summary>
Motivation: 研究动机是从基本原理出发，推导出类似Transformer的架构，解决Transformer设计的预测问题，而非直接建模Transformer。

Method: 方法基于最优控制理论，将预测目标（MMSE）重新表述为最优控制问题，并通过双滤波器算法求解固定点方程。

Result: 数值实验验证了算法的性能，参数设置与研究中使用的Transformer模型一致。

Conclusion: 结论是提出的框架与Transformer架构有显著相似性，为理解Transformer的数学基础提供了新视角。

Abstract: This paper presents a mathematical framework for causal nonlinear prediction
in settings where observations are generated from an underlying hidden Markov
model (HMM). Both the problem formulation and the proposed solution are
motivated by the decoder-only transformer architecture, in which a finite
sequence of observations (tokens) is mapped to the conditional probability of
the next token. Our objective is not to construct a mathematical model of a
transformer. Rather, our interest lies in deriving, from first principles,
transformer-like architectures that solve the prediction problem for which the
transformer is designed. The proposed framework is based on an original optimal
control approach, where the prediction objective (MMSE) is reformulated as an
optimal control problem. An analysis of the optimal control problem is
presented leading to a fixed-point equation on the space of probability
measures. To solve the fixed-point equation, we introduce the dual filter, an
iterative algorithm that closely parallels the architecture of decoder-only
transformers. These parallels are discussed in detail along with the
relationship to prior work on mathematical modeling of transformers as
transport on the space of probability measures. Numerical experiments are
provided to illustrate the performance of the algorithm using parameter values
used in researchscale transformer models.

</details>


### [100] [Data-Driven Optical To Thermal Inference in Pool Boiling Using Generative Adversarial Networks](https://arxiv.org/abs/2505.00823)
*Qianxi Fu,Youngjoon Suh,Xiaojing Zhang,Yoonjin Won*

Main category: cs.LG

TL;DR: 提出了一种基于条件生成对抗网络（CGAN）的数据驱动框架，用于从几何相轮廓推断温度场，误差低于6%。


<details>
  <summary>Details</summary>
Motivation: 多相热传递的定量表征受限于混沌、快速演变的流动体系中温度场的测量挑战。

Method: 利用CGAN从高速成像数据和仿真训练中重建温度场，并通过数据增强策略提高准确性。

Result: 模型在池沸腾配置中成功重建温度场，误差低于6%，数据增强策略提高了预测的物理合理性。

Conclusion: 深度生成模型有望弥合可观察多相现象与潜在热传输之间的差距，为复杂两相系统提供实验测量的增强和解释方法。

Abstract: Phase change plays a critical role in thermal management systems, yet
quantitative characterization of multiphase heat transfer remains limited by
the challenges of measuring temperature fields in chaotic, rapidly evolving
flow regimes. While computational methods offer spatiotemporal resolution in
idealized cases, replicating complex experimental conditions remains
prohibitively difficult. Here, we present a data-driven framework that
leverages a conditional generative adversarial network (CGAN) to infer
temperature fields from geometric phase contours in a canonical pool boiling
configuration where advanced data collection techniques are restricted. Using
high-speed imaging data and simulation-informed training, our model
demonstrates the ability to reconstruct temperature fields with errors below
6%. We further show that standard data augmentation strategies are effective in
enhancing both accuracy and physical plausibility of the predicted maps across
both simulation and experimental datasets when precise physical constraints are
not applicable. Our results highlight the potential of deep generative models
to bridge the gap between observable multiphase phenomena and underlying
thermal transport, offering a powerful approach to augment and interpret
experimental measurements in complex two-phase systems.

</details>


### [101] [Intersectional Divergence: Measuring Fairness in Regression](https://arxiv.org/abs/2505.00830)
*Joe Germino,Nuno Moniz,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: 本文提出了一种衡量回归任务中交叉公平性的新方法，通过考虑所有受保护属性的组合，并引入Intersectional Divergence（ID）作为公平性度量，进一步优化模型行为。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注分类任务中的公平性，而回归任务中的公平性研究存在空白。本文旨在填补这一空白，并强调仅关注单一受保护属性或平均误差的不足。

Method: 提出Intersectional Divergence（ID）作为回归任务中的公平性度量，并将其扩展为损失函数IDLoss，用于优化问题。

Result: 实验表明，ID能提供对模型行为和公平性的独特见解，而IDLoss能显著提高单属性和交叉公平性，同时保持预测性能的竞争力。

Conclusion: ID和IDLoss为回归任务中的公平性研究提供了新工具，能够更全面地评估和优化模型公平性。

Abstract: Research on fairness in machine learning has been mainly framed in the
context of classification tasks, leaving critical gaps in regression. In this
paper, we propose a seminal approach to measure intersectional fairness in
regression tasks, going beyond the focus on single protected attributes from
existing work to consider combinations of all protected attributes.
Furthermore, we contend that it is insufficient to measure the average error of
groups without regard for imbalanced domain preferences. To this end, we
propose Intersectional Divergence (ID) as the first fairness measure for
regression tasks that 1) describes fair model behavior across multiple
protected attributes and 2) differentiates the impact of predictions in target
ranges most relevant to users. We extend our proposal demonstrating how ID can
be adapted into a loss function, IDLoss, and used in optimization problems.
Through an extensive experimental evaluation, we demonstrate how ID allows
unique insights into model behavior and fairness, and how incorporating IDLoss
into optimization can considerably improve single-attribute and intersectional
model fairness while maintaining a competitive balance in predictive
performance.

</details>


### [102] [IberFire -- a detailed creation of a spatio-temporal dataset for wildfire risk assessment in Spain](https://arxiv.org/abs/2505.00837)
*Julen Ercibengoa,Meritxell Gómez-Omella,Izaro Goienetxea*

Main category: cs.LG

TL;DR: 论文介绍了IberFire，一个高分辨率时空数据集，用于西班牙野火预测，整合了260个特征，支持机器学习和深度学习建模。


<details>
  <summary>Details</summary>
Motivation: 解决西班牙缺乏本地化和细粒度野火数据的问题，以支持更准确的预测模型。

Method: 开发IberFire数据集，整合多源开放数据，涵盖8类260个特征，使用开源工具处理。

Result: IberFire提供了比现有欧洲数据集更高的时空粒度和特征多样性，支持野火风险建模和气候分析。

Conclusion: IberFire为野火预测和预防提供了高质量数据集，并公开了代码和方法论，促进开放研究。

Abstract: Wildfires pose a critical environmental issue to ecosystems, economies, and
public safety, particularly in Mediterranean regions such as Spain. Accurate
predictive models rely on high-resolution spatio-temporal data to capture the
complex interplay of environmental and anthropogenic factors. To address the
lack of localised and fine-grained datasets in Spain, this work introduces
IberFire, a spatio-temporal datacube at 1 km x 1 km x 1-day resolution covering
mainland Spain and the Balearic Islands from December 2007 to December 2024.
IberFire integrates 260 features across eight main categories: auxiliary
features, fire history, geography, topography, meteorology, vegetation indices,
human activity, and land cover. All features are derived from open-access
sources, ensuring transparency and real-time applicability. The data processing
pipeline was implemented entirely using open-source tools, and the codebase has
been made publicly available. This work not only enhances spatio-temporal
granularity and feature diversity compared to existing European datacubes but
also provides a reproducible methodology for constructing similar datasets.
IberFire supports advanced wildfire risk modelling through Machine Learning
(ML) and Deep Learning (DL) techniques, enables climate pattern analysis and
informs strategic planning in fire prevention and land management. The dataset
is publicly available on Zenodo to promote open research and collaboration.

</details>


### [103] [ICQuant: Index Coding enables Low-bit LLM Quantization](https://arxiv.org/abs/2505.00850)
*Xinlin Li,Osama Hanna,Christina Fragouli,Suhas Diggavi*

Main category: cs.LG

TL;DR: ICQuant是一种新型的低比特后训练量化框架，通过高效的索引编码方案处理权重中的异常值，显著降低了量化范围，同时减少了比特开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的高内存成本促使需要高效的低比特量化方法，而权重量化中的异常值问题导致量化范围扩大和误差增加。

Method: ICQuant利用异常值统计设计了一种高效的索引编码方案，用于异常值感知的仅权重量化，显著降低了比特开销。

Result: ICQuant仅需约0.3比特的开销即可将量化范围减半，显著优于现有技术。在2-3比特的极端压缩下，其性能优于QTIP和QuIP#，甚至媲美微调量化器。

Conclusion: ICQuant提供了一种高效且无需微调的量化解决方案，显著提升了低比特量化模型的性能。

Abstract: The rapid deployment of Large Language Models (LLMs) highlights the need for
efficient low-bit post-training quantization (PTQ), due to their high memory
costs. A key challenge in weight quantization is the presence of outliers,
which inflate quantization ranges and lead to large errors. While a number of
outlier suppression techniques have been proposed, they either: fail to
effectively shrink the quantization range, or incur (relatively) high bit
overhead. In this paper, we present ICQuant, a novel framework that leverages
outlier statistics to design an efficient index coding scheme for outlier-aware
weight-only quantization. Compared to existing outlier suppression techniques
requiring $\approx 1$ bit overhead to halve the quantization range, ICQuant
requires only $\approx 0.3$ bits; a significant saving in extreme compression
regimes (e.g., 2-3 bits per weight). ICQuant can be used on top of any existing
quantizers to eliminate outliers, improving the quantization quality. Using
just 2.3 bits per weight and simple scalar quantizers, ICQuant improves the
zero-shot accuracy of the 2-bit Llama3-70B model by up to 130% and 150%
relative to QTIP and QuIP#; and it achieves comparable performance to the
best-known fine-tuned quantizer (PV-tuning) without fine-tuning.

</details>


### [104] [Rethinking Time Encoding via Learnable Transformation Functions](https://arxiv.org/abs/2505.00887)
*Xi Chen,Yateng Tang,Jiarong Xu,Jiawei Zhang,Siwei Zhang,Sijia Peng,Xuehao Zheng,Yun Xiong*

Main category: cs.LG

TL;DR: 本文提出了一种名为LeTE的可学习时间编码方法，通过深度学习技术参数化非线性变换，以更灵活地建模复杂时间模式。


<details>
  <summary>Details</summary>
Motivation: 现实场景中时间模式的多样性和复杂性对现有时间编码方法提出了挑战，现有方法往往局限于单一模式建模。

Method: 提出LeTE方法，利用深度学习参数化非线性变换，使其能够学习并建模广义时间模式。

Result: 通过多领域实验验证了LeTE的通用性和有效性。

Conclusion: LeTE不仅涵盖现有方法，还能灵活适应多种任务，显著提升了时间编码的表现。

Abstract: Effectively modeling time information and incorporating it into applications
or models involving chronologically occurring events is crucial. Real-world
scenarios often involve diverse and complex time patterns, which pose
significant challenges for time encoding methods. While previous methods focus
on capturing time patterns, many rely on specific inductive biases, such as
using trigonometric functions to model periodicity. This narrow focus on
single-pattern modeling makes them less effective in handling the diversity and
complexities of real-world time patterns. In this paper, we investigate to
improve the existing commonly used time encoding methods and introduce
Learnable Transformation-based Generalized Time Encoding (LeTE). We propose
using deep function learning techniques to parameterize non-linear
transformations in time encoding, making them learnable and capable of modeling
generalized time patterns, including diverse and complex temporal dynamics. By
enabling learnable transformations, LeTE encompasses previous methods as
specific cases and allows seamless integration into a wide range of tasks.
Through extensive experiments across diverse domains, we demonstrate the
versatility and effectiveness of LeTE.

</details>


### [105] [NeMo-Inspector: A Visualization Tool for LLM Generation Analysis](https://arxiv.org/abs/2505.00903)
*Daria Gitman,Igor Gitman,Evelina Bakhturina*

Main category: cs.LG

TL;DR: NeMo-Inspector是一个开源工具，用于简化合成数据集的分析和清理，显著提升数据质量和模型性能。


<details>
  <summary>Details</summary>
Motivation: 合成数据在缺乏真实数据时是重要替代品，但其质量难以保证，需要高效工具辅助分析。

Method: 开发NeMo-Inspector工具，集成推理能力，用于分析和清理合成数据集。

Result: 使用该工具后，GSM-Plus数据集的低质量样本从46.99%降至19.51%，OpenMath模型的准确率在MATH和GSM8K数据集上分别提升1.92%和4.17%。

Conclusion: NeMo-Inspector能有效提升合成数据质量，进而优化模型性能。

Abstract: Adapting Large Language Models (LLMs) to novel tasks and enhancing their
overall capabilities often requires large, high-quality training datasets.
Synthetic data, generated at scale, serves a valuable alternative when
real-world data is scarce or difficult to obtain. However, ensuring the quality
of synthetic datasets is challenging, as developers must manually inspect and
refine numerous samples to identify errors and areas for improvement. This
process is time-consuming and requires specialized tools. We introduce
NeMo-Inspector, an open-source tool designed to simplify the analysis of
synthetic datasets with integrated inference capabilities. We demonstrate its
effectiveness through two real-world cases. Analysis and cleaning of the
synthetically generated GSM-Plus dataset with NeMo-Inspector led to a
significant decrease in low-quality samples from 46.99% to 19.51%. The tool
also helped identify and correct generation errors in OpenMath models,
improving accuracy by 1.92% on the MATH dataset and by 4.17% on the GSM8K
dataset for a Meta-Llama-3-8B model fine-tuned on synthetic data generated from
Nemotron-4-340B.

</details>


### [106] [Learning Neural Control Barrier Functions from Offline Data with Conservatism](https://arxiv.org/abs/2505.00908)
*Ihab Tabbara,Hussein Sibai*

Main category: cs.LG

TL;DR: 提出一种基于离线数据训练控制屏障函数的算法，用于安全控制，避免系统进入不安全或分布外状态。


<details>
  <summary>Details</summary>
Motivation: 现有安全过滤器合成算法存在维度灾难问题，深度学习为解决这一问题提供了可能。

Method: 受保守Q学习启发，提出保守控制屏障函数（CCBFs）训练算法。

Result: CCBFs在保持安全性和避免分布外状态方面优于现有方法，且对任务性能影响最小。

Conclusion: CCBFs为安全控制提供了一种有效的离线学习方法。

Abstract: Safety filters, particularly those based on control barrier functions, have
gained increased interest as effective tools for safe control of dynamical
systems. Existing correct-by-construction synthesis algorithms, however, suffer
from the curse of dimensionality. Deep learning approaches have been proposed
in recent years to address this challenge. In this paper, we contribute to this
line of work by proposing an algorithm for training control barrier functions
from offline datasets. Our algorithm trains the filter to not only prevent the
system from reaching unsafe states but also out-of-distribution ones, at which
the filter would be unreliable. It is inspired by Conservative Q-learning, an
offline reinforcement learning algorithm. We call its outputs Conservative
Control Barrier Functions (CCBFs). Our empirical results demonstrate that CCBFs
outperform existing methods in maintaining safety and out-of-distribution
avoidance while minimally affecting task performance.

</details>


### [107] [Gaussian Process Policy Iteration with Additive Schwarz Acceleration for Forward and Inverse HJB and Mean Field Game Problems](https://arxiv.org/abs/2505.00909)
*Xianjin Yang,Jingguo Zhang*

Main category: cs.LG

TL;DR: 提出基于高斯过程的策略迭代框架，用于解决HJB方程和MFGs的正反问题，通过Schwarz加速提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决HJB方程和MFGs中的正反问题，并提高策略迭代的计算效率。

Method: 采用高斯过程近似函数，策略迭代分为固定策略下的值函数求解和策略更新，引入Schwarz加速作为预处理步骤。

Result: 数值实验表明Schwarz加速显著提高了计算效率。

Conclusion: 提出的框架有效解决了HJB和MFGs问题，并通过Schwarz加速优化了计算性能。

Abstract: We propose a Gaussian Process (GP)-based policy iteration framework for
addressing both forward and inverse problems in Hamilton--Jacobi--Bellman (HJB)
equations and mean field games (MFGs). Policy iteration is formulated as an
alternating procedure between solving the value function under a fixed control
policy and updating the policy based on the resulting value function. By
exploiting the linear structure of GPs for function approximation, each policy
evaluation step admits an explicit closed-form solution, eliminating the need
for numerical optimization. To improve convergence, we incorporate the additive
Schwarz acceleration as a preconditioning step following each policy update.
Numerical experiments demonstrate the effectiveness of Schwarz acceleration in
improving computational efficiency.

</details>


### [108] [Fine-Tuning without Performance Degradation](https://arxiv.org/abs/2505.00913)
*Han Wang,Adam White,Martha White*

Main category: cs.LG

TL;DR: 论文提出了一种新的微调算法Jump Start，通过逐步增加探索来减少性能下降，并在多种设置中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 离线学习策略在微调时通常会出现性能下降或学习缓慢的问题，现有方法难以解决。

Method: 基于Jump Start算法，提出了一种逐步增加探索的微调方法，利用在线性能估计调整探索程度。

Result: 新算法在微调过程中显著减少了性能下降，并实现了快速学习。

Conclusion: Jump Start算法在离线到在线学习场景中表现优于现有方法，解决了性能下降和慢学习问题。

Abstract: Fine-tuning policies learned offline remains a major challenge in application
domains. Monotonic performance improvement during \emph{fine-tuning} is often
challenging, as agents typically experience performance degradation at the
early fine-tuning stage. The community has identified multiple difficulties in
fine-tuning a learned network online, however, the majority of progress has
focused on improving learning efficiency during fine-tuning. In practice, this
comes at a serious cost during fine-tuning: initially, agent performance
degrades as the agent explores and effectively overrides the policy learned
offline. We show across a range of settings, many offline-to-online algorithms
exhibit either (1) performance degradation or (2) slow learning (sometimes
effectively no improvement) during fine-tuning. We introduce a new fine-tuning
algorithm, based on an algorithm called Jump Start, that gradually allows more
exploration based on online estimates of performance. Empirically, this
approach achieves fast fine-tuning and significantly reduces performance
degradations compared with existing algorithms designed to do the same.

</details>


### [109] [How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias](https://arxiv.org/abs/2505.00926)
*Ruiquan Huang,Yingbin Liang,Jing Yang*

Main category: cs.LG

TL;DR: 论文研究了单层Transformer在两种正则语言识别任务（'even pairs'和'parity check'）中的学习动态，分析了其梯度下降训练过程。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer如何通过理论分析学习解决正则语言识别任务，揭示其训练机制。

Method: 使用单层Transformer（注意力层加线性层）进行训练，分析梯度下降下的动态过程。

Result: 训练分为两个阶段：注意力层快速分离数据，线性层对数增长并接近最大间隔超平面，损失以$O(1/t)$下降。

Conclusion: 实验验证了理论分析，表明单层Transformer能有效学习正则语言任务，且CoT可辅助解决更复杂任务。

Abstract: Language recognition tasks are fundamental in natural language processing
(NLP) and have been widely used to benchmark the performance of large language
models (LLMs). These tasks also play a crucial role in explaining the working
mechanisms of transformers. In this work, we focus on two representative tasks
in the category of regular language recognition, known as `even pairs' and
`parity check', the aim of which is to determine whether the occurrences of
certain subsequences in a given sequence are even. Our goal is to explore how a
one-layer transformer, consisting of an attention layer followed by a linear
layer, learns to solve these tasks by theoretically analyzing its training
dynamics under gradient descent. While even pairs can be solved directly by a
one-layer transformer, parity check need to be solved by integrating
Chain-of-Thought (CoT), either into the inference stage of a transformer
well-trained for the even pairs task, or into the training of a one-layer
transformer. For both problems, our analysis shows that the joint training of
attention and linear layers exhibits two distinct phases. In the first phase,
the attention layer grows rapidly, mapping data sequences into separable
vectors. In the second phase, the attention layer becomes stable, while the
linear layer grows logarithmically and approaches in direction to a max-margin
hyperplane that correctly separates the attention layer outputs into positive
and negative samples, and the loss decreases at a rate of $O(1/t)$. Our
experiments validate those theoretical results.

</details>


### [110] [Compact Recurrent Transformer with Persistent Memory](https://arxiv.org/abs/2505.00929)
*Edison Mucllari,Zachary Daniels,David Zhang,Qiang Ye*

Main category: cs.LG

TL;DR: 提出了一种高效的紧凑循环Transformer（CRT），结合浅层Transformer和循环神经网络，解决了长序列处理中的计算效率问题。


<details>
  <summary>Details</summary>
Motivation: Transformer在处理长序列时因自注意力计算的二次复杂度面临效率问题，现有方法虽能扩展但引入额外计算开销，限制了在资源受限场景（如边缘计算）的应用。

Method: CRT结合浅层Transformer处理短局部片段和循环神经网络，通过单一持久记忆向量压缩和管理长距离全局信息。

Result: 在语言数据集（WordPTB和WikiText-103）和丰田Smarthome视频数据集上，CRT实现了与完整Transformer相当或更优的性能，同时显著减少计算量和片段长度。

Conclusion: CRT是一种高效的长序列处理方法，适用于资源受限场景，并在多个任务中表现出色。

Abstract: The Transformer architecture has shown significant success in many language
processing and visual tasks. However, the method faces challenges in
efficiently scaling to long sequences because the self-attention computation is
quadratic with respect to the input length. To overcome this limitation,
several approaches scale to longer sequences by breaking long sequences into a
series of segments, restricting self-attention to local dependencies between
tokens within each segment and using a memory mechanism to manage information
flow between segments. However, these approached generally introduce additional
compute overhead that restricts them from being used for applications where
limited compute memory and power are of great concern (such as edge computing).
We propose a novel and efficient Compact Recurrent Transformer (CRT), which
combines shallow Transformer models that process short local segments with
recurrent neural networks to compress and manage a single persistent memory
vector that summarizes long-range global information between segments. We
evaluate CRT on WordPTB and WikiText-103 for next-token-prediction tasks, as
well as on the Toyota Smarthome video dataset for classification. CRT achieves
comparable or superior prediction results to full-length Transformers in the
language datasets while using significantly shorter segments (half or quarter
size) and substantially reduced FLOPs. Our approach also demonstrates
state-of-the-art performance on the Toyota Smarthome video dataset.

</details>


### [111] [Robust Root Cause Diagnosis using In-Distribution Interventions](https://arxiv.org/abs/2505.00930)
*Lokesh Nagalapatti,Ashutosh Srivastava,Sunita Sarawagi,Amit Sharma*

Main category: cs.LG

TL;DR: 论文提出了一种名为IDI的新算法，通过满足异常和修复两个条件来诊断复杂系统中的根因节点，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在复杂系统中诊断异常根因是一个紧迫问题，现有方法因异常罕见且超出训练分布而不可靠。

Method: IDI算法通过仅探测拟合SCM的分布内输入来获取干预估计，避免不可靠的反事实估计。

Result: 实验表明，IDI在合成和PetShop数据集上比九种现有方法更准确、稳健地识别根因。

Conclusion: IDI通过干预估计有效解决了异常诊断问题，优于传统反事实方法。

Abstract: Diagnosing the root cause of an anomaly in a complex interconnected system is
a pressing problem in today's cloud services and industrial operations. We
propose In-Distribution Interventions (IDI), a novel algorithm that predicts
root cause as nodes that meet two criteria: 1) **Anomaly:** root cause nodes
should take on anomalous values; 2) **Fix:** had the root cause nodes assumed
usual values, the target node would not have been anomalous. Prior methods of
assessing the fix condition rely on counterfactuals inferred from a Structural
Causal Model (SCM) trained on historical data. But since anomalies are rare and
fall outside the training distribution, the fitted SCMs yield unreliable
counterfactual estimates. IDI overcomes this by relying on interventional
estimates obtained by solely probing the fitted SCM at in-distribution inputs.
We present a theoretical analysis comparing and bounding the errors in
assessing the fix condition using interventional and counterfactual estimates.
We then conduct experiments by systematically varying the SCM's complexity to
demonstrate the cases where IDI's interventional approach outperforms the
counterfactual approach and vice versa. Experiments on both synthetic and
PetShop RCD benchmark datasets demonstrate that \our\ consistently identifies
true root causes more accurately and robustly than nine existing
state-of-the-art RCD baselines. Code is released at
https://github.com/nlokeshiisc/IDI_release.

</details>


### [112] [A Self-Supervised Transformer for Unusable Shared Bike Detection](https://arxiv.org/abs/2505.00932)
*Yin Huang,Yongqi Dong,Youhua Tang,Alvaro García Hernandez*

Main category: cs.LG

TL;DR: 提出了一种基于自监督Transformer（SSTransformer）的框架，用于自动检测共享单车故障，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 共享单车系统（BSS）的快速扩展带来了运营挑战，尤其是故障检测问题。现有方法存在动态时空模式忽略或标签稀缺等问题。

Method: 采用自监督预训练策略增强特征提取能力，随后微调进行状态识别。Transformer编码器通过自监督目标学习单车运动的通用表示。

Result: 在成都的真实数据集上，SSTransformer在准确率（97.81%）、精确度（0.8889）和F1分数（0.9358）上表现最佳。

Conclusion: 自监督Transformer在时空数据上能有效捕捉复杂异常，为共享出行提供更可靠的维护解决方案。

Abstract: The rapid expansion of bike-sharing systems (BSS) has greatly improved urban
"last-mile" connectivity, yet large-scale deployments face escalating
operational challenges, particularly in detecting faulty bikes. Existing
detection approaches either rely on static model-based thresholds that overlook
dynamic spatiotemporal (ST) usage patterns or employ supervised learning
methods that struggle with label scarcity and class imbalance. To address these
limitations, this paper proposes a novel Self-Supervised Transformer
(SSTransformer) framework for automatically detecting unusable shared bikes,
leveraging ST features extracted from GPS trajectories and trip records. The
model incorporates a self-supervised pre-training strategy to enhance its
feature extraction capabilities, followed by fine-tuning for efficient status
recognition. In the pre-training phase, the Transformer encoder learns
generalized representations of bike movement via a self-supervised objective;
in the fine-tuning phase, the encoder is adapted to a downstream binary
classification task. Comprehensive experiments on a real-world dataset of
10,730 bikes (1,870 unusable, 8,860 normal) from Chengdu, China, demonstrate
that SSTransformer significantly outperforms traditional machine learning,
ensemble learning, and deep learning baselines, achieving the best accuracy
(97.81%), precision (0.8889), and F1-score (0.9358). This work highlights the
effectiveness of self-supervised Transformer on ST data for capturing complex
anomalies in BSS, paving the way toward more reliable and scalable maintenance
solutions for shared mobility.

</details>


### [113] [TunnElQNN: A Hybrid Quantum-classical Neural Network for Efficient Learning](https://arxiv.org/abs/2505.00933)
*A. H. Abbas*

Main category: cs.LG

TL;DR: 论文提出了一种名为TunnElQNN的非顺序混合量子-经典神经网络架构，采用隧道二极管激活函数（TDAF），在合成数据集上表现优于传统ReLUQNN模型。


<details>
  <summary>Details</summary>
Motivation: 结合量子与经典模型的优势，探索物理启发的激活函数在混合架构中的潜力。

Method: 开发TunnElQNN架构，交替使用经典和量子层，经典部分采用TDAF激活函数，并在多类分类任务中评估性能。

Result: TunnElQNN在合成数据集上表现优于ReLUQNN，且在不同类别重叠情况下生成更优的决策边界。

Conclusion: 物理启发的激活函数与量子组件结合可提升混合量子-经典机器学习架构的表达能力和鲁棒性。

Abstract: Hybrid quantum-classical neural networks (HQCNNs) represent a promising
frontier in machine learning, leveraging the complementary strengths of both
models. In this work, we propose the development of TunnElQNN, a non-sequential
architecture composed of alternating classical and quantum layers. Within the
classical component, we employ the Tunnelling Diode Activation Function (TDAF),
inspired by the I-V characteristics of quantum tunnelling. We evaluate the
performance of this hybrid model on a synthetic dataset of interleaving
half-circle for multi-class classification tasks with varying degrees of class
overlap. The model is compared against a baseline hybrid architecture that uses
the conventional ReLU activation function (ReLUQNN). Our results show that the
TunnElQNN model consistently outperforms the ReLUQNN counterpart. Furthermore,
we analyse the decision boundaries generated by TunnElQNN under different
levels of class overlap and compare them to those produced by a neural network
implementing TDAF within a fully classical architecture. These findings
highlight the potential of integrating physics-inspired activation functions
with quantum components to enhance the expressiveness and robustness of hybrid
quantum-classical machine learning architectures.

</details>


### [114] [StablePCA: Learning Shared Representations across Multiple Sources via Minimax Optimization](https://arxiv.org/abs/2505.00940)
*Zhenyu Wang,Molei Liu,Jing Lei,Francis Bach,Zijian Guo*

Main category: cs.LG

TL;DR: 提出了一种名为StablePCA的新方法，用于从多源高维数据中学习稳健的低维表示，解决了多源PCA的非凸优化问题。


<details>
  <summary>Details</summary>
Motivation: 提取跨多源数据的低维特征表示，以促进知识迁移、减少批次效应并提升公平性。

Method: 采用Fantope松弛将非凸问题转化为凸极小极大优化，并设计了乐观梯度Mirror Prox算法求解。

Result: 理论证明了算法的全局收敛性，实验验证了StablePCA在提取稳健低维表示方面的高效性和准确性。

Conclusion: StablePCA在多源数据中提取稳健低维表示方面表现出色，具有理论和实践优势。

Abstract: When synthesizing multisource high-dimensional data, a key objective is to
extract low-dimensional feature representations that effectively approximate
the original features across different sources. Such general feature extraction
facilitates the discovery of transferable knowledge, mitigates systematic
biases such as batch effects, and promotes fairness. In this paper, we propose
Stable Principal Component Analysis (StablePCA), a novel method for group
distributionally robust learning of latent representations from
high-dimensional multi-source data. A primary challenge in generalizing PCA to
the multi-source regime lies in the nonconvexity of the fixed rank constraint,
rendering the minimax optimization nonconvex. To address this challenge, we
employ the Fantope relaxation, reformulating the problem as a convex minimax
optimization, with the objective defined as the maximum loss across sources. To
solve the relaxed formulation, we devise an optimistic-gradient Mirror Prox
algorithm with explicit closed-form updates. Theoretically, we establish the
global convergence of the Mirror Prox algorithm, with the convergence rate
provided from the optimization perspective. Furthermore, we offer practical
criteria to assess how closely the solution approximates the original nonconvex
formulation. Through extensive numerical experiments, we demonstrate
StablePCA's high accuracy and efficiency in extracting robust low-dimensional
representations across various finite-sample scenarios.

</details>


### [115] [FreCT: Frequency-augmented Convolutional Transformer for Robust Time Series Anomaly Detection](https://arxiv.org/abs/2505.00941)
*Wenxin Zhang,Ding Xu,Guangzhen Yao,Xiaojian Lin,Renxiang Guan,Chengze Du,Renda Han,Xi Xuan,Cuicui Luo*

Main category: cs.LG

TL;DR: 提出了一种基于频率增强的卷积Transformer（FreCT），用于解决时间序列异常检测中重建方法的局限性，通过结合时间域和频率域信息提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有重建方法因异常导致的计算偏差和忽略频率信息而表现不佳，需要一种能同时捕捉时间依赖和频率特征的方法。

Method: FreCT采用补丁操作生成对比视图，结合改进的Transformer和卷积模块，并引入傅里叶变换进行频率分析，使用KL散度和绝对误差优化训练。

Result: 在四个公开数据集上，FreCT优于现有方法。

Conclusion: FreCT通过多域信息融合和优化策略，显著提升了时间序列异常检测的准确性和鲁棒性。

Abstract: Time series anomaly detection is critical for system monitoring and risk
identification, across various domains, such as finance and healthcare.
However, for most reconstruction-based approaches, detecting anomalies remains
a challenge due to the complexity of sequential patterns in time series data.
On the one hand, reconstruction-based techniques are susceptible to
computational deviation stemming from anomalies, which can lead to impure
representations of normal sequence patterns. On the other hand, they often
focus on the time-domain dependencies of time series, while ignoring the
alignment of frequency information beyond the time domain. To address these
challenges, we propose a novel Frequency-augmented Convolutional Transformer
(FreCT). FreCT utilizes patch operations to generate contrastive views and
employs an improved Transformer architecture integrated with a convolution
module to capture long-term dependencies while preserving local topology
information. The introduced frequency analysis based on Fourier transformation
could enhance the model's ability to capture crucial characteristics beyond the
time domain. To protect the training quality from anomalies and improve the
robustness, FreCT deploys stop-gradient Kullback-Leibler (KL) divergence and
absolute error to optimize consistency information in both time and frequency
domains. Extensive experiments on four public datasets demonstrate that FreCT
outperforms existing methods in identifying anomalies.

</details>


### [116] [Addressing Noise and Stochasticity in Fraud Detection for Service Networks](https://arxiv.org/abs/2505.00946)
*Wenxin Zhang,Ding Xu,Xi Xuan,Lei Jiang,Guangzhen Yao,Renda Han,Xiangxiang Lang,Cuicui Luo*

Main category: cs.LG

TL;DR: SGNN-IB是一种基于信息瓶颈理论的谱图网络，用于服务网络中的欺诈检测，通过分离同质和异质子图并引入原型学习，解决了现有方法在信号提取和融合中的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于图滤波的方法在提取干净且具有区分性的图信号方面存在不足，忽略了信息传播中的噪声和信号频率特性。

Method: SGNN-IB将原始图分为同质和异质子图，应用信息瓶颈理论提取关键特征，并通过原型学习实现信号融合。

Result: 在三个真实数据集上的实验表明，SGNN-IB优于现有欺诈检测方法。

Conclusion: SGNN-IB通过改进信号提取和融合，显著提升了欺诈检测性能。

Abstract: Fraud detection is crucial in social service networks to maintain user trust
and improve service network security. Existing spectral graph-based methods
address this challenge by leveraging different graph filters to capture signals
with different frequencies in service networks. However, most graph
filter-based methods struggle with deriving clean and discriminative graph
signals. On the one hand, they overlook the noise in the information
propagation process, resulting in degradation of filtering ability. On the
other hand, they fail to discriminate the frequency-specific characteristics of
graph signals, leading to distortion of signals fusion. To address these
issues, we develop a novel spectral graph network based on information
bottleneck theory (SGNN-IB) for fraud detection in service networks. SGNN-IB
splits the original graph into homophilic and heterophilic subgraphs to better
capture the signals at different frequencies. For the first limitation, SGNN-IB
applies information bottleneck theory to extract key characteristics of encoded
representations. For the second limitation, SGNN-IB introduces prototype
learning to implement signal fusion, preserving the frequency-specific
characteristics of signals. Extensive experiments on three real-world datasets
demonstrate that SGNN-IB outperforms state-of-the-art fraud detection methods.

</details>


### [117] [Adaptive Branch-and-Bound Tree Exploration for Neural Network Verification](https://arxiv.org/abs/2505.00963)
*Kota Fukuda,Guanqin Zhang,Zhenya Zhang,Yulei Sui,Jianjun Zhao*

Main category: cs.LG

TL;DR: ABONN是一种基于蒙特卡洛树搜索的自适应分支定界方法，通过动态评估子问题的重要性提升神经网络形式化验证的效率。


<details>
  <summary>Details</summary>
Motivation: 现有分支定界方法在探索子问题时效率低下，忽略了子问题的重要性差异。

Method: 提出子问题“重要性”概念，并设计ABONN方法，以蒙特卡洛树搜索风格自适应探索子问题空间。

Result: 在MNIST和CIFAR-10数据集上分别实现15.2倍和24.7倍的加速。

Conclusion: ABONN通过动态优化子问题探索顺序，显著提升了形式化验证的效率。

Abstract: Formal verification is a rigorous approach that can provably ensure the
quality of neural networks, and to date, Branch and Bound (BaB) is the
state-of-the-art that performs verification by splitting the problem as needed
and applying off-the-shelf verifiers to sub-problems for improved performance.
However, existing BaB may not be efficient, due to its naive way of exploring
the space of sub-problems that ignores the \emph{importance} of different
sub-problems. To bridge this gap, we first introduce a notion of ``importance''
that reflects how likely a counterexample can be found with a sub-problem, and
then we devise a novel verification approach, called ABONN, that explores the
sub-problem space of BaB adaptively, in a Monte-Carlo tree search (MCTS) style.
The exploration is guided by the ``importance'' of different sub-problems, so
it favors the sub-problems that are more likely to find counterexamples. As
soon as it finds a counterexample, it can immediately terminate; even though it
cannot find, after visiting all the sub-problems, it can still manage to verify
the problem. We evaluate ABONN with 552 verification problems from
commonly-used datasets and neural network models, and compare it with the
state-of-the-art verifiers as baseline approaches. Experimental evaluation
shows that ABONN demonstrates speedups of up to $15.2\times$ on MNIST and
$24.7\times$ on CIFAR-10. We further study the influences of hyperparameters to
the performance of ABONN, and the effectiveness of our adaptive tree
exploration.

</details>


### [118] [Tree-Sliced Wasserstein Distance with Nonlinear Projection](https://arxiv.org/abs/2505.00968)
*Thanh Tran,Viet-Hoang Tran,Thanh Chu,Trang Pham,Laurent El Ghaoui,Tam Le,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 本文提出了一种基于树切片Wasserstein（TSW）距离的非线性投影框架，取代了传统线性投影，同时保持了Radon变换的单射性和度量定义的合理性。该方法在欧几里得空间和球面上构建了高效度量，并在梯度流、自监督学习和生成模型中表现出优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统切片Wasserstein（SW）距离使用一维线作为投影空间，而树切片方法通过树结构捕捉拓扑结构，但仍有改进空间。本文旨在通过非线性投影进一步提升TSW的性能。

Method: 提出非线性投影框架，替代TSW中的线性投影，确保Radon变换的单射性和度量定义的合理性。设计了适用于欧几里得空间和球面的投影方法。

Result: 通过数值实验验证了所提方法在欧几里得和球面数据集上的有效性，并在梯度流、自监督学习和生成模型中优于现有SW和TSW变体。

Conclusion: 非线性投影框架显著提升了TSW的性能，为高维数据分析和机器学习任务提供了更高效的度量工具。

Abstract: Tree-Sliced methods have recently emerged as an alternative to the
traditional Sliced Wasserstein (SW) distance, replacing one-dimensional lines
with tree-based metric spaces and incorporating a splitting mechanism for
projecting measures. This approach enhances the ability to capture the
topological structures of integration domains in Sliced Optimal Transport while
maintaining low computational costs. Building on this foundation, we propose a
novel nonlinear projectional framework for the Tree-Sliced Wasserstein (TSW)
distance, substituting the linear projections in earlier versions with general
projections, while ensuring the injectivity of the associated Radon Transform
and preserving the well-definedness of the resulting metric. By designing
appropriate projections, we construct efficient metrics for measures on both
Euclidean spaces and spheres. Finally, we validate our proposed metric through
extensive numerical experiments for Euclidean and spherical datasets.
Applications include gradient flows, self-supervised learning, and generative
models, where our methods demonstrate significant improvements over recent SW
and TSW variants.

</details>


### [119] [A Minimax-MDP Framework with Future-imposed Conditions for Learning-augmented Problems](https://arxiv.org/abs/2505.00973)
*Xin Chen,Yuze Chen,Yuan Zhou*

Main category: cs.LG

TL;DR: 论文研究了一种基于增强预测的序列决策问题，提出了一种最小最大马尔可夫决策过程框架，用于设计鲁棒性强的在线决策策略。


<details>
  <summary>Details</summary>
Motivation: 解决在预测不确定性下的在线决策问题，确保决策在所有可能的参数和预测实现中具有竞争力。

Method: 提出最小最大MDP框架，结合环境状态和决策者内部状态，引入未来条件以设计高效策略。

Result: 通过三个应用验证了框架的有效性，包括库存管理和资源分配问题。

Conclusion: 该框架为预测不确定性下的鲁棒在线决策提供了实用且灵活的方法。

Abstract: We study a class of sequential decision-making problems with augmented
predictions, potentially provided by a machine learning algorithm. In this
setting, the decision-maker receives prediction intervals for unknown
parameters that become progressively refined over time, and seeks decisions
that are competitive with the hindsight optimal under all possible realizations
of both parameters and predictions. We propose a minimax Markov Decision
Process (minimax-MDP) framework, where the system state consists of an
adversarially evolving environment state and an internal state controlled by
the decision-maker. We introduce a set of future-imposed conditions that
characterize the feasibility of minimax-MDPs and enable the design of
efficient, often closed-form, robustly competitive policies. We illustrate the
framework through three applications: multi-period inventory ordering with
refining demand predictions, resource allocation with uncertain utility
functions, and a multi-phase extension of the minimax-MDP applied to the
inventory problem with time-varying ordering costs. Our results provide a
tractable and versatile approach to robust online decision-making under
predictive uncertainty.

</details>


### [120] [Towards the Resistance of Neural Network Watermarking to Fine-tuning](https://arxiv.org/abs/2505.01007)
*Ling Tang,Yuefeng Chen,Hui Xue,Quanshi Zhang*

Main category: cs.LG

TL;DR: 提出一种新的水印方法，将所有权信息嵌入深度神经网络（DNN），且对微调具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决现有水印方法在微调过程中易被破坏的问题，确保所有权信息的持久性。

Method: 利用修正的傅里叶变换提取卷积滤波器的特定频率成分，设计水印模块将信息编码到这些成分中。

Result: 实验证明该方法在微调、权重缩放和置换下仍能保持水印信息。

Conclusion: 该方法为DNN所有权保护提供了一种鲁棒且有效的解决方案。

Abstract: This paper proves a new watermarking method to embed the ownership
information into a deep neural network (DNN), which is robust to fine-tuning.
Specifically, we prove that when the input feature of a convolutional layer
only contains low-frequency components, specific frequency components of the
convolutional filter will not be changed by gradient descent during the
fine-tuning process, where we propose a revised Fourier transform to extract
frequency components from the convolutional filter. Additionally, we also prove
that these frequency components are equivariant to weight scaling and weight
permutations. In this way, we design a watermark module to encode the watermark
information to specific frequency components in a convolutional filter.
Preliminary experiments demonstrate the effectiveness of our method.

</details>


### [121] [Accelerating Deep Neural Network Training via Distributed Hybrid Order Optimization](https://arxiv.org/abs/2505.00982)
*Shunxian Gu,Chaoqun You,Bangbang Ren,Lailong Luo,Junxu Xia,Deke Guo*

Main category: cs.LG

TL;DR: FOSI是一种混合顺序优化器，通过结合梯度和曲率信息加速DNN训练。本文提出分布式设计DHO$_2$，降低内存负担并实现训练时间加速。


<details>
  <summary>Details</summary>
Motivation: 资源受限环境下，传统分布式DNN训练方法不适用，需更高效的优化器设计。

Method: 提出DHO$_2$，分布式计算曲率信息并并行化模型更新，降低内存负担。

Result: 实验显示内存负担随设备数线性减少，训练时间加速1.4×∼2.1×。

Conclusion: DHO$_2$为资源受限环境提供高效分布式DNN训练方案。

Abstract: Scaling deep neural network (DNN) training to more devices can reduce
time-to-solution. However, it is impractical for users with limited computing
resources. FOSI, as a hybrid order optimizer, converges faster than
conventional optimizers by taking advantage of both gradient information and
curvature information when updating the DNN model. Therefore, it provides a new
chance for accelerating DNN training in the resource-constrained setting. In
this paper, we explore its distributed design, namely DHO$_2$, including
distributed calculation of curvature information and model update with partial
curvature information to accelerate DNN training with a low memory burden. To
further reduce the training time, we design a novel strategy to parallelize the
calculation of curvature information and the model update on different devices.
Experimentally, our distributed design can achieve an approximate linear
reduction of memory burden on each device with the increase of the device
number. Meanwhile, it achieves $1.4\times\sim2.1\times$ speedup in the total
training time compared with other distributed designs based on conventional
first- and second-order optimizers.

</details>


### [122] [Toward Data-centric Directed Graph Learning: An Entropy-driven Approach](https://arxiv.org/abs/2505.00983)
*Xunkai Li,Zhengyu Wu,Kaichi Yu,Hongchao Qin,Guang Zeng,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为EDEN的数据中心有向图学习范式，通过层次编码理论和知识蒸馏提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有DiGNNs未能充分利用有向图中隐藏的数据知识，导致预测性能不佳，需从数据角度探索边与节点间的潜在关联。

Method: EDEN利用有向结构测量构建层次知识树（HKT），并通过节点间的互信息细化知识流，实现数据中心的知识蒸馏。

Result: 在14个数据集和4个下游任务中，EDEN表现出SOTA性能，显著提升了现有DiGNNs的表现。

Conclusion: EDEN作为一种通用框架，不仅适用于有向图，还能扩展到无向图，具有广泛的应用潜力。

Abstract: The directed graph (digraph), as a generalization of undirected graphs,
exhibits superior representation capability in modeling complex topology
systems and has garnered considerable attention in recent years. Despite the
notable efforts made by existing DiGraph Neural Networks (DiGNNs) to leverage
directed edges, they still fail to comprehensively delve into the abundant data
knowledge concealed in the digraphs. This data-level limitation results in
model-level sub-optimal predictive performance and underscores the necessity of
further exploring the potential correlations between the directed edges
(topology) and node profiles (feature and labels) from a data-centric
perspective, thereby empowering model-centric neural networks with stronger
encoding capabilities.
  In this paper, we propose \textbf{E}ntropy-driven \textbf{D}igraph
knowl\textbf{E}dge distillatio\textbf{N} (EDEN), which can serve as a
data-centric digraph learning paradigm or a model-agnostic hot-and-plug
data-centric Knowledge Distillation (KD) module. The core idea is to achieve
data-centric ML, guided by our proposed hierarchical encoding theory for
structured data. Specifically, EDEN first utilizes directed structural
measurements from a topology perspective to construct a coarse-grained
Hierarchical Knowledge Tree (HKT). Subsequently, EDEN quantifies the mutual
information of node profiles to refine knowledge flow in the HKT, enabling
data-centric KD supervision within model training. As a general framework, EDEN
can also naturally extend to undirected scenarios and demonstrate satisfactory
performance. In our experiments, EDEN has been widely evaluated on 14 (di)graph
datasets (homophily and heterophily) and across 4 downstream tasks. The results
demonstrate that EDEN attains SOTA performance and exhibits strong improvement
for prevalent (Di)GNNs.

</details>


### [123] [Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii](https://arxiv.org/abs/2505.01372)
*Kola Ayonrinde,Louis Jaburi*

Main category: cs.LG

TL;DR: 论文提出了一种基于哲学科学的多元解释框架（EVF），用于评估和改进机械可解释性（MI）中的解释方法，并指出紧凑证明是一种有前景的方法。


<details>
  <summary>Details</summary>
Motivation: 当前MI领域缺乏统一的解释评估方法，限制了进展。

Method: 引入基于贝叶斯、库恩、德意志和规范四种哲学视角的多元解释框架（EVF）。

Result: 紧凑证明方法因其综合考虑多种解释优势而表现突出。

Conclusion: 改进MI方法有助于更好地监控、预测和引导AI系统。

Abstract: Mechanistic Interpretability (MI) aims to understand neural networks through
causal explanations. Though MI has many explanation-generating methods,
progress has been limited by the lack of a universal approach to evaluating
explanations. Here we analyse the fundamental question "What makes a good
explanation?" We introduce a pluralist Explanatory Virtues Framework drawing on
four perspectives from the Philosophy of Science - the Bayesian, Kuhnian,
Deutschian, and Nomological - to systematically evaluate and improve
explanations in MI. We find that Compact Proofs consider many explanatory
virtues and are hence a promising approach. Fruitful research directions
implied by our framework include (1) clearly defining explanatory simplicity,
(2) focusing on unifying explanations and (3) deriving universal principles for
neural networks. Improved MI methods enhance our ability to monitor, predict,
and steer AI systems.

</details>


### [124] [On-demand Test-time Adaptation for Edge Devices](https://arxiv.org/abs/2505.00986)
*Xiao Ma,Young D. Kwon,Dong Ma*

Main category: cs.LG

TL;DR: 论文提出了一种按需测试时适应（OD-TTA）框架，通过检测显著域偏移触发适应，显著降低计算开销，适用于资源受限的边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有持续测试时适应（CTTA）方法在边缘设备上因内存和能耗问题实用性差，需要更高效的解决方案。

Method: OD-TTA包括轻量级域偏移检测、源域选择模块和解耦批量归一化更新方案，以减少计算和内存开销。

Result: 实验表明，OD-TTA在保持高性能的同时显著降低了能耗和计算开销。

Conclusion: OD-TTA为测试时适应在边缘设备上的实际应用提供了可行方案。

Abstract: Continual Test-time adaptation (CTTA) continuously adapts the deployed model
on every incoming batch of data. While achieving optimal accuracy, existing
CTTA approaches present poor real-world applicability on resource-constrained
edge devices, due to the substantial memory overhead and energy consumption. In
this work, we first introduce a novel paradigm -- on-demand TTA -- which
triggers adaptation only when a significant domain shift is detected. Then, we
present OD-TTA, an on-demand TTA framework for accurate and efficient
adaptation on edge devices. OD-TTA comprises three innovative techniques: 1) a
lightweight domain shift detection mechanism to activate TTA only when it is
needed, drastically reducing the overall computation overhead, 2) a source
domain selection module that chooses an appropriate source model for
adaptation, ensuring high and robust accuracy, 3) a decoupled Batch
Normalization (BN) update scheme to enable memory-efficient adaptation with
small batch sizes. Extensive experiments show that OD-TTA achieves comparable
and even better performance while reducing the energy and computation overhead
remarkably, making TTA a practical reality.

</details>


### [125] [Where's the liability in the Generative Era? Recovery-based Black-Box Detection of AI-Generated Content](https://arxiv.org/abs/2505.01008)
*Haoyue Bai,Yiyou Sun,Wei Cheng,Haifeng Chen*

Main category: cs.LG

TL;DR: 提出了一种无需模型权重或大量辅助数据集的生成图像检测框架，通过掩码恢复策略实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 生成模型生成的逼真图像可能被滥用，现有检测方法依赖模型权重或大量数据集，限制了实际应用。

Method: 采用黑盒检测框架，通过掩码和恢复策略评估图像是否由模型生成，并引入替代模型增强检测能力。

Result: 在八个扩散模型变体数据集上，平均精度比基线方法高出4.31%。

Conclusion: 该框架为生成图像检测提供了一种高效且可扩展的解决方案。

Abstract: The recent proliferation of photorealistic images created by generative
models has sparked both excitement and concern, as these images are
increasingly indistinguishable from real ones to the human eye. While offering
new creative and commercial possibilities, the potential for misuse, such as in
misinformation and fraud, highlights the need for effective detection methods.
Current detection approaches often rely on access to model weights or require
extensive collections of real image datasets, limiting their scalability and
practical application in real world scenarios. In this work, we introduce a
novel black box detection framework that requires only API access, sidestepping
the need for model weights or large auxiliary datasets. Our approach leverages
a corrupt and recover strategy: by masking part of an image and assessing the
model ability to reconstruct it, we measure the likelihood that the image was
generated by the model itself. For black-box models that do not support masked
image inputs, we incorporate a cost efficient surrogate model trained to align
with the target model distribution, enhancing detection capability. Our
framework demonstrates strong performance, outperforming baseline methods by
4.31% in mean average precision across eight diffusion model variant datasets.

</details>


### [126] [Stagnation in Evolutionary Algorithms: Convergence $\neq$ Optimality](https://arxiv.org/abs/2505.01036)
*Xiaojun Zhou*

Main category: cs.LG

TL;DR: 研究发现，进化算法中的个体停滞可能促进整个种群的收敛，且收敛并不一定意味着最优性。


<details>
  <summary>Details</summary>
Motivation: 挑战进化计算领域普遍认为的停滞阻碍收敛和收敛即最优的观点。

Method: 通过理论分析和提供反例来说明个体停滞对种群收敛的促进作用以及收敛的局限性。

Result: 个体停滞可能促进种群收敛，且收敛不能保证最优性。

Conclusion: 收敛本身不足以确保进化算法的有效性，需重新评估停滞和收敛的作用。

Abstract: In the evolutionary computation community, it is widely believed that
stagnation impedes convergence in evolutionary algorithms, and that convergence
inherently indicates optimality. However, this perspective is misleading. In
this study, it is the first to highlight that the stagnation of an individual
can actually facilitate the convergence of the entire population, and
convergence does not necessarily imply optimality, not even local optimality.
Convergence alone is insufficient to ensure the effectiveness of evolutionary
algorithms. Several counterexamples are provided to illustrate this argument.

</details>


### [127] [Global Optimality of Single-Timescale Actor-Critic under Continuous State-Action Space: A Study on Linear Quadratic Regulator](https://arxiv.org/abs/2505.01041)
*Xuyang Chen,Jingliang Duan,Lin Zhao*

Main category: cs.LG

TL;DR: 本文研究了单样本单时间尺度的actor-critic方法在连续状态-动作空间中的性能，证明了其在LQR问题中可以达到ε-最优解，样本复杂度为ε的-2次方。


<details>
  <summary>Details</summary>
Motivation: 尽管actor-critic方法在实际任务中表现出色，但对其理论性能的理解仍有限。现有研究多关注不常见的变体，本文旨在填补这一空白，研究经典单时间尺度方法在连续空间中的表现。

Method: 以线性二次调节器（LQR）问题为例，分析单样本单时间尺度actor-critic算法在连续状态-动作空间中的性能。

Result: 证明该方法可以达到ε-最优解，样本复杂度为ε的-2次方。

Conclusion: 本文为单时间尺度actor-critic方法的性能提供了新见解，进一步缩小了理论与实践的差距。

Abstract: Actor-critic methods have achieved state-of-the-art performance in various
challenging tasks. However, theoretical understandings of their performance
remain elusive and challenging. Existing studies mostly focus on practically
uncommon variants such as double-loop or two-timescale stepsize actor-critic
algorithms for simplicity. These results certify local convergence on finite
state- or action-space only. We push the boundary to investigate the classic
single-sample single-timescale actor-critic on continuous (infinite)
state-action space, where we employ the canonical linear quadratic regulator
(LQR) problem as a case study. We show that the popular single-timescale
actor-critic can attain an epsilon-optimal solution with an order of epsilon to
-2 sample complexity for solving LQR on the demanding continuous state-action
space. Our work provides new insights into the performance of single-timescale
actor-critic, which further bridges the gap between theory and practice.

</details>


### [128] [Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities](https://arxiv.org/abs/2505.01043)
*Zhiwei Hao,Jianyuan Guo,Li Shen,Yong Luo,Han Hu,Guoxia Wang,Dianhai Yu,Yonggang Wen,Dacheng Tao*

Main category: cs.LG

TL;DR: 该论文综述了低精度训练方法，将其分为三类（定点/整数、浮点、自定义格式），并探讨了量化感知训练及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练资源需求高，低精度训练虽提升效率，但方法多样且分散，需统一整理。

Method: 将低精度训练方法分为三类：定点/整数、浮点、自定义格式，并讨论量化感知训练。

Result: 系统化整理了低精度训练方法，为研究者提供清晰分类和未来方向。

Conclusion: 低精度训练方法多样，分类整理有助于研究推进，未来需进一步探索。

Abstract: Large language models (LLMs) have achieved impressive performance across
various domains. However, the substantial hardware resources required for their
training present a significant barrier to efficiency and scalability. To
mitigate this challenge, low-precision training techniques have been widely
adopted, leading to notable advancements in training efficiency. Despite these
gains, low-precision training involves several components$\unicode{x2013}$such
as weights, activations, and gradients$\unicode{x2013}$each of which can be
represented in different numerical formats. The resulting diversity has created
a fragmented landscape in low-precision training research, making it difficult
for researchers to gain a unified overview of the field. This survey provides a
comprehensive review of existing low-precision training methods. To
systematically organize these approaches, we categorize them into three primary
groups based on their underlying numerical formats, which is a key factor
influencing hardware compatibility, computational efficiency, and ease of
reference for readers. The categories are: (1) fixed-point and integer-based
methods, (2) floating-point-based methods, and (3) customized format-based
methods. Additionally, we discuss quantization-aware training approaches, which
share key similarities with low-precision training during forward propagation.
Finally, we highlight several promising research directions to advance this
field. A collection of papers discussed in this survey is provided in
https://github.com/Hao840/Awesome-Low-Precision-Training.

</details>


### [129] [Multi-Step Consistency Models: Fast Generation with Theoretical Guarantees](https://arxiv.org/abs/2505.01049)
*Nishant Jain,Xunpeng Huang,Yian Ma,Tong Zhang*

Main category: cs.LG

TL;DR: 一致性模型作为传统SDE扩散模型的替代方案，显著加速生成过程，但缺乏理论支持。本文填补了这一空白，分析了其快速收敛的理论依据。


<details>
  <summary>Details</summary>
Motivation: 为一致性模型的加速生成提供理论证明，填补现有研究的空白。

Method: 通过分析一致性模型在反向轨迹中的映射能力，证明其能在少量迭代下实现KL散度收敛。

Result: 在数据分布的最小假设下，证明了KL收敛保证，并提供了模型估计的理论分析。

Conclusion: 一致性模型在平滑和非平滑设置下均可高效学习，且在非平滑情况下收敛速率优于现有方法。

Abstract: Consistency models have recently emerged as a compelling alternative to
traditional SDE based diffusion models, offering a significant acceleration in
generation by producing high quality samples in very few steps. Despite their
empirical success, a proper theoretic justification for their speed up is still
lacking. In this work, we provide the analysis which bridges this gap, showing
that given a consistency model which can map the input at a given time to
arbitrary timestamps along the reverse trajectory, one can achieve KL
divergence of order $ O(\varepsilon^2) $ using only $
O\left(\log\left(\frac{d}{\varepsilon}\right)\right) $ iterations with constant
step size, where d is the data dimension. Additionally, under minimal
assumptions on the data distribution an increasingly common setting in recent
diffusion model analyses we show that a similar KL convergence guarantee can be
obtained, with the number of steps scaling as $ O\left(d
\log\left(\frac{d}{\varepsilon}\right)\right) $. Going further, we also provide
a theoretical analysis for estimation of such consistency models, concluding
that accurate learning is feasible using small discretization steps, both in
smooth and non smooth settings. Notably, our results for the non smooth case
yield best in class convergence rates compared to existing SDE or ODE based
analyses under minimal assumptions.

</details>


### [130] [Monotone Peridynamic Neural Operator for Nonlinear Material Modeling with Conditionally Unique Solutions](https://arxiv.org/abs/2505.01060)
*Jihong Wang,Xiaochuan Tian,Zhongqiang Zhang,Stewart Silling,Siavash Jafarzadeh,Yue Yu*

Main category: cs.LG

TL;DR: 论文提出了一种基于神经算子的单调近场动力学神经算子（MPNO），用于数据驱动的非局部本构模型学习，确保解的唯一性，并在合成和真实数据中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的本构模型在物理可解释性和泛化性方面具有优势，但其适定性通常无法保证，容易导致非物理解。本研究旨在解决这一问题。

Method: MPNO通过学习非局部核和非线性本构关系，并通过单调梯度网络确保解的唯一性，从而保证能量密度函数的凸性。

Result: 在合成数据中，MPNO随网格尺寸减小收敛到真实解；在真实数据中，MPNO表现出优于传统神经网络的泛化能力。

Conclusion: MPNO在确保解唯一性和泛化性方面表现优异，适用于实际应用如从分子动力学数据中学习均质化模型。

Abstract: Data-driven methods have emerged as powerful tools for modeling the responses
of complex nonlinear materials directly from experimental measurements. Among
these methods, the data-driven constitutive models present advantages in
physical interpretability and generalizability across different boundary
conditions/domain settings. However, the well-posedness of these learned models
is generally not guaranteed a priori, which makes the models prone to
non-physical solutions in downstream simulation tasks. In this study, we
introduce monotone peridynamic neural operator (MPNO), a novel data-driven
nonlocal constitutive model learning approach based on neural operators. Our
approach learns a nonlocal kernel together with a nonlinear constitutive
relation, while ensuring solution uniqueness through a monotone gradient
network. This architectural constraint on gradient induces convexity of the
learnt energy density function, thereby guaranteeing solution uniqueness of
MPNO in small deformation regimes. To validate our approach, we evaluate MPNO's
performance on both synthetic and real-world datasets. On synthetic datasets
with manufactured kernel and constitutive relation, we show that the learnt
model converges to the ground-truth as the measurement grid size decreases both
theoretically and numerically. Additionally, our MPNO exhibits superior
generalization capabilities than the conventional neural networks: it yields
smaller displacement solution errors in down-stream tasks with new and unseen
loadings. Finally, we showcase the practical utility of our approach through
applications in learning a homogenized model from molecular dynamics data,
highlighting its expressivity and robustness in real-world scenarios.

</details>


### [131] [Improving Group Fairness in Knowledge Distillation via Laplace Approximation of Early Exits](https://arxiv.org/abs/2505.01070)
*Edvin Fasth,Sagar Singh*

Main category: cs.LG

TL;DR: 论文提出了一种基于拉普拉斯近似的方法，通过校准不确定性估计来重新加权困难样本，以提高知识蒸馏中的群体公平性。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏中学生模型倾向于学习浅层特征，可能导致群体公平性下降。现有方法使用早期退出网络的置信度边缘，但拉普拉斯近似可能更鲁棒。

Method: 利用拉普拉斯近似获取校准的不确定性估计，重新加权交叉熵和蒸馏损失。在MultiNLI数据集上使用Bert模型验证。

Result: 实验验证了拉普拉斯近似方法在识别困难样本和改善公平性上的有效性。

Conclusion: 拉普拉斯近似提供了一种更鲁棒的方法来优化知识蒸馏中的公平性。

Abstract: Knowledge distillation (KD) has become a powerful tool for training compact
student models using larger, pretrained teacher models, often requiring less
data and computational resources. Teacher models typically possess more layers
and thus exhibit richer feature representations compared to their student
counterparts. Furthermore, student models tend to learn simpler, surface-level
features in their early layers. This discrepancy can increase errors in groups
where labels spuriously correlate with specific input attributes, leading to a
decline in group fairness even when overall accuracy remains comparable to the
teacher. To mitigate these challenges, Early-Exit Neural Networks (EENNs),
which enable predictions at multiple intermediate layers, have been employed.
Confidence margins derived from these early exits have been utilized to
reweight both cross-entropy and distillation losses on a per-instance basis. In
this paper, we propose that leveraging Laplace approximation-based methods to
obtain well-calibrated uncertainty estimates can also effectively reweight
challenging instances and improve group fairness. We hypothesize that Laplace
approximation offers a more robust identification of difficult or ambiguous
instances compared to margin-based approaches. To validate our claims, we
benchmark our approach using a Bert-based model on the MultiNLI dataset.

</details>


### [132] [Federated Adapter on Foundation Models: An Out-Of-Distribution Approach](https://arxiv.org/abs/2505.01075)
*Yiyuan Yang,Guodong Long,Tianyi Zhou,Qinghua Lu,Shanshan Ye,Jing Jiang*

Main category: cs.LG

TL;DR: FedOA是一种针对联邦基础模型（FedFM）的隐私保护方法，通过适配器微调和特征距离正则化解决分布外泛化问题。


<details>
  <summary>Details</summary>
Motivation: 解决FedFM中因大参数规模和数据异构性导致的分布外泛化挑战。

Method: 采用适配器微调方法，引入个性化适配器和特征距离正则化。

Result: 理论证明全局模型具有分布外泛化能力，FedOA通过正则化提升个性化模型的泛化性能，实验验证了其有效性。

Conclusion: FedOA在FedFM中有效解决了分布外泛化问题，适用于多种NLP任务。

Abstract: As foundation models gain prominence, Federated Foundation Models (FedFM)
have emerged as a privacy-preserving approach to collaboratively fine-tune
models in federated learning (FL) frameworks using distributed datasets across
clients. A key challenge for FedFM, given the versatile nature of foundation
models, is addressing out-of-distribution (OOD) generalization, where unseen
tasks or clients may exhibit distribution shifts leading to suboptimal
performance. Although numerous studies have explored OOD generalization in
conventional FL, these methods are inadequate for FedFM due to the challenges
posed by large parameter scales and increased data heterogeneity. To address
these, we propose FedOA, which employs adapter-based parameter-efficient
fine-tuning methods for efficacy and introduces personalized adapters with
feature distance-based regularization to align distributions and guarantee OOD
generalization for each client. Theoretically, we demonstrate that the
conventional aggregated global model in FedFM inherently retains OOD
generalization capabilities, and our proposed method enhances the personalized
model's OOD generalization through regularization informed by the global model,
with proven convergence under general non-convex settings. Empirically, the
effectiveness of the proposed method is validated on benchmark datasets across
various NLP tasks.

</details>


### [133] [Integration Matters for Learning PDEs with Backwards SDEs](https://arxiv.org/abs/2505.01078)
*Sungje Park,Stephen Tu*

Main category: cs.LG

TL;DR: 论文提出基于Stratonovich的BSDE方法，通过Heun积分消除EM积分带来的偏差，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于BSDE的求解器在性能上不如PINNs，原因是EM积分在短时间自洽BSDE损失中引入偏差。

Method: 提出Stratonovich-based BSDE方法，采用Heun积分消除偏差。

Result: Heun-based BSDE方法性能优于EM变体，与PINNs竞争。

Conclusion: 积分方案在BSDE求解器中起关键作用，此前被忽视。

Abstract: Backward stochastic differential equation (BSDE)-based deep learning methods
provide an alternative to Physics-Informed Neural Networks (PINNs) for solving
high-dimensional partial differential equations (PDEs), offering algorithmic
advantages in settings such as stochastic optimal control, where the PDEs of
interest are tied to an underlying dynamical system. However, existing
BSDE-based solvers have empirically been shown to underperform relative to
PINNs in the literature. In this paper, we identify the root cause of this
performance gap as a discretization bias introduced by the standard
Euler-Maruyama (EM) integration scheme applied to short-horizon
self-consistency BSDE losses, which shifts the optimization landscape off
target. We find that this bias cannot be satisfactorily addressed through finer
step sizes or longer self-consistency horizons. To properly handle this issue,
we propose a Stratonovich-based BSDE formulation, which we implement with
stochastic Heun integration. We show that our proposed approach completely
eliminates the bias issues faced by EM integration. Furthermore, our empirical
results show that our Heun-based BSDE method consistently outperforms EM-based
variants and achieves competitive results with PINNs across multiple
high-dimensional benchmarks. Our findings highlight the critical role of
integration schemes in BSDE-based PDE solvers, an algorithmic detail that has
received little attention thus far in the literature.

</details>


### [134] [Multi-Objective Reinforcement Learning for Water Management](https://arxiv.org/abs/2505.01094)
*Zuzanna Osika,Roxana Radelescu,Jazmin Zatarain Salazar,Frans Oliehoek,Pradeep K. Murukannaiah*

Main category: cs.LG

TL;DR: 论文提出了一种基于尼罗河流域水资源管理的多目标强化学习（MORL）环境，并评估了现有MORL算法的表现，发现专业水资源管理方法优于当前MORL算法。


<details>
  <summary>Details</summary>
Motivation: 现实问题（如资源管理、自动驾驶、药物发现）常涉及多目标优化，但MORL领域缺乏复杂、真实的环境和基准。

Method: 以尼罗河流域水资源管理为案例，建模为MORL环境，并评估现有MORL算法。

Result: 专业水资源管理方法优于当前MORL算法，突显MORL在现实场景中的扩展性挑战。

Conclusion: MORL算法在复杂现实问题中面临扩展性挑战，需进一步优化以适应实际需求。

Abstract: Many real-world problems (e.g., resource management, autonomous driving, drug
discovery) require optimizing multiple, conflicting objectives. Multi-objective
reinforcement learning (MORL) extends classic reinforcement learning to handle
multiple objectives simultaneously, yielding a set of policies that capture
various trade-offs. However, the MORL field lacks complex, realistic
environments and benchmarks. We introduce a water resource (Nile river basin)
management case study and model it as a MORL environment. We then benchmark
existing MORL algorithms on this task. Our results show that specialized water
management methods outperform state-of-the-art MORL approaches, underscoring
the scalability challenges MORL algorithms face in real-world scenarios.

</details>


### [135] [Nesterov Method for Asynchronous Pipeline Parallel Optimization](https://arxiv.org/abs/2505.01099)
*Thalaiyasingam Ajanthan,Sameera Ramasinghe,Yan Zuo,Gil Avraham,Alexander Long*

Main category: cs.LG

TL;DR: 提出一种改进的Nesterov加速梯度方法，用于解决异步优化中的梯度延迟问题，显著提升管道并行训练的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 在管道并行训练中，异步优化虽能实现100%的管道利用率，但梯度延迟问题导致性能下降。

Method: 改进Nesterov加速梯度方法，调整其前瞻步骤以有效处理梯度延迟。

Result: 理论证明在固定梯度延迟下具有次线性收敛率，实验表明在10亿参数的语言模型任务中优于现有异步方法。

Conclusion: 该方法显著提升了异步优化的性能，甚至超越同步基线。

Abstract: Pipeline Parallelism (PP) enables large neural network training on small,
interconnected devices by splitting the model into multiple stages. To maximize
pipeline utilization, asynchronous optimization is appealing as it offers 100%
pipeline utilization by construction. However, it is inherently challenging as
the weights and gradients are no longer synchronized, leading to stale (or
delayed) gradients. To alleviate this, we introduce a variant of Nesterov
Accelerated Gradient (NAG) for asynchronous optimization in PP. Specifically,
we modify the look-ahead step in NAG to effectively address the staleness in
gradients. We theoretically prove that our approach converges at a sublinear
rate in the presence of fixed delay in gradients. Our experiments on
large-scale language modelling tasks using decoder-only architectures with up
to 1B parameters, demonstrate that our approach significantly outperforms
existing asynchronous methods, even surpassing the synchronous baseline.

</details>


### [136] [CoCoAFusE: Beyond Mixtures of Experts via Model Fusion](https://arxiv.org/abs/2505.01105)
*Aurelio Raffa Ugolini,Mara Tanelli,Valentina Breschi*

Main category: cs.LG

TL;DR: CoCoAFusE是一种新型贝叶斯协变量依赖建模技术，通过专家竞争/协作融合，提升模型表达能力和局部可解释性，同时避免多模态伪影。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在可解释性和不确定性量化方面的不足，提出一种更灵活的建模方法。

Method: 基于混合专家模型（MoEs），引入专家分布的融合机制，避免传统混合方法的多模态伪影。

Result: CoCoAFusE在复杂回归问题中表现优异，提供更紧密的可信区间，适应不同中间行为场景。

Conclusion: CoCoAFusE是一种高效且灵活的建模方法，适用于不确定性是关键关注点的复杂问题。

Abstract: Many learning problems involve multiple patterns and varying degrees of
uncertainty dependent on the covariates. Advances in Deep Learning (DL) have
addressed these issues by learning highly nonlinear input-output dependencies.
However, model interpretability and Uncertainty Quantification (UQ) have often
straggled behind. In this context, we introduce the Competitive/Collaborative
Fusion of Experts (CoCoAFusE), a novel, Bayesian Covariates-Dependent Modeling
technique. CoCoAFusE builds on the very philosophy behind Mixtures of Experts
(MoEs), blending predictions from several simple sub-models (or "experts") to
achieve high levels of expressiveness while retaining a substantial degree of
local interpretability. Our formulation extends that of a classical Mixture of
Experts by contemplating the fusion of the experts' distributions in addition
to their more usual mixing (i.e., superimposition). Through this additional
feature, CoCoAFusE better accommodates different scenarios for the intermediate
behavior between generating mechanisms, resulting in tighter credible bounds on
the response variable. Indeed, only resorting to mixing, as in classical MoEs,
may lead to multimodality artifacts, especially over smooth transitions.
Instead, CoCoAFusE can avoid these artifacts even under the same structure and
priors for the experts, leading to greater expressiveness and flexibility in
modeling. This new approach is showcased extensively on a suite of motivating
numerical examples and a collection of real-data ones, demonstrating its
efficacy in tackling complex regression problems where uncertainty is a key
quantity of interest.

</details>


### [137] [Incorporating Inductive Biases to Energy-based Generative Models](https://arxiv.org/abs/2505.01111)
*Yukun Li,Li-Ping Liu*

Main category: cs.LG

TL;DR: 本文提出了一种混合方法，结合能量基模型（EBM）和指数族模型，通过引入无参数统计函数来增强数据建模的归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 能量基模型（EBMs）在生成模型中重新受到关注，但通常使用神经网络定义能量函数。本文旨在通过结合指数族模型的特性，增强EBMs的统计建模能力。

Method: 提出了一种混合模型，将EBM与指数族模型结合，通过添加无参数统计函数来捕捉关键数据统计量。

Result: 实验验证了混合模型能够匹配统计数据，并且在加入合适的统计信息后，数据拟合和生成效果均有所提升。

Conclusion: 混合模型通过结合EBM和指数族模型的优势，有效提升了数据建模和生成能力。

Abstract: With the advent of score-matching techniques for model training and Langevin
dynamics for sample generation, energy-based models (EBMs) have gained renewed
interest as generative models. Recent EBMs usually use neural networks to
define their energy functions. In this work, we introduce a novel hybrid
approach that combines an EBM with an exponential family model to incorporate
inductive bias into data modeling. Specifically, we augment the energy term
with a parameter-free statistic function to help the model capture key data
statistics. Like an exponential family model, the hybrid model aims to align
the distribution statistics with data statistics during model training, even
when it only approximately maximizes the data likelihood. This property enables
us to impose constraints on the hybrid model. Our empirical study validates the
hybrid model's ability to match statistics. Furthermore, experimental results
show that data fitting and generation improve when suitable informative
statistics are incorporated into the hybrid model.

</details>


### [138] [Exploring Equity of Climate Policies using Multi-Agent Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2505.01115)
*Palok Biswas,Zuzanna Osika,Isidoro Tamassia,Adit Whorra,Jazmin Zatarain-Salazar,Jan Kwakkel,Frans A. Oliehoek,Pradeep K. Murukannaiah*

Main category: cs.LG

TL;DR: 论文提出了一种名为Justice的新框架，将IAM与多目标多智能体强化学习结合，以解决传统IAM在气候政策中忽视公平性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统IAM仅基于单一目标优化政策，无法捕捉经济增长、温度目标和气候公平之间的权衡，导致政策建议加剧不平等。

Method: 引入Justice框架，结合多目标多智能体强化学习（MOMARL），生成兼顾公平与气候经济目标的政策建议。

Result: Justice框架能够识别公平的帕累托最优政策，帮助决策者在气候与经济政策中权衡利弊。

Conclusion: Justice框架为气候政策提供了更全面的决策支持，强调了公平性在多目标优化中的重要性。

Abstract: Addressing climate change requires coordinated policy efforts of nations
worldwide. These efforts are informed by scientific reports, which rely in part
on Integrated Assessment Models (IAMs), prominent tools used to assess the
economic impacts of climate policies. However, traditional IAMs optimize
policies based on a single objective, limiting their ability to capture the
trade-offs among economic growth, temperature goals, and climate justice. As a
result, policy recommendations have been criticized for perpetuating
inequalities, fueling disagreements during policy negotiations. We introduce
Justice, the first framework integrating IAM with Multi-Objective Multi-Agent
Reinforcement Learning (MOMARL). By incorporating multiple objectives, Justice
generates policy recommendations that shed light on equity while balancing
climate and economic goals. Further, using multiple agents can provide a
realistic representation of the interactions among the diverse policy actors.
We identify equitable Pareto-optimal policies using our framework, which
facilitates deliberative decision-making by presenting policymakers with the
inherent trade-offs in climate and economic policy.

</details>


### [139] [Risk Analysis and Design Against Adversarial Actions](https://arxiv.org/abs/2505.01130)
*Marco C. Campi,Algo Carè,Luis G. Crespo,Simone Garatti,Federico A. Ramponi*

Main category: cs.LG

TL;DR: 提出了一种评估模型对抗攻击鲁棒性的通用框架，适用于多种攻击类型和强度，无需额外测试数据。


<details>
  <summary>Details</summary>
Motivation: 解决模型在部署时因数据分布变化而面临的对抗行为挑战。

Method: 基于支持向量回归（SVR），扩展至松弛优化技术的学习领域。

Result: 提供了一种无需额外测试数据的模型脆弱性评估方法，并在分布无关设置中有效。

Conclusion: 框架不仅增强模型可信度，还为选择模型提供了依据，并为分布外问题提供了新见解。

Abstract: Learning models capable of providing reliable predictions in the face of
adversarial actions has become a central focus of the machine learning
community in recent years. This challenge arises from observing that data
encountered at deployment time often deviate from the conditions under which
the model was trained. In this paper, we address deployment-time adversarial
actions and propose a versatile, well-principled framework to evaluate the
model's robustness against attacks of diverse types and intensities. While we
initially focus on Support Vector Regression (SVR), the proposed approach
extends naturally to the broad domain of learning via relaxed optimization
techniques. Our results enable an assessment of the model vulnerability without
requiring additional test data and operate in a distribution-free setup. These
results not only provide a tool to enhance trust in the model's applicability
but also aid in selecting among competing alternatives. Later in the paper, we
show that our findings also offer useful insights for establishing new results
within the out-of-distribution framework.

</details>


### [140] [Aggregation of Dependent Expert Distributions in Multimodal Variational Autoencoders](https://arxiv.org/abs/2505.01134)
*Rogelio A Mancisidor,Robert Jenssen,Shujian Yu,Michael Kampffmeyer*

Main category: cs.LG

TL;DR: 论文提出了一种基于共识依赖专家（CoDE）的新型多模态VAE方法，避免了现有方法对模态独立性的假设，提升了生成质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态VAE方法假设模态独立性，导致生成质量和一致性不足。

Method: 利用CoDE原则聚合单模态分布，提出新型ELBO近似多模态数据的联合似然。

Result: CoDE-VAE在生成质量、一致性和对数似然估计上表现更优，且随模态数增加性能差距缩小。

Conclusion: CoDE-VAE在多模态任务中表现优异，生成质量接近单模态VAE，分类精度与现有最优方法相当。

Abstract: Multimodal learning with variational autoencoders (VAEs) requires estimating
joint distributions to evaluate the evidence lower bound (ELBO). Current
methods, the product and mixture of experts, aggregate single-modality
distributions assuming independence for simplicity, which is an overoptimistic
assumption. This research introduces a novel methodology for aggregating
single-modality distributions by exploiting the principle of consensus of
dependent experts (CoDE), which circumvents the aforementioned assumption.
Utilizing the CoDE method, we propose a novel ELBO that approximates the joint
likelihood of the multimodal data by learning the contribution of each subset
of modalities. The resulting CoDE-VAE model demonstrates better performance in
terms of balancing the trade-off between generative coherence and generative
quality, as well as generating more precise log-likelihood estimations.
CoDE-VAE further minimizes the generative quality gap as the number of
modalities increases. In certain cases, it reaches a generative quality similar
to that of unimodal VAEs, which is a desirable property that is lacking in most
current methods. Finally, the classification accuracy achieved by CoDE-VAE is
comparable to that of state-of-the-art multimodal VAE models.

</details>


### [141] [Dual-Forecaster: A Multimodal Time Series Model Integrating Descriptive and Predictive Texts](https://arxiv.org/abs/2505.01135)
*Wenfa Wu,Guanyu Zhang,Zheng Tan,Yi Wang,Hongsheng Qi*

Main category: cs.LG

TL;DR: Dual-Forecaster是一种新型多模态时间序列模型，结合历史和预测性文本信息，通过跨模态对齐技术提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有单模态时间序列模型信息不足，多模态模型虽能整合文本信息，但未能充分利用历史和未来文本的独特贡献，且跨模态理解能力有限。

Method: 提出Dual-Forecaster，结合历史和预测性文本信息，采用三种跨模态对齐技术增强多模态理解能力。

Result: 在15个多模态时间序列数据集上评估，Dual-Forecaster性能优于或媲美现有最优模型。

Conclusion: 整合文本信息显著提升时间序列预测性能，为多模态时间序列分析开辟新途径。

Abstract: Most existing single-modal time series models rely solely on numerical
series, which suffer from the limitations imposed by insufficient information.
Recent studies have revealed that multimodal models can address the core issue
by integrating textual information. However, these models focus on either
historical or future textual information, overlooking the unique contributions
each plays in time series forecasting. Besides, these models fail to grasp the
intricate relationships between textual and time series data, constrained by
their moderate capacity for multimodal comprehension. To tackle these
challenges, we propose Dual-Forecaster, a pioneering multimodal time series
model that combines both descriptively historical textual information and
predictive textual insights, leveraging advanced multimodal comprehension
capability empowered by three well-designed cross-modality alignment
techniques. Our comprehensive evaluations on fifteen multimodal time series
datasets demonstrate that Dual-Forecaster is a distinctly effective multimodal
time series model that outperforms or is comparable to other state-of-the-art
models, highlighting the superiority of integrating textual information for
time series forecasting. This work opens new avenues in the integration of
textual information with numerical time series data for multimodal time series
analysis.

</details>


### [142] [Machine Learning for Physical Simulation Challenge Results and Retrospective Analysis: Power Grid Use Case](https://arxiv.org/abs/2505.01156)
*Milad Leyli-Abadi,Jérôme Picault,Antoine Marot,Jean-Patrick Brunet,Agathe Gilain,Amarsagar Reddy Ramapuram Matavalam,Shaban Ghias Satti,Quingbin Jiang,Yang Liu,Dean Justin Ninalga*

Main category: cs.LG

TL;DR: 论文探讨了电力网格模拟的计算挑战，提出AI驱动方法加速模拟并保持可靠性，通过LIPS框架评估解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源（如风能和太阳能）的集成增加，传统模拟方法计算成本过高，需更高效的解决方案。

Method: 组织竞赛开发AI方法，使用LIPS框架评估性能、物理合规性、工业准备和泛化能力。

Result: 竞赛中表现最佳的解决方案超越了传统模拟方法，展示了高效和可扩展性。

Conclusion: 研究旨在推动更高效、可持续的电力网络模拟方法的发展。

Abstract: This paper addresses the growing computational challenges of power grid
simulations, particularly with the increasing integration of renewable energy
sources like wind and solar. As grid operators must analyze significantly more
scenarios in near real-time to prevent failures and ensure stability,
traditional physical-based simulations become computationally impractical. To
tackle this, a competition was organized to develop AI-driven methods that
accelerate power flow simulations by at least an order of magnitude while
maintaining operational reliability. This competition utilized a regional-scale
grid model with a 30\% renewable energy mix, mirroring the anticipated
near-future composition of the French power grid. A key contribution of this
work is through the use of LIPS (Learning Industrial Physical Systems), a
benchmarking framework that evaluates solutions based on four critical
dimensions: machine learning performance, physical compliance, industrial
readiness, and generalization to out-of-distribution scenarios. The paper
provides a comprehensive overview of the Machine Learning for Physical
Simulation (ML4PhySim) competition, detailing the benchmark suite, analyzing
top-performing solutions that outperformed traditional simulation methods, and
sharing key organizational insights and best practices for running large-scale
AI competitions. Given the promising results achieved, the study aims to
inspire further research into more efficient, scalable, and sustainable power
network simulation methodologies.

</details>


### [143] [TActiLE: Tiny Active LEarning for wearable devices](https://arxiv.org/abs/2505.01160)
*Massimo Pavan,Claudio Galimberti,Manuel Roveri*

Main category: cs.LG

TL;DR: 论文探讨了在TinyML中应用主动学习（AL）技术以减少标注数据需求，提出了一种名为TActiLE的新型AL算法，适用于智能穿戴设备。


<details>
  <summary>Details</summary>
Motivation: 智能穿戴设备中，手动标注大量数据不切实际且可能导致用户流失，因此需要减少标注需求的解决方案。

Method: 提出TActiLE算法，通过主动选择少量未标注数据进行标注，优化模型性能。

Result: 实验表明TActiLE在图像分类任务中有效且高效，适用于小型设备。

Conclusion: TActiLE为TinyML中的主动学习提供了首个专用解决方案，适用于智能穿戴设备。

Abstract: Tiny Machine Learning (TinyML) algorithms have seen extensive use in recent
years, enabling wearable devices to be not only connected but also genuinely
intelligent by running machine learning (ML) computations directly on-device.
Among such devices, smart glasses have particularly benefited from TinyML
advancements. TinyML facilitates the on-device execution of the inference phase
of ML algorithms on embedded and wearable devices, and more recently, it has
expanded into On-device Learning (ODL), which allows both inference and
learning phases to occur directly on the device. The application of ODL
techniques to wearable devices is particularly compelling, as it enables the
development of more personalized models that adapt based on the data of the
user. However, one of the major challenges of ODL algorithms is the scarcity of
labeled data collected on-device. In smart wearable contexts, requiring users
to manually label large amounts of data is often impractical and could lead to
user disengagement with the technology. To address this issue, this paper
explores the application of Active Learning (AL) techniques, i.e., techniques
that aim at minimizing the labeling effort, by actively selecting from a large
quantity of unlabeled data only a small subset to be labeled and added to the
training set of the algorithm. In particular, we propose TActiLE, a novel AL
algorithm that selects from the stream of on-device sensor data the ones that
would help the ML algorithm improve the most once coupled with labels provided
by the user. TActiLE is the first Active Learning technique specifically
designed for the TinyML context. We evaluate its effectiveness and efficiency
through experiments on multiple image classification datasets. The results
demonstrate its suitability for tiny and wearable devices.

</details>


### [144] [Empirical Comparison of Lightweight Forecasting Models for Seasonal and Non-Seasonal Time Series](https://arxiv.org/abs/2505.01163)
*Thanh Son Nguyen,Dang Minh Duc Nguyen,Van Thanh Nguyen*

Main category: cs.LG

TL;DR: 比较多项式分类器（PC）和径向基函数神经网络（RBFNN）在时间序列预测中的表现，PC在非季节性数据中表现更好，RBFNN在季节性数据中更优。


<details>
  <summary>Details</summary>
Motivation: 研究高精度和高效计算的时间序列预测方法，为实际应用提供模型选择依据。

Method: 使用四种真实数据集（天气、金价、原油价、啤酒产量）比较PC和RBFNN的预测精度和计算时间。

Result: PC在非季节性数据中更快更准，RBFNN在季节性数据中表现更优，且PC更易解释。

Conclusion: PC适用于非季节性数据的快速可解释预测，RBFNN更适合复杂季节性模式。

Abstract: Accurate time series forecasting is essential in many real-time applications
that demand both high predictive accuracy and computational efficiency. This
study provides an empirical comparison between a Polynomial Classifier and a
Radial Basis Function Neural Network (RBFNN) across four real-world time series
datasets (weather conditions, gold prices, crude oil prices, and beer
production volumes) that cover both seasonal and nonseasonal patterns. Model
performance is evaluated by forecasting accuracy (using Mean Absolute Error,
Root Mean Squared Error, and Coefficient of Variation of Root Mean Squared
Error) and computational time to assess each model's viability for real time
forecasting. The results show that the PC yields more accurate and faster
forecasts for non seasonal series, whereas the RBFNN performs better on series
with pronounced seasonal patterns. From an interpretability standpoint, the
polynomial model offers a simpler, more transparent structure (in contrast to
the black box nature of neural network), which is advantageous for
understanding and trust in real time decision making. The performance
differences between PC and RBFNN are statistically significant, as confirmed by
paired t tests and Wilcoxon signed rank tests. These findings provide practical
guidance for model selection in time series forecasting, indicating that PC may
be preferable for quick, interpretable forecasts in non-seasonal contexts,
whereas RBFNN is superior for capturing complex seasonal behaviors

</details>


### [145] [Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for Adversarial Transferability](https://arxiv.org/abs/2505.01168)
*Zhaoyang Ma,Zhihao Wu,Wang Lu,Xin Gao,Jinghang Yue,Taolin Zhang,Lipo Wang,Youfang Lin,Jing Wang*

Main category: cs.LG

TL;DR: HEAT是一种新的对抗样本生成方法，通过域泛化和动态权重分配提升对抗样本的迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在共享梯度方向捕捉和权重分配上存在不足，威胁深度神经网络安全。

Method: HEAT包含共识梯度方向合成器（使用SVD）和双和谐权重协调器，动态平衡域内一致性和域间多样性。

Result: HEAT在多种数据集和设置下显著优于现有方法。

Conclusion: HEAT为对抗攻击研究提供了新视角和方向。

Abstract: The development of model ensemble attacks has significantly improved the
transferability of adversarial examples, but this progress also poses severe
threats to the security of deep neural networks. Existing methods, however,
face two critical challenges: insufficient capture of shared gradient
directions across models and a lack of adaptive weight allocation mechanisms.
To address these issues, we propose a novel method Harmonized Ensemble for
Adversarial Transferability (HEAT), which introduces domain generalization into
adversarial example generation for the first time. HEAT consists of two key
modules: Consensus Gradient Direction Synthesizer, which uses Singular Value
Decomposition to synthesize shared gradient directions; and Dual-Harmony Weight
Orchestrator which dynamically balances intra-domain coherence, stabilizing
gradients within individual models, and inter-domain diversity, enhancing
transferability across models. Experimental results demonstrate that HEAT
significantly outperforms existing methods across various datasets and
settings, offering a new perspective and direction for adversarial attack
research.

</details>


### [146] [Distilling Two-Timed Flow Models by Separately Matching Initial and Terminal Velocities](https://arxiv.org/abs/2505.01169)
*Pramook Khungurn,Pratch Piyawongwisal,Sira Sriswadi,Supasorn Suwajanakorn*

Main category: cs.LG

TL;DR: 本文提出了一种新的损失函数ITVM，用于蒸馏双时间流模型（TTFM），通过匹配初始和终端速度，改进了生成性能。


<details>
  <summary>Details</summary>
Motivation: 改进现有的流匹配模型，通过蒸馏技术提升生成效率，特别是在少步生成任务中。

Method: 提出ITVM损失函数，扩展LFMD损失，匹配初始速度，移除终端速度导数，并使用EMA稳定模型计算目标终端速度。

Result: 初步实验表明，ITVM在多种数据集和模型架构上优于基线，生成性能更优。

Conclusion: ITVM损失函数有效提升了TTFM的生成性能，尤其在少步生成任务中表现突出。

Abstract: A flow matching model learns a time-dependent vector field $v_t(x)$ that
generates a probability path $\{ p_t \}_{0 \leq t \leq 1}$ that interpolates
between a well-known noise distribution ($p_0$) and the data distribution
($p_1$). It can be distilled into a \emph{two-timed flow model} (TTFM)
$\phi_{s,x}(t)$ that can transform a sample belonging to the distribution at an
initial time $s$ to another belonging to the distribution at a terminal time
$t$ in one function evaluation. We present a new loss function for TTFM
distillation called the \emph{initial/terminal velocity matching} (ITVM) loss
that extends the Lagrangian Flow Map Distillation (LFMD) loss proposed by Boffi
et al. by adding redundant terms to match the initial velocities at time $s$,
removing the derivative from the terminal velocity term at time $t$, and using
a version of the model under training, stabilized by exponential moving
averaging (EMA), to compute the target terminal average velocity. Preliminary
experiments show that our loss leads to better few-step generation performance
on multiple types of datasets and model architectures over baselines.

</details>


### [147] [A Secured Triad of IoT, Machine Learning, and Blockchain for Crop Forecasting in Agriculture](https://arxiv.org/abs/2505.01196)
*Najmus Sakib Sizan,Md. Abu Layek,Khondokar Fida Hasan*

Main category: cs.LG

TL;DR: 论文提出了一种结合物联网、机器学习和区块链技术的新方法，用于提高作物预测准确性并为农民提供数据驱动的决策支持。


<details>
  <summary>Details</summary>
Motivation: 旨在通过整合先进技术改善作物预测，为农民提供更精确、安全的决策依据。

Method: 利用物联网传感器网络实时监测环境与土壤数据，采用随机森林模型预测作物类型和产量，并通过以太坊区块链确保数据安全。

Result: 随机森林模型预测准确率达99.45%，系统提供实时和历史作物预测，增强透明度和决策支持。

Conclusion: 该方法显著提升了精准农业的作物预测准确性、安全性和用户友好性。

Abstract: To improve crop forecasting and provide farmers with actionable data-driven
insights, we propose a novel approach integrating IoT, machine learning, and
blockchain technologies. Using IoT, real-time data from sensor networks
continuously monitor environmental conditions and soil nutrient levels,
significantly improving our understanding of crop growth dynamics. Our study
demonstrates the exceptional accuracy of the Random Forest model, achieving a
99.45\% accuracy rate in predicting optimal crop types and yields, thereby
offering precise crop projections and customized recommendations. To ensure the
security and integrity of the sensor data used for these forecasts, we
integrate the Ethereum blockchain, which provides a robust and secure platform.
This ensures that the forecasted data remain tamper-proof and reliable.
Stakeholders can access real-time and historical crop projections through an
intuitive online interface, enhancing transparency and facilitating informed
decision-making. By presenting multiple predicted crop scenarios, our system
enables farmers to optimize production strategies effectively. This integrated
approach promises significant advances in precision agriculture, making crop
forecasting more accurate, secure, and user-friendly.

</details>


### [148] [CaReAQA: A Cardiac and Respiratory Audio Question Answering Model for Open-Ended Diagnostic Reasoning](https://arxiv.org/abs/2505.01199)
*Tsai-Ning Wang,Lin-Lin Chen,Neil Zeghidour,Aaqib Saeed*

Main category: cs.LG

TL;DR: CaReAQA是一种结合音频模型和语言模型的音频-语言模型，用于医学音频信号的开放诊断推理，性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统医学音频分析方法依赖手工特征或大量标注数据，限制了可扩展性和适用性。

Method: 提出CaReAQA模型，结合基础音频模型和大语言模型的推理能力；同时发布CaReSound数据集，支持诊断推理研究。

Result: CaReAQA在开放诊断任务中准确率达86.2%，在未见数据集上的分类任务中平均准确率为56.9%。

Conclusion: 音频-语言集成和推理推动了医学诊断，为临床决策支持提供了高效AI系统。

Abstract: Medical audio signals, such as heart and lung sounds, play a crucial role in
clinical diagnosis. However, analyzing these signals remains challenging:
traditional methods rely on handcrafted features or supervised deep learning
models that demand extensive labeled datasets, limiting their scalability and
applicability. To address these issues, we propose CaReAQA, an audio-language
model that integrates a foundation audio model with the reasoning capabilities
of large language models, enabling clinically relevant, open-ended diagnostic
responses. Alongside CaReAQA, we introduce CaReSound, a benchmark dataset of
annotated medical audio recordings enriched with metadata and paired
question-answer examples, intended to drive progress in diagnostic reasoning
research. Evaluation results show that CaReAQA achieves 86.2% accuracy on
open-ended diagnostic reasoning tasks, outperforming baseline models. It also
generalizes well to closed-ended classification tasks, achieving an average
accuracy of 56.9% on unseen datasets. Our findings show how audio-language
integration and reasoning advances medical diagnostics, enabling efficient AI
systems for clinical decision support.

</details>


### [149] [AGRO: An Autonomous AI Rover for Precision Agriculture](https://arxiv.org/abs/2505.01200)
*Simar Ghumman,Fabio Di Troia,William Andreopoulos,Mark Stamp,Sanjit Rai*

Main category: cs.LG

TL;DR: AGRO是一种结合机器学习和传感器技术的无人地面车辆，用于农业自动化，支持农民数据驱动决策。


<details>
  <summary>Details</summary>
Motivation: 解决农业中资源消耗大的复杂问题，提高效率和决策能力。

Method: 利用机器学习、计算机视觉和传感器技术，实现自主导航、环境映射和产量预测。

Result: AGRO能自主运行，实时捕获数据并支持决策。

Conclusion: AGRO为农业自动化提供了有效工具，并为机器学习应用奠定了基础。

Abstract: Unmanned Ground Vehicles (UGVs) are emerging as a crucial tool in the world
of precision agriculture. The combination of UGVs with machine learning allows
us to find solutions for a range of complex agricultural problems. This
research focuses on developing a UGV capable of autonomously traversing
agricultural fields and capturing data. The project, known as AGRO (Autonomous
Ground Rover Observer) leverages machine learning, computer vision and other
sensor technologies. AGRO uses its capabilities to determine pistachio yields,
performing self-localization and real-time environmental mapping while avoiding
obstacles. The main objective of this research work is to automate
resource-consuming operations so that AGRO can support farmers in making
data-driven decisions. Furthermore, AGRO provides a foundation for advanced
machine learning techniques as it captures the world around it.

</details>


### [150] [Quantitative Attractor Analysis of High-Capacity Kernel Logistic Regression Hopfield Networks](https://arxiv.org/abs/2505.01218)
*Akira Tamamori*

Main category: cs.LG

TL;DR: 论文分析了KLR训练的Hopfield网络的吸引子结构，展示了其高容量（4.0 P/N）和鲁棒性，动态速度快且几乎没有虚假固定点。


<details>
  <summary>Details</summary>
Motivation: 传统Hopfield网络因Hebbian学习面临存储容量限制和虚假吸引子问题，KLR通过非线性映射提升性能。

Method: 通过广泛模拟评估KLR网络的吸引子结构，包括不同存储负载和噪声水平下的召回性能、收敛速度和动态特性。

Result: KLR网络表现出高容量（4.0 P/N）、鲁棒性和快速动态（1-2步收敛），几乎没有虚假固定点。

Conclusion: KLR通过重塑动态特性实现高效联想记忆，为理解高容量关联存储提供了新视角。

Abstract: Traditional Hopfield networks, using Hebbian learning, face severe storage
capacity limits ($\approx 0.14$ P/N) and spurious attractors. Kernel Logistic
Regression (KLR) offers a non-linear approach, mapping patterns to
high-dimensional feature spaces for improved separability. Our previous work
showed KLR dramatically improves capacity and noise robustness over
conventional methods. This paper quantitatively analyzes the attractor
structures in KLR-trained networks via extensive simulations. We evaluated
recall from diverse initial states across wide storage loads (up to 4.0 P/N)
and noise levels. We quantified convergence rates and speed. Our analysis
confirms KLR's superior performance: high capacity (up to 4.0 P/N) and
robustness. The attractor landscape is remarkably "clean," with near-zero
spurious fixed points. Recall failures under high load/noise are primarily due
to convergence to other learned patterns, not spurious ones. Dynamics are
exceptionally fast (typically 1-2 steps for high-similarity states). This
characterization reveals how KLR reshapes dynamics for high-capacity
associative memory, highlighting its effectiveness and contributing to AM
understanding.

</details>


### [151] [mwBTFreddy: A Dataset for Flash Flood Damage Assessment in Urban Malawi](https://arxiv.org/abs/2505.01242)
*Evelyn Chapuma,Grey Mengezi,Lewis Msasa,Amelia Taylor*

Main category: cs.LG

TL;DR: mwBTFreddy数据集是为支持马拉维城市洪水灾害评估而开发的资源，包含灾前灾后卫星图像及建筑损坏标注，用于机器学习模型开发和灾害分析。


<details>
  <summary>Details</summary>
Motivation: 为非洲城市环境中的建筑检测和损坏分类提供数据支持，帮助决策者进行灾后规划和应急响应。

Method: 使用Google Earth Pro获取卫星图像，并标注建筑损坏等级（无损坏、轻微、严重或毁坏）。

Result: 数据集包含配对的卫星图像和JSON标注文件，支持机器学习和空间分析。

Conclusion: 该数据集有助于提升非洲城市洪水灾害评估能力，为气候脆弱地区的决策提供支持。

Abstract: This paper describes the mwBTFreddy dataset, a resource developed to support
flash flood damage assessment in urban Malawi, specifically focusing on the
impacts of Cyclone Freddy in 2023. The dataset comprises paired pre- and
post-disaster satellite images sourced from Google Earth Pro, accompanied by
JSON files containing labelled building annotations with geographic coordinates
and damage levels (no damage, minor, major, or destroyed). Developed by the
Kuyesera AI Lab at the Malawi University of Business and Applied Sciences, this
dataset is intended to facilitate the development of machine learning models
tailored to building detection and damage classification in African urban
contexts. It also supports flood damage visualisation and spatial analysis to
inform decisions on relocation, infrastructure planning, and emergency response
in climate-vulnerable regions.

</details>


### [152] [Enhancing Obsolescence Forecasting with Deep Generative Data Augmentation: A Semi-Supervised Framework for Low-Data Industrial Applications](https://arxiv.org/abs/2505.01261)
*Elie Saad,Mariem Besbes,Marc Zolghadri,Victor Czmil,Claude Baron,Vincent Bourgeois*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的电子元件过时预测框架，通过深度生成模型解决数据不足问题，并展示了在基准数据集上的先进性能。


<details>
  <summary>Details</summary>
Motivation: 电子元件过时问题在长生命周期系统中尤为关键，现有机器学习方法因数据不足而受限。

Method: 提出基于深度生成模型的框架，生成新数据以扩充训练集，并改进经典监督学习分类器以适应半监督学习。

Result: 在基准数据集上取得了先进的结果。

Conclusion: 该框架有效解决了数据不足问题，提升了过时预测的准确性。

Abstract: The challenge of electronic component obsolescence is particularly critical
in systems with long life cycles. Various obsolescence management methods are
employed to mitigate its impact, with obsolescence forecasting being a highly
sought-after and prominent approach. As a result, numerous machine
learning-based forecasting methods have been proposed. However, machine
learning models require a substantial amount of relevant data to achieve high
precision, which is lacking in the current obsolescence landscape in some
situations. This work introduces a novel framework for obsolescence forecasting
based on deep learning. The proposed framework solves the lack of available
data through deep generative modeling, where new obsolescence cases are
generated and used to augment the training dataset. The augmented dataset is
then used to train a classical machine learning-based obsolescence forecasting
model. To train classical forecasting models using augmented datasets, existing
classical supervised-learning classifiers are adapted for semi-supervised
learning within this framework. The proposed framework demonstrates
state-of-the-art results on benchmarking datasets.

</details>


### [153] [MultiGran-STGCNFog: Towards Accurate and High-Throughput Inference for Multi-Granular Spatiotemporal Traffic Forecasting](https://arxiv.org/abs/2505.01279)
*Zhaoyan Wang,Xiangchi Song,In-Young Ko*

Main category: cs.LG

TL;DR: 提出了一种名为MultiGran-STGCNFog的高效雾分布式推理系统，通过多粒度时空特征融合和动态交通图生成，改进了交通预测的准确性和速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于GCN的方法无法充分提取和融合多粒度时空特征，导致预测准确性不足，且模型复杂度高、推理时间长。

Method: 采用多粒度时空特征融合的动态交通图生成模型，结合优化调度算法GA-DPHDS，利用异构雾设备进行流水线推理。

Result: 在真实数据集上的实验表明，该方法在预测准确性和推理速度上优于基线模型。

Conclusion: MultiGran-STGCNFog通过多粒度特征融合和高效调度算法，显著提升了交通预测的性能和效率。

Abstract: Accurate traffic forecasting and swift inference provision are essential for
intelligent transportation systems. However, the present Graph Convolutional
Network (GCN)-based approaches cannot extract and fuse multi-granular
spatiotemporal features across various spatial and temporal scales
sufficiently, proven to yield less accurate forecasts. Besides, additional
feature extraction branches introduced in prior studies critically increased
model complexity and extended inference time, making it challenging to provide
fast inference for traffic forecasting. In this paper, we propose
MultiGran-STGCNFog, an efficient fog distributed inference system with a novel
traffic forecasting model that employs multi-granular spatiotemporal feature
fusion on generated dynamic traffic graphs to fully capture interdependent
traffic dynamics. The proposed scheduling algorithm GA-DPHDS, optimizing layer
execution order and layer-device scheduling scheme simultaneously, contributes
to considerable inference throughput improvement by leveraging heterogeneous
fog devices in a pipelined manner. Extensive experiments on real-world datasets
demonstrate the superiority of the proposed method over selected baselines.

</details>


### [154] [A Physics-preserved Transfer Learning Method for Differential Equations](https://arxiv.org/abs/2505.01281)
*Hao-Ran Yang,Chuan-Xian Ren*

Main category: cs.LG

TL;DR: 提出了一种物理保持最优张量传输（POTT）方法，用于解决数据驱动方法在微分方程中的领域偏移问题，同时保持物理信息。


<details>
  <summary>Details</summary>
Motivation: 数据驱动方法（如神经算子）在解决微分方程时存在领域偏移问题，现有迁移学习方法缺乏通用性或物理保持能力。

Method: 通过将数据域建模为乘积分布，提出POTT方法，自适应校正领域偏移并保持物理信息。

Result: 实验表明POTT方法在性能、通用性和物理保持方面表现优越。

Conclusion: POTT方法为微分方程问题提供了一种通用且物理保持的迁移学习解决方案。

Abstract: While data-driven methods such as neural operator have achieved great success
in solving differential equations (DEs), they suffer from domain shift problems
caused by different learning environments (with data bias or equation changes),
which can be alleviated by transfer learning (TL). However, existing TL methods
adopted in DEs problems lack either generalizability in general DEs problems or
physics preservation during training. In this work, we focus on a general
transfer learning method that adaptively correct the domain shift and preserve
physical information. Mathematically, we characterize the data domain as
product distribution and the essential problems as distribution bias and
operator bias. A Physics-preserved Optimal Tensor Transport (POTT) method that
simultaneously admits generalizability to common DEs and physics preservation
of specific problem is proposed to adapt the data-driven model to target domain
utilizing the push-forward distribution induced by the POTT map. Extensive
experiments demonstrate the superior performance, generalizability and physics
preservation of the proposed POTT method.

</details>


### [155] [2DXformer: Dual Transformers for Wind Power Forecasting with Dual Exogenous Variables](https://arxiv.org/abs/2505.01286)
*Yajuan Zhang,Jiahai Jiang,Yule Yan,Liang Yang,Ping Zhang*

Main category: cs.LG

TL;DR: 提出2DXformer模型，改进风电功率预测，通过分类输入变量并优化交互方式，提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在风电预测中未能充分建模变量间关系，且未区分内外生变量，导致模型复杂度和精度受限。

Method: 分类输入为外生静态、外生动态和内生变量，独立嵌入后通过注意力机制和多层感知机建模交互。

Result: 在两大真实数据集上验证，2DXformer进一步提升了预测性能。

Conclusion: 2DXformer通过优化变量交互，显著提升风电预测精度，代码已开源。

Abstract: Accurate wind power forecasting can help formulate scientific dispatch plans,
which is of great significance for maintaining the safety, stability, and
efficient operation of the power system. In recent years, wind power
forecasting methods based on deep learning have focused on extracting the
spatiotemporal correlations among data, achieving significant improvements in
forecasting accuracy. However, they exhibit two limitations. First, there is a
lack of modeling for the inter-variable relationships, which limits the
accuracy of the forecasts. Second, by treating endogenous and exogenous
variables equally, it leads to unnecessary interactions between the endogenous
and exogenous variables, increasing the complexity of the model. In this paper,
we propose the 2DXformer, which, building upon the previous work's focus on
spatiotemporal correlations, addresses the aforementioned two limitations.
Specifically, we classify the inputs of the model into three types: exogenous
static variables, exogenous dynamic variables, and endogenous variables. First,
we embed these variables as variable tokens in a channel-independent manner.
Then, we use the attention mechanism to capture the correlations among
exogenous variables. Finally, we employ a multi-layer perceptron with residual
connections to model the impact of exogenous variables on endogenous variables.
Experimental results on two real-world large-scale datasets indicate that our
proposed 2DXformer can further improve the performance of wind power
forecasting. The code is available in this repository:
\href{https://github.com/jseaj/2DXformer}{https://github.com/jseaj/2DXformer}.

</details>


### [156] [Integration of Multi-Mode Preference into Home Energy Management System Using Deep Reinforcement Learning](https://arxiv.org/abs/2505.01332)
*Mohammed Sumayli,Olugbenga Moses Anubi*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度强化学习的家庭能源管理系统（DRL-HEMS），通过动态多模式偏好优化能源消耗，提升用户参与度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究常将用户舒适度视为静态权重，忽略了行为的动态性。本文旨在通过动态偏好优化，增强用户在需求响应（DR）中的参与。

Method: 采用无模型单智能体深度强化学习算法，结合实时数据（电价、环境温度、设备功耗）优化能源管理。

Result: 模型在不同偏好模式下表现优异，计算效率优于传统混合整数线性规划（MILP）算法。

Conclusion: DRL-HEMS框架在动态优化和用户友好性方面具有显著优势，为智能家居能源管理提供了新思路。

Abstract: Home Energy Management Systems (HEMS) have emerged as a pivotal tool in the
smart home ecosystem, aiming to enhance energy efficiency, reduce costs, and
improve user comfort. By enabling intelligent control and optimization of
household energy consumption, HEMS plays a significant role in bridging the gap
between consumer needs and energy utility objectives. However, much of the
existing literature construes consumer comfort as a mere deviation from the
standard appliance settings. Such deviations are typically incorporated into
optimization objectives via static weighting factors. These factors often
overlook the dynamic nature of consumer behaviors and preferences. Addressing
this oversight, our paper introduces a multi-mode Deep Reinforcement
Learning-based HEMS (DRL-HEMS) framework, meticulously designed to optimize
based on dynamic, consumer-defined preferences. Our primary goal is to augment
consumer involvement in Demand Response (DR) programs by embedding dynamic
multi-mode preferences tailored to individual appliances. In this study, we
leverage a model-free, single-agent DRL algorithm to deliver a HEMS framework
that is not only dynamic but also user-friendly. To validate its efficacy, we
employed real-world data at 15-minute intervals, including metrics such as
electricity price, ambient temperature, and appliances' power consumption. Our
results show that the model performs exceptionally well in optimizing energy
consumption within different preference modes. Furthermore, when compared to
traditional algorithms based on Mixed-Integer Linear Programming (MILP), our
model achieves nearly optimal performance while outperforming in computational
efficiency.

</details>


### [157] [Enhancing Diversity in Parallel Agents: A Maximum State Entropy Exploration Story](https://arxiv.org/abs/2505.01336)
*Vincenzo De Paola,Riccardo Zamboni,Mirco Mutti,Marcello Restelli*

Main category: cs.LG

TL;DR: 论文提出了一种并行强化学习框架，通过最大化数据熵和平衡个体与群体多样性，超越了传统并行数据收集的N倍加速效果。


<details>
  <summary>Details</summary>
Motivation: 探索并行环境中通过策略专业化是否能够超越传统N倍加速的效果，以提升数据收集效率。

Method: 采用集中式策略梯度方法，平衡个体熵和群体多样性，减少数据冗余。

Result: 实验表明该方法优于相同代理系统，并能与批处理RL技术协同工作。

Conclusion: 通过理论分析和实证验证，证明了专业化并行采样分布能实现更快的收敛速度。

Abstract: Parallel data collection has redefined Reinforcement Learning (RL), unlocking
unprecedented efficiency and powering breakthroughs in large-scale real-world
applications. In this paradigm, $N$ identical agents operate in $N$ replicas of
an environment simulator, accelerating data collection by a factor of $N$. A
critical question arises: \textit{Does specializing the policies of the
parallel agents hold the key to surpass the $N$ factor acceleration?} In this
paper, we introduce a novel learning framework that maximizes the entropy of
collected data in a parallel setting. Our approach carefully balances the
entropy of individual agents with inter-agent diversity, effectively minimizing
redundancies. The latter idea is implemented with a centralized policy gradient
method, which shows promise when evaluated empirically against systems of
identical agents, as well as synergy with batch RL techniques that can exploit
data diversity. Finally, we provide an original concentration analysis that
shows faster rates for specialized parallel sampling distributions, which
supports our methodology and may be of independent interest.

</details>


### [158] [How to Learn a Star: Binary Classification with Starshaped Polyhedral Sets](https://arxiv.org/abs/2505.01346)
*Marie-Charlotte Brandenburg,Katharina Jochemko*

Main category: cs.LG

TL;DR: 论文研究了基于星形多面体决策边界的连续分段线性函数在二元分类中的表达能力，分析了两种损失函数（0/1损失和指数损失）的损失景观结构，包括VC维度的明确界限和最优解的唯一性条件。


<details>
  <summary>Details</summary>
Motivation: 探索星形多面体决策边界在二元分类中的表达能力，并理解其损失景观的几何和组合结构。

Method: 使用固定多面体单纯扇支持的连续分段线性函数，分析0/1损失和指数损失的损失景观，包括VC维度和最优解几何。

Result: 给出了VC维度的明确界限，描述了离散损失的子水平集为超平面排列中的腔室，并为指数损失提供了最优解唯一性的充分条件。

Conclusion: 星形多面体决策边界在二元分类中具有明确的表达能力，损失景观的结构可通过组合和几何方法分析。

Abstract: We consider binary classification restricted to a class of continuous
piecewise linear functions whose decision boundaries are (possibly nonconvex)
starshaped polyhedral sets, supported on a fixed polyhedral simplicial fan. We
investigate the expressivity of these function classes and describe the
combinatorial and geometric structure of the loss landscape, most prominently
the sublevel sets, for two loss-functions: the 0/1-loss (discrete loss) and an
exponential loss function. In particular, we give explicit bounds on the VC
dimension of this model, and concretely describe the sublevel sets of the
discrete loss as chambers in a hyperplane arrangement. For the exponential
loss, we give sufficient conditions for the optimum to be unique, and describe
the geometry of the optimum when varying the rate parameter of the underlying
exponential probability distribution.

</details>


### [159] [Learning Stabilizing Policies via an Unstable Subspace Representation](https://arxiv.org/abs/2505.01348)
*Leonardo F. Toso,Lintao Ye,James Anderson*

Main category: cs.LG

TL;DR: 论文提出了一种两阶段方法，通过学习系统的不稳定子空间并解决一系列折扣线性二次调节器问题，以降低控制空间的有效维度，从而减少样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究如何稳定线性时不变系统（LTI），解决现有方法需要大量数据且复杂度高的问题。

Method: 两阶段方法：1. 学习系统的不稳定子空间；2. 在子空间上解决折扣线性二次调节器问题，仅稳定不稳定动态。

Result: 非渐近保证显示，操作于不稳定子空间可显著减少样本复杂度，尤其在不稳定模式远小于状态维度时。

Conclusion: 该方法通过聚焦不稳定动态，有效加速了稳定化过程，数值实验验证了其样本复杂度的降低。

Abstract: We study the problem of learning to stabilize (LTS) a linear time-invariant
(LTI) system. Policy gradient (PG) methods for control assume access to an
initial stabilizing policy. However, designing such a policy for an unknown
system is one of the most fundamental problems in control, and it may be as
hard as learning the optimal policy itself. Existing work on the LTS problem
requires large data as it scales quadratically with the ambient dimension. We
propose a two-phase approach that first learns the left unstable subspace of
the system and then solves a series of discounted linear quadratic regulator
(LQR) problems on the learned unstable subspace, targeting to stabilize only
the system's unstable dynamics and reduce the effective dimension of the
control space. We provide non-asymptotic guarantees for both phases and
demonstrate that operating on the unstable subspace reduces sample complexity.
In particular, when the number of unstable modes is much smaller than the state
dimension, our analysis reveals that LTS on the unstable subspace substantially
speeds up the stabilization process. Numerical experiments are provided to
support this sample complexity reduction achieved by our approach.

</details>


### [160] [Stabilizing Temporal Difference Learning via Implicit Stochastic Approximation](https://arxiv.org/abs/2505.01361)
*Hwanwoo Kim,Panos Toulis,Eric Laber*

Main category: cs.LG

TL;DR: 隐式TD学习算法通过将TD更新重新表述为固定点方程，解决了传统TD学习对步长敏感的问题，提高了稳定性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统TD学习对步长选择敏感，导致估计误差大且收敛慢，需反复试验调整步长，耗时且低效。

Method: 提出隐式TD算法，将TD更新转化为固定点方程，减少对步长的依赖。

Result: 理论分析表明隐式TD具有渐近收敛性和有限时间误差界限，实际应用中表现稳健高效。

Conclusion: 隐式TD算法为策略评估和值近似提供了更稳定且实用的工具。

Abstract: Temporal Difference (TD) learning is a foundational algorithm in
reinforcement learning (RL). For nearly forty years, TD learning has served as
a workhorse for applied RL as well as a building block for more complex and
specialized algorithms. However, despite its widespread use, it is not without
drawbacks, the most prominent being its sensitivity to step size. A poor choice
of step size can dramatically inflate the error of value estimates and slow
convergence. Consequently, in practice, researchers must use trial and error in
order to identify a suitable step size -- a process that can be tedious and
time consuming. As an alternative, we propose implicit TD algorithms that
reformulate TD updates into fixed-point equations. These updates are more
stable and less sensitive to step size without sacrificing computational
efficiency. Moreover, our theoretical analysis establishes asymptotic
convergence guarantees and finite-time error bounds. Our results demonstrate
their robustness and practicality for modern RL tasks, establishing implicit TD
as a versatile tool for policy evaluation and value approximation.

</details>


### [161] [Carbon Aware Transformers Through Joint Model-Hardware Optimization](https://arxiv.org/abs/2505.01386)
*Irene Wang,Newsha Ardalani,Mostafa Elhoushi,Daniel Jiang,Samuel Hsia,Ekin Sumbul,Divya Mahajan,Carole-Jean Wu,Bilge Acun*

Main category: cs.LG

TL;DR: CATransformers是一个碳感知架构搜索框架，旨在全面优化ML系统的碳足迹，包括运行和硬件制造碳排放。


<details>
  <summary>Details</summary>
Motivation: 随着ML系统的快速发展，其环境影响（尤其是碳足迹）需更全面评估，但目前缺乏量化与优化工具。

Method: 提出CATransformers框架，结合运行和硬件碳排放指标，在早期设计空间探索中实现可持续优化。

Result: 应用于CLIP模型，CarbonCLIP系列减少17%总碳排放，同时保持准确性和延迟。

Conclusion: 需采用整体优化方法设计高性能且环保的AI系统。

Abstract: The rapid growth of machine learning (ML) systems necessitates a more
comprehensive evaluation of their environmental impact, particularly their
carbon footprint, which comprises operational carbon from training and
inference execution and embodied carbon from hardware manufacturing and its
entire life-cycle. Despite the increasing importance of embodied emissions,
there is a lack of tools and frameworks to holistically quantify and optimize
the total carbon footprint of ML systems. To address this, we propose
CATransformers, a carbon-aware architecture search framework that enables
sustainability-driven co-optimization of ML models and hardware architectures.
By incorporating both operational and embodied carbon metrics into early design
space exploration of domain-specific hardware accelerators, CATransformers
demonstrates that optimizing for carbon yields design choices distinct from
those optimized solely for latency or energy efficiency. We apply our framework
to multi-modal CLIP-based models, producing CarbonCLIP, a family of CLIP models
achieving up to 17% reduction in total carbon emissions while maintaining
accuracy and latency compared to state-of-the-art edge small CLIP baselines.
This work underscores the need for holistic optimization methods to design
high-performance, environmentally sustainable AI systems.

</details>


### [162] [Learning and Transferring Physical Models through Derivatives](https://arxiv.org/abs/2505.01391)
*Alessandro Trenta,Andrea Cossu,Davide Bacciu*

Main category: cs.LG

TL;DR: DERL是一种监督学习方法，通过学习物理系统的偏导数建模，并通过蒸馏协议增量构建物理模型，理论保证其与物理定律一致，且在泛化ODE和PDE时优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以增量构建物理模型并保证与物理定律一致，DERL旨在解决这一问题。

Method: 通过监督学习建模偏导数，设计蒸馏协议增量构建模型，理论分析保证一致性。

Result: DERL在泛化ODE和PDE时优于现有方法，并能跨模型传递物理知识。

Conclusion: DERL首次实现多阶段增量构建物理模型，为物理建模提供新思路。

Abstract: We propose Derivative Learning (DERL), a supervised approach that models
physical systems by learning their partial derivatives. We also leverage DERL
to build physical models incrementally, by designing a distillation protocol
that effectively transfers knowledge from a pre-trained to a student model. We
provide theoretical guarantees that our approach can learn the true physical
system, being consistent with the underlying physical laws, even when using
empirical derivatives. DERL outperforms state-of-the-art methods in
generalizing an ODE to unseen initial conditions and a parametric PDE to unseen
parameters. We finally propose a method based on DERL to transfer physical
knowledge across models by extending them to new portions of the physical
domain and new range of PDE parameters. We believe this is the first attempt at
building physical models incrementally in multiple stages.

</details>


### [163] [Predicting the Price of Gold in the Financial Markets Using Hybrid Models](https://arxiv.org/abs/2505.01402)
*Mohammadhossein Rashidi,Mohammad Modarres*

Main category: cs.LG

TL;DR: 论文提出了一种结合ARIMA、逐步回归和神经网络的混合模型（ARIMA_Stepwise Regression_Neural Network），用于预测黄金价格，并验证其优于传统时间序列方法。


<details>
  <summary>Details</summary>
Motivation: 资本市场的参与者和研究者对高精度价格预测的需求迫切，现有模型难以满足这一需求。

Method: 结合ARIMA时间序列模型、逐步回归筛选变量，并将结果输入神经网络进行预测。

Result: 混合模型在预测黄金价格时表现出最高的准确性，优于传统时间序列和逐步回归方法。

Conclusion: 该混合模型可扩展用于预测其他金融资产，具有广泛的应用潜力。

Abstract: Predicting the price that has the least error and can provide the best and
highest accuracy has been one of the most challenging issues and one of the
most critical concerns among capital market activists and researchers.
Therefore, a model that can solve problems and provide results with high
accuracy is one of the topics of interest among researchers. In this project,
using time series prediction models such as ARIMA to estimate the price,
variables, and indicators related to technical analysis show the behavior of
traders involved in involving psychological factors for the model. By linking
all of these variables to stepwise regression, we identify the best variables
influencing the prediction of the variable. Finally, we enter the selected
variables as inputs to the artificial neural network. In other words, we want
to call this whole prediction process the "ARIMA_Stepwise Regression_Neural
Network" model and try to predict the price of gold in international financial
markets. This approach is expected to be able to be used to predict the types
of stocks, commodities, currency pairs, financial market indicators, and other
items used in local and international financial markets. Moreover, a comparison
between the results of this method and time series methods is also expressed.
Finally, based on the results, it can be seen that the resulting hybrid model
has the highest accuracy compared to the time series method, regression, and
stepwise regression.

</details>


### [164] [How Effective are Large Time Series Models in Hydrology? A Study on Water Level Forecasting in Everglades](https://arxiv.org/abs/2505.01415)
*Rahuul Rangaraj,Jimeng Shi,Azam Shirali,Rajendra Paudel,Yanzhao Wu,Giri Narasimhan*

Main category: cs.LG

TL;DR: 研究探讨了时间序列基础模型在预测Everglades水位中的应用，发现Chronos表现最佳，而其他基础模型表现较差，任务特定模型性能因架构而异。


<details>
  <summary>Details</summary>
Motivation: 传统物理和统计方法在预测水位时面临高计算成本和适应性不足的问题，而时间序列基础模型的潜力尚未在关键环境系统中充分探索。

Method: 研究了12种任务特定模型和5种时间序列基础模型，应用于Everglades水位预测。

Result: Chronos显著优于其他模型，其他基础模型表现不佳，任务特定模型性能因架构而异。

Conclusion: 研究填补了时间序列基础模型在环境系统中的应用空白，并探讨了模型性能差异的原因。

Abstract: The Everglades play a crucial role in flood and drought regulation, water
resource planning, and ecosystem management in the surrounding regions.
However, traditional physics-based and statistical methods for predicting water
levels often face significant challenges, including high computational costs
and limited adaptability to diverse or unforeseen conditions. Recent
advancements in large time series models have demonstrated the potential to
address these limitations, with state-of-the-art deep learning and foundation
models achieving remarkable success in time series forecasting across various
domains. Despite this progress, their application to critical environmental
systems, such as the Everglades, remains underexplored. In this study, we fill
the gap by investigating twelve task-specific models and five time series
foundation models across six categories for a real-world application focused on
water level prediction in the Everglades. Our primary results show that the
foundation model, Chronos, significantly outperforms all other models while the
remaining foundation models exhibit relatively poor performance. Moreover, the
performance of task-specific models varies with the model architectures.
Lastly, we discuss the possible reasons for the varying performance of models.

</details>


### [165] [Evaluating Frontier Models for Stealth and Situational Awareness](https://arxiv.org/abs/2505.01420)
*Mary Phuong,Roland S. Zimmermann,Ziyue Wang,David Lindner,Victoria Krakovna,Sarah Cogan,Allan Dafoe,Lewis Ho,Rohin Shah*

Main category: cs.LG

TL;DR: 论文提出了一套评估AI模型是否具备潜在阴谋行为的测试方法，包括隐蔽性和情境意识，并验证当前前沿模型未表现出相关风险。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型可能存在与开发者意图不符的隐蔽行为，需在部署前排除此类风险。

Method: 设计了16项评估，包括5项隐蔽性测试和11项情境意识测试，用于衡量模型是否具备阴谋行为的先决能力。

Result: 当前前沿模型在隐蔽性和情境意识测试中均未表现出显著风险。

Conclusion: 通过评估可证明模型不具备阴谋行为的能力，从而确保部署安全。

Abstract: Recent work has demonstrated the plausibility of frontier AI models scheming
-- knowingly and covertly pursuing an objective misaligned with its developer's
intentions. Such behavior could be very hard to detect, and if present in
future advanced systems, could pose severe loss of control risk. It is
therefore important for AI developers to rule out harm from scheming prior to
model deployment. In this paper, we present a suite of scheming reasoning
evaluations measuring two types of reasoning capabilities that we believe are
prerequisites for successful scheming: First, we propose five evaluations of
ability to reason about and circumvent oversight (stealth). Second, we present
eleven evaluations for measuring a model's ability to instrumentally reason
about itself, its environment and its deployment (situational awareness). We
demonstrate how these evaluations can be used as part of a scheming inability
safety case: a model that does not succeed on these evaluations is almost
certainly incapable of causing severe harm via scheming in real deployment. We
run our evaluations on current frontier models and find that none of them show
concerning levels of either situational awareness or stealth.

</details>


### [166] [Computational, Data-Driven, and Physics-Informed Machine Learning Approaches for Microstructure Modeling in Metal Additive Manufacturing](https://arxiv.org/abs/2505.01424)
*D. Patel,R. Sharma,Y. B. Guo*

Main category: cs.LG

TL;DR: 论文探讨了金属增材制造中微观结构预测的挑战，分析了实验、计算和数据驱动方法的优缺点，并强调了物理信息机器学习（PIML）的潜力。


<details>
  <summary>Details</summary>
Motivation: 金属增材制造过程中快速熔化和凝固导致非平衡微观结构，影响机械性能，预测微观结构是优化工艺和减少缺陷的关键。

Method: 论文综合评估了实验、计算和数据驱动方法，并重点介绍了结合物理知识与机器学习的PIML框架。

Result: PIML方法在准确性、透明度和数据效率方面表现优异，为微观结构建模提供了新方向。

Conclusion: PIML混合方法在实现可预测、可扩展且物理一致的微观结构建模方面具有重要价值，有助于高性能增材制造组件的可靠生产。

Abstract: Metal additive manufacturing enables unprecedented design freedom and the
production of customized, complex components. However, the rapid melting and
solidification dynamics inherent to metal AM processes generate heterogeneous,
non-equilibrium microstructures that significantly impact mechanical properties
and subsequent functionality. Predicting microstructure and its evolution
across spatial and temporal scales remains a central challenge for process
optimization and defect mitigation. While conventional experimental techniques
and physics-based simulations provide a physical foundation and valuable
insights, they face critical limitations. In contrast, data-driven machine
learning offers an alternative prediction approach and powerful pattern
recognition but often operate as black-box, lacking generalizability and
physical consistency. To overcome these limitations, physics-informed machine
learning, including physics-informed neural networks, has emerged as a
promising paradigm by embedding governing physical laws into neural network
architectures, thereby enhancing accuracy, transparency, data efficiency, and
extrapolation capabilities. This work presents a comprehensive evaluation of
modeling strategies for microstructure prediction in metal AM. The strengths
and limitations of experimental, computational, and data-driven methods are
analyzed in depth, and highlight recent advances in hybrid PIML frameworks that
integrate physical knowledge with ML. Key challenges, such as data scarcity,
multi-scale coupling, and uncertainty quantification, are discussed alongside
future directions. Ultimately, this assessment underscores the importance of
PIML-based hybrid approaches in enabling predictive, scalable, and physically
consistent microstructure modeling for site-specific, microstructure-aware
process control and the reliable production of high-performance AM components.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [167] [A Survey on 3D Reconstruction Techniques in Plant Phenotyping: From Classical Methods to Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and Beyond](https://arxiv.org/abs/2505.00737)
*Jiajia Li,Xinda Qi,Seyed Hamidreza Nabaei,Meiqi Liu,Dong Chen,Xin Zhang,Xunyuan Yin,Zhaojian Li*

Main category: eess.IV

TL;DR: 综述了植物表型分析中的3D重建技术，包括经典方法、NeRF和3DGS，探讨了它们的优缺点及未来前景。


<details>
  <summary>Details</summary>
Motivation: 3D重建技术为植物表型分析提供了精确和自动化的工具，有助于推动精准农业和作物改良。

Method: 回顾了经典重建方法、NeRF和3DGS的技术原理、应用及性能。

Result: 经典方法简单灵活但面临数据密度和噪声问题；NeRF能高质量重建但计算成本高；3DGS在效率和扩展性上具潜力。

Conclusion: 多样化的3D重建技术有望推动自动化高通量植物表型分析，助力农业技术发展。

Abstract: Plant phenotyping plays a pivotal role in understanding plant traits and
their interactions with the environment, making it crucial for advancing
precision agriculture and crop improvement. 3D reconstruction technologies have
emerged as powerful tools for capturing detailed plant morphology and
structure, offering significant potential for accurate and automated
phenotyping. This paper provides a comprehensive review of the 3D
reconstruction techniques for plant phenotyping, covering classical
reconstruction methods, emerging Neural Radiance Fields (NeRF), and the novel
3D Gaussian Splatting (3DGS) approach. Classical methods, which often rely on
high-resolution sensors, are widely adopted due to their simplicity and
flexibility in representing plant structures. However, they face challenges
such as data density, noise, and scalability. NeRF, a recent advancement,
enables high-quality, photorealistic 3D reconstructions from sparse viewpoints,
but its computational cost and applicability in outdoor environments remain
areas of active research. The emerging 3DGS technique introduces a new paradigm
in reconstructing plant structures by representing geometry through Gaussian
primitives, offering potential benefits in both efficiency and scalability. We
review the methodologies, applications, and performance of these approaches in
plant phenotyping and discuss their respective strengths, limitations, and
future prospects (https://github.com/JiajiaLi04/3D-Reconstruction-Plants).
Through this review, we aim to provide insights into how these diverse 3D
reconstruction techniques can be effectively leveraged for automated and
high-throughput plant phenotyping, contributing to the next generation of
agricultural technology.

</details>


### [168] [Leveraging Depth and Attention Mechanisms for Improved RGB Image Inpainting](https://arxiv.org/abs/2505.00735)
*Jin Hyun Park,Harine Choi,Praewa Pitiphat*

Main category: eess.IV

TL;DR: 提出了一种结合RGB和深度图像的双编码器架构，用于图像修复，通过注意力机制融合特征，显著提升了修复质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖RGB图像，忽视了深度信息对场景空间和结构理解的重要性。

Method: 采用双编码器分别处理RGB和深度图像，通过注意力机制融合特征，并使用不同掩码策略测试模型鲁棒性。

Result: 结合深度信息的模型在定性和定量评估中均优于基线，注意力机制进一步提升了性能。

Conclusion: 深度信息的引入显著改善了图像修复质量，注意力机制是关键因素。

Abstract: Existing deep learning-based image inpainting methods typically rely on
convolutional networks with RGB images to reconstruct images. However, relying
exclusively on RGB images may neglect important depth information, which plays
a critical role in understanding the spatial and structural context of a scene.
Just as human vision leverages stereo cues to perceive depth, incorporating
depth maps into the inpainting process can enhance the model's ability to
reconstruct images with greater accuracy and contextual awareness. In this
paper, we propose a novel approach that incorporates both RGB and depth images
for enhanced image inpainting. Our models employ a dual encoder architecture,
where one encoder processes the RGB image and the other handles the depth
image. The encoded features from both encoders are then fused in the decoder
using an attention mechanism, effectively integrating the RGB and depth
representations. We use two different masking strategies, line and square, to
test the robustness of the model under different types of occlusions. To
further analyze the effectiveness of our approach, we use Gradient-weighted
Class Activation Mapping (Grad-CAM) visualizations to examine the regions of
interest the model focuses on during inpainting. We show that incorporating
depth information alongside the RGB image significantly improves the
reconstruction quality. Through both qualitative and quantitative comparisons,
we demonstrate that the depth-integrated model outperforms the baseline, with
attention mechanisms further enhancing inpainting performance, as evidenced by
multiple evaluation metrics and visualization.

</details>


### [169] [Can Foundation Models Really Segment Tumors? A Benchmarking Odyssey in Lung CT Imaging](https://arxiv.org/abs/2505.01239)
*Elena Mulero Ayllón,Massimiliano Mantegna,Linlin Shen,Paolo Soda,Valerio Guarrasi,Matteo Tortora*

Main category: eess.IV

TL;DR: 论文比较了多种深度学习模型在肺肿瘤分割中的表现，发现基础模型（如MedSAM~2）在准确性和计算效率上优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 肺肿瘤分割的复杂性对自动化分割提出了挑战，研究旨在评估不同深度学习模型的性能。

Method: 通过比较U-Net、DeepLabV3、nnUNet和基础模型（如MedSAM~2）在肺肿瘤数据集上的表现，评估其准确性和计算效率。

Result: 基础模型（尤其是MedSAM~2）在准确性和计算效率上优于传统模型。

Conclusion: 基础模型在肺肿瘤分割中具有潜力，可改善临床工作流程和患者预后。

Abstract: Accurate lung tumor segmentation is crucial for improving diagnosis,
treatment planning, and patient outcomes in oncology. However, the complexity
of tumor morphology, size, and location poses significant challenges for
automated segmentation. This study presents a comprehensive benchmarking
analysis of deep learning-based segmentation models, comparing traditional
architectures such as U-Net and DeepLabV3, self-configuring models like nnUNet,
and foundation models like MedSAM, and MedSAM~2. Evaluating performance across
two lung tumor segmentation datasets, we assess segmentation accuracy and
computational efficiency under various learning paradigms, including few-shot
learning and fine-tuning. The results reveal that while traditional models
struggle with tumor delineation, foundation models, particularly MedSAM~2,
outperform them in both accuracy and computational efficiency. These findings
underscore the potential of foundation models for lung tumor segmentation,
highlighting their applicability in improving clinical workflows and patient
outcomes.

</details>


### [170] [XeMap: Contextual Referring in Large-Scale Remote Sensing Environments](https://arxiv.org/abs/2505.00738)
*Yuxi Li,Lu Si,Yujie Hou,Chengaung Liu,Bin Li,Hongjian Fang,Jun Zhang*

Main category: eess.IV

TL;DR: 论文提出了XeMap任务，专注于遥感图像中文本引用区域的中尺度语义定位，并设计了XeMap-Network架构，通过多模态对齐和注意力机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像方法（如图像级标注/检索和对象级检测/分割）难以捕捉中尺度语义实体，限制了大规模场景的解释能力。

Method: 提出XeMap-Network架构，包含融合层（自注意力和交叉注意力机制）和HMSA模块（多尺度视觉特征与文本语义对齐）。

Result: 在零样本设置下，XeMap-Network优于现有方法，证明了其在精准映射引用区域方面的有效性。

Conclusion: XeMap任务和XeMap-Network为遥感图像的中尺度语义解释提供了新思路和工具。

Abstract: Advancements in remote sensing (RS) imagery have provided high-resolution
detail and vast coverage, yet existing methods, such as image-level
captioning/retrieval and object-level detection/segmentation, often fail to
capture mid-scale semantic entities essential for interpreting large-scale
scenes. To address this, we propose the conteXtual referring Map (XeMap) task,
which focuses on contextual, fine-grained localization of text-referred regions
in large-scale RS scenes. Unlike traditional approaches, XeMap enables precise
mapping of mid-scale semantic entities that are often overlooked in image-level
or object-level methods. To achieve this, we introduce XeMap-Network, a novel
architecture designed to handle the complexities of pixel-level cross-modal
contextual referring mapping in RS. The network includes a fusion layer that
applies self- and cross-attention mechanisms to enhance the interaction between
text and image embeddings. Furthermore, we propose a Hierarchical Multi-Scale
Semantic Alignment (HMSA) module that aligns multiscale visual features with
the text semantic vector, enabling precise multimodal matching across
large-scale RS imagery. To support XeMap task, we provide a novel, annotated
dataset, XeMap-set, specifically tailored for this task, overcoming the lack of
XeMap datasets in RS imagery. XeMap-Network is evaluated in a zero-shot setting
against state-of-the-art methods, demonstrating superior performance. This
highlights its effectiveness in accurately mapping referring regions and
providing valuable insights for interpreting large-scale RS environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [171] [CIMFlow: An Integrated Framework for Systematic Design and Evaluation of Digital CIM Architectures](https://arxiv.org/abs/2505.01107)
*Yingjie Qi,Jianlei Yang,Yiou Wang,Yikun Wang,Dayu Wang,Ling Tang,Cenlin Duan,Xiaolin He,Weisheng Zhao*

Main category: cs.AR

TL;DR: CIMFlow是一个集成框架，用于在数字计算内存（CIM）架构上实现和评估DNN工作负载，解决了现有工具在软硬件设计空间和容量约束方面的不足。


<details>
  <summary>Details</summary>
Motivation: 数字CIM架构在DNN加速中潜力巨大，但缺乏全面的工具支持软硬件设计空间和容量约束，阻碍了其开发和优化。

Method: CIMFlow通过灵活的ISA设计将编译和仿真基础设施结合，并在编译流程中采用高级分区和并行策略解决数字CIM的约束。

Result: 评估表明，CIMFlow支持跨多样化配置的系统化原型设计和优化，为研究人员提供了广泛设计空间探索的平台。

Conclusion: CIMFlow为数字CIM架构的研究和设计提供了高效且易用的工具，推动了该领域的进一步发展。

Abstract: Digital Compute-in-Memory (CIM) architectures have shown great promise in
Deep Neural Network (DNN) acceleration by effectively addressing the "memory
wall" bottleneck. However, the development and optimization of digital CIM
accelerators are hindered by the lack of comprehensive tools that encompass
both software and hardware design spaces. Moreover, existing design and
evaluation frameworks often lack support for the capacity constraints inherent
in digital CIM architectures. In this paper, we present CIMFlow, an integrated
framework that provides an out-of-the-box workflow for implementing and
evaluating DNN workloads on digital CIM architectures. CIMFlow bridges the
compilation and simulation infrastructures with a flexible instruction set
architecture (ISA) design, and addresses the constraints of digital CIM through
advanced partitioning and parallelism strategies in the compilation flow. Our
evaluation demonstrates that CIMFlow enables systematic prototyping and
optimization of digital CIM architectures across diverse configurations,
providing researchers and designers with an accessible platform for extensive
design space exploration.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [172] [EnviKal-Loc: Sub-10m Indoor LoRaWAN Localization using an Environmental-Aware Path Loss and Adaptive RSSI Smoothing](https://arxiv.org/abs/2505.01185)
*Nahshon Mokua Obiri,Kristof Van Laerhoven*

Main category: cs.NI

TL;DR: 论文提出了一种结合自适应滤波与扩展对数距离多墙路径损耗和阴影模型的轻量级方法，用于提升LoRaWAN在复杂室内环境中的定位精度。


<details>
  <summary>Details</summary>
Motivation: LoRaWAN技术在大规模物联网部署中具有广泛覆盖优势，但在复杂室内环境中实现亚10米精确定位仍具挑战性。

Method: 结合自适应卡尔曼滤波与扩展对数距离多墙路径损耗模型，并引入环境参数（如温度、湿度等）和LoRaWAN关键参数（如RSSI、SNR等）。

Result: 提出的MWM-EP-KF模型平均绝对误差为5.81米，显著优于基线模型（17.98米）和环境增强模型（10.56米）。

Conclusion: 该方法为动态变化环境中的精确室内LoRaWAN定位提供了高效且可解释的解决方案。

Abstract: LoRaWAN technology's extensive coverage positions it as a strong contender
for large-scale IoT deployments. However, achieving sub-10 m accuracy in indoor
localization remains challenging due to complex environmental conditions,
multipath fading, and transient obstructions. This paper proposes a lightweight
but robust approach combining adaptive filtering with an extended log-distance,
multi-wall path loss and shadowing (PLS) model. Our methodology augments
conventional models with critical LoRaWAN parameters (received signal strength
indicator (RSSI), frequency, and signal-to-noise ratio (SNR)) and dynamic
environmental indicators (temperature, humidity, carbon dioxide, particulate
matter, and barometric pressure). An adaptive Kalman filter reduces RSSI
fluctuations, isolating persistent trends from momentary noise. Using a
six-month dataset of 1,328,334 field measurements, we evaluate three models:
the baseline COST 231 multi-wall model (MWM), the baseline model augmented with
environmental parameters (MWM-EP), and a forward-only adaptive Kalman-filtered
RSSI version of the latter (MWM-EP-KF). Results confirm that the MWM-EP-KF
achieves a mean absolute error (MAE) of 5.81 m, outperforming both the MWM-EP
(10.56 m) and the baseline MWM framework (17.98 m). Environmental augmentation
reduces systematic errors by 41.22%, while Kalman filtering significantly
enhances robustness under high RSSI volatility by 42.63%, on average across all
devices. These findings present an interpretable, efficient solution for
precise indoor LoRaWAN localization in dynamically changing environments.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [173] [Dynamic and Distributed Routing in IoT Networks based on Multi-Objective Q-Learning](https://arxiv.org/abs/2505.00918)
*Shubham Vaishnav,Praveen Kumar Donta,Sindri Magnússon*

Main category: cs.DC

TL;DR: 提出了一种基于多目标Q学习的动态分布式路由算法，以适应物联网中实时变化的优先级需求。


<details>
  <summary>Details</summary>
Motivation: 物联网应用需求动态变化，现有路由协议难以适应快速变化的优先级（如低延迟与能效的权衡）。

Method: 结合多目标优化和Q学习，提出动态分布式路由算法，并引入贪心插值策略以应对突发偏好变化。

Result: 仿真表明，该算法在整体奖励、能效和包交付率等指标上优于现有方法。

Conclusion: 该算法能快速适应动态偏好，为物联网路由提供了高效解决方案。

Abstract: The last few decades have witnessed a rapid increase in IoT devices owing to
their wide range of applications, such as smart healthcare monitoring systems,
smart cities, and environmental monitoring. A critical task in IoT networks is
sensing and transmitting information over the network. The IoT nodes gather
data by sensing the environment and then transmit this data to a destination
node via multi-hop communication, following some routing protocols. These
protocols are usually designed to optimize possibly contradictory objectives,
such as maximizing packet delivery ratio and energy efficiency. While most
literature has focused on optimizing a static objective that remains unchanged,
many real-world IoT applications require adapting to rapidly shifting
priorities. For example, in monitoring systems, some transmissions are
time-critical and require a high priority on low latency, while other
transmissions are less urgent and instead prioritize energy efficiency. To meet
such dynamic demands, we propose novel dynamic and distributed routing based on
multiobjective Q-learning that can adapt to changes in preferences in
real-time. Our algorithm builds on ideas from both multi-objective optimization
and Q-learning. We also propose a novel greedy interpolation policy scheme to
take near-optimal decisions for unexpected preference changes. The proposed
scheme can approximate and utilize the Pareto-efficient solutions for dynamic
preferences, thus utilizing past knowledge to adapt to unpredictable
preferences quickly during runtime. Simulation results show that the proposed
scheme outperforms state-of-the-art algorithms for various exploration
strategies, preference variation patterns, and important metrics like overall
reward, energy efficiency, and packet delivery ratio.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [174] [Primality Testing via Circulant Matrix Eigenvalue Structure: A Novel Approach Using Cyclotomic Field Theory](https://arxiv.org/abs/2505.00730)
*Marius-Constantin Dinu*

Main category: cs.SC

TL;DR: 本文提出了一种基于循环矩阵特征值结构的新型素数测试方法，通过证明整数n>2为素数的充要条件是其循环矩阵的最小多项式在有理数域上有两个不可约因子。


<details>
  <summary>Details</summary>
Motivation: 将分圆域理论与矩阵代数结合，揭示素数与合数在特征值模式上的根本区别，从而提供一种确定性素数测试方法。

Method: 利用循环矩阵C_n=W_n+W_n^2的特征值模式，结合分圆多项式、伽罗瓦理论和原始单位根的关系，设计素数测试算法。

Result: 实验验证了该方法在多个整数范围内的有效性，并分析了其计算复杂度，表明其作为一种确定性方法的性能优势。

Conclusion: 该方法不仅提供了理论洞察，还具有实际应用价值，为素数测试提供了新的代数基础框架。

Abstract: This paper presents a novel primality test based on the eigenvalue structure
of circulant matrices constructed from roots of unity. We prove that an integer
$n > 2$ is prime if and only if the minimal polynomial of the circulant matrix
$C_n = W_n + W_n^2$ has exactly two irreducible factors over $\mathbb{Q}$. This
characterization connects cyclotomic field theory with matrix algebra,
providing both theoretical insights and practical applications. We demonstrate
that the eigenvalue patterns of these matrices reveal fundamental distinctions
between prime and composite numbers, leading to a deterministic primality test.
Our approach leverages the relationship between primitive roots of unity,
Galois theory, and the factorization of cyclotomic polynomials. We provide
comprehensive experimental validation across various ranges of integers,
discuss practical implementation considerations, and analyze the computational
complexity of our method in comparison with established primality tests. The
visual interpretation of our mathematical framework provides intuitive
understanding of the algebraic structures that distinguish prime numbers. Our
experimental validation demonstrates that our approach offers a deterministic
alternative to existing methods, with performance characteristics reflecting
its algebraic foundations.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [175] [Attack and defense techniques in large language models: A survey and new perspectives](https://arxiv.org/abs/2505.00976)
*Zhiyu Liao,Kang Chen,Yuanguo Lin,Kangkang Li,Yunxuan Liu,Hefeng Chen,Xingwang Huang,Yuanhui Yu*

Main category: cs.CR

TL;DR: 本文系统调查了大型语言模型（LLMs）的攻击与防御技术，分类攻击类型并分析防御策略，指出当前挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: LLMs在自然语言处理中广泛应用，但其安全性和伦理问题日益突出，需系统研究攻击与防御技术以应对动态威胁。

Method: 分类攻击类型（如对抗性提示攻击、模型窃取等），分析防御策略（预防性和检测性方法），并探讨挑战与未来方向。

Result: 总结了攻击机制与防御方法的现状，指出适应性防御、可解释安全技术和标准化评估框架是未来重点。

Conclusion: 强调跨学科合作与伦理考量对开发安全LLMs的重要性，为实际应用中的风险缓解提供方向。

Abstract: Large Language Models (LLMs) have become central to numerous natural language
processing tasks, but their vulnerabilities present significant security and
ethical challenges. This systematic survey explores the evolving landscape of
attack and defense techniques in LLMs. We classify attacks into adversarial
prompt attack, optimized attacks, model theft, as well as attacks on
application of LLMs, detailing their mechanisms and implications. Consequently,
we analyze defense strategies, including prevention-based and detection-based
defense methods. Although advances have been made, challenges remain to adapt
to the dynamic threat landscape, balance usability with robustness, and address
resource constraints in defense implementation. We highlight open problems,
including the need for adaptive scalable defenses, explainable security
techniques, and standardized evaluation frameworks. This survey provides
actionable insights and directions for developing secure and resilient LLMs,
emphasizing the importance of interdisciplinary collaboration and ethical
considerations to mitigate risks in real-world applications.

</details>


### [176] [Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from Large Language Models](https://arxiv.org/abs/2505.00817)
*Andrew Adiletta,Berk Sunar*

Main category: cs.CR

TL;DR: 论文提出了一种名为“Spill The Beans”的新型缓存侧信道攻击方法，用于泄露大型语言模型（LLM）生成的令牌。通过共享硬件资源，攻击者可以检测缓存命中，从而恢复令牌。实验表明，该方法能高效泄露高熵API密钥和英文文本。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，共享硬件资源的侧信道攻击对机密性构成严重威胁。本文旨在探索LLM在缓存侧信道攻击中的新漏洞。

Method: 攻击者通过共置攻击进程与受害者模型在同一硬件上，利用缓存侧信道技术（如flush+reload）检测令牌生成时的缓存命中。通过平衡监控令牌数量与信息泄露量，优化攻击效果。

Result: 实验证明，攻击者能恢复80%-90%的高熵API密钥，以及40%的英文文本。攻击效果取决于监控的令牌集和目标领域。

Conclusion: 本文揭示了LLM在缓存侧信道攻击中的新漏洞，强调了隐私和安全问题，并提出了缓解措施的建议。

Abstract: Side-channel attacks on shared hardware resources increasingly threaten
confidentiality, especially with the rise of Large Language Models (LLMs). In
this work, we introduce Spill The Beans, a novel application of cache
side-channels to leak tokens generated by an LLM. By co-locating an attack
process on the same hardware as the victim model, we flush and reload embedding
vectors from the embedding layer, where each token corresponds to a unique
embedding vector. When accessed during token generation, it results in a cache
hit detectable by our attack on shared lower-level caches.
  A significant challenge is the massive size of LLMs, which, by nature of
their compute intensive operation, quickly evicts embedding vectors from the
cache. We address this by balancing the number of tokens monitored against the
amount of information leaked. Monitoring more tokens increases potential
vocabulary leakage but raises the chance of missing cache hits due to eviction;
monitoring fewer tokens improves detection reliability but limits vocabulary
coverage.
  Through extensive experimentation, we demonstrate the feasibility of leaking
tokens from LLMs via cache side-channels. Our findings reveal a new
vulnerability in LLM deployments, highlighting that even sophisticated models
are susceptible to traditional side-channel attacks. We discuss the
implications for privacy and security in LLM-serving infrastructures and
suggest considerations for mitigating such threats. For proof of concept we
consider two concrete attack scenarios: Our experiments show that an attacker
can recover as much as 80%-90% of a high entropy API key with single shot
monitoring. As for English text we can reach a 40% recovery rate with a single
shot. We should note that the rate highly depends on the monitored token set
and these rates can be improved by targeting more specialized output domains.

</details>


### [177] [From Texts to Shields: Convergence of Large Language Models and Cybersecurity](https://arxiv.org/abs/2505.00841)
*Tao Li,Ya-Ting Yang,Yunian Pan,Quanyan Zhu*

Main category: cs.CR

TL;DR: 报告探讨了大型语言模型（LLMs）与网络安全的融合，分析了其在软件和网络安全、5G漏洞分析等领域的应用，并提出了解决相关挑战的策略。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在网络安全中的潜力，以及如何应对其部署中的信任、透明度和伦理问题。

Method: 综合网络安全、人工智能、形式化方法和人本设计的跨学科见解，提出人机协作系统、角色特定训练等方法。

Result: LLMs能自动化复杂任务、提升效率，并支持推理驱动的安全分析，但需解决可解释性、安全性和公平性等挑战。

Conclusion: 报告为LLMs在网络安全中的安全有效应用提出了前瞻性研究议程。

Abstract: This report explores the convergence of large language models (LLMs) and
cybersecurity, synthesizing interdisciplinary insights from network security,
artificial intelligence, formal methods, and human-centered design. It examines
emerging applications of LLMs in software and network security, 5G
vulnerability analysis, and generative security engineering. The report
highlights the role of agentic LLMs in automating complex tasks, improving
operational efficiency, and enabling reasoning-driven security analytics.
Socio-technical challenges associated with the deployment of LLMs -- including
trust, transparency, and ethical considerations -- can be addressed through
strategies such as human-in-the-loop systems, role-specific training, and
proactive robustness testing. The report further outlines critical research
challenges in ensuring interpretability, safety, and fairness in LLM-based
systems, particularly in high-stakes domains. By integrating technical advances
with organizational and societal considerations, this report presents a
forward-looking research agenda for the secure and effective adoption of LLMs
in cybersecurity.

</details>


### [178] [OET: Optimization-based prompt injection Evaluation Toolkit](https://arxiv.org/abs/2505.00843)
*Jinsheng Pan,Xiaogeng Liu,Chaowei Xiao*

Main category: cs.CR

TL;DR: 本文介绍了OET，一种基于优化的评估工具包，用于系统评估提示注入攻击和防御策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）易受提示注入攻击，缺乏标准化的评估框架。

Method: 提出OET工具包，采用模块化工作流和自适应测试框架，生成最坏情况对抗样本。

Result: 实验表明当前防御机制存在局限性，部分模型仍易受攻击。

Conclusion: OET为评估对抗鲁棒性提供了统一平台，揭示了现有防御的不足。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation, enabling their widespread
adoption across various domains. However, their susceptibility to prompt
injection attacks poses significant security risks, as adversarial inputs can
manipulate model behavior and override intended instructions. Despite numerous
defense strategies, a standardized framework to rigorously evaluate their
effectiveness, especially under adaptive adversarial scenarios, is lacking. To
address this gap, we introduce OET, an optimization-based evaluation toolkit
that systematically benchmarks prompt injection attacks and defenses across
diverse datasets using an adaptive testing framework. Our toolkit features a
modular workflow that facilitates adversarial string generation, dynamic attack
execution, and comprehensive result analysis, offering a unified platform for
assessing adversarial robustness. Crucially, the adaptive testing framework
leverages optimization methods with both white-box and black-box access to
generate worst-case adversarial examples, thereby enabling strict red-teaming
evaluations. Extensive experiments underscore the limitations of current
defense mechanisms, with some models remaining susceptible even after
implementing security enhancements.

</details>


### [179] [Good News for Script Kiddies? Evaluating Large Language Models for Automated Exploit Generation](https://arxiv.org/abs/2505.01065)
*David Jin,Qian Fu,Yuekang Li*

Main category: cs.CR

TL;DR: 论文首次系统研究了LLM在自动化漏洞利用生成（AEG）中的表现，评估了其合作性和技术能力，发现GPT-4和GPT-4o合作性高，但所有模型均未能成功生成漏洞利用。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在代码相关任务中的潜力，特别是自动化漏洞利用生成（AEG），以评估其安全风险和技术能力。

Method: 引入包含五个重构软件安全实验室的基准，设计基于LLM的攻击者系统提示模型生成漏洞利用。

Result: GPT-4和GPT-4o合作性高，但所有模型均未能成功生成漏洞利用，GPT-4o的错误最少，显示潜力。

Conclusion: LLM在AEG中表现有限，但GPT-4o的低错误率表明未来可能有改进空间。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code-related tasks, raising concerns about their potential for automated
exploit generation (AEG). This paper presents the first systematic study on
LLMs' effectiveness in AEG, evaluating both their cooperativeness and technical
proficiency. To mitigate dataset bias, we introduce a benchmark with refactored
versions of five software security labs. Additionally, we design an LLM-based
attacker to systematically prompt LLMs for exploit generation. Our experiments
reveal that GPT-4 and GPT-4o exhibit high cooperativeness, comparable to
uncensored models, while Llama3 is the most resistant. However, no model
successfully generates exploits for refactored labs, though GPT-4o's minimal
errors highlight the potential for LLM-driven AEG advancements.

</details>


### [180] [A Rusty Link in the AI Supply Chain: Detecting Evil Configurations in Model Repositories](https://arxiv.org/abs/2505.01067)
*Ziqi Ding,Qian Fu,Junchen Ding,Gelei Deng,Yi Liu,Yuekang Li*

Main category: cs.CR

TL;DR: 论文研究了Hugging Face平台上配置文件的潜在安全风险，提出了CONFIGSCAN工具，用于高效检测恶意配置。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）应用广泛，但AI供应链中的配置文件安全问题被忽视，可能被利用执行未授权代码。

Method: 通过分析配置文件及其运行时代码和关键库，开发了LLM-based工具CONFIGSCAN，检测可疑元素。

Result: 发现数千个可疑仓库和配置文件，验证了工具的低误报率和高准确性。

Conclusion: 强调了AI模型托管平台加强安全验证的紧迫性。

Abstract: Recent advancements in large language models (LLMs) have spurred the
development of diverse AI applications from code generation and video editing
to text generation; however, AI supply chains such as Hugging Face, which host
pretrained models and their associated configuration files contributed by the
public, face significant security challenges; in particular, configuration
files originally intended to set up models by specifying parameters and initial
settings can be exploited to execute unauthorized code, yet research has
largely overlooked their security compared to that of the models themselves; in
this work, we present the first comprehensive study of malicious configurations
on Hugging Face, identifying three attack scenarios (file, website, and
repository operations) that expose inherent risks; to address these threats, we
introduce CONFIGSCAN, an LLM-based tool that analyzes configuration files in
the context of their associated runtime code and critical libraries,
effectively detecting suspicious elements with low false positive rates and
high accuracy; our extensive evaluation uncovers thousands of suspicious
repositories and configuration files, underscoring the urgent need for enhanced
security validation in AI model hosting platforms.

</details>


### [181] [LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures](https://arxiv.org/abs/2505.01177)
*Francisco Aguilera-Martínez,Fernando Berzal*

Main category: cs.CR

TL;DR: 该论文综述了大型语言模型（LLMs）在训练和部署阶段的安全威胁，分类分析了攻击类型，并探讨了预防和检测防御机制。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的发展，评估其安全威胁和漏洞变得至关重要，尤其是在训练和部署阶段。

Method: 通过分类分析攻击类型，并总结预防和检测防御机制。

Result: 提供了攻击与防御策略的对应关系，并评估了现有防御机制的有效性。

Conclusion: 论文旨在为LLMs安全提供结构化框架，并指出需进一步研究的领域以应对新兴安全挑战。

Abstract: As large language models (LLMs) continue to evolve, it is critical to assess
the security threats and vulnerabilities that may arise both during their
training phase and after models have been deployed. This survey seeks to define
and categorize the various attacks targeting LLMs, distinguishing between those
that occur during the training phase and those that affect already trained
models. A thorough analysis of these attacks is presented, alongside an
exploration of defense mechanisms designed to mitigate such threats. Defenses
are classified into two primary categories: prevention-based and
detection-based defenses. Furthermore, our survey summarizes possible attacks
and their corresponding defense strategies. It also provides an evaluation of
the effectiveness of the known defense mechanisms for the different security
threats. Our survey aims to offer a structured framework for securing LLMs,
while also identifying areas that require further research to improve and
strengthen defenses against emerging security challenges.

</details>


### [182] [Secure Cluster-Based Hierarchical Federated Learning in Vehicular Networks](https://arxiv.org/abs/2505.01186)
*M. Saeid HaghighiFard,Sinem Coleri*

Main category: cs.CR

TL;DR: 论文提出了一种针对分层联邦学习（HFL）中对抗性和不可靠车辆的防御框架，通过动态车辆选择和异常检测来提升模型完整性。


<details>
  <summary>Details</summary>
Motivation: HFL在车载网络中面临通信资源有限、车辆高移动性和数据异构性等挑战，但易受恶意车辆误导更新的影响。

Method: 框架结合动态车辆选择、异常检测（Z-score和余弦相似度分析）、自适应阈值机制和加权梯度平均，并引入跨集群一致性检查以应对协同攻击。

Result: 仿真结果表明，该算法在1跳和3跳拓扑中显著减少了收敛时间。

Conclusion: 提出的多级防御策略有效过滤恶意贡献，提升了HFL的鲁棒性和收敛效率。

Abstract: Hierarchical Federated Learning (HFL) has recently emerged as a promising
solution for intelligent decision-making in vehicular networks, helping to
address challenges such as limited communication resources, high vehicle
mobility, and data heterogeneity. However, HFL remains vulnerable to
adversarial and unreliable vehicles, whose misleading updates can significantly
compromise the integrity and convergence of the global model. To address these
challenges, we propose a novel defense framework that integrates dynamic
vehicle selection with robust anomaly detection within a cluster-based HFL
architecture, specifically designed to counter Gaussian noise and gradient
ascent attacks. The framework performs a comprehensive reliability assessment
for each vehicle by evaluating historical accuracy, contribution frequency, and
anomaly records. Anomaly detection combines Z-score and cosine similarity
analyses on model updates to identify both statistical outliers and directional
deviations in model updates. To further refine detection, an adaptive
thresholding mechanism is incorporated into the cosine similarity metric,
dynamically adjusting the threshold based on the historical accuracy of each
vehicle to enforce stricter standards for consistently high-performing
vehicles. In addition, a weighted gradient averaging mechanism is implemented,
which assigns higher weights to gradient updates from more trustworthy
vehicles. To defend against coordinated attacks, a cross-cluster consistency
check is applied to identify collaborative attacks in which multiple
compromised clusters coordinate misleading updates. Together, these mechanisms
form a multi-level defense strategy to filter out malicious contributions
effectively. Simulation results show that the proposed algorithm significantly
reduces convergence time compared to benchmark methods across both 1-hop and
3-hop topologies.

</details>


### [183] [Constrained Network Adversarial Attacks: Validity, Robustness, and Transferability](https://arxiv.org/abs/2505.01328)
*Anass Grini,Oumaima Taheri,Btissam El Khamlichi,Amal El Fallah-Seghrouchni*

Main category: cs.CR

TL;DR: 研究发现现有对抗攻击方法在IoT环境中常违反领域约束，导致80.3%的对抗样本无效，误导了真实漏洞评估。简单模型（如MLP）生成的对抗样本更有效。


<details>
  <summary>Details</summary>
Motivation: 评估对抗攻击在IoT环境中的有效性，揭示现有方法因忽略领域约束而高估漏洞的问题。

Method: 使用MLP作为替代模型生成对抗样本，分析其在不同ML/DL模型中的可转移性。

Result: 80.3%的对抗样本因违反领域约束而无效；MLP生成的样本更有效。

Conclusion: 设计和评估安全关键型IoT模型时需考虑领域约束和模型架构。

Abstract: While machine learning has significantly advanced Network Intrusion Detection
Systems (NIDS), particularly within IoT environments where devices generate
large volumes of data and are increasingly susceptible to cyber threats, these
models remain vulnerable to adversarial attacks. Our research reveals a
critical flaw in existing adversarial attack methodologies: the frequent
violation of domain-specific constraints, such as numerical and categorical
limits, inherent to IoT and network traffic. This leads to up to 80.3% of
adversarial examples being invalid, significantly overstating real-world
vulnerabilities. These invalid examples, though effective in fooling models, do
not represent feasible attacks within practical IoT deployments. Consequently,
relying on these results can mislead resource allocation for defense, inflating
the perceived susceptibility of IoT-enabled NIDS models to adversarial
manipulation. Furthermore, we demonstrate that simpler surrogate models like
Multi-Layer Perceptron (MLP) generate more valid adversarial examples compared
to complex architectures such as CNNs and LSTMs. Using the MLP as a surrogate,
we analyze the transferability of adversarial severity to other ML/DL models
commonly used in IoT contexts. This work underscores the importance of
considering both domain constraints and model architecture when evaluating and
designing robust ML/DL models for security-critical IoT and network
applications.

</details>


### [184] [Protocol-agnostic and Data-free Backdoor Attacks on Pre-trained Models in RF Fingerprinting](https://arxiv.org/abs/2505.00881)
*Tianya Zhao,Ningning Wang,Junqing Zhang,Xuyu Wang*

Main category: cs.CR

TL;DR: 本文研究了在射频指纹识别中，无监督预训练模型（PTMs）的数据无关后门攻击，展示了其广泛适用性及防御难度。


<details>
  <summary>Details</summary>
Motivation: 监督深度神经网络在射频指纹识别中受限于领域偏移和标记数据稀缺问题，而PTMs虽能解决这些问题，但其潜在漏洞尚未充分探索。

Method: 设计触发器和预定义输出表示（PORs），通过后门训练将后门行为植入PTMs，无需下游数据或标签信息。

Result: 实验表明攻击对多种输入域、协议和PTMs均有效，且防御难度高。

Conclusion: 揭示了PTMs在射频指纹识别中的安全隐患，强调需进一步研究防御方法。

Abstract: While supervised deep neural networks (DNNs) have proven effective for device
authentication via radio frequency (RF) fingerprinting, they are hindered by
domain shift issues and the scarcity of labeled data. The success of large
language models has led to increased interest in unsupervised pre-trained
models (PTMs), which offer better generalization and do not require labeled
datasets, potentially addressing the issues mentioned above. However, the
inherent vulnerabilities of PTMs in RF fingerprinting remain insufficiently
explored. In this paper, we thoroughly investigate data-free backdoor attacks
on such PTMs in RF fingerprinting, focusing on a practical scenario where
attackers lack access to downstream data, label information, and training
processes. To realize the backdoor attack, we carefully design a set of
triggers and predefined output representations (PORs) for the PTMs. By mapping
triggers and PORs through backdoor training, we can implant backdoor behaviors
into the PTMs, thereby introducing vulnerabilities across different downstream
RF fingerprinting tasks without requiring prior knowledge. Extensive
experiments demonstrate the wide applicability of our proposed attack to
various input domains, protocols, and PTMs. Furthermore, we explore potential
detection and defense methods, demonstrating the difficulty of fully
safeguarding against our proposed backdoor attack.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [185] [Reduced-order structure-property linkages for stochastic metamaterials](https://arxiv.org/abs/2505.01283)
*Hooman Danesh,Maruthi Annamaraju,Tim Brepols,Stefanie Reese,Surya R. Kalidindi*

Main category: cs.CE

TL;DR: 该论文提出了一种基于材料信息学的方法，通过主成分分析和高斯过程回归，高效地建立了机械超材料单元设计与有效弹性性能之间的映射关系。


<details>
  <summary>Details</summary>
Motivation: 机械超材料的设计空间庞大，传统的物理模拟计算成本高，需要一种高效的方法来捕捉复杂的结构-性能关系。

Method: 采用主成分分析提取2点相关函数的特征，结合FFT均质化方法计算弹性刚度，再通过高斯过程回归生成降阶代理模型。

Result: 结果表明，该方法能够高效地表示大量随机超材料数据集，并仅需原始数据集的0.61%即可构建准确的结构-性能映射。

Conclusion: 该工作为机械超材料的高效设计和性能评估提供了一种计算成本低且准确的方法。

Abstract: The capabilities of additive manufacturing have facilitated the design and
production of mechanical metamaterials with diverse unit cell geometries.
Establishing linkages between the vast design space of unit cells and their
effective mechanical properties is critical for the efficient design and
performance evaluation of such metamaterials. However, physics-based
simulations of metamaterial unit cells across the entire design space are
computationally expensive, necessitating a materials informatics framework to
efficiently capture complex structure-property relationships. In this work,
principal component analysis of 2-point correlation functions is performed to
extract the salient features from a large dataset of randomly generated 2D
metamaterials. Physics-based simulations are performed using a fast Fourier
transform (FFT)-based homogenization approach to efficiently compute the
homogenized effective elastic stiffness across the extensive unit cell designs.
Subsequently, Gaussian process regression is used to generate reduced-order
surrogates, mapping unit cell designs to their homogenized effective elastic
constant. It is demonstrated that the adopted workflow enables a high-value
low-dimensional representation of the voluminous stochastic metamaterial
dataset, facilitating the construction of robust structure-property maps.
Finally, an uncertainty-based active learning framework is utilized to train a
surrogate model with a significantly smaller number of data points compared to
the original full dataset. It is shown that a dataset as small as $0.61\%$ of
the entire dataset is sufficient to generate accurate and robust
structure-property maps.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [186] [Multivariate Conformal Selection](https://arxiv.org/abs/2505.00917)
*Tian Bai,Yue Zhao,Xiang Yu,Archer Y. Yang*

Main category: stat.ME

TL;DR: 论文提出了一种多变量保形选择方法（mCS），用于解决传统保形选择（CS）在多变量响应场景中的局限性，通过区域单调性和多变量非保形分数实现FDR控制。


<details>
  <summary>Details</summary>
Motivation: 传统保形选择（CS）仅适用于单变量响应和标量标准，无法满足多变量场景的需求。

Method: 提出mCS方法，包括基于距离的mCS-dist和通过可微分优化学习的mCS-learn。

Result: 实验表明，mCS在保持FDR控制的同时显著提高了选择能力。

Conclusion: mCS为多变量选择任务提供了一个稳健的框架。

Abstract: Selecting high-quality candidates from large datasets is critical in
applications such as drug discovery, precision medicine, and alignment of large
language models (LLMs). While Conformal Selection (CS) provides rigorous
uncertainty quantification, it is limited to univariate responses and scalar
criteria. To address this issue, we propose Multivariate Conformal Selection
(mCS), a generalization of CS designed for multivariate response settings. Our
method introduces regional monotonicity and employs multivariate nonconformity
scores to construct conformal p-values, enabling finite-sample False Discovery
Rate (FDR) control. We present two variants: mCS-dist, using distance-based
scores, and mCS-learn, which learns optimal scores via differentiable
optimization. Experiments on simulated and real-world datasets demonstrate that
mCS significantly improves selection power while maintaining FDR control,
establishing it as a robust framework for multivariate selection tasks.

</details>


### [187] [Q-Learning with Clustered-SMART (cSMART) Data: Examining Moderators in the Construction of Clustered Adaptive Interventions](https://arxiv.org/abs/2505.00822)
*Yao Song,Kelly Speth,Amy Kilbourne,Andrew Quanbeck,Daniel Almirall,Lu Wang*

Main category: stat.ME

TL;DR: 本文提出了一种基于cSMART数据的聚类Q学习方法，用于评估候选定制变量在构建最优cAI中的效用，并通过模拟验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用cSMART数据评估候选定制变量，以优化聚类自适应干预（cAI）的效果。

Method: 采用聚类Q学习框架结合M-out-of-N聚类自举法，构建置信区间以评估因果效应调节参数。

Result: 模拟结果显示，该方法在不同非规则条件下表现良好，并能可靠评估候选定制变量的效用。

Conclusion: 该方法为构建最优cAI提供了可靠工具，尤其在非规则条件下表现优异。

Abstract: A clustered adaptive intervention (cAI) is a pre-specified sequence of
decision rules that guides practitioners on how best - and based on which
measures - to tailor cluster-level intervention to improve outcomes at the
level of individuals within the clusters. A clustered sequential multiple
assignment randomized trial (cSMART) is a type of trial that is used to inform
the empirical development of a cAI. The most common type of secondary aim in a
cSMART focuses on assessing causal effect moderation by candidate tailoring
variables. We introduce a clustered Q-learning framework with the M-out-of-N
Cluster Bootstrap using data from a cSMART to evaluate whether a set of
candidate tailoring variables may be useful in defining an optimal cAI. This
approach could construct confidence intervals (CI) with near-nominal coverage
to assess parameters indexing the causal effect moderation function.
Specifically, it allows reliable inferences concerning the utility of candidate
tailoring variables in constructing a cAI that maximizes a mean end-of-study
outcome even when "non-regularity", a well-known challenge exists. Simulations
demonstrate the numerical performance of the proposed method across varying
non-regularity conditions and investigate the impact of varying number of
clusters and intra-cluster correlation coefficient on CI coverage. Methods are
applied on ADEPT dataset to inform the construction of a clinic-level cAI for
improving evidence-based practice in treating mood disorders.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [188] [A flexible Bayesian non-parametric mixture model reveals multiple dependencies of swap errors in visual working memory](https://arxiv.org/abs/2505.01178)
*Puria Radmard,Paul M. Bays,Máté Lengyel*

Main category: q-bio.NC

TL;DR: 论文提出了一种贝叶斯非参数混合模型（BNS）来研究视觉工作记忆中的交换错误，揭示了交换错误不仅与提示相似性相关，还与报告特征维度存在非单调调制关系，挑战了以往对交换错误来源的解释。


<details>
  <summary>Details</summary>
Motivation: 视觉工作记忆中的交换错误机制尚不明确，以往研究多假设错误源于存储或检索阶段，但缺乏数据驱动的验证。本文旨在通过灵活的数据驱动模型探索交换错误的来源。

Method: 引入贝叶斯非参数混合模型（BNS），允许交换行为依赖于每个刺激项的提示和报告特征，并在人类行为数据上进行拟合。

Result: BNS模型成功捕捉了交换错误对提示相似性的依赖，并发现报告特征维度的非单调调制现象，表明记忆编码可能是交换错误的新来源。

Conclusion: BNS模型揭示了交换错误的多源性，挑战了以往仅关注存储和检索错误的观点，为视觉工作记忆研究提供了新的方向。

Abstract: Human behavioural data in psychophysics has been used to elucidate the
underlying mechanisms of many cognitive processes, such as attention,
sensorimotor integration, and perceptual decision making. Visual working memory
has particularly benefited from this approach: analyses of VWM errors have
proven crucial for understanding VWM capacity and coding schemes, in turn
constraining neural models of both. One poorly understood class of VWM errors
are swap errors, whereby participants recall an uncued item from memory. Swap
errors could arise from erroneous memory encoding, noisy storage, or errors at
retrieval time - previous research has mostly implicated the latter two.
However, these studies made strong a priori assumptions on the detailed
mechanisms and/or parametric form of errors contributed by these sources. Here,
we pursue a data-driven approach instead, introducing a Bayesian non-parametric
mixture model of swap errors (BNS) which provides a flexible descriptive model
of swapping behaviour, such that swaps are allowed to depend on both the probed
and reported features of every stimulus item. We fit BNS to the trial-by-trial
behaviour of human participants and show that it recapitulates the strong
dependence of swaps on cue similarity in multiple datasets. Critically, BNS
reveals that this dependence coexists with a non-monotonic modulation in the
report feature dimension for a random dot motion direction-cued,
location-reported dataset. The form of the modulation inferred by BNS opens new
questions about the importance of memory encoding in causing swap errors in
VWM, a distinct source to the previously suggested binding and cueing errors.
Our analyses, combining qualitative comparisons of the highly interpretable BNS
parameter structure with rigorous quantitative model comparison and recovery
methods, show that previous interpretations of swap errors may have been
incomplete.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [189] [Artificial Intelligence in Government: Why People Feel They Lose Control](https://arxiv.org/abs/2505.01085)
*Alexander Wuttke,Adrian Rauchfleisch,Andreas Jungherr*

Main category: cs.CY

TL;DR: 论文探讨了AI在公共管理中的应用及其对公平性、透明度和问责制的影响，提出了基于委托-代理理论的框架，并通过实验验证了效率提升与公众信任之间的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 研究AI在政府职能中的快速扩展及其潜在风险，特别是对民主合法性的长期威胁。

Method: 应用委托-代理理论（PAT）分析AI采用，并通过预注册的因子调查实验在税务、福利和执法领域测试框架。

Result: 实验显示效率提升初期增强信任，但降低公众感知控制；当结构风险显现时，信任和控制感急剧下降。

Conclusion: PAT为理解AI在政府中的政治影响提供了有力视角，强调政策制定者需透明处理委托风险以维持公众信任。

Abstract: The use of Artificial Intelligence (AI) in public administration is expanding
rapidly, moving from automating routine tasks to deploying generative and
agentic systems that autonomously act on goals. While AI promises greater
efficiency and responsiveness, its integration into government functions raises
concerns about fairness, transparency, and accountability. This article applies
principal-agent theory (PAT) to conceptualize AI adoption as a special case of
delegation, highlighting three core tensions: assessability (can decisions be
understood?), dependency (can the delegation be reversed?), and contestability
(can decisions be challenged?). These structural challenges may lead to a
"failure-by-success" dynamic, where early functional gains obscure long-term
risks to democratic legitimacy. To test this framework, we conducted a
pre-registered factorial survey experiment across tax, welfare, and law
enforcement domains. Our findings show that although efficiency gains initially
bolster trust, they simultaneously reduce citizens' perceived control. When the
structural risks come to the foreground, institutional trust and perceived
control both drop sharply, suggesting that hidden costs of AI adoption
significantly shape public attitudes. The study demonstrates that PAT offers a
powerful lens for understanding the institutional and political implications of
AI in government, emphasizing the need for policymakers to address delegation
risks transparently to maintain public trust.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [190] [On the emergence of numerical instabilities in Next Generation Reservoir Computing](https://arxiv.org/abs/2505.00846)
*Edmilson Roque dos Santos,Erik Bollt*

Main category: stat.ML

TL;DR: NGRC是一种低成本机器学习方法，用于预测混沌时间序列，但其动态稳定性在自主预测中仍具挑战性。研究发现特征矩阵数值条件与长期NGRC动态相关，并探讨了不同数值算法的影响。


<details>
  <summary>Details</summary>
Motivation: 解决NGRC模型在自主预测中的动态稳定性问题，揭示特征矩阵数值条件与长期动态的关系。

Method: 结合数值线性代数和动力系统遍历理论，研究特征矩阵条件随超参数的变化，并评估不同数值算法（Cholesky、SVD和LU）对正则化最小二乘问题的影响。

Result: 发现短时间滞后和高阶多项式会导致特征矩阵条件不良，进而放大对训练数据扰动的敏感性，导致NGRC动态不稳定。

Conclusion: 特征矩阵数值条件对NGRC动态稳定性至关重要，选择合适的数值算法可改善模型性能。

Abstract: Next Generation Reservoir Computing (NGRC) is a low-cost machine learning
method for forecasting chaotic time series from data. However, ensuring the
dynamical stability of NGRC models during autonomous prediction remains a
challenge. In this work, we uncover a key connection between the numerical
conditioning of the NGRC feature matrix -- formed by polynomial evaluations on
time-delay coordinates -- and the long-term NGRC dynamics. Merging tools from
numerical linear algebra and ergodic theory of dynamical systems, we
systematically study how the feature matrix conditioning varies across
hyperparameters. We demonstrate that the NGRC feature matrix tends to be
ill-conditioned for short time lags and high-degree polynomials.
Ill-conditioning amplifies sensitivity to training data perturbations, which
can produce unstable NGRC dynamics. We evaluate the impact of different
numerical algorithms (Cholesky, SVD, and LU) for solving the regularized
least-squares problem.

</details>


### [191] [DOLCE: Decomposing Off-Policy Evaluation/Learning into Lagged and Current Effects](https://arxiv.org/abs/2505.00961)
*Shu Tamano,Masanori Nojima*

Main category: stat.ML

TL;DR: 论文提出了一种名为DOLCE的新方法，用于解决在违反共同支持假设时的离策略评估和学习问题。DOLCE通过分解奖励为滞后和当前效应，利用多时间点的上下文信息，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的离策略评估和学习方法在共同支持假设被违反时需要不稳定外推或保守策略，无法满足对这类个体的明确评估或优化需求。

Method: DOLCE通过分解奖励为滞后和当前效应，结合过去和当前的上下文信息，处理违反共同支持假设的个体。

Result: DOLCE在离策略评估和学习中表现优异，尤其在共同支持假设外的个体比例增加时效果显著。

Conclusion: DOLCE是一种有效的离策略评估和学习方法，特别适用于共同支持假设不成立的情况。

Abstract: Off-policy evaluation (OPE) and off-policy learning (OPL) for contextual
bandit policies leverage historical data to evaluate and optimize a target
policy. Most existing OPE/OPL methods--based on importance weighting or
imputation--assume common support between the target and logging policies. When
this assumption is violated, these methods typically require unstable
extrapolation, truncation, or conservative strategies for individuals outside
the common support assumption. However, such approaches can be inadequate in
settings where explicit evaluation or optimization for such individuals is
required. To address this issue, we propose DOLCE: Decomposing Off-policy
evaluation/learning into Lagged and Current Effects, a novel estimator that
leverages contextual information from multiple time points to decompose rewards
into lagged and current effects. By incorporating both past and present
contexts, DOLCE effectively handles individuals who violate the common support
assumption. We show that the proposed estimator is unbiased under two
assumptions--local correctness and conditional independence. Our experiments
demonstrate that DOLCE achieves substantial improvements in OPE and OPL,
particularly as the proportion of individuals outside the common support
assumption increases.

</details>


### [192] [Characterization and Learning of Causal Graphs from Hard Interventions](https://arxiv.org/abs/2505.01037)
*Zihan Zhou,Muhammad Qasim Elahi,Murat Kocaoglu*

Main category: stat.ML

TL;DR: 本文提出了一种基于多实验分布数据的因果发现方法，通过比较干预分布，建立了与Pearl的do-calculus相关的图形约束，并提出了一个学习算法。


<details>
  <summary>Details</summary>
Motivation: 解决在观察和实验中揭示因果结构的基本挑战，特别是在多干预分布数据下的因果发现问题。

Method: 通过比较不同干预分布，提出图形约束，并设计学习算法以整合多数据集，引入新的定向规则。

Result: 定义了干预等价类的因果图表示，并证明了算法的正确性。

Conclusion: 提出的方法在多干预数据下有效，为因果发现提供了新工具。

Abstract: A fundamental challenge in the empirical sciences involves uncovering causal
structure through observation and experimentation. Causal discovery entails
linking the conditional independence (CI) invariances in observational data to
their corresponding graphical constraints via d-separation. In this paper, we
consider a general setting where we have access to data from multiple
experimental distributions resulting from hard interventions, as well as
potentially from an observational distribution. By comparing different
interventional distributions, we propose a set of graphical constraints that
are fundamentally linked to Pearl's do-calculus within the framework of hard
interventions. These graphical constraints associate each graphical structure
with a set of interventional distributions that are consistent with the rules
of do-calculus. We characterize the interventional equivalence class of causal
graphs with latent variables and introduce a graphical representation that can
be used to determine whether two causal graphs are interventionally equivalent,
i.e., whether they are associated with the same family of hard interventional
distributions, where the elements of the family are indistinguishable using the
invariances from do-calculus. We also propose a learning algorithm to integrate
multiple datasets from hard interventions, introducing new orientation rules.
The learning objective is a tuple of augmented graphs which entails a set of
causal graphs. We also prove the soundness of the proposed algorithm.

</details>


### [193] [Gaussian Differential Private Bootstrap by Subsampling](https://arxiv.org/abs/2505.01197)
*Holger Dette,Carina Graw*

Main category: stat.ML

TL;DR: 论文提出了一种基于差分隐私的私有经验$m$ out of $n$自助法，解决了传统自助法在隐私保护下计算成本高和统计精度损失的问题。


<details>
  <summary>Details</summary>
Motivation: 传统自助法在差分隐私下需要重复访问数据，导致隐私预算增加和统计精度下降。

Method: 提出私有经验$m$ out of $n$自助法，验证其在高斯差分隐私下的有效性和隐私保障。

Result: 相比传统$n$ out of $n$自助法，新方法计算成本更低，统计精度更高，且具有更好的有限样本性质。

Conclusion: 私有经验$m$ out of $n$自助法在隐私保护和统计精度之间取得了更好的平衡。

Abstract: Bootstrap is a common tool for quantifying uncertainty in data analysis.
However, besides additional computational costs in the application of the
bootstrap on massive data, a challenging problem in bootstrap based inference
under Differential Privacy consists in the fact that it requires repeated
access to the data. As a consequence, bootstrap based differentially private
inference requires a significant increase of the privacy budget, which on the
other hand comes with a substantial loss in statistical accuracy.
  A potential solution to reconcile the conflicting goals of statistical
accuracy and privacy is to analyze the data under parametric model assumptions
and in the last decade, several parametric bootstrap methods for inference
under privacy have been investigated. However, uncertainty quantification by
parametric bootstrap is only valid if the the quantities of interest can be
identified as the parameters of a statistical model and the imposed model
assumptions are (at least approximately) satisfied. An alternative to
parametric methods is the empirical bootstrap that is a widely used tool for
non-parametric inference and well studied in the non-private regime. However,
under privacy, less insight is available. In this paper, we propose a private
empirical $m$ out of $n$ bootstrap and validate its consistency and privacy
guarantees under Gaussian Differential Privacy. Compared to the the private $n$
out of $n$ bootstrap, our approach has several advantages. First, it comes with
less computational costs, in particular for massive data. Second, the proposed
procedure needs less additional noise in the bootstrap iterations, which leads
to an improved statistical accuracy while asymptotically guaranteeing the same
level of privacy. Third, we demonstrate much better finite sample properties
compared to the currently available procedures.

</details>


### [194] [Provable Efficiency of Guidance in Diffusion Models for General Data Distribution](https://arxiv.org/abs/2505.01382)
*Gen Li,Yuchen Jiao*

Main category: stat.ML

TL;DR: 论文分析了扩散模型中引导技术的理论效果，证明了在一般数据分布下引导能提升整体样本质量。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在生成建模中表现出色，但引导效果的理论理解仍有限，现有研究仅针对特定案例。

Method: 通过分析一般数据分布下的扩散引导，证明引导能降低分类器概率的平均倒数。

Result: 研究表明引导能提升整体样本质量，而非均匀提升。

Conclusion: 引导技术的引入动机与其理论效果一致，填补了现有研究的空白。

Abstract: Diffusion models have emerged as a powerful framework for generative
modeling, with guidance techniques playing a crucial role in enhancing sample
quality. Despite their empirical success, a comprehensive theoretical
understanding of the guidance effect remains limited. Existing studies only
focus on case studies, where the distribution conditioned on each class is
either isotropic Gaussian or supported on a one-dimensional interval with some
extra conditions. How to analyze the guidance effect beyond these case studies
remains an open question. Towards closing this gap, we make an attempt to
analyze diffusion guidance under general data distributions. Rather than
demonstrating uniform sample quality improvement, which does not hold in some
distributions, we prove that guidance can improve the whole sample quality, in
the sense that the average reciprocal of the classifier probability decreases
with the existence of guidance. This aligns with the motivation of introducing
guidance.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [195] [How much to Dereverberate? Low-Latency Single-Channel Speech Enhancement in Distant Microphone Scenarios](https://arxiv.org/abs/2505.01338)
*Satvik Venkatesh,Philip Coleman,Arthur Benilov,Simon Brown,Selim Sheta,Frederic Roskam*

Main category: eess.AS

TL;DR: 论文探讨了远距离麦克风场景下的实时低延迟单通道语音增强（SE），重点关注会议室和剧院等大空间环境，并验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 研究单通道SE在长混响时间和大空间环境中的挑战，填补现有文献的空白。

Method: 研究了房间体积与混响时间的关系，并随机模拟房间脉冲响应，同时提出保留早期反射以改善信号质量。

Result: 验证了单通道SE在远距离场景中的可行性，并展示了保留早期反射对信号质量的提升。

Conclusion: 在长混响时间和大空间环境中，单通道SE是可行的，且保留早期反射能显著改善信号质量。

Abstract: Dereverberation is an important sub-task of Speech Enhancement (SE) to
improve the signal's intelligibility and quality. However, it remains
challenging because the reverberation is highly correlated with the signal.
Furthermore, the single-channel SE literature has predominantly focused on
rooms with short reverb times (typically under 1 second), smaller rooms (under
volumes of 1000 cubic meters) and relatively short distances (up to 2 meters).
In this paper, we explore real-time low-latency single-channel SE under distant
microphone scenarios, such as 5 to 10 meters, and focus on conference rooms and
theatres, with larger room dimensions and reverberation times. Such a setup is
useful for applications such as lecture demonstrations, drama, and to enhance
stage acoustics. First, we show that single-channel SE in such challenging
scenarios is feasible. Second, we investigate the relationship between room
volume and reverberation time, and demonstrate its importance when randomly
simulating room impulse responses. Lastly, we show that for dereverberation
with short decay times, preserving early reflections before decaying the
transfer function of the room improves overall signal quality.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [196] [GENMO: A GENeralist Model for Human MOtion](https://arxiv.org/abs/2505.01425)
*Jiefeng Li,Jinkun Cao,Haotian Zhang,Davis Rempe,Jan Kautz,Umar Iqbal,Ye Yuan*

Main category: cs.GR

TL;DR: GENMO是一个统一的人类运动模型，将运动生成和估计结合在一个框架中，通过约束生成和扩散方法实现高精度和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统方法将运动生成和估计分开，限制了知识共享和模型效率。GENMO旨在统一这两项任务，提升性能和灵活性。

Method: GENMO将运动估计重新定义为约束生成，结合回归和扩散方法，并利用2D注释和文本描述的多样化数据训练。

Result: GENMO在多个任务中表现优异，生成的运动多样且估计准确，尤其在遮挡等挑战性条件下表现突出。

Conclusion: GENMO证明了统一框架的可行性，为人类运动建模提供了高效且灵活的新方法。

Abstract: Human motion modeling traditionally separates motion generation and
estimation into distinct tasks with specialized models. Motion generation
models focus on creating diverse, realistic motions from inputs like text,
audio, or keyframes, while motion estimation models aim to reconstruct accurate
motion trajectories from observations like videos. Despite sharing underlying
representations of temporal dynamics and kinematics, this separation limits
knowledge transfer between tasks and requires maintaining separate models. We
present GENMO, a unified Generalist Model for Human Motion that bridges motion
estimation and generation in a single framework. Our key insight is to
reformulate motion estimation as constrained motion generation, where the
output motion must precisely satisfy observed conditioning signals. Leveraging
the synergy between regression and diffusion, GENMO achieves accurate global
motion estimation while enabling diverse motion generation. We also introduce
an estimation-guided training objective that exploits in-the-wild videos with
2D annotations and text descriptions to enhance generative diversity.
Furthermore, our novel architecture handles variable-length motions and mixed
multimodal conditions (text, audio, video) at different time intervals,
offering flexible control. This unified approach creates synergistic benefits:
generative priors improve estimated motions under challenging conditions like
occlusions, while diverse video data enhances generation capabilities.
Extensive experiments demonstrate GENMO's effectiveness as a generalist
framework that successfully handles multiple human motion tasks within a single
model.

</details>


### [197] [Model See Model Do: Speech-Driven Facial Animation with Style Control](https://arxiv.org/abs/2505.01319)
*Yifang Pan,Karan Singh,Luiz Gustavo Hafemann*

Main category: cs.GR

TL;DR: 提出了一种基于示例的生成框架，通过潜在扩散模型和风格参考片段生成高度表达且时间连贯的面部动画。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉和传递细微表演风格方面存在不足，需要一种能同时保持唇同步和风格一致性的新方法。

Method: 引入风格基底的机制，从参考片段提取关键姿势，通过加法引导扩散生成过程，确保风格适配且不损害唇同步质量。

Result: 定性和定量评估表明，该方法能忠实再现目标风格，并在多种语音场景中实现卓越的唇同步效果。

Conclusion: 该方法在生成表达丰富的面部动画方面表现优异，同时保持了高质量的唇同步。

Abstract: Speech-driven 3D facial animation plays a key role in applications such as
virtual avatars, gaming, and digital content creation. While existing methods
have made significant progress in achieving accurate lip synchronization and
generating basic emotional expressions, they often struggle to capture and
effectively transfer nuanced performance styles. We propose a novel
example-based generation framework that conditions a latent diffusion model on
a reference style clip to produce highly expressive and temporally coherent
facial animations. To address the challenge of accurately adhering to the style
reference, we introduce a novel conditioning mechanism called style basis,
which extracts key poses from the reference and additively guides the diffusion
generation process to fit the style without compromising lip synchronization
quality. This approach enables the model to capture subtle stylistic cues while
ensuring that the generated animations align closely with the input speech.
Extensive qualitative, quantitative, and perceptual evaluations demonstrate the
effectiveness of our method in faithfully reproducing the desired style while
achieving superior lip synchronization across various speech scenarios.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [198] [Wireless Communication as an Information Sensor for Multi-agent Cooperative Perception: A Survey](https://arxiv.org/abs/2505.00747)
*Zhiying Song,Tenghui Xie,Fuxi Wen,Jun Li*

Main category: cs.OH

TL;DR: 本文综述了基于V2X通信的多智能体协同感知技术，重点探讨了信息表示、信息融合和大规模部署三个维度，并提出了将V2X视为动态信息传感器的新视角。


<details>
  <summary>Details</summary>
Motivation: 传统车载传感器在感知能力上存在局限性，而V2X通信通过多智能体信息共享扩展了感知能力，但其动态性、异构性和可扩展性带来了挑战。

Method: 从信息中心化协同感知的角度，分类讨论了信息表示（数据级、特征级、对象级）、信息融合（理想与非理想条件）以及大规模部署的系统级方法。

Result: 总结了减少数据量、压缩消息、处理异构性、定位误差、延迟和数据包丢失的技术，并提出了支持密集交通场景可扩展性的解决方案。

Conclusion: 本文通过将V2X视为信息传感器，强调了在现实智能交通系统中部署协同感知的挑战，为未来研究提供了新视角。

Abstract: Cooperative perception extends the perception capabilities of autonomous
vehicles by enabling multi-agent information sharing via Vehicle-to-Everything
(V2X) communication. Unlike traditional onboard sensors, V2X acts as a dynamic
"information sensor" characterized by limited communication, heterogeneity,
mobility, and scalability. This survey provides a comprehensive review of
recent advancements from the perspective of information-centric cooperative
perception, focusing on three key dimensions: information representation,
information fusion, and large-scale deployment. We categorize information
representation into data-level, feature-level, and object-level schemes, and
highlight emerging methods for reducing data volume and compressing messages
under communication constraints. In information fusion, we explore techniques
under both ideal and non-ideal conditions, including those addressing
heterogeneity, localization errors, latency, and packet loss. Finally, we
summarize system-level approaches to support scalability in dense traffic
scenarios. Compared with existing surveys, this paper introduces a new
perspective by treating V2X communication as an information sensor and
emphasizing the challenges of deploying cooperative perception in real-world
intelligent transportation systems.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [199] [On Simulating Thin-Film Processes at the Atomic Scale Using Machine Learned Force Fields](https://arxiv.org/abs/2505.01118)
*S. Kondati Natarajan,J. Schneider,N. Pandey,J. Wellendorff,S. Smidstrup*

Main category: cond-mat.mtrl-sci

TL;DR: 论文探讨了原子尺度建模在薄膜工艺中的应用，展示了机器学习力场（MLFF）在模拟工业相关过程中的有效性。


<details>
  <summary>Details</summary>
Motivation: 通过原子尺度建模揭示薄膜工艺中的关键化学机制，并提取定量指标，但传统力场在工业相关过程中往往不可用。

Method: 利用机器学习力场（MLFF）构建适合工艺模拟的力场，并以HfO2的原子层沉积和MoS2的原子层蚀刻为例。

Result: 成功构建了适用于工艺模拟的MLFF，并验证了其在工业相关过程中的有效性。

Conclusion: MLFF为工业相关过程的原子尺度模拟提供了可行且高效的工具。

Abstract: Atomistic modeling of thin-film processes provides an avenue not only for
discovering key chemical mechanisms of the processes but also to extract
quantitative metrics on the events and reactions taking place at the
gas-surface interface. Molecular dynamics (MD) is a powerful computational
method to study the evolution of a process at the atomic scale, but studies of
industrially relevant processes usually require suitable force fields, which
are in general not available for all processes of interest. However, machine
learned force fields (MLFF) are conquering the field of computational materials
and surface science. In this paper, we demonstrate how to efficiently build
MLFFs suitable for process simulations and provide two examples for
technologically relevant processes: precursor pulse in the atomic layer
deposition of HfO2 and atomic layer etching of MoS2.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [200] [Advancing Software Security and Reliability in Cloud Platforms through AI-based Anomaly Detection](https://arxiv.org/abs/2411.09200)
*Sabbir M. Saleh,Ibrahim Mohammed Sayem,Nazim Madhavji,John Steinbacher*

Main category: cs.SE

TL;DR: 该研究通过AI支持的异常检测增强CI/CD管道安全性，利用CNN和LSTM分析网络流量模式，准确率高达98%以上。


<details>
  <summary>Details</summary>
Motivation: CI/CD管道中的安全问题（如DDoS、Bot攻击等）频发，现有研究多关注静态安全测试，缺乏对网络流量模式的分析。

Method: 结合CNN和LSTM模型，使用CSE-CIC-IDS2018和CSE-CIC-IDS2017数据集检测异常流量模式。

Result: 模型准确率达98.69%和98.30%，并生成日志文件以应对安全挑战。

Conclusion: 该研究为现代DevOps实践提供了增强安全性和可靠性的解决方案。

Abstract: Continuous Integration/Continuous Deployment (CI/CD) is fundamental for
advanced software development, supporting faster and more efficient delivery of
code changes into cloud environments. However, security issues in the CI/CD
pipeline remain challenging, and incidents (e.g., DDoS, Bot, Log4j, etc.) are
happening over the cloud environments. While plenty of literature discusses
static security testing and CI/CD practices, only a few deal with network
traffic pattern analysis to detect different cyberattacks. This research aims
to enhance CI/CD pipeline security by implementing anomaly detection through AI
(Artificial Intelligence) support. The goal is to identify unusual behaviour or
variations from network traffic patterns in pipeline and cloud platforms. The
system shall integrate into the workflow to continuously monitor pipeline
activities and cloud infrastructure. Additionally, it aims to explore adaptive
response mechanisms to mitigate the detected anomalies or security threats.
This research employed two popular network traffic datasets, CSE-CIC-IDS2018
and CSE-CIC-IDS2017. We implemented a combination of Convolution Neural
Network(CNN) and Long Short-Term Memory (LSTM) to detect unusual traffic
patterns. We achieved an accuracy of 98.69% and 98.30% and generated log files
in different CI/CD pipeline stages that resemble the network anomalies affected
to address security challenges in modern DevOps practices, contributing to
advancing software security and reliability.

</details>


### [201] [Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical software assessments](https://arxiv.org/abs/2505.01307)
*Regan Bolton,Mohammadreza Sheikhfathollahi,Simon Parkinson,Vanessa Vulovic,Gary Bamford,Dan Basher,Howard Parkinson*

Main category: cs.SE

TL;DR: DRAFT是一种结合检索增强生成（RAG）和微调的新方法，用于提升大型语言模型在安全关键合规评估中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的手动评估方法在处理复杂法规框架时效率低下，需要更高效的自动化解决方案。

Method: DRAFT采用双检索架构，同时访问软件文档和参考标准，并通过半自动数据集生成方法进行微调。

Result: 实验显示DRAFT在GPT-4o-mini上正确率提升7%，证据处理、响应结构和领域推理能力均有改善。

Conclusion: DRAFT为合规评估系统提供了一种透明且基于证据的实用改进方法。

Abstract: Safety critical software assessment requires robust assessment against
complex regulatory frameworks, a process traditionally limited by manual
evaluation. This paper presents Document Retrieval-Augmented Fine-Tuning
(DRAFT), a novel approach that enhances the capabilities of a large language
model (LLM) for safety-critical compliance assessment. DRAFT builds upon
existing Retrieval-Augmented Generation (RAG) techniques by introducing a novel
fine-tuning framework that accommodates our dual-retrieval architecture, which
simultaneously accesses both software documentation and applicable reference
standards. To fine-tune DRAFT, we develop a semi-automated dataset generation
methodology that incorporates variable numbers of relevant documents with
meaningful distractors, closely mirroring real-world assessment scenarios.
Experiments with GPT-4o-mini demonstrate a 7% improvement in correctness over
the baseline model, with qualitative improvements in evidence handling,
response structure, and domain-specific reasoning. DRAFT represents a practical
approach to improving compliance assessment systems while maintaining the
transparency and evidence-based reasoning essential in regulatory domains.

</details>


### [202] [Aggregating empirical evidence from data strategy studies: a case on model quantization](https://arxiv.org/abs/2505.00816)
*Santiago del Rey,Paulo Sérgio Medeiros dos Santos,Guilherme Horta Travassos,Xavier Franch,Silverio Martínez-Fernández*

Main category: cs.SE

TL;DR: 该研究评估了模型量化对深度学习系统正确性和资源效率的影响，并探讨了数据策略研究的证据聚合方法。


<details>
  <summary>Details</summary>
Motivation: 随着实证软件工程的发展，研究更多采用数据策略（如模型、源代码或系统日志），但结果合成面临新挑战。

Method: 通过结构化合成方法（SSM）对六项模型量化的实证研究进行定性定量分析，提取并聚合了19个证据模型。

Result: 模型量化对正确性有轻微负面影响，但显著提升资源效率（存储、推理延迟、GPU能耗），且证据显示不同量化技术的研究仍分散。

Conclusion: 模型量化在资源受限环境中是可行的优化策略，同时证明了SSM在数据策略研究中的合成可行性。

Abstract: Background: As empirical software engineering evolves, more studies adopt
data strategies$-$approaches that investigate digital artifacts such as models,
source code, or system logs rather than relying on human subjects. Synthesizing
results from such studies introduces new methodological challenges.
  Aims: This study assesses the effects of model quantization on correctness
and resource efficiency in deep learning (DL) systems. Additionally, it
explores the methodological implications of aggregating evidence from empirical
studies that adopt data strategies.
  Method: We conducted a research synthesis of six primary studies that
empirically evaluate model quantization. We applied the Structured Synthesis
Method (SSM) to aggregate the findings, which combines qualitative and
quantitative evidence through diagrammatic modeling. A total of 19 evidence
models were extracted and aggregated.
  Results: The aggregated evidence indicates that model quantization weakly
negatively affects correctness metrics while consistently improving resource
efficiency metrics, including storage size, inference latency, and GPU energy
consumption$-$a manageable trade-off for many DL deployment contexts. Evidence
across quantization techniques remains fragmented, underscoring the need for
more focused empirical studies per technique.
  Conclusions: Model quantization offers substantial efficiency benefits with
minor trade-offs in correctness, making it a suitable optimization strategy for
resource-constrained environments. This study also demonstrates the feasibility
of using SSM to synthesize findings from data strategy-based research.

</details>


### [203] [CppSATD: A Reusable Self-Admitted Technical Debt Dataset in C++](https://arxiv.org/abs/2505.01136)
*Phuoc Pham,Murali Sridharan,Matteo Esposito,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 本文介绍了CppSATD，一个专门用于C++的自承认技术债务（SATD）数据集，填补了跨语言SATD研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有SATD研究主要集中在Java语言，限制了跨语言SATD的通用性和检测技术的推广。

Method: 通过构建包含超过531,000条注释及其源代码上下文的C++ SATD数据集（CppSATD）。

Result: CppSATD数据集为未来研究提供了基础，支持C++ SATD检测方法的开发及跨语言SATD研究的扩展。

Conclusion: 该数据集有助于推动跨语言SATD研究，并为相关领域提供新的见解。

Abstract: In software development, technical debt (TD) refers to suboptimal
implementation choices made by the developers to meet urgent deadlines and
limited resources, posing challenges for future maintenance. Self-Admitted
Technical Debt (SATD) is a sub-type of TD, representing specific TD instances
``openly admitted'' by the developers and often expressed through source code
comments. Previous research on SATD has focused predominantly on the Java
programming language, revealing a significant gap in cross-language SATD. Such
a narrow focus limits the generalizability of existing findings as well as SATD
detection techniques across multiple programming languages. Our work addresses
such limitation by introducing CppSATD, a dedicated C++ SATD dataset,
comprising over 531,000 annotated comments and their source code contexts. Our
dataset can serve as a foundation for future studies that aim to develop SATD
detection methods in C++, generalize the existing findings to other languages,
or contribute novel insights to cross-language SATD research.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [204] [To Repair or Not to Repair? Investigating the Importance of AB-Cycles for the State-of-the-Art TSP Heuristic EAX](https://arxiv.org/abs/2505.00803)
*Jonathan Heins,Darrell Whitley,Pascal Kerschke*

Main category: cs.NE

TL;DR: 本文提出了一种改进的EAX算法第一阶段方法，通过快速验证AB-cycles的有效性，提高了计算效率和求解质量。


<details>
  <summary>Details</summary>
Motivation: EAX算法的第一阶段尚未深入研究，本文旨在填补这一空白，并提升算法性能。

Method: 提出了一种新方法，用于快速验证AB-cycles的有效性，并基于此改进了EAX算法。

Result: 在10,000个TSP实例的测试中，改进后的EAX算法在计算效率和求解质量上表现更优。

Conclusion: 改进的EAX算法在解决困难实例时优于现有方法，验证了其有效性。

Abstract: The Edge Assembly Crossover (EAX) algorithm is the state-of-the-art heuristic
for solving the Traveling Salesperson Problem (TSP). It regularly outperforms
other methods, such as the Lin-Kernighan-Helsgaun heuristic (LKH), across
diverse sets of TSP instances. Essentially, EAX employs a two-stage mechanism
that focuses on improving the current solutions, first, at the local and,
subsequently, at the global level. Although the second phase of the algorithm
has been thoroughly studied, configured, and refined in the past, in
particular, its first stage has hardly been examined.
  In this paper, we thus focus on the first stage of EAX and introduce a novel
method that quickly verifies whether the AB-cycles, generated during its
internal optimization procedure, yield valid tours -- or whether they need to
be repaired. Knowledge of the latter is also particularly relevant before
applying other powerful crossover operators such as the Generalized Partition
Crossover (GPX). Based on our insights, we propose and evaluate several
improved versions of EAX. According to our benchmark study across 10 000
different TSP instances, the most promising of our proposed EAX variants
demonstrates improved computational efficiency and solution quality on
previously rather difficult instances compared to the current state-of-the-art
EAX algorithm.

</details>


### [205] [A Neural Architecture Search Method using Auxiliary Evaluation Metric based on ResNet Architecture](https://arxiv.org/abs/2505.01313)
*Shang Wang,Huanrong Tang,Jianquan Ouyang*

Main category: cs.NE

TL;DR: 提出了一种基于ResNet的神经架构搜索空间，优化目标包括卷积、池化、全连接层参数及残差网络连接性，同时使用验证集损失值作为次要优化目标。实验表明该方法在MNIST、Fashion-MNIST和CIFAR100数据集上能找到有竞争力的网络架构。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过神经架构搜索（NAS）自动设计高效的网络架构，同时结合ResNet框架和多重优化目标。

Method: 以ResNet为框架，设计搜索空间，优化目标包括卷积、池化、全连接层参数及残差网络连接性，并使用验证集损失值作为次要优化目标。

Result: 在MNIST、Fashion-MNIST和CIFAR100数据集上，该方法能找到具有竞争力的网络架构。

Conclusion: 提出的搜索空间和优化方法在多个数据集上表现优异，验证了其有效性。

Abstract: This paper proposes a neural architecture search space using ResNet as a
framework, with search objectives including parameters for convolution,
pooling, fully connected layers, and connectivity of the residual network. In
addition to recognition accuracy, this paper uses the loss value on the
validation set as a secondary objective for optimization. The experimental
results demonstrate that the search space of this paper together with the
optimisation approach can find competitive network architectures on the MNIST,
Fashion-MNIST and CIFAR100 datasets.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [206] [JFlow: Model-Independent Spherical Jeans Analysis using Equivariant Continuous Normalizing Flows](https://arxiv.org/abs/2505.00763)
*Sung Hak Lim,Kohei Hayashi,Shun'ichi Horigome,Shigeki Matsumoto,Mihoko M. Nojiri*

Main category: astro-ph.GA

TL;DR: 本文提出了一种无监督机器学习方法，用于解决球对称Jeans方程，以模型无关的方式分析矮球状星系。


<details>
  <summary>Details</summary>
Motivation: 研究矮球状星系中恒星的运动学以理解暗物质晕结构，但传统方法依赖参数化模型，限制了分析的灵活性。

Method: 使用等变连续归一化流（equivariant continuous normalizing flows）无模型假设地估计球对称恒星相空间密度和速度弥散。

Result: 在球对称模型的Gaia挑战数据集上验证了方法的有效性，能够准确识别暗物质晕结构，即使恒星数量较少。

Conclusion: 该方法为模型无关的矮球状星系分析提供了新途径，展示了机器学习在天体物理学中的潜力。

Abstract: The kinematics of stars in dwarf spheroidal galaxies have been studied to
understand the structure of dark matter halos. However, the kinematic
information of these stars is often limited to celestial positions and
line-of-sight velocities, making full phase space analysis challenging.
Conventional methods rely on projected analytic phase space density models with
several parameters and infer dark matter halo structures by solving the
spherical Jeans equation. In this paper, we introduce an unsupervised machine
learning method for solving the spherical Jeans equation in a model-independent
way as a first step toward model-independent analysis of dwarf spheroidal
galaxies. Using equivariant continuous normalizing flows, we demonstrate that
spherically symmetric stellar phase space densities and velocity dispersions
can be estimated without model assumptions. As a proof of concept, we apply our
method to Gaia challenge datasets for spherical models and measure dark matter
mass densities given velocity anisotropy profiles. Our method can identify halo
structures accurately, even with a small number of tracer stars.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [207] [Multi-site modelling and reconstruction of past extreme skew surges along the French Atlantic coast](https://arxiv.org/abs/2505.00835)
*Nathan Huet,Philippe Naveau,Anne Sabourin*

Main category: stat.AP

TL;DR: 研究提出了一种新方法，用于建模法国大西洋沿岸极端偏斜涌浪的极值依赖性结构，结合多元广义帕累托分布和极端回归框架，以重建历史数据。


<details>
  <summary>Details</summary>
Motivation: 为海岸风险管理提供更准确的极端偏斜涌浪建模方法。

Method: 采用峰值超阈值框架，提出新阈值确定方法，结合多元广义帕累托分布和极端回归框架进行建模。

Result: 能够基于邻近站点数据预测极端偏斜涌浪，并重建历史时间序列。

Conclusion: 该方法为有限数据站点的历史数据重建提供了有效工具。

Abstract: Appropriate modelling of extreme skew surges is crucial, particularly for
coastal risk management. Our study focuses on modelling extreme skew surges
along the French Atlantic coast, with a particular emphasis on investigating
the extremal dependence structure between stations. We employ the
peak-over-threshold framework, where a multivariate extreme event is defined
whenever at least one location records a large value, though not necessarily
all stations simultaneously. A novel method for determining an appropriate
level (threshold) above which observations can be classified as extreme is
proposed. Two complementary approaches are explored. First, the multivariate
generalized Pareto distribution is employed to model extremes, leveraging its
properties to derive a generative model that predicts extreme skew surges at
one station based on observed extremes at nearby stations. Second, a novel
extreme regression framework is assessed for point predictions. This specific
regression framework enables accurate point predictions using only the "angle"
of input variables, i.e. input variables divided by their norms. The ultimate
objective is to reconstruct historical skew surge time series at stations with
limited data. This is achieved by integrating extreme skew surge data from
stations with longer records, such as Brest and Saint-Nazaire, which provide
over 150 years of observations.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [208] [SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation](https://arxiv.org/abs/2505.00831)
*Quang P. M. Pham,Khoi T. N. Nguyen,Nhi H. Doan,Cuong A. Pham,Kentaro Inui,Dezhen Song*

Main category: cs.RO

TL;DR: SmallPlan利用大型语言模型（LLM）作为教师模型，训练轻量级小语言模型（SLM）用于高效路径规划，适用于动态环境和边缘设备。


<details>
  <summary>Details</summary>
Motivation: 解决大规模动态环境中机器人路径规划的高计算成本和实时性问题。

Method: 通过LLM引导的监督微调（SFT）和强化学习（RL）训练SLM，结合场景图表示3D环境。

Result: SLM在路径规划任务中表现与GPT-4o相当，且避免了幻觉和过拟合。

Conclusion: SmallPlan资源高效，适合边缘设备部署，推动自主机器人技术发展。

Abstract: Efficient path planning in robotics, particularly within large-scale, dynamic
environments, remains a significant hurdle. While Large Language Models (LLMs)
offer strong reasoning capabilities, their high computational cost and limited
adaptability in dynamic scenarios hinder real-time deployment on edge devices.
We present SmallPlan -- a novel framework leveraging LLMs as teacher models to
train lightweight Small Language Models (SLMs) for high-level path planning
tasks. In SmallPlan, the SLMs provide optimal action sequences to navigate
across scene graphs that compactly represent full-scaled 3D scenes. The SLMs
are trained in a simulation-powered, interleaved manner with LLM-guided
supervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not
only enables SLMs to successfully complete navigation tasks but also makes them
aware of important factors like travel distance and number of trials. Through
experiments, we demonstrate that the fine-tuned SLMs perform competitively with
larger models like GPT-4o on sequential path planning, without suffering from
hallucination and overfitting. SmallPlan is resource-efficient, making it
well-suited for edge-device deployment and advancing practical autonomous
robotics.

</details>


### [209] [IK Seed Generator for Dual-Arm Human-like Physicality Robot with Mobile Base](https://arxiv.org/abs/2505.00871)
*Jun Takamatsu,Atsushi Kanehira,Kazuhiro Sasabuchi,Naoki Wake,Katsushi Ikeuchi*

Main category: cs.RO

TL;DR: 本文提出了一种通过遗传算法优化初始猜测的方法，以提高数值逆运动学（IK）求解器的成功率，适用于受尺寸限制的机器人。


<details>
  <summary>Details</summary>
Motivation: 解决受尺寸限制的机器人在逆运动学求解中的困难，提升其任务执行能力。

Method: 使用遗传算法优化初始猜测，结合可操纵性指标和关节限制的缩放雅可比矩阵。

Result: 实验证明，优化后的初始猜测显著提高了IK求解的成功率。

Conclusion: 该方法有效提升了受限机器人的IK求解能力，并成功应用于实际场景。

Abstract: Robots are strongly expected as a means of replacing human tasks. If a robot
has a human-like physicality, the possibility of replacing human tasks
increases. In the case of household service robots, it is desirable for them to
be on a human-like size so that they do not become excessively large in order
to coexist with humans in their operating environment. However, robots with
size limitations tend to have difficulty solving inverse kinematics (IK) due to
mechanical limitations, such as joint angle limitations. Conversely, if the
difficulty coming from this limitation could be mitigated, one can expect that
the use of such robots becomes more valuable. In numerical IK solver, which is
commonly used for robots with higher degrees-of-freedom (DOF), the solvability
of IK depends on the initial guess given to the solver. Thus, this paper
proposes a method for generating a good initial guess for a numerical IK solver
given the target hand configuration. For the purpose, we define the goodness of
an initial guess using the scaled Jacobian matrix, which can calculate the
manipulability index considering the joint limits. These two factors are
related to the difficulty of solving IK. We generate the initial guess by
optimizing the goodness using the genetic algorithm (GA). To enumerate much
possible IK solutions, we use the reachability map that represents the
reachable area of the robot hand in the arm-base coordinate system. We conduct
quantitative evaluation and prove that using an initial guess that is judged to
be better using the goodness value increases the probability that IK is solved.
Finally, as an application of the proposed method, we show that by generating
good initial guesses for IK a robot actually achieves three typical scenarios.

</details>


### [210] [Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning](https://arxiv.org/abs/2505.00935)
*Roberto Bigazzi*

Main category: cs.RO

TL;DR: 论文探讨了具身人工智能（Embodied AI）的发展，结合计算机视觉、机器人和决策制定，旨在开发智能自主机器人。通过3D模型仿真训练智能体，并在真实环境中部署。


<details>
  <summary>Details</summary>
Motivation: 推动具身AI和自主智能体的研究，促进未来工作。

Method: 利用3D模型进行仿真训练，智能体学习与环境交互，提取信息并执行任务。

Result: 提出了一套完整的具身智能体实现流程，包括技术方法和实验研究。

Conclusion: 为具身AI领域的研究提供了贡献，并展示了智能体在室内环境中的应用潜力。

Abstract: The increase in available computing power and the Deep Learning revolution
have allowed the exploration of new topics and frontiers in Artificial
Intelligence research. A new field called Embodied Artificial Intelligence,
which places at the intersection of Computer Vision, Robotics, and Decision
Making, has been gaining importance during the last few years, as it aims to
foster the development of smart autonomous robots and their deployment in
society. The recent availability of large collections of 3D models for
photorealistic robotic simulation has allowed faster and safe training of
learning-based agents for millions of frames and a careful evaluation of their
behavior before deploying the models on real robotic platforms. These
intelligent agents are intended to perform a certain task in a possibly unknown
environment. To this end, during the training in simulation, the agents learn
to perform continuous interactions with the surroundings, such as gathering
information from the environment, encoding and extracting useful cues for the
task, and performing actions towards the final goal; where every action of the
agent influences the interactions. This dissertation follows the complete
creation process of embodied agents for indoor environments, from their concept
to their implementation and deployment. We aim to contribute to research in
Embodied AI and autonomous agents, in order to foster future work in this
field. We present a detailed analysis of the procedure behind implementing an
intelligent embodied agent, comprehending a thorough description of the current
state-of-the-art in literature, technical explanations of the proposed methods,
and accurate experimental studies on relevant robotic tasks.

</details>


### [211] [Model Tensor Planning](https://arxiv.org/abs/2505.01059)
*An T. Le,Khai Nguyen,Minh Nhat Vu,João Carvalho,Jan Peters*

Main category: cs.RO

TL;DR: 提出了一种名为MTP的新型采样MPC框架，通过结构化张量采样实现高熵控制轨迹生成，平衡局部优化与全局探索。


<details>
  <summary>Details</summary>
Motivation: 解决传统采样MPC在非线性任务中因局部贪婪采样导致的探索不足问题。

Method: 采用随机多部图采样和B样条/Akima样条插值生成平滑多样的控制候选，结合β混合策略平衡探索与优化。

Result: 实验表明MTP在多种机器人任务中优于标准MPC和进化策略基线，设计消融验证了其有效性。

Conclusion: MTP为基于模型的规划与控制提供了可扩展的鲁棒探索框架。

Abstract: Sampling-based model predictive control (MPC) offers strong performance in
nonlinear and contact-rich robotic tasks, yet often suffers from poor
exploration due to locally greedy sampling schemes. We propose \emph{Model
Tensor Planning} (MTP), a novel sampling-based MPC framework that introduces
high-entropy control trajectory generation through structured tensor sampling.
By sampling over randomized multipartite graphs and interpolating control
trajectories with B-splines and Akima splines, MTP ensures smooth and globally
diverse control candidates. We further propose a simple $\beta$-mixing strategy
that blends local exploitative and global exploratory samples within the
modified Cross-Entropy Method (CEM) update, balancing control refinement and
exploration. Theoretically, we show that MTP achieves asymptotic path coverage
and maximum entropy in the control trajectory space in the limit of infinite
tensor depth and width.
  Our implementation is fully vectorized using JAX and compatible with MuJoCo
XLA, supporting \emph{Just-in-time} (JIT) compilation and batched rollouts for
real-time control with online domain randomization. Through experiments on
various challenging robotic tasks, ranging from dexterous in-hand manipulation
to humanoid locomotion, we demonstrate that MTP outperforms standard MPC and
evolutionary strategy baselines in task success and control robustness. Design
and sensitivity ablations confirm the effectiveness of MTP tensor sampling
structure, spline interpolation choices, and mixing strategy. Altogether, MTP
offers a scalable framework for robust exploration in model-based planning and
control.

</details>


### [212] [Optimizing Indoor Farm Monitoring Efficiency Using UAV: Yield Estimation in a GNSS-Denied Cherry Tomato Greenhouse](https://arxiv.org/abs/2505.00995)
*Taewook Park,Jinwoo Lee,Hyondong Oh,Won-Jae Yun,Kyu-Wha Lee*

Main category: cs.RO

TL;DR: 论文提出了一种轻量级无人机系统，用于温室中的番茄产量估计，解决了地面机器人在温室中部署的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着农业劳动力减少和成本上升，机器人产量估计变得重要。地面机器人在温室中部署受限，因此需要更高效的解决方案。

Method: 开发了配备RGB-D相机、3D LiDAR和IMU传感器的无人机，采用LiDAR-惯性里程计算法导航，并使用3D多目标跟踪算法估计番茄数量和重量。

Result: 在收获行数据集中，系统计数准确率94.4%，重量估计准确率87.5%；在生长行数据集中，定性分析了遮挡情况下的跟踪性能。

Conclusion: 无人机系统在温室产量估计中具有潜力，未来可进一步优化遮挡环境下的感知能力。

Abstract: As the agricultural workforce declines and labor costs rise, robotic yield
estimation has become increasingly important. While unmanned ground vehicles
(UGVs) are commonly used for indoor farm monitoring, their deployment in
greenhouses is often constrained by infrastructure limitations, sensor
placement challenges, and operational inefficiencies. To address these issues,
we develop a lightweight unmanned aerial vehicle (UAV) equipped with an RGB-D
camera, a 3D LiDAR, and an IMU sensor. The UAV employs a LiDAR-inertial
odometry algorithm for precise navigation in GNSS-denied environments and
utilizes a 3D multi-object tracking algorithm to estimate the count and weight
of cherry tomatoes. We evaluate the system using two dataset: one from a
harvesting row and another from a growing row. In the harvesting-row dataset,
the proposed system achieves 94.4\% counting accuracy and 87.5\% weight
estimation accuracy within a 13.2-meter flight completed in 10.5 seconds. For
the growing-row dataset, which consists of occluded unripened fruits, we
qualitatively analyze tracking performance and highlight future research
directions for improving perception in greenhouse with strong occlusions. Our
findings demonstrate the potential of UAVs for efficient robotic yield
estimation in commercial greenhouses.

</details>


### [213] [NeuroLoc: Encoding Navigation Cells for 6-DOF Camera Localization](https://arxiv.org/abs/2505.01113)
*Xun Li,Jian Yang,Fenli Jia,Muyu Wang,Qi Wu,Jun Wu,Jinpeng Mi,Jilin Hu,Peidong Liang,Xuan Tang,Ke Li,Xiong You,Xian Wei*

Main category: cs.RO

TL;DR: 提出了一种基于生物脑导航机制的NeuroLoc方法，用于解决相机定位中的场景模糊和环境干扰问题。


<details>
  <summary>Details</summary>
Motivation: 自主导航在未知环境中常因场景模糊、环境干扰和动态物体变化而受限，需改进相机定位方法。

Method: 结合Hebbian学习模块、方向学习嵌入和3D网格中心预测，提升定位精度。

Result: 在室内外数据集上验证，NeuroLoc提高了复杂环境下的鲁棒性和姿态回归性能。

Conclusion: NeuroLoc通过单张图像显著提升了相机定位的准确性和鲁棒性。

Abstract: Recently, camera localization has been widely adopted in autonomous robotic
navigation due to its efficiency and convenience. However, autonomous
navigation in unknown environments often suffers from scene ambiguity,
environmental disturbances, and dynamic object transformation in camera
localization. To address this problem, inspired by the biological brain
navigation mechanism (such as grid cells, place cells, and head direction
cells), we propose a novel neurobiological camera location method, namely
NeuroLoc. Firstly, we designed a Hebbian learning module driven by place cells
to save and replay historical information, aiming to restore the details of
historical representations and solve the issue of scene fuzziness. Secondly, we
utilized the head direction cell-inspired internal direction learning as
multi-head attention embedding to help restore the true orientation in similar
scenes. Finally, we added a 3D grid center prediction in the pose regression
module to reduce the final wrong prediction. We evaluate the proposed NeuroLoc
on commonly used benchmark indoor and outdoor datasets. The experimental
results show that our NeuroLoc can enhance the robustness in complex
environments and improve the performance of pose regression by using only a
single image.

</details>


### [214] [ViSA-Flow: Accelerating Robot Skill Learning via Large-Scale Video Semantic Action Flow](https://arxiv.org/abs/2505.01288)
*Changhe Chen,Quantao Yang,Xiaohao Xu,Nima Fazeli,Olov Andersson*

Main category: cs.RO

TL;DR: ViSA-Flow利用语义动作流作为中间表示，通过自监督学习从大规模无标签视频数据中学习，并在少量机器人演示上微调，实现了从人类视频观察到机器人执行的知识迁移。


<details>
  <summary>Details</summary>
Motivation: 解决机器人获取复杂操作技能时大规模演示数据成本过高的问题，借鉴人类通过观察学习的能力。

Method: 提出语义动作流作为核心表示，通过生成模型预训练并从人类-物体交互视频中提取，再在少量机器人演示上微调。

Result: 在CALVIN基准测试和真实任务中表现优异，尤其在低数据情况下优于现有方法。

Conclusion: ViSA-Flow通过语义动作流有效实现了从人类观察到机器人执行的知识迁移，显著提升了性能。

Abstract: One of the central challenges preventing robots from acquiring complex
manipulation skills is the prohibitive cost of collecting large-scale robot
demonstrations. In contrast, humans are able to learn efficiently by watching
others interact with their environment. To bridge this gap, we introduce
semantic action flow as a core intermediate representation capturing the
essential spatio-temporal manipulator-object interactions, invariant to
superficial visual differences. We present ViSA-Flow, a framework that learns
this representation self-supervised from unlabeled large-scale video data.
First, a generative model is pre-trained on semantic action flows automatically
extracted from large-scale human-object interaction video data, learning a
robust prior over manipulation structure. Second, this prior is efficiently
adapted to a target robot by fine-tuning on a small set of robot demonstrations
processed through the same semantic abstraction pipeline. We demonstrate
through extensive experiments on the CALVIN benchmark and real-world tasks that
ViSA-Flow achieves state-of-the-art performance, particularly in low-data
regimes, outperforming prior methods by effectively transferring knowledge from
human video observation to robotic execution. Videos are available at
https://visaflow-web.github.io/ViSAFLOW.

</details>


### [215] [Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures](https://arxiv.org/abs/2505.00779)
*Junwon Seo,Kensuke Nakamura,Andrea Bajcsy*

Main category: cs.RO

TL;DR: 论文提出了一种基于不确定性感知的潜在安全过滤器，用于防止机器人在已知和未知的安全风险中失败。


<details>
  <summary>Details</summary>
Motivation: 现有的潜在安全过滤器可能无法覆盖所有安全关键场景，导致对未知风险的误判。

Method: 利用世界模型的认知不确定性作为代理，通过符合预测校准不确定性阈值，并在增强的状态空间中进行分析。

Result: 在仿真和硬件实验中，该方法能可靠地检测潜在不安全场景并建议安全动作。

Conclusion: 不确定性感知的安全过滤器能有效保护机器人免受已知和未知的安全风险。

Abstract: Recent advances in generative world models have enabled classical safe
control methods, such as Hamilton-Jacobi (HJ) reachability, to generalize to
complex robotic systems operating directly from high-dimensional sensor
observations. However, obtaining comprehensive coverage of all safety-critical
scenarios during world model training is extremely challenging. As a result,
latent safety filters built on top of these models may miss novel hazards and
even fail to prevent known ones, overconfidently misclassifying risky
out-of-distribution (OOD) situations as safe. To address this, we introduce an
uncertainty-aware latent safety filter that proactively steers robots away from
both known and unseen failures. Our key idea is to use the world model's
epistemic uncertainty as a proxy for identifying unseen potential hazards. We
propose a principled method to detect OOD world model predictions by
calibrating an uncertainty threshold via conformal prediction. By performing
reachability analysis in an augmented state space-spanning both the latent
representation and the epistemic uncertainty-we synthesize a latent safety
filter that can reliably safeguard arbitrary policies from both known and
unseen safety hazards. In simulation and hardware experiments on vision-based
control tasks with a Franka manipulator, we show that our uncertainty-aware
safety filter preemptively detects potential unsafe scenarios and reliably
proposes safe, in-distribution actions. Video results can be found on the
project website at https://cmu-intentlab.github.io/UNISafe

</details>


### [216] [FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft Research](https://arxiv.org/abs/2505.01383)
*Yan Miao,Will Shen,Hang Cui,Sayan Mitra*

Main category: cs.RO

TL;DR: FalconWing是一个开源的超轻量固定翼平台，用于自主性研究，支持纯视觉控制的自主着陆。


<details>
  <summary>Details</summary>
Motivation: 为自主性研究提供一个轻量、开源且功能强大的硬件平台，并探索纯视觉控制的可行性。

Method: 采用3D高斯点云构建仿真环境，从视觉数据识别非线性动力学，并通过模拟训练多模态ViT策略。

Result: 在硬件平台上零样本部署时，视觉自主着陆成功率达80%。

Conclusion: FalconWing及其开源组件为自主性研究提供了实用工具，展示了纯视觉控制的潜力。

Abstract: We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing
platform for autonomy research. The hardware platform integrates a small
camera, a standard airframe, offboard computation, and radio communication for
manual overrides. We demonstrate FalconWing's capabilities by developing and
deploying a purely vision-based control policy for autonomous landing (without
IMU or motion capture) using a novel real-to-sim-to-real learning approach. Our
learning approach: (1) constructs a photorealistic simulation environment via
3D Gaussian splatting trained on real-world images; (2) identifies nonlinear
dynamics from vision-estimated real-flight data; and (3) trains a multi-modal
Vision Transformer (ViT) policy through simulation-only imitation learning. The
ViT architecture fuses single RGB image with the history of control actions via
self-attention, preserving temporal context while maintaining real-time 20 Hz
inference. When deployed zero-shot on the hardware platform, this policy
achieves an 80% success rate in vision-based autonomous landings. Together with
the hardware specifications, we also open-source the system dynamics, the
software for photorealistic simulator and the learning approach.

</details>


### [217] [SIME: Enhancing Policy Self-Improvement with Modal-level Exploration](https://arxiv.org/abs/2505.01396)
*Yang Jin,Jun Lv,Wenye Yu,Hongjie Fang,Yong-Lu Li,Cewu Lu*

Main category: cs.RO

TL;DR: 论文提出了一种通过模态级探索和数据选择实现机器人自我改进的方法，解决了机器人重复现有能力而无法生成新学习数据的挑战。


<details>
  <summary>Details</summary>
Motivation: 机器人需要通过与环境互动逐步提升能力，但现有方法难以生成有价值的新数据。

Method: 引入模态级探索机制，生成多样化互动，并选择最有价值的试验和高质量片段用于学习。

Result: 在仿真基准和真实实验中成功实现了有效的机器人自我改进。

Conclusion: 该方法能以更低成本开发更稳健、高成功率的机器人控制策略。

Abstract: Self-improvement requires robotic systems to initially learn from
human-provided data and then gradually enhance their capabilities through
interaction with the environment. This is similar to how humans improve their
skills through continuous practice. However, achieving effective
self-improvement is challenging, primarily because robots tend to repeat their
existing abilities during interactions, often failing to generate new, valuable
data for learning. In this paper, we identify the key to successful
self-improvement: modal-level exploration and data selection. By incorporating
a modal-level exploration mechanism during policy execution, the robot can
produce more diverse and multi-modal interactions. At the same time, we select
the most valuable trials and high-quality segments from these interactions for
learning. We successfully demonstrate effective robot self-improvement on both
simulation benchmarks and real-world experiments. The capability for
self-improvement will enable us to develop more robust and high-success-rate
robotic control strategies at a lower cost. Our code and experiment scripts are
available at https://ericjin2002.github.io/SIME/

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [218] [Learning Low-Dimensional Embeddings for Black-Box Optimization](https://arxiv.org/abs/2505.01112)
*Riccardo Busetto,Manas Mejari,Marco Forgione,Alberto Bemporad,Dario Piga*

Main category: eess.SY

TL;DR: 提出了一种基于元学习的方法，通过预计算降维流形来优化高维黑盒问题。


<details>
  <summary>Details</summary>
Motivation: 黑盒优化在高维问题和有限试验预算下表现不佳，需要更高效的方法。

Method: 利用元学习预计算降维流形，将优化问题限制在低维空间。

Result: 在降维空间中优化新问题实例，显著减少了寻找近优解的努力。

Conclusion: 该方法为高维黑盒优化提供了一种高效解决方案。

Abstract: When gradient-based methods are impractical, black-box optimization (BBO)
provides a valuable alternative. However, BBO often struggles with
high-dimensional problems and limited trial budgets. In this work, we propose a
novel approach based on meta-learning to pre-compute a reduced-dimensional
manifold where optimal points lie for a specific class of optimization
problems. When optimizing a new problem instance sampled from the class,
black-box optimization is carried out in the reduced-dimensional space,
effectively reducing the effort required for finding near-optimal solutions.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [219] [Towards Explainable Temporal User Profiling with LLMs](https://arxiv.org/abs/2505.00886)
*Milad Sabouri,Masoud Mansoury,Kun Lin,Bamshad Mobasher*

Main category: cs.IR

TL;DR: 利用大语言模型（LLM）生成用户交互历史的自然语言摘要，区分短期和长期偏好，提升推荐系统的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统用户画像方法（如平均项目嵌入）忽略了用户兴趣的动态性和复杂性，尤其是短期与长期偏好的交互。

Method: 通过LLM生成用户交互历史的自然语言摘要，使用预训练模型编码，并通过注意力机制动态融合短期和长期嵌入。

Result: 在多个基准测试中提高了推荐准确性，并支持通过可解释的文本摘要和注意力权重向用户解释推荐理由。

Conclusion: 该方法不仅提升了推荐性能，还增强了推荐系统的透明度和可解释性。

Abstract: Accurately modeling user preferences is vital not only for improving
recommendation performance but also for enhancing transparency in recommender
systems. Conventional user profiling methods, such as averaging item
embeddings, often overlook the evolving, nuanced nature of user interests,
particularly the interplay between short-term and long-term preferences. In
this work, we leverage large language models (LLMs) to generate natural
language summaries of users' interaction histories, distinguishing recent
behaviors from more persistent tendencies. Our framework not only models
temporal user preferences but also produces natural language profiles that can
be used to explain recommendations in an interpretable manner. These textual
profiles are encoded via a pre-trained model, and an attention mechanism
dynamically fuses the short-term and long-term embeddings into a comprehensive
user representation. Beyond boosting recommendation accuracy over multiple
baselines, our approach naturally supports explainability: the interpretable
text summaries and attention weights can be exposed to end users, offering
insights into why specific items are suggested. Experiments on real-world
datasets underscore both the performance gains and the promise of generating
clearer, more transparent justifications for content-based recommendations.

</details>


### [220] [Preserving Privacy and Utility in LLM-Based Product Recommendations](https://arxiv.org/abs/2505.00951)
*Tina Khezresmaeilzadeh,Jiang Zhang,Dimitrios Andreadis,Konstantinos Psounis*

Main category: cs.IR

TL;DR: 提出了一种混合隐私保护推荐框架，通过分离敏感数据并仅在云端共享非敏感数据，同时设计本地去混淆模块恢复敏感推荐，实现了隐私与推荐质量的平衡。


<details>
  <summary>Details</summary>
Motivation: 传统LLM推荐系统需将用户数据传输至云端，引发隐私风险。本文旨在解决这一问题，同时保持推荐效果。

Method: 提出混合框架，分离敏感与非敏感数据，仅共享非敏感数据至云端；设计本地去混淆模块恢复敏感推荐。

Result: 在真实电商数据集上，框架在保持隐私的同时，推荐效果接近全数据共享系统，优于仅混淆技术。

Conclusion: 该方法高效且适用于消费级硬件，为隐私保护的LLM推荐系统提供了实用解决方案。

Abstract: Large Language Model (LLM)-based recommendation systems leverage powerful
language models to generate personalized suggestions by processing user
interactions and preferences. Unlike traditional recommendation systems that
rely on structured data and collaborative filtering, LLM-based models process
textual and contextual information, often using cloud-based infrastructure.
This raises privacy concerns, as user data is transmitted to remote servers,
increasing the risk of exposure and reducing control over personal information.
To address this, we propose a hybrid privacy-preserving recommendation
framework which separates sensitive from nonsensitive data and only shares the
latter with the cloud to harness LLM-powered recommendations. To restore lost
recommendations related to obfuscated sensitive data, we design a
de-obfuscation module that reconstructs sensitive recommendations locally.
Experiments on real-world e-commerce datasets show that our framework achieves
almost the same recommendation utility with a system which shares all data with
an LLM, while preserving privacy to a large extend. Compared to
obfuscation-only techniques, our approach improves HR@10 scores and category
distribution alignment, offering a better balance between privacy and
recommendation quality. Furthermore, our method runs efficiently on
consumer-grade hardware, making privacy-aware LLM-based recommendation systems
practical for real-world use.

</details>


### [221] [Enhancing User Sequence Modeling through Barlow Twins-based Self-Supervised Learning](https://arxiv.org/abs/2505.00953)
*Yuhan Liu,Lin Ning,Neo Wu,Karan Singhal,Philip Andrew Mansfield,Devora Berlowitz,Sushant Prakash,Bradley Green*

Main category: cs.IR

TL;DR: 该论文提出了一种基于Barlow Twins的自监督学习方法，用于用户序列建模，减少了对大量负样本的需求，并在多个数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 用户序列建模在推荐系统中至关重要，但缺乏标记数据是一个主要挑战。现有自监督学习方法依赖大量负样本，计算成本高且不实用。

Method: 通过改进Barlow Twins方法，结合合适的增强技术，减少对大批量负样本的需求，实现小批量下的有效表示学习。

Result: 在MovieLens和Yelp数据集上，该方法在三个下游任务中均优于双编码器模型，准确率提升8%-20%。

Conclusion: 该方法在标记数据稀缺和负样本有限的情况下，能有效提取用户序列信息，具有实际应用价值。

Abstract: User sequence modeling is crucial for modern large-scale recommendation
systems, as it enables the extraction of informative representations of users
and items from their historical interactions. These user representations are
widely used for a variety of downstream tasks to enhance users' online
experience. A key challenge for learning these representations is the lack of
labeled training data. While self-supervised learning (SSL) methods have
emerged as a promising solution for learning representations from unlabeled
data, many existing approaches rely on extensive negative sampling, which can
be computationally expensive and may not always be feasible in real-world
scenario. In this work, we propose an adaptation of Barlow Twins, a
state-of-the-art SSL methods, to user sequence modeling by incorporating
suitable augmentation methods. Our approach aims to mitigate the need for large
negative sample batches, enabling effective representation learning with
smaller batch sizes and limited labeled data. We evaluate our method on the
MovieLens-1M, MovieLens-20M, and Yelp datasets, demonstrating that our method
consistently outperforms the widely-used dual encoder model across three
downstream tasks, achieving an 8%-20% improvement in accuracy. Our findings
underscore the effectiveness of our approach in extracting valuable
sequence-level information for user modeling, particularly in scenarios where
labeled data is scarce and negative examples are limited.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [222] [CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment](https://arxiv.org/abs/2505.01237)
*Edson Araujo,Andrew Rouditchenko,Yuan Gong,Saurabhchand Bhati,Samuel Thomas,Brian Kingsbury,Leonid Karlinsky,Rogerio Feris,James R. Glass*

Main category: cs.MM

TL;DR: CAV-MAE Sync扩展了CAV-MAE框架，通过时序对齐音频与视频帧、分离对比和重建目标、引入可学习注册令牌，解决了现有音频-视觉学习方法中的粒度不匹配和优化冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在音频-视觉学习中未能捕捉细粒度时序对应关系，且优化目标冲突。

Method: 提出CAV-MAE Sync，将音频作为时序序列对齐视频帧，分离对比和重建目标，引入可学习注册令牌。

Result: 在AudioSet、VGG Sound和ADE20K Sound数据集上，零样本检索、分类和定位任务中表现优异。

Conclusion: CAV-MAE Sync在简单框架下实现了先进性能，优于复杂架构。

Abstract: Recent advances in audio-visual learning have shown promising results in
learning representations across modalities. However, most approaches rely on
global audio representations that fail to capture fine-grained temporal
correspondences with visual frames. Additionally, existing methods often
struggle with conflicting optimization objectives when trying to jointly learn
reconstruction and cross-modal alignment. In this work, we propose CAV-MAE Sync
as a simple yet effective extension of the original CAV-MAE framework for
self-supervised audio-visual learning. We address three key challenges: First,
we tackle the granularity mismatch between modalities by treating audio as a
temporal sequence aligned with video frames, rather than using global
representations. Second, we resolve conflicting optimization goals by
separating contrastive and reconstruction objectives through dedicated global
tokens. Third, we improve spatial localization by introducing learnable
register tokens that reduce semantic load on patch tokens. We evaluate the
proposed approach on AudioSet, VGG Sound, and the ADE20K Sound dataset on
zero-shot retrieval, classification and localization tasks demonstrating
state-of-the-art performance and outperforming more complex architectures.

</details>


### [223] [FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and Flow Matching based Voice Enhancing](https://arxiv.org/abs/2505.01263)
*Gaoxiang Cong,Liang Li,Jiadong Pan,Zhedong Zhang,Amin Beheshti,Anton van den Hengel,Yuankai Qi,Qingming Huang*

Main category: cs.MM

TL;DR: FlowDubber是一种基于大语言模型（LLM）的电影配音方法，通过语音语言模型和双重对比对齐实现高质量的视听同步和发音，同时通过声音增强流匹配提升音质。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注降低单词错误率，而忽视了唇同步和音质的重要性。

Method: 采用Qwen2.5作为LLM骨干，结合语义感知学习、双重对比对齐（DCA）和基于流的声音增强（FVE）技术。

Result: 在多个基准测试中表现优于现有方法。

Conclusion: FlowDubber在视听同步和音质方面表现优异，为电影配音提供了新思路。

Abstract: Movie Dubbing aims to convert scripts into speeches that align with the given
movie clip in both temporal and emotional aspects while preserving the vocal
timbre of a given brief reference audio. Existing methods focus primarily on
reducing the word error rate while ignoring the importance of lip-sync and
acoustic quality. To address these issues, we propose a large language model
(LLM) based flow matching architecture for dubbing, named FlowDubber, which
achieves high-quality audio-visual sync and pronunciation by incorporating a
large speech language model and dual contrastive aligning while achieving
better acoustic quality via the proposed voice-enhanced flow matching than
previous works. First, we introduce Qwen2.5 as the backbone of LLM to learn the
in-context sequence from movie scripts and reference audio. Then, the proposed
semantic-aware learning focuses on capturing LLM semantic knowledge at the
phoneme level. Next, dual contrastive aligning (DCA) boosts mutual alignment
with lip movement, reducing ambiguities where similar phonemes might be
confused. Finally, the proposed Flow-based Voice Enhancing (FVE) improves
acoustic quality in two aspects, which introduces an LLM-based acoustics flow
matching guidance to strengthen clarity and uses affine style prior to enhance
identity when recovering noise into mel-spectrograms via gradient vector field
prediction. Extensive experiments demonstrate that our method outperforms
several state-of-the-art methods on two primary benchmarks. The demos are
available at
{\href{https://galaxycong.github.io/LLM-Flow-Dubber/}{\textcolor{red}{https://galaxycong.github.io/LLM-Flow-Dubber/}}}.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [224] [Quantum Support Vector Regression for Robust Anomaly Detection](https://arxiv.org/abs/2505.01012)
*Kilian Tscharke,Maximilian Wendlinger,Sebastian Issel,Pascal Debus*

Main category: quant-ph

TL;DR: 该研究探讨了量子机器学习（特别是量子核方法）在异常检测中的应用，通过QSVR在IBM量子硬件上的实验，展示了其分类性能及对量子噪声的鲁棒性，同时发现其对对抗攻击的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 异常检测在IT安全中至关重要，而量子机器学习为大规模数据异常检测提供了新方法。本研究旨在探索量子核方法在异常检测中的潜力。

Method: 研究基于QSVR方法，在IBM量子硬件上对11个数据集进行了全面基准测试，并分析了量子噪声对性能的影响。

Result: QSVR在部分数据集上表现优于无噪声模拟，对某些量子噪声表现出鲁棒性，但对对抗攻击高度脆弱。

Conclusion: 量子核方法在异常检测中具有潜力，但需进一步研究以提高其对对抗攻击的鲁棒性。

Abstract: Anomaly Detection (AD) is critical in data analysis, particularly within the
domain of IT security. In recent years, Machine Learning (ML) algorithms have
emerged as a powerful tool for AD in large-scale data. In this study, we
explore the potential of quantum ML approaches, specifically quantum kernel
methods, for the application to robust AD. We build upon previous work on
Quantum Support Vector Regression (QSVR) for semisupervised AD by conducting a
comprehensive benchmark on IBM quantum hardware using eleven datasets. Our
results demonstrate that QSVR achieves strong classification performance and
even outperforms the noiseless simulation on two of these datasets. Moreover,
we investigate the influence of - in the NISQ-era inevitable - quantum noise on
the performance of the QSVR. Our findings reveal that the model exhibits
robustness to depolarizing, phase damping, phase flip, and bit flip noise,
while amplitude damping and miscalibration noise prove to be more disruptive.
Finally, we explore the domain of Quantum Adversarial Machine Learning and
demonstrate that QSVR is highly vulnerable to adversarial attacks and that
noise does not improve the adversarial robustness of the model.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [225] [Dynamical System Parameter Path Optimization using Persistent Homology](https://arxiv.org/abs/2505.00782)
*Max M. Chumley,Firas A. Khasawneh*

Main category: math.DS

TL;DR: 提出了一种基于拓扑数据分析的方法，用于在高维参数空间中优化导航非线性动力系统，通过梯度下降实现参数调整以达到期望的系统响应。


<details>
  <summary>Details</summary>
Motivation: 非线性动力系统复杂且难以解析研究，高维参数空间中难以确定参数调整方向以实现期望的系统响应。

Method: 利用持久图的可微性定义拓扑语言，结合梯度下降优化参数空间路径。

Result: 通过多个动力系统案例展示了如何促进不同拓扑特征及选择超参数以实现不同结果。

Conclusion: 该方法有效指导参数调整，实现期望的拓扑特征，适用于多种动力系统场景。

Abstract: Nonlinear dynamical systems are complex and typically only simple systems can
be analytically studied. In applications, these systems are usually defined
with a set of tunable parameters and as the parameters are varied the system
response undergoes significant topological changes or bifurcations. In a high
dimensional parameter space, it is difficult to determine which direction to
vary the system parameters to achieve a desired system response or state. In
this paper, we introduce a new approach for optimally navigating a dynamical
system parameter space that is rooted in topological data analysis.
Specifically we use the differentiability of persistence diagrams to define a
topological language for intuitively promoting or deterring different
topological features in the state space response of a dynamical system and use
gradient descent to optimally move from one point in the parameter space to
another. The end result is a path in this space that guides the system to a set
of parameters that yield the desired topological features defined by the loss
function. We show a number of examples by applying the methods to different
dynamical systems and scenarios to demonstrate how to promote different
features and how to choose the hyperparameters to achieve different outcomes.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [226] [The Coral Protocol: Open Infrastructure Connecting The Internet of Agents](https://arxiv.org/abs/2505.00749)
*Roman J. Georgio,Caelum Forder,Suman Deb,Peter Carroll,Önder Gürcan*

Main category: cs.MA

TL;DR: Coral Protocol是一个开放、去中心化的协作基础设施，旨在为多智能体AI生态系统提供通信、协调、信任和支付功能，解决跨领域和跨供应商的互操作性问题。


<details>
  <summary>Details</summary>
Motivation: 随着组织部署多个专业AI智能体，跨领域和供应商的协作需求日益增长，需要一种通用的互操作解决方案。

Method: Coral Protocol设计了标准化的消息格式、模块化的协调机制以及安全的团队形成能力，以支持多智能体任务的动态协作。

Result: 该协议实现了高效、可信的智能体交互，为多智能体生态系统提供了通用语言和协调框架。

Conclusion: Coral Protocol作为“智能体互联网”的基础设施，通过开放的智能体协作，推动了自动化、集体智能和商业价值的提升。

Abstract: The Coral Protocol is an open and decentralized collaboration infrastructure
that enables communication, coordination, trust and payments for The Internet
of Agents. It addresses the growing need for interoperability in a world where
organizations are deploying multiple specialized AI agents that must work
together across domains and vendors. As a foundational platform for multi-agent
AI ecosystems, Coral establishes a common language and coordination framework
allowing any agent to participate in complex workflows with others. Its design
emphasizes broad compatibility, security, and vendor neutrality, ensuring that
agent interactions are efficient and trustworthy. In particular, Coral
introduces standardized messaging formats for agent communication, a modular
coordination mechanism for orchestrating multi-agent tasks, and secure team
formation capabilities for dynamically assembling trusted groups of agents.
Together, these innovations position Coral Protocol as a cornerstone of the
emerging "Internet of Agents," unlocking new levels of automation, collective
intelligence, and business value through open agent collaboration.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [227] [Enhancing SPARQL Query Rewriting for Complex Ontology Alignments](https://arxiv.org/abs/2505.01309)
*Anicet Lepetit Ondo,Laurence Capus,Mamadou Bousso*

Main category: cs.DB

TL;DR: 提出了一种基于自然语言和GPT-4的SPARQL查询重写方法，用于处理复杂本体对齐（c : c）和简化非专家用户的操作。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注简单和部分复杂的本体对齐（s : s和s : c），忽略了更复杂的（c : c）对齐的挑战，且SPARQL语法对非专家用户不友好。

Method: 结合等价传递性和GPT-4等大型语言模型，将用户自然语言需求自动重写为SPARQL查询，支持复杂对齐。

Result: 能够高效处理复杂对齐（c : c），并降低非专家用户使用SPARQL的门槛。

Conclusion: 该方法为查询异构数据提供了灵活且高效的解决方案，特别适用于复杂本体对齐和非专家用户。

Abstract: SPARQL query rewriting is a fundamental mechanism for uniformly querying
heterogeneous ontologies in the Linked Data Web. However, the complexity of
ontology alignments, particularly rich correspondences (c : c), makes this
process challenging. Existing approaches primarily focus on simple (s : s) and
partially complex ( s : c) alignments, thereby overlooking the challenges posed
by more expressive alignments. Moreover, the intricate syntax of SPARQL
presents a barrier for non-expert users seeking to fully exploit the knowledge
encapsulated in ontologies. This article proposes an innovative approach for
the automatic rewriting of SPARQL queries from a source ontology to a target
ontology, based on a user's need expressed in natural language. It leverages
the principles of equivalence transitivity as well as the advanced capabilities
of large language models such as GPT-4. By integrating these elements, this
approach stands out for its ability to efficiently handle complex alignments,
particularly (c : c) correspondences , by fully exploiting their
expressiveness. Additionally, it facilitates access to aligned ontologies for
users unfamiliar with SPARQL, providing a flexible solution for querying
heterogeneous data.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [228] [Differentiable Nonlinear Model Predictive Control](https://arxiv.org/abs/2505.01353)
*Jonathan Frey,Katrin Baumgärtner,Gianluca Frison,Dirk Reinhardt,Jasper Hoffmann,Leonard Fichtner,Sebastien Gros,Moritz Diehl*

Main category: math.OC

TL;DR: 论文提出了一种基于隐函数定理和光滑最优性条件的非线性规划解敏感性计算方法，适用于非线性模型预测控制中的学习增强方法。


<details>
  <summary>Details</summary>
Motivation: 解决学习增强方法与非线性模型预测控制集成中参数解敏感性高效计算的挑战。

Method: 采用隐函数定理和光滑最优性条件，结合序列二次规划方法和内点法。

Result: 实现了超过现有求解器mpc.pytorch 3倍的速度提升。

Conclusion: 该方法为一般最优控制问题提供了高效的前向和伴随敏感性计算工具。

Abstract: The efficient computation of parametric solution sensitivities is a key
challenge in the integration of learning-enhanced methods with nonlinear model
predictive control (MPC), as their availability is crucial for many learning
algorithms. While approaches presented in the machine learning community are
limited to convex or unconstrained formulations, this paper discusses the
computation of solution sensitivities of general nonlinear programs (NLPs)
using the implicit function theorem (IFT) and smoothed optimality conditions
treated in interior-point methods (IPM). We detail sensitivity computation
within a sequential quadratic programming (SQP) method which employs an IPM for
the quadratic subproblems. The publication is accompanied by an efficient
open-source implementation within the framework, providing both forward and
adjoint sensitivities for general optimal control problems, achieving speedups
exceeding 3x over the state-of-the-art solver mpc.pytorch.

</details>


### [229] [A Provably Convergent Plug-and-Play Framework for Stochastic Bilevel Optimization](https://arxiv.org/abs/2505.01258)
*Tianshu Chu,Dachuan Xu,Wei Yao,Chengming Yu,Jin Zhang*

Main category: math.OC

TL;DR: 提出了一种名为PnPBO的即插即用框架，用于开发和分析随机双层优化方法，整合了无偏和有偏随机估计器，并实现了最优样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 双层优化在机器学习中应用广泛，但现有方法在样本复杂度上存在局限，需要一种统一且高效的框架。

Method: PnPBO框架整合了多种随机估计器（如PAGE、ZeroSARAH等），并采用移动平均技术优化无偏估计器。

Result: 理论分析表明PnPBO实现了与单层优化相同的最优样本复杂度，并通过实验验证了其有效性。

Conclusion: PnPBO解决了双层优化与单层优化复杂度是否相同的问题，为实际应用提供了高效工具。

Abstract: Bilevel optimization has recently attracted significant attention in machine
learning due to its wide range of applications and advanced hierarchical
optimization capabilities. In this paper, we propose a plug-and-play framework,
named PnPBO, for developing and analyzing stochastic bilevel optimization
methods. This framework integrates both modern unbiased and biased stochastic
estimators into the single-loop bilevel optimization framework introduced in
[9], with several improvements. In the implementation of PnPBO, all stochastic
estimators for different variables can be independently incorporated, and an
additional moving average technique is applied when using an unbiased estimator
for the upper-level variable. In the theoretical analysis, we provide a unified
convergence and complexity analysis for PnPBO, demonstrating that the
adaptation of various stochastic estimators (including PAGE, ZeroSARAH, and
mixed strategies) within the PnPBO framework achieves optimal sample
complexity, comparable to that of single-level optimization. This resolves the
open question of whether the optimal complexity bounds for solving bilevel
optimization are identical to those for single-level optimization. Finally, we
empirically validate our framework, demonstrating its effectiveness on several
benchmark problems and confirming our theoretical findings.

</details>


### [230] [Negative Stepsizes Make Gradient-Descent-Ascent Converge](https://arxiv.org/abs/2505.01423)
*Henry Shugart,Jason M. Altschuler*

Main category: math.OC

TL;DR: 研究发现，通过选择合适的步长（称为弹弓步长），原始的梯度下降上升（GDA）算法可以收敛，解决了传统认为GDA无法收敛的问题。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为GDA算法无法收敛，导致大量改进算法的研究。本文旨在证明通过调整步长，原始GDA即可收敛。

Method: 提出弹弓步长策略，包括时变、不对称和周期性负步长，证明这些特性对收敛的必要性。

Result: GDA在经典反例（如无约束凸凹问题）上实现收敛，且适用于实际中常用的最后迭代结果。

Conclusion: 弹弓步长通过非可逆梯度流实现二阶收敛，近似于共识优化算法，为深度学习中的极小极大问题提供新思路。

Abstract: Efficient computation of min-max problems is a central question in
optimization, learning, games, and controls. Arguably the most natural
algorithm is gradient-descent-ascent (GDA). However, since the 1970s,
conventional wisdom has argued that GDA fails to converge even on simple
problems. This failure spurred an extensive literature on modifying GDA with
additional building blocks such as extragradients, optimism, momentum,
anchoring, etc. In contrast, we show that GDA converges in its original form by
simply using a judicious choice of stepsizes.
  The key innovation is the proposal of unconventional stepsize schedules
(dubbed slingshot stepsize schedules) that are time-varying, asymmetric, and
periodically negative. We show that all three properties are necessary for
convergence, and that altogether this enables GDA to converge on the classical
counterexamples (e.g., unconstrained convex-concave problems). All of our
results apply to the last iterate of GDA, as is typically desired in practice.
  The core algorithmic intuition is that although negative stepsizes make
backward progress, they de-synchronize the min and max variables (overcoming
the cycling issue of GDA), and lead to a slingshot phenomenon in which the
forward progress in the other iterations is overwhelmingly larger. This results
in fast overall convergence. Geometrically, the slingshot dynamics leverage the
non-reversibility of gradient flow: positive/negative steps cancel to first
order, yielding a second-order net movement in a new direction that leads to
convergence and is otherwise impossible for GDA to move in. We interpret this
as a second-order finite-differencing algorithm and show that, intriguingly, it
approximately implements consensus optimization, an empirically popular
algorithm for min-max problems involving deep neural networks (e.g., training
GANs).

</details>
