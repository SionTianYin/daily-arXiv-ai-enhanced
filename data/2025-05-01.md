<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 48]
- [cs.CV](#cs.CV) [Total: 67]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.LG](#cs.LG) [Total: 44]
- [cs.RO](#cs.RO) [Total: 11]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [q-bio.CB](#q-bio.CB) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.IT](#cs.IT) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 7]
- [math.ST](#math.ST) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [physics.data-an](#physics.data-an) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.CR](#cs.CR) [Total: 26]
- [stat.ML](#stat.ML) [Total: 5]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models](https://arxiv.org/abs/2504.21012)
*Makoto Sato*

Main category: cs.CL

TL;DR: 论文提出了一个框架（TIP和TQP）来量化分析LLM的认知行为，发现LLM在语义融合提示下与人类不同，缺乏概念整合能力。


<details>
  <summary>Details</summary>
Motivation: 探究人类直觉思维的底层机制，通过比较人类与LLM的认知动态，揭示其差异。

Method: 提出TIP触发LLM行为变化，TQP量化评估变化，通过实验比较语义融合与非融合提示对LLM的影响。

Result: LLM在语义融合提示下未表现出与人类相似的响应差异，缺乏概念整合能力。

Conclusion: LLM尚未复制人类直觉中的概念整合过程，该方法为量化认知响应提供了新工具。

Abstract: What underlies intuitive human thinking? One approach to this question is to
compare the cognitive dynamics of humans and large language models (LLMs).
However, such a comparison requires a method to quantitatively analyze AI
cognitive behavior under controlled conditions. While anecdotal observations
suggest that certain prompts can dramatically change LLM behavior, these
observations have remained largely qualitative. Here, we propose a two-part
framework to investigate this phenomenon: a Transition-Inducing Prompt (TIP)
that triggers a rapid shift in LLM responsiveness, and a Transition Quantifying
Prompt (TQP) that evaluates this change using a separate LLM. Through
controlled experiments, we examined how LLMs react to prompts embedding two
semantically distant concepts (e.g., mathematical aperiodicity and traditional
crafts)--either fused together or presented separately--by changing their
linguistic quality and affective tone. Whereas humans tend to experience
heightened engagement when such concepts are meaningfully blended producing a
novel concept--a form of conceptual fusion--current LLMs showed no significant
difference in responsiveness between semantically fused and non-fused prompts.
This suggests that LLMs may not yet replicate the conceptual integration
processes seen in human intuition. Our method enables fine-grained,
reproducible measurement of cognitive responsiveness, and may help illuminate
key differences in how intuition and conceptual leaps emerge in artificial
versus human minds.

</details>


### [2] [Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge](https://arxiv.org/abs/2504.21013)
*Antoun Yaacoub,Zainab Assaghir,Lionel Prevost,Jérôme Da-Rugna*

Main category: cs.CL

TL;DR: 研究分析了Google Gemini 1.5-flash文本模型生成的计算机科学多选题反馈的语言特征，包括可读性、词汇丰富度和适应性，揭示了反馈语气与题目难度之间的动态交互作用。


<details>
  <summary>Details</summary>
Motivation: AI生成的反馈在教育中潜力巨大，但对其语言特征的全面理解仍有限。

Method: 分析了1,200多道多选题的反馈，计算了语言指标（如可读性、词汇丰富度），并训练了RoBERTa多任务学习模型预测这些指标。

Result: 模型预测可读性和词汇丰富度的平均绝对误差分别为2.0和0.03，发现反馈语气与题目难度有显著交互作用。

Conclusion: 研究为开发个性化AI反馈机制提供了依据，同时强调了设计中的伦理考量。

Abstract: Artificial Intelligence (AI)-generated feedback in educational settings has
garnered considerable attention due to its potential to enhance learning
outcomes. However, a comprehensive understanding of the linguistic
characteristics of AI-generated feedback, including readability, lexical
richness, and adaptability across varying challenge levels, remains limited.
This study delves into the linguistic and structural attributes of feedback
generated by Google's Gemini 1.5-flash text model for computer science
multiple-choice questions (MCQs). A dataset of over 1,200 MCQs was analyzed,
considering three difficulty levels (easy, medium, hard) and three feedback
tones (supportive, neutral, challenging). Key linguistic metrics, such as
length, readability scores (Flesch-Kincaid Grade Level), vocabulary richness,
and lexical density, were computed and examined. A fine-tuned RoBERTa-based
multi-task learning (MTL) model was trained to predict these linguistic
properties, achieving a Mean Absolute Error (MAE) of 2.0 for readability and
0.03 for vocabulary richness. The findings reveal significant interaction
effects between feedback tone and question difficulty, demonstrating the
dynamic adaptation of AI-generated feedback within diverse educational
contexts. These insights contribute to the development of more personalized and
effective AI-driven feedback mechanisms, highlighting the potential for
improved learning outcomes while underscoring the importance of ethical
considerations in their design and deployment.

</details>


### [3] [Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments](https://arxiv.org/abs/2504.21016)
*Ngoc C. Lê,Hai-Chung Nguyen-Phung,Thu-Huong Pham Thi,Hue Vu,Phuong-Thao Nguyen Thi,Thu-Thuy Tran,Hong-Nhung Le Thi,Thuy-Duong Nguyen-Thi,Thanh-Huy Nguyen*

Main category: cs.CL

TL;DR: 研究提出了一种命名实体识别（NER）方法，用于辅助越南的COVID-19疫情防控，并构建了一个手动标注的越南语嵌套命名实体识别数据集。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情全球蔓延，越南通过追踪、定位和隔离接触者有效防控，但手动操作效率低下。

Method: 采用命名实体识别技术，构建新的实体类型，并开发了一个手动标注的越南语嵌套命名实体识别数据集。

Result: 提出了一个适用于越南COVID-19防控的NER系统，并提供了新的数据集。

Conclusion: 该研究为越南的疫情追踪提供了自动化工具，提高了防控效率。

Abstract: The COVID-19 pandemic caused great losses worldwide, efforts are taken place
to prevent but many countries have failed. In Vietnam, the traceability,
localization, and quarantine of people who contact with patients contribute to
effective disease prevention. However, this is done by hand, and take a lot of
work. In this research, we describe a named-entity recognition (NER) study that
assists in the prevention of COVID-19 pandemic in Vietnam. We also present our
manually annotated COVID-19 dataset with nested named entity recognition task
for Vietnamese which be defined new entity types using for our system.

</details>


### [4] [ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese](https://arxiv.org/abs/2504.21017)
*Hai-Chung Nguyen-Phung,Ngoc C. Lê,Van-Chien Nguyen,Hang Thi Nguyen,Thuy Phuong Thi Nguyen*

Main category: cs.CL

TL;DR: 论文介绍了首个关于COVID-19的越南语多跨度抽取机器阅读理解数据集ViQA-COVID，旨在支持疾病预防和促进越南语及多语言MRC研究。


<details>
  <summary>Details</summary>
Motivation: COVID-19对全球经济和社会的严重影响，以及AI在疾病预防中的必要性，促使作者创建首个越南语MRC数据集。

Method: 作者开发了ViQA-COVID数据集，这是首个针对越南语的多跨度抽取MRC数据集，可用于构建模型和系统。

Result: ViQA-COVID成为首个支持越南语及多语言MRC研究的数据集，填补了相关领域的空白。

Conclusion: ViQA-COVID的创建为COVID-19预防和越南语MRC研究提供了重要资源，并有望推动多语言MRC的发展。

Abstract: After two years of appearance, COVID-19 has negatively affected people and
normal life around the world. As in May 2022, there are more than 522 million
cases and six million deaths worldwide (including nearly ten million cases and
over forty-three thousand deaths in Vietnam). Economy and society are both
severely affected. The variant of COVID-19, Omicron, has broken disease
prevention measures of countries and rapidly increased number of infections.
Resources overloading in treatment and epidemics prevention is happening all
over the world. It can be seen that, application of artificial intelligence
(AI) to support people at this time is extremely necessary. There have been
many studies applying AI to prevent COVID-19 which are extremely useful, and
studies on machine reading comprehension (MRC) are also in it. Realizing that,
we created the first MRC dataset about COVID-19 for Vietnamese: ViQA-COVID and
can be used to build models and systems, contributing to disease prevention.
Besides, ViQA-COVID is also the first multi-span extraction MRC dataset for
Vietnamese, we hope that it can contribute to promoting MRC studies in
Vietnamese and multilingual.

</details>


### [5] [HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization](https://arxiv.org/abs/2504.21018)
*Enes Özeren,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: HYPEROFA提出了一种基于超网络的适应性词嵌入初始化方法，优于随机初始化和OFA方法。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型在低资源语言上表现不佳，现有方法（如OFA）的词嵌入初始化限制了表达能力。

Method: 使用超网络从多语言词向量空间映射到PLM的词嵌入空间，为目标语言生成灵活的词嵌入。

Result: HYPEROFA在持续预训练收敛和下游任务性能上优于随机初始化，且匹配或超越OFA。

Conclusion: HYPEROFA是一种有效的适应性词嵌入初始化方法，适用于低资源语言。

Abstract: Many pre-trained language models (PLMs) exhibit suboptimal performance on
mid- and low-resource languages, largely due to limited exposure to these
languages during pre-training. A common strategy to address this is to
introduce new tokens specific to the target languages, initialize their
embeddings, and apply continual pre-training on target-language data. Among
such methods, OFA (Liu et al., 2024a) proposes a similarity-based subword
embedding initialization heuristic that is both effective and efficient.
However, OFA restricts target-language token embeddings to be convex
combinations of a fixed number of source-language embeddings, which may limit
expressiveness. To overcome this limitation, we propose HYPEROFA, a
hypernetwork-based approach for more adaptive token embedding initialization.
The hypernetwork is trained to map from an external multilingual word vector
space to the PLMs token embedding space using source-language tokens. Once
trained, it can generate flexible embeddings for target-language tokens,
serving as a good starting point for continual pretraining. Experiments
demonstrate that HYPEROFA consistently outperforms random initialization
baseline and matches or exceeds the performance of OFA in both continual
pre-training convergence and downstream task performance. We make the code
publicly available.

</details>


### [6] [Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations](https://arxiv.org/abs/2504.21019)
*Yinghan Zhou,Juan Wen,Wanli Peng,Yiming Xue,Ziwei Zhang,Zhengxian Wu*

Main category: cs.CL

TL;DR: 论文提出了一种新的AI生成文本检测方法DP-Net，通过动态扰动和强化学习解决泛化性和鲁棒性问题，实验表明其在跨域场景和对抗攻击下表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，AI生成文本的滥用问题日益严重，亟需一种同时具备高泛化性和鲁棒性的检测方法。

Method: 将鲁棒性视为域偏移的一种形式，提出DP-Net方法，通过强化学习引入动态扰动，优化奖励和动作设计。

Result: DP-Net在三种跨域场景中泛化能力显著优于现有方法，同时在两种文本对抗攻击下表现最佳。

Conclusion: DP-Net通过统一机制有效解决了AI生成文本检测中的泛化性和鲁棒性问题，为未来研究提供了新思路。

Abstract: The growing popularity of large language models has raised concerns regarding
the potential to misuse AI-generated text (AIGT). It becomes increasingly
critical to establish an excellent AIGT detection method with high
generalization and robustness. However, existing methods either focus on model
generalization or concentrate on robustness. The unified mechanism, to
simultaneously address the challenges of generalization and robustness, is less
explored. In this paper, we argue that robustness can be view as a specific
form of domain shift, and empirically reveal an intrinsic mechanism for model
generalization of AIGT detection task. Then, we proposed a novel AIGT detection
method (DP-Net) via dynamic perturbations introduced by a reinforcement
learning with elaborated reward and action. Experimentally, extensive results
show that the proposed DP-Net significantly outperforms some state-of-the-art
AIGT detection methods for generalization capacity in three cross-domain
scenarios. Meanwhile, the DP-Net achieves best robustness under two text
adversarial attacks. The code is publicly available at
https://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net.

</details>


### [7] [Context-Enhanced Contrastive Search for Improved LLM Text Generation](https://arxiv.org/abs/2504.21020)
*Jaydip Sen,Rohit Pandey,Hetvi Waghela*

Main category: cs.CL

TL;DR: 论文提出了一种改进的对比搜索算法CECS，通过动态上下文重要性加权和多级对比搜索等技术，显著提升了生成文本的连贯性和相关性。


<details>
  <summary>Details</summary>
Motivation: 传统解码方法在生成长文本时存在重复或不连贯的问题，需要一种新方法来平衡流畅性、创造性和精确性。

Method: 提出Context-Enhanced Contrastive Search (CECS)，结合动态上下文重要性加权、多级对比搜索和自适应温度控制。

Result: 实验结果显示CECS在BLEU、ROUGE和语义相似度等指标上优于现有对比搜索技术。

Conclusion: CECS在连贯性和相关性上表现优异，适用于法律文件起草、客服聊天机器人和内容营销等领域。

Abstract: Recently, Large Language Models (LLMs) have demonstrated remarkable
advancements in Natural Language Processing (NLP). However, generating
high-quality text that balances coherence, diversity, and relevance remains
challenging. Traditional decoding methods, such as bean search and top-k
sampling, often struggle with either repetitive or incoherent outputs,
particularly in tasks that require long-form text generation. To address these
limitations, the paper proposes a novel enhancement of the well-known
Contrastive Search algorithm, Context-Enhanced Contrastive Search (CECS) with
contextual calibration. The proposed scheme introduces several novelties
including dynamic contextual importance weighting, multi-level Contrastive
Search, and adaptive temperature control, to optimize the balance between
fluency, creativity, and precision. The performance of CECS is evaluated using
several standard metrics such as BLEU, ROUGE, and semantic similarity.
Experimental results demonstrate significant improvements in both coherence and
relevance of the generated texts by CECS outperforming the existing Contrastive
Search techniques. The proposed algorithm has several potential applications in
the real world including legal document drafting, customer service chatbots,
and content marketing.

</details>


### [8] [ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees](https://arxiv.org/abs/2504.21022)
*Jun Wang,David Smith Sundarsingh,Jyotirmoy V. Deshmukh,Yiannis Kantaros*

Main category: cs.CL

TL;DR: ConformalNL2LTL是一种新的自然语言到LTL的翻译方法，通过结合问答问题和LLMs，利用保形预测实现不确定性感知，确保翻译准确率并最小化求助率。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏正确性保证，需要手动定义LTL任务，ConformalNL2LTL旨在解决这一问题。

Method: 通过问答问题和LLMs迭代构建LTL公式，利用保形预测量化不确定性。

Result: 理论及实证结果表明，该方法能实现用户指定的翻译准确率，同时最小化求助率。

Conclusion: ConformalNL2LTL提供了一种可靠且高效的NL到LTL翻译方法。

Abstract: Linear Temporal Logic (LTL) has become a prevalent specification language for
robotic tasks. To mitigate the significant manual effort and expertise required
to define LTL-encoded tasks, several methods have been proposed for translating
Natural Language (NL) instructions into LTL formulas, which, however, lack
correctness guarantees. To address this, we introduce a new NL-to-LTL
translation method, called ConformalNL2LTL, that can achieve user-defined
translation success rates over unseen NL commands. Our method constructs LTL
formulas iteratively by addressing a sequence of open-vocabulary
Question-Answering (QA) problems with LLMs. To enable uncertainty-aware
translation, we leverage conformal prediction (CP), a distribution-free
uncertainty quantification tool for black-box models. CP enables our method to
assess the uncertainty in LLM-generated answers, allowing it to proceed with
translation when sufficiently confident and request help otherwise. We provide
both theoretical and empirical results demonstrating that ConformalNL2LTL
achieves user-specified translation accuracy while minimizing help rates.

</details>


### [9] [Param$Δ$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost](https://arxiv.org/abs/2504.21023)
*Sheng Cao,Mingrui Wu,Karthik Prasad,Yuandong Tian,Zechun Liu*

Main category: cs.CL

TL;DR: 论文提出了一种名为$Param\Delta$的新方法，通过计算已有后训练模型与基础模型权重的差异，并将其应用于更新后的基础模型，无需额外训练即可实现后训练能力。


<details>
  <summary>Details</summary>
Motivation: 后训练阶段需要大量高质量数据和计算资源，且存在过拟合风险。$Param\Delta$旨在通过知识迁移简化这一过程，降低成本。

Method: 计算后训练模型权重与基础模型权重的差异，并将该差异应用于更新后的基础模型，形成$Param\Delta$模型。

Result: 在多个模型上的实验表明，$Param\Delta$模型性能接近直接后训练模型（如达到Llama3.1-inst模型性能的95%）。

Conclusion: $Param\Delta$为开放权重社区提供了一种零成本加速模型迭代的方法，充分利用现有模型资源。

Abstract: The post-training phase of large language models is essential for enhancing
capabilities such as instruction-following, reasoning, and alignment with human
preferences. However, it demands extensive high-quality data and poses risks
like overfitting, alongside significant computational costs due to repeated
post-training and evaluation after each base model update. This paper
introduces $Param\Delta$, a novel method that streamlines post-training by
transferring knowledge from an existing post-trained model to a newly updated
base model with ZERO additional training. By computing the difference between
post-trained model weights ($\Theta_\text{post}$) and base model weights
($\Theta_\text{base}$), and adding this to the updated base model
($\Theta'_\text{base}$), we define $Param\Delta$ Model as:
$\Theta_{\text{Param}\Delta} = \Theta_\text{post} - \Theta_\text{base} +
\Theta'_\text{base}$. This approach surprisingly equips the new base model with
post-trained capabilities, achieving performance comparable to direct
post-training. We did analysis on LLama3, Llama3.1, Qwen, and
DeepSeek-distilled models. Results indicate $Param\Delta$ Model effectively
replicates traditional post-training. For example, the $Param\Delta$ Model
obtained from 70B Llama3-inst, Llama3-base, Llama3.1-base models attains
approximately 95\% of Llama3.1-inst model's performance on average.
$Param\Delta$ brings a new perspective on how to fully leverage models in the
open-weight community, where checkpoints for base and instruct models are
readily available and frequently updated, by providing a cost-free framework to
accelerate the iterative cycle of model development.

</details>


### [10] [WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model](https://arxiv.org/abs/2504.21024)
*Tianqing Fang,Hongming Zhang,Zhisong Zhang,Kaixin Ma,Wenhao Yu,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 提出了一种结合世界模型LLM的新框架，通过增强探索和利用预训练知识，解决了自主学习中性能停滞的问题，实验显示性能提升10%。


<details>
  <summary>Details</summary>
Motivation: 现有自主学习方法在性能提升过程中容易停滞，原因是环境探索不足和预训练知识利用不充分。

Method: 引入一个共同演化的世界模型LLM，预测环境状态并生成训练数据，同时作为推理时的想象引擎。

Result: 在真实网络环境中实验，性能提升10%，无需依赖闭源模型蒸馏。

Conclusion: 世界模型的整合是实现持续适应性提升的关键。

Abstract: Agent self-improvement, where the backbone Large Language Model (LLM) of the
agent are trained on trajectories sampled autonomously based on their own
policies, has emerged as a promising approach for enhancing performance. Recent
advancements, particularly in web environments, face a critical limitation:
their performance will reach a stagnation point during autonomous learning
cycles, hindering further improvement. We argue that this stems from limited
exploration of the web environment and insufficient exploitation of pre-trained
web knowledge in LLMs. To improve the performance of self-improvement, we
propose a novel framework that introduces a co-evolving World Model LLM. This
world model predicts the next observation based on the current observation and
action within the web environment. Leveraging LLMs' pretrained knowledge of
abundant web content, the World Model serves dual roles: (1) as a virtual web
server generating self-instructed training data to continuously refine the
agent's policy, and (2) as an imagination engine during inference, enabling
look-ahead simulation to guide action selection for the agent LLM. Experiments
in real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a
10% performance gain over existing self-evolving agents, demonstrating the
efficacy and generalizability of our approach, without using any distillation
from more powerful close-sourced models. Our work establishes the necessity of
integrating world models into autonomous agent frameworks to unlock sustained
adaptability.

</details>


### [11] [Durghotona GPT: A Web Scraping and Large Language Model Based Framework to Generate Road Accident Dataset Automatically in Bangladesh](https://arxiv.org/abs/2504.21025)
*MD Thamed Bin Zaman Chowdhury,Moazzem Hossain,Md. Ridwanul Islam*

Main category: cs.CL

TL;DR: 论文提出了一种名为'Durghotona GPT'的新框架，结合网络爬虫和大语言模型（LLMs），从孟加拉国主要报纸自动生成全面的交通事故数据集。


<details>
  <summary>Details</summary>
Motivation: 交通事故导致重大经济损失和社会问题，准确及时的数据对预测和缓解事故至关重要。

Method: 从三家报纸收集事故新闻，使用GPT-4、GPT-3.5和Llama-3处理数据，提取信息并分类。

Result: Llama-3表现接近GPT-4，准确率达89%，是成本效益高的替代方案。框架显著提升数据质量和可用性。

Conclusion: 该框架支持交通安全分析等关键应用，未来将扩展数据收集方法并优化LLMs。

Abstract: Road accidents pose significant concerns globally. They lead to large
financial losses, injuries, disabilities, and societal challenges. Accurate and
timely accident data is essential for predicting and mitigating these events.
This paper presents a novel framework named 'Durghotona GPT' that integrates
web scraping and Large Language Models (LLMs) to automate the generation of
comprehensive accident datasets from prominent national dailies in Bangladesh.
The authors collected accident reports from three major newspapers: Prothom
Alo, Dhaka Tribune, and The Daily Star. The collected news was then processed
using the newest available LLMs: GPT-4, GPT-3.5, and Llama-3. The framework
efficiently extracts relevant information, categorizes reports, and compiles
detailed datasets. Thus, this framework overcomes limitations of manual data
collection methods such as delays, errors, and communication gaps. The authors'
evaluation demonstrates that Llama-3, an open-source model, performs comparably
to GPT-4. It achieved 89% accuracy in the authors' evaluation. Therefore, it
can be considered a cost-effective alternative for similar tasks. The results
suggest that the framework developed by the authors can drastically enhance the
quality and availability of accident data. As a result, it can support critical
applications in traffic safety analysis, urban planning, and public health. The
authors also developed an interface for 'Durghotona GPT' for ease of use as
part of this paper. Future work will focus on expanding data collection methods
and refining LLMs to further increase dataset accuracy and applicability.

</details>


### [12] [Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models](https://arxiv.org/abs/2504.21026)
*Manish Pandey,Nageshwar Prasad Yadav,Mokshada Adduru,Sawan Rai*

Main category: cs.CL

TL;DR: 论文研究了多语言社交媒体中混合代码文本的辱骂性语言检测，针对低资源语言（如泰卢固语和尼泊尔语）提出了新的数据集和模型比较。


<details>
  <summary>Details</summary>
Motivation: 随着多语言用户在社交媒体上的增多，混合代码文本中的辱骂性语言检测变得更具挑战性，尤其是低资源语言的研究不足。

Method: 研究引入了一个手动标注的数据集，包含泰卢固语-英语和尼泊尔语-英语混合代码评论，并通过多种机器学习、深度学习和大型语言模型进行评估。

Result: 研究发现混合代码文本中的辱骂性语言检测存在挑战，并通过模型比较提供了关键见解。

Conclusion: 研究为低资源语言的NLP研究提供了基准，有助于开发更强大的多语言社交媒体内容审核策略。

Abstract: With the growing presence of multilingual users on social media, detecting
abusive language in code-mixed text has become increasingly challenging.
Code-mixed communication, where users seamlessly switch between English and
their native languages, poses difficulties for traditional abuse detection
models, as offensive content may be context-dependent or obscured by linguistic
blending. While abusive language detection has been extensively explored for
high-resource languages like English and Hindi, low-resource languages such as
Telugu and Nepali remain underrepresented, leaving gaps in effective
moderation. In this study, we introduce a novel, manually annotated dataset of
2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized
as abusive and non-abusive, collected from various social media platforms. The
dataset undergoes rigorous preprocessing before being evaluated across multiple
Machine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We
experimented with models including Logistic Regression, Random Forest, Support
Vector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing
their performance through hyperparameter tuning, and evaluate it using 10-fold
cross-validation and statistical significance testing (t-test). Our findings
provide key insights into the challenges of detecting abusive language in
code-mixed settings and offer a comparative analysis of computational
approaches. This study contributes to advancing NLP for low-resource languages
by establishing benchmarks for abusive language detection in Telugu-English and
Nepali-English code-mixed text. The dataset and insights can aid in the
development of more robust moderation strategies for multilingual social media
environments.

</details>


### [13] [UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2504.21027)
*Yu Zheng,Longyi Liu,Yuming Lin,Jie Feng,Guozhen Zhang,Depeng Jin,Yong Li*

Main category: cs.CL

TL;DR: 论文提出了UrbanPlanBench基准和UrbanPlanText数据集，评估大语言模型（LLMs）在城市规划中的表现，发现其专业能力不足，但通过微调可提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在城市规划领域的潜力，填补其与人类专家之间的差距。

Method: 引入UrbanPlanBench基准和UrbanPlanText数据集（30,000+指令对），评估并微调LLMs。

Result: LLMs在规划法规理解等方面表现不佳（70%模型低于标准），但微调后性能有所提升。

Conclusion: LLMs在城市规划中仍有改进空间，公开资源旨在促进人机协作。

Abstract: The advent of Large Language Models (LLMs) holds promise for revolutionizing
various fields traditionally dominated by human expertise. Urban planning, a
professional discipline that fundamentally shapes our daily surroundings, is
one such field heavily relying on multifaceted domain knowledge and experience
of human experts. The extent to which LLMs can assist human practitioners in
urban planning remains largely unexplored. In this paper, we introduce a
comprehensive benchmark, UrbanPlanBench, tailored to evaluate the efficacy of
LLMs in urban planning, which encompasses fundamental principles, professional
knowledge, and management and regulations, aligning closely with the
qualifications expected of human planners. Through extensive evaluation, we
reveal a significant imbalance in the acquisition of planning knowledge among
LLMs, with even the most proficient models falling short of meeting
professional standards. For instance, we observe that 70% of LLMs achieve
subpar performance in understanding planning regulations compared to other
aspects. Besides the benchmark, we present the largest-ever supervised
fine-tuning (SFT) dataset, UrbanPlanText, comprising over 30,000 instruction
pairs sourced from urban planning exams and textbooks. Our findings demonstrate
that fine-tuned models exhibit enhanced performance in memorization tests and
comprehension of urban planning knowledge, while there exists significant room
for improvement, particularly in tasks requiring domain-specific terminology
and reasoning. By making our benchmark, dataset, and associated evaluation and
fine-tuning toolsets publicly available at
https://github.com/tsinghua-fib-lab/PlanBench, we aim to catalyze the
integration of LLMs into practical urban planning, fostering a symbiotic
collaboration between human expertise and machine intelligence.

</details>


### [14] [Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts](https://arxiv.org/abs/2504.21117)
*Hanhua Hong,Chenghao Xiao,Yang Wang,Yiqi Liu,Wenge Rong,Chenghua Lin*

Main category: cs.CL

TL;DR: 提出一种基于反转学习的方法，自动生成高效、模型特定的评估提示，提升LLM评估的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: NLG系统评估因输出多样性而复杂，人工评估存在不一致性和偏见，LLM评估则对提示设计敏感。

Method: 通过反转学习，从模型输出反向映射到输入指令，自动生成评估提示，仅需单一样本。

Result: 方法无需手动设计提示，显著提升评估效率和鲁棒性。

Conclusion: 为LLM评估提供了更高效和鲁棒的新方向。

Abstract: Evaluating natural language generation (NLG) systems is challenging due to
the diversity of valid outputs. While human evaluation is the gold standard, it
suffers from inconsistencies, lack of standardisation, and demographic biases,
limiting reproducibility. LLM-based evaluation offers a scalable alternative
but is highly sensitive to prompt design, where small variations can lead to
significant discrepancies. In this work, we propose an inversion learning
method that learns effective reverse mappings from model outputs back to their
input instructions, enabling the automatic generation of highly effective,
model-specific evaluation prompts. Our method requires only a single evaluation
sample and eliminates the need for time-consuming manual prompt engineering,
thereby improving both efficiency and robustness. Our work contributes toward a
new direction for more robust and efficient LLM-based evaluation.

</details>


### [15] [LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge](https://arxiv.org/abs/2504.21132)
*Naheed Rayhan,Md. Ashrafuzzaman*

Main category: cs.CL

TL;DR: LLM ENHANCER系统通过整合多源在线数据提升LLMs的准确性，减少幻觉，同时保持回答的自然性。


<details>
  <summary>Details</summary>
Motivation: LLMs在关键场景中因生成不准确信息和无法有效利用外部知识而受限。

Method: 系统并行整合Google、Wikipedia等在线资源，利用向量嵌入筛选信息后输入LLM。

Result: 系统显著减少了LLMs的幻觉问题，同时保持了回答的自然性和准确性。

Conclusion: LLM ENHANCER为LLMs在现实场景中的应用提供了更可靠的解决方案。

Abstract: Large Language Models (LLMs), such as ChatGPT, have demonstrated the
capability to generate human like, natural responses across a range of tasks,
including task oriented dialogue and question answering. However, their
application in real world, critical scenarios is often hindered by a tendency
to produce inaccurate information and a limited ability to leverage external
knowledge sources. This paper introduces the LLM ENHANCER system, designed to
integrate multiple online sources such as Google, Wikipedia, and DuckDuckGo to
enhance data accuracy. The LLMs employed within this system are open source.
The data acquisition process for the LLM ENHANCER system operates in parallel,
utilizing custom agent tools to manage the flow of information. Vector
embeddings are used to identify the most pertinent information, which is
subsequently supplied to the LLM for user interaction. The LLM ENHANCER system
mitigates hallucinations in chat based LLMs while preserving response
naturalness and accuracy.

</details>


### [16] [Detecting Manipulated Contents Using Knowledge-Grounded Inference](https://arxiv.org/abs/2504.21165)
*Mark Huasong Meng,Ruizhe Wang,Meng Xu,Chuan Yan,Guangdong Bai*

Main category: cs.CL

TL;DR: 本文提出了一种名为Manicod的工具，用于检测零日操纵内容，通过实时上下文信息和检索增强生成技术，结合大型语言模型进行推理，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖训练时的固有知识或手动整理的上下文，无法有效检测零日操纵内容，因此需要一种能够利用实时上下文信息的解决方案。

Method: Manicod通过主流搜索引擎获取输入声明的上下文信息，利用检索增强生成技术将其向量化，再通过大型语言模型进行推理，生成判断和解释。

Result: 在包含4270条操纵假新闻的数据集上，Manicod的F1得分为0.856，比现有方法在事实核查和声明验证任务上高出1.9倍。

Conclusion: Manicod通过结合实时上下文和LLM推理，显著提升了零日操纵内容的检测能力，为假新闻识别提供了新思路。

Abstract: The detection of manipulated content, a prevalent form of fake news, has been
widely studied in recent years. While existing solutions have been proven
effective in fact-checking and analyzing fake news based on historical events,
the reliance on either intrinsic knowledge obtained during training or manually
curated context hinders them from tackling zero-day manipulated content, which
can only be recognized with real-time contextual information. In this work, we
propose Manicod, a tool designed for detecting zero-day manipulated content.
Manicod first sources contextual information about the input claim from
mainstream search engines, and subsequently vectorizes the context for the
large language model (LLM) through retrieval-augmented generation (RAG). The
LLM-based inference can produce a "truthful" or "manipulated" decision and
offer a textual explanation for the decision. To validate the effectiveness of
Manicod, we also propose a dataset comprising 4270 pieces of manipulated fake
news derived from 2500 recent real-world news headlines. Manicod achieves an
overall F1 score of 0.856 on this dataset and outperforms existing methods by
up to 1.9x in F1 score on their benchmarks on fact-checking and claim
verification.

</details>


### [17] [Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare](https://arxiv.org/abs/2504.21191)
*Lovedeep Gondara,Jonathan Simkin,Graham Sayle,Shebnum Devji,Gregory Arbour,Raymond Ng*

Main category: cs.CL

TL;DR: 研究探讨了语言模型选择的四个关键问题，比较了微调与零样本、领域邻近与通用预训练模型、进一步领域预训练的价值，以及小语言模型（SLMs）与大语言模型（LLMs）在特定任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 指导语言模型选择，特别是在专业领域任务中，评估微调、领域预训练和模型规模对性能的影响。

Method: 使用BCCR的电子病理报告，评估三种不同难度和数据量的分类场景，比较SLMs和LLM的零样本与微调表现。

Result: 微调显著提升SLMs性能，优于零样本LLM；领域邻近和领域预训练对复杂任务尤其有益；SLMs在资源与性能权衡上优于LLMs。

Conclusion: 在专业领域任务中，微调SLMs表现优于零样本LLMs，且SLMs在资源效率上更具优势，证明了其持续相关性。

Abstract: This study aims to guide language model selection by investigating: 1) the
necessity of finetuning versus zero-shot usage, 2) the benefits of
domain-adjacent versus generic pretrained models, 3) the value of further
domain-specific pretraining, and 4) the continued relevance of Small Language
Models (SLMs) compared to Large Language Models (LLMs) for specific tasks.
Using electronic pathology reports from the British Columbia Cancer Registry
(BCCR), three classification scenarios with varying difficulty and data size
are evaluated. Models include various SLMs and an LLM. SLMs are evaluated both
zero-shot and finetuned; the LLM is evaluated zero-shot only. Finetuning
significantly improved SLM performance across all scenarios compared to their
zero-shot results. The zero-shot LLM outperformed zero-shot SLMs but was
consistently outperformed by finetuned SLMs. Domain-adjacent SLMs generally
performed better than the generic SLM after finetuning, especially on harder
tasks. Further domain-specific pretraining yielded modest gains on easier tasks
but significant improvements on the complex, data-scarce task. The results
highlight the critical role of finetuning for SLMs in specialized domains,
enabling them to surpass zero-shot LLM performance on targeted classification
tasks. Pretraining on domain-adjacent or domain-specific data provides further
advantages, particularly for complex problems or limited finetuning data. While
LLMs offer strong zero-shot capabilities, their performance on these specific
tasks did not match that of appropriately finetuned SLMs. In the era of LLMs,
SLMs remain relevant and effective, offering a potentially superior
performance-resource trade-off compared to LLMs.

</details>


### [18] [Automatic Legal Writing Evaluation of LLMs](https://arxiv.org/abs/2504.21202)
*Ramon Pires,Roseval Malaquias Junior,Rodrigo Nogueira*

Main category: cs.CL

TL;DR: 论文提出了一个名为oab-bench的法律写作评估基准，基于巴西律师考试，评估了四种大型语言模型的性能，并探讨了LLMs作为自动化评估工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于评估法律写作的复杂性，缺乏公开、更新频繁且包含全面评估指南的测试数据集，因此需要构建一个可靠的基准。

Method: 利用巴西律师考试的105个问题构建oab-bench，包含评估指南和参考材料，测试了四种LLMs的性能，并研究了LLMs作为自动化评估工具的可行性。

Result: Claude-3.5 Sonnet表现最佳，平均得分7.93/10，并通过所有21次考试。前沿模型（如OpenAI的o1）在评估通过考试时与人类评分高度相关。

Conclusion: oab-bench为法律写作评估提供了可靠基准，LLMs在自动化评估中表现出潜力，尽管法律写作评估具有主观性。

Abstract: Despite the recent advances in Large Language Models, benchmarks for
evaluating legal writing remain scarce due to the inherent complexity of
assessing open-ended responses in this domain. One of the key challenges in
evaluating language models on domain-specific tasks is finding test datasets
that are public, frequently updated, and contain comprehensive evaluation
guidelines. The Brazilian Bar Examination meets these requirements. We
introduce oab-bench, a benchmark comprising 105 questions across seven areas of
law from recent editions of the exam. The benchmark includes comprehensive
evaluation guidelines and reference materials used by human examiners to ensure
consistent grading. We evaluate the performance of four LLMs on oab-bench,
finding that Claude-3.5 Sonnet achieves the best results with an average score
of 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can
serve as reliable automated judges for evaluating legal writing. Our
experiments show that frontier models like OpenAI's o1 achieve a strong
correlation with human scores when evaluating approved exams, suggesting their
potential as reliable automated evaluators despite the inherently subjective
nature of legal writing assessment. The source code and the benchmark --
containing questions, evaluation guidelines, model-generated responses, and
their respective automated evaluations -- are publicly available.

</details>


### [19] [Pretraining Large Brain Language Model for Active BCI: Silent Speech](https://arxiv.org/abs/2504.21214)
*Jinzhao Zhou,Zehong Cao,Yiqun Duan,Connor Barkley,Daniel Leong,Xiaowei Jiang,Quoc-Toan Nguyen,Ziyi Zhao,Thomas Do,Yu-Cheng Chang,Sheng-Fu Liang,Chin-teng Lin*

Main category: cs.CL

TL;DR: 本文提出了一种用于主动脑机接口（BCI）的无声语音解码方法，通过预训练大型脑语言模型（LBLM）和新的未来时频预测（FSTP）范式，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 探索更自然灵活的BCI通信方式，解决传统BCI应用的限制。

Method: 提出LBLM模型和FSTP预训练范式，通过自回归建模学习EEG信号的时频依赖关系，并在下游任务中进行微调。

Result: 在跨会话设置中，LBLM在语义级和词级分类任务中分别达到47.0%和39.6%的准确率，优于基线方法。

Conclusion: 研究为主动BCI系统的无声语音解码提供了创新解决方案，并贡献了新的数据集。

Abstract: This paper explores silent speech decoding in active brain-computer interface
(BCI) systems, which offer more natural and flexible communication than
traditional BCI applications. We collected a new silent speech dataset of over
120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing
24 commonly used English words for language model pretraining and decoding.
Following the recent success of pretraining large models with self-supervised
paradigms to enhance EEG classification performance, we propose Large Brain
Language Model (LBLM) pretrained to decode silent speech for active BCI. To
pretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining
paradigm to learn effective representations from unlabeled EEG data. Unlike
existing EEG pretraining methods that mainly follow a masked-reconstruction
paradigm, our proposed FSTP method employs autoregressive modeling in temporal
and frequency domains to capture both temporal and spectral dependencies from
EEG signals. After pretraining, we finetune our LBLM on downstream tasks,
including word-level and semantic-level classification. Extensive experiments
demonstrate significant performance gains of the LBLM over fully-supervised and
pretrained baseline models. For instance, in the difficult cross-session
setting, our model achieves 47.0\% accuracy on semantic-level classification
and 39.6\% in word-level classification, outperforming baseline methods by
5.4\% and 7.3\%, respectively. Our research advances silent speech decoding in
active BCI systems, offering an innovative solution for EEG language model
pretraining and a new dataset for fundamental research.

</details>


### [20] [Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math](https://arxiv.org/abs/2504.21233)
*Haoran Xu,Baolin Peng,Hany Awadalla,Dongdong Chen,Yen-Chun Chen,Mei Gao,Young Jin Kim,Yunsheng Li,Liliang Ren,Yelong Shen,Shuohang Wang,Weijian Xu,Jianfeng Gao,Weizhu Chen*

Main category: cs.CL

TL;DR: 论文提出了一种系统化的训练方法，通过四个步骤提升小型语言模型（SLM）的推理能力，并在Phi-4-Mini模型上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）通过Chain-of-Thought（CoT）技术显著提升了推理能力，但小型语言模型（SLM）由于模型容量有限，推理能力提升仍具挑战性。

Method: 训练方法包括四个步骤：大规模中训练、监督微调、Rollout DPO和强化学习。

Result: 在数学推理任务中，Phi-4-Mini-Reasoning模型表现优于更大的模型，如DeepSeek-R1-Distill-Qwen-7B和DeepSeek-R1-Distill-Llama-8B。

Conclusion: 精心设计的训练方法和大规模高质量CoT数据可以有效解锁小型模型的推理能力。

Abstract: Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities
in Large Language Models (LLMs) by training them to explicitly generate
intermediate reasoning steps. While LLMs readily benefit from such techniques,
improving reasoning in Small Language Models (SLMs) remains challenging due to
their limited model capacity. Recent work by Deepseek-R1 demonstrates that
distillation from LLM-generated synthetic data can substantially improve the
reasoning ability of SLM. However, the detailed modeling recipe is not
disclosed. In this work, we present a systematic training recipe for SLMs that
consists of four steps: (1) large-scale mid-training on diverse distilled
long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3)
Rollout DPO leveraging a carefully curated preference dataset, and (4)
Reinforcement Learning (RL) with Verifiable Reward. We apply our method on
Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning
model exceeds, on math reasoning tasks, much larger reasoning models, e.g.,
outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and
DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate
that a carefully designed training recipe, with large-scale high-quality CoT
data, is effective to unlock strong reasoning capabilities even in
resource-constrained small models.

</details>


### [21] [Memorization and Knowledge Injection in Gated LLMs](https://arxiv.org/abs/2504.21239)
*Xu Pan,Ely Hahami,Zechen Zhang,Haim Sompolinsky*

Main category: cs.CL

TL;DR: MEGa框架通过将记忆直接嵌入LLM权重中，解决了LLM无法持续学习和整合新知识的问题，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: LLM在持续学习和整合新知识方面存在局限，而人类能持续学习。现有方法（如大上下文窗口或外部记忆缓冲区）未能模拟日常生活事件。

Method: 提出MEGa框架，将事件记忆嵌入LLM的权重中，通过门控机制激活相关记忆权重。

Result: 在虚构角色和维基百科事件数据集上，MEGa优于基线方法，有效减轻灾难性遗忘。

Conclusion: MEGa受人类大脑互补记忆系统启发，为LLM持续学习提供了新思路。

Abstract: Large Language Models (LLMs) currently struggle to sequentially add new
memories and integrate new knowledge. These limitations contrast with the human
ability to continuously learn from new experiences and acquire knowledge
throughout life. Most existing approaches add memories either through large
context windows or external memory buffers (e.g., Retrieval-Augmented
Generation), and studies on knowledge injection rarely test scenarios
resembling everyday life events. In this work, we introduce a continual
learning framework, Memory Embedded in Gated LLMs (MEGa), which injects event
memories directly into the weights of LLMs. Each memory is stored in a
dedicated set of gated low-rank weights. During inference, a gating mechanism
activates relevant memory weights by matching query embeddings to stored memory
embeddings. This enables the model to both recall entire memories and answer
related questions. On two datasets - fictional characters and Wikipedia events
- MEGa outperforms baseline approaches in mitigating catastrophic forgetting.
Our model draws inspiration from the complementary memory system of the human
brain.

</details>


### [22] [Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA](https://arxiv.org/abs/2504.21252)
*Xuanzhao Dong,Wenhui Zhu,Hao Wang,Xiwen Chen,Peijie Qiu,Rui Yin,Yi Su,Yalin Wang*

Main category: cs.CL

TL;DR: Discuss-RAG是一个通过协作代理推理增强医学问答系统的模块，显著提升了答案准确性。


<details>
  <summary>Details</summary>
Motivation: 医学问答任务对大型语言模型具有挑战性，现有检索增强生成系统存在推理行为建模不足和依赖低质量语料的问题。

Method: 提出Discuss-RAG模块，引入总结代理协调医学专家团队模拟多轮头脑风暴，并通过决策代理评估检索内容。

Result: 在四个医学问答基准数据集上，Discuss-RAG显著优于MedRAG，BioASQ和PubMedQA的答案准确性分别提升16.67%和12.20%。

Conclusion: Discuss-RAG通过协作代理推理有效提升了医学问答系统的性能，具有实际应用潜力。

Abstract: Medical question answering (QA) is a reasoning-intensive task that remains
challenging for large language models (LLMs) due to hallucinations and outdated
domain knowledge. Retrieval-Augmented Generation (RAG) provides a promising
post-training solution by leveraging external knowledge. However, existing
medical RAG systems suffer from two key limitations: (1) a lack of modeling for
human-like reasoning behaviors during information retrieval, and (2) reliance
on suboptimal medical corpora, which often results in the retrieval of
irrelevant or noisy snippets. To overcome these challenges, we propose
Discuss-RAG, a plug-and-play module designed to enhance the medical QA RAG
system through collaborative agent-based reasoning. Our method introduces a
summarizer agent that orchestrates a team of medical experts to emulate
multi-turn brainstorming, thereby improving the relevance of retrieved content.
Additionally, a decision-making agent evaluates the retrieved snippets before
their final integration. Experimental results on four benchmark medical QA
datasets show that Discuss-RAG consistently outperforms MedRAG, especially
significantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on
PubMedQA. The code is available at: https://github.com/LLM-VLM-GSL/Discuss-RAG.

</details>


### [23] [BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models](https://arxiv.org/abs/2504.21299)
*Zhiting Fan,Ruizhe Chen,Zuozhu Liu*

Main category: cs.CL

TL;DR: BiasGuard是一种新型偏见检测工具，通过两阶段方法（基于公平规范的显式推理和强化学习）提升偏见检测的准确性，优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如公平分类器和基于LLM的评判）在理解意图和公平判断标准方面存在局限性，需要更有效的偏见检测工具。

Method: BiasGuard采用两阶段方法：第一阶段基于公平规范显式推理，第二阶段通过强化学习增强推理和判断能力。

Result: 在五个数据集上的实验表明，BiasGuard在准确性和减少过度公平误判方面优于现有工具。

Conclusion: BiasGuard证明了推理增强决策的重要性，其两阶段优化流程有效提升了偏见检测能力。

Abstract: Identifying bias in LLM-generated content is a crucial prerequisite for
ensuring fairness in LLMs. Existing methods, such as fairness classifiers and
LLM-based judges, face limitations related to difficulties in understanding
underlying intentions and the lack of criteria for fairness judgment. In this
paper, we introduce BiasGuard, a novel bias detection tool that explicitly
analyzes inputs and reasons through fairness specifications to provide accurate
judgments. BiasGuard is implemented through a two-stage approach: the first
stage initializes the model to explicitly reason based on fairness
specifications, while the second stage leverages reinforcement learning to
enhance its reasoning and judgment capabilities. Our experiments, conducted
across five datasets, demonstrate that BiasGuard outperforms existing tools,
improving accuracy and reducing over-fairness misjudgments. We also highlight
the importance of reasoning-enhanced decision-making and provide evidence for
the effectiveness of our two-stage optimization pipeline.

</details>


### [24] [Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges](https://arxiv.org/abs/2504.21303)
*Xiao Xiao,Yu Su,Sijing Zhang,Zhang Chen,Yadong Chen,Tian Liu*

Main category: cs.CL

TL;DR: 本文提出了一种基于贝叶斯方法的LLM能力评估框架，通过概率推断整合先验知识，解决了小样本场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统评估框架依赖确定性标量指标，无法充分捕捉LLM的概率输出特性。

Method: 将模型能力视为潜在变量，利用精心设计的查询集诱导判别性响应，将模型排名问题形式化为贝叶斯假设检验。

Result: 实验表明，该方法在GPT系列模型上优于传统评估方法，即使样本量减少仍保持统计稳健性。

Conclusion: 该研究通过贝叶斯推断与实际部署约束的结合，推动了LLM评估方法的发展。

Abstract: Large language models (LLMs) exhibit probabilistic output characteristics,
yet conventional evaluation frameworks rely on deterministic scalar metrics.
This study introduces a Bayesian approach for LLM capability assessment that
integrates prior knowledge through probabilistic inference, addressing
limitations under limited-sample regimes. By treating model capabilities as
latent variables and leveraging a curated query set to induce discriminative
responses, we formalize model ranking as a Bayesian hypothesis testing problem
over mutually exclusive capability intervals. Experimental evaluations with
GPT-series models demonstrate that the proposed method achieves superior
discrimination compared to conventional evaluation methods. Results indicate
that even with reduced sample sizes, the approach maintains statistical
robustness while providing actionable insights, such as probabilistic
statements about a model's likelihood of surpassing specific baselines. This
work advances LLM evaluation methodologies by bridging Bayesian inference with
practical constraints in real-world deployment scenarios.

</details>


### [25] [Does the Prompt-based Large Language Model Recognize Students' Demographics and Introduce Bias in Essay Scoring?](https://arxiv.org/abs/2504.21330)
*Kaixun Yang,Mladen Raković,Dragan Gašević,Guanliang Chen*

Main category: cs.CL

TL;DR: 研究探讨了基于提示的LLM在自动作文评分中的偏见问题，发现模型能推断学生人口统计信息，且评分偏见与其预测能力相关。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法需要技术背景，而提示工具使AES更易用，但偏见问题尚未明确。

Method: 使用GPT-4o和25,000篇议论文数据集，设计提示推断人口统计信息并评估评分公平性。

Result: LLM能推断学生语言背景，评分偏见在正确预测时更显著，非母语者评分误差增加。

Conclusion: 提示范式下LLM的评分偏见与人口统计预测能力相关，需进一步研究公平性。

Abstract: Large Language Models (LLMs) are widely used in Automated Essay Scoring (AES)
due to their ability to capture semantic meaning. Traditional fine-tuning
approaches required technical expertise, limiting accessibility for educators
with limited technical backgrounds. However, prompt-based tools like ChatGPT
have made AES more accessible, enabling educators to obtain machine-generated
scores using natural-language prompts (i.e., the prompt-based paradigm).
Despite advancements, prior studies have shown bias in fine-tuned LLMs,
particularly against disadvantaged groups. It remains unclear whether such
biases persist or are amplified in the prompt-based paradigm with cutting-edge
tools. Since such biases are believed to stem from the demographic information
embedded in pre-trained models (i.e., the ability of LLMs' text embeddings to
predict demographic attributes), this study explores the relationship between
the model's predictive power of students' demographic attributes based on their
written works and its predictive bias in the scoring task in the prompt-based
paradigm. Using a publicly available dataset of over 25,000 students'
argumentative essays, we designed prompts to elicit demographic inferences
(i.e., gender, first-language background) from GPT-4o and assessed fairness in
automated scoring. Then we conducted multivariate regression analysis to
explore the impact of the model's ability to predict demographics on its
scoring outcomes. Our findings revealed that (i) prompt-based LLMs can somewhat
infer students' demographics, particularly their first-language backgrounds,
from their essays; (ii) scoring biases are more pronounced when the LLM
correctly predicts students' first-language background than when it does not;
and (iii) scoring error for non-native English speakers increases when the LLM
correctly identifies them as non-native.

</details>


### [26] [Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction](https://arxiv.org/abs/2504.21372)
*Máté Gedeon*

Main category: cs.CL

TL;DR: 论文提出了一种模块化的语音事件提取框架SpeechEE，结合高性能ASR和语义搜索增强的LLM提示，显著提升了事件触发和参数分类的性能。


<details>
  <summary>Details</summary>
Motivation: 语音事件提取（SpeechEE）是ASR和NLP交叉领域的挑战性任务，需要从口语中识别结构化事件信息。

Method: 采用模块化流水线框架，结合ASR和语义搜索增强的LLM提示，通过混合过滤机制分类语音段，并使用动态增强的少样本LLM提示提取事件触发和参数。

Result: 使用o1-mini模型在触发分类和参数分类上分别达到63.3%和27.8%的F1分数，优于先前基准。

Conclusion: 流水线方法结合检索增强的LLM可与端到端系统媲美，同时保持可解释性和模块化，为未来结合文本和声学特征的混合模型提供了方向。

Abstract: Speech Event Extraction (SpeechEE) is a challenging task that lies at the
intersection of Automatic Speech Recognition (ASR) and Natural Language
Processing (NLP), requiring the identification of structured event information
from spoken language. In this work, we present a modular, pipeline-based
SpeechEE framework that integrates high-performance ASR with semantic
search-enhanced prompting of Large Language Models (LLMs). Our system first
classifies speech segments likely to contain events using a hybrid filtering
mechanism including rule-based, BERT-based, and LLM-based models. It then
employs few-shot LLM prompting, dynamically enriched via semantic similarity
retrieval, to identify event triggers and extract corresponding arguments. We
evaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini)
highlighting significant performance gains with o1-mini, which achieves 63.3%
F1 on trigger classification and 27.8% F1 on argument classification,
outperforming prior benchmarks. Our results demonstrate that pipeline
approaches, when empowered by retrieval-augmented LLMs, can rival or exceed
end-to-end systems while maintaining interpretability and modularity. This work
provides practical insights into LLM-driven event extraction and opens pathways
for future hybrid models combining textual and acoustic features.

</details>


### [27] [The Distribution of Dependency Distance and Hierarchical Distance in Contemporary Written Japanese and Its Influencing Factors](https://arxiv.org/abs/2504.21421)
*Linxuan Wang,Shuiyuan Yu*

Main category: cs.CL

TL;DR: 研究探讨了日语中依存距离（DD）和层次距离（HD）的关系，发现谓词的配价是影响MDD和MHD权衡关系的根本因素。


<details>
  <summary>Details</summary>
Motivation: 探索日语中DD和HD的关系，以及谓词配价对线性复杂性和层次复杂性的调节作用。

Method: 通过固定和不固定句子长度，比较DD和HD的概率分布，分析MDD和MHD随句子长度的变化及其相关性。

Result: 谓词配价是MDD和MHD权衡关系的根本原因，且对HD分布的影响大于DD，导致MDD均值低于MHD。

Conclusion: 日语母语者通过谓词配价调节语言复杂性，配价阈值决定MDD和MHD的相对大小。

Abstract: To explore the relationship between dependency distance (DD) and hierarchical
distance (HD) in Japanese, we compared the probability distributions of DD and
HD with and without sentence length fixed, and analyzed the changes in mean
dependency distance (MDD) and mean hierarchical distance (MHD) as sentence
length increases, along with their correlation coefficient based on the
Balanced Corpus of Contemporary Written Japanese. It was found that the valency
of the predicates is the underlying factor behind the trade-off relation
between MDD and MHD in Japanese. Native speakers of Japanese regulate the
linear complexity and hierarchical complexity through the valency of the
predicates, and the relative sizes of MDD and MHD depend on whether the
threshold of valency has been reached. Apart from the cognitive load, the
valency of the predicates also affects the probability distributions of DD and
HD. The effect of the valency of the predicates on the distribution of HD is
greater than on that of DD, which leads to differences in their probability
distributions and causes the mean of MDD to be lower than that of MHD.

</details>


### [28] [RWKV-X: A Linear Complexity Hybrid Language Model](https://arxiv.org/abs/2504.21463)
*Haowen Hou,Zhiyi Huang,Kaifeng Tan,Rongchang Lu,Fei Richard Yu*

Main category: cs.CL

TL;DR: RWKV-X是一种新型混合架构，结合了RWKV的短程建模效率和稀疏注意力机制，用于捕捉长程上下文，具有线性训练时间和恒定推理时间。


<details>
  <summary>Details</summary>
Motivation: 解决传统混合方法因全注意力层导致的二次复杂度问题，同时提升长程上下文建模能力。

Method: 结合RWKV的短程建模与稀疏注意力机制，实现线性训练和恒定推理时间。

Result: 在64K标记序列上预训练后，RWKV-X在64K passkey检索基准上表现接近完美，优于RWKV-7模型，且支持百万标记序列解码。

Conclusion: RWKV-X是一种高效、可扩展的通用语言建模骨干，适用于长短上下文任务。

Abstract: In this paper, we introduce \textbf{RWKV-X}, a novel hybrid architecture that
combines the efficiency of RWKV for short-range modeling with a sparse
attention mechanism designed to capture long-range context. Unlike previous
hybrid approaches that rely on full attention layers and retain quadratic
complexity, RWKV-X achieves linear-time complexity in training and
constant-time complexity in inference decoding. We demonstrate that RWKV-X,
when continually pretrained on 64K-token sequences, achieves near-perfect
accuracy on the 64K passkey retrieval benchmark. It consistently outperforms
prior RWKV-7 models on long-context benchmarks, while maintaining strong
performance on short-context tasks. These results highlight RWKV-X as a
scalable and efficient backbone for general-purpose language modeling, capable
of decoding sequences up to 1 million tokens with stable speed and memory
usage. To facilitate further research and analysis, we have made the
checkpoints and the associated code publicly accessible at:
https://github.com/howard-hou/RWKV-X.

</details>


### [29] [Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging](https://arxiv.org/abs/2504.21474)
*Hadi Bayrami Asl Tekanlou,Jafar Razmara,Mahsa Sanaei,Mostafa Rahgouy,Hamed Babaei Giglou*

Main category: cs.CL

TL;DR: Homa系统利用OntoAligner工具包，结合RAG技术，将主题标注问题转化为对齐任务，匹配记录与GND分类，评估其在多语言记录中的效果。


<details>
  <summary>Details</summary>
Motivation: 解决SemEval-2025任务5中的主题标注问题，提升数字图书馆中主题标注的自动化水平。

Method: 使用OntoAligner工具包，结合检索增强生成（RAG）技术，将主题标注问题转化为语义相似度对齐任务。

Result: 实验展示了该方法在多语言记录中的优势和局限性。

Conclusion: 对齐技术在改进数字图书馆主题标注方面具有潜力。

Abstract: This paper presents our system, Homa, for SemEval-2025 Task 5: Subject
Tagging, which focuses on automatically assigning subject labels to technical
records from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage
OntoAligner, a modular ontology alignment toolkit, to address this task by
integrating retrieval-augmented generation (RAG) techniques. Our approach
formulates the subject tagging problem as an alignment task, where records are
matched to GND categories based on semantic similarity. We evaluate
OntoAligner's adaptability for subject indexing and analyze its effectiveness
in handling multilingual records. Experimental results demonstrate the
strengths and limitations of this method, highlighting the potential of
alignment techniques for improving subject tagging in digital libraries.

</details>


### [30] [Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines](https://arxiv.org/abs/2504.21475)
*Serry Sibaee,Samar Ahmed,Abdullah Al Harbi,Omer Nacar,Adel Ammar,Yasser Habashi,Wadii Boulila*

Main category: cs.CL

TL;DR: 该研究开发了一种基于Transformer的阿拉伯语反向词典系统，通过半编码器神经网络架构实现最先进性能，并提出了阿拉伯语词典定义的质量标准。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语自然语言处理中的关键空白，提供基于描述或含义查找单词的工具。

Method: 采用半编码器神经网络架构，结合几何递减层，使用预训练模型（如ARBERTv2）进行实验。

Result: ARBERTv2表现最佳（排名得分0.0644），并提出了8项构建高质量反向词典资源的标准。

Conclusion: 该研究显著推动了阿拉伯语计算语言学，为语言学习、学术写作和专业交流提供了实用工具。

Abstract: This study addresses the critical gap in Arabic natural language processing
by developing an effective Arabic Reverse Dictionary (RD) system that enables
users to find words based on their descriptions or meanings. We present a novel
transformer-based approach with a semi-encoder neural network architecture
featuring geometrically decreasing layers that achieves state-of-the-art
results for Arabic RD tasks. Our methodology incorporates a comprehensive
dataset construction process and establishes formal quality standards for
Arabic lexicographic definitions. Experiments with various pre-trained models
demonstrate that Arabic-specific models significantly outperform general
multilingual embeddings, with ARBERTv2 achieving the best ranking score
(0.0644). Additionally, we provide a formal abstraction of the reverse
dictionary task that enhances theoretical understanding and develop a modular,
extensible Python library (RDTL) with configurable training pipelines. Our
analysis of dataset quality reveals important insights for improving Arabic
definition construction, leading to eight specific standards for building
high-quality reverse dictionary resources. This work contributes significantly
to Arabic computational linguistics and provides valuable tools for language
learning, academic writing, and professional communication in Arabic.

</details>


### [31] [Improving Informally Romanized Language Identification](https://arxiv.org/abs/2504.21540)
*Adrian Benton,Alexander Gutkin,Christo Kirov,Brian Roark*

Main category: cs.CL

TL;DR: 论文提出了一种通过合成训练数据来提高拉丁化文本语言识别（LID）准确性的方法，显著提升了20种印度语言的识别性能。


<details>
  <summary>Details</summary>
Motivation: 拉丁化文本因拼写变异性高导致语言难以区分（如印地语和乌尔都语），现有方法识别效果不佳。

Method: 通过合成包含自然拼写变异的训练样本，结合线性分类器或额外真实文本训练。

Result: 在Bhasha-Abhijnaanam评估集上，F1分数从74.7%提升至85.4%（仅合成数据）和88.2%（结合真实文本）。

Conclusion: 合成数据训练显著提升了拉丁化文本的语言识别性能，优于传统方法和高容量模型。

Abstract: The Latin script is often used to informally write languages with non-Latin
native scripts. In many cases (e.g., most languages in India), there is no
conventional spelling of words in the Latin script, hence there will be high
spelling variability in written text. Such romanization renders languages that
are normally easily distinguished based on script highly confusable, such as
Hindi and Urdu. In this work, we increase language identification (LID)
accuracy for romanized text by improving the methods used to synthesize
training sets. We find that training on synthetic samples which incorporate
natural spelling variation yields higher LID system accuracy than including
available naturally occurring examples in the training set, or even training
higher capacity models. We demonstrate new state-of-the-art LID performance on
romanized text from 20 Indic languages in the Bhasha-Abhijnaanam evaluation set
(Madhani et al., 2023a), improving test F1 from the reported 74.7% (using a
pretrained neural model) to 85.4% using a linear classifier trained solely on
synthetic data and 88.2% when also training on available harvested text.

</details>


### [32] [TartuNLP at SemEval-2025 Task 5: Subject Tagging as Two-Stage Information Retrieval](https://arxiv.org/abs/2504.21547)
*Aleksei Dorkin,Kairit Sirts*

Main category: cs.CL

TL;DR: 论文提出了一种两阶段信息检索系统，用于为图书馆记录分配主题标签，结合了双编码器和交叉编码器，显著提高了召回率。


<details>
  <summary>Details</summary>
Motivation: 帮助图书馆员为图书馆记录分配主题标签，通过文档内容从大型主题分类中检索相关标签。

Method: 将任务视为信息检索问题，使用双编码器进行粗粒度候选提取，交叉编码器进行细粒度重排序。

Result: 该方法显著提高了召回率，并在定性评估中表现出竞争力。

Conclusion: 两阶段信息检索系统在主题标签分配任务中表现出高效性和竞争力。

Abstract: We present our submission to the Task 5 of SemEval-2025 that aims to aid
librarians in assigning subject tags to the library records by producing a list
of likely relevant tags for a given document. We frame the task as an
information retrieval problem, where the document content is used to retrieve
subject tags from a large subject taxonomy. We leverage two types of encoder
models to build a two-stage information retrieval system -- a bi-encoder for
coarse-grained candidate extraction at the first stage, and a cross-encoder for
fine-grained re-ranking at the second stage. This approach proved effective,
demonstrating significant improvements in recall compared to single-stage
methods and showing competitive results according to qualitative evaluation.

</details>


### [33] [Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models](https://arxiv.org/abs/2504.21553)
*Lucas Maisonnave,Cyril Moineau,Olivier Bichler,Fabrice Rastello*

Main category: cs.CL

TL;DR: 该论文研究了LLaMA架构及其衍生模型中的量化问题，提出了一种针对激活异常值的新型混合精度量化方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的规模给部署和推理带来了挑战，因此需要更高效的量化方法。

Method: 提出了一种混合精度量化方法，对LLaMA架构中特定投影层使用更高精度（FP16或FP8），其余部分量化到更低比特宽度。

Result: 实验表明，该方法在LLaMA2、LLaMA3和Mistral模型上显著降低了困惑度并提升了零样本准确率。

Conclusion: 研究强调了模型特定特征在量化中的重要性，为资源受限环境中的LLMs部署提供了有效解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various natural language processing tasks. However, their size presents
significant challenges for deployment and inference. This paper investigates
the quantization of LLMs, focusing on the LLaMA architecture and its
derivatives. We challenge existing assumptions about activation outliers in
LLMs and propose a novel mixed-precision quantization approach tailored for
LLaMA-like models. Our method leverages the observation that activation spikes
in LLaMA architectures are predominantly concentrated in specific projection
layers. By applying higher precision (FP16 or FP8) to these layers while
quantizing the rest of the model to lower bit-widths, we achieve superior
performance compared to existing quantization techniques. Experimental results
on LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in
perplexity and zero-shot accuracy, particularly for 8-bit per-tensor
quantization. Our approach outperforms general-purpose methods designed to
handle outliers across all architecture types, highlighting the benefits of
architecture-specific quantization strategies. This research contributes to the
ongoing efforts to make LLMs more efficient and deployable, potentially
enabling their use in resource-constrained environments. Our findings emphasize
the importance of considering model-specific characteristics in developing
effective quantization pipelines for state-of-the-art language models by
identifying and targeting a small number of projections that concentrate
activation spikes.

</details>


### [34] [DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing](https://arxiv.org/abs/2504.21589)
*Lisa Kluge,Maximilian Kähler*

Main category: cs.CL

TL;DR: 本文介绍了为SemEval-2025任务5开发的系统，利用LLM进行自动主题标注，结合少样本提示和后处理步骤，在定量排名中位列第四，但在专家定性评估中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为技术图书馆的开放获取目录开发自动化主题标注系统，提升标注效率和准确性。

Method: 使用少样本提示技术结合多个LLM生成关键词，并通过后处理步骤映射到目标词汇表，进行集成投票和相关性排序。

Result: 系统在定量排名中第四，但在专家定性评估中表现最佳。

Conclusion: 该方法在自动化主题标注中具有潜力，尤其在专家评估中表现优异。

Abstract: This paper presents our system developed for the SemEval-2025 Task 5:
LLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical
Library's Open-Access Catalog. Our system relies on prompting a selection of
LLMs with varying examples of intellectually annotated records and asking the
LLMs to similarly suggest keywords for new records. This few-shot prompting
technique is combined with a series of post-processing steps that map the
generated keywords to the target vocabulary, aggregate the resulting subject
terms to an ensemble vote and, finally, rank them as to their relevance to the
record. Our system is fourth in the quantitative ranking in the all-subjects
track, but achieves the best result in the qualitative ranking conducted by
subject indexing experts.

</details>


### [35] [Robust Misinformation Detection by Visiting Potential Commonsense Conflict](https://arxiv.org/abs/2504.21604)
*Bing Wang,Ximing Li,Changchun Li,Bingrui Zhao,Bo Fu,Renchu Guan,Shengsheng Wang*

Main category: cs.CL

TL;DR: 提出了一种名为MD-PCC的新型插件式增强方法，用于自动检测网络虚假信息，通过利用常识冲突作为特征增强，并在新数据集CoMis上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 互联网技术的发展导致虚假信息泛滥，亟需自动检测方法。研究发现虚假文章更易涉及常识冲突，因此利用这一特征提升检测效果。

Method: 构建常识表达式以表示潜在常识冲突，并将其作为增强特征用于训练任何现有的虚假信息检测方法。同时收集了一个基于常识冲突的新数据集CoMis。

Result: MD-PCC在4个公共基准数据集和CoMis上均优于现有基线方法。

Conclusion: MD-PCC通过利用常识冲突特征，显著提升了虚假信息检测的性能，为未来研究提供了新方向。

Abstract: The development of Internet technology has led to an increased prevalence of
misinformation, causing severe negative effects across diverse domains. To
mitigate this challenge, Misinformation Detection (MD), aiming to detect online
misinformation automatically, emerges as a rapidly growing research topic in
the community. In this paper, we propose a novel plug-and-play augmentation
method for the MD task, namely Misinformation Detection with Potential
Commonsense Conflict (MD-PCC). We take inspiration from the prior studies
indicating that fake articles are more likely to involve commonsense conflict.
Accordingly, we construct commonsense expressions for articles, serving to
express potential commonsense conflicts inferred by the difference between
extracted commonsense triplet and golden ones inferred by the well-established
commonsense reasoning tool COMET. These expressions are then specified for each
article as augmentation. Any specific MD methods can be then trained on those
commonsense-augmented articles. Besides, we also collect a novel
commonsense-oriented dataset named CoMis, whose all fake articles are caused by
commonsense conflict. We integrate MD-PCC with various existing MD backbones
and compare them across both 4 public benchmark datasets and CoMis. Empirical
results demonstrate that MD-PCC can consistently outperform the existing MD
baselines.

</details>


### [36] [RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations](https://arxiv.org/abs/2504.21605)
*Jonas Gwozdz,Andreas Both*

Main category: cs.CL

TL;DR: 提出了一种基于RDF的框架，用于评估多语言大语言模型（LLM）在知识冲突情况下的可靠性，重点关注知识泄漏、错误检测和多语言一致性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统评估LLM在信息冲突情况下的可靠性的方法，尤其是在多语言环境中。

Method: 通过四种上下文条件（完整、不完整、冲突和无上下文）在德语和英语中捕获模型响应，并利用RDF结构化表示进行分析。

Result: 实验表明，该框架能有效揭示模型在上下文优先级和语言特定性能上的关键模式，且词汇足以覆盖28个问题的所有评估方面。

Conclusion: 提出的框架为评估LLM在多语言环境中的知识冲突提供了全面且结构化的方法。

Abstract: Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet
systematically assessing their reliability with conflicting information remains
difficult. We propose an RDF-based framework to assess multilingual LLM
quality, focusing on knowledge conflicts. Our approach captures model responses
across four distinct context conditions (complete, incomplete, conflicting, and
no-context information) in German and English. This structured representation
enables the comprehensive analysis of knowledge leakage-where models favor
training data over provided context-error detection, and multilingual
consistency. We demonstrate the framework through a fire safety domain
experiment, revealing critical patterns in context prioritization and
language-specific performance, and demonstrating that our vocabulary was
sufficient to express every assessment facet encountered in the 28-question
study.

</details>


### [37] [Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn Instruction-Following Ability](https://arxiv.org/abs/2504.21625)
*Jiaming Wang*

Main category: cs.CL

TL;DR: Meeseeks是一个新的指令跟随基准测试，通过迭代反馈模拟真实人机交互，支持模型自我修正，并全面评估LLMs的指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 现有指令跟随基准多为单轮或不允许自我修正，无法反映真实使用场景。Meeseeks旨在填补这一空白。

Method: 采用迭代反馈机制，允许模型自我修正，并通过38个能力标签在三个维度（意图识别、内容验证、输出结构验证）评估LLMs。

Result: Meeseeks为LLMs在实际应用中的指令跟随能力提供了有价值的见解。

Conclusion: Meeseeks通过模拟真实交互和全面评估，提升了LLMs指令跟随能力的评测标准。

Abstract: The ability to follow instructions accurately is fundamental for Large
Language Models (LLMs) to serve as reliable agents in real-world applications.
While existing instruction-following benchmarks are either single-turn or
introduce new requirements in each turn without allowing self-correction,
Meeseeks simulates realistic human-LLM interactions through an iterative
feedback process. This design enables models to self-correct based on specific
requirement failures, better reflecting real-world user-end usage patterns. The
benchmark implements a comprehensive evaluation system with 38 capability tags
organized across three dimensions: Intent Recognition, Granular Content
Validation, and Output Structure Validation. Through rigorous evaluation across
LLMs, Meeseeks provides valuable insights into LLMs' instruction-following
capabilities in practical applications.

</details>


### [38] [Sadeed: Advancing Arabic Diacritization Through Small Language Model](https://arxiv.org/abs/2504.21635)
*Zeina Aldallal,Sara Chrouf,Khalil Hennara,Mohamed Motaism Hamed,Muhammad Hreden,Safwan AlModhayan*

Main category: cs.CL

TL;DR: Sadeed是一种基于Kuwain 1.5B微调的阿拉伯语文本标注模型，性能优于传统模型，并提出了新的评测基准SadeedDiac-25。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语文本标注因形态丰富而具挑战性，需高效解决方案。

Method: 基于Kuwain 1.5B微调，使用高质量标注数据集，并设计数据清理流程。

Result: Sadeed性能优于传统模型，接近专有大模型，且计算资源需求低。

Conclusion: Sadeed与SadeedDiac-25为阿拉伯语NLP应用提供了坚实基础。

Abstract: Arabic text diacritization remains a persistent challenge in natural language
processing due to the language's morphological richness. In this paper, we
introduce Sadeed, a novel approach based on a fine-tuned decoder-only language
model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model
originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully
curated, high-quality diacritized datasets, constructed through a rigorous
data-cleaning and normalization pipeline. Despite utilizing modest
computational resources, Sadeed achieves competitive results compared to
proprietary large language models and outperforms traditional models trained on
similar domains. Additionally, we highlight key limitations in current
benchmarking practices for Arabic diacritization. To address these issues, we
introduce SadeedDiac-25, a new benchmark designed to enable fairer and more
comprehensive evaluation across diverse text genres and complexity levels.
Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing
Arabic NLP applications, including machine translation, text-to-speech, and
language learning tools.

</details>


### [39] [20min-XD: A Comparable Corpus of Swiss News Articles](https://arxiv.org/abs/2504.21677)
*Michelle Wastl,Jannis Vamvas,Selena Calleri,Rico Sennrich*

Main category: cs.CL

TL;DR: 20min-XD是一个法德双语新闻语料库，包含15,000对文章，基于语义相似度自动对齐，适用于NLP研究和语言学分析。


<details>
  <summary>Details</summary>
Motivation: 构建一个跨语言、文档级别的可比语料库，以支持多种NLP应用和语言学研究的需要。

Method: 从瑞士新闻网站20 Minuten/20 minutes收集2015至2024年的文章，通过语义相似度自动对齐。

Result: 语料库包含从近似翻译到松散相关文章的广泛跨语言相似性，公开了文档和句子对齐版本及相关代码。

Conclusion: 20min-XD是一个有价值的资源，适用于NLP和语言学研究的多样化需求。

Abstract: We present 20min-XD (20 Minuten cross-lingual document-level), a
French-German, document-level comparable corpus of news articles, sourced from
the Swiss online news outlet 20 Minuten/20 minutes. Our dataset comprises
around 15,000 article pairs spanning 2015 to 2024, automatically aligned based
on semantic similarity. We detail the data collection process and alignment
methodology. Furthermore, we provide a qualitative and quantitative analysis of
the corpus. The resulting dataset exhibits a broad spectrum of cross-lingual
similarity, ranging from near-translations to loosely related articles, making
it valuable for various NLP applications and broad linguistically motivated
studies. We publicly release the dataset in document- and sentence-aligned
versions and code for the described experiments.

</details>


### [40] [Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders](https://arxiv.org/abs/2504.21681)
*Andrei-Alexandru Manea,Jindřich Libovický*

Main category: cs.CL

TL;DR: 研究多语言视觉-语言任务中并行数据对预训练模型迁移的影响，发现机器翻译的任务数据效果最佳，但某些语言中真实平行数据表现更好，且多语言训练对大多数语言有益。


<details>
  <summary>Details</summary>
Motivation: 现有预训练视觉-语言模型及下游任务数据多为英文，多语言任务需依赖跨语言迁移，但并行数据的影响（如领域和语言数量）未被充分研究。

Method: 通过并行数据迁移已训练编码器，分析并行数据的领域和语言数量对迁移效果的影响。

Result: 机器翻译的任务数据平均效果最佳，但某些语言中真实平行数据表现更优；多语言训练对大多数语言有益。

Conclusion: 并行数据的领域和语言数量对迁移效果有显著影响，多语言训练是提升多语言任务性能的有效策略。

Abstract: Most pre-trained Vision-Language (VL) models and training data for the
downstream tasks are only available in English. Therefore, multilingual VL
tasks are solved using cross-lingual transfer: fine-tune a multilingual
pre-trained model or transfer the text encoder using parallel data. We study
the alternative approach: transferring an already trained encoder using
parallel data. We investigate the effect of parallel data: domain and the
number of languages, which were out of focus in previous work. Our results show
that even machine-translated task data are the best on average, caption-like
authentic parallel data outperformed it in some languages. Further, we show
that most languages benefit from multilingual training.

</details>


### [41] [Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning](https://arxiv.org/abs/2504.21685)
*Reem Abdel-Salam,Mary Adewunmi*

Main category: cs.CL

TL;DR: 论文提出了一种结合词性标注信息（POS）和参数高效微调技术（PEFT）的方法，用于提升社交媒体中健康提及分类（HMC）的准确性。


<details>
  <summary>Details</summary>
Motivation: 健康提及分类在实时追踪和公共卫生监测中至关重要，但由于其复杂性（如比喻性语言和描述性术语），传统方法效果有限。

Method: 采用词性标注信息（POS）和参数高效微调技术（PEFT）的组合方法，并在RHDM、PHM和Illness三个数据集上进行实验。

Result: 实验表明，结合POS和PEFT的方法显著提升了F1分数，同时使用更小的模型和更高效的训练。

Conclusion: 该方法为社交媒体中的健康提及分类提供了一种高效且准确的解决方案。

Abstract: Health Mention Classification (HMC) plays a critical role in leveraging
social media posts for real-time tracking and public health monitoring.
Nevertheless, the process of HMC presents significant challenges due to its
intricate nature, primarily stemming from the contextual aspects of health
mentions, such as figurative language and descriptive terminology, rather than
explicitly reflecting a personal ailment. To address this problem, we argue
that clearer mentions can be achieved through conventional fine-tuning with
enhanced parameters of biomedical natural language methods (NLP). In this
study, we explore different techniques such as the utilisation of
part-of-speech (POS) tagger information, improving on PEFT techniques, and
different combinations thereof. Extensive experiments are conducted on three
widely used datasets: RHDM, PHM, and Illness. The results incorporated POS
tagger information, and leveraging PEFT techniques significantly improves
performance in terms of F1-score compared to state-of-the-art methods across
all three datasets by utilising smaller models and efficient training.
Furthermore, the findings highlight the effectiveness of incorporating POS
tagger information and leveraging PEFT techniques for HMC. In conclusion, the
proposed methodology presents a potentially effective approach to accurately
classifying health mentions in social media posts while optimising the model
size and training efficiency.

</details>


### [42] [Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models](https://arxiv.org/abs/2504.21742)
*Emelie Hallenberg*

Main category: cs.CL

TL;DR: 利用大语言模型分析希腊爱情小说中的文学母题，发现其共性与差异。


<details>
  <summary>Details</summary>
Motivation: 探究希腊爱情小说中文学母题的共性与变化，揭示潜在趋势或外部影响。

Method: 使用精细调整的大语言模型提取并分析文本中的文学母题。

Result: 部分母题贯穿始终，其他母题频率波动，显示趋势或外部影响。

Conclusion: 方法有效提取文学母题，支持定量与定性分析。

Abstract: The Greek fictional narratives often termed love novels or romances, ranging
from the first century CE to the middle of the 15th century, have long been
considered as similar in many ways, not least in the use of particular literary
motifs. By applying the use of fine-tuned large language models, this study
aims to investigate which motifs exactly that the texts in this corpus have in
common, and in which ways they differ from each other. The results show that
while some motifs persist throughout the corpus, others fluctuate in frequency,
indicating certain trends or external influences. Conclusively, the method
proves to adequately extract literary motifs according to a set definition,
providing data for both quantitative and qualitative analyses.

</details>


### [43] [Improving Retrieval-Augmented Neural Machine Translation with Monolingual Data](https://arxiv.org/abs/2504.21747)
*Maxime Bouthors,Josep Crego,François Yvon*

Main category: cs.CL

TL;DR: 论文探讨了如何利用目标语言单语语料库改进检索增强神经机器翻译（RANMT）系统，设计了跨语言检索系统，实验证明其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统RANMT系统依赖双语语料库，但目标语言单语语料库在许多场景下更易获取，研究旨在利用这些资源提升翻译性能。

Method: 设计改进的跨语言检索系统，结合句子级和词级匹配目标进行训练，并在两种RANMT架构上实验验证。

Result: 实验表明，新方法在控制环境和实际场景中均优于传统TM模型和通用跨语言检索方法。

Conclusion: 利用目标语言单语语料库的跨语言检索方法显著提升了翻译性能，优于现有技术。

Abstract: Conventional retrieval-augmented neural machine translation (RANMT) systems
leverage bilingual corpora, e.g., translation memories (TMs). Yet, in many
settings, in-domain monolingual target-side corpora are often available. This
work explores ways to take advantage of such resources by retrieving relevant
segments directly in the target language, based on a source-side query. For
this, we design improved cross-lingual retrieval systems, trained with both
sentence level and word-level matching objectives. In our experiments with two
RANMT architectures, we first demonstrate the benefits of such cross-lingual
objectives in a controlled setting, obtaining translation performances that
surpass standard TM-based models. We then showcase our method on a real-world
set-up, where the target monolingual resources far exceed the amount of
parallel data and observe large improvements of our new techniques, which
outperform both the baseline setting, and general-purpose cross-lingual
retrievers.

</details>


### [44] [MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness](https://arxiv.org/abs/2504.21773)
*Junsheng Huang,Zhitao He,Sandeep Polisetty,Qingyun Wang,May Fung*

Main category: cs.CL

TL;DR: 论文提出了一种新方法MAC-Tuning，用于在多问题设置下提升大语言模型（LLMs）的置信度估计能力，解决了幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，其生成虚假事实（幻觉）的问题日益突出。现有研究多关注单一问题设置，而多问题设置下的模型知识边界意识研究不足。

Method: 提出了MAC-Tuning方法，通过在指令数据微调中分离答案预测和置信度估计的学习过程。

Result: 实验表明，该方法在平均精度上比基线方法提升了25%。

Conclusion: MAC-Tuning在多问题设置下有效提升了LLMs的置信度估计能力，为解决幻觉问题提供了新思路。

Abstract: With the widespread application of large language models (LLMs), the issue of
generating non-existing facts, known as hallucination, has garnered increasing
attention. Previous research in enhancing LLM confidence estimation mainly
focuses on the single problem setting. However, LLM awareness of its internal
parameterized knowledge boundary under the more challenging multi-problem
setting, which requires answering multiple problems accurately simultaneously,
remains underexplored. To bridge this gap, we introduce a novel method,
Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates
the learning of answer prediction and confidence estimation during fine-tuning
on instruction data. Extensive experiments demonstrate that our method
outperforms baselines by up to 25% in average precision.

</details>


### [45] [WebThinker: Empowering Large Reasoning Models with Deep Research Capability](https://arxiv.org/abs/2504.21776)
*Xiaoxi Li,Jiajie Jin,Guanting Dong,Hongjin Qian,Yutao Zhu,Yongkang Wu,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.CL

TL;DR: WebThinker是一个增强大型推理模型（LRMs）的研究代理，通过动态搜索网络、提取信息并实时生成报告，显著提升了复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型推理模型依赖静态知识，难以处理复杂、知识密集型任务，尤其是需要综合网络信息的科研报告。

Method: WebThinker结合了深度网络探索模块和自主思考-搜索-草拟策略，并通过RL训练优化性能。

Result: 在多个复杂推理基准和科研报告任务中，WebThinker显著优于现有方法和专有系统。

Conclusion: WebThinker提升了LRMs在复杂场景中的可靠性和适用性，为更强大的深度研究系统铺平了道路。

Abstract: Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate
impressive long-horizon reasoning capabilities. However, their reliance on
static internal knowledge limits their performance on complex,
knowledge-intensive tasks and hinders their ability to produce comprehensive
research reports requiring synthesis of diverse web information. To address
this, we propose \textbf{WebThinker}, a deep research agent that empowers LRMs
to autonomously search the web, navigate web pages, and draft research reports
during the reasoning process. WebThinker integrates a \textbf{Deep Web
Explorer} module, enabling LRMs to dynamically search, navigate, and extract
information from the web when encountering knowledge gaps. It also employs an
\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to
seamlessly interleave reasoning, information gathering, and report writing in
real time. To further enhance research tool utilization, we introduce an
\textbf{RL-based training strategy} via iterative online Direct Preference
Optimization (DPO). Extensive experiments on complex reasoning benchmarks
(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)
demonstrate that WebThinker significantly outperforms existing methods and
strong proprietary systems. Our approach enhances LRM reliability and
applicability in complex scenarios, paving the way for more capable and
versatile deep research systems. The code is available at
https://github.com/RUC-NLPIR/WebThinker.

</details>


### [46] [How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues](https://arxiv.org/abs/2504.21800)
*Suhas BN,Dominik Mattioli,Saeed Abdullah,Rosa I. Arriaga,Chris W. Wiese,Andrew M. Sherrill*

Main category: cs.CL

TL;DR: 论文探讨了在PTSD治疗中使用合成数据替代真实数据的潜力，发现合成数据在结构上接近真实对话，但在临床关键指标上表现不足。


<details>
  <summary>Details</summary>
Motivation: 隐私问题、真实数据获取困难和高标注成本推动了合成数据在医疗领域的应用。

Method: 系统比较真实与合成对话，使用语言、结构和协议特定指标，并引入PE专用评估框架。

Result: 合成数据在结构上与真实数据相似（如说话者切换比0.98 vs. 0.99），但未能捕捉关键临床指标（如痛苦监测）。

Conclusion: 合成数据可缓解数据稀缺和隐私问题，但需开发超越表面流畅性的临床保真度评估指标。

Abstract: The growing adoption of synthetic data in healthcare is driven by privacy
concerns, limited access to real-world data, and the high cost of annotation.
This work explores the use of synthetic Prolonged Exposure (PE) therapeutic
conversations for Post-Traumatic Stress Disorder (PTSD) as a scalable
alternative for training and evaluating clinical models. We systematically
compare real and synthetic dialogues using linguistic, structural, and
protocol-specific metrics, including turn-taking patterns and treatment
fidelity. We also introduce and evaluate PE-specific metrics derived from
linguistic analysis and semantic modeling, offering a novel framework for
assessing clinical fidelity beyond surface fluency. Our findings show that
although synthetic data holds promise for mitigating data scarcity and
protecting patient privacy, it can struggle to capture the subtle dynamics of
therapeutic interactions. In our dataset, synthetic dialogues match structural
features of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99),
however, synthetic interactions do not adequately reflect key fidelity markers
(e.g., distress monitoring). We highlight gaps in existing evaluation
frameworks and advocate for fidelity-aware metrics that go beyond surface
fluency to uncover clinically significant failures. Our findings clarify where
synthetic data can effectively complement real-world datasets -- and where
critical limitations remain.

</details>


### [47] [DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition](https://arxiv.org/abs/2504.21801)
*Z. Z. Ren,Zhihong Shao,Junxiao Song,Huajian Xin,Haocheng Wang,Wanjia Zhao,Liyue Zhang,Zhe Fu,Qihao Zhu,Dejian Yang,Z. F. Wu,Zhibin Gou,Shirong Ma,Hongxuan Tang,Yuxuan Liu,Wenjun Gao,Daya Guo,Chong Ruan*

Main category: cs.CL

TL;DR: DeepSeek-Prover-V2是一个开源大型语言模型，专为Lean 4中的形式定理证明设计，通过递归定理证明流程初始化数据，结合非正式和正式数学推理，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 旨在通过结合非正式和正式数学推理，提升大型语言模型在形式定理证明中的性能，缩小形式与非形式推理之间的差距。

Method: 利用DeepSeek-V3分解复杂问题为子目标，生成链式推理过程，作为强化学习的初始数据，训练出统一模型DeepSeek-Prover-V2-671B。

Result: 在MiniF2F-test中达到88.9%通过率，解决PutnamBench的49/658问题，并在ProverBench中表现良好，解决6/15 AIME问题。

Conclusion: DeepSeek-Prover-V2在形式定理证明中表现优异，缩小了形式与非形式推理的差距，为未来研究提供了新方向。

Abstract: We introduce DeepSeek-Prover-V2, an open-source large language model designed
for formal theorem proving in Lean 4, with initialization data collected
through a recursive theorem proving pipeline powered by DeepSeek-V3. The
cold-start training procedure begins by prompting DeepSeek-V3 to decompose
complex problems into a series of subgoals. The proofs of resolved subgoals are
synthesized into a chain-of-thought process, combined with DeepSeek-V3's
step-by-step reasoning, to create an initial cold start for reinforcement
learning. This process enables us to integrate both informal and formal
mathematical reasoning into a unified model. The resulting model,
DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural
theorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49
out of 658 problems from PutnamBench. In addition to standard benchmarks, we
introduce ProverBench, a collection of 325 formalized problems, to enrich our
evaluation, including 15 selected problems from the recent AIME competitions
(years 24-25). Further evaluation on these 15 AIME problems shows that the
model successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of
these problems using majority voting, highlighting that the gap between formal
and informal mathematical reasoning in large language models is substantially
narrowing.

</details>


### [48] [TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments](https://arxiv.org/abs/2504.21851)
*Sichang Tu,Abigail Powers,Stephen Doogan,Jinho D. Choi*

Main category: cs.CL

TL;DR: 研究开发了基于LLM的对话系统TRUST，用于标准化诊断访谈，填补心理健康领域的技术空白。


<details>
  <summary>Details</summary>
Motivation: 现有技术未探索LLM在标准化诊断访谈中的应用，研究旨在提升心理健康服务的可及性。

Method: 提出TRUST框架，结合临床对话行为模式和患者模拟方法，用于PTSD诊断。

Result: 专家评估显示TRUST表现接近真实临床访谈，具备实用性。

Conclusion: TRUST框架展现了提升心理健康服务可用性的潜力。

Abstract: Objectives: While Large Language Models (LLMs) have been widely used to
assist clinicians and support patients, no existing work has explored dialogue
systems for standard diagnostic interviews and assessments. This study aims to
bridge the gap in mental healthcare accessibility by developing an LLM-powered
dialogue system that replicates clinician behavior. Materials and Methods: We
introduce TRUST, a framework of cooperative LLM modules capable of conducting
formal diagnostic interviews and assessments for Post-Traumatic Stress Disorder
(PTSD). To guide the generation of appropriate clinical responses, we propose a
Dialogue Acts schema specifically designed for clinical interviews.
Additionally, we develop a patient simulation approach based on real-life
interview transcripts to replace time-consuming and costly manual testing by
clinicians. Results: A comprehensive set of evaluation metrics is designed to
assess the dialogue system from both the agent and patient simulation
perspectives. Expert evaluations by conversation and clinical specialists show
that TRUST performs comparably to real-life clinical interviews. Discussion:
Our system performs at the level of average clinicians, with room for future
enhancements in communication styles and response appropriateness. Conclusions:
Our TRUST framework shows its potential to facilitate mental healthcare
availability.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [49] [Can a Large Language Model Assess Urban Design Quality? Evaluating Walkability Metrics Across Expertise Levels](https://arxiv.org/abs/2504.21040)
*Chenyi Cai,Kosuke Kuriyama,Youlong Gu,Filip Biljecki,Pieter Herthogs*

Main category: cs.CV

TL;DR: 研究探讨了如何通过整合专家知识提升多模态大语言模型（MLLMs）在评估城市步行性方面的能力，发现专家知识能提高模型的一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 探索专家知识对MLLMs在评估城市设计质量中的影响，以提升其可靠性和能力。

Method: 收集步行性指标并分类，设计不同清晰度和特异性的提示，测试MLLM（ChatGPT-4）对街景图像步行性的评估能力。

Result: MLLMs能基于通用知识提供评估，但易过于乐观或误解指标；整合专家知识后，评估表现更一致和集中。

Conclusion: 专家知识能显著提升MLLMs在城市设计评估中的性能，但需注意其局限性。

Abstract: Urban street environments are vital to supporting human activity in public
spaces. The emergence of big data, such as street view images (SVIs) combined
with multimodal large language models (MLLMs), is transforming how researchers
and practitioners investigate, measure, and evaluate semantic and visual
elements of urban environments. Considering the low threshold for creating
automated evaluative workflows using MLLMs, it is crucial to explore both the
risks and opportunities associated with these probabilistic models. In
particular, the extent to which the integration of expert knowledge can
influence the performance of MLLMs in evaluating the quality of urban design
has not been fully explored. This study sets out an initial exploration of how
integrating more formal and structured representations of expert urban design
knowledge into the input prompts of an MLLM (ChatGPT-4) can enhance the model's
capability and reliability in evaluating the walkability of built environments
using SVIs. We collect walkability metrics from the existing literature and
categorize them using relevant ontologies. We then select a subset of these
metrics, focusing on the subthemes of pedestrian safety and attractiveness, and
develop prompts for the MLLM accordingly. We analyze the MLLM's ability to
evaluate SVI walkability subthemes through prompts with varying levels of
clarity and specificity regarding evaluation criteria. Our experiments
demonstrate that MLLMs are capable of providing assessments and interpretations
based on general knowledge and can support the automation of multimodal
image-text evaluations. However, they generally provide more optimistic scores
and can make mistakes when interpreting the provided metrics, resulting in
incorrect evaluations. By integrating expert knowledge, the MLLM's evaluative
performance exhibits higher consistency and concentration.

</details>


### [50] [Legilimens: Performant Video Analytics on the System-on-Chip Edge](https://arxiv.org/abs/2504.21136)
*Murali Ramanujam,Yinwei Dai,Kyle Jamieson,Ravi Netravali*

Main category: cs.CV

TL;DR: Legilimens是一种针对移动边缘设备（如无人机和行车记录仪）的持续学习系统，利用设备内存中的基础模型和少量样本高效适应新场景，显著降低重训练成本并提升准确性。


<details>
  <summary>Details</summary>
Motivation: 传统边缘服务器资源有限，而移动边缘设备具有丰富的统一内存池但计算能力较弱，需要一种适应这种资源特性的持续学习系统。

Method: Legilimens通过高效选择样本、部分更新基础模型以及计算资源共享，实现轻量级重训练。

Result: 相比现有系统，Legilimens将重训练成本降低2.8-10倍，准确性提升18-45%。

Conclusion: Legilimens为移动边缘设备的持续学习提供了一种高效解决方案，显著提升了性能和资源利用率。

Abstract: Continually retraining models has emerged as a primary technique to enable
high-accuracy video analytics on edge devices. Yet, existing systems employ
such adaptation by relying on the spare compute resources that traditional
(memory-constrained) edge servers afford. In contrast, mobile edge devices such
as drones and dashcams offer a fundamentally different resource profile:
weak(er) compute with abundant unified memory pools. We present Legilimens, a
continuous learning system for the mobile edge's System-on-Chip GPUs. Our
driving insight is that visually distinct scenes that require retraining
exhibit substantial overlap in model embeddings; if captured into a base model
on device memory, specializing to each new scene can become lightweight,
requiring very few samples. To practically realize this approach, Legilimens
presents new, compute-efficient techniques to (1) select high-utility data
samples for retraining specialized models, (2) update the base model without
complete retraining, and (3) time-share compute resources between retraining
and live inference for maximal accuracy. Across diverse workloads, Legilimens
lowers retraining costs by 2.8-10x compared to existing systems, resulting in
18-45% higher accuracies.

</details>


### [51] [Emotion Recognition in Contemporary Dance Performances Using Laban Movement Analysis](https://arxiv.org/abs/2504.21154)
*Muhammad Turab,Philippe Colantoni,Damien Muselet,Alain Tremeau*

Main category: cs.CV

TL;DR: 提出了一种改进Laban运动分析特征描述符的新框架，用于当代舞蹈中的情感识别，准确率达96.85%。


<details>
  <summary>Details</summary>
Motivation: 改进现有情感识别方法，结合定量和定性特征，提升当代舞蹈中的情感识别效果。

Method: 从3D关键点数据中提取特征，训练多种分类器（如随机森林和支持向量机），并使用可解释机器学习方法分析特征。

Result: 最高准确率达到96.85%，在表演分析、舞蹈训练和人机交互中有应用潜力。

Conclusion: 该框架显著提升了当代舞蹈中的情感识别效果，具有广泛的应用前景。

Abstract: This paper presents a novel framework for emotion recognition in contemporary
dance by improving existing Laban Movement Analysis (LMA) feature descriptors
and introducing robust, novel descriptors that capture both quantitative and
qualitative aspects of the movement. Our approach extracts expressive
characteristics from 3D keypoints data of professional dancers performing
contemporary dance under various emotional states, and trains multiple
classifiers, including Random Forests and Support Vector Machines.
Additionally, we provide in-depth explanation of features and their impact on
model predictions using explainable machine learning methods. Overall, our
study improves emotion recognition in contemporary dance and offers promising
applications in performance analysis, dance training, and human--computer
interaction, with a highest accuracy of 96.85\%.

</details>


### [52] [Dance Style Recognition Using Laban Movement Analysis](https://arxiv.org/abs/2504.21166)
*Muhammad Turab,Philippe Colantoni,Damien Muselet,Alain Tremeau*

Main category: cs.CV

TL;DR: 论文提出了一种结合3D姿态估计、3D人体网格重建和地板感知身体建模的新方法，用于提取Laban运动分析（LMA）特征，并通过滑动窗口方法捕捉时间上下文，显著提高了舞蹈风格识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有舞蹈风格识别方法多关注跨帧运动分析，缺乏对时间上下文和动态过渡的捕捉，因此需要一种能够为LMA特征添加时间背景的方法。

Method: 提出了一种新流程，结合3D姿态估计、3D人体网格重建和地板感知身体建模提取LMA特征，并采用滑动窗口方法捕捉时间演化。使用机器学习分类并利用可解释AI方法评估特征贡献。

Result: 所提方法在舞蹈风格识别中达到99.18%的最高分类准确率，表明时间上下文的加入显著提升了性能。

Conclusion: 通过引入时间上下文和新型特征提取流程，舞蹈风格识别的准确性得到显著提升，为复杂人类活动分析提供了新思路。

Abstract: The growing interest in automated movement analysis has presented new
challenges in recognition of complex human activities including dance. This
study focuses on dance style recognition using features extracted using Laban
Movement Analysis. Previous studies for dance style recognition often focus on
cross-frame movement analysis, which limits the ability to capture temporal
context and dynamic transitions between movements. This gap highlights the need
for a method that can add temporal context to LMA features. For this, we
introduce a novel pipeline which combines 3D pose estimation, 3D human mesh
reconstruction, and floor aware body modeling to effectively extract LMA
features. To address the temporal limitation, we propose a sliding window
approach that captures movement evolution across time in features. These
features are then used to train various machine learning methods for
classification, and their explainability explainable AI methods to evaluate the
contribution of each feature to classification performance. Our proposed method
achieves a highest classification accuracy of 99.18\% which shows that the
addition of temporal context significantly improves dance style recognition
performance.

</details>


### [53] [Geolocating Earth Imagery from ISS: Integrating Machine Learning with Astronaut Photography for Enhanced Geographic Mapping](https://arxiv.org/abs/2504.21194)
*Vedika Srivastava,Hemant Kumar Singh,Jaisal Singh*

Main category: cs.CV

TL;DR: 本文提出了一种利用机器学习算法从国际空间站（ISS）图像中定位地球位置的新方法，通过三种不同的图像处理管道（神经网络、SIFT方法和GPT-4模型）实现了高精度的地理特征识别。


<details>
  <summary>Details</summary>
Motivation: ISS拍摄的照片虽然坐标精确，但具体地理位置常未标注，研究旨在填补这一空白。

Method: 采用三种图像处理管道：神经网络、SIFT方法和GPT-4模型，分别针对不同需求处理高分辨率ISS图像。

Result: 在140多张ISS图像上的测试表明，神经网络方法在地理特征匹配上表现优异，SIFT方法擅长处理放大图像，GPT-4模型则能提供丰富的地理描述。

Conclusion: 该研究提升了空间图像地理定位的准确性和效率，对遥感、地球观测及环境监测具有重要意义。

Abstract: This paper presents a novel approach to geolocating images captured from the
International Space Station (ISS) using advanced machine learning algorithms.
Despite having precise ISS coordinates, the specific Earth locations depicted
in astronaut-taken photographs often remain unidentified. Our research
addresses this gap by employing three distinct image processing pipelines: a
Neural Network based approach, a SIFT based method, and GPT-4 model. Each
pipeline is tailored to process high-resolution ISS imagery, identifying both
natural and man-made geographical features. Through extensive evaluation on a
diverse dataset of over 140 ISS images, our methods demonstrate significant
promise in automated geolocation with varied levels of success. The NN approach
showed a high success rate in accurately matching geographical features, while
the SIFT pipeline excelled in processing zoomed-in images. GPT-4 model provided
enriched geographical descriptions alongside location predictions. This
research contributes to the fields of remote sensing and Earth observation by
enhancing the accuracy and efficiency of geolocating space-based imagery,
thereby aiding environmental monitoring and global mapping efforts.

</details>


### [54] [MemeBLIP2: A novel lightweight multimodal system to detect harmful memes](https://arxiv.org/abs/2504.21226)
*Jiaqi Liu,Ran Tong,Aowei Shen,Shuzheng Li,Changlin Yang,Lisha Xu*

Main category: cs.CV

TL;DR: MemeBLIP2是一个轻量级多模态系统，通过有效结合图像和文本特征检测有害表情包。


<details>
  <summary>Details</summary>
Motivation: 表情包常结合图像和简短文本传播幽默或观点，但部分包含有害内容如仇恨言论，需有效检测。

Method: 基于BLIP-2核心模型，添加模块对齐图像与文本表征并融合，提升分类效果。

Result: 在PrideMM数据集上验证，MemeBLIP2能捕捉多模态细微线索，改进有害内容检测。

Conclusion: MemeBLIP2通过多模态融合有效识别有害表情包，包括讽刺或文化特定内容。

Abstract: Memes often merge visuals with brief text to share humor or opinions, yet
some memes contain harmful messages such as hate speech. In this paper, we
introduces MemeBLIP2, a light weight multimodal system that detects harmful
memes by combining image and text features effectively. We build on previous
studies by adding modules that align image and text representations into a
shared space and fuse them for better classification. Using BLIP-2 as the core
vision-language model, our system is evaluated on the PrideMM datasets. The
results show that MemeBLIP2 can capture subtle cues in both modalities, even in
cases with ironic or culturally specific content, thereby improving the
detection of harmful material.

</details>


### [55] [T2ID-CAS: Diffusion Model and Class Aware Sampling to Mitigate Class Imbalance in Neck Ultrasound Anatomical Landmark Detection](https://arxiv.org/abs/2504.21231)
*Manikanta Varaganti,Amulya Vankayalapati,Nour Awad,Gregory R. Dion,Laura J. Brattain*

Main category: cs.CV

TL;DR: 论文提出了一种结合文本到图像潜在扩散模型和类感知采样的混合方法（T2ID-CAS），用于解决颈部超声中类不平衡问题，显著提升了目标检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 颈部超声在气道管理中至关重要，但数据集中关键结构（如气管环和声带）的类不平衡问题影响了目标检测模型的性能。

Method: 采用T2ID-CAS方法，结合文本到图像潜在扩散模型和类感知采样，生成高质量的合成样本以增强少数类的表示。

Result: 实验结果显示，T2ID-CAS在YOLOv9模型中实现了88.2的平均精度，显著优于基线模型的66。

Conclusion: T2ID-CAS是一种计算高效且可扩展的解决方案，能够有效缓解AI辅助超声引导干预中的类不平衡问题。

Abstract: Neck ultrasound (US) plays a vital role in airway management by providing
non-invasive, real-time imaging that enables rapid and precise interventions.
Deep learning-based anatomical landmark detection in neck US can further
facilitate procedural efficiency. However, class imbalance within datasets,
where key structures like tracheal rings and vocal folds are underrepresented,
presents significant challenges for object detection models. To address this,
we propose T2ID-CAS, a hybrid approach that combines a text-to-image latent
diffusion model with class-aware sampling to generate high-quality synthetic
samples for underrepresented classes. This approach, rarely explored in the
ultrasound domain, improves the representation of minority classes.
Experimental results using YOLOv9 for anatomical landmark detection in neck US
demonstrated that T2ID-CAS achieved a mean Average Precision of 88.2,
significantly surpassing the baseline of 66. This highlights its potential as a
computationally efficient and scalable solution for mitigating class imbalance
in AI-assisted ultrasound-guided interventions.

</details>


### [56] [Subject Information Extraction for Novelty Detection with Domain Shifts](https://arxiv.org/abs/2504.21247)
*Yangyang Qu,Dazhi Fu,Jicong Fan*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过分离主体信息和背景变化来提升无监督新颖性检测在域偏移情况下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设训练和测试数据来自同一域，忽略了域偏移问题，导致正常数据被误分类为新颖。

Method: 提出一种方法，最小化主体和背景表示之间的互信息，并用深度高斯混合模型建模背景变化，仅基于主体表示进行新颖性检测。

Result: 实验表明，该方法在未见域上表现优异，显著优于基线方法，尤其在域偏移较大时。

Conclusion: 该方法有效解决了域偏移问题，提升了新颖性检测的性能。

Abstract: Unsupervised novelty detection (UND), aimed at identifying novel samples, is
essential in fields like medical diagnosis, cybersecurity, and industrial
quality control. Most existing UND methods assume that the training data and
testing normal data originate from the same domain and only consider the
distribution variation between training data and testing data. However, in real
scenarios, it is common for normal testing and training data to originate from
different domains, a challenge known as domain shift. The discrepancies between
training and testing data often lead to incorrect classification of normal data
as novel by existing methods. A typical situation is that testing normal data
and training data describe the same subject, yet they differ in the background
conditions. To address this problem, we introduce a novel method that separates
subject information from background variation encapsulating the domain
information to enhance detection performance under domain shifts. The proposed
method minimizes the mutual information between the representations of the
subject and background while modelling the background variation using a deep
Gaussian mixture model, where the novelty detection is conducted on the subject
representations solely and hence is not affected by the variation of domains.
Extensive experiments demonstrate that our model generalizes effectively to
unseen domains and significantly outperforms baseline methods, especially under
substantial domain shifts between training and testing data.

</details>


### [57] [Multi-modal Transfer Learning for Dynamic Facial Emotion Recognition in the Wild](https://arxiv.org/abs/2504.21248)
*Ezra Engel,Lishan Li,Chris Hudy,Robert Schleusner*

Main category: cs.CV

TL;DR: 本文探讨了多模态迁移学习在视频面部表情识别（FER）中的应用，使用预训练网络组合提升DFEW数据集的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别在多个领域有重要应用，但由于面部特征的细微变化，准确分类具有挑战性。

Method: 结合预训练的ResNets、OpenPose和OmniVec网络，研究跨时间多模态特征对分类准确率的影响。

Result: 多模态特征生成器略微提升了基于Transformer的分类模型的准确率。

Conclusion: 多模态迁移学习对提升FER性能具有潜在价值。

Abstract: Facial expression recognition (FER) is a subset of computer vision with
important applications for human-computer-interaction, healthcare, and customer
service. FER represents a challenging problem-space because accurate
classification requires a model to differentiate between subtle changes in
facial features. In this paper, we examine the use of multi-modal transfer
learning to improve performance on a challenging video-based FER dataset,
Dynamic Facial Expression in-the-Wild (DFEW). Using a combination of pretrained
ResNets, OpenPose, and OmniVec networks, we explore the impact of
cross-temporal, multi-modal features on classification accuracy. Ultimately, we
find that these finely-tuned multi-modal feature generators modestly improve
accuracy of our transformer-based classification model.

</details>


### [58] [Embracing Collaboration Over Competition: Condensing Multiple Prompts for Visual In-Context Learning](https://arxiv.org/abs/2504.21263)
*Jinpeng Wang,Tianci Luo,Yaohua Zha,Yan Feng,Ruisheng Luo,Bin Chen,Tao Dai,Long Chen,Yaowei Wang,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 论文提出了一种名为Condenser的轻量级外部插件，通过多提示协作压缩细粒度上下文，解决了视觉上下文学习（VICL）中提示选择的问题。


<details>
  <summary>Details</summary>
Motivation: 当前VICL方法假设存在单一理想提示，但实践中可能存在多个合适提示，单独使用时效果不佳，导致选择困难和有用上下文的遗漏。

Method: 提出提示压缩（prompt condensation）方法，设计Condenser插件，通过多提示协作高效整合上下文信息，保持分辨率。

Result: 实验表明Condenser在基准任务中优于现有方法，具有更好的上下文压缩能力、可扩展性和计算效率。

Conclusion: Condenser为VICL提供了一种高效且竞争力强的解决方案，代码已开源。

Abstract: Visual In-Context Learning (VICL) enables adaptively solving vision tasks by
leveraging pixel demonstrations, mimicking human-like task completion through
analogy. Prompt selection is critical in VICL, but current methods assume the
existence of a single "ideal" prompt in a pool of candidates, which in practice
may not hold true. Multiple suitable prompts may exist, but individually they
often fall short, leading to difficulties in selection and the exclusion of
useful context. To address this, we propose a new perspective: prompt
condensation. Rather than relying on a single prompt, candidate prompts
collaborate to efficiently integrate informative contexts without sacrificing
resolution. We devise Condenser, a lightweight external plugin that compresses
relevant fine-grained context across multiple prompts. Optimized end-to-end
with the backbone, Condenser ensures accurate integration of contextual cues.
Experiments demonstrate Condenser outperforms state-of-the-arts across
benchmark tasks, showing superior context compression, scalability with more
prompts, and enhanced computational efficiency compared to ensemble methods,
positioning it as a highly competitive solution for VICL. Code is open-sourced
at https://github.com/gimpong/CVPR25-Condenser.

</details>


### [59] [CoCoDiff: Diversifying Skeleton Action Features via Coarse-Fine Text-Co-Guided Latent Diffusion](https://arxiv.org/abs/2504.21266)
*Zhifu Zhao,Hanyang Hua,Jianan Li,Shaoxin Wu,Fu Li,Yangtao Zhou,Yang Li*

Main category: cs.CV

TL;DR: CoCoDiff通过潜空间扩散模型和多粒度文本引导生成多样且语义一致的特征，提升动作识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过扩展样本空间提升特征多样性，但效率低且语义不一致。

Method: CoCoDiff结合潜空间扩散模型和粗-细粒度文本引导，生成多样且语义一致的动作特征。

Result: 在NTU RGB+D等基准测试中达到SOTA性能。

Conclusion: CoCoDiff作为即插即用模块，高效提升动作识别性能。

Abstract: In action recognition tasks, feature diversity is essential for enhancing
model generalization and performance. Existing methods typically promote
feature diversity by expanding the training data in the sample space, which
often leads to inefficiencies and semantic inconsistencies. To overcome these
problems, we propose a novel Coarse-fine text co-guidance Diffusion model
(CoCoDiff). CoCoDiff generates diverse yet semantically consistent features in
the latent space by leveraging diffusion and multi-granularity textual
guidance. Specifically, our approach feeds spatio-temporal features extracted
from skeleton sequences into a latent diffusion model to generate diverse
action representations. Meanwhile, we introduce a coarse-fine text co-guided
strategy that leverages textual information from large language models (LLMs)
to ensure semantic consistency between the generated features and the original
inputs. It is noted that CoCoDiff operates as a plug-and-play auxiliary module
during training, incurring no additional inference cost. Extensive experiments
demonstrate that CoCoDiff achieves SOTA performance on skeleton-based action
recognition benchmarks, including NTU RGB+D, NTU RGB+D 120 and
Kinetics-Skeleton.

</details>


### [60] [Mamba Based Feature Extraction And Adaptive Multilevel Feature Fusion For 3D Tumor Segmentation From Multi-modal Medical Image](https://arxiv.org/abs/2504.21281)
*Zexin Ji,Beiji Zou,Xiaoyan Kui,Hua Li,Pierre Vera,Su Ruan*

Main category: cs.CV

TL;DR: 提出了一种基于Mamba的特征提取和自适应多级特征融合方法，用于多模态3D医学图像中的肿瘤分割，解决了传统CNN和Transformer方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 多模态3D医学图像分割面临图像强度和肿瘤形态变化的挑战，传统CNN难以捕捉全局特征，而Transformer计算成本高。Mamba模型结合了线性扩展性和长距离建模能力，但在多模态特征融合上仍有不足。

Method: 开发了特定模态的Mamba编码器提取长程相关特征，设计了双级协同集成块动态融合多模态和多级特征，解码器结合语义信息和细节生成分割图。

Result: 在PET/CT和MRI多序列数据集上的实验表明，该方法在性能上优于现有的CNN、Transformer和Mamba方法。

Conclusion: 该方法有效解决了多模态3D医学图像分割中的特征提取和融合问题，具有较高的实用性和竞争力。

Abstract: Multi-modal 3D medical image segmentation aims to accurately identify tumor
regions across different modalities, facing challenges from variations in image
intensity and tumor morphology. Traditional convolutional neural network
(CNN)-based methods struggle with capturing global features, while
Transformers-based methods, despite effectively capturing global context,
encounter high computational costs in 3D medical image segmentation. The Mamba
model combines linear scalability with long-distance modeling, making it a
promising approach for visual representation learning. However, Mamba-based 3D
multi-modal segmentation still struggles to leverage modality-specific features
and fuse complementary information effectively. In this paper, we propose a
Mamba based feature extraction and adaptive multilevel feature fusion for 3D
tumor segmentation using multi-modal medical image. We first develop the
specific modality Mamba encoder to efficiently extract long-range relevant
features that represent anatomical and pathological structures present in each
modality. Moreover, we design an bi-level synergistic integration block that
dynamically merges multi-modal and multi-level complementary features by the
modality attention and channel attention learning. Lastly, the decoder combines
deep semantic information with fine-grained details to generate the tumor
segmentation map. Experimental results on medical image datasets (PET/CT and
MRI multi-sequence) show that our approach achieve competitive performance
compared to the state-of-the-art CNN, Transformer, and Mamba-based approaches.

</details>


### [61] [Can We Achieve Efficient Diffusion without Self-Attention? Distilling Self-Attention into Convolutions](https://arxiv.org/abs/2504.21292)
*ZiYi Dong,Chengxing Zhou,Weijian Deng,Pengxu Wei,Xiangyang Ji,Liang Lin*

Main category: cs.CV

TL;DR: 论文提出ΔConvFusion，用金字塔卷积块替代自注意力模块，显著降低计算成本，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 研究发现预训练扩散模型中的自注意力主要呈现局部化模式，挑战了全局交互的必要性假设。

Method: 通过蒸馏注意力模式为局部卷积操作，提出ΔConvFusion，保留其他组件不变。

Result: ΔConvFusion计算成本降低6929倍，效率超过LinFusion 5.42倍，生成质量不降。

Conclusion: 局部卷积可替代自注意力，为高效图像生成提供新思路。

Abstract: Contemporary diffusion models built upon U-Net or Diffusion Transformer (DiT)
architectures have revolutionized image generation through transformer-based
attention mechanisms. The prevailing paradigm has commonly employed
self-attention with quadratic computational complexity to handle global spatial
relationships in complex images, thereby synthesizing high-fidelity images with
coherent visual semantics.Contrary to conventional wisdom, our systematic
layer-wise analysis reveals an interesting discrepancy: self-attention in
pre-trained diffusion models predominantly exhibits localized attention
patterns, closely resembling convolutional inductive biases. This suggests that
global interactions in self-attention may be less critical than commonly
assumed.Driven by this, we propose \(\Delta\)ConvFusion to replace conventional
self-attention modules with Pyramid Convolution Blocks
(\(\Delta\)ConvBlocks).By distilling attention patterns into localized
convolutional operations while keeping other components frozen,
\(\Delta\)ConvFusion achieves performance comparable to transformer-based
counterparts while reducing computational cost by 6929$\times$ and surpassing
LinFusion by 5.42$\times$ in efficiency--all without compromising generative
fidelity.

</details>


### [62] [Learning Multi-view Multi-class Anomaly Detection](https://arxiv.org/abs/2504.21294)
*Qianzi Yu,Yang Cao,Yu Kang*

Main category: cs.CV

TL;DR: MVMCAD模型通过多视图整合和异常信号增强，在多视图多类异常检测中取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有MCAD模型在多视图场景中表现不佳，未能有效建模视图间关系和互补信息。

Method: 提出半冻结编码器、异常放大模块和跨特征损失，优化多视图特征建模和异常检测。

Result: 在Real-IAD数据集上，图像级和像素级检测分别达到91.0/88.6/82.1和99.1/43.9/48.2/95.2。

Conclusion: MVMCAD在多视图多类异常检测中表现卓越，验证了方法的有效性。

Abstract: The latest trend in anomaly detection is to train a unified model instead of
training a separate model for each category. However, existing multi-class
anomaly detection (MCAD) models perform poorly in multi-view scenarios because
they often fail to effectively model the relationships and complementary
information among different views. In this paper, we introduce a Multi-View
Multi-Class Anomaly Detection model (MVMCAD), which integrates information from
multiple views to accurately identify anomalies. Specifically, we propose a
semi-frozen encoder, where a pre-encoder prior enhancement mechanism is added
before the frozen encoder, enabling stable cross-view feature modeling and
efficient adaptation for improved anomaly detection. Furthermore, we propose an
Anomaly Amplification Module (AAM) that models global token interactions and
suppresses normal regions to enhance anomaly signals, leading to improved
detection performance in multi-view settings. Finally, we propose a
Cross-Feature Loss that aligns shallow encoder features with deep decoder
features and vice versa, enhancing the model's sensitivity to anomalies at
different semantic levels under multi-view scenarios. Extensive experiments on
the Real-IAD dataset for multi-view multi-class anomaly detection validate the
effectiveness of our approach, achieving state-of-the-art performance of
91.0/88.6/82.1 and 99.1/43.9/48.2/95.2 for image-level and the pixel-level,
respectively.

</details>


### [63] [CMD: Constraining Multimodal Distribution for Domain Adaptation in Stereo Matching](https://arxiv.org/abs/2504.21302)
*Zhelun Shen,Zhuo Li,Chenming Wu,Zhibo Rao,Lina Liu,Yuchao Dai,Liangjun Zhang*

Main category: cs.CV

TL;DR: 论文提出CMD方法，通过约束多模态分布提升无监督域适应场景下的立体匹配性能。


<details>
  <summary>Details</summary>
Motivation: 现有学习基立体匹配方法在无监督域适应场景中因多模态分布问题导致泛化性能下降。

Method: 引入不确定性正则化最小化和各向异性软argmin，鼓励网络生成单模态分布。

Result: 实验表明，CMD方法在多代表性立体匹配网络中提升了泛化性能。

Conclusion: CMD有效解决了无监督域适应中的多模态分布问题，提升了预测准确性。

Abstract: Recently, learning-based stereo matching methods have achieved great
improvement in public benchmarks, where soft argmin and smooth L1 loss play a
core contribution to their success. However, in unsupervised domain adaptation
scenarios, we observe that these two operations often yield multimodal
disparity probability distributions in target domains, resulting in degraded
generalization. In this paper, we propose a novel approach, Constrain
Multi-modal Distribution (CMD), to address this issue. Specifically, we
introduce \textit{uncertainty-regularized minimization} and \textit{anisotropic
soft argmin} to encourage the network to produce predominantly unimodal
disparity distributions in the target domain, thereby improving prediction
accuracy. Experimentally, we apply the proposed method to multiple
representative stereo-matching networks and conduct domain adaptation from
synthetic data to unlabeled real-world scenes. Results consistently demonstrate
improved generalization in both top-performing and domain-adaptable
stereo-matching models. The code for CMD will be available at:
\href{https://github.com/gallenszl/CMD}{https://github.com/gallenszl/CMD}.

</details>


### [64] [The Dual Power of Interpretable Token Embeddings: Jailbreaking Attacks and Defenses for Diffusion Model Unlearning](https://arxiv.org/abs/2504.21307)
*Siyi Chen,Yimeng Zhang,Sijia Liu,Qing Qu*

Main category: cs.CV

TL;DR: 论文提出了一种可解释的攻击方法，通过正交攻击令牌嵌入揭示未学习模型中仍保留有害概念的原因，并设计了一种防御方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型可能记忆并生成有害内容，现有微调方法易被绕过，且缺乏对未学习模型仍保留概念的解释。

Method: 提出学习正交可解释攻击令牌嵌入的方法，分解为可解释文本元素，并设计防御方法。

Result: 攻击令牌嵌入具有鲁棒性和可迁移性，防御方法对现有攻击也有效。

Conclusion: 攻击和防御策略均有效，为理解未学习模型行为提供了新视角。

Abstract: Despite the remarkable generalization capabilities of diffusion models,
recent studies have shown that these models can memorize and generate harmful
content when prompted with specific text instructions. Although fine-tuning
approaches have been developed to mitigate this issue by unlearning harmful
concepts, these methods can be easily circumvented through jailbreaking
attacks. This indicates that the harmful concept has not been fully erased from
the model. However, existing attack methods, while effective, lack
interpretability regarding why unlearned models still retain the concept,
thereby hindering the development of defense strategies. In this work, we
address these limitations by proposing an attack method that learns an
orthogonal set of interpretable attack token embeddings. The attack token
embeddings can be decomposed into human-interpretable textual elements,
revealing that unlearned models still retain the target concept through
implicit textual components. Furthermore, these attack token embeddings are
robust and transferable across text prompts, initial noises, and unlearned
models. Finally, leveraging this diverse set of embeddings, we design a defense
method applicable to both our proposed attack and existing attack methods.
Experimental results demonstrate the effectiveness of both our attack and
defense strategies.

</details>


### [65] [AGHI-QA: A Subjective-Aligned Dataset and Metric for AI-Generated Human Images](https://arxiv.org/abs/2504.21308)
*Yunhao Li,Sijing Wu,Wei Sun,Zhichao Zhang,Yucheng Zhu,Zicheng Zhang,Huiyu Duan,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 论文提出了AGHI-QA，首个针对AI生成人类图像（AGHIs）质量评估的大规模基准数据集，并开发了AGHI-Assessor，一种结合多模态模型和人体特征的新质量评估方法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像质量评估（IQA）方法仅提供全局质量评分，无法对结构复杂的人类图像进行细粒度评估，而AI生成的人类图像常存在解剖和纹理失真问题。

Method: 构建了包含4,000张图像的AGHI-QA数据集，通过主观研究收集多维度标注（如质量评分、文本-图像对应性、可见和失真身体部位）。提出AGHI-Assessor，结合多模态模型和人体特征进行质量预测。

Result: AGHI-Assessor在多维质量评估和结构失真检测中表现优异，显著优于现有IQA方法和领先的多模态模型。

Conclusion: AGHI-QA和AGHI-Assessor填补了人类图像质量评估的空白，为未来研究提供了重要工具。

Abstract: The rapid development of text-to-image (T2I) generation approaches has
attracted extensive interest in evaluating the quality of generated images,
leading to the development of various quality assessment methods for
general-purpose T2I outputs. However, existing image quality assessment (IQA)
methods are limited to providing global quality scores, failing to deliver
fine-grained perceptual evaluations for structurally complex subjects like
humans, which is a critical challenge considering the frequent anatomical and
textural distortions in AI-generated human images (AGHIs). To address this gap,
we introduce AGHI-QA, the first large-scale benchmark specifically designed for
quality assessment of AGHIs. The dataset comprises 4,000 images generated from
400 carefully crafted text prompts using 10 state of-the-art T2I models. We
conduct a systematic subjective study to collect multidimensional annotations,
including perceptual quality scores, text-image correspondence scores, visible
and distorted body part labels. Based on AGHI-QA, we evaluate the strengths and
weaknesses of current T2I methods in generating human images from multiple
dimensions. Furthermore, we propose AGHI-Assessor, a novel quality metric that
integrates the large multimodal model (LMM) with domain-specific human features
for precise quality prediction and identification of visible and distorted body
parts in AGHIs. Extensive experimental results demonstrate that AGHI-Assessor
showcases state-of-the-art performance, significantly outperforming existing
IQA methods in multidimensional quality assessment and surpassing leading LMMs
in detecting structural distortions in AGHIs.

</details>


### [66] [An Evaluation of a Visual Question Answering Strategy for Zero-shot Facial Expression Recognition in Still Images](https://arxiv.org/abs/2504.21309)
*Modesto Castrillón-Santana,Oliverio J Santana,David Freire-Obregón,Daniel Hernández-Sosa,Javier Lorenzo-Navarro*

Main category: cs.CV

TL;DR: 论文探讨了零样本面部表情识别（FER）的挑战，提出了一种结合视觉语言模型（VLM）的解决方案，并在多个基准数据集上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在FER领域取得进展，但在零样本场景下性能显著下降，因此需要探索新的方法（如结合VLM）以提升泛化能力。

Method: 采用视觉问答策略，评估了多种本地执行的VLM，并与现有FER模型进行比较。

Result: 部分VLM在零样本FER场景中表现优异，表明其在提升泛化能力方面的潜力。

Conclusion: 需要进一步研究VLM在FER中的应用，以改善模型在新场景中的表现。

Abstract: Facial expression recognition (FER) is a key research area in computer vision
and human-computer interaction. Despite recent advances in deep learning,
challenges persist, especially in generalizing to new scenarios. In fact,
zero-shot FER significantly reduces the performance of state-of-the-art FER
models. To address this problem, the community has recently started to explore
the integration of knowledge from Large Language Models for visual tasks. In
this work, we evaluate a broad collection of locally executed Visual Language
Models (VLMs), avoiding the lack of task-specific knowledge by adopting a
Visual Question Answering strategy. We compare the proposed pipeline with
state-of-the-art FER models, both integrating and excluding VLMs, evaluating
well-known FER benchmarks: AffectNet, FERPlus, and RAF-DB. The results show
excellent performance for some VLMs in zero-shot FER scenarios, indicating the
need for further exploration to improve FER generalization.

</details>


### [67] [Text-Conditioned Diffusion Model for High-Fidelity Korean Font Generation](https://arxiv.org/abs/2504.21325)
*Abdul Sami,Avinash Kumar,Irfanullah Memon,Youngwon Jo,Muhammad Rizwan,Jaeyoung Choi*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的自动字体生成方法，用于高质量生成韩文字体，仅需单一样本图像，解决了传统方法的不稳定性和细节捕捉问题。


<details>
  <summary>Details</summary>
Motivation: 传统AFG方法（如GANs和VAEs）在训练中不稳定且易出现模式崩溃，难以捕捉字体细节，尤其在复杂语言（如韩文和中文）中表现不佳。

Method: 采用扩散模型逐步细化噪声图像，结合文本编码器处理音标表示，使用预训练风格编码器和感知损失提升生成质量。

Result: 在2000多个韩文字符上测试，模型能稳定生成高质量、多样化的字体图像，优于基准方法。

Conclusion: 该方法为生成真实韩文字体提供了可靠工具，适用于手写和印刷风格。

Abstract: Automatic font generation (AFG) is the process of creating a new font using
only a few examples of the style images. Generating fonts for complex languages
like Korean and Chinese, particularly in handwritten styles, presents
significant challenges. Traditional AFGs, like Generative adversarial networks
(GANs) and Variational Auto-Encoders (VAEs), are usually unstable during
training and often face mode collapse problems. They also struggle to capture
fine details within font images. To address these problems, we present a
diffusion-based AFG method which generates high-quality, diverse Korean font
images using only a single reference image, focusing on handwritten and printed
styles. Our approach refines noisy images incrementally, ensuring stable
training and visually appealing results. A key innovation is our text encoder,
which processes phonetic representations to generate accurate and contextually
correct characters, even for unseen characters. We used a pre-trained style
encoder from DG FONT to effectively and accurately encode the style images. To
further enhance the generation quality, we used perceptual loss that guides the
model to focus on the global style of generated images. Experimental results on
over 2000 Korean characters demonstrate that our model consistently generates
accurate and detailed font images and outperforms benchmark methods, making it
a reliable tool for generating authentic Korean fonts across different styles.

</details>


### [68] [Simple Visual Artifact Detection in Sora-Generated Videos](https://arxiv.org/abs/2504.21334)
*Misora Sugiyama,Hirokatsu Kataoka*

Main category: cs.CV

TL;DR: OpenAI的Sora视频生成模型存在视觉伪影问题，研究提出多标签分类框架评估四种常见伪影类型，使用ResNet-50模型达到94.14%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成模型的发展，理解其局限性并确保安全部署变得至关重要，尤其是视觉伪影可能影响质量或传播虚假信息。

Method: 研究使用多标签分类框架，基于300个手动标注的Sora生成视频帧，训练多种2D CNN架构（如ResNet-50、EfficientNet-B3/B4、ViT-Base）。

Result: ResNet-50模型表现最佳，多标签分类平均准确率达94.14%。

Conclusion: 该研究为视频质量评估、视觉风险识别及安全部署提供了数据集和分析方法。

Abstract: The December 2024 release of OpenAI's Sora, a powerful video generation model
driven by natural language prompts, highlights a growing convergence between
large language models (LLMs) and video synthesis. As these multimodal systems
evolve into video-enabled LLMs (VidLLMs), capable of interpreting, generating,
and interacting with visual content, understanding their limitations and
ensuring their safe deployment becomes essential. This study investigates
visual artifacts frequently found and reported in Sora-generated videos, which
can compromise quality, mislead viewers, or propagate disinformation. We
propose a multi-label classification framework targeting four common artifact
label types: label 1: boundary / edge defects, label 2: texture / noise issues,
label 3: movement / joint anomalies, and label 4: object mismatches /
disappearances. Using a dataset of 300 manually annotated frames extracted from
15 Sora-generated videos, we trained multiple 2D CNN architectures (ResNet-50,
EfficientNet-B3 / B4, ViT-Base). The best-performing model trained by ResNet-50
achieved an average multi-label classification accuracy of 94.14%. This work
supports the broader development of VidLLMs by contributing to (1) the creation
of datasets for video quality evaluation, (2) interpretable artifact-based
analysis beyond language metrics, and (3) the identification of visual risks
relevant to factuality and safety.

</details>


### [69] [UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation](https://arxiv.org/abs/2504.21336)
*Linshan Wu,Yuxiang Nie,Sunan He,Jiaxin Zhuang,Hao Chen*

Main category: cs.CV

TL;DR: UniBiomed是一种新型的多模态生物医学图像基础模型，结合了多模态大语言模型（MLLM）和Segment Anything Model（SAM），能够统一生成临床文本和分割生物医学对象，实现端到端的自动化解释。


<details>
  <summary>Details</summary>
Motivation: 传统AI方法在生物医学图像分析中依赖分离的训练模型（如LLM和分割模型），导致实际部署不灵活且无法利用整体生物医学信息。UniBiomed旨在解决这一问题。

Method: UniBiomed通过整合MLLM和SAM，开发了一个大规模数据集（包含2700万张图像、标注和文本描述），并在84个数据集上验证其性能。

Result: UniBiomed在分割、疾病识别、区域感知诊断、视觉问答和报告生成等任务中达到最先进水平，且无需人工干预。

Conclusion: UniBiomed代表了生物医学AI的新突破，显著提高了诊断效率，为生物医学图像分析提供了更准确和高效的解决方案。

Abstract: Multi-modal interpretation of biomedical images opens up novel opportunities
in biomedical image analysis. Conventional AI approaches typically rely on
disjointed training, i.e., Large Language Models (LLMs) for clinical text
generation and segmentation models for target extraction, which results in
inflexible real-world deployment and a failure to leverage holistic biomedical
information. To this end, we introduce UniBiomed, the first universal
foundation model for grounded biomedical image interpretation. UniBiomed is
based on a novel integration of Multi-modal Large Language Model (MLLM) and
Segment Anything Model (SAM), which effectively unifies the generation of
clinical texts and the segmentation of corresponding biomedical objects for
grounded interpretation. In this way, UniBiomed is capable of tackling a wide
range of biomedical tasks across ten diverse biomedical imaging modalities. To
develop UniBiomed, we curate a large-scale dataset comprising over 27 million
triplets of images, annotations, and text descriptions across ten imaging
modalities. Extensive validation on 84 internal and external datasets
demonstrated that UniBiomed achieves state-of-the-art performance in
segmentation, disease recognition, region-aware diagnosis, visual question
answering, and report generation. Moreover, unlike previous models that rely on
clinical experts to pre-diagnose images and manually craft precise textual or
visual prompts, UniBiomed can provide automated and end-to-end grounded
interpretation for biomedical image analysis. This represents a novel paradigm
shift in clinical workflows, which will significantly improve diagnostic
efficiency. In summary, UniBiomed represents a novel breakthrough in biomedical
AI, unlocking powerful grounded interpretation capabilities for more accurate
and efficient biomedical image analysis.

</details>


### [70] [Towards Improved Cervical Cancer Screening: Vision Transformer-Based Classification and Interpretability](https://arxiv.org/abs/2504.21340)
*Khoa Tuan Nguyen,Ho-min Park,Gaeun Oh,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

TL;DR: 提出了一种基于EVA-02变换器模型的宫颈细胞图像分类新方法，通过四步流程优化模型性能，F1分数达到0.85227，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 改进宫颈癌筛查中的细胞图像分类性能，提供更准确的诊断工具。

Method: 四步流程：微调EVA-02、特征提取、多模型特征选择、训练新神经网络（可选损失加权）。

Result: 最佳模型F1分数0.85227，优于基线（0.84878）；通过Kernel SHAP分析提供可解释性。

Conclusion: 该方法在性能和可解释性上均优于基线，为宫颈癌筛查提供了有效工具。

Abstract: We propose a novel approach to cervical cell image classification for
cervical cancer screening using the EVA-02 transformer model. We developed a
four-step pipeline: fine-tuning EVA-02, feature extraction, selecting important
features through multiple machine learning models, and training a new
artificial neural network with optional loss weighting for improved
generalization. With this design, our best model achieved an F1-score of
0.85227, outperforming the baseline EVA-02 model (0.84878). We also utilized
Kernel SHAP analysis and identified key features correlating with cell
morphology and staining characteristics, providing interpretable insights into
the decision-making process of the fine-tuned model. Our code is available at
https://github.com/Khoa-NT/isbi2025_ps3c.

</details>


### [71] [Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early Lung Cancer Detection](https://arxiv.org/abs/2504.21344)
*Luoting Zhuang,Seyed Mohammad Hossein Tabatabaei,Ramin Salehi-Rad,Linh M. Tran,Denise R. Aberle,Ashley E. Prosper,William Hsu*

Main category: cs.CV

TL;DR: 该研究提出了一种结合放射科医生评估的语义特征和深度学习的方法，用于预测肺癌，提高了模型的解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型依赖手动标注、解释性差且对成像变化敏感，限制了临床应用。本研究旨在整合语义特征，学习临床相关且可解释的特征。

Method: 使用多个CT扫描数据集，通过参数高效的微调方法优化预训练的CLIP模型，对齐成像和语义特征，预测一年内肺癌诊断。

Result: 模型在外部数据集上表现优异（AUROC:0.90，AUPRC:0.78），并能预测语义特征（如结节边缘、一致性等），提供可解释的输出。

Conclusion: 该方法能准确分类肺结节，提供可解释的预测结果，帮助临床医生理解模型决策，并具有跨临床环境的泛化能力。

Abstract: Objective: A number of machine learning models have utilized semantic
features, deep features, or both to assess lung nodule malignancy. However,
their reliance on manual annotation during inference, limited interpretability,
and sensitivity to imaging variations hinder their application in real-world
clinical settings. Thus, this research aims to integrate semantic features
derived from radiologists' assessments of nodules, allowing the model to learn
clinically relevant, robust, and explainable features for predicting lung
cancer. Methods: We obtained 938 low-dose CT scans from the National Lung
Screening Trial with 1,246 nodules and semantic features. The Lung Image
Database Consortium dataset contains 1,018 CT scans, with 2,625 lesions
annotated for nodule characteristics. Three external datasets were obtained
from UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We
finetuned a pretrained Contrastive Language-Image Pretraining model with a
parameter-efficient fine-tuning approach to align imaging and semantic features
and predict the one-year lung cancer diagnosis. Results: We evaluated the
performance of the one-year diagnosis of lung cancer with AUROC and AUPRC and
compared it to three state-of-the-art models. Our model demonstrated an AUROC
of 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on
external datasets. Using CLIP, we also obtained predictions on semantic
features, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and
pleural attachment (0.84), that can be used to explain model predictions.
Conclusion: Our approach accurately classifies lung nodules as benign or
malignant, providing explainable outputs, aiding clinicians in comprehending
the underlying meaning of model predictions. This approach also prevents the
model from learning shortcuts and generalizes across clinical settings.

</details>


### [72] [Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing](https://arxiv.org/abs/2504.21356)
*Hong Zhang,Zhongjie Duan,Xingjun Wang,Yingda Chen,Yuze Zhao,Yu Zhang*

Main category: cs.CV

TL;DR: Nexus-Gen是一个统一的多模态大语言模型，通过结合LLM的语言推理能力和扩散模型的图像合成能力，解决了现有开源统一模型与领域专用架构之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有开源统一模型在多模态理解和生成能力上表现不如领域专用架构，因此需要一种更高效的统一模型。

Method: 采用双阶段对齐训练：LLM学习预测图像嵌入，视觉解码器重建高保真图像；引入预填充自回归策略以避免误差累积。

Result: Nexus-Gen能够全面处理图像理解、生成和编辑任务。

Conclusion: Nexus-Gen通过双阶段训练和预填充策略，成功整合了多模态能力，并开源了模型和代码以推动领域发展。

Abstract: Unified multimodal large language models (MLLMs) aim to integrate multimodal
understanding and generation abilities through a single framework. Despite
their versatility, existing open-source unified models exhibit performance gaps
against domain-specific architectures. To bridge this gap, we present
Nexus-Gen, a unified model that synergizes the language reasoning capabilities
of LLMs with the image synthesis power of diffusion models. To align the
embedding space of the LLM and diffusion model, we conduct a dual-phase
alignment training process. (1) The autoregressive LLM learns to predict image
embeddings conditioned on multimodal inputs, while (2) the vision decoder is
trained to reconstruct high-fidelity images from these embeddings. During
training the LLM, we identified a critical discrepancy between the
autoregressive paradigm's training and inference phases, where error
accumulation in continuous embedding space severely degrades generation
quality. To avoid this issue, we introduce a prefilled autoregression strategy
that prefills input sequence with position-embedded special tokens instead of
continuous embeddings. Through dual-phase training, Nexus-Gen has developed the
integrated capability to comprehensively address the image understanding,
generation and editing tasks. All models, datasets, and codes are published at
https://github.com/modelscope/Nexus-Gen.git to facilitate further advancements
across the field.

</details>


### [73] [Revisiting Diffusion Autoencoder Training for Image Reconstruction Quality](https://arxiv.org/abs/2504.21368)
*Pramook Khungurn,Sukit Seripanitkarn,Phonphrm Thawatdamrongkit,Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: 提出一种新的扩散自编码器（DAE）训练方法，通过分阶段训练优化图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统DAE使用线性噪声计划，导致图像模糊，因高噪声阶段过长。利用潜在编码已含结构信息的特点，优化细节重建。

Method: 分两阶段训练：第一阶段强制最高噪声训练，使潜在编码包含结构信息；第二阶段调整噪声计划，侧重低噪声优化细节。

Result: 生成图像在结构和细节上均更准确，同时保留潜在编码的有用特性。

Conclusion: 新方法显著提升DAE的图像重建质量，平衡结构与细节。

Abstract: Diffusion autoencoders (DAEs) are typically formulated as a noise prediction
model and trained with a linear-$\beta$ noise schedule that spends much of its
sampling steps at high noise levels. Because high noise levels are associated
with recovering large-scale image structures and low noise levels with
recovering details, this configuration can result in low-quality and blurry
images. However, it should be possible to improve details while spending fewer
steps recovering structures because the latent code should already contain
structural information. Based on this insight, we propose a new DAE training
method that improves the quality of reconstructed images. We divide training
into two phases. In the first phase, the DAE is trained as a vanilla
autoencoder by always setting the noise level to the highest, forcing the
encoder and decoder to populate the latent code with structural information. In
the second phase, we incorporate a noise schedule that spends more time in the
low-noise region, allowing the DAE to learn how to perfect the details. Our
method results in images that have accurate high-level structures and low-level
details while still preserving useful properties of the latent codes.

</details>


### [74] [IDDM: Bridging Synthetic-to-Real Domain Gap from Physics-Guided Diffusion for Real-world Image Dehazing](https://arxiv.org/abs/2504.21385)
*Shijun Zhou,Yajing Liu,Chunhui Hao,Zhiyuan Liu,Jiandong Tian*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的图像去雾方法IDDM，通过结合大气散射模型和噪声扩散，解决了合成数据与真实场景之间的域差距问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于合成数据训练的去雾算法在真实场景中泛化能力不足，需要一种能有效桥接域差距的方法。

Method: IDDM利用扩散过程逐步引入雾和噪声，再通过去噪Unet学习清晰图像的分布，结合大气散射模型提供物理指导。

Result: IDDM在合成数据训练下，能有效恢复真实世界的雾化图像，实验验证了其优越性能。

Conclusion: IDDM通过物理引导的扩散模型，成功实现了域泛化，为真实场景去雾提供了新思路。

Abstract: Due to the domain gap between real-world and synthetic hazy images, current
data-driven dehazing algorithms trained on synthetic datasets perform well on
synthetic data but struggle to generalize to real-world scenarios. To address
this challenge, we propose \textbf{I}mage \textbf{D}ehazing \textbf{D}iffusion
\textbf{M}odels (IDDM), a novel diffusion process that incorporates the
atmospheric scattering model into noise diffusion. IDDM aims to use the gradual
haze formation process to help the denoising Unet robustly learn the
distribution of clear images from the conditional input hazy images. We design
a specialized training strategy centered around IDDM. Diffusion models are
leveraged to bridge the domain gap from synthetic to real-world, while the
atmospheric scattering model provides physical guidance for haze formation.
During the forward process, IDDM simultaneously introduces haze and noise into
clear images, and then robustly separates them during the sampling process. By
training with physics-guided information, IDDM shows the ability of domain
generalization, and effectively restores the real-world hazy images despite
being trained on synthetic datasets. Extensive experiments demonstrate the
effectiveness of our method through both quantitative and qualitative
comparisons with state-of-the-art approaches.

</details>


### [75] [Comparison of Different Deep Neural Network Models in the Cultural Heritage Domain](https://arxiv.org/abs/2504.21387)
*Teodor Boyadzhiev,Gabriele Lagani,Luca Ciampi,Giuseppe Amato,Krassimira Ivanova*

Main category: cs.CV

TL;DR: 比较了卷积神经网络和Transformer架构在文化遗产任务中的表现，发现DenseNet在效率与计算性方面最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨计算机视觉和深度学习在文化遗产保护与游客体验中的应用，比较两种主流技术的知识迁移能力。

Method: 测试了VGG、ResNet、DenseNet、Visual Transformer、Swin Transformer和PoolFormer等架构在文化遗产任务中的表现。

Result: DenseNet在效率与计算性方面表现最佳。

Conclusion: DenseNet是文化遗产任务中最优的深度学习架构选择。

Abstract: The integration of computer vision and deep learning is an essential part of
documenting and preserving cultural heritage, as well as improving visitor
experiences. In recent years, two deep learning paradigms have been established
in the field of computer vision: convolutional neural networks and transformer
architectures. The present study aims to make a comparative analysis of some
representatives of these two techniques of their ability to transfer knowledge
from generic dataset, such as ImageNet, to cultural heritage specific tasks.
The results of testing examples of the architectures VGG, ResNet, DenseNet,
Visual Transformer, Swin Transformer, and PoolFormer, showed that DenseNet is
the best in terms of efficiency-computability ratio.

</details>


### [76] [Static or Dynamic: Towards Query-Adaptive Token Selection for Video Question Answering](https://arxiv.org/abs/2504.21403)
*Yumeng Shi,Quanyu Long,Wenya Wang*

Main category: cs.CV

TL;DR: 提出了一种名为EXPLORE-THEN-SELECT的令牌选择策略，通过动态调整静态和动态信息的需求，优化视频问答中的令牌使用效率。


<details>
  <summary>Details</summary>
Motivation: 解决视频问答中因长视频生成大量令牌导致的内存效率低下和模型性能问题。

Method: 提出EXPLORE-THEN-SELECT策略，先探索静态和动态令牌分配，再基于查询感知的注意力指标选择最优组合。

Result: 在多个视频问答基准测试中性能提升高达5.8%。

Conclusion: 该框架无需模型更新即可高效优化令牌使用，适用于多种视频语言模型。

Abstract: Video question answering benefits from the rich information available in
videos, enabling a wide range of applications. However, the large volume of
tokens generated from longer videos presents significant challenges to memory
efficiency and model performance. To alleviate this issue, existing works
propose to compress video inputs, but usually overlooking the varying
importance of static and dynamic information across different queries, leading
to inefficient token usage within limited budgets. To tackle this, we propose a
novel token selection strategy, EXPLORE-THEN-SELECT, that adaptively adjust
static and dynamic information needed based on question requirements. Our
framework first explores different token allocations between static frames,
which preserve spatial details, and dynamic frames, which capture temporal
changes. Next, it employs a query-aware attention-based metric to select the
optimal token combination without model updates. Our proposed framework is
plug-and-play that can be seamlessly integrated within diverse video-language
models. Extensive experiments show that our method achieves significant
performance improvements (up to 5.8%) among various video question answering
benchmarks.

</details>


### [77] [Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining](https://arxiv.org/abs/2504.21414)
*Qi Fan,Kaiqi Liu,Nian Liu,Hisham Cholakkal,Rao Muhammad Anwer,Wenbin Li,Yang Gao*

Main category: cs.CV

TL;DR: 提出了一种无需重新训练的方法（ISA），通过自适应调整模型结构来解决跨域小样本分割问题。


<details>
  <summary>Details</summary>
Motivation: 跨域小样本分割（CD-FSS）面临目标域多样性和支持数据有限的挑战，现有方法需重新训练，成本高。

Method: 通过结构Fisher评分自适应识别域特定模型结构，并分层训练选定的结构。

Result: 实验证明ISA方法在多个CD-FSS基准上表现优异。

Conclusion: ISA方法有效解决了域偏移问题，无需重新训练或重新设计模型。

Abstract: Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel
classes in new domains, which is often challenging due to the diverse
characteristics of target domains and the limited availability of support data.
Most CD-FSS methods redesign and retrain in-domain FSS models using various
domain-generalization techniques, which are effective but costly to train. To
address these issues, we propose adapting informative model structures of the
well-trained FSS model for target domains by learning domain characteristics
from few-shot labeled support samples during inference, thereby eliminating the
need for retraining. Specifically, we first adaptively identify domain-specific
model structures by measuring parameter importance using a novel structure
Fisher score in a data-dependent manner. Then, we progressively train the
selected informative model structures with hierarchically constructed training
samples, progressing from fewer to more support shots. The resulting
Informative Structure Adaptation (ISA) method effectively addresses domain
shifts and equips existing well-trained in-domain FSS models with flexible
adaptation capabilities for new domains, eliminating the need to redesign or
retrain CD-FSS models on base data. Extensive experiments validate the
effectiveness of our method, demonstrating superior performance across multiple
CD-FSS benchmarks.

</details>


### [78] [Diff-Prompt: Diffusion-Driven Prompt Generator with Mask Supervision](https://arxiv.org/abs/2504.21423)
*Weicai Yan,Wang Lin,Zirun Guo,Ye Wang,Fangming Feng,Xiaoda Yang,Zehan Wang,Tao Jin*

Main category: cs.CV

TL;DR: 论文提出Diff-Prompt，利用扩散模型生成丰富且细粒度的提示信息，以提升复杂下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法在复杂细粒度任务中性能提升有限，因其直接优化提示生成参数，限制了提示表示的丰富性和特异性。

Method: 方法分为三阶段：1) 训练Mask-VAE压缩掩码到潜空间；2) 用改进的DiT训练潜空间提示生成器；3) 在语义空间对齐生成器与预训练模型，并用生成提示微调模型。

Result: 在引用表达式理解任务中，Diff-Prompt在R@1和R@5上分别提升8.87和14.05，优于其他方法。

Conclusion: 实验验证了方法的有效性，展示了生成模型在提示生成中的潜力。

Abstract: Prompt learning has demonstrated promising results in fine-tuning pre-trained
multimodal models. However, the performance improvement is limited when applied
to more complex and fine-grained tasks. The reason is that most existing
methods directly optimize the parameters involved in the prompt generation
process through loss backpropagation, which constrains the richness and
specificity of the prompt representations. In this paper, we propose
Diffusion-Driven Prompt Generator (Diff-Prompt), aiming to use the diffusion
model to generate rich and fine-grained prompt information for complex
downstream tasks. Specifically, our approach consists of three stages. In the
first stage, we train a Mask-VAE to compress the masks into latent space. In
the second stage, we leverage an improved Diffusion Transformer (DiT) to train
a prompt generator in the latent space, using the masks for supervision. In the
third stage, we align the denoising process of the prompt generator with the
pre-trained model in the semantic space, and use the generated prompts to
fine-tune the model. We conduct experiments on a complex pixel-level downstream
task, referring expression comprehension, and compare our method with various
parameter-efficient fine-tuning approaches. Diff-Prompt achieves a maximum
improvement of 8.87 in R@1 and 14.05 in R@5 compared to the foundation model
and also outperforms other state-of-the-art methods across multiple metrics.
The experimental results validate the effectiveness of our approach and
highlight the potential of using generative models for prompt generation. Code
is available at https://github.com/Kelvin-ywc/diff-prompt.

</details>


### [79] [SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding](https://arxiv.org/abs/2504.21435)
*Chenkai Zhang,Yiming Lei,Zeming Liu,Haitao Leng,ShaoGuo Liu,Tingting Gao,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: 论文提出了SeriesBench基准和PC-DCoT框架，用于评估和改进多模态大语言模型对叙事驱动视频的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注独立视频的视觉元素，而现实中的视频多为连续叙事，需要更深入的叙事理解能力。

Method: 通过精选剧集、长跨度叙事标注和全信息转换方法构建SeriesBench，并提出PC-DCoT框架增强模型对情节和角色关系的分析能力。

Result: 实验表明现有模型在叙事理解上仍有挑战，而PC-DCoT能显著提升性能。

Conclusion: SeriesBench和PC-DCoT强调了提升模型叙事理解能力的必要性，为未来MLLM发展提供了方向。

Abstract: With the rapid development of Multi-modal Large Language Models (MLLMs), an
increasing number of benchmarks have been established to evaluate the video
understanding capabilities of these models. However, these benchmarks focus on
\textbf{standalone} videos and mainly assess ``visual elements'' like human
actions and object states. In reality, contemporary videos often encompass
complex and continuous narratives, typically presented as a \textbf{series}. To
address this challenge, we propose \textbf{SeriesBench}, a benchmark consisting
of 105 carefully curated narrative-driven series, covering 28 specialized tasks
that require deep narrative understanding. Specifically, we first select a
diverse set of drama series spanning various genres. Then, we introduce a novel
long-span narrative annotation method, combined with a full-information
transformation approach to convert manual annotations into diverse task
formats. To further enhance model capacity for detailed analysis of plot
structures and character relationships within series, we propose a novel
narrative reasoning framework, \textbf{PC-DCoT}. Extensive results on
\textbf{SeriesBench} indicate that existing MLLMs still face significant
challenges in understanding narrative-driven series, while \textbf{PC-DCoT}
enables these MLLMs to achieve performance improvements. Overall, our
\textbf{SeriesBench} and \textbf{PC-DCoT} highlight the critical necessity of
advancing model capabilities to understand narrative-driven series, guiding the
future development of MLLMs. SeriesBench is publicly available at
https://github.com/zackhxn/SeriesBench-CVPR2025.

</details>


### [80] [Rethinking Visual Layer Selection in Multimodal LLMs](https://arxiv.org/abs/2504.21447)
*Haoran Chen,Junyan Lin,Xinhao Chen,Yue Fan,Xin Jin,Hui Su,Jianfeng Dong,Jinlan Fu,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 提出了一种基于层间表示相似性的方法，将CLIP-ViT层分为浅、中、深三类，并评估其对MLLM性能的影响。实验表明，不同任务需要不同层特征，轻量级融合方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM通常基于经验选择视觉特征，缺乏系统性分析，本文旨在通过层间相似性分析优化视觉特征选择。

Method: 提出层间表示相似性方法，将CLIP-ViT层分类，并在不同参数规模的LLaVA模型上进行实验。

Result: 实验发现：(1) 深层对OCR任务关键；(2) 浅中层在计数、定位等任务中表现更好；(3) 轻量级融合方法在9/10数据集上优于基线。

Conclusion: 本文首次系统研究了MLLM中视觉层选择问题，为未来视觉表示学习研究奠定了基础。

Abstract: Multimodal large language models (MLLMs) have achieved impressive performance
across a wide range of tasks, typically using CLIP-ViT as their visual encoder
due to its strong text-image alignment capabilities. While prior studies
suggest that different CLIP-ViT layers capture different types of information,
with shallower layers focusing on fine visual details and deeper layers
aligning more closely with textual semantics, most MLLMs still select visual
features based on empirical heuristics rather than systematic analysis. In this
work, we propose a Layer-wise Representation Similarity approach to group
CLIP-ViT layers with similar behaviors into {shallow, middle, and deep}
categories and assess their impact on MLLM performance. Building on this
foundation, we revisit the visual layer selection problem in MLLMs at scale,
training LLaVA-style models ranging from 1.4B to 7B parameters. Through
extensive experiments across 10 datasets and 4 tasks, we find that: (1) deep
layers are essential for OCR tasks; (2) shallow and middle layers substantially
outperform deep layers on reasoning tasks involving counting, positioning, and
object localization; (3) a lightweight fusion of features across shallow,
middle, and deep layers consistently outperforms specialized fusion baselines
and single-layer selections, achieving gains on 9 out of 10 datasets. Our work
offers the first principled study of visual layer selection in MLLMs, laying
the groundwork for deeper investigations into visual representation learning
for MLLMs.

</details>


### [81] [VR-FuseNet: A Fusion of Heterogeneous Fundus Data and Explainable Deep Network for Diabetic Retinopathy Classification](https://arxiv.org/abs/2504.21464)
*Shamim Rahim Refat,Ziyan Shirin Raha,Shuvashis Sarker,Faika Fairuj Preotee,MD. Musfikur Rahman,Tashreef Muhammad,Mohammad Shafiul Islam*

Main category: cs.CV

TL;DR: 本文提出了一种名为VR-FuseNet的混合深度学习模型，用于自动化糖尿病视网膜病变检测，结合了VGG19和ResNet50V2的优势，准确率达91.824%，并通过XAI技术增强临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是导致糖尿病患者失明的主要原因，现有方法存在数据集不平衡和泛化能力不足的问题，需要更准确高效的自动化检测方法。

Method: 使用五个公开数据集构建混合数据集，应用SMOTE和CLAHE进行预处理，提出VR-FuseNet模型，结合VGG19和ResNet50V2的特征提取能力。

Result: 模型准确率达91.824%，优于单一架构，并通过XAI技术生成可视化解释。

Conclusion: VR-FuseNet在糖尿病视网膜病变分类任务中表现出色，结合XAI技术提升了临床实用性。

Abstract: Diabetic retinopathy is a severe eye condition caused by diabetes where the
retinal blood vessels get damaged and can lead to vision loss and blindness if
not treated. Early and accurate detection is key to intervention and stopping
the disease progressing. For addressing this disease properly, this paper
presents a comprehensive approach for automated diabetic retinopathy detection
by proposing a new hybrid deep learning model called VR-FuseNet. Diabetic
retinopathy is a major eye disease and leading cause of blindness especially
among diabetic patients so accurate and efficient automated detection methods
are required. To address the limitations of existing methods including dataset
imbalance, diversity and generalization issues this paper presents a hybrid
dataset created from five publicly available diabetic retinopathy datasets.
Essential preprocessing techniques such as SMOTE for class balancing and CLAHE
for image enhancement are applied systematically to the dataset to improve the
robustness and generalizability of the dataset. The proposed VR-FuseNet model
combines the strengths of two state-of-the-art convolutional neural networks,
VGG19 which captures fine-grained spatial features and ResNet50V2 which is
known for its deep hierarchical feature extraction. This fusion improves the
diagnostic performance and achieves an accuracy of 91.824%. The model
outperforms individual architectures on all performance metrics demonstrating
the effectiveness of hybrid feature extraction in Diabetic Retinopathy
classification tasks. To make the proposed model more clinically useful and
interpretable this paper incorporates multiple XAI techniques. These techniques
generate visual explanations that clearly indicate the retinal features
affecting the model's prediction such as microaneurysms, hemorrhages and
exudates so that clinicians can interpret and validate.

</details>


### [82] [Multiview Point Cloud Registration via Optimization in an Autoencoder Latent Space](https://arxiv.org/abs/2504.21467)
*Luc Vedrenne,Sylvain Faisan,Denis Fortun*

Main category: cs.CV

TL;DR: POLAR是一种多视角点云刚性配准方法，通过潜在空间转换和优化策略，解决了现有方法在大视角、高退化和大初始角度下的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视角配准中难以处理大量视角、高退化和大初始角度的问题。

Method: 将配准问题转换到预训练自编码器的潜在空间，设计考虑退化的损失函数，并采用多起点优化策略。

Result: 在合成和真实数据上显著优于现有方法。

Conclusion: POLAR是一种高效、鲁棒的多视角点云配准方法。

Abstract: Point cloud rigid registration is a fundamental problem in 3D computer
vision. In the multiview case, we aim to find a set of 6D poses to align a set
of objects. Methods based on pairwise registration rely on a subsequent
synchronization algorithm, which makes them poorly scalable with the number of
views. Generative approaches overcome this limitation, but are based on
Gaussian Mixture Models and use an Expectation-Maximization algorithm. Hence,
they are not well suited to handle large transformations. Moreover, most
existing methods cannot handle high levels of degradations. In this paper, we
introduce POLAR (POint cloud LAtent Registration), a multiview registration
method able to efficiently deal with a large number of views, while being
robust to a high level of degradations and large initial angles. To achieve
this, we transpose the registration problem into the latent space of a
pretrained autoencoder, design a loss taking degradations into account, and
develop an efficient multistart optimization strategy. Our proposed method
significantly outperforms state-of-the-art approaches on synthetic and real
data. POLAR is available at github.com/pypolar/polar or as a standalone package
which can be installed with pip install polaregistration.

</details>


### [83] [Quaternion Nuclear Norms Over Frobenius Norms Minimization for Robust Matrix Completion](https://arxiv.org/abs/2504.21468)
*Yu Guo,Guoqing Chen,Tieyong Zeng,Qiyu Jin,Michael Kwok-Po Ng*

Main category: cs.CV

TL;DR: 提出了一种新的非凸近似方法（QNOF）用于四元数矩阵的秩估计，并将其扩展到鲁棒矩阵补全问题，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 多维数据表示中隐藏结构的恢复是一个普遍挑战，四元数矩阵为此提供了自然建模框架。

Method: 引入QNOF作为四元数矩阵秩的非凸近似，利用四元数奇异值分解简化求解，并扩展到鲁棒矩阵补全问题。

Result: QNOF在实验中表现优于现有方法，证明了其有效性。

Conclusion: QNOF是一种参数无关且尺度不变的方法，适用于多维数据恢复问题。

Abstract: Recovering hidden structures from incomplete or noisy data remains a
pervasive challenge across many fields, particularly where multi-dimensional
data representation is essential. Quaternion matrices, with their ability to
naturally model multi-dimensional data, offer a promising framework for this
problem. This paper introduces the quaternion nuclear norm over the Frobenius
norm (QNOF) as a novel nonconvex approximation for the rank of quaternion
matrices. QNOF is parameter-free and scale-invariant. Utilizing quaternion
singular value decomposition, we prove that solving the QNOF can be simplified
to solving the singular value $L_1/L_2$ problem. Additionally, we extend the
QNOF to robust quaternion matrix completion, employing the alternating
direction multiplier method to derive solutions that guarantee weak convergence
under mild conditions. Extensive numerical experiments validate the proposed
model's superiority, consistently outperforming state-of-the-art quaternion
methods.

</details>


### [84] [Robust Orthogonal NMF with Label Propagation for Image Clustering](https://arxiv.org/abs/2504.21472)
*Jingjing Liu,Nian Wu,Xianchao Xiu,Jianhua Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为RONMF的鲁棒正交非负矩阵分解方法，通过结合图拉普拉斯和标签传播作为正则项，并引入非凸结构和正交约束，提高了对噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有NMF方法对噪声敏感且难以利用有限的监督信息，需要一种更鲁棒的解决方案。

Method: 提出RONMF框架，结合图拉普拉斯、标签传播、非凸结构和正交约束，并使用ADMM算法优化。

Result: 在八个公开图像数据集上验证，RONMF在多种标准指标上优于现有NMF方法，且表现出优秀的鲁棒性。

Conclusion: RONMF是一种高效且鲁棒的NMF方法，适用于图像聚类任务。

Abstract: Non-negative matrix factorization (NMF) is a popular unsupervised learning
approach widely used in image clustering. However, in real-world clustering
scenarios, most existing NMF methods are highly sensitive to noise corruption
and are unable to effectively leverage limited supervised information. To
overcome these drawbacks, we propose a unified non-convex framework with label
propagation called robust orthogonal nonnegative matrix factorization (RONMF).
This method not only considers the graph Laplacian and label propagation as
regularization terms but also introduces a more effective non-convex structure
to measure the reconstruction error and imposes orthogonal constraints on the
basis matrix to reduce the noise corruption, thereby achieving higher
robustness. To solve RONMF, we develop an alternating direction method of
multipliers (ADMM)-based optimization algorithm. In particular, all subproblems
have closed-form solutions, which ensures its efficiency. Experimental
evaluations on eight public image datasets demonstrate that the proposed RONMF
outperforms state-of-the-art NMF methods across various standard metrics and
shows excellent robustness. The code will be available at
https://github.com/slinda-liu.

</details>


### [85] [GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers](https://arxiv.org/abs/2504.21476)
*Xinyu Li,Qi Yao,Yuanda Wang*

Main category: cs.CV

TL;DR: GarmentDiffusion是一种新型生成模型，能够从多模态输入（文本、图像和不完整缝纫图案）生成厘米级精度的矢量3D缝纫图案，效率比现有方法高100倍。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成多样化缝纫图案时，要么依赖单一输入模态，要么生成效率不足。

Method: 通过将3D缝纫图案参数编码为紧凑的边缘标记表示，并使用扩散变换器同时去噪所有边缘标记，实现高效生成。

Result: 在DressCodeData和GarmentCodeData数据集上取得最新最优结果，生成速度比SewingGPT快100倍。

Conclusion: GarmentDiffusion为缝纫图案生成提供了一种高效且多模态的方法，显著提升了生成速度和精度。

Abstract: Garment sewing patterns are fundamental design elements that bridge the gap
between design concepts and practical manufacturing. The generative modeling of
sewing patterns is crucial for creating diversified garments. However, existing
approaches are limited either by reliance on a single input modality or by
suboptimal generation efficiency. In this work, we present
\textbf{\textit{GarmentDiffusion}}, a new generative model capable of producing
centimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text,
image, and incomplete sewing pattern). Our method efficiently encodes 3D sewing
pattern parameters into compact edge token representations, achieving a
sequence length that is $\textbf{10}\times$ shorter than that of the
autoregressive SewingGPT in DressCode. By employing a diffusion transformer, we
simultaneously denoise all edge tokens along the temporal axis, while
maintaining a constant number of denoising steps regardless of dataset-specific
edge and panel statistics. With all combination of designs of our model, the
sewing pattern generation speed is accelerated by $\textbf{100}\times$ compared
to SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well
as on the largest sewing pattern dataset, namely GarmentCodeData. The project
website is available at https://shenfu-research.github.io/Garment-Diffusion/.

</details>


### [86] [CAE-DFKD: Bridging the Transferability Gap in Data-Free Knowledge Distillation](https://arxiv.org/abs/2504.21478)
*Zherui Zhang,Changwei Wang,Rongtao Xu,Wenhao Xu,Shibiao Xu,Yu Zhang,Li Guo*

Main category: cs.CV

TL;DR: CAE-DFKD提出了一种新的无数据知识蒸馏方法，通过嵌入级别的改进提升模型泛化能力，并在效率和下游任务表现上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有DFKD方法主要关注图像识别性能，忽略了学习表示的可迁移性。CAE-DFKD旨在解决这一问题。

Method: CAE-DFKD在嵌入级别改进生成器训练范式，提升模型泛化能力。

Result: CAE-DFKD在效率、图像识别性能和下游任务表现上均优于现有方法。

Conclusion: CAE-DFKD是一种高效且泛化能力强的无数据知识蒸馏方法。

Abstract: Data-Free Knowledge Distillation (DFKD) enables the knowledge transfer from
the given pre-trained teacher network to the target student model without
access to the real training data. Existing DFKD methods focus primarily on
improving image recognition performance on associated datasets, often
neglecting the crucial aspect of the transferability of learned
representations. In this paper, we propose Category-Aware Embedding Data-Free
Knowledge Distillation (CAE-DFKD), which addresses at the embedding level the
limitations of previous rely on image-level methods to improve model
generalization but fail when directly applied to DFKD. The superiority and
flexibility of CAE-DFKD are extensively evaluated, including:
\textit{\textbf{i.)}} Significant efficiency advantages resulting from altering
the generator training paradigm; \textit{\textbf{ii.)}} Competitive performance
with existing DFKD state-of-the-art methods on image recognition tasks;
\textit{\textbf{iii.)}} Remarkable transferability of data-free learned
representations demonstrated in downstream tasks.

</details>


### [87] [DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling for Image Restoration](https://arxiv.org/abs/2504.21487)
*Hebaixu Wang,Jing Zhang,Haonan Guo,Di Wang,Jiayi Ma,Bo Du*

Main category: cs.CV

TL;DR: DGSolver是一种扩散通用求解器，通过高精度求解器和后验采样提升图像修复质量，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在减少采样步骤时引入累积误差，且难以平衡退化表示和修复质量。

Method: 推导通用扩散模型的精确ODE，设计高阶求解器和队列加速采样策略，结合后验采样优化噪声估计。

Result: 实验表明DGSolver在修复精度、稳定性和扩展性上优于现有方法。

Conclusion: DGSolver通过高效采样和后验采样显著提升了图像修复性能。

Abstract: Diffusion models have achieved remarkable progress in universal image
restoration. While existing methods speed up inference by reducing sampling
steps, substantial step intervals often introduce cumulative errors. Moreover,
they struggle to balance the commonality of degradation representations and
restoration quality. To address these challenges, we introduce
\textbf{DGSolver}, a diffusion generalist solver with universal posterior
sampling. We first derive the exact ordinary differential equations for
generalist diffusion models and tailor high-order solvers with a queue-based
accelerated sampling strategy to improve both accuracy and efficiency. We then
integrate universal posterior sampling to better approximate
manifold-constrained gradients, yielding a more accurate noise estimation and
correcting errors in inverse inference. Extensive experiments show that
DGSolver outperforms state-of-the-art methods in restoration accuracy,
stability, and scalability, both qualitatively and quantitatively. Code and
models will be available at https://github.com/MiliLab/DGSolver.

</details>


### [88] [ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic Segmentation of Remote Sensing Imagery](https://arxiv.org/abs/2504.21491)
*Qinfeng Zhu,Yunxi Jiang,Lei Fan*

Main category: cs.CV

TL;DR: ClassWise-CRF是一种结果级类别特定融合架构，通过两阶段过程选择专家网络并自适应加权其预测，结合CRF优化空间一致性和边界精度，显著提升遥感图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决多网络融合中类别特定优化不足的问题，提升语义分割性能。

Method: 两阶段过程：贪婪算法选择专家网络，自适应加权融合预测；结合CRF优化空间一致性和边界精度。

Result: 在LoveDA和Vaihingen数据集上，mIoU分别提升1.00%/0.68%和0.87%/0.91%。

Conclusion: ClassWise-CRF有效且通用，显著提升遥感图像语义分割性能。

Abstract: We propose a result-level category-specific fusion architecture called
ClassWise-CRF. This architecture employs a two-stage process: first, it selects
expert networks that perform well in specific categories from a pool of
candidate networks using a greedy algorithm; second, it integrates the
segmentation predictions of these selected networks by adaptively weighting
their contributions based on their segmentation performance in each category.
Inspired by Conditional Random Field (CRF), the ClassWise-CRF architecture
treats the segmentation predictions from multiple networks as confidence vector
fields. It leverages segmentation metrics (such as Intersection over Union)
from the validation set as priors and employs an exponential weighting strategy
to fuse the category-specific confidence scores predicted by each network. This
fusion method dynamically adjusts the weights of each network for different
categories, achieving category-specific optimization. Building on this, the
architecture further optimizes the fused results using unary and pairwise
potentials in CRF to ensure spatial consistency and boundary accuracy. To
validate the effectiveness of ClassWise-CRF, we conducted experiments on two
remote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced
semantic segmentation networks. The results show that the ClassWise-CRF
architecture significantly improves segmentation performance: on the LoveDA
dataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on
the validation set and by 0.68% on the test set; on the Vaihingen dataset, the
mIoU improved by 0.87% on the validation set and by 0.91% on the test set.
These results fully demonstrate the effectiveness and generality of the
ClassWise-CRF architecture in semantic segmentation of remote sensing images.
The full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF.

</details>


### [89] [Consistency-aware Fake Videos Detection on Short Video Platforms](https://arxiv.org/abs/2504.21495)
*Junxi Wang,Jize liu,Na Zhang,Yaxiong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种利用跨模态矛盾检测假新闻的新方法，通过跨模态一致性学习和多模态协作诊断模块提升检测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用跨模态不一致性作为判别特征，导致检测精度不足。

Method: 提出跨模态一致性学习（CMCL）和多模态协作诊断（MMCD）模块，包括伪标签生成、一致性诊断、多模态特征融合和概率分数融合。

Result: 在FakeSV和FakeTT基准测试中表现出色。

Conclusion: 通过显式利用跨模态矛盾，显著提升了假视频检测性能。

Abstract: This paper focuses to detect the fake news on the short video platforms.
While significant research efforts have been devoted to this task with notable
progress in recent years, current detection accuracy remains suboptimal due to
the rapid evolution of content manipulation and generation technologies.
Existing approaches typically employ a cross-modal fusion strategy that
directly combines raw video data with metadata inputs before applying a
classification layer. However, our empirical observations reveal a critical
oversight: manipulated content frequently exhibits inter-modal inconsistencies
that could serve as valuable discriminative features, yet remain underutilized
in contemporary detection frameworks. Motivated by this insight, we propose a
novel detection paradigm that explicitly identifies and leverages cross-modal
contradictions as discriminative cues. Our approach consists of two core
modules: Cross-modal Consistency Learning (CMCL) and Multi-modal Collaborative
Diagnosis (MMCD). CMCL includes Pseudo-label Generation (PLG) and Cross-modal
Consistency Diagnosis (CMCD). In PLG, a Multimodal Large Language Model is used
to generate pseudo-labels for evaluating cross-modal semantic consistency.
Then, CMCD extracts [CLS] tokens and computes cosine loss to quantify
cross-modal inconsistencies. MMCD further integrates multimodal features
through Multimodal Feature Fusion (MFF) and Probability Scores Fusion (PSF).
MFF employs a co-attention mechanism to enhance semantic interactions across
different modalities, while a Transformer is utilized for comprehensive feature
fusion. Meanwhile, PSF further integrates the fake news probability scores
obtained in the previous step. Extensive experiments on established benchmarks
(FakeSV and FakeTT) demonstrate our model exhibits outstanding performance in
Fake videos detection.

</details>


### [90] [MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance](https://arxiv.org/abs/2504.21497)
*Mengting Wei,Yante Li,Tuomas Varanka,Yan Jiang,Licai Sun,Guoying Zhao*

Main category: cs.CV

TL;DR: 提出了一种将3D人脸参数模型与潜在扩散框架结合的视频人脸重演方法，提升形状一致性和运动控制。


<details>
  <summary>Details</summary>
Motivation: 改进现有视频人脸生成方法在形状一致性和运动控制方面的不足。

Method: 使用FLAME模型作为3D人脸参数表示，结合深度图、法线图和渲染图，增强潜在扩散模型，并通过多层融合模块结合身份和运动特征。

Result: 在基准数据集上生成高质量人脸动画，精确建模表情和头部姿态变化，并在域外图像上表现优异。

Conclusion: 该方法通过3D参数模型实现精确对齐，显著提升了人脸重演的质量和泛化能力。

Abstract: In this paper, we propose a method for video face reenactment that integrates
a 3D face parametric model into a latent diffusion framework, aiming to improve
shape consistency and motion control in existing video-based face generation
approaches. Our approach employs the FLAME (Faces Learned with an Articulated
Model and Expressions) model as the 3D face parametric representation,
providing a unified framework for modeling face expressions and head pose. This
enables precise extraction of detailed face geometry and motion features from
driving videos. Specifically, we enhance the latent diffusion model with rich
3D expression and detailed pose information by incorporating depth maps, normal
maps, and rendering maps derived from FLAME sequences. A multi-layer face
movements fusion module with integrated self-attention mechanisms is used to
combine identity and motion latent features within the spatial domain. By
utilizing the 3D face parametric model as motion guidance, our method enables
parametric alignment of face identity between the reference image and the
motion captured from the driving video. Experimental results on benchmark
datasets show that our method excels at generating high-quality face animations
with precise expression and head pose variation modeling. In addition, it
demonstrates strong generalization performance on out-of-domain images. Code is
publicly available at https://github.com/weimengting/MagicPortrait.

</details>


### [91] [SAM4EM: Efficient memory-based two stage prompt-free segment anything model adapter for complex 3D neuroscience electron microscopy stacks](https://arxiv.org/abs/2504.21544)
*Uzair Shah,Marco Agus,Daniya Boges,Vanessa Chiappini,Mahmood Alzubaidi,Jens Schneider,Markus Hadwiger,Pierre J. Magistretti,Mowafa Househ,Corrado Calı*

Main category: cs.CV

TL;DR: SAM4EM是一种利用Segment Anything Model（SAM）和高级微调策略对电子显微镜（EM）数据中复杂神经结构进行3D分割的新方法。


<details>
  <summary>Details</summary>
Motivation: 解决电子显微镜数据中复杂神经结构（如线粒体、胶质细胞和突触）的3D分割问题，提升分割精度。

Method: 开发了无提示适配器、基于LoRA的双阶段微调方法，以及3D记忆注意力机制，确保3D堆栈中的分割一致性。

Result: 在神经科学分割基准测试中，显著优于现有方法，特别是在胶质细胞和突触后密度的分割上。

Conclusion: SAM4EM在复杂神经结构的3D分割中表现出色，为相关研究提供了新工具和基准数据集。

Abstract: We present SAM4EM, a novel approach for 3D segmentation of complex neural
structures in electron microscopy (EM) data by leveraging the Segment Anything
Model (SAM) alongside advanced fine-tuning strategies. Our contributions
include the development of a prompt-free adapter for SAM using two stage mask
decoding to automatically generate prompt embeddings, a dual-stage fine-tuning
method based on Low-Rank Adaptation (LoRA) for enhancing segmentation with
limited annotated data, and a 3D memory attention mechanism to ensure
segmentation consistency across 3D stacks. We further release a unique
benchmark dataset for the segmentation of astrocytic processes and synapses. We
evaluated our method on challenging neuroscience segmentation benchmarks,
specifically targeting mitochondria, glia, and synapses, with significant
accuracy improvements over state-of-the-art (SOTA) methods, including recent
SAM-based adapters developed for the medical domain and other vision
transformer-based approaches. Experimental results indicate that our approach
outperforms existing solutions in the segmentation of complex processes like
glia and post-synaptic densities. Our code and models are available at
https://github.com/Uzshah/SAM4EM.

</details>


### [92] [Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models](https://arxiv.org/abs/2504.21559)
*Sangmin Woo,Kang Zhou,Yun Zhou,Shuai Wang,Sheng Guan,Haibo Ding,Lin Lee Cheong*

Main category: cs.CV

TL;DR: 通过视觉提示工程（BBVPE）框架，动态选择最优视觉提示以减少大型视觉语言模型（LVLM）中的物体幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLM）常出现物体幻觉问题，影响其可靠性。研究发现，简单的视觉提示（如边界框、圆圈）能显著缓解这一问题，但不同提示效果差异较大。

Method: 提出Black-Box Visual Prompt Engineering（BBVPE）框架，通过候选视觉提示池和路由模型动态选择最优提示，无需访问模型内部。

Result: 在POPE和CHAIR等基准测试中，BBVPE有效减少了物体幻觉。

Conclusion: BBVPE是一种模型无关的黑盒方法，适用于开源和专有LVLM，能显著提升模型可靠性。

Abstract: Large Vision Language Models (LVLMs) often suffer from object hallucination,
which undermines their reliability. Surprisingly, we find that simple
object-based visual prompting -- overlaying visual cues (e.g., bounding box,
circle) on images -- can significantly mitigate such hallucination; however,
different visual prompts (VPs) vary in effectiveness. To address this, we
propose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify
optimal VPs that enhance LVLM responses without needing access to model
internals. Our approach employs a pool of candidate VPs and trains a router
model to dynamically select the most effective VP for a given input image. This
black-box approach is model-agnostic, making it applicable to both open-source
and proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR
demonstrate that BBVPE effectively reduces object hallucination.

</details>


### [93] [Iterative Trajectory Exploration for Multimodal Agents](https://arxiv.org/abs/2504.21561)
*Pengxiang Li,Zhi Gao,Bofei Zhang,Yapeng Mi,Xiaojian Ma,Chenrui Shi,Tao Yuan,Yuwei Wu,Yunde Jia,Song-Chun Zhu,Qing Li*

Main category: cs.CV

TL;DR: SPORT是一种在线自探索方法，通过逐步偏好优化改进多模态代理的轨迹，无需专家标注。


<details>
  <summary>Details</summary>
Motivation: 现有代理需要大量专家数据进行微调以适应新环境，SPORT旨在通过自生成任务和学习解决任务来减少对专家数据的依赖。

Method: SPORT通过任务合成、步骤采样、步骤验证和偏好调优四个迭代组件实现，利用语言模型生成任务并通过AI反馈优化策略。

Result: 在GTA和GAIA基准测试中，SPORT代理分别实现了6.41%和3.64%的性能提升。

Conclusion: SPORT展示了通过自生成任务和偏好优化提升多模态代理能力的潜力，具有泛化性和有效性。

Abstract: Multimodal agents, which integrate a controller (e.g., a large language
model) with external tools, have demonstrated remarkable capabilities in
tackling complex tasks. However, existing agents need to collect a large number
of expert data for fine-tuning to adapt to new environments. In this paper, we
propose an online self-exploration method for multimodal agents, namely SPORT,
via step-wise preference optimization to refine the trajectories of agents,
which automatically generates tasks and learns from solving the generated
tasks, without any expert annotation. SPORT operates through four iterative
components: task synthesis, step sampling, step verification, and preference
tuning. First, we synthesize multi-modal tasks using language models. Then, we
introduce a novel search scheme, where step sampling and step verification are
executed alternately to solve each generated task. We employ a verifier to
provide AI feedback to construct step-wise preference data. The data is
subsequently used to update the controller's policy through preference tuning,
producing a SPORT Agent. By interacting with real environments, the SPORT Agent
evolves into a more refined and capable system. Evaluation in the GTA and GAIA
benchmarks show that the SPORT Agent achieves 6.41\% and 3.64\% improvements,
underscoring the generalization and effectiveness introduced by our method. The
project page is https://SPORT-Agents.github.io.

</details>


### [94] [eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes](https://arxiv.org/abs/2504.21562)
*Henry John Krumb,Anirban Mukhopadhyay*

Main category: cs.CV

TL;DR: 无线胶囊内窥镜通过神经细胞自动机（NCA）实现出血分割和深度估计，模型轻量化后可在微型设备（如ESP32）上高效运行，显著提升诊断精度和定位能力。


<details>
  <summary>Details</summary>
Motivation: 无线胶囊内窥镜生成大量视频数据，传统方法难以实时处理且定位困难，需轻量化模型实现高效病理检测。

Method: 通过蒸馏大型基础模型到NCA架构，生成伪真值训练NCA，并将其移植到ESP32微控制器，优化推理速度。

Result: NCA在出血分割上比其他轻量模型更准确（Dice），参数减少100倍以上，深度估计效果优于伪真值，ESP32-S3推理速度提升3倍。

Conclusion: NCA首次在微型设备上实现可靠出血分割和深度估计，为胶囊内窥镜的精确诊断和定位提供了新方法。

Abstract: Wireless Capsule Endoscopy is a non-invasive imaging method for the entire
gastrointestinal tract, and is a pain-free alternative to traditional
endoscopy. It generates extensive video data that requires significant review
time, and localizing the capsule after ingestion is a challenge. Techniques
like bleeding detection and depth estimation can help with localization of
pathologies, but deep learning models are typically too large to run directly
on the capsule. Neural Cellular Automata (NCA) for bleeding segmentation and
depth estimation are trained on capsule endoscopic images. For monocular depth
estimation, we distill a large foundation model into the lean NCA architecture,
by treating the outputs of the foundation model as pseudo ground truth. We then
port the trained NCA to the ESP32 microcontroller, enabling efficient image
processing on hardware as small as a camera capsule. NCA are more accurate
(Dice) than other portable segmentation models, while requiring more than 100x
fewer parameters stored in memory than other small-scale models. The visual
results of NCA depth estimation look convincing, and in some cases beat the
realism and detail of the pseudo ground truth. Runtime optimizations on the
ESP32-S3 accelerate the average inference speed significantly, by more than
factor 3. With several algorithmic adjustments and distillation, it is possible
to eNCApsulate NCA models into microcontrollers that fit into wireless capsule
endoscopes. This is the first work that enables reliable bleeding segmentation
and depth estimation on a miniaturized device, paving the way for precise
diagnosis combined with visual odometry as a means of precise localization of
the capsule -- on the capsule.

</details>


### [95] [Cascade Detector Analysis and Application to Biomedical Microscopy](https://arxiv.org/abs/2504.21598)
*Thomas L. Athey,Shashata Sawmya,Nir Shavit*

Main category: cs.CV

TL;DR: 利用级联检测器高效识别多分辨率图像中的稀疏目标，减少计算时间30-75%。


<details>
  <summary>Details</summary>
Motivation: 随着计算机视觉模型和生物医学数据集的规模增长，需要更高效的推理算法。

Method: 通过级联检测器在不同分辨率下识别目标，并分析其准确性和分类器调用次数。

Result: 多级检测器在荧光细胞检测、细胞器分割和组织分割中性能相当，但时间减少30-75%。

Conclusion: 该方法适用于多种计算机视觉模型和数据领域。

Abstract: As both computer vision models and biomedical datasets grow in size, there is
an increasing need for efficient inference algorithms. We utilize cascade
detectors to efficiently identify sparse objects in multiresolution images.
Given an object's prevalence and a set of detectors at different resolutions
with known accuracies, we derive the accuracy, and expected number of
classifier calls by a cascade detector. These results generalize across number
of dimensions and number of cascade levels. Finally, we compare one- and
two-level detectors in fluorescent cell detection, organelle segmentation, and
tissue segmentation across various microscopy modalities. We show that the
multi-level detector achieves comparable performance in 30-75% less time. Our
work is compatible with a variety of computer vision models and data domains.

</details>


### [96] [Mcity Data Engine: Iterative Model Improvement Through Open-Vocabulary Data Selection](https://arxiv.org/abs/2504.21614)
*Daniel Bogdoll,Rajanikant Patnaik Ananta,Abeyankar Giridharan,Isabel Moore,Gregory Stevens,Henry X. Liu*

Main category: cs.CV

TL;DR: Mcity Data Engine是一个开源系统，旨在解决智能交通系统中数据选择和标注的挑战，特别是针对长尾类别。


<details>
  <summary>Details</summary>
Motivation: 随着数据量的增加，选择和标注适合训练机器学习模型的样本变得困难，尤其是在智能交通系统中。现有工业数据引擎多为专有，缺乏开源解决方案。

Method: Mcity Data Engine提供完整的数据开发周期模块，从数据采集到模型部署，专注于通过开放词汇数据选择处理罕见和新类别。

Result: 系统代码已在GitHub上公开，采用MIT许可证。

Conclusion: Mcity Data Engine填补了开源社区在数据选择和模型训练工具上的空白，特别适用于智能交通领域。

Abstract: With an ever-increasing availability of data, it has become more and more
challenging to select and label appropriate samples for the training of machine
learning models. It is especially difficult to detect long-tail classes of
interest in large amounts of unlabeled data. This holds especially true for
Intelligent Transportation Systems (ITS), where vehicle fleets and roadside
perception systems generate an abundance of raw data. While industrial,
proprietary data engines for such iterative data selection and model training
processes exist, researchers and the open-source community suffer from a lack
of an openly available system. We present the Mcity Data Engine, which provides
modules for the complete data-based development cycle, beginning at the data
acquisition phase and ending at the model deployment stage. The Mcity Data
Engine focuses on rare and novel classes through an open-vocabulary data
selection process. All code is publicly available on GitHub under an MIT
license: https://github.com/mcity/mcity_data_engine

</details>


### [97] [Diffusion-based Adversarial Identity Manipulation for Facial Privacy Protection](https://arxiv.org/abs/2504.21646)
*Liqin Wang,Qianyue Hu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: DiffAIM是一种基于扩散模型的对抗性人脸生成方法，旨在保护隐私，通过操纵低维潜在空间生成自然且高迁移性的对抗人脸。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护方法无法生成自然的对抗人脸，导致人脸识别系统可能被滥用，引发隐私问题。

Method: 在扩散模型的反向扩散过程中，迭代注入基于梯度的对抗性身份引导，优化身份收敛和语义差异，同时通过结构保持正则化保持面部结构一致性。

Result: DiffAIM在面部验证和识别任务中表现出更强的黑盒攻击迁移性和视觉质量，并在商业API（如Face++和Aliyun）上验证了有效性。

Conclusion: DiffAIM为隐私保护提供了一种高效且视觉自然的方法，适用于对抗恶意人脸识别系统。

Abstract: The success of face recognition (FR) systems has led to serious privacy
concerns due to potential unauthorized surveillance and user tracking on social
networks. Existing methods for enhancing privacy fail to generate natural face
images that can protect facial privacy. In this paper, we propose
diffusion-based adversarial identity manipulation (DiffAIM) to generate natural
and highly transferable adversarial faces against malicious FR systems. To be
specific, we manipulate facial identity within the low-dimensional latent space
of a diffusion model. This involves iteratively injecting gradient-based
adversarial identity guidance during the reverse diffusion process,
progressively steering the generation toward the desired adversarial faces. The
guidance is optimized for identity convergence towards a target while promoting
semantic divergence from the source, facilitating effective impersonation while
maintaining visual naturalness. We further incorporate structure-preserving
regularization to preserve facial structure consistency during manipulation.
Extensive experiments on both face verification and identification tasks
demonstrate that compared with the state-of-the-art, DiffAIM achieves stronger
black-box attack transferability while maintaining superior visual quality. We
also demonstrate the effectiveness of the proposed approach for commercial FR
APIs, including Face++ and Aliyun.

</details>


### [98] [HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation](https://arxiv.org/abs/2504.21650)
*Haiyang Zhou,Wangbo Yu,Jiawen Guan,Xinhua Cheng,Yonghong Tian,Li Yuan*

Main category: cs.CV

TL;DR: HoloTime框架通过视频扩散模型生成全景视频，并结合4D场景重建技术，提升VR/AR沉浸体验。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型局限于静态3D场景或对象级动态，无法满足沉浸式4D体验需求。

Method: 提出HoloTime框架，包括全景视频生成（Panoramic Animator）和4D场景重建（Panoramic Space-Time Reconstruction）。

Result: 实验表明，HoloTime在全景视频生成和4D场景重建上优于现有方法。

Conclusion: HoloTime能创建更真实、沉浸的4D环境，提升VR/AR用户体验。

Abstract: The rapid advancement of diffusion models holds the promise of
revolutionizing the application of VR and AR technologies, which typically
require scene-level 4D assets for user experience. Nonetheless, existing
diffusion models predominantly concentrate on modeling static 3D scenes or
object-level dynamics, constraining their capacity to provide truly immersive
experiences. To address this issue, we propose HoloTime, a framework that
integrates video diffusion models to generate panoramic videos from a single
prompt or reference image, along with a 360-degree 4D scene reconstruction
method that seamlessly transforms the generated panoramic video into 4D assets,
enabling a fully immersive 4D experience for users. Specifically, to tame video
diffusion models for generating high-fidelity panoramic videos, we introduce
the 360World dataset, the first comprehensive collection of panoramic videos
suitable for downstream 4D scene reconstruction tasks. With this curated
dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion
model that can convert panoramic images into high-quality panoramic videos.
Following this, we present Panoramic Space-Time Reconstruction, which leverages
a space-time depth estimation method to transform the generated panoramic
videos into 4D point clouds, enabling the optimization of a holistic 4D
Gaussian Splatting representation to reconstruct spatially and temporally
consistent 4D scenes. To validate the efficacy of our method, we conducted a
comparative analysis with existing approaches, revealing its superiority in
both panoramic video generation and 4D scene reconstruction. This demonstrates
our method's capability to create more engaging and realistic immersive
environments, thereby enhancing user experiences in VR and AR applications.

</details>


### [99] [Visual Text Processing: A Comprehensive Review and Unified Evaluation](https://arxiv.org/abs/2504.21682)
*Yan Shu,Weichao Zeng,Fangmin Zhao,Zeyu Chen,Zhenhang Li,Xiaomeng Yang,Yu Zhou,Paolo Rota,Xiang Bai,Lianwen Jin,Xu-Cheng Yin,Nicu Sebe*

Main category: cs.CV

TL;DR: 该论文综述了视觉文本处理的最新进展，提出了VTPBench基准和VTPScore评估指标，并分析了20多个模型，指出当前技术的改进空间。


<details>
  <summary>Details</summary>
Motivation: 视觉文本在文档和场景图像中具有重要语义信息，但因其独特性质，处理仍面临挑战。研究旨在探索适合不同任务的文本特征及其有效整合方法。

Method: 通过多视角分析视觉文本处理技术，引入VTPBench基准和基于MLLMs的VTPScore评估指标，并对20多个模型进行实证研究。

Result: 研究发现当前技术仍有显著改进空间，VTPBench和VTPScore为未来研究提供了可靠基准和评估工具。

Conclusion: 该工作为视觉文本处理领域的基础资源，旨在推动未来探索和创新。

Abstract: Visual text is a crucial component in both document and scene images,
conveying rich semantic information and attracting significant attention in the
computer vision community. Beyond traditional tasks such as text detection and
recognition, visual text processing has witnessed rapid advancements driven by
the emergence of foundation models, including text image reconstruction and
text image manipulation. Despite significant progress, challenges remain due to
the unique properties that differentiate text from general objects. Effectively
capturing and leveraging these distinct textual characteristics is essential
for developing robust visual text processing models. In this survey, we present
a comprehensive, multi-perspective analysis of recent advancements in visual
text processing, focusing on two key questions: (1) What textual features are
most suitable for different visual text processing tasks? (2) How can these
distinctive text features be effectively incorporated into processing
frameworks? Furthermore, we introduce VTPBench, a new benchmark that
encompasses a broad range of visual text processing datasets. Leveraging the
advanced visual quality assessment capabilities of multimodal large language
models (MLLMs), we propose VTPScore, a novel evaluation metric designed to
ensure fair and reliable evaluation. Our empirical study with more than 20
specific models reveals substantial room for improvement in the current
techniques. Our aim is to establish this work as a fundamental resource that
fosters future exploration and innovation in the dynamic field of visual text
processing. The relevant repository is available at
https://github.com/shuyansy/Visual-Text-Processing-survey.

</details>


### [100] [Enhancing Self-Supervised Fine-Grained Video Object Tracking with Dynamic Memory Prediction](https://arxiv.org/abs/2504.21692)
*Zihan Zhou,Changrui Dai,Aibo Song,Xiaolin Fang*

Main category: cs.CV

TL;DR: 论文提出了一种动态记忆预测（DMP）框架，通过多参考帧直接增强帧重建，提升视频分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有帧重建方法在复杂场景（如遮挡或快速运动）中忽视多参考帧的直接参与，限制了重建和决策效果。

Method: DMP框架包含动态选择参考帧的记忆引擎和双向目标预测网络，利用多参考帧提升模型鲁棒性。

Result: 实验表明，该算法在对象分割和关键点跟踪任务上优于现有自监督技术。

Conclusion: DMP框架通过多参考帧的动态利用，显著提升了视频分析的准确性和鲁棒性。

Abstract: Successful video analysis relies on accurate recognition of pixels across
frames, and frame reconstruction methods based on video correspondence learning
are popular due to their efficiency. Existing frame reconstruction methods,
while efficient, neglect the value of direct involvement of multiple reference
frames for reconstruction and decision-making aspects, especially in complex
situations such as occlusion or fast movement. In this paper, we introduce a
Dynamic Memory Prediction (DMP) framework that innovatively utilizes multiple
reference frames to concisely and directly enhance frame reconstruction. Its
core component is a Reference Frame Memory Engine that dynamically selects
frames based on object pixel features to improve tracking accuracy. In
addition, a Bidirectional Target Prediction Network is built to utilize
multiple reference frames to improve the robustness of the model. Through
experiments, our algorithm outperforms the state-of-the-art self-supervised
techniques on two fine-grained video object tracking tasks: object segmentation
and keypoint tracking.

</details>


### [101] [REHEARSE-3D: A Multi-modal Emulated Rain Dataset for 3D Point Cloud De-raining](https://arxiv.org/abs/2504.21699)
*Abu Mohammed Raisuddin,Jesper Holmblad,Hamed Haghighi,Yuri Poledna,Maikol Funk Drechsler,Valentina Donzella,Eren Erdal Aksoy*

Main category: cs.CV

TL;DR: 论文提出REHEARSE-3D数据集，用于解决自动驾驶中LiDAR点云因降雨干扰导致的精度问题，并评估了多种去雨方法。


<details>
  <summary>Details</summary>
Motivation: 降雨干扰LiDAR点云质量，影响自动驾驶安全性，需研究去雨技术。

Method: 发布大规模多模态模拟降雨数据集REHEARSE-3D，包含高分辨率LiDAR和4D雷达数据，并评估统计与深度学习模型。

Result: 数据集独特且规模大，支持点级天气影响分析，并提供了去雨基准模型。

Conclusion: REHEARSE-3D为3D点云去雨研究提供重要资源，未来将公开数据集和模型。

Abstract: Sensor degradation poses a significant challenge in autonomous driving.
During heavy rainfall, the interference from raindrops can adversely affect the
quality of LiDAR point clouds, resulting in, for instance, inaccurate point
measurements. This, in turn, can potentially lead to safety concerns if
autonomous driving systems are not weather-aware, i.e., if they are unable to
discern such changes. In this study, we release a new, large-scale, multi-modal
emulated rain dataset, REHEARSE-3D, to promote research advancements in 3D
point cloud de-raining. Distinct from the most relevant competitors, our
dataset is unique in several respects. First, it is the largest point-wise
annotated dataset, and second, it is the only one with high-resolution LiDAR
data (LiDAR-256) enriched with 4D Radar point clouds logged in both daytime and
nighttime conditions in a controlled weather environment. Furthermore,
REHEARSE-3D involves rain-characteristic information, which is of significant
value not only for sensor noise modeling but also for analyzing the impact of
weather at a point level. Leveraging REHEARSE-3D, we benchmark raindrop
detection and removal in fused LiDAR and 4D Radar point clouds. Our
comprehensive study further evaluates the performance of various statistical
and deep-learning models. Upon publication, the dataset and benchmark models
will be made publicly available at: https://sporsho.github.io/REHEARSE3D.

</details>


### [102] [Vision Transformers in Precision Agriculture: A Comprehensive Survey](https://arxiv.org/abs/2504.21706)
*Saber Mehdipour,Seyed Abolghasem Mirroshandel,Seyed Amirhossein Tabatabaei*

Main category: cs.CV

TL;DR: 本文综述了视觉变换器（ViTs）在精准农业中的应用，探讨了其从分类到检测和分割的任务，并与传统CNN进行了比较。


<details>
  <summary>Details</summary>
Motivation: 传统植物病害检测方法在可扩展性和准确性上存在局限，ViTs因其处理长距离依赖和视觉任务的优势成为有前景的替代方案。

Method: 介绍了ViTs的基础架构及其从NLP到计算机视觉的过渡，分析了ViTs如何缓解传统模型的归纳偏差，并综述了相关文献、数据集和性能指标。

Result: 提供了ViTs与CNNs的比较分析，探讨了混合模型和性能提升，同时解决了数据需求、计算成本和模型可解释性等技术挑战。

Conclusion: ViTs有望变革精准农业，本文为研究者和从业者提供了深入理解其潜力的资源，并展望了未来研究方向。

Abstract: Detecting plant diseases is a crucial aspect of modern agriculture - it plays
a key role in maintaining crop health and increasing overall yield. Traditional
approaches, though still valuable, often rely on manual inspection or
conventional machine learning techniques, both of which face limitations in
scalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as
a promising alternative, offering benefits such as improved handling of
long-range dependencies and better scalability for visual tasks. This survey
explores the application of ViTs in precision agriculture, covering tasks from
classification to detection and segmentation. We begin by introducing the
foundational architecture of ViTs and discuss their transition from Natural
Language Processing (NLP) to computer vision. The discussion includes the
concept of inductive bias in traditional models like Convolutional Neural
Networks (CNNs), and how ViTs mitigate these biases. We provide a comprehensive
review of recent literature, focusing on key methodologies, datasets, and
performance metrics. The survey also includes a comparative analysis of CNNs
and ViTs, with a look at hybrid models and performance enhancements. Technical
challenges - such as data requirements, computational demands, and model
interpretability - are addressed alongside potential solutions. Finally, we
outline potential research directions and technological advancements that could
further support the integration of ViTs in real-world agricultural settings.
Our goal with this study is to offer practitioners and researchers a deeper
understanding of how ViTs are poised to transform smart and precision
agriculture.

</details>


### [103] [VividListener: Expressive and Controllable Listener Dynamics Modeling for Multi-Modal Responsive Interaction](https://arxiv.org/abs/2504.21718)
*Shiying Li,Xingqun Qi,Bingkun Yang,Chen Weile,Zezhao Tian,Muyi Sun,Qifeng Liu,Man Zhang,Zhenan Sun*

Main category: cs.CV

TL;DR: 论文提出VividListener框架，用于生成具有细腻情感和表达反应的听者头部动态，解决了现有研究在长序列建模和情感强度控制上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注短期的听者行为生成，缺乏对运动变化和情感强度的精细控制，且缺乏大规模的多模态标注数据集。

Method: 提出VividListener框架，包含Responsive Interaction Module（RIM）和Emotional Intensity Tags（EIT），利用多模态条件指导听者动态建模。

Result: 在ListenerX数据集上的实验表明，VividListener实现了最先进的性能，生成具有表达性和可控性的听者动态。

Conclusion: VividListener框架成功解决了听者动态建模中的精细控制和情感表达问题，为虚拟对话建模提供了有效工具。

Abstract: Generating responsive listener head dynamics with nuanced emotions and
expressive reactions is crucial for practical dialogue modeling in various
virtual avatar animations. Previous studies mainly focus on the direct
short-term production of listener behavior. They overlook the fine-grained
control over motion variations and emotional intensity, especially in
long-sequence modeling. Moreover, the lack of long-term and large-scale paired
speaker-listener corpora including head dynamics and fine-grained
multi-modality annotations (e.g., text-based expression descriptions, emotional
intensity) also limits the application of dialogue modeling.Therefore, we first
newly collect a large-scale multi-turn dataset of 3D dyadic conversation
containing more than 1.4M valid frames for multi-modal responsive interaction,
dubbed ListenerX. Additionally, we propose VividListener, a novel framework
enabling fine-grained, expressive and controllable listener dynamics modeling.
This framework leverages multi-modal conditions as guiding principles for
fostering coherent interactions between speakers and listeners.Specifically, we
design the Responsive Interaction Module (RIM) to adaptively represent the
multi-modal interactive embeddings. RIM ensures the listener dynamics achieve
fine-grained semantic coordination with textual descriptions and adjustments,
while preserving expressive reaction with speaker behavior. Meanwhile, we
design the Emotional Intensity Tags (EIT) for emotion intensity editing with
multi-modal information integration, applying to both text descriptions and
listener motion amplitude.Extensive experiments conducted on our newly
collected ListenerX dataset demonstrate that VividListener achieves
state-of-the-art performance, realizing expressive and controllable listener
dynamics.

</details>


### [104] [Common3D: Self-Supervised Learning of 3D Morphable Models for Common Objects in Neural Feature Space](https://arxiv.org/abs/2504.21749)
*Leonhard Sommer,Olaf Dünkel,Christian Theobalt,Adam Kortylewski*

Main category: cs.CV

TL;DR: Common3D是一种自监督学习方法，通过视频学习3D可变形模型（3DMMs），无需3D数据标注，适用于多种视觉任务。


<details>
  <summary>Details</summary>
Motivation: 现有3DMMs仅适用于少数对象类别（如人脸或人体），且需要大量3D数据和类别特定训练。Common3D旨在解决这一问题，通过自监督学习泛化到常见对象。

Method: Common3D通过对象中心视频学习3DMMs，使用神经特征表示外观，而非RGB颜色，并通过对比学习目标优化特征。

Result: Common3D在3D姿态估计和语义对应任务中表现优于现有方法，且首次实现零样本多任务处理。

Conclusion: Common3D为自监督学习3DMMs提供了新思路，显著提升了泛化能力和任务适应性。

Abstract: 3D morphable models (3DMMs) are a powerful tool to represent the possible
shapes and appearances of an object category. Given a single test image, 3DMMs
can be used to solve various tasks, such as predicting the 3D shape, pose,
semantic correspondence, and instance segmentation of an object. Unfortunately,
3DMMs are only available for very few object categories that are of particular
interest, like faces or human bodies, as they require a demanding 3D data
acquisition and category-specific training process. In contrast, we introduce a
new method, Common3D, that learns 3DMMs of common objects in a fully
self-supervised manner from a collection of object-centric videos. For this
purpose, our model represents objects as a learned 3D template mesh and a
deformation field that is parameterized as an image-conditioned neural network.
Different from prior works, Common3D represents the object appearance with
neural features instead of RGB colors, which enables the learning of more
generalizable representations through an abstraction from pixel intensities.
Importantly, we train the appearance features using a contrastive objective by
exploiting the correspondences defined through the deformable template mesh.
This leads to higher quality correspondence features compared to related works
and a significantly improved model performance at estimating 3D object pose and
semantic correspondence. Common3D is the first completely self-supervised
method that can solve various vision tasks in a zero-shot manner.

</details>


### [105] [Anatomical Similarity as a New Metric to Evaluate Brain Generative Models](https://arxiv.org/abs/2504.21771)
*Bahram Jafrasteh,Wei Peng,Cheng Wan,Yimin Luo,Ehsan Adeli,Qingyu Zhao*

Main category: cs.CV

TL;DR: 提出了一种名为WASABI的新指标，用于评估合成脑MRI的解剖学真实性，通过体积测量和Wasserstein距离比较真实与合成解剖结构的分布。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注纹理和感知，缺乏对解剖学真实性的敏感性，而解剖学真实性对临床意义至关重要。

Method: 利用SynthSeg工具进行脑区分割，计算各脑区体积，并使用多变量Wasserstein距离比较真实与合成MRI的解剖分布。

Result: 在五个生成模型和两个真实数据集上的实验表明，WASABI比传统图像级指标更敏感，能检测到解剖学差异。

Conclusion: 研究呼吁评估范式应从视觉检查和传统指标转向解剖学真实性，作为临床有意义脑MRI合成的关键标准。

Abstract: Generative models enhance neuroimaging through data augmentation, quality
improvement, and rare condition studies. Despite advances in realistic
synthetic MRIs, evaluations focus on texture and perception, lacking
sensitivity to crucial anatomical fidelity. This study proposes a new metric,
called WASABI (Wasserstein-Based Anatomical Brain Index), to assess the
anatomical realism of synthetic brain MRIs. WASABI leverages \textit{SynthSeg},
a deep learning-based brain parcellation tool, to derive volumetric measures of
brain regions in each MRI and uses the multivariate Wasserstein distance to
compare distributions between real and synthetic anatomies. Based on controlled
experiments on two real datasets and synthetic MRIs from five generative
models, WASABI demonstrates higher sensitivity in quantifying anatomical
discrepancies compared to traditional image-level metrics, even when synthetic
images achieve near-perfect visual quality. Our findings advocate for shifting
the evaluation paradigm beyond visual inspection and conventional metrics,
emphasizing anatomical fidelity as a crucial benchmark for clinically
meaningful brain MRI synthesis. Our code is available at
https://github.com/BahramJafrasteh/wasabi-mri.

</details>


### [106] [Anomaly-Driven Approach for Enhanced Prostate Cancer Segmentation](https://arxiv.org/abs/2504.21789)
*Alessia Hu,Regina Beets-Tan,Lishan Cai,Eduardo Pooch*

Main category: cs.CV

TL;DR: 该研究提出了一种结合异常检测的深度学习分割框架（adU-Net），通过异常图引导模型识别前列腺癌，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: MRI在识别临床显著前列腺癌（csPCa）中至关重要，但自动化方法面临数据不平衡、肿瘤大小多变和标注数据不足等挑战。

Method: 研究引入adU-Net，将基于双参数MRI序列的异常图融入分割框架，并通过Fixed-Point GAN生成异常图以突出癌变区域。

Result: 在外部测试集上，adU-Net的平均得分（AUROC和AP的均值）为0.618，优于基线模型nnU-Net（0.605）。

Conclusion: 结合异常检测的分割方法提升了泛化能力和性能，尤其是基于ADC的异常图，为自动化csPCa识别提供了新方向。

Abstract: Magnetic Resonance Imaging (MRI) plays an important role in identifying
clinically significant prostate cancer (csPCa), yet automated methods face
challenges such as data imbalance, variable tumor sizes, and a lack of
annotated data. This study introduces Anomaly-Driven U-Net (adU-Net), which
incorporates anomaly maps derived from biparametric MRI sequences into a deep
learning-based segmentation framework to improve csPCa identification. We
conduct a comparative analysis of anomaly detection methods and evaluate the
integration of anomaly maps into the segmentation pipeline. Anomaly maps,
generated using Fixed-Point GAN reconstruction, highlight deviations from
normal prostate tissue, guiding the segmentation model to potential cancerous
regions. We compare the performance by using the average score, computed as the
mean of the AUROC and Average Precision (AP). On the external test set, adU-Net
achieves the best average score of 0.618, outperforming the baseline nnU-Net
model (0.605). The results demonstrate that incorporating anomaly detection
into segmentation improves generalization and performance, particularly with
ADC-based anomaly maps, offering a promising direction for automated csPCa
identification.

</details>


### [107] [A simple and effective approach for body part recognition on CT scans based on projection estimation](https://arxiv.org/abs/2504.21810)
*Franko Hrzic,Mohammadreza Movahhedi,Ophelie Lavoie-Gagne,Ata Kiapour*

Main category: cs.CV

TL;DR: 提出了一种基于2D X射线估计的3D CT扫描方法，用于高效识别14个身体区域，显著优于其他方法。


<details>
  <summary>Details</summary>
Motivation: CT数据标注复杂且耗时，现有方法常忽略多区域信息，需更高效解决方案。

Method: 利用2D X射线估计3D CT扫描，识别14个身体区域，并与2.5D、3D及基础模型对比。

Result: EffNet-B0模型表现最佳，F1-Score达0.980±0.016，显著优于其他方法。

Conclusion: 该方法简单高效，适用于构建高质量医学数据集。

Abstract: It is well known that machine learning models require a high amount of
annotated data to obtain optimal performance. Labelling Computed Tomography
(CT) data can be a particularly challenging task due to its volumetric nature
and often missing and$/$or incomplete associated meta-data. Even inspecting one
CT scan requires additional computer software, or in the case of programming
languages $-$ additional programming libraries. This study proposes a simple,
yet effective approach based on 2D X-ray-like estimation of 3D CT scans for
body region identification. Although body region is commonly associated with
the CT scan, it often describes only the focused major body region neglecting
other anatomical regions present in the observed CT. In the proposed approach,
estimated 2D images were utilized to identify 14 distinct body regions,
providing valuable information for constructing a high-quality medical dataset.
To evaluate the effectiveness of the proposed method, it was compared against
2.5D, 3D and foundation model (MI2) based approaches. Our approach outperformed
the others, where it came on top with statistical significance and F1-Score for
the best-performing model EffNet-B0 of 0.980 $\pm$ 0.016 in comparison to the
0.840 $\pm$ 0.114 (2.5D DenseNet-161), 0.854 $\pm$ 0.096 (3D VoxCNN), and 0.852
$\pm$ 0.104 (MI2 foundation model). The utilized dataset comprised three
different clinical centers and counted 15,622 CT scans (44,135 labels).

</details>


### [108] [Why Compress What You Can Generate? When GPT-4o Generation Ushers in Image Compression Fields](https://arxiv.org/abs/2504.21814)
*Yixin Gao,Xiaohan Pan,Xin Li,Zhibo Chen*

Main category: cs.CV

TL;DR: 论文探讨了利用AIGC基础模型（如GPT-4o）进行图像压缩的新范式，通过文本或多模态编码生成图像，而非传统像素级压缩。


<details>
  <summary>Details</summary>
Motivation: AIGC基础模型的快速发展为图像压缩提供了新思路，尤其是GPT-4o的跨模态生成能力，激发了探索其在图像压缩领域的潜力。

Method: 研究了文本编码和多模态编码（文本+极低分辨率图像）两种范式，提出结构光栅扫描提示工程机制，将图像转化为文本空间作为生成条件。

Result: 实验表明，该方法在超低比特率下性能优于现有多模态/生成式图像压缩方法。

Conclusion: AIGC生成在图像压缩领域具有巨大潜力，为未来研究提供了新方向。

Abstract: The rapid development of AIGC foundation models has revolutionized the
paradigm of image compression, which paves the way for the abandonment of most
pixel-level transform and coding, compelling us to ask: why compress what you
can generate if the AIGC foundation model is powerful enough to faithfully
generate intricate structure and fine-grained details from nothing more than
some compact descriptors, i.e., texts, or cues. Fortunately, recent GPT-4o
image generation of OpenAI has achieved impressive cross-modality generation,
editing, and design capabilities, which motivates us to answer the above
question by exploring its potential in image compression fields. In this work,
we investigate two typical compression paradigms: textual coding and multimodal
coding (i.e., text + extremely low-resolution image), where all/most
pixel-level information is generated instead of compressing via the advanced
GPT-4o image generation function. The essential challenge lies in how to
maintain semantic and structure consistency during the decoding process. To
overcome this, we propose a structure raster-scan prompt engineering mechanism
to transform the image into textual space, which is compressed as the condition
of GPT-4o image generation. Extensive experiments have shown that the
combination of our designed structural raster-scan prompts and GPT-4o's image
generation function achieved the impressive performance compared with recent
multimodal/generative image compression at ultra-low bitrate, further
indicating the potential of AIGC generation in image compression fields.

</details>


### [109] [Early Exit and Multi Stage Knowledge Distillation in VLMs for Video Summarization](https://arxiv.org/abs/2504.21831)
*Anas Anwarul Haq Khan,Utkarsh Verma,Prateek Chanda,Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: DEEVISum是一种轻量高效的视觉语言模型，用于视频摘要，结合多模态提示和多阶段知识蒸馏（MSKD）与早期退出（EE），在性能和效率间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 设计一个轻量且高效的视觉语言模型，用于视频摘要任务，同时保持高性能和低计算成本。

Method: 结合多模态提示（文本和音频信号），采用多阶段知识蒸馏（MSKD）和早期退出（EE）技术。

Result: 在TVSum数据集上，最佳模型（PaLI Gemma2 3B + MSKD）F1得分为61.1，性能接近更大模型，同时计算成本更低。MSKD比基线蒸馏提升1.33% F1，EE减少21%推理时间。

Conclusion: DEEVISum在视频摘要任务中高效且性能优越，代码和数据集已公开以支持进一步研究。

Abstract: We introduce DEEVISum (Distilled Early Exit Vision language model for
Summarization), a lightweight, efficient, and scalable vision language model
designed for segment wise video summarization. Leveraging multi modal prompts
that combine textual and audio derived signals, DEEVISum incorporates Multi
Stage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance
between performance and efficiency. MSKD offers a 1.33% absolute F1 improvement
over baseline distillation (0.5%), while EE reduces inference time by
approximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset,
our best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing
the performance of significantly larger models, all while maintaining a lower
computational footprint. We publicly release our code and processed dataset to
support further research.

</details>


### [110] [3D Stylization via Large Reconstruction Model](https://arxiv.org/abs/2504.21836)
*Ipek Oztas,Duygu Ceylan,Aysegul Dundar*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练或优化的3D外观风格化方法，利用大型重建模型中的注意力机制实现高效高质量的风格迁移。


<details>
  <summary>Details</summary>
Motivation: 随着文本或图像引导的3D生成器的发展，用户对生成过程的控制需求增加，尤其是外观风格化。

Method: 通过分析大型重建模型中的注意力块，发现其能捕捉外观特征，从而通过注入风格图像特征实现风格迁移。

Result: 方法在3D外观风格化上表现优异，显著提升效率并保持高质量视觉效果。

Conclusion: 该方法简单有效，为3D生成的外观控制提供了新思路。

Abstract: With the growing success of text or image guided 3D generators, users demand
more control over the generation process, appearance stylization being one of
them. Given a reference image, this requires adapting the appearance of a
generated 3D asset to reflect the visual style of the reference while
maintaining visual consistency from multiple viewpoints. To tackle this
problem, we draw inspiration from the success of 2D stylization methods that
leverage the attention mechanisms in large image generation models to capture
and transfer visual style. In particular, we probe if large reconstruction
models, commonly used in the context of 3D generation, has a similar
capability. We discover that the certain attention blocks in these models
capture the appearance specific features. By injecting features from a visual
style image to such blocks, we develop a simple yet effective 3D appearance
stylization method. Our method does not require training or test time
optimization. Through both quantitative and qualitative evaluations, we
demonstrate that our approach achieves superior results in terms of 3D
appearance stylization, significantly improving efficiency while maintaining
high-quality visual outcomes.

</details>


### [111] [Active Light Modulation to Counter Manipulation of Speech Visual Content](https://arxiv.org/abs/2504.21846)
*Hadleigh Schwartz,Xiaofeng Yan,Charles J. Carver,Xia Zhou*

Main category: cs.CV

TL;DR: Spotlight是一种低开销、非侵入性的系统，通过动态物理签名保护实时演讲视频免受视觉伪造。


<details>
  <summary>Details</summary>
Motivation: 高知名度演讲视频易受伪造，因其可访问性和影响力。

Method: 利用调制光嵌入动态物理签名，生成紧凑的、姿态不变的视频特征，并通过光学调制方案实现高比特率嵌入。

Result: 实验显示Spotlight在检测伪造视频时AUC≥0.99，真阳性率100%，且对各种录制条件和攻击具有高鲁棒性。

Conclusion: Spotlight为实时演讲视频提供了高效且安全的防伪造解决方案。

Abstract: High-profile speech videos are prime targets for falsification, owing to
their accessibility and influence. This work proposes Spotlight, a low-overhead
and unobtrusive system for protecting live speech videos from visual
falsification of speaker identity and lip and facial motion. Unlike predominant
falsification detection methods operating in the digital domain, Spotlight
creates dynamic physical signatures at the event site and embeds them into all
video recordings via imperceptible modulated light. These physical signatures
encode semantically-meaningful features unique to the speech event, including
the speaker's identity and facial motion, and are cryptographically-secured to
prevent spoofing. The signatures can be extracted from any video downstream and
validated against the portrayed speech content to check its integrity. Key
elements of Spotlight include (1) a framework for generating extremely compact
(i.e., 150-bit), pose-invariant speech video features, based on
locality-sensitive hashing; and (2) an optical modulation scheme that embeds
>200 bps into video while remaining imperceptible both in video and live.
Prototype experiments on extensive video datasets show Spotlight achieves AUCs
$\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified
videos. Further, Spotlight is highly robust across recording conditions, video
post-processing techniques, and white-box adversarial attacks on its video
feature extraction methodologies.

</details>


### [112] [Differentiable Room Acoustic Rendering with Multi-View Vision Priors](https://arxiv.org/abs/2504.21847)
*Derong Jin,Ruohan Gao*

Main category: cs.CV

TL;DR: AV-DAR框架通过结合视觉线索和声学束追踪，实现了高效、可解释且准确的房间声学渲染，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖数据密集型学习模型或计算昂贵的物理建模，AV-DAR旨在解决这一问题。

Method: 利用多视角图像提取视觉线索，结合声学束追踪进行物理建模。

Result: 在六个真实环境中表现优异，性能接近数据量多10倍的模型，相对提升16.6%至50.9%。

Conclusion: AV-DAR为房间声学渲染提供了一种高效且准确的解决方案。

Abstract: An immersive acoustic experience enabled by spatial audio is just as crucial
as the visual aspect in creating realistic virtual environments. However,
existing methods for room impulse response estimation rely either on
data-demanding learning-based models or computationally expensive physics-based
modeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic
Rendering (AV-DAR), a framework that leverages visual cues extracted from
multi-view images and acoustic beam tracing for physics-based room acoustic
rendering. Experiments across six real-world environments from two datasets
demonstrate that our multimodal, physics-based approach is efficient,
interpretable, and accurate, significantly outperforming a series of prior
methods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves
comparable performance to models trained on 10 times more data while delivering
relative gains ranging from 16.6% to 50.9% when trained at the same scale.

</details>


### [113] [COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning](https://arxiv.org/abs/2504.21850)
*Xindi Wu,Hee Seung Hwang,Polina Kirichenko,Olga Russakovsky*

Main category: cs.CV

TL;DR: COMPACT提出了一种通过控制训练数据的组合复杂性来提升多模态大语言模型（MLLMs）在复杂视觉语言任务中表现的方法。


<details>
  <summary>Details</summary>
Motivation: MLLMs在简单任务上表现良好，但在需要多能力组合的复杂任务中表现不佳，传统视觉指令调优（VIT）未关注训练数据的组合复杂性。

Method: COMPACT生成一个明确控制训练数据组合复杂性的数据集，使MLLMs能更高效地学习复杂能力。

Result: COMPACT在数据量仅为LLaVA-665k VIT的10%时，性能相当甚至更优，尤其在需要多能力的复杂任务上表现显著提升（如MMStar和MM-Vet）。

Conclusion: COMPACT提供了一种可扩展且数据高效的方法，显著提升了MLLMs在复杂视觉语言任务中的表现。

Abstract: Multimodal Large Language Models (MLLMs) excel at simple vision-language
tasks but struggle when faced with complex tasks that require multiple
capabilities, such as simultaneously recognizing objects, counting them, and
understanding their spatial relationships. This might be partially the result
of the fact that Visual Instruction Tuning (VIT), a critical training step for
MLLMs, has traditionally focused on scaling data volume, but not the
compositional complexity of training examples. We propose COMPACT
(COMPositional Atomic-to-complex visual Capability Tuning), which generates a
training dataset explicitly controlling for the compositional complexity of the
training examples. The data from COMPACT allows MLLMs to train on combinations
of atomic capabilities to learn complex capabilities more efficiently. Across
all benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT
while using less than 10% of its data budget, and even outperforms it on
several, especially those involving complex multi-capability tasks. For
example, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0%
improvement on MM-Vet compared to the full-scale VIT on particularly complex
questions that require four or more atomic capabilities. COMPACT offers a
scalable, data-efficient, visual compositional tuning recipe to improve on
complex visual-language tasks.

</details>


### [114] [A Survey of Interactive Generative Video](https://arxiv.org/abs/2504.21853)
*Jiwen Yu,Yiran Qin,Haoxuan Che,Quande Liu,Xintao Wang,Pengfei Wan,Di Zhang,Kun Gai,Hao Chen,Xihui Liu*

Main category: cs.CV

TL;DR: 本文定义了交互式生成视频（IGV）技术，并探讨了其在游戏、具身AI和自动驾驶领域的应用。提出了一个包含五个模块的框架，并分析了技术挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 满足对高质量、交互式视频内容的需求，推动IGV技术的发展。

Method: 通过调查当前IGV应用，提出一个包含生成、控制、记忆、动态和智能五个模块的框架。

Result: 总结了IGV在多个领域的应用潜力，并指出了技术挑战和未来研究方向。

Conclusion: 系统分析有助于推动IGV技术的发展，实现更复杂和实用的应用。

Abstract: Interactive Generative Video (IGV) has emerged as a crucial technology in
response to the growing demand for high-quality, interactive video content
across various domains. In this paper, we define IGV as a technology that
combines generative capabilities to produce diverse high-quality video content
with interactive features that enable user engagement through control signals
and responsive feedback. We survey the current landscape of IGV applications,
focusing on three major domains: 1) gaming, where IGV enables infinite
exploration in virtual worlds; 2) embodied AI, where IGV serves as a
physics-aware environment synthesizer for training agents in multimodal
interaction with dynamically evolving scenes; and 3) autonomous driving, where
IGV provides closed-loop simulation capabilities for safety-critical testing
and validation. To guide future development, we propose a comprehensive
framework that decomposes an ideal IGV system into five essential modules:
Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we
systematically analyze the technical challenges and future directions in
realizing each component for an ideal IGV system, such as achieving real-time
generation, enabling open-domain control, maintaining long-term coherence,
simulating accurate physics, and integrating causal reasoning. We believe that
this systematic analysis will facilitate future research and development in the
field of IGV, ultimately advancing the technology toward more sophisticated and
practical applications.

</details>


### [115] [ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction](https://arxiv.org/abs/2504.21855)
*Qihao Liu,Ju He,Qihang Yu,Liang-Chieh Chen,Alan Yuille*

Main category: cs.CV

TL;DR: ReVision是一个结合3D物理知识的视频生成框架，显著提升了复杂运动和交互视频的生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有视频生成模型在复杂运动和交互方面的不足。

Method: 通过三阶段流程：粗视频生成、3D特征提取与物理优化、反馈生成高质量视频。

Result: 在Stable Video Diffusion上验证，ReVision显著提升运动保真度和连贯性，性能优于更大模型。

Conclusion: 结合3D物理知识的小模型也能高效生成复杂视频，为物理合理视频生成提供新思路。

Abstract: In recent years, video generation has seen significant advancements. However,
challenges still persist in generating complex motions and interactions. To
address these challenges, we introduce ReVision, a plug-and-play framework that
explicitly integrates parameterized 3D physical knowledge into a pretrained
conditional video generation model, significantly enhancing its ability to
generate high-quality videos with complex motion and interactions.
Specifically, ReVision consists of three stages. First, a video diffusion model
is used to generate a coarse video. Next, we extract a set of 2D and 3D
features from the coarse video to construct a 3D object-centric representation,
which is then refined by our proposed parameterized physical prior model to
produce an accurate 3D motion sequence. Finally, this refined motion sequence
is fed back into the same video diffusion model as additional conditioning,
enabling the generation of motion-consistent videos, even in scenarios
involving complex actions and interactions. We validate the effectiveness of
our approach on Stable Video Diffusion, where ReVision significantly improves
motion fidelity and coherence. Remarkably, with only 1.5B parameters, it even
outperforms a state-of-the-art video generation model with over 13B parameters
on complex video generation by a substantial margin. Our results suggest that,
by incorporating 3D physical knowledge, even a relatively small video diffusion
model can generate complex motions and interactions with greater realism and
controllability, offering a promising solution for physically plausible video
generation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [116] [A Formalism for Optimal Search with Dynamic Heuristics](https://arxiv.org/abs/2504.21131)
*Remo Christen,Florian Pommerening,Clemens Büchner,Malte Helmert*

Main category: cs.AI

TL;DR: 本文研究了动态启发式在A*算法中的应用，提出了一个通用框架，并证明了其最优性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在使用动态启发式时忽略了其复杂性，本文旨在解决这一问题。

Method: 提出了一个通用算法框架，并具体实例化为动态启发式的A*算法。

Result: 证明了该框架的最优性，并将经典规划方法视为其特例。

Conclusion: 本文为动态启发式搜索提供了理论基础和通用解决方案。

Abstract: While most heuristics studied in heuristic search depend only on the state,
some accumulate information during search and thus also depend on the search
history. Various existing approaches use such dynamic heuristics in
$\mathrm{A}^*$-like algorithms and appeal to classic results for $\mathrm{A}^*$
to show optimality. However, doing so ignores the complexities of searching
with a mutable heuristic. In this paper we formalize the idea of dynamic
heuristics and use them in a generic algorithm framework. We study a particular
instantiation that models $\mathrm{A}^*$ with dynamic heuristics and show
general optimality results. Finally we show how existing approaches from
classical planning can be viewed as special cases of this instantiation, making
it possible to directly apply our optimality results.

</details>


### [117] [AffectEval: A Modular and Customizable Framework for Affective Computing](https://arxiv.org/abs/2504.21184)
*Emily Zhou,Khushboo Khatri,Yixue Zhao,Bhaskar Krishnamachari*

Main category: cs.AI

TL;DR: AffectEval是一个模块化、可定制的框架，旨在减少情感计算流水线开发中的手动工作和重复劳动，验证显示其能减少90%的编程工作量。


<details>
  <summary>Details</summary>
Motivation: 情感计算领域缺乏支持多模态、多领域情感识别应用的软件框架，导致开发流水线时重复劳动。

Method: 引入AffectEval框架，通过模块化和可定制化设计减少手动工作，并通过复制先前实验验证其效果。

Result: AffectEval能减少高达90%的编程工作量（以代码行数减少衡量）。

Conclusion: AffectEval显著降低了情感计算流水线开发的复杂性和工作量，具有广泛的应用潜力。

Abstract: The field of affective computing focuses on recognizing, interpreting, and
responding to human emotions, and has broad applications across education,
child development, and human health and wellness. However, developing affective
computing pipelines remains labor-intensive due to the lack of software
frameworks that support multimodal, multi-domain emotion recognition
applications. This often results in redundant effort when building pipelines
for different applications. While recent frameworks attempt to address these
challenges, they remain limited in reducing manual effort and ensuring
cross-domain generalizability. We introduce AffectEval, a modular and
customizable framework to facilitate the development of affective computing
pipelines while reducing the manual effort and duplicate work involved in
developing such pipelines. We validate AffectEval by replicating prior
affective computing experiments, and we demonstrate that our framework reduces
programming effort by up to 90%, as measured by the reduction in raw lines of
code.

</details>


### [118] [Theoretical Foundations for Semantic Cognition in Artificial Intelligence](https://arxiv.org/abs/2504.21218)
*Sebastian Dumbrava*

Main category: cs.AI

TL;DR: 本文提出了一种基于结构化语义状态的模块化认知架构，用于人工智能，通过动态语言表达集合定义信念状态，并引入可操作的概念框架。


<details>
  <summary>Details</summary>
Motivation: 旨在构建一种能够自我调节、具备反思和目标导向思维的认知代理，结合哲学、认知科学和神经科学的理论。

Method: 开发了一个分层的框架，包括语义惰性的认知状态（epistemic vacuum）和生成性结构（Null Tower），适用于符号和神经网络系统。

Result: 提出了一种可实现的认知架构，支持代理的结构化、可解释的推理、记忆和信念调节。

Conclusion: 该工作为构建具备高级认知能力的代理提供了理论基础和实现路径。

Abstract: This monograph presents a modular cognitive architecture for artificial
intelligence grounded in the formal modeling of belief as structured semantic
state. Belief states are defined as dynamic ensembles of linguistic expressions
embedded within a navigable manifold, where operators enable assimilation,
abstraction, nullification, memory, and introspection. Drawing from philosophy,
cognitive science, and neuroscience, we develop a layered framework that
enables self-regulating epistemic agents capable of reflective, goal-directed
thought. At the core of this framework is the epistemic vacuum: a class of
semantically inert cognitive states that serves as the conceptual origin of
belief space. From this foundation, the Null Tower arises as a generative
structure recursively built through internal representational capacities. The
theoretical constructs are designed to be implementable in both symbolic and
neural systems, including large language models, hybrid agents, and adaptive
memory architectures. This work offers a foundational substrate for
constructing agents that reason, remember, and regulate their beliefs in
structured, interpretable ways.

</details>


### [119] [Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2504.21277)
*Guanghao Zhou,Panjia Qiu,Cen Chen,Jie Wang,Zheming Yang,Jian Xu,Minghui Qiu*

Main category: cs.AI

TL;DR: 综述探讨了强化学习在多模态大语言模型推理中的应用，总结了关键算法、奖励机制及实际应用，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在多模态输入推理方面仍面临挑战，强化学习的整合有望提升其推理能力。

Method: 系统回顾了基于强化学习的推理方法，包括无价值与基于价值的方法，优化推理轨迹和多模态信息对齐。

Result: 总结了基准数据集、评估协议及现有局限，如稀疏奖励和跨模态推理效率问题。

Conclusion: 提出了未来研究方向，旨在解决当前瓶颈，为研究者提供结构化指南。

Abstract: The integration of reinforcement learning (RL) into the reasoning
capabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as
a transformative research direction. While MLLMs significantly extend Large
Language Models (LLMs) to handle diverse modalities such as vision, audio, and
video, enabling robust reasoning across multimodal inputs remains a major
challenge. This survey systematically reviews recent advances in RL-based
reasoning for MLLMs, covering key algorithmic designs, reward mechanism
innovations, and practical applications. We highlight two main RL
paradigms--value-free and value-based methods--and analyze how RL enhances
reasoning abilities by optimizing reasoning trajectories and aligning
multimodal information. Furthermore, we provide an extensive overview of
benchmark datasets, evaluation protocols, and existing limitations, and propose
future research directions to address current bottlenecks such as sparse
rewards, inefficient cross-modal reasoning, and real-world deployment
constraints. Our goal is to offer a comprehensive and structured guide to
researchers interested in advancing RL-based reasoning in the multimodal era.

</details>


### [120] [Phi-4-reasoning Technical Report](https://arxiv.org/abs/2504.21318)
*Marah Abdin,Sahaj Agarwal,Ahmed Awadallah,Vidhisha Balachandran,Harkirat Behl,Lingjiao Chen,Gustavo de Rosa,Suriya Gunasekar,Mojan Javaheripi,Neel Joshi,Piero Kauffmann,Yash Lara,Caio César Teodoro Mendes,Arindam Mitra,Besmira Nushi,Dimitris Papailiopoulos,Olli Saarikivi,Shital Shah,Vaishnavi Shrivastava,Vibhav Vineet,Yue Wu,Safoora Yousefi,Guoqing Zheng*

Main category: cs.AI

TL;DR: Phi-4-reasoning 是一个14B参数的推理模型，通过监督微调和强化学习优化，在复杂推理任务中表现优异，甚至超越更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过数据精选和监督微调提升推理模型的性能，并探索强化学习对模型表现的进一步优化。

Method: 使用精选的“可教学”提示和监督微调训练Phi-4，并通过强化学习生成更长的推理链以提升性能。

Result: Phi-4-reasoning及其强化学习变体在多项推理任务中表现优异，接近或超越更大规模的模型。

Conclusion: 数据精选和监督微调对推理模型性能至关重要，强化学习可进一步优化表现，同时评估方法需改进。

Abstract: We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that
achieves strong performance on complex reasoning tasks. Trained via supervised
fine-tuning of Phi-4 on carefully curated set of "teachable" prompts-selected
for the right level of complexity and diversity-and reasoning demonstrations
generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains
that effectively leverage inference-time compute. We further develop
Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based
reinforcement learning that offers higher performance by generating longer
reasoning traces. Across a wide range of reasoning tasks, both models
outperform significantly larger open-weight models such as
DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full
DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and
scientific reasoning, coding, algorithmic problem solving, planning, and
spatial understanding. Interestingly, we observe a non-trivial transfer of
improvements to general-purpose benchmarks as well. In this report, we provide
insights into our training data, our training methodologies, and our
evaluations. We show that the benefit of careful data curation for supervised
fine-tuning (SFT) extends to reasoning language models, and can be further
amplified by reinforcement learning (RL). Finally, our evaluation points to
opportunities for improving how we assess the performance and robustness of
reasoning models.

</details>


### [121] [IRL Dittos: Embodied Multimodal AI Agent Interactions in Open Spaces](https://arxiv.org/abs/2504.21347)
*Seonghee Lee,Denae Ford,John Tang,Sasa Junuzovic,Asta Roseway,Ed Cutrell,Kori Inkpen*

Main category: cs.AI

TL;DR: IRL Ditto是一个AI驱动的实体代理，用于在共享办公空间中代表远程同事，促进实时互动。研究发现，其增强社交关系的能力取决于用户与代理来源的既有关系基础。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过实体代理（IRL Ditto）在共享办公空间中模拟远程同事的存在，以增强同事间的互动和关系。

Method: 通过为期四天的研究，评估IRL Ditto在不同社交熟悉度下模拟存在和促进有意义互动的能力。

Result: 研究发现，IRL Ditto增强社交关系的能力与用户和代理来源的既有关系密切相关。

Conclusion: 研究揭示了实体代理在丰富分布式团队工作场所动态中的潜在作用。

Abstract: We introduce the In Real Life (IRL) Ditto, an AI-driven embodied agent
designed to represent remote colleagues in shared office spaces, creating
opportunities for real-time exchanges even in their absence. IRL Ditto offers a
unique hybrid experience by allowing in-person colleagues to encounter a
digital version of their remote teammates, initiating greetings, updates, or
small talk as they might in person. Our research question examines: How can the
IRL Ditto influence interactions and relationships among colleagues in a shared
office space? Through a four-day study, we assessed IRL Ditto's ability to
strengthen social ties by simulating presence and enabling meaningful
interactions across different levels of social familiarity. We find that
enhancing social relationships depended deeply on the foundation of the
relationship participants had with the source of the IRL Ditto. This study
provides insights into the role of embodied agents in enriching workplace
dynamics for distributed teams.

</details>


### [122] [ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning](https://arxiv.org/abs/2504.21370)
*Jingyang Yi,Jiazheng Wang*

Main category: cs.AI

TL;DR: 论文提出ShorterBetter方法，通过强化学习让推理模型自动找到最优推理长度，减少输出长度80%同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型如OpenAI o3和DeepSeek-R1在复杂任务中表现优异，但过长的推理路径导致效率低下，需要优化。

Method: 采用强化学习方法，定义样本最优长度（SOL），动态引导模型生成最短正确输出。

Result: 在DeepSeek-Distill-Qwen-1.5B模型上，输出长度减少80%，准确性不变。

Conclusion: 过长的推理路径常导致方向迷失，表明推理模型的扩展CoT具有高度可压缩性。

Abstract: Reasoning models such as OpenAI o3 and DeepSeek-R1 have demonstrated strong
performance on reasoning-intensive tasks through extended Chain-of-Thought
(CoT) prompting. While longer reasoning traces can facilitate a more thorough
exploration of solution paths for complex problems, researchers have observed
that these models often "overthink", leading to inefficient inference. In this
paper, we introduce ShorterBetter, a simple yet effective reinforcement
learning methed that enables reasoning language models to discover their own
optimal CoT lengths without human intervention. By sampling multiple outputs
per problem and defining the Sample Optimal Length (SOL) as the shortest
correct response among all the outputs, our method dynamically guides the model
toward optimal inference lengths. Applied to the DeepSeek-Distill-Qwen-1.5B
model, ShorterBetter achieves up to an 80% reduction in output length on both
in-domain and out-of-domain reasoning tasks while maintaining accuracy. Our
analysis shows that overly long reasoning traces often reflect loss of
reasoning direction, and thus suggests that the extended CoT produced by
reasoning models is highly compressible.

</details>


### [123] [NGENT: Next-Generation AI Agents Must Integrate Multi-Domain Abilities to Achieve Artificial General Intelligence](https://arxiv.org/abs/2504.21433)
*Zhicong Li,Hangyu Mao,Jiangjin Yin,Mingzhe Xing,Zhiwei Xu,Yuanxing Zhang,Yang Xiao*

Main category: cs.AI

TL;DR: 论文主张下一代AI代理（NGENT）需整合跨领域能力以推进通用人工智能（AGI）发展。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理虽在特定任务（如机器人、角色扮演、工具使用）中表现优异，但局限于狭窄领域。

Method: 提出将各领域优势整合为统一框架，涵盖文本、视觉、机器人、强化学习、情感智能等。

Result: 跨领域整合不仅可行，且是实现人类智能般灵活性的关键。

Conclusion: 开发多领域AI代理是实现AGI的重要步骤。

Abstract: This paper argues that the next generation of AI agent (NGENT) should
integrate across-domain abilities to advance toward Artificial General
Intelligence (AGI). Although current AI agents are effective in specialized
tasks such as robotics, role-playing, and tool-using, they remain confined to
narrow domains. We propose that future AI agents should synthesize the
strengths of these specialized systems into a unified framework capable of
operating across text, vision, robotics, reinforcement learning, emotional
intelligence, and beyond. This integration is not only feasible but also
essential for achieving the versatility and adaptability that characterize
human intelligence. The convergence of technologies across AI domains, coupled
with increasing user demand for cross-domain capabilities, suggests that such
integration is within reach. Ultimately, the development of these versatile
agents is a critical step toward realizing AGI. This paper explores the
rationale for this shift, potential pathways for achieving it.

</details>


### [124] [A Study on Group Decision Making Problem Based on Fuzzy Reasoning and Bayesian Networks](https://arxiv.org/abs/2504.21568)
*Shui-jin Rong,Wei Guo,Da-qing Zhang*

Main category: cs.AI

TL;DR: 提出一种结合模糊推理和贝叶斯网络的群决策系统，用于多目标属性问题，通过模糊规则库和贝叶斯网络优化决策，实验表明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决多目标属性群决策问题中的定量挑战，如尺度差异和专家语言变量。

Method: 构建模糊规则库和分层贝叶斯网络，动态优化条件概率表，建模多维指标的非线性相关性。

Result: 在综合评价案例中，分类准确率达86.0%，F1值比传统方法提高53.4%。

Conclusion: 该方法在规则构建和排序一致性上表现优异，适用于多样化群决策场景。

Abstract: Aiming at the group decision - making problem with multi - objective
attributes, this study proposes a group decision - making system that
integrates fuzzy inference and Bayesian network. A fuzzy rule base is
constructed by combining threshold values, membership functions, expert
experience, and domain knowledge to address quantitative challenges such as
scale differences and expert linguistic variables. A hierarchical Bayesian
network is designed, featuring a directed acyclic graph with nodes selected by
experts, and maximum likelihood estimation is used to dynamically optimize the
conditional probability table, modeling the nonlinear correlations among
multidimensional indices for posterior probability aggregation. In a
comprehensive student evaluation case, this method is compared with the
traditional weighted scoring approach. The results indicate that the proposed
method demonstrates effectiveness in both rule criterion construction and
ranking consistency, with a classification accuracy of 86.0% and an F1 value
improvement of 53.4% over the traditional method. Additionally, computational
experiments on real - world datasets across various group decision scenarios
assess the method's performance and robustness, providing evidence of its
reliability in diverse contexts.

</details>


### [125] [Designing Control Barrier Function via Probabilistic Enumeration for Safe Reinforcement Learning Navigation](https://arxiv.org/abs/2504.21643)
*Luca Marzari,Francesco Trotti,Enrico Marchesini,Alessandro Farinelli*

Main category: cs.AI

TL;DR: 提出了一种基于神经网络验证的分层控制框架，用于确保强化学习导航策略的安全性，并在仿真和真实机器人上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在动态和不确定的真实环境中，实现安全的自主导航系统对机器人部署至关重要。

Method: 利用概率枚举识别不安全操作区域，构建基于控制屏障函数（CBF）的安全控制层，适用于任意策略。

Result: 实验表明，该方法能纠正不安全行为，同时保持高效导航性能。

Conclusion: 分层验证系统有望在复杂场景中实现安全、鲁棒的导航行为。

Abstract: Achieving safe autonomous navigation systems is critical for deploying robots
in dynamic and uncertain real-world environments. In this paper, we propose a
hierarchical control framework leveraging neural network verification
techniques to design control barrier functions (CBFs) and policy correction
mechanisms that ensure safe reinforcement learning navigation policies. Our
approach relies on probabilistic enumeration to identify unsafe regions of
operation, which are then used to construct a safe CBF-based control layer
applicable to arbitrary policies. We validate our framework both in simulation
and on a real robot, using a standard mobile robot benchmark and a highly
dynamic aquatic environmental monitoring task. These experiments demonstrate
the ability of the proposed solution to correct unsafe actions while preserving
efficient navigation behavior. Our results show the promise of developing
hierarchical verification-based systems to enable safe and robust navigation
behaviors in complex scenarios.

</details>


### [126] [AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization](https://arxiv.org/abs/2504.21659)
*Haotian Luo,Haiying He,Yibo Wang,Jinluan Yang,Rui Liu,Naiqiang Tan,Xiaochun Cao,Dacheng Tao,Li Shen*

Main category: cs.AI

TL;DR: 论文提出了一种自适应推理框架，通过结合长短推理路径和双层偏好训练，显著降低推理成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 研究发现长推理模型在不同问题上的效果差异显著，需要根据输入调整推理深度，但现有方法仅减少冗余路径，未能探索更高效策略。

Method: 提出两阶段框架：1）构建混合推理模型，结合长短推理路径；2）通过双层偏好训练选择合适推理风格并优化推理简洁性。

Result: 实验表明，该方法在五个数学数据集上平均推理长度减少50%以上，显著降低推理成本且保持性能。

Conclusion: 自适应策略能有效优化大语言模型的推理效率，未来可进一步探索其潜力。

Abstract: Recently, long-thought reasoning models achieve strong performance on complex
reasoning tasks, but often incur substantial inference overhead, making
efficiency a critical concern. Our empirical analysis reveals that the benefit
of using Long-CoT varies across problems: while some problems require elaborate
reasoning, others show no improvement, or even degraded accuracy. This
motivates adaptive reasoning strategies that tailor reasoning depth to the
input. However, prior work primarily reduces redundancy within long reasoning
paths, limiting exploration of more efficient strategies beyond the Long-CoT
paradigm. To address this, we propose a novel two-stage framework for adaptive
and efficient reasoning. First, we construct a hybrid reasoning model by
merging long and short CoT models to enable diverse reasoning styles. Second,
we apply bi-level preference training to guide the model to select suitable
reasoning styles (group-level), and prefer concise and correct reasoning within
each style group (instance-level). Experiments demonstrate that our method
significantly reduces inference costs compared to other baseline approaches,
while maintaining performance. Notably, on five mathematical datasets, the
average length of reasoning is reduced by more than 50%, highlighting the
potential of adaptive strategies to optimize reasoning efficiency in large
language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1

</details>


### [127] [Extension-ranking Semantics for Abstract Argumentation Preprint](https://arxiv.org/abs/2504.21683)
*Kenneth Skiba,Tjitze Rienstra,Matthias Thimm,Jesse Heyninck,Gabriele Kern-Isberner*

Main category: cs.AI

TL;DR: 本文提出了一种基于论证可接受性的框架，用于对抽象论证中的论证集进行排序。通过扩展Dung的语义为扩展排序语义，生成论证集的预序，并引入行为良好的语义应满足的原则。


<details>
  <summary>Details</summary>
Motivation: 旨在为抽象论证中的论证集提供一种通用的排序方法，以衡量其可接受性。

Method: 扩展Dung的语义为扩展排序语义，引入多个基本原则，并结合简单基础关系构建语义家族。

Result: 提出了一系列扩展排序语义，并评估了其行为。

Conclusion: 该框架为论证集的排序提供了灵活且理论支持的方法。

Abstract: In this paper, we present a general framework for ranking sets of arguments
in abstract argumentation based on their plausibility of acceptance. We present
a generalisation of Dung's extension semantics as extension-ranking semantics,
which induce a preorder over the power set of all arguments, allowing us to
state that one set is "closer" to being acceptable than another. To evaluate
the extension-ranking semantics, we introduce a number of principles that a
well-behaved extension-ranking semantics should satisfy. We consider several
simple base relations, each of which models a single central aspect of
argumentative reasoning. The combination of these base relations provides us
with a family of extension-ranking semantics. We also adapt a number of
approaches from the literature for ranking extensions to be usable in the
context of extension-ranking semantics, and evaluate their behaviour.

</details>


### [128] [Automatic Mapping of AutomationML Files to Ontologies for Graph Queries and Validation](https://arxiv.org/abs/2504.21694)
*Tom Westermann,Malte Ramonat,Johannes Hujer,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: 本文介绍了AutomationML的更新版本体和RDF转换方法，支持工业知识图谱的集成。


<details>
  <summary>Details</summary>
Motivation: AutomationML作为自动化领域的开放数据交换格式，其扩展语义限制了XML工具的适用性，需要更高效的查询和验证方法。

Method: 提供AutomationML的本体概念和RDF转换映射，实现模型到RDF三元组的自动转换。

Result: 研究表明，转换为OWL后，查询和验证能力显著提升。

Conclusion: AutomationML到RDF的转换为工业知识图谱提供了新的可能性。

Abstract: AutomationML has seen widespread adoption as an open data exchange format in
the automation domain. It is an open and vendor neutral standard based on the
extensible markup language XML. However, AutomationML extends XML with
additional semantics, that limit the applicability of common XML-tools for
applications like querying or data validation. This article provides
practitioners with 1) an up-to-date ontology of the concepts in the
AutomationML-standard, as well as 2) a declarative mapping to automatically
transform any AutomationML model into RDF triples. Together, these artifacts
allow practitioners an easy integration of AutomationML information into
industrial knowledge graphs. A study on examples from the automation domain
concludes that transforming AutomationML to OWL opens up new powerful ways for
querying and validation that are impossible without transformation.

</details>


### [129] [Is Intermediate Fusion All You Need for UAV-based Collaborative Perception?](https://arxiv.org/abs/2504.21774)
*Jiuwu Hao,Liguo Sun,Yuting Wan,Yueyang Wu,Ti Xiang,Haolin Song,Pin Lv*

Main category: cs.AI

TL;DR: 提出了一种基于延迟中间融合（LIF）的高效通信协作感知框架，通过交换紧凑的检测结果和特征级融合，减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有无人机协作感知方法未考虑无人机视角特性，导致通信开销大。

Method: 采用延迟中间融合框架（LIF），结合视觉引导位置嵌入（VPE）和基于框的虚拟增强特征（BoBEV），并引入不确定性驱动通信机制。

Result: 实验表明，LIF在低通信带宽下表现优异。

Conclusion: LIF框架高效实用，适用于无人机协作感知。

Abstract: Collaborative perception enhances environmental awareness through inter-agent
communication and is regarded as a promising solution to intelligent
transportation systems. However, existing collaborative methods for Unmanned
Aerial Vehicles (UAVs) overlook the unique characteristics of the UAV
perspective, resulting in substantial communication overhead. To address this
issue, we propose a novel communication-efficient collaborative perception
framework based on late-intermediate fusion, dubbed LIF. The core concept is to
exchange informative and compact detection results and shift the fusion stage
to the feature representation level. In particular, we leverage vision-guided
positional embedding (VPE) and box-based virtual augmented feature (BoBEV) to
effectively integrate complementary information from various agents.
Additionally, we innovatively introduce an uncertainty-driven communication
mechanism that uses uncertainty evaluation to select high-quality and reliable
shared areas. Experimental results demonstrate that our LIF achieves superior
performance with minimal communication bandwidth, proving its effectiveness and
practicality. Code and models are available at https://github.com/uestchjw/LIF.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [130] [Model Connectomes: A Generational Approach to Data-Efficient Language Models](https://arxiv.org/abs/2504.21047)
*Klemen Kotar,Greta Tuckute*

Main category: cs.LG

TL;DR: 论文提出了一种结合进化和个体学习的框架，通过‘外循环’进化塑造‘内循环’学习，使人工网络更接近生物神经网络的特性。在语言任务中，该模型表现优于或与对照组相当。


<details>
  <summary>Details</summary>
Motivation: 生物神经网络通过进化和个体学习形成，而人工神经网络缺乏这种多代约束。研究旨在缩小两者差距。

Method: 提出一个框架，包含‘外循环’进化和‘内循环’学习，训练模型继承‘模型连接组’后接触100M标记的语料库。

Result: 模型在自然语言处理任务及与人类行为和脑数据的对齐上表现优于或与对照组相当。

Conclusion: ‘模型连接组’在低数据环境下作为高效先验，缩小了人工模型与生物神经网络的差距。

Abstract: Biological neural networks are shaped both by evolution across generations
and by individual learning within an organism's lifetime, whereas standard
artificial neural networks undergo a single, large training procedure without
inherited constraints. In this preliminary work, we propose a framework that
incorporates this crucial generational dimension - an "outer loop" of evolution
that shapes the "inner loop" of learning - so that artificial networks better
mirror the effects of evolution and individual learning in biological
organisms. Focusing on language, we train a model that inherits a "model
connectome" from the outer evolution loop before exposing it to a
developmental-scale corpus of 100M tokens. Compared with two closely matched
control models, we show that the connectome model performs better or on par on
natural language processing tasks as well as alignment to human behavior and
brain data. These findings suggest that a model connectome serves as an
efficient prior for learning in low-data regimes - narrowing the gap between
single-generation artificial models and biologically evolved neural networks.

</details>


### [131] [Multimodal Large Language Models for Medicine: A Comprehensive Survey](https://arxiv.org/abs/2504.21051)
*Jiarui Ye,Hao Tang*

Main category: cs.LG

TL;DR: 本文综述了多模态大语言模型（MLLMs）在医疗健康领域的应用，包括背景介绍、三大应用方向、数据模式及挑战与解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着GPT-4的发布，MLLMs在多模态任务中的潜力受到广泛关注，尤其在医疗健康领域的研究需求迫切。

Method: 通过综述330篇相关论文，总结了MLLMs在医疗报告、诊断和治疗中的应用，并分析了六种主流数据模式及其评估基准。

Result: MLLMs在医疗健康领域展现出显著能力，但面临数据、隐私和模型适应性等挑战。

Conclusion: 本文为MLLMs在医疗健康领域的研究提供了系统综述，并提出了解决当前挑战的可行方法。

Abstract: MLLMs have recently become a focal point in the field of artificial
intelligence research. Building on the strong capabilities of LLMs, MLLMs are
adept at addressing complex multi-modal tasks. With the release of GPT-4, MLLMs
have gained substantial attention from different domains. Researchers have
begun to explore the potential of MLLMs in the medical and healthcare domain.
In this paper, we first introduce the background and fundamental concepts
related to LLMs and MLLMs, while emphasizing the working principles of MLLMs.
Subsequently, we summarize three main directions of application within
healthcare: medical reporting, medical diagnosis, and medical treatment. Our
findings are based on a comprehensive review of 330 recent papers in this area.
We illustrate the remarkable capabilities of MLLMs in these domains by
providing specific examples. For data, we present six mainstream modes of data
along with their corresponding evaluation benchmarks. At the end of the survey,
we discuss the challenges faced by MLLMs in the medical and healthcare domain
and propose feasible methods to mitigate or overcome these issues.

</details>


### [132] [NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models](https://arxiv.org/abs/2504.21053)
*Yi Zhou,Wenpeng Xing,Dezhang Kong,Changting Lin,Meng Han*

Main category: cs.LG

TL;DR: 提出一种新方法，通过分析和修改神经元来解除大型语言模型的安全对齐，揭示当前对齐技术的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过修改神经元解除安全对齐，以揭示现有对齐技术的漏洞。

Method: 分三步：神经元激活分析、基于相似性的神经元识别、神经元再学习以移除安全约束。

Result: 实验表明，该方法能以最小微调有效移除安全约束。

Conclusion: 当前对齐技术存在脆弱性，需开发更鲁棒的防御机制。

Abstract: Safety alignment in large language models (LLMs) is achieved through
fine-tuning mechanisms that regulate neuron activations to suppress harmful
content. In this work, we propose a novel approach to induce disalignment by
identifying and modifying the neurons responsible for safety constraints. Our
method consists of three key steps: Neuron Activation Analysis, where we
examine activation patterns in response to harmful and harmless prompts to
detect neurons that are critical for distinguishing between harmful and
harmless inputs; Similarity-Based Neuron Identification, which systematically
locates the neurons responsible for safe alignment; and Neuron Relearning for
Safety Removal, where we fine-tune these selected neurons to restore the
model's ability to generate previously restricted responses. Experimental
results demonstrate that our method effectively removes safety constraints with
minimal fine-tuning, highlighting a critical vulnerability in current alignment
techniques. Our findings underscore the need for robust defenses against
adversarial fine-tuning attacks on LLMs.

</details>


### [133] [Modeling and Performance Analysis for Semantic Communications Based on Empirical Results](https://arxiv.org/abs/2504.21055)
*Shuai Ma,Bin Shen,Chuanhui Zhang,Youlong Wu,Hang Li,Shiyin Li,Guangming Shi,Naofal Al-Dhahir*

Main category: cs.LG

TL;DR: 论文提出了一种Alpha-Beta-Gamma（ABG）公式，用于建模语义通信中端到端性能指标与信噪比（SNR）的关系，并基于此设计了自适应功率控制方案。


<details>
  <summary>Details</summary>
Motivation: 由于深度学习的语义编码器和解码器具有黑盒特性，分析语义通信性能是一个挑战性问题。

Method: 提出ABG公式，适用于图像重建和推理任务，并推导了MS-SSIM与量化输出比特数的闭式关系。基于ABG公式，设计了自适应功率控制方案和最优功率分配方案。

Result: ABG公式能很好地拟合常用深度学习网络（如SCUNet、Vision Transformer），并验证了功率分配方案的有效性。

Conclusion: ABG公式为语义通信提供了首个端到端性能与SNR的理论关系，功率分配方案显著提升了系统能效和服务质量。

Abstract: Due to the black-box characteristics of deep learning based semantic encoders
and decoders, finding a tractable method for the performance analysis of
semantic communications is a challenging problem. In this paper, we propose an
Alpha-Beta-Gamma (ABG) formula to model the relationship between the end-to-end
measurement and SNR, which can be applied for both image reconstruction tasks
and inference tasks. Specifically, for image reconstruction tasks, the proposed
ABG formula can well fit the commonly used DL networks, such as SCUNet, and
Vision Transformer, for semantic encoding with the multi scale-structural
similarity index measure (MS-SSIM) measurement. Furthermore, we find that the
upper bound of the MS-SSIM depends on the number of quantized output bits of
semantic encoders, and we also propose a closed-form expression to fit the
relationship between the MS-SSIM and quantized output bits. To the best of our
knowledge, this is the first theoretical expression between end-to-end
performance metrics and SNR for semantic communications. Based on the proposed
ABG formula, we investigate an adaptive power control scheme for semantic
communications over random fading channels, which can effectively guarantee
quality of service (QoS) for semantic communications, and then design the
optimal power allocation scheme to maximize the energy efficiency of the
semantic communication system. Furthermore, by exploiting the bisection
algorithm, we develop the power allocation scheme to maximize the minimum QoS
of multiple users for OFDMA downlink semantic communication Extensive
simulations verify the effectiveness and superiority of the proposed ABG
formula and power allocation schemes.

</details>


### [134] [A Hamiltonian Higher-Order Elasticity Framework for Dynamic Diagnostics(2HOED)](https://arxiv.org/abs/2504.21062)
*Ngueuleweu Tiwang Gildas*

Main category: cs.LG

TL;DR: 2HOED框架结合机器学习、区块链和因果推理，通过哈密顿力学分析复杂系统的能量动态，揭示线性模型无法捕捉的失效模式和反馈循环。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如机器学习、区块链和因果推理）各自独立，无法全面解析复杂系统的能量动态，2HOED旨在填补这一空白。

Method: 基于经典力学的哈密顿能量模型，扩展至经济学弹性项，分析系统的位置、速度、加速度和弹性变化，生成动态能量图谱。

Result: 2HOED揭示了系统的韧性、临界点和反馈循环，提供了多阶段政策杠杆，优于传统线性模型。

Conclusion: 2HOED为跨学科研究提供了便携、可解释且计算轻量的工具，帮助决策者预测危机并设计适应性政策。

Abstract: Machine learning detects patterns, block chain guarantees trust and
immutability, and modern causal inference identifies directional linkages, yet
none alone exposes the full energetic anatomy of complex systems; the
Hamiltonian Higher Order Elasticity Dynamics(2HOED) framework bridges these
gaps. Grounded in classical mechanics but extended to Economics order
elasticity terms, 2HOED represents economic, social, and physical systems as
energy-based Hamiltonians whose position, velocity, acceleration, and jerk of
elasticity jointly determine systemic power, Inertia, policy sensitivity, and
marginal responses. Because the formalism is scaling free and coordinate
agnostic, it transfers seamlessly from financial markets to climate science,
from supply chain logistics to epidemiology, thus any discipline in which
adaptation and shocks coexist. By embedding standard econometric variables
inside a Hamiltonian, 2HOED enriches conventional economic analysis with
rigorous diagnostics of resilience, tipping points, and feedback loops,
revealing failure modes invisible to linear models. Wavelet spectra, phase
space attractors, and topological persistence diagrams derived from 2HOED
expose multistage policy leverage that machine learning detects only
empirically and block chain secures only after the fact. For economists,
physicians and other scientists, the method opens a new causal energetic
channel linking biological or mechanical elasticity to macro level outcomes.
Portable, interpretable, and computationally light, 2HOED turns data streams
into dynamical energy maps, empowering decision makers to anticipate crises,
design adaptive policies, and engineer robust systems delivering the predictive
punch of AI with the explanatory clarity of physics.

</details>


### [135] [Token-Level Prompt Mixture with Parameter-Free Routing for Federated Domain Generalization](https://arxiv.org/abs/2504.21063)
*Shuai Gong,Chaoran Cui,Xiaolin Dong,Xiushan Nie,Lei Zhu,Xiaojun Chang*

Main category: cs.LG

TL;DR: TRIP提出了一种基于令牌级提示混合的无参数路由框架，用于联邦域泛化（FedDG），通过令牌聚类和最优传输实现高效通信，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决现有FedDG方法中单一全局提示导致的性能下降问题，以及基于MoE的方法中图像级专家分配粗糙和通信成本高的问题。

Method: TRIP采用令牌级提示混合，通过参数免费路由机制（基于令牌聚类和最优传输）分配令牌给特定专家，并合成实例特定提示。

Result: 在四个基准测试中，TRIP实现了最优的泛化性能，每轮通信仅需1K参数。

Conclusion: TRIP通过令牌级专家分配和无参数路由，显著提升了FedDG的性能和通信效率。

Abstract: Federated domain generalization (FedDG) aims to learn a globally
generalizable model from decentralized clients with heterogeneous data while
preserving privacy. Recent studies have introduced prompt learning to adapt
vision-language models (VLMs) in FedDG by learning a single global prompt.
However, such a one-prompt-fits-all learning paradigm typically leads to
performance degradation on personalized samples. Although the mixture of
experts (MoE) offers a promising solution for specialization, existing
MoE-based methods suffer from coarse image-level expert assignment and high
communication costs from parameterized routers. To address these limitations,
we propose TRIP, a Token-level prompt mixture with parameter-free routing
framework for FedDG, which treats multiple prompts as distinct experts. Unlike
existing image-level routing designs, TRIP assigns different tokens within an
image to specific experts. To ensure communication efficiency, TRIP
incorporates a parameter-free routing mechanism based on token clustering and
optimal transport. The instance-specific prompt is then synthesized by
aggregating experts, weighted by the number of tokens assigned to each.
Additionally, TRIP develops an unbiased learning strategy for prompt experts,
leveraging the VLM's zero-shot generalization capability. Extensive experiments
across four benchmarks demonstrate that TRIP achieves optimal generalization
results, with communication of only 1K parameters per round. Our code is
available at https://github.com/GongShuai8210/TRIP.

</details>


### [136] [Frequency Feature Fusion Graph Network For Depression Diagnosis Via fNIRS](https://arxiv.org/abs/2504.21064)
*Chengkai Yang,Xingping Dong,Xiaofen Zong*

Main category: cs.LG

TL;DR: 提出了一种基于离散傅里叶变换（DFT）的新型生物标志物，并结合定制化的TGCN架构，显著提升了抑郁症诊断的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有GNN模型因缺乏鲁棒的时间生物标志物而受限，需改进以更好地捕捉脑通道功能连接的时空特征。

Method: 利用DFT设计生物标志物，构建TGCN模型，并在1,086名受试者数据集上训练，使用PSM创建子集。

Result: 新生物标志物提升了脑通道时间特征的表示，F1分数在真实数据集和PSM数据集中均有提高。

Conclusion: 该方法为抑郁症诊断工具的发展提供了潜在贡献，并通过SHAP验证了模型的可解释性。

Abstract: Data-driven approaches for depression diagnosis have emerged as a significant
research focus in neuromedicine, driven by the development of relevant
datasets. Recently, graph neural network (GNN)-based models have gained
widespread adoption due to their ability to capture brain channel functional
connectivity from both spatial and temporal perspectives. However, their
effectiveness is hindered by the absence of a robust temporal biomarker. In
this paper, we introduce a novel and effective biomarker for depression
diagnosis by leveraging the discrete Fourier transform (DFT) and propose a
customized graph network architecture based on Temporal Graph Convolutional
Network (TGCN). Our model was trained on a dataset comprising 1,086 subjects,
which is over 10 times larger than previous datasets in the field of depression
diagnosis. Furthermore, to align with medical requirements, we performed
propensity score matching (PSM) to create a refined subset, referred to as the
PSM dataset. Experimental results demonstrate that incorporating our newly
designed biomarker enhances the representation of temporal characteristics in
brain channels, leading to improved F1 scores in both the real-world dataset
and the PSM dataset. This advancement has the potential to contribute to the
development of more effective depression diagnostic tools. In addition, we used
SHapley Additive exPlaination (SHAP) to validate the interpretability of our
model, ensuring its practical applicability in medical settings.

</details>


### [137] [A 3D pocket-aware and affinity-guided diffusion model for lead optimization](https://arxiv.org/abs/2504.21065)
*Anjie Qiao,Junjie Xie,Weifeng Huang,Hao Zhang,Jiahua Rao,Shuangjia Zheng,Yuedong Yang,Zhen Wang,Guo-Bo Li,Jinping Lei*

Main category: cs.LG

TL;DR: Diffleop是一种3D口袋感知和亲和力引导的扩散模型，用于优化分子结合亲和力，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 分子优化在药物发现中至关重要，但现有深度学习模型在考虑结合亲和力方面存在不足。

Method: 提出Diffleop模型，通过结合蛋白-配体亲和力知识引导去噪采样，生成高亲和力分子。

Result: Diffleop在多项指标上优于基线模型，尤其在结合亲和力方面表现突出。

Conclusion: Diffleop为分子优化提供了一种高效且亲和力导向的新方法。

Abstract: Molecular optimization, aimed at improving binding affinity or other
molecular properties, is a crucial task in drug discovery that often relies on
the expertise of medicinal chemists. Recently, deep learning-based 3D
generative models showed promise in enhancing the efficiency of molecular
optimization. However, these models often struggle to adequately consider
binding affinities with protein targets during lead optimization. Herein, we
propose a 3D pocket-aware and affinity-guided diffusion model, named Diffleop,
to optimize molecules with enhanced binding affinity. The model explicitly
incorporates the knowledge of protein-ligand binding affinity to guide the
denoising sampling for molecule generation with high affinity. The
comprehensive evaluations indicated that Diffleop outperforms baseline models
across multiple metrics, especially in terms of binding affinity.

</details>


### [138] [A Brief Review for Compression and Transfer Learning Techniques in DeepFake Detection](https://arxiv.org/abs/2504.21066)
*Andreas Karathanasis,John Violos,Ioannis Kompatsiaris,Symeon Papadopoulos*

Main category: cs.LG

TL;DR: 论文探讨了在边缘设备上部署深度伪造检测模型的压缩和迁移学习方法，以解决资源限制问题。


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源有限，但需保护数据隐私，因此需优化模型以减少计算和内存需求。

Method: 采用剪枝、知识蒸馏、量化、微调和适配器技术，并在多个数据集上评估。

Result: 高压缩率（90%）下性能不变，但测试数据来自不同模型时出现领域泛化问题。

Conclusion: 压缩和迁移学习有效，但需解决跨模型泛化问题。

Abstract: Training and deploying deepfake detection models on edge devices offers the
advantage of maintaining data privacy and confidentiality by processing it
close to its source. However, this approach is constrained by the limited
computational and memory resources available at the edge. To address this
challenge, we explore compression techniques to reduce computational demands
and inference time, alongside transfer learning methods to minimize training
overhead. Using the Synthbuster, RAISE, and ForenSynths datasets, we evaluate
the effectiveness of pruning, knowledge distillation (KD), quantization,
fine-tuning, and adapter-based techniques. Our experimental results demonstrate
that both compression and transfer learning can be effectively achieved, even
with a high compression level of 90%, remaining at the same performance level
when the training and validation data originate from the same DeepFake model.
However, when the testing dataset is generated by DeepFake models not present
in the training set, a domain generalization issue becomes evident.

</details>


### [139] [R^2VFL: A Robust Random Vector Functional Link Network with Huber-Weighted Framework](https://arxiv.org/abs/2504.21069)
*Anuradha Kumari,Mushir Akhtar,P. N. Suganthan,M. Tanveer*

Main category: cs.LG

TL;DR: 提出了一种名为R2VFL的鲁棒框架，结合Huber加权函数和类别概率，有效减少噪声和异常值对RVFL神经网络的影响，并提出了两种变体R2VFL-A和R2VFL-M。


<details>
  <summary>Details</summary>
Motivation: RVFL神经网络在处理噪声和异常值时表现不佳，假设所有数据样本贡献均等，导致模型鲁棒性不足。

Method: 引入Huber加权函数减少异常值影响，结合类别概率机制降低噪声数据权重；提出两种计算类别中心的方法（平均值和中位数），形成R2VFL-A和R2VFL-M变体。

Result: 在47个UCI数据集上验证了模型的优越性，并在EEG信号分类中表现出色。

Conclusion: R2VFL框架显著提升了模型的鲁棒性和实用性，尤其在生物医学领域具有广泛应用潜力。

Abstract: The random vector functional link (RVFL) neural network has shown significant
potential in overcoming the constraints of traditional artificial neural
networks, such as excessive computation time and suboptimal solutions. However,
RVFL faces challenges when dealing with noise and outliers, as it assumes all
data samples contribute equally. To address this issue, we propose a novel
robust framework, R2VFL, RVFL with Huber weighting function and class
probability, which enhances the model's robustness and adaptability by
effectively mitigating the impact of noise and outliers in the training data.
The Huber weighting function reduces the influence of outliers, while the class
probability mechanism assigns less weight to noisy data points, resulting in a
more resilient model. We explore two distinct approaches for calculating class
centers within the R2VFL framework: the simple average of all data points in
each class and the median of each feature, the later providing a robust
alternative by minimizing the effect of extreme values. These approaches give
rise to two novel variants of the model-R2VFL-A and R2VFL-M. We extensively
evaluate the proposed models on 47 UCI datasets, encompassing both binary and
multiclass datasets, and conduct rigorous statistical testing, which confirms
the superiority of the proposed models. Notably, the models also demonstrate
exceptional performance in classifying EEG signals, highlighting their
practical applicability in real-world biomedical domain.

</details>


### [140] [A Survey on Parameter-Efficient Fine-Tuning for Foundation Models in Federated Learning](https://arxiv.org/abs/2504.21099)
*Jieming Bian,Yuanzhe Peng,Lei Wang,Yin Huang,Jie Xu*

Main category: cs.LG

TL;DR: 该论文综述了参数高效微调（PEFT）与联邦学习（FL）的结合，系统分类了现有方法，并分析了其在数据异构性、通信效率等方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决大规模基础模型在联邦学习中微调时的高计算成本问题。

Method: 将PEFT方法分为三类：加法PEFT、选择性PEFT和重参数化PEFT，并分析其在联邦学习中的应用。

Result: 总结了PEFT在联邦学习中的优势与挑战，并探讨了未来研究方向。

Conclusion: PEFT与FL的结合为高效、隐私保护的模型微调提供了潜力，但仍需进一步研究。

Abstract: Foundation models have revolutionized artificial intelligence by providing
robust, versatile architectures pre-trained on large-scale datasets. However,
adapting these massive models to specific downstream tasks requires
fine-tuning, which can be prohibitively expensive in computational resources.
Parameter-Efficient Fine-Tuning (PEFT) methods address this challenge by
selectively updating only a small subset of parameters. Meanwhile, Federated
Learning (FL) enables collaborative model training across distributed clients
without sharing raw data, making it ideal for privacy-sensitive applications.
This survey provides a comprehensive review of the integration of PEFT
techniques within federated learning environments. We systematically categorize
existing approaches into three main groups: Additive PEFT (which introduces new
trainable parameters), Selective PEFT (which fine-tunes only subsets of
existing parameters), and Reparameterized PEFT (which transforms model
architectures to enable efficient updates). For each category, we analyze how
these methods address the unique challenges of federated settings, including
data heterogeneity, communication efficiency, computational constraints, and
privacy concerns. We further organize the literature based on application
domains, covering both natural language processing and computer vision tasks.
Finally, we discuss promising research directions, including scaling to larger
foundation models, theoretical analysis of federated PEFT methods, and
sustainable approaches for resource-constrained environments.

</details>


### [141] [SMOGAN: Synthetic Minority Oversampling with GAN Refinement for Imbalanced Regression](https://arxiv.org/abs/2504.21152)
*Shayan Alahyari,Mike Domaratzki*

Main category: cs.LG

TL;DR: SMOGAN是一种用于不平衡回归的两阶段过采样框架，通过DistGAN过滤层改进合成样本，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 不平衡回归中目标变量偏斜导致模型在稀疏区域表现差，现有方法生成的合成样本不能准确反映真实分布。

Method: SMOGAN分为两阶段：第一阶段生成初始合成样本，第二阶段用DistGAN过滤并优化样本。

Result: 在23个不平衡数据集上，SMOGAN表现优于传统过采样方法。

Conclusion: SMOGAN通过分布感知的GAN有效提升不平衡回归性能。

Abstract: Imbalanced regression refers to prediction tasks where the target variable is
skewed. This skewness hinders machine learning models, especially neural
networks, which concentrate on dense regions and therefore perform poorly on
underrepresented (minority) samples. Despite the importance of this problem,
only a few methods have been proposed for imbalanced regression. Many of the
available solutions for imbalanced regression adapt techniques from the class
imbalance domain, such as linear interpolation and the addition of Gaussian
noise, to create synthetic data in sparse regions. However, in many cases, the
underlying distribution of the data is complex and non-linear. Consequently,
these approaches generate synthetic samples that do not accurately represent
the true feature-target relationship. To overcome these limitations, we propose
SMOGAN, a two-step oversampling framework for imbalanced regression. In Stage
1, an existing oversampler generates initial synthetic samples in sparse target
regions. In Stage 2, we introduce DistGAN, a distribution-aware GAN that serves
as SMOGAN's filtering layer and refines these samples via adversarial loss
augmented with a Maximum Mean Discrepancy objective, aligning them with the
true joint feature-target distribution. Extensive experiments on 23 imbalanced
datasets show that SMOGAN consistently outperforms the default oversampling
method without the DistGAN filtering layer.

</details>


### [142] [Efficient LLMs with AMP: Attention Heads and MLP Pruning](https://arxiv.org/abs/2504.21174)
*Leandro Giusti Mugnaini,Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Victor Zacarias,Edson Bollis,Lucas Pellicer,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: AMP是一种新型结构化剪枝方法，通过移除LLM中不太重要的结构（如MHA和MLP），高效压缩模型，同时保持预测能力，并在推理速度和资源受限环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的高计算成本和慢推理速度限制了其在资源受限环境中的部署，因此需要一种高效的剪枝方法。

Method: AMP通过将输入数据投影到权重上评估结构重要性，移除MHA和MLP中不太关键的部分，克服现有技术的局限性。

Result: AMP在常识推理任务上超越现有技术1.49个百分点，实现30%剪枝率且零样本任务性能影响最小，同时提升推理速度。

Conclusion: AMP是一种灵活高效的剪枝方法，适用于多种LLM家族，适合资源受限环境部署。

Abstract: Deep learning drives a new wave in computing systems and triggers the
automation of increasingly complex problems. In particular, Large Language
Models (LLMs) have significantly advanced cognitive tasks, often matching or
even surpassing human-level performance. However, their extensive parameters
result in high computational costs and slow inference, posing challenges for
deployment in resource-limited settings. Among the strategies to overcome the
aforementioned challenges, pruning emerges as a successful mechanism since it
reduces model size while maintaining predictive ability. In this paper, we
introduce AMP: Attention Heads and MLP Pruning, a novel structured pruning
method that efficiently compresses LLMs by removing less critical structures
within Multi-Head Attention (MHA) and Multilayer Perceptron (MLP). By
projecting the input data onto weights, AMP assesses structural importance and
overcomes the limitations of existing techniques, which often fall short in
flexibility or efficiency. In particular, AMP surpasses the current
state-of-the-art on commonsense reasoning tasks by up to 1.49 percentage
points, achieving a 30% pruning ratio with minimal impact on zero-shot task
performance. Moreover, AMP also improves inference speeds, making it
well-suited for deployment in resource-constrained environments. We confirm the
flexibility of AMP on different families of LLMs, including LLaMA and Phi.

</details>


### [143] [GLIP-OOD: Zero-Shot Graph OOD Detection with Foundation Model](https://arxiv.org/abs/2504.21186)
*Haoyan Xu,Zhengtao Yao,Xuzhi Zhang,Ziyi Wang,Langzhou He,Yushun Dong,Philip S. Yu,Mengyuan Li,Yue Zhao*

Main category: cs.LG

TL;DR: 该论文首次探索了图结构数据中的零样本OOD检测，利用图基础模型（GFM）和LLM生成的伪OOD标签，实现了无需监督的OOD检测，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 图结构数据的零样本OOD检测尚未充分研究，主要因复杂关系结构和缺乏大规模预训练模型。

Method: 使用GFM进行零样本OOD检测，并引入GLIP-OOD框架，利用LLM生成伪OOD标签。

Result: 在多个数据集上优于现有监督方法，并在四个基准数据集上达到最先进性能。

Conclusion: 该研究首次实现了完全零样本的节点级图OOD检测，为动态开放环境中的机器学习系统提供了安全保障。

Abstract: Out-of-distribution (OOD) detection is critical for ensuring the safety and
reliability of machine learning systems, particularly in dynamic and open-world
environments. In the vision and text domains, zero-shot OOD detection - which
requires no training on in-distribution (ID) data - has made significant
progress through the use of large-scale pretrained models such as
vision-language models (VLMs) and large language models (LLMs). However,
zero-shot OOD detection in graph-structured data remains largely unexplored,
primarily due to the challenges posed by complex relational structures and the
absence of powerful, large-scale pretrained models for graphs. In this work, we
take the first step toward enabling zero-shot graph OOD detection by leveraging
a graph foundation model (GFM). We show that, when provided only with class
label names, the GFM can perform OOD detection without any node-level
supervision - outperforming existing supervised methods across multiple
datasets. To address the more practical setting where OOD label names are
unavailable, we introduce GLIP-OOD, a novel framework that employs LLMs to
generate semantically informative pseudo-OOD labels from unlabeled data. These
labels enable the GFM to capture nuanced semantic boundaries between ID and OOD
classes and perform fine-grained OOD detection - without requiring any labeled
nodes. Our approach is the first to enable node-level graph OOD detection in a
fully zero-shot setting, and achieves state-of-the-art performance on four
benchmark text-attributed graph datasets.

</details>


### [144] [LIFT: LLM-Based Pragma Insertion for HLS via GNN Supervised Fine-Tuning](https://arxiv.org/abs/2504.21187)
*Neha Prakriya,Zijian Ding,Yizhou Sun,Jason Cong*

Main category: cs.LG

TL;DR: LIFT是一个基于大型语言模型（LLM）的HLS编码助手，能自动生成性能关键pragma，显著提升FPGA设计性能。


<details>
  <summary>Details</summary>
Motivation: FPGA在数据中心应用广泛，但高性能设计需要专家知识和手动优化，LIFT旨在解决这一问题。

Method: 通过结合LLM和GNN，LIFT能够理解代码结构和依赖关系，自动生成优化pragma。

Result: LIFT的性能比AutoDSE和HARP分别提升3.52倍和2.16倍，比GPT-4o提升66倍。

Conclusion: LIFT为FPGA高性能设计提供了自动化解决方案，显著减少了对专家知识的依赖。

Abstract: FPGAs are increasingly adopted in datacenter environments for their
reconfigurability and energy efficiency. High-Level Synthesis (HLS) tools have
eased FPGA programming by raising the abstraction level from RTL to untimed
C/C++, yet attaining high performance still demands expert knowledge and
iterative manual insertion of optimization pragmas to modify the
microarchitecture. To address this challenge, we propose LIFT, a large language
model (LLM)-based coding assistant for HLS that automatically generates
performance-critical pragmas given a C/C++ design. We fine-tune the LLM by
tightly integrating and supervising the training process with a graph neural
network (GNN), combining the sequential modeling capabilities of LLMs with the
structural and semantic understanding of GNNs necessary for reasoning over code
and its control/data dependencies. On average, LIFT produces designs that
improve performance by 3.52x and 2.16x than prior state-of the art AutoDSE and
HARP respectively, and 66x than GPT-4o.

</details>


### [145] [Artificial Intelligence for Personalized Prediction of Alzheimer's Disease Progression: A Survey of Methods, Data Challenges, and Future Directions](https://arxiv.org/abs/2504.21189)
*Gulsah Hancerliogullari Koksalmis,Bulent Soykan,Laura J. Brattain,Hsin-Hsiung Huang*

Main category: cs.LG

TL;DR: 本文综述了人工智能在预测阿尔茨海默病（AD）个性化进展中的应用，包括多种AI方法、数据挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: AD进展的个体差异大，需要个性化预测模型以改善预后和护理计划。

Method: 综述了状态空间模型、深度学习（如RNN）、图神经网络（GNN）和数字孪生等方法，并探讨了数据挑战及解决方案（如VAE和GAN）。

Result: 总结了当前方法的优缺点，强调多模态整合和模型可解释性的重要性。

Conclusion: 提出了未来研究方向，如混合模型、因果推理和联邦学习，并呼吁解决外部验证和伦理问题。

Abstract: Alzheimer's Disease (AD) is marked by significant inter-individual
variability in its progression, complicating accurate prognosis and
personalized care planning. This heterogeneity underscores the critical need
for predictive models capable of forecasting patient-specific disease
trajectories. Artificial Intelligence (AI) offers powerful tools to address
this challenge by analyzing complex, multi-modal, and longitudinal patient
data. This paper provides a comprehensive survey of AI methodologies applied to
personalized AD progression prediction. We review key approaches including
state-space models for capturing temporal dynamics, deep learning techniques
like Recurrent Neural Networks for sequence modeling, Graph Neural Networks
(GNNs) for leveraging network structures, and the emerging concept of AI-driven
digital twins for individualized simulation. Recognizing that data limitations
often impede progress, we examine common challenges such as high
dimensionality, missing data, and dataset imbalance. We further discuss
AI-driven mitigation strategies, with a specific focus on synthetic data
generation using Variational Autoencoders (VAEs) and Generative Adversarial
Networks (GANs) to augment and balance datasets. The survey synthesizes the
strengths and limitations of current approaches, emphasizing the trend towards
multimodal integration and the persistent need for model interpretability and
generalizability. Finally, we identify critical open challenges, including
robust external validation, clinical integration, and ethical considerations,
and outline promising future research directions such as hybrid models, causal
inference, and federated learning. This review aims to consolidate current
knowledge and guide future efforts in developing clinically relevant AI tools
for personalized AD prognostication.

</details>


### [146] [TT-LoRA MoE: Unifying Parameter-Efficient Fine-Tuning and Sparse Mixture-of-Experts](https://arxiv.org/abs/2504.21190)
*Pradip Kunwar,Minh N. Vu,Maanak Gupta,Mahmoud Abdelsalam,Manish Bhattarai*

Main category: cs.LG

TL;DR: TT-LoRA MoE结合了参数高效微调（PEFT）和稀疏MoE路由，通过分阶段训练和冻结专家适配器，解决了大规模模型部署中的计算和内存效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统MoE方法在专家数量增加时计算开销大，TT-LoRA MoE旨在通过分阶段优化和稀疏路由提升效率和灵活性。

Method: 分两阶段：1）独立训练轻量级张量低秩适配器（TT-LoRA专家）；2）冻结适配器，训练稀疏MoE路由器动态选择专家。

Result: 仅需2%的LoRA、0.3%的Adapter和0.03%的AdapterFusion参数，多任务性能优于AdapterFusion 4分。

Conclusion: TT-LoRA MoE显著提升计算效率和灵活性，适用于大规模多任务推理部署。

Abstract: We propose Tensor-Trained Low-Rank Adaptation Mixture of Experts (TT-LoRA
MoE), a novel computational framework integrating Parameter-Efficient
Fine-Tuning (PEFT) with sparse MoE routing to address scalability challenges in
large model deployments. Unlike traditional MoE approaches, which face
substantial computational overhead as expert counts grow, TT-LoRA MoE
decomposes training into two distinct, optimized stages. First, we
independently train lightweight, tensorized low-rank adapters (TT-LoRA
experts), each specialized for specific tasks. Subsequently, these expert
adapters remain frozen, eliminating inter-task interference and catastrophic
forgetting in multi-task setting. A sparse MoE router, trained separately,
dynamically leverages base model representations to select exactly one
specialized adapter per input at inference time, automating expert selection
without explicit task specification. Comprehensive experiments confirm our
architecture retains the memory efficiency of low-rank adapters, seamlessly
scales to large expert pools, and achieves robust task-level optimization. This
structured decoupling significantly enhances computational efficiency and
flexibility: uses only 2% of LoRA, 0.3% of Adapters and 0.03% of AdapterFusion
parameters and outperforms AdapterFusion by 4 value in multi-tasking, enabling
practical and scalable multi-task inference deployments.

</details>


### [147] [Graph Synthetic Out-of-Distribution Exposure with Large Language Models](https://arxiv.org/abs/2504.21198)
*Haoyan Xu,Zhengtao Yao,Ziyi Wang,Zhan Cheng,Xiyang Hu,Mengyuan Li,Yue Zhao*

Main category: cs.LG

TL;DR: GOE-LLM利用大语言模型（LLM）进行图OOD检测，无需真实OOD数据，通过生成伪OOD节点提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有图OOD检测方法依赖真实OOD数据，但获取困难且成本高，因此需要一种无需真实OOD数据的方法。

Method: GOE-LLM通过LLM零样本标注识别伪OOD节点，并生成合成OOD节点，用于训练ID分类器。

Result: GOE-LLM在多个基准数据集上显著优于不使用OOD暴露的方法，并与依赖真实OOD数据的方法性能相当。

Conclusion: GOE-LLM为图OOD检测提供了一种高效且实用的解决方案，无需真实OOD数据。

Abstract: Out-of-distribution (OOD) detection in graphs is critical for ensuring model
robustness in open-world and safety-sensitive applications. Existing approaches
to graph OOD detection typically involve training an in-distribution (ID)
classifier using only ID data, followed by the application of post-hoc OOD
scoring techniques. Although OOD exposure - introducing auxiliary OOD samples
during training - has proven to be an effective strategy for enhancing
detection performance, current methods in the graph domain generally assume
access to a set of real OOD nodes. This assumption, however, is often
impractical due to the difficulty and cost of acquiring representative OOD
samples. In this paper, we introduce GOE-LLM, a novel framework that leverages
Large Language Models (LLMs) for OOD exposure in graph OOD detection without
requiring real OOD nodes. GOE-LLM introduces two pipelines: (1) identifying
pseudo-OOD nodes from the initially unlabeled graph using zero-shot LLM
annotations, and (2) generating semantically informative synthetic OOD nodes
via LLM-prompted text generation. These pseudo-OOD nodes are then used to
regularize the training of the ID classifier for improved OOD awareness. We
evaluate our approach across multiple benchmark datasets, showing that GOE-LLM
significantly outperforms state-of-the-art graph OOD detection methods that do
not use OOD exposure and achieves comparable performance to those relying on
real OOD data.

</details>


### [148] [FedHERO: A Federated Learning Approach for Node Classification Task on Heterophilic Graphs](https://arxiv.org/abs/2504.21206)
*Zihan Chen,Xingbo Fu,Yushun Dong,Jundong Li,Cong Shen*

Main category: cs.LG

TL;DR: FedHERO是一个联邦图学习框架，旨在有效处理异构图数据，通过双通道GNN和结构学习器提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有FGL方法假设图数据是同质的，但在异构数据下性能下降，需解决这一问题。

Method: 提出FedHERO框架，使用双通道GNN和结构学习器提取局部图的通用结构知识。

Result: 实验验证FedHERO在异构数据下优于现有方法。

Conclusion: FedHERO为处理不同节点邻域分布模式的图数据提供了新方法。

Abstract: Federated Graph Learning (FGL) empowers clients to collaboratively train
Graph neural networks (GNNs) in a distributed manner while preserving data
privacy. However, FGL methods usually require that the graph data owned by all
clients is homophilic to ensure similar neighbor distribution patterns of
nodes. Such an assumption ensures that the learned knowledge is consistent
across the local models from all clients. Therefore, these local models can be
properly aggregated as a global model without undermining the overall
performance. Nevertheless, when the neighbor distribution patterns of nodes
vary across different clients (e.g., when clients hold graphs with different
levels of heterophily), their local models may gain different and even conflict
knowledge from their node-level predictive tasks. Consequently, aggregating
these local models usually leads to catastrophic performance deterioration on
the global model. To address this challenge, we propose FedHERO, an FGL
framework designed to harness and share insights from heterophilic graphs
effectively. At the heart of FedHERO is a dual-channel GNN equipped with a
structure learner, engineered to discern the structural knowledge encoded in
the local graphs. With this specialized component, FedHERO enables the local
model for each client to identify and learn patterns that are universally
applicable across graphs with different patterns of node neighbor
distributions. FedHERO not only enhances the performance of individual client
models by leveraging both local and shared structural insights but also sets a
new precedent in this field to effectively handle graph data with various node
neighbor distribution patterns. We conduct extensive experiments to validate
the superior performance of FedHERO against existing alternatives.

</details>


### [149] [A Cost-Effective LLM-based Approach to Identify Wildlife Trafficking in Online Marketplaces](https://arxiv.org/abs/2504.21211)
*Juliana Barbosa,Ulhas Gondhali,Gohar Petrossian,Kinshuk Sharma,Sunandan Chakraborty,Jennifer Jacquet,Juliana Freire*

Main category: cs.LG

TL;DR: 论文提出了一种利用大型语言模型（LLM）生成伪标签的低成本方法，用于构建高效分类器，以识别野生动物非法贸易的在线广告。


<details>
  <summary>Details</summary>
Motivation: 野生动物非法贸易对生态和公共健康构成威胁，而电子商务平台为非法交易提供了便利。识别这些广告需要大量标注数据，但传统方法成本高且耗时。

Method: 利用LLM为少量数据生成伪标签，再用这些标签训练专用分类器，同时自动收集多样化的样本以降低成本。

Result: 实验表明，该方法构建的分类器F1分数高达95%，优于直接使用LLM且成本更低。

Conclusion: 该方法为野生动物非法贸易分析提供了一种高效、低成本的解决方案，并展示了实际应用效果。

Abstract: Wildlife trafficking remains a critical global issue, significantly impacting
biodiversity, ecological stability, and public health. Despite efforts to
combat this illicit trade, the rise of e-commerce platforms has made it easier
to sell wildlife products, putting new pressure on wild populations of
endangered and threatened species. The use of these platforms also opens a new
opportunity: as criminals sell wildlife products online, they leave digital
traces of their activity that can provide insights into trafficking activities
as well as how they can be disrupted. The challenge lies in finding these
traces. Online marketplaces publish ads for a plethora of products, and
identifying ads for wildlife-related products is like finding a needle in a
haystack. Learning classifiers can automate ad identification, but creating
them requires costly, time-consuming data labeling that hinders support for
diverse ads and research questions. This paper addresses a critical challenge
in the data science pipeline for wildlife trafficking analytics: generating
quality labeled data for classifiers that select relevant data. While large
language models (LLMs) can directly label advertisements, doing so at scale is
prohibitively expensive. We propose a cost-effective strategy that leverages
LLMs to generate pseudo labels for a small sample of the data and uses these
labels to create specialized classification models. Our novel method
automatically gathers diverse and representative samples to be labeled while
minimizing the labeling costs. Our experimental evaluation shows that our
classifiers achieve up to 95% F1 score, outperforming LLMs at a lower cost. We
present real use cases that demonstrate the effectiveness of our approach in
enabling analyses of different aspects of wildlife trafficking.

</details>


### [150] [ABG-NAS: Adaptive Bayesian Genetic Neural Architecture Search for Graph Representation Learning](https://arxiv.org/abs/2504.21254)
*Sixuan Wang,Jiao Yin,Jinli Cao,MingJian Tang,Hua Wang,Yanchun Zhang*

Main category: cs.LG

TL;DR: ABG-NAS是一种自动化图神经网络架构搜索框架，通过综合搜索空间、自适应遗传优化和贝叶斯调优模块，显著提升图表示学习的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络难以适应复杂多样的图结构，限制了其泛化能力，因此需要一种自动化方法来设计更高效的架构。

Method: ABG-NAS包含三个核心组件：综合架构搜索空间（CASS）、自适应遗传优化策略（AGOS）和贝叶斯引导调优模块（BGTM），分别用于探索操作、优化搜索过程和调优超参数。

Result: 在多个基准数据集上，ABG-NAS表现优于手动设计的GNN和其他NAS方法。

Conclusion: ABG-NAS为图表示学习提供了可扩展且自适应的解决方案，具有广泛的应用潜力。

Abstract: Effective and efficient graph representation learning is essential for
enabling critical downstream tasks, such as node classification, link
prediction, and subgraph search. However, existing graph neural network (GNN)
architectures often struggle to adapt to diverse and complex graph structures,
limiting their ability to provide robust and generalizable representations. To
address this challenge, we propose ABG-NAS, a novel framework for automated
graph neural network architecture search tailored for efficient graph
representation learning. ABG-NAS encompasses three key components: a
Comprehensive Architecture Search Space (CASS), an Adaptive Genetic
Optimization Strategy (AGOS), and a Bayesian-Guided Tuning Module (BGTM). CASS
systematically explores diverse propagation (P) and transformation (T)
operations, enabling the discovery of GNN architectures capable of capturing
intricate graph characteristics. AGOS dynamically balances exploration and
exploitation, ensuring search efficiency and preserving solution diversity.
BGTM further optimizes hyperparameters periodically, enhancing the scalability
and robustness of the resulting architectures. Empirical evaluations on
benchmark datasets (Cora, PubMed, Citeseer, and CoraFull) demonstrate that
ABG-NAS consistently outperforms both manually designed GNNs and
state-of-the-art neural architecture search (NAS) methods. These results
highlight the potential of ABG-NAS to advance graph representation learning by
providing scalable and adaptive solutions for diverse graph structures. Our
code is publicly available at https://github.com/sserranw/ABG-NAS.

</details>


### [151] [Multi-Domain Causal Discovery in Bijective Causal Models](https://arxiv.org/abs/2504.21261)
*Kasra Jalaldoust,Saber Salehkaleybar,Negar Kiyavash*

Main category: cs.LG

TL;DR: 论文提出了一种在多域设置下进行因果发现的方法，通过假设因果函数跨域不变而噪声分布可变，利用双射生成机制（BGM）放宽了功能假设，并推导了统计测试以确定目标变量的父集。


<details>
  <summary>Details</summary>
Motivation: 解决多域设置下因果发现的限制，通过放宽功能假设并利用噪声分布的可变性，提高因果图的发现能力。

Method: 采用双射生成机制（BGM）确保噪声与内生变量之间的函数关系是双射且可微的，并推导统计测试以识别父集。

Result: 实验验证了理论发现，BGM能够推广多种模型（如加性噪声模型、LiNGAM等），并在合成和真实数据集上表现良好。

Conclusion: BGM为多域因果发现提供了更通用的框架，放宽了功能假设，并通过实验验证了其有效性。

Abstract: We consider the problem of causal discovery (a.k.a., causal structure
learning) in a multi-domain setting. We assume that the causal functions are
invariant across the domains, while the distribution of the exogenous noise may
vary. Under causal sufficiency (i.e., no confounders exist), we show that the
causal diagram can be discovered under less restrictive functional assumptions
compared to previous work. What enables causal discovery in this setting is
bijective generation mechanisms (BGM), which ensures that the functional
relation between the exogenous noise $E$ and the endogenous variable $Y$ is
bijective and differentiable in both directions at every level of the cause
variable $X = x$. BGM generalizes a variety of models including additive noise
model, LiNGAM, post-nonlinear model, and location-scale noise model. Further,
we derive a statistical test to find the parents set of the target variable.
Experiments on various synthetic and real-world datasets validate our
theoretical findings.

</details>


### [152] [Orthogonal Factor-Based Biclustering Algorithm (BCBOF) for High-Dimensional Data and Its Application in Stock Trend Prediction](https://arxiv.org/abs/2504.21289)
*Yan Huang,Da-Qing Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于正交因子的双聚类算法（BCBOF），解决了高维数据处理中的距离集中和局部结构破坏问题，并在股票预测中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统双聚类算法在高维数据中因距离集中和线性降维破坏局部结构而失效，需改进。

Method: 构建正交因子空间，以正交子空间坐标聚类，生成双聚类结果，并应用于股票预测。

Result: BCBOF优于现有方法，虚拟交易实验显示其生成的策略带来更高收益。

Conclusion: BCBOF有效解决高维数据问题，并在股票预测中表现优异。

Abstract: Biclustering is an effective technique in data mining and pattern
recognition. Biclustering algorithms based on traditional clustering face two
fundamental limitations when processing high-dimensional data: (1) The distance
concentration phenomenon in high-dimensional spaces leads to data sparsity,
rendering similarity measures ineffective; (2) Mainstream linear dimensionality
reduction methods disrupt critical local structural patterns. To apply
biclustering to high-dimensional datasets, we propose an orthogonal
factor-based biclustering algorithm (BCBOF). First, we constructed orthogonal
factors in the vector space of the high-dimensional dataset. Then, we performed
clustering using the coordinates of the original data in the orthogonal
subspace as clustering targets. Finally, we obtained biclustering results of
the original dataset. Since dimensionality reduction was applied before
clustering, the proposed algorithm effectively mitigated the data sparsity
problem caused by high dimensionality. Additionally, we applied this
biclustering algorithm to stock technical indicator combinations and stock
price trend prediction. Biclustering results were transformed into fuzzy rules,
and we incorporated profit-preserving and stop-loss rules into the rule set,
ultimately forming a fuzzy inference system for stock price trend predictions
and trading signals. To evaluate the performance of BCBOF, we compared it with
existing biclustering methods using multiple evaluation metrics. The results
showed that our algorithm outperformed other biclustering techniques. To
validate the effectiveness of the fuzzy inference system, we conducted virtual
trading experiments using historical data from 10 A-share stocks. The
experimental results showed that the generated trading strategies yielded
higher returns for investors.

</details>


### [153] [Fairness in Graph Learning Augmented with Machine Learning: A Survey](https://arxiv.org/abs/2504.21296)
*Renqiang Luo,Ziqi Xu,Xikun Zhang,Qing Qing,Huafei Huang,Enyan Dai,Zhe Wang,Bo Yang*

Main category: cs.LG

TL;DR: 本文探讨了图学习与机器学习结合（GL-ML）中的公平性挑战，分析了其复杂机制及潜在歧视性影响，并提出了四种改进公平性的关键技术。


<details>
  <summary>Details</summary>
Motivation: 传统图学习模型结合机器学习技术虽在多领域取得成功，但其复杂机制可能导致公平性问题，影响高风险应用（如推荐系统、贷款审批等）。

Method: 系统分析了GL-ML中的公平性挑战，研究了图学习与机器学习的交互机制，并探讨了四种改进公平性的技术。

Result: 揭示了GL-ML中公平性问题的根源及其广泛影响，为未来研究奠定了基础。

Conclusion: 本文为GL-ML公平性研究提供了重要框架，推动了该领域的进一步创新。

Abstract: Augmenting specialised machine learning techniques into traditional graph
learning models has achieved notable success across various domains, including
federated graph learning, dynamic graph learning, and graph transformers.
However, the intricate mechanisms of these specialised techniques introduce
significant challenges in maintaining model fairness, potentially resulting in
discriminatory outcomes in high-stakes applications such as recommendation
systems, disaster response, criminal justice, and loan approval. This paper
systematically examines the unique fairness challenges posed by Graph Learning
augmented with Machine Learning (GL-ML). It highlights the complex interplay
between graph learning mechanisms and machine learning techniques, emphasising
how the augmentation of machine learning both enhances and complicates
fairness. Additionally, we explore four critical techniques frequently employed
to improve fairness in GL-ML methods. By thoroughly investigating the root
causes and broader implications of fairness challenges in this rapidly evolving
field, this work establishes a robust foundation for future research and
innovation in GL-ML fairness.

</details>


### [154] [Unsupervised Feature Transformation via In-context Generation, Generator-critic LLM Agents, and Duet-play Teaming](https://arxiv.org/abs/2504.21304)
*Nanxu Gong,Xinyuan Wang,Wangyang Ying,Haoyue Bai,Sixun Dong,Haifeng Chen,Yanjie Fu*

Main category: cs.LG

TL;DR: 提出了一种基于LLM代理和上下文学习的生成器-批评者框架，用于无监督特征转换，提升数据准备和AI实用性。


<details>
  <summary>Details</summary>
Motivation: 在材料性能筛选等领域，数据维度高且标注成本高，需要高效无监督的特征转换方法。现有方法无法高效处理大量特征组合且多为监督式设计。

Method: 采用生成器-批评者双代理框架，通过三步流程：批评者诊断数据生成建议，生成器根据建议生成特征转换，迭代优化。

Result: 实验表明，该框架在特征转换效率、鲁棒性和实用性上优于监督基线。

Conclusion: 该框架为无监督特征转换提供了高效解决方案，并可扩展至人机协作场景。

Abstract: Feature transformation involves generating a new set of features from the
original dataset to enhance the data's utility. In certain domains like
material performance screening, dimensionality is large and collecting labels
is expensive and lengthy. It highly necessitates transforming feature spaces
efficiently and without supervision to enhance data readiness and AI utility.
However, existing methods fall short in efficient navigation of a vast space of
feature combinations, and are mostly designed for supervised settings. To fill
this gap, our unique perspective is to leverage a generator-critic duet-play
teaming framework using LLM agents and in-context learning to derive
pseudo-supervision from unsupervised data. The framework consists of three
interconnected steps: (1) Critic agent diagnoses data to generate actionable
advice, (2) Generator agent produces tokenized feature transformations guided
by the critic's advice, and (3) Iterative refinement ensures continuous
improvement through feedback between agents. The generator-critic framework can
be generalized to human-agent collaborative generation, by replacing the critic
agent with human experts. Extensive experiments demonstrate that the proposed
framework outperforms even supervised baselines in feature transformation
efficiency, robustness, and practical applicability across diverse datasets.

</details>


### [155] [Capturing Conditional Dependence via Auto-regressive Diffusion Models](https://arxiv.org/abs/2504.21314)
*Xunpeng Huang,Yujin Han,Difan Zou,Yian Ma,Tong Zhang*

Main category: cs.LG

TL;DR: 扩散模型在图像和视频生成中表现优异，但难以捕捉现实世界中的高层次关系。本文提出自回归扩散模型（AR-diffusion）以更好地捕捉数据中的条件依赖结构，并提供了理论分析和实证结果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现良好，但无法有效学习现实世界中的物理规律和稳定对象关系，原因是其未能充分捕捉数据中的条件依赖结构。

Method: 研究自回归扩散模型（AR-diffusion）的有效性，并首次在温和数据假设下提出其采样误差的理论结果。

Result: 理论表明，AR-diffusion在逼近数据条件分布时误差更小，且推理时间仅略高于普通扩散模型。实证显示，AR-diffusion能捕捉明显的条件依赖结构，而普通扩散模型（DDPM）则无法做到。

Conclusion: AR-diffusion在数据具有条件依赖结构时表现优于DDPM，但在无依赖结构时与DDPM相当，具有实际应用价值。

Abstract: Diffusion models have demonstrated appealing performance in both image and
video generation. However, many works discover that they struggle to capture
important, high-level relationships that are present in the real world. For
example, they fail to learn physical laws from data, and even fail to
understand that the objects in the world exist in a stable fashion. This is due
to the fact that important conditional dependence structures are not adequately
captured in the vanilla diffusion models. In this work, we initiate an in-depth
study on strengthening the diffusion model to capture the conditional
dependence structures in the data. In particular, we examine the efficacy of
the auto-regressive (AR) diffusion models for such purpose and develop the
first theoretical results on the sampling error of AR diffusion models under
(possibly) the mildest data assumption. Our theoretical findings indicate that,
compared with typical diffusion models, the AR variant produces samples with a
reduced gap in approximating the data conditional distribution. On the other
hand, the overall inference time of the AR-diffusion models is only moderately
larger than that for the vanilla diffusion models, making them still practical
for large scale applications. We also provide empirical results showing that
when there is clear conditional dependence structure in the data, the AR
diffusion models captures such structure, whereas vanilla DDPM fails to do so.
On the other hand, when there is no obvious conditional dependence across
patches of the data, AR diffusion does not outperform DDPM.

</details>


### [156] [Q-function Decomposition with Intervention Semantics with Factored Action Spaces](https://arxiv.org/abs/2504.21326)
*Junkyu Lee,Tian Gao,Elliot Nelson,Miao Liu,Debarun Bhattacharjya,Songtao Lu*

Main category: cs.LG

TL;DR: 论文提出了一种基于因果统计的动作分解强化学习方法，通过投影Q函数降低动作空间的维度，提高了样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决离散组合动作空间带来的挑战，避免枚举所有动作组合。

Method: 利用因果统计中的无未观测混杂因子设定，定义投影子空间上的Q函数，提出动作分解强化学习框架。

Result: 在模型强化学习环境中降低了样本复杂度，并在在线连续控制环境和离线真实世界环境中优于基线方法。

Conclusion: 动作分解强化学习方法能有效提高样本效率，适用于复杂动作空间问题。

Abstract: Many practical reinforcement learning environments have a discrete factored
action space that induces a large combinatorial set of actions, thereby posing
significant challenges. Existing approaches leverage the regular structure of
the action space and resort to a linear decomposition of Q-functions, which
avoids enumerating all combinations of factored actions. In this paper, we
consider Q-functions defined over a lower dimensional projected subspace of the
original action space, and study the condition for the unbiasedness of
decomposed Q-functions using causal effect estimation from the no unobserved
confounder setting in causal statistics. This leads to a general scheme which
we call action decomposed reinforcement learning that uses the projected
Q-functions to approximate the Q-function in standard model-free reinforcement
learning algorithms. The proposed approach is shown to improve sample
complexity in a model-based reinforcement learning setting. We demonstrate
improvements in sample efficiency compared to state-of-the-art baselines in
online continuous control environments and a real-world offline sepsis
treatment environment.

</details>


### [157] [A Generalized Meta Federated Learning Framework with Theoretical Convergence Guarantees](https://arxiv.org/abs/2504.21327)
*Mohammad Vahid Jamali,Hamid Saber,Jung Hyun Bae*

Main category: cs.LG

TL;DR: 论文提出了一种广义的元联邦学习框架，通过最小化代理在任意数量微调步骤后的本地模型损失，改进了传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统元联邦学习方法仅优化一步微调后的损失，但在数据分布高度异构时，代理可能需要多步微调，因此需要更通用的框架。

Method: 提出广义框架，允许任意数量的微调步骤，并设计了一种改进的FedAvg算法，进行了理论收敛分析。

Result: 实验表明，该方法在真实数据集上具有更高的准确性和更快的收敛速度。

Conclusion: 广义框架和算法在异构数据场景下显著提升了模型个性化效果。

Abstract: Meta federated learning (FL) is a personalized variant of FL, where multiple
agents collaborate on training an initial shared model without exchanging raw
data samples. The initial model should be trained in a way that current or new
agents can easily adapt it to their local datasets after one or a few
fine-tuning steps, thus improving the model personalization. Conventional meta
FL approaches minimize the average loss of agents on the local models obtained
after one step of fine-tuning. In practice, agents may need to apply several
fine-tuning steps to adapt the global model to their local data, especially
under highly heterogeneous data distributions across agents. To this end, we
present a generalized framework for the meta FL by minimizing the average loss
of agents on their local model after any arbitrary number $\nu$ of fine-tuning
steps. For this generalized framework, we present a variant of the well-known
federated averaging (FedAvg) algorithm and conduct a comprehensive theoretical
convergence analysis to characterize the convergence speed as well as behavior
of the meta loss functions in both the exact and approximated cases. Our
experiments on real-world datasets demonstrate superior accuracy and faster
convergence for the proposed scheme compared to conventional approaches.

</details>


### [158] [Multi-level datasets training method in Physics-Informed Neural Networks](https://arxiv.org/abs/2504.21328)
*Yao-Hsuan Tsai,Hsiao-Tung Juan,Pao-Hsiung Chiu,Chao-An Lin*

Main category: cs.LG

TL;DR: 该论文提出了一种基于多网格方法的改进PINNs框架，用于解决高频率和多尺度PDE问题，显著提高了训练精度和收敛性。


<details>
  <summary>Details</summary>
Motivation: 尽管PINNs在解决PDE方面表现出潜力，但在处理高频率或刚性问题时存在精度和收敛性问题。本文旨在通过多网格方法优化训练过程。

Method: 采用多网格训练策略，通过不同级别的训练样本逐步消除频率误差，避免了对神经网络结构和超参数的繁琐调整。

Result: 在1D和2D问题上验证了方法的有效性，精度提升了30%-60%，并在高雷诺数（Re=5000）的复杂问题中表现出色。

Conclusion: 提出的框架显著提升了PINNs在高频率和多尺度PDE问题中的性能，展示了与迁移学习结合的潜力。

Abstract: Physics-Informed Neural Networks have emerged as a promising methodology for
solving PDEs, gaining significant attention in computer science and various
physics-related fields. Despite being demonstrated the ability to incorporate
the physics of laws for versatile applications, PINNs still struggle with the
challenging problems which are stiff to be solved and/or have high-frequency
components in the solutions, resulting in accuracy and convergence issues. It
may not only increase computational costs, but also lead to accuracy loss or
solution divergence. In this study, an alternative approach is proposed to
mitigate the above-mentioned problems. Inspired by the multi-grid method in CFD
community, the underlying idea of the current approach is to efficiently remove
different frequency errors via training with different levels of training
samples, resulting in a simpler way to improve the training accuracy without
spending time in fine-tuning of neural network structures, loss weights as well
as hyperparameters. To demonstrate the efficacy of current approach, we first
investigate canonical 1D ODE with high-frequency component and 2D
convection-diffusion equation with V-cycle training strategy. Finally, the
current method is employed for the classical benchmark problem of steady
Lid-driven cavity flows at different Reynolds numbers, to investigate the
applicability and efficacy for the problem involved multiple modes of high and
low frequency. By virtue of various training sequence modes, improvement
through predictions lead to 30% to 60% accuracy improvement. We also
investigate the synergies between current method and transfer learning
techniques for more challenging problems (i.e., higher Re). From the present
results, it also revealed that the current framework can produce good
predictions even for the case of Re=5000, demonstrating the ability to solve
complex high-frequency PDEs.

</details>


### [159] [Generative QoE Modeling: A Lightweight Approach for Telecom Networks](https://arxiv.org/abs/2504.21353)
*Vinti Nayar,Kanica Sachdev,Brejesh Lall*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级生成建模框架（VQ-HMM），用于QoE预测，平衡计算效率、可解释性和预测准确性。


<details>
  <summary>Details</summary>
Motivation: QoE预测对优化资源管理和提升用户满意度至关重要，但现有深度学习方法复杂且资源消耗大。

Method: 通过向量量化（VQ）将连续特征离散化，结合隐马尔可夫模型（HMM）进行时序建模。

Result: 实验表明，该方法在实时和资源受限环境中表现良好，支持概率推理。

Conclusion: 该框架为复杂深度学习方法提供了轻量级替代方案，适用于计算资源有限或延迟敏感的场景。

Abstract: Quality of Experience (QoE) prediction plays a crucial role in optimizing
resource management and enhancing user satisfaction across both
telecommunication and OTT services. While recent advances predominantly rely on
deep learning models, this study introduces a lightweight generative modeling
framework that balances computational efficiency, interpretability, and
predictive accuracy. By validating the use of Vector Quantization (VQ) as a
preprocessing technique, continuous network features are effectively
transformed into discrete categorical symbols, enabling integration with a
Hidden Markov Model (HMM) for temporal sequence modeling. This VQ-HMM pipeline
enhances the model's capacity to capture dynamic QoE patterns while supporting
probabilistic inference on new and unseen data. Experimental results on
publicly available time-series datasets incorporating both objective indicators
and subjective QoE scores demonstrate the viability of this approach in
real-time and resource-constrained environments, where inference latency is
also critical. The framework offers a scalable alternative to complex deep
learning methods, particularly in scenarios with limited computational
resources or where latency constraints are critical.

</details>


### [160] [A comparative study of deep learning and ensemble learning to extend the horizon of traffic forecasting](https://arxiv.org/abs/2504.21358)
*Xiao Zheng,Saeed Asadi Bagloee,Majid Sarvi*

Main category: cs.LG

TL;DR: 本文比较了多种机器学习方法在长期交通流量预测中的表现，发现时间嵌入对周期性建模至关重要，且XGBoost在仅使用时间特征时表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决长期交通预测的挑战，评估机器学习方法在大时间跨度（30天）预测中的有效性。

Method: 开发了XGBoost和多种深度学习方法（如RNN和Transformer），并利用时间嵌入增强模型对周期性和事件的理解。

Result: 时间嵌入显著提升预测效果，XGBoost表现与深度学习方法相当。Transformer在长序列依赖中有效，但随着预测时间延长，周期性建模更为关键。

Conclusion: 长期交通预测的关键在于周期性建模，时间嵌入和XGBoost是有效的解决方案，为未来研究提供了参考。

Abstract: Traffic forecasting is vital for Intelligent Transportation Systems, for
which Machine Learning (ML) methods have been extensively explored to develop
data-driven Artificial Intelligence (AI) solutions. Recent research focuses on
modelling spatial-temporal correlations for short-term traffic prediction,
leaving the favourable long-term forecasting a challenging and open issue. This
paper presents a comparative study on large-scale real-world signalized
arterials and freeway traffic flow datasets, aiming to evaluate promising ML
methods in the context of large forecasting horizons up to 30 days. Focusing on
modelling capacity for temporal dynamics, we develop one ensemble ML method,
eXtreme Gradient Boosting (XGBoost), and a range of Deep Learning (DL) methods,
including Recurrent Neural Network (RNN)-based methods and the state-of-the-art
Transformer-based method. Time embedding is leveraged to enhance their
understanding of seasonality and event factors. Experimental results highlight
that while the attention mechanism/Transformer framework is effective for
capturing long-range dependencies in sequential data, as the forecasting
horizon extends, the key to effective traffic forecasting gradually shifts from
temporal dependency capturing to periodicity modelling. Time embedding is
particularly effective in this context, helping naive RNN outperform Informer
by 31.1% for 30-day-ahead forecasting. Meanwhile, as an efficient and robust
model, XGBoost, while learning solely from time features, performs
competitively with DL methods. Moreover, we investigate the impacts of various
factors like input sequence length, holiday traffic, data granularity, and
training data size. The findings offer valuable insights and serve as a
reference for future long-term traffic forecasting research and the improvement
of AI's corresponding learning capabilities.

</details>


### [161] [Synergy-CLIP: Extending CLIP with Multi-modal Integration for Robust Representation Learning](https://arxiv.org/abs/2504.21375)
*Sangyeon Cho,Jangyeong Jeon,Mingi Kim,Junyeong Kim*

Main category: cs.LG

TL;DR: Synergy-CLIP扩展了CLIP架构，通过整合视觉、文本和音频模态来增强多模态表示学习，并提出了VGG-sound+数据集以解决数据平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注双模态交互，无法充分利用多模态数据的丰富性，且缺乏平衡的多模态数据集。

Method: 提出Synergy-CLIP框架，通过对比学习对齐三种模态的潜在信息，并构建VGG-sound+数据集。

Result: 在零样本分类等任务中表现优于基线，并能重建缺失模态。

Conclusion: Synergy-CLIP为多模态表示学习提供了新方向，并展示了模态协同的潜力。

Abstract: Multi-modal representation learning has become a pivotal area in artificial
intelligence, enabling the integration of diverse modalities such as vision,
text, and audio to solve complex problems. However, existing approaches
predominantly focus on bimodal interactions, such as image-text pairs, which
limits their ability to fully exploit the richness of multi-modal data.
Furthermore, the integration of modalities in equal-scale environments remains
underexplored due to the challenges of constructing large-scale, balanced
datasets. In this study, we propose Synergy-CLIP, a novel framework that
extends the contrastive language-image pre-training (CLIP) architecture to
enhance multi-modal representation learning by integrating visual, textual, and
audio modalities. Unlike existing methods that focus on adapting individual
modalities to vanilla-CLIP, Synergy-CLIP aligns and captures latent information
across three modalities equally. To address the high cost of constructing
large-scale multi-modal datasets, we introduce VGG-sound+, a triple-modal
dataset designed to provide equal-scale representation of visual, textual, and
audio data. Synergy-CLIP is validated on various downstream tasks, including
zero-shot classification, where it outperforms existing baselines.
Additionally, we introduce a missing modality reconstruction task,
demonstrating Synergy-CLIP's ability to extract synergy among modalities in
realistic application scenarios. These contributions provide a robust
foundation for advancing multi-modal representation learning and exploring new
research directions.

</details>


### [162] [Sparse-to-Sparse Training of Diffusion Models](https://arxiv.org/abs/2504.21380)
*Inês Cardoso Oliveira,Decebal Constantin Mocanu,Luis A. Leiva*

Main category: cs.LG

TL;DR: 本文提出了一种稀疏到稀疏的训练范式，首次应用于扩散模型（DMs），旨在提高训练和推理效率。实验表明，稀疏DMs在减少参数和计算量的同时，性能与密集模型相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 尽管DMs在图像合成等任务中表现出色，但其计算资源需求高。此前工作主要关注推理效率，本文则首次探索稀疏训练对DMs的影响。

Method: 采用三种方法（Static-DM、RigL-DM和MagRan-DM）在六个数据集上训练稀疏DMs（Latent Diffusion和ChiroDiff），研究稀疏性对性能的影响。

Result: 稀疏DMs在减少可训练参数和FLOPs的同时，性能与密集模型相当或更优，并确定了稀疏训练的安全有效值。

Conclusion: 稀疏到稀疏训练可显著提升DMs的效率，同时保持性能，为资源受限场景提供了可行方案。

Abstract: Diffusion models (DMs) are a powerful type of generative models that have
achieved state-of-the-art results in various image synthesis tasks and have
shown potential in other domains, such as natural language processing and
temporal data modeling. Despite their stable training dynamics and ability to
produce diverse high-quality samples, DMs are notorious for requiring
significant computational resources, both in the training and inference stages.
Previous work has focused mostly on increasing the efficiency of model
inference. This paper introduces, for the first time, the paradigm of
sparse-to-sparse training to DMs, with the aim of improving both training and
inference efficiency. We focus on unconditional generation and train sparse DMs
from scratch (Latent Diffusion and ChiroDiff) on six datasets using three
different methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of
sparsity in model performance. Our experiments show that sparse DMs are able to
match and often outperform their Dense counterparts, while substantially
reducing the number of trainable parameters and FLOPs. We also identify safe
and effective values to perform sparse-to-sparse training of DMs.

</details>


### [163] [FAST-Q: Fast-track Exploration with Adversarially Balanced State Representations for Counterfactual Action Estimation in Offline Reinforcement Learning](https://arxiv.org/abs/2504.21383)
*Pulkit Agrawal,Rukma Talwadker,Aditya Pareek,Tridib Mukherjee*

Main category: cs.LG

TL;DR: FAST-Q提出了一种新的离线强化学习方法，通过梯度反转学习和Q值分解策略，解决了现有方法在稀疏状态空间和玩家心理驱动的推荐系统中的局限性，显著提升了玩家收益和平台表现。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法在处理稀疏状态空间和玩家心理驱动的推荐系统时表现不佳，亟需一种新方法来解决这些问题。

Method: FAST-Q结合梯度反转学习构建平衡状态表示，支持离线反事实探索，并提出Q值分解策略进行多目标优化。

Result: FAST-Q在玩家收益、平台停留时间和推荐成本等方面显著优于现有方法，具体表现为玩家收益提升0.15%，LTV提升2%，推荐驱动的参与度提升0.4%，平台停留时间提升2%，推荐成本降低10%。

Conclusion: FAST-Q通过创新的方法在复杂推荐系统中实现了显著性能提升，为离线强化学习在类似高波动性场景中的应用提供了新思路。

Abstract: Recent advancements in state-of-the-art (SOTA) offline reinforcement learning
(RL) have primarily focused on addressing function approximation errors, which
contribute to the overestimation of Q-values for out-of-distribution actions, a
challenge that static datasets exacerbate. However, high stakes applications
such as recommendation systems in online gaming, introduce further complexities
due to player's psychology (intent) driven by gameplay experiences and the
inherent volatility on the platform. These factors create highly sparse,
partially overlapping state spaces across policies, further influenced by the
experiment path selection logic which biases state spaces towards specific
policies. Current SOTA methods constrain learning from such offline data by
clipping known counterfactual actions as out-of-distribution due to poor
generalization across unobserved states. Further aggravating conservative
Q-learning and necessitating more online exploration. FAST-Q introduces a novel
approach that (1) leverages Gradient Reversal Learning to construct balanced
state representations, regularizing the policy-specific bias between the
player's state and action thereby enabling counterfactual estimation; (2)
supports offline counterfactual exploration in parallel with static data
exploitation; and (3) proposes a Q-value decomposition strategy for
multi-objective optimization, facilitating explainable recommendations over
short and long-term objectives. These innovations demonstrate superiority of
FAST-Q over prior SOTA approaches and demonstrates at least 0.15 percent
increase in player returns, 2 percent improvement in lifetime value (LTV), 0.4
percent enhancement in the recommendation driven engagement, 2 percent
improvement in the player's platform dwell time and an impressive 10 percent
reduction in the costs associated with the recommendation, on our volatile
gaming platform.

</details>


### [164] [Enhanced Semi-Supervised Stamping Process Monitoring with Physically-Informed Feature Extraction](https://arxiv.org/abs/2504.21389)
*Jianyu Zhang,Jianshe Feng,Yizhang Zhu,Fanyu Qi*

Main category: cs.LG

TL;DR: 提出了一种半监督的冲压过程异常监测框架，结合加速度信号和物理信息，有效捕捉异常，减少批量缺陷风险并提高产量。


<details>
  <summary>Details</summary>
Motivation: 解决冲压过程中频繁出现的异常问题，提高生产效率和产品质量。

Method: 提出混合特征提取算法，结合数据驱动和物理机制；建立半监督异常检测模型，仅用正常样本构建基线模型，并设计新的偏差评分量化异常程度。

Result: 验证了特征提取方法的有效性，并在实际冲压车间数据中展示了框架的优越性能。

Conclusion: 该框架能有效监测冲压过程异常，提升生产质量和效率。

Abstract: In tackling frequent anomalies in stamping processes, this study introduces a
novel semi-supervised in-process anomaly monitoring framework, utilizing
accelerometer signals and physics information, to capture the process anomaly
effectively. The proposed framework facilitates the construction of a
monitoring model with imbalanced sample distribution, which enables in-process
condition monitoring in real-time to prevent batch anomalies, which helps to
reduce batch defects risk and enhance production yield. Firstly, to effectively
capture key features from raw data containing redundant information, a hybrid
feature extraction algorithm is proposed to utilize data-driven methods and
physical mechanisms simultaneously. Secondly, to address the challenge brought
by imbalanced sample distribution, a semi-supervised anomaly detection model is
established, which merely employs normal samples to build a golden baseline
model, and a novel deviation score is proposed to quantify the anomaly level of
each online stamping stroke. The effectiveness of the proposed feature
extraction method is validated with various classification algorithms. A
real-world in-process dataset from stamping manufacturing workshop is employed
to illustrate the superiority of proposed semi-supervised framework with
enhance performance for process anomaly monitoring.

</details>


### [165] [MPEC: Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers](https://arxiv.org/abs/2504.21427)
*Shermin Shahbazi,Mohammad-Reza Nasiri,Majid Ramezani*

Main category: cs.LG

TL;DR: MPEC方法通过保留EEG数据的流形结构，结合协方差矩阵和RBF核的特征工程，以及改进的K-means聚类，显著提升了EEG信号分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有EEG信号分类方法未能考虑数据的非欧几里得流形结构，导致性能不佳。

Method: MPEC结合协方差矩阵和RBF核进行特征提取，并使用改进的K-means算法在黎曼流形空间聚类，最后集成多个分类器。

Result: 在BCI Competition IV数据集2a上验证了MPEC的显著性能提升。

Conclusion: MPEC通过保留流形结构和集成分类器，有效提升了EEG信号分类的准确性。

Abstract: Accurate classification of EEG signals is crucial for brain-computer
interfaces (BCIs) and neuroprosthetic applications, yet many existing methods
fail to account for the non-Euclidean, manifold structure of EEG data,
resulting in suboptimal performance. Preserving this manifold information is
essential to capture the true geometry of EEG signals, but traditional
classification techniques largely overlook this need. To this end, we propose
MPEC (Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based
Classifiers), that introduces two key innovations: (1) a feature engineering
phase that combines covariance matrices and Radial Basis Function (RBF) kernels
to capture both linear and non-linear relationships among EEG channels, and (2)
a clustering phase that employs a modified K-means algorithm tailored for the
Riemannian manifold space, ensuring local geometric sensitivity. Ensembling
multiple clustering-based classifiers, MPEC achieves superior results,
validated by significant improvements on the BCI Competition IV dataset 2a.

</details>


### [166] [Whispers of Data: Unveiling Label Distributions in Federated Learning Through Virtual Client Simulation](https://arxiv.org/abs/2504.21436)
*Zhixuan Ma,Haichang Gao,Junxiang Huang,Ping Wang*

Main category: cs.LG

TL;DR: 提出一种新颖的标签分布推断攻击方法，适用于多种场景，且在差分隐私防御下仍有效。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中标签推断攻击对受害者客户端设置敏感且防御策略下表现不佳，需更稳定且适应性强的方法。

Method: 估计受害者数据集大小，构建虚拟客户端，量化标签时间泛化性，训练推断模型预测标签分布。

Result: 在多个数据集上验证，优于现有技术，且在差分隐私防御下仍有效。

Conclusion: 该方法稳定、适应性强，具有实际应用潜力。

Abstract: Federated Learning enables collaborative training of a global model across
multiple geographically dispersed clients without the need for data sharing.
However, it is susceptible to inference attacks, particularly label inference
attacks.
  Existing studies on label distribution inference exhibits sensitive to the
specific settings of the victim client and typically underperforms under
defensive strategies. In this study, we propose a novel label distribution
inference attack that is stable and adaptable to various scenarios.
Specifically, we estimate the size of the victim client's dataset and construct
several virtual clients tailored to the victim client. We then quantify the
temporal generalization of each class label for the virtual clients and utilize
the variation in temporal generalization to train an inference model that
predicts the label distribution proportions of the victim client.
  We validate our approach on multiple datasets, including MNIST,
Fashion-MNIST, FER2013, and AG-News. The results demonstrate the superiority of
our method compared to state-of-the-art techniques. Furthermore, our attack
remains effective even under differential privacy defense mechanisms,
underscoring its potential for real-world applications.

</details>


### [167] [xEEGNet: Towards Explainable AI in EEG Dementia Classification](https://arxiv.org/abs/2504.21457)
*Andrea Zanola,Louis Fabrice Tshimanga,Federico Del Pup,Marco Baiesi,Manfredo Atzori*

Main category: cs.LG

TL;DR: xEEGNet是一种新型、紧凑且可解释的神经网络，用于EEG数据分析，具有参数少、抗过拟合和可解释性强的特点，适用于多种神经系统疾病分类。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度学习模型在EEG数据分析中参数过多、难以解释的问题，同时保持分类性能。

Method: 通过分析并改进ShallowNet结构，逐步实现从“黑箱”到透明模型的转变，使用Nested-Leave-N-Subjects-Out交叉验证评估性能。

Result: xEEGNet仅用168个参数（比ShallowNet少200倍），性能相近（中位数下降1.5%），且能有效减少数据分割间的变异性。

Conclusion: 研究表明，小型架构如xEEGNet在EEG病理分类中同样有效，兼具性能和可解释性。

Abstract: This work presents xEEGNet, a novel, compact, and explainable neural network
for EEG data analysis. It is fully interpretable and reduces overfitting
through major parameter reduction. As an applicative use case, we focused on
classifying common dementia conditions, Alzheimer's and frontotemporal
dementia, versus controls. xEEGNet is broadly applicable to other neurological
conditions involving spectral alterations. We initially used ShallowNet, a
simple and popular model from the EEGNet-family. Its structure was analyzed and
gradually modified to move from a "black box" to a more transparent model,
without compromising performance. The learned kernels and weights were examined
from a clinical standpoint to assess medical relevance. Model variants,
including ShallowNet and the final xEEGNet, were evaluated using robust
Nested-Leave-N-Subjects-Out cross-validation for unbiased performance
estimates. Variability across data splits was explained using embedded EEG
representations, grouped by class and set, with pairwise separability to
quantify group distinction. Overfitting was assessed through
training-validation loss correlation and training speed. xEEGNet uses only 168
parameters, 200 times fewer than ShallowNet, yet retains interpretability,
resists overfitting, achieves comparable median performance (-1.5%), and
reduces variability across splits. This variability is explained by embedded
EEG representations: higher accuracy correlates with greater separation between
test set controls and Alzheimer's cases, without significant influence from
training data. xEEGNet's ability to filter specific EEG bands, learn
band-specific topographies, and use relevant spectral features demonstrates its
interpretability. While large deep learning models are often prioritized for
performance, this study shows smaller architectures like xEEGNet can be equally
effective in EEG pathology classification.

</details>


### [168] [Deep Learning Optimization Using Self-Adaptive Weighted Auxiliary Variables](https://arxiv.org/abs/2504.21501)
*Yaru Liu,Yiqi Gu,Michael K. Ng*

Main category: cs.LG

TL;DR: 提出了一种新的优化框架，通过引入辅助变量和自适应权重改进深度神经网络的梯度下降效率。


<details>
  <summary>Details</summary>
Motivation: 梯度下降在深度学习中因损失函数高度非凸和梯度消失问题效率低下，需改进优化方法。

Method: 引入辅助变量分离网络层，重新设计损失函数，并使用自适应权重保持一致性。

Result: 数值实验验证了新框架的一致性和对梯度下降的有效性及鲁棒性。

Conclusion: 新优化框架显著提升了梯度下降在深度学习中的表现。

Abstract: In this paper, we develop a new optimization framework for the least squares
learning problem via fully connected neural networks or physics-informed neural
networks. The gradient descent sometimes behaves inefficiently in deep learning
because of the high non-convexity of loss functions and the vanishing gradient
issue. Our idea is to introduce auxiliary variables to separate the layers of
the deep neural networks and reformulate the loss functions for ease of
optimization. We design the self-adaptive weights to preserve the consistency
between the reformulated loss and the original mean squared loss, which
guarantees that optimizing the new loss helps optimize the original problem.
Numerical experiments are presented to verify the consistency and show the
effectiveness and robustness of our models over gradient descent.

</details>


### [169] [Towards proactive self-adaptive AI for non-stationary environments with dataset shifts](https://arxiv.org/abs/2504.21565)
*David Fernández Narro,Pablo Ferri,Juan M. García-Gómez,Carlos Sáez*

Main category: cs.LG

TL;DR: 提出了一种主动自适应的AI方法（pro-adaptive），通过建模AI参数的时间轨迹来预测短期参数值，以应对非平稳环境中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 医疗环境中，AI模型常因数据分布随时间变化（如时间性数据集偏移）而性能下降，且缺乏及时的新标注数据用于重新训练。

Method: 使用多项式样条基和功能性数据分析框架建模参数时间轨迹，并在逻辑回归模型中验证其对先验概率偏移、协变量偏移和概念偏移的适应性。

Result: 在模拟数据集和真实COVID-19数据集上的实验表明，该方法显著提升了AI在偏移环境中的性能，无需更新训练数据。

Conclusion: 为动态非平稳环境中的自适应AI研究奠定了基础，适用于医疗等数据保护严格的领域。

Abstract: Artificial Intelligence (AI) models deployed in production frequently face
challenges in maintaining their performance in non-stationary environments.
This issue is particularly noticeable in medical settings, where temporal
dataset shifts often occur. These shifts arise when the distributions of
training data differ from those of the data encountered during deployment over
time. Further, new labeled data to continuously retrain AI is not typically
available in a timely manner due to data access limitations. To address these
challenges, we propose a proactive self-adaptive AI approach, or pro-adaptive,
where we model the temporal trajectory of AI parameters, allowing us to
short-term forecast parameter values. To this end, we use polynomial spline
bases, within an extensible Functional Data Analysis framework. We validate our
methodology with a logistic regression model addressing prior probability
shift, covariate shift, and concept shift. This validation is conducted on both
a controlled simulated dataset and a publicly available real-world COVID-19
dataset from Mexico, with various shifts occurring between 2020 and 2024. Our
results indicate that this approach enhances the performance of AI against
shifts compared to baseline stable models trained at different time distances
from the present, without requiring updated training data. This work lays the
foundation for pro-adaptive AI research against dynamic, non-stationary
environments, being compatible with data protection, in resilient AI production
environments for health.

</details>


### [170] [On Advancements of the Forward-Forward Algorithm](https://arxiv.org/abs/2504.21662)
*Mauricio Ortiz Torres,Markus Lange,Arne P. Raulf*

Main category: cs.LG

TL;DR: Forward-Forward算法通过改进技术（如卷积通道分组、学习率调度和独立块结构）在CIFAR10数据集上表现更优，测试错误率降低20%。同时，轻量级模型在低容量硬件上表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决复杂任务并优化算法性能，同时适应低容量硬件的需求。

Method: 采用卷积通道分组、学习率调度和独立块结构进行训练。

Result: 测试错误率降低20%，轻量级模型在低容量硬件上表现良好（测试错误率21±6%）。

Conclusion: 改进的Forward-Forward算法在性能和硬件适应性上均有提升，为未来神经网络验证研究奠定基础。

Abstract: The Forward-Forward algorithm has evolved in machine learning research,
tackling more complex tasks that mimic real-life applications. In the last
years, it has been improved by several techniques to perform better than its
original version, handling a challenging dataset like CIFAR10 without losing
its flexibility and low memory usage. We have shown in our results that
improvements are achieved through a combination of convolutional channel
grouping, learning rate schedules, and independent block structures during
training that lead to a 20\% decrease in test error percentage. Additionally,
to approach further implementations on low-capacity hardware projects we have
presented a series of lighter models that achieve low test error percentages
within (21$\pm$6)\% and number of trainable parameters between 164,706 and
754,386. This serving also as a basis for our future study on complete
verification and validation of these kinds of neural networks.

</details>


### [171] [Recursive KL Divergence Optimization: A Dynamic Framework for Representation Learning](https://arxiv.org/abs/2504.21707)
*Anthony D Martin*

Main category: cs.LG

TL;DR: 论文提出了一种递归KL散度优化（RKDO）方法，通过动态调整局部条件分布的KL散度，改进了现有表示学习目标，实验显示其效率和性能显著优于静态方法。


<details>
  <summary>Details</summary>
Motivation: 现有表示学习框架（如I-Con）通过固定邻域条件的KL散度统一学习范式，但忽视了学习过程中的递归结构。

Method: 引入RKDO，将表示学习视为KL散度在数据邻域上的动态演化，涵盖对比学习、聚类和降维方法。

Result: 实验表明，RKDO在三个数据集上损失值降低约30%，计算资源节省60%-80%。

Conclusion: RKDO的递归更新机制为表示学习提供了更高效的优化路径，尤其适用于资源受限场景。

Abstract: We propose a generalization of modern representation learning objectives by
reframing them as recursive divergence alignment processes over localized
conditional distributions While recent frameworks like Information Contrastive
Learning I-Con unify multiple learning paradigms through KL divergence between
fixed neighborhood conditionals we argue this view underplays a crucial
recursive structure inherent in the learning process. We introduce Recursive KL
Divergence Optimization RKDO a dynamic formalism where representation learning
is framed as the evolution of KL divergences across data neighborhoods. This
formulation captures contrastive clustering and dimensionality reduction
methods as static slices while offering a new path to model stability and local
adaptation. Our experiments demonstrate that RKDO offers dual efficiency
advantages approximately 30 percent lower loss values compared to static
approaches across three different datasets and 60 to 80 percent reduction in
computational resources needed to achieve comparable results. This suggests
that RKDOs recursive updating mechanism provides a fundamentally more efficient
optimization landscape for representation learning with significant
implications for resource constrained applications.

</details>


### [172] [Learning Heterogeneous Performance-Fairness Trade-offs in Federated Learning](https://arxiv.org/abs/2504.21775)
*Rongguang Ye,Ming Tang*

Main category: cs.LG

TL;DR: HetPFL提出了一种新方法，通过自适应偏好采样和超网络融合，有效学习联邦学习中的局部和全局Pareto前沿，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在联邦学习中采用均匀偏好采样分布，忽略了局部Pareto前沿的异质性，且未考虑局部与全局Pareto前沿的差距。

Method: HetPFL包含偏好采样适应（PSA）和偏好感知超网络融合（PHF），分别优化局部采样分布和全局超网络性能。

Result: 实验表明，HetPFL在四个数据集上显著优于七个基线方法，且理论证明其收敛性更强。

Conclusion: HetPFL通过自适应采样和融合策略，有效解决了联邦学习中的性能-公平性权衡问题。

Abstract: Recent methods leverage a hypernet to handle the performance-fairness
trade-offs in federated learning. This hypernet maps the clients' preferences
between model performance and fairness to preference-specifc models on the
trade-off curve, known as local Pareto front. However, existing methods
typically adopt a uniform preference sampling distribution to train the
hypernet across clients, neglecting the inherent heterogeneity of their local
Pareto fronts. Meanwhile, from the perspective of generalization, they do not
consider the gap between local and global Pareto fronts on the global dataset.
To address these limitations, we propose HetPFL to effectively learn both local
and global Pareto fronts. HetPFL comprises Preference Sampling Adaptation (PSA)
and Preference-aware Hypernet Fusion (PHF). PSA adaptively determines the
optimal preference sampling distribution for each client to accommodate
heterogeneous local Pareto fronts. While PHF performs preference-aware fusion
of clients' hypernets to ensure the performance of the global Pareto front. We
prove that HetPFL converges linearly with respect to the number of rounds,
under weaker assumptions than existing methods. Extensive experiments on four
datasets show that HetPFL significantly outperforms seven baselines in terms of
the quality of learned local and global Pareto fronts.

</details>


### [173] [Stable Trajectory Clustering: An Efficient Split and Merge Algorithm](https://arxiv.org/abs/2504.21808)
*Atieh Rahmani,Mansoor Davoodi,Justin M. Calabrese*

Main category: cs.LG

TL;DR: 本文提出基于DBSCAN线段聚类的全轨迹和子轨迹聚类算法，引入稳定轨迹聚类算法，通过选择性忽略瞬态偏差提升聚类稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹聚类算法因临时异常数据分割轨迹，导致聚类模式模糊和可靠性降低，需改进。

Method: 基于DBSCAN线段聚类，利用线段的分裂与合并事件，结合滑动窗口模型和平均欧氏距离。

Result: 实验表明，稳定轨迹聚类算法能有效提升聚类稳定性和可解释性。

Conclusion: 选择性忽略瞬态偏差可优化轨迹聚类效果，算法在真实数据集上表现良好。

Abstract: Clustering algorithms group data points by characteristics to identify
patterns. Over the past two decades, researchers have extended these methods to
analyze trajectories of humans, animals, and vehicles, studying their behavior
and movement across applications. This paper presents whole-trajectory
clustering and sub-trajectory clustering algorithms based on DBSCAN line
segment clustering, which encompasses two key events: split and merge of line
segments. The events are employed by object movement history and the average
Euclidean distance between line segments. In this framework, whole-trajectory
clustering considers entire entities' trajectories, whereas sub-trajectory
clustering employs a sliding window model to identify similar sub-trajectories.
Many existing trajectory clustering algorithms respond to temporary anomalies
in data by splitting trajectories, which often obscures otherwise consistent
clustering patterns and leads to less reliable insights. We introduce the
stable trajectory clustering algorithm, which leverages the mean absolute
deviation concept to demonstrate that selective omission of transient
deviations not only preserves the integrity of clusters but also improves their
stability and interpretability. We run all proposed algorithms on real
trajectory datasets to illustrate their effectiveness and sensitivity to
parameter variations.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [174] [Automated Generation of Precedence Graphs in Digital Value Chains for Automotive Production](https://arxiv.org/abs/2504.19835)
*Cornelius Hake,Christian Friedrich*

Main category: cs.RO

TL;DR: 论文提出了一种基于混合整数线性规划的自动化调度算法，优化汽车制造中的数字价值链，显著提升效率与适应性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在优化汽车制造中电子控制单元的识别、软件刷新、定制和调试流程，以解决传统方法效率低、成本高的问题。

Method: 提出了一种新颖的前驱图设计，结合混合整数线性编程技术，实现自动化调度算法。

Result: 算法减少了昂贵硬件和软件的生产站数量，提高产能利用率，任务并行化优化，准备时间减少50%，调度活动大幅简化。

Conclusion: 自动化调度在效率、功能和适应性上显著优于传统手动方法，支持车辆特定配置并提升响应速度。

Abstract: This study examines the digital value chain in automotive manufacturing,
focusing on the identification, software flashing, customization, and
commissioning of electronic control units in vehicle networks. A novel
precedence graph design is proposed to optimize this process chain using an
automated scheduling algorithm that employs mixed integer linear programming
techniques. The results show significant improvements in key metrics. The
algorithm reduces the number of production stations equipped with expensive
hardware and software to execute digital value chain processes, while
increasing capacity utilization through efficient scheduling and reduced idle
time. Task parallelization is optimized, resulting in streamlined workflows and
increased throughput. Compared to the traditional method, the automated
approach has reduced preparation time by 50% and reduced scheduling activities,
as it now takes two minutes to create the precedence graph. The flexibility of
the algorithm's constraints allows for vehicle-specific configurations while
maintaining high responsiveness, eliminating backup stations and facilitating
the integration of new topologies. Automated scheduling significantly
outperforms manual methods in efficiency, functionality, and adaptability.

</details>


### [175] [LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics](https://arxiv.org/abs/2504.21716)
*Marc Glocker,Peter Hönig,Matthias Hirschmanner,Markus Vincze*

Main category: cs.RO

TL;DR: 该论文提出了一种基于LLM驱动的多智能体机器人系统，用于家庭物品的自主管理，通过任务规划、记忆增强和上下文学习实现高效操作。


<details>
  <summary>Details</summary>
Motivation: 解决家庭环境中机器人自主管理物品的复杂性问题，同时避免显式模型训练的需求。

Method: 采用路由代理、任务规划代理和知识库代理三种专用智能体，结合RAG和Grounded SAM等技术实现任务规划和场景理解。

Result: 在三种家庭场景中表现出高任务规划准确性和记忆召回率，Qwen2.5和LLaMA3.1在不同任务中表现最佳。

Conclusion: 该系统通过多智能体协作和上下文学习，显著提升了家庭物品管理的效率和准确性。

Abstract: We present an embodied robotic system with an LLM-driven agent-orchestration
architecture for autonomous household object management. The system integrates
memory-augmented task planning, enabling robots to execute high-level user
commands while tracking past actions. It employs three specialized agents: a
routing agent, a task planning agent, and a knowledge base agent, each powered
by task-specific LLMs. By leveraging in-context learning, our system avoids the
need for explicit model training. RAG enables the system to retrieve context
from past interactions, enhancing long-term object tracking. A combination of
Grounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating
semantic scene understanding for task planning. Evaluation across three
household scenarios demonstrates high task planning accuracy and an improvement
in memory recall due to RAG. Specifically, Qwen2.5 yields best performance for
specialized agents, while LLaMA3.1 excels in routing tasks. The source code is
available at: https://github.com/marc1198/chat-hsr.

</details>


### [176] [UAV-VLN: End-to-End Vision Language guided Navigation for UAVs](https://arxiv.org/abs/2504.21432)
*Pranav Saxena,Nishant Raghuvanshi,Neena Goveas*

Main category: cs.RO

TL;DR: UAV-VLN是一个结合大型语言模型（LLM）与视觉感知的端到端框架，用于无人机（UAV）的自然语言导航，能够解析自由形式的指令并规划可行路径。


<details>
  <summary>Details</summary>
Motivation: 解决AI自主导航中基于自然语言指令在未知环境中导航的核心挑战。

Method: 整合LLM的常识推理能力和视觉模型的对象检测，通过跨模态对齐机制实现语言意图与视觉上下文的融合。

Result: 在多样化的室内外导航场景中，UAV-VLN表现出对新指令和环境的泛化能力，显著提高了指令遵循准确性和轨迹效率。

Conclusion: LLM驱动的视觉语言接口为无人机提供了安全、直观且可泛化的自主导航能力。

Abstract: A core challenge in AI-guided autonomy is enabling agents to navigate
realistically and effectively in previously unseen environments based on
natural language commands. We propose UAV-VLN, a novel end-to-end
Vision-Language Navigation (VLN) framework for Unmanned Aerial Vehicles (UAVs)
that seamlessly integrates Large Language Models (LLMs) with visual perception
to facilitate human-interactive navigation. Our system interprets free-form
natural language instructions, grounds them into visual observations, and plans
feasible aerial trajectories in diverse environments.
  UAV-VLN leverages the common-sense reasoning capabilities of LLMs to parse
high-level semantic goals, while a vision model detects and localizes
semantically relevant objects in the environment. By fusing these modalities,
the UAV can reason about spatial relationships, disambiguate references in
human instructions, and plan context-aware behaviors with minimal task-specific
supervision. To ensure robust and interpretable decision-making, the framework
includes a cross-modal grounding mechanism that aligns linguistic intent with
visual context.
  We evaluate UAV-VLN across diverse indoor and outdoor navigation scenarios,
demonstrating its ability to generalize to novel instructions and environments
with minimal task-specific training. Our results show significant improvements
in instruction-following accuracy and trajectory efficiency, highlighting the
potential of LLM-driven vision-language interfaces for safe, intuitive, and
generalizable UAV autonomy.

</details>


### [177] [RoboGround: Robotic Manipulation with Grounded Vision-Language Priors](https://arxiv.org/abs/2504.21530)
*Haifeng Huang,Xinyi Chen,Yilun Chen,Hao Li,Xiaoshen Han,Zehan Wang,Tai Wang,Jiangmiao Pang,Zhou Zhao*

Main category: cs.RO

TL;DR: 论文提出了一种基于grounding masks的中间表示方法RoboGround，用于提升机器人操作策略的泛化能力，并通过自动化生成大规模模拟数据验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过中间表示（grounding masks）提升机器人操作策略的泛化能力，结合视觉语言模型的优势。

Method: 提出RoboGround系统，利用grounding masks作为中间表示指导策略网络，并通过自动化管道生成大规模模拟数据。

Result: 实验表明grounding masks作为中间指导显著提升了策略的泛化能力。

Conclusion: grounding masks是一种有效的中间表示方法，能够显著提升机器人操作策略的泛化性能。

Abstract: Recent advancements in robotic manipulation have highlighted the potential of
intermediate representations for improving policy generalization. In this work,
we explore grounding masks as an effective intermediate representation,
balancing two key advantages: (1) effective spatial guidance that specifies
target objects and placement areas while also conveying information about
object shape and size, and (2) broad generalization potential driven by
large-scale vision-language models pretrained on diverse grounding datasets. We
introduce RoboGround, a grounding-aware robotic manipulation system that
leverages grounding masks as an intermediate representation to guide policy
networks in object manipulation tasks. To further explore and enhance
generalization, we propose an automated pipeline for generating large-scale,
simulated data with a diverse set of objects and instructions. Extensive
experiments show the value of our dataset and the effectiveness of grounding
masks as intermediate guidance, significantly enhancing the generalization
abilities of robot policies.

</details>


### [178] [UAV Marketplace Simulation Tool for BVLOS Operations](https://arxiv.org/abs/2504.21428)
*Kıvanç Şerefoğlu,Önder Gürcan,Reyhan Aydoğan*

Main category: cs.RO

TL;DR: 该论文提出了一种用于评估多无人机（UAV）团队形成的仿真工具，支持超视距（BVLOS）任务，模拟动态和对抗条件下的协作与任务执行。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一个工具，用于测试和改进无人机在对抗性环境中的团队协作策略，尤其是在存在拜占庭无人机干扰的情况下。

Method: 方法是通过仿真工具建模无人机协作和任务执行，支持配置任务参数和对抗行为，并记录模拟日志和性能指标以便统计分析。

Result: 结果是该工具能够帮助研究人员在受控环境中集成和比较不同的团队形成策略，适用于实际应用的无人机协调策略测试和改进。

Conclusion: 结论是该仿真工具为无人机团队协作策略的研究和优化提供了灵活且实用的解决方案。

Abstract: We present a simulation tool for evaluating team formation in autonomous
multi-UAV (Unmanned Aerial Vehicle) missions that operate Beyond Visual Line of
Sight (BVLOS). The tool models UAV collaboration and mission execution in
dynamic and adversarial conditions, where Byzantine UAVs attempt to disrupt
operations. Our tool allows researchers to integrate and compare various team
formation strategies in a controlled environment with configurable mission
parameters and adversarial behaviors. The log of each simulation run is stored
in a structured way along with performance metrics so that statistical analysis
could be done straightforwardly. The tool is versatile for testing and
improving UAV coordination strategies in real-world applications.

</details>


### [179] [Real Time Semantic Segmentation of High Resolution Automotive LiDAR Scans](https://arxiv.org/abs/2504.21602)
*Hannes Reichert,Benjamin Serfling,Elijah Schüssler,Kerim Turacan,Konrad Doll,Bernhard Sick*

Main category: cs.RO

TL;DR: 提出了一种针对高分辨率LiDAR传感器的语义分割框架，解决了精度和实时处理需求，并提供了新的数据集和ROS2实现。


<details>
  <summary>Details</summary>
Motivation: 现有方法多基于过时的低分辨率LiDAR传感器，难以满足实时性需求，需针对现代高分辨率传感器优化。

Method: 利用表面法线作为输入特征，提出新的语义分割方法，并基于128层LiDAR采集数据集。

Result: 框架在精度和实时性上表现优异，填补了前沿研究与实际应用间的差距。

Conclusion: 该方法为自动驾驶系统提供了实用工具，数据集和代码已开源。

Abstract: In recent studies, numerous previous works emphasize the importance of
semantic segmentation of LiDAR data as a critical component to the development
of driver-assistance systems and autonomous vehicles. However, many
state-of-the-art methods are tested on outdated, lower-resolution LiDAR sensors
and struggle with real-time constraints. This study introduces a novel semantic
segmentation framework tailored for modern high-resolution LiDAR sensors that
addresses both accuracy and real-time processing demands. We propose a novel
LiDAR dataset collected by a cutting-edge automotive 128 layer LiDAR in urban
traffic scenes. Furthermore, we propose a semantic segmentation method
utilizing surface normals as strong input features. Our approach is bridging
the gap between cutting-edge research and practical automotive applications.
Additionaly, we provide a Robot Operating System (ROS2) implementation that we
operate on our research vehicle. Our dataset and code are publicly available:
https://github.com/kav-institute/SemanticLiDAR.

</details>


### [180] [SimPRIVE: a Simulation framework for Physical Robot Interaction with Virtual Environments](https://arxiv.org/abs/2504.21454)
*Federico Nesti,Gianluca D'Amico,Mauro Marinoni,Giorgio Buttazzo*

Main category: cs.RO

TL;DR: SimPRIVE是一个用于物理机器人与虚拟环境交互的仿真框架，支持ROS 2的移动机器人通过数字孪生在Unreal Engine 5构建的虚拟世界中运行，用于安全高效地测试复杂算法。


<details>
  <summary>Details</summary>
Motivation: 机器学习和强化学习在物理系统中的行为不可预测，而真实世界测试成本高且危险，因此需要一种高效的仿真测试平台。

Method: 提出SimPRIVE框架，将物理机器人与虚拟环境结合，通过数字孪生技术运行，支持自定义虚拟场景和快速渲染。

Result: 验证了强化学习代理在虚拟办公室环境中避障的有效性，物理机器人在有限空间内无碰撞运行。

Conclusion: SimPRIVE为复杂算法的测试提供了低成本、低风险的解决方案，适用于硬件和软件堆栈的全面测试。

Abstract: The use of machine learning in cyber-physical systems has attracted the
interest of both industry and academia. However, no general solution has yet
been found against the unpredictable behavior of neural networks and
reinforcement learning agents. Nevertheless, the improvements of
photo-realistic simulators have paved the way towards extensive testing of
complex algorithms in different virtual scenarios, which would be expensive and
dangerous to implement in the real world.
  This paper presents SimPRIVE, a simulation framework for physical robot
interaction with virtual environments, which operates as a vehicle-in-the-loop
platform, rendering a virtual world while operating the vehicle in the real
world.
  Using SimPRIVE, any physical mobile robot running on ROS 2 can easily be
configured to move its digital twin in a virtual world built with the Unreal
Engine 5 graphic engine, which can be populated with objects, people, or other
vehicles with programmable behavior.
  SimPRIVE has been designed to accommodate custom or pre-built virtual worlds
while being light-weight to contain execution times and allow fast rendering.
Its main advantage lies in the possibility of testing complex algorithms on the
full software and hardware stack while minimizing the risks and costs of a test
campaign. The framework has been validated by testing a reinforcement learning
agent trained for obstacle avoidance on an AgileX Scout Mini rover that
navigates a virtual office environment where everyday objects and people are
placed as obstacles. The physical rover moves with no collision in an indoor
limited space, thanks to a LiDAR-based heuristic.

</details>


### [181] [Multi-Goal Dexterous Hand Manipulation using Probabilistic Model-based Reinforcement Learning](https://arxiv.org/abs/2504.21585)
*Yingzhuo Jiang,Wenjun Huang,Rongdun Lin,Chenyang Miao,Tianfu Sun,Yunduan Cui*

Main category: cs.RO

TL;DR: 本文提出了一种基于模型强化学习的方法（GC-PMPC），用于解决多目标灵巧手操控任务，通过概率神经网络集合和高频异步MPC策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决高维度灵巧手动态建模和实时控制频率要求的挑战，以实现高效的多目标操控任务学习。

Method: 设计了Goal-Conditioned Probabilistic Model Predictive Control (GC-PMPC)，结合概率神经网络集合和异步MPC策略。

Result: 在模拟实验中，GC-PMPC在四种Shadow Hand操控场景中表现优于现有方法，并在实际DexHand 021平台上高效完成任务。

Conclusion: GC-PMPC在灵巧手操控任务中表现出卓越的学习效率和性能，为低成本平台提供了可行的解决方案。

Abstract: This paper tackles the challenge of learning multi-goal dexterous hand
manipulation tasks using model-based Reinforcement Learning. We propose
Goal-Conditioned Probabilistic Model Predictive Control (GC-PMPC) by designing
probabilistic neural network ensembles to describe the high-dimensional
dexterous hand dynamics and introducing an asynchronous MPC policy to meet the
control frequency requirements in real-world dexterous hand systems. Extensive
evaluations on four simulated Shadow Hand manipulation scenarios with randomly
generated goals demonstrate GC-PMPC's superior performance over
state-of-the-art baselines. It successfully drives a cable-driven Dexterous
hand, DexHand 021 with 12 Active DOFs and 5 tactile sensors, to learn
manipulating a cubic die to three goal poses within approximately 80 minutes of
interactions, demonstrating exceptional learning efficiency and control
performance on a cost-effective dexterous hand platform.

</details>


### [182] [One Net to Rule Them All: Domain Randomization in Quadcopter Racing Across Different Platforms](https://arxiv.org/abs/2504.21586)
*Robin Ferede,Till Blaha,Erin Lucassen,Christophe De Wagter,Guido C. H. E. de Croon*

Main category: cs.RO

TL;DR: 提出了一种基于神经网络的通用无人机竞速控制器，通过域随机化训练，能在不同物理特性的四旋翼无人机上稳定工作。


<details>
  <summary>Details</summary>
Motivation: 解决高速无人机竞速中单一控制器难以适应不同平台的问题。

Method: 使用域随机化训练神经网络控制器，仅依赖当前状态直接计算电机指令。

Result: 在3英寸和5英寸竞速无人机上验证了控制器的通用性，尽管速度略低于专用控制器，但适应性更强。

Conclusion: 域随机化能有效提升控制器的通用性，为通用AI控制器的发展提供了方向。

Abstract: In high-speed quadcopter racing, finding a single controller that works well
across different platforms remains challenging. This work presents the first
neural network controller for drone racing that generalizes across physically
distinct quadcopters. We demonstrate that a single network, trained with domain
randomization, can robustly control various types of quadcopters. The network
relies solely on the current state to directly compute motor commands. The
effectiveness of this generalized controller is validated through real-world
tests on two substantially different crafts (3-inch and 5-inch race
quadcopters). We further compare the performance of this generalized controller
with controllers specifically trained for the 3-inch and 5-inch drone, using
their identified model parameters with varying levels of domain randomization
(0%, 10%, 20%, 30%). While the generalized controller shows slightly slower
speeds compared to the fine-tuned models, it excels in adaptability across
different platforms. Our results show that no randomization fails sim-to-real
transfer while increasing randomization improves robustness but reduces speed.
Despite this trade-off, our findings highlight the potential of domain
randomization for generalizing controllers, paving the way for universal AI
controllers that can adapt to any platform.

</details>


### [183] [Leveraging Pre-trained Large Language Models with Refined Prompting for Online Task and Motion Planning](https://arxiv.org/abs/2504.21596)
*Huihui Guo,Huilong Pi,Yunchuan Qin,Zhuo Tang,Kenli Li*

Main category: cs.RO

TL;DR: LLM-PAS是一个由大型语言模型（LLM）辅助的闭环任务规划与执行系统，通过将部分约束检查转移到执行阶段，提高了任务执行的稳定性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的发展，智能机器人需要具备任务规划和稳定执行的能力，以应对复杂任务和环境异常。

Method: LLM-PAS结合传统任务规划与执行阶段，利用LLM处理异常，并提出First Look Prompting（FLP）方法优化规划目标生成。

Result: 实验表明，LLM-PAS在处理任务执行中的异常情况时表现出高效性和鲁棒性。

Conclusion: LLM-PAS通过闭环规划和LLM辅助，显著提升了智能机器人在复杂任务中的执行能力。

Abstract: With the rapid advancement of artificial intelligence, there is an increasing
demand for intelligent robots capable of assisting humans in daily tasks and
performing complex operations. Such robots not only require task planning
capabilities but must also execute tasks with stability and robustness. In this
paper, we present a closed-loop task planning and acting system, LLM-PAS, which
is assisted by a pre-trained Large Language Model (LLM). While LLM-PAS plans
long-horizon tasks in a manner similar to traditional task and motion planners,
it also emphasizes the execution phase of the task. By transferring part of the
constraint-checking process from the planning phase to the execution phase,
LLM-PAS enables exploration of the constraint space and delivers more accurate
feedback on environmental anomalies during execution. The reasoning
capabilities of the LLM allow it to handle anomalies that cannot be addressed
by the robust executor. To further enhance the system's ability to assist the
planner during replanning, we propose the First Look Prompting (FLP) method,
which induces LLM to generate effective PDDL goals. Through comparative
prompting experiments and systematic experiments, we demonstrate the
effectiveness and robustness of LLM-PAS in handling anomalous conditions during
task execution.

</details>


### [184] [Self-Supervised Monocular Visual Drone Model Identification through Improved Occlusion Handling](https://arxiv.org/abs/2504.21695)
*Stavrow A. Bahnam,Christophe De Wagter,Guido C. H. E. de Croon*

Main category: cs.RO

TL;DR: 论文提出了一种自监督学习方案，通过单目视频和飞行控制器数据训练基于神经网络的无人机模型，解决了GPS缺失环境下的运动估计问题。


<details>
  <summary>Details</summary>
Motivation: 在GPS缺失环境中，无人机运动估计至关重要。传统视觉方法在高速飞行和复杂视觉条件下表现不佳，而现有无人机模型依赖外部运动捕捉系统，限制了可扩展性。

Method: 提出自监督学习方案，先训练自监督相对位姿估计模型作为教师，再训练无人机模型。改进遮挡处理方法，提升高速飞行时的性能。

Result: 位姿估计的均方根误差平均降低15%，无人机模型在高速飞行时比教师模型更准确。集成到VIO系统后，在复杂轨迹上表现优越。

Conclusion: 自监督学习为无人机在真实环境中的高速飞行和状态估计提供了新方法，缩小了实验室与真实应用之间的差距。

Abstract: Ego-motion estimation is vital for drones when flying in GPS-denied
environments. Vision-based methods struggle when flight speed increases and
close-by objects lead to difficult visual conditions with considerable motion
blur and large occlusions. To tackle this, vision is typically complemented by
state estimation filters that combine a drone model with inertial measurements.
However, these drone models are currently learned in a supervised manner with
ground-truth data from external motion capture systems, limiting scalability to
different environments and drones. In this work, we propose a self-supervised
learning scheme to train a neural-network-based drone model using only onboard
monocular video and flight controller data (IMU and motor feedback). We achieve
this by first training a self-supervised relative pose estimation model, which
then serves as a teacher for the drone model. To allow this to work at high
speed close to obstacles, we propose an improved occlusion handling method for
training self-supervised pose estimation models. Due to this method, the root
mean squared error of resulting odometry estimates is reduced by an average of
15%. Moreover, the student neural drone model can be successfully obtained from
the onboard data. It even becomes more accurate at higher speeds compared to
its teacher, the self-supervised vision-based model. We demonstrate the value
of the neural drone model by integrating it into a traditional filter-based VIO
system (ROVIO), resulting in superior odometry accuracy on aggressive 3D racing
trajectories near obstacles. Self-supervised learning of ego-motion estimation
represents a significant step toward bridging the gap between flying in
controlled, expensive lab environments and real-world drone applications. The
fusion of vision and drone models will enable higher-speed flight and improve
state estimation, on any drone in any environment.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [185] [Transcending Dimensions using Generative AI: Real-Time 3D Model Generation in Augmented Reality](https://arxiv.org/abs/2504.21033)
*Majid Behravan,Maryam Haghani,Denis Gracanin*

Main category: cs.GR

TL;DR: 研究结合生成式AI和AR技术，简化3D建模流程，使非专业用户也能轻松生成和操作3D模型。


<details>
  <summary>Details</summary>
Motivation: 传统3D建模技术门槛高、耗时长，研究旨在通过AI和AR技术降低使用门槛。

Method: 利用Shap-E等AI模型和Mask R-CNN等对象检测方法，解决2D图像转3D模型及AR环境中的交互问题。

Result: 35名参与者的系统可用性评分（SUS）为69.64，熟悉AR/VR技术的用户评分更高（80.71）。

Conclusion: 该系统在游戏、教育和AR电商等领域具有应用潜力，为非专业用户提供了直观的3D建模工具。

Abstract: Traditional 3D modeling requires technical expertise, specialized software,
and time-intensive processes, making it inaccessible for many users. Our
research aims to lower these barriers by combining generative AI and augmented
reality (AR) into a cohesive system that allows users to easily generate,
manipulate, and interact with 3D models in real time, directly within AR
environments. Utilizing cutting-edge AI models like Shap-E, we address the
complex challenges of transforming 2D images into 3D representations in AR
environments. Key challenges such as object isolation, handling intricate
backgrounds, and achieving seamless user interaction are tackled through
advanced object detection methods, such as Mask R-CNN. Evaluation results from
35 participants reveal an overall System Usability Scale (SUS) score of 69.64,
with participants who engaged with AR/VR technologies more frequently rating
the system significantly higher, at 80.71. This research is particularly
relevant for applications in gaming, education, and AR-based e-commerce,
offering intuitive, model creation for users without specialized skills.

</details>


### [186] [GauSS-MI: Gaussian Splatting Shannon Mutual Information for Active 3D Reconstruction](https://arxiv.org/abs/2504.21067)
*Yuhan Xie,Yixi Cai,Yinqiang Zhang,Lei Yang,Jia Pan*

Main category: cs.GR

TL;DR: 论文提出了一种基于高斯溅射的视觉不确定性量化方法（GauSS-MI），用于实时选择最佳视角，提升3D重建的视觉质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注几何完整性，而忽略了重建模型中的视觉不确定性，导致视角选择不够高效。

Method: 提出概率模型量化高斯溅射的视觉不确定性，利用香农互信息准则（GauSS-MI）实时评估新视角的视觉互信息。

Result: 实验表明，该方法在模拟和真实场景中均能显著提升视觉质量和重建效率。

Conclusion: GauSS-MI为主动3D重建提供了一种高效的视角选择方法，填补了视觉不确定性量化的空白。

Abstract: This research tackles the challenge of real-time active view selection and
uncertainty quantification on visual quality for active 3D reconstruction.
Visual quality is a critical aspect of 3D reconstruction. Recent advancements
such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have
notably enhanced the image rendering quality of reconstruction models.
Nonetheless, the efficient and effective acquisition of input images for
reconstruction-specifically, the selection of the most informative
viewpoint-remains an open challenge, which is crucial for active
reconstruction. Existing studies have primarily focused on evaluating geometric
completeness and exploring unobserved or unknown regions, without direct
evaluation of the visual uncertainty within the reconstruction model. To
address this gap, this paper introduces a probabilistic model that quantifies
visual uncertainty for each Gaussian. Leveraging Shannon Mutual Information, we
formulate a criterion, Gaussian Splatting Shannon Mutual Information
(GauSS-MI), for real-time assessment of visual mutual information from novel
viewpoints, facilitating the selection of next best view. GauSS-MI is
implemented within an active reconstruction system integrated with a view and
motion planner. Extensive experiments across various simulated and real-world
scenes showcase the superior visual quality and reconstruction efficiency
performance of the proposed system.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [187] [Don't Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense Retrieval](https://arxiv.org/abs/2504.21015)
*Aarush Sinha*

Main category: cs.IR

TL;DR: 论文提出了一种基于大语言模型（LLM）生成硬负例的端到端方法，无需依赖文档库，性能与传统方法相当。


<details>
  <summary>Details</summary>
Motivation: 传统硬负例挖掘方法（如BM25或交叉编码器）计算成本高且需访问完整文档库，因此提出一种更高效的替代方案。

Method: 使用LLM生成查询，并仅基于查询文本生成硬负例，形成端到端流程。

Result: 实验表明，该方法在多个基准数据集上性能与BM25和交叉编码器相当。

Conclusion: 该方法提供了一种更简单高效的训练高性能检索模型的方式，无需牺牲效果。

Abstract: Training effective dense retrieval models often relies on hard negative (HN)
examples mined from the document corpus via methods like BM25 or cross-encoders
(CE), processes that can be computationally demanding and require full corpus
access. This paper introduces a different approach, an end-to-end pipeline
where a Large Language Model (LLM) first generates a query from a passage, and
then generates a hard negative example using \emph{only} that query text. This
corpus-free negative generation contrasts with standard mining techniques. We
evaluated this \textsc{LLM Query $\rightarrow$ LLM HN} approach against
traditional \textsc{LLM Query $\rightarrow$ BM25 HN} and \textsc{LLM Query
$\rightarrow$ CE HN} pipelines using E5-Base and GTE-Base models on several
BEIR benchmark datasets. Our results show the proposed all-LLM pipeline
achieves performance identical to both the BM25 and the computationally
intensive CE baselines across nDCG@10, Precision@10, and Recall@100 metrics.
This demonstrates that our corpus-free negative generation method matches the
effectiveness of complex, corpus-dependent mining techniques, offering a
potentially simpler and more efficient pathway for training high-performance
retrievers without sacrificing results. We make the dataset including the
queries and the hard-negatives for all three methods publicly available
https://huggingface.co/collections/chungimungi/arxiv-hard-negatives-68027bbc601ff6cc8eb1f449.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [188] [DGFNet: End-to-End Audio-Visual Source Separation Based on Dynamic Gating Fusion](https://arxiv.org/abs/2504.21366)
*Yinfeng Yu,Shiyu Sun*

Main category: cs.SD

TL;DR: 论文提出了一种基于门控机制的动态融合方法，以解决音频-视觉源分离中模态融合不足或信息丢失的问题，并通过音频注意力模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在音频和视觉特征融合时存在信息丢失或模态交互不足的问题，限制了模型性能。

Method: 采用动态门控机制调整模态融合程度，并引入音频注意力模块增强音频特征表达能力。

Result: 在两个基准数据集上取得了显著性能提升。

Conclusion: 动态融合方法和音频注意力模块有效提升了音频-视觉源分离任务的性能。

Abstract: Current Audio-Visual Source Separation methods primarily adopt two design
strategies. The first strategy involves fusing audio and visual features at the
bottleneck layer of the encoder, followed by processing the fused features
through the decoder. However, when there is a significant disparity between the
two modalities, this approach may lead to the loss of critical information. The
second strategy avoids direct fusion and instead relies on the decoder to
handle the interaction between audio and visual features. Nonetheless, if the
encoder fails to integrate information across modalities adequately, the
decoder may be unable to effectively capture the complex relationships between
them. To address these issues, this paper proposes a dynamic fusion method
based on a gating mechanism that dynamically adjusts the modality fusion
degree. This approach mitigates the limitations of solely relying on the
decoder and facilitates efficient collaboration between audio and visual
features. Additionally, an audio attention module is introduced to enhance the
expressive capacity of audio features, thereby further improving model
performance. Experimental results demonstrate that our method achieves
significant performance improvements on two benchmark datasets, validating its
effectiveness and advantages in Audio-Visual Source Separation tasks.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [189] [Passive Measurement of Autonomic Arousal in Real-World Settings](https://arxiv.org/abs/2504.21242)
*Samy Abdel-Ghaffar,Isaac Galatzer-Levy,Conor Heneghan,Xin Liu,Sarah Kernasovskiy,Brennan Garrett,Andrew Barakat,Daniel McDuff*

Main category: cs.HC

TL;DR: Fitbit Body Response Algorithm通过远程手腕传感器连续测量自主神经系统（ANS）活动，验证了其在真实环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要在实验室环境中验证，缺乏真实环境中的ANS活动测量方法。

Method: 通过Trier Social Stress Test和生态瞬时评估（EMA）验证算法，结合多传感器数据预测感知压力。

Result: 模型预测感知压力的准确率为0.85，优于仅使用部分信号的方法。

Conclusion: 该算法在真实环境中有效，并解决了实验室环境未涉及的挑战。

Abstract: The autonomic nervous system (ANS) is activated during stress, which can have
negative effects on cardiovascular health, sleep, the immune system, and mental
health. While there are ways to quantify ANS activity in laboratories, there is
a paucity of methods that have been validated in real-world contexts. We
present the Fitbit Body Response Algorithm, an approach to continuous remote
measurement of ANS activation through widely available remote wrist-based
sensors. The design was validated via two experiments, a Trier Social Stress
Test (n = 45) and ecological momentary assessments (EMA) of perceived stress
(n=87), providing both controlled and ecologically valid test data. Model
performance predicting perceived stress when using all available sensor
modalities was consistent with expectations (accuracy=0.85) and outperformed
models with access to only a subset of the signals. We discuss and address
challenges to sensing that arise in real world settings that do not present in
conventional lab environments.

</details>


### [190] [Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning](https://arxiv.org/abs/2504.21731)
*Feiyu Lu,Mengyu Chen,Hsiang Hsu,Pranav Deshpande,Cheng Yao Wang,Blair MacIntyre*

Main category: cs.HC

TL;DR: 论文探讨了如何利用强化学习（RL）在混合现实（MR）中动态优化3D内容布局，以适应用户姿态和环境变化。


<details>
  <summary>Details</summary>
Motivation: MR中虚拟内容的动态布局是一个挑战，传统优化方法难以适应实时变化的需求。

Method: 采用强化学习方法，结合用户姿态和环境信息，实现连续3D内容布局优化。

Result: 初步实验表明，RL能有效优化内容布局，提升用户体验。

Conclusion: RL在MR内容布局中具有潜力，未来可探索个性化优化方向。

Abstract: Mixed Reality (MR) could assist users' tasks by continuously integrating
virtual content with their view of the physical environment. However, where and
how to place these content to best support the users has been a challenging
problem due to the dynamic nature of MR experiences. In contrast to prior work
that investigates optimization-based methods, we are exploring how
reinforcement learning (RL) could assist with continuous 3D content placement
that is aware of users' poses and their surrounding environments. Through an
initial exploration and preliminary evaluation, our results demonstrate the
potential of RL to position content that maximizes the reward for users on the
go. We further identify future directions for research that could harness the
power of RL for personalized and optimized UI and content placement in MR.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [191] [Redundancy Analysis and Mitigation for Machine Learning-Based Process Monitoring of Additive Manufacturing](https://arxiv.org/abs/2504.21317)
*Jiarui Xie,Yaoyao Fiona Zhao*

Main category: cs.CE

TL;DR: 论文提出了一种多级冗余缓解（MLRM）框架，用于解决基于机器学习的增材制造过程监控系统中的冗余问题，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对冗余的统一定义和系统评估方法，导致设备成本高、模型性能下降和计算需求大，阻碍了工业应用。

Method: 定义了样本级、特征级和模型级冗余，并提出MLRM框架，结合数据注册、降维、跨模态知识迁移和模型剪枝等方法。

Result: 在定向能量沉积（DED）案例中，延迟减少91%，错误率降低47%，存储需求减少99.4%，同时降低了传感器成本和能耗。

Conclusion: 通过定义冗余并提出系统缓解框架，该研究为高效机器学习监控系统在生产环境中的应用提供了关键支持。

Abstract: The deployment of machine learning (ML)-based process monitoring systems has
significantly advanced additive manufacturing (AM) by enabling real-time defect
detection, quality assessment, and process optimization. However, redundancy is
a critical yet often overlooked challenge in the deployment and operation of
ML-based AM process monitoring systems. Excessive redundancy leads to increased
equipment costs, compromised model performance, and high computational
requirements, posing barriers to industrial adoption. However, existing
research lacks a unified definition of redundancy and a systematic framework
for its evaluation and mitigation. This paper defines redundancy in ML-based AM
process monitoring and categorizes it into sample-level, feature-level, and
model-level redundancy. A comprehensive multi-level redundancy mitigation
(MLRM) framework is proposed, incorporating advanced methods such as data
registration, downscaling, cross-modality knowledge transfer, and model pruning
to systematically reduce redundancy while improving model performance. The
framework is validated through an ML-based in-situ defect detection case study
for directed energy deposition (DED), demonstrating a 91% reduction in latency,
a 47% decrease in error rate, and a 99.4% reduction in storage requirements.
Additionally, the proposed approach lowers sensor costs and energy consumption,
enabling a lightweight, cost-effective, and scalable monitoring system. By
defining redundancy and introducing a structured mitigation framework, this
study establishes redundancy analysis and mitigation as a key enabler of
efficient ML-based process monitoring in production environments.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [192] [Advancing Multi-Agent Systems Through Model Context Protocol: Architecture, Implementation, and Applications](https://arxiv.org/abs/2504.21030)
*Naveen Krishnan*

Main category: cs.MA

TL;DR: 本文提出了一种基于模型上下文协议（MCP）的框架，用于解决多智能体系统在上下文管理、协调效率和可扩展性方面的挑战，并通过案例研究展示了其性能提升。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂问题解决中具有潜力，但在上下文管理、协调效率和可扩展性方面存在挑战，需要一种标准化方法。

Method: 开发了MCP框架，包括统一的理论基础、高级上下文管理技术和可扩展协调模式，并通过案例研究验证。

Result: 在多个领域（如企业知识管理、协作研究和分布式问题解决）中，MCP框架显著优于传统方法。

Conclusion: 该研究为更强大、协作和上下文感知的人工智能系统提供了基础，并指出了未来研究方向和应用潜力。

Abstract: Multi-agent systems represent a significant advancement in artificial
intelligence, enabling complex problem-solving through coordinated specialized
agents. However, these systems face fundamental challenges in context
management, coordination efficiency, and scalable operation. This paper
introduces a comprehensive framework for advancing multi-agent systems through
Model Context Protocol (MCP), addressing these challenges through standardized
context sharing and coordination mechanisms. We extend previous work on AI
agent architectures by developing a unified theoretical foundation, advanced
context management techniques, and scalable coordination patterns. Through
detailed implementation case studies across enterprise knowledge management,
collaborative research, and distributed problem-solving domains, we demonstrate
significant performance improvements compared to traditional approaches. Our
evaluation methodology provides a systematic assessment framework with
benchmark tasks and datasets specifically designed for multi-agent systems. We
identify current limitations, emerging research opportunities, and potential
transformative applications across industries. This work contributes to the
evolution of more capable, collaborative, and context-aware artificial
intelligence systems that can effectively address complex real-world
challenges.

</details>


### [193] [Multi-Agent Reinforcement Learning for Resources Allocation Optimization: A Survey](https://arxiv.org/abs/2504.21048)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Jimmy Cao,Ryszard Kowalczyk*

Main category: cs.MA

TL;DR: 本文综述了多智能体强化学习（MARL）在资源分配优化（RAO）中的应用，总结了核心概念、分类和分类法，并指出了当前挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: MARL能够处理动态和去中心化的资源分配问题，对工业4.0发展至关重要。本文旨在为研究者和实践者提供MARL在RAO中的潜力与方向。

Method: 通过综述近期MARL算法，涵盖核心概念、分类和分类法，分析研究现状。

Result: 总结了MARL在RAO中的应用现状，并识别了主要挑战和未来研究方向。

Conclusion: MARL在RAO中具有巨大潜力，本文为相关研究提供了系统性的参考和指导。

Abstract: Multi-Agent Reinforcement Learning (MARL) has become a powerful framework for
numerous real-world applications, modeling distributed decision-making and
learning from interactions with complex environments. Resource Allocation
Optimization (RAO) benefits significantly from MARL's ability to tackle dynamic
and decentralized contexts. MARL-based approaches are increasingly applied to
RAO challenges across sectors playing pivotal roles to Industry 4.0
developments. This survey provides a comprehensive review of recent MARL
algorithms for RAO, encompassing core concepts, classifications, and a
structured taxonomy. By outlining the current research landscape and
identifying primary challenges and future directions, this survey aims to
support researchers and practitioners in leveraging MARL's potential to advance
resource allocation solutions.

</details>


### [194] [MF-LLM: Simulating Collective Decision Dynamics via a Mean-Field Large Language Model Framework](https://arxiv.org/abs/2504.21582)
*Qirui Mi,Mengyue Yang,Xiangning Yu,Zhiyu Zhao,Cheng Deng,Bo An,Haifeng Zhang,Xu Chen,Jun Wang*

Main category: cs.MA

TL;DR: 提出MF-LLM框架，通过微观决策与宏观群体的反馈循环模拟集体决策，结合IB-Tune优化LLM，显著提升模拟效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在模拟集体决策时与真实数据存在偏差，需改进。

Method: MF-LLM框架交替使用策略模型和均值场模型，结合IB-Tune微调LLM。

Result: 在真实数据集上，KL散度降低47%，支持趋势预测和干预规划。

Conclusion: MF-LLM为高保真社会模拟提供了可扩展的基础。

Abstract: Simulating collective decision-making involves more than aggregating
individual behaviors; it arises from dynamic interactions among individuals.
While large language models (LLMs) show promise for social simulation, existing
approaches often exhibit deviations from real-world data. To address this gap,
we propose the Mean-Field LLM (MF-LLM) framework, which explicitly models the
feedback loop between micro-level decisions and macro-level population. MF-LLM
alternates between two models: a policy model that generates individual actions
based on personal states and group-level information, and a mean field model
that updates the population distribution from the latest individual decisions.
Together, they produce rollouts that simulate the evolving trajectories of
collective decision-making. To better match real-world data, we introduce
IB-Tune, a fine-tuning method for LLMs grounded in the information bottleneck
principle, which maximizes the relevance of population distributions to future
actions while minimizing redundancy with historical data. We evaluate MF-LLM on
a real-world social dataset, where it reduces KL divergence to human population
distributions by 47 percent over non-mean-field baselines, and enables accurate
trend forecasting and intervention planning. It generalizes across seven
domains and four LLM backbones, providing a scalable foundation for
high-fidelity social simulation.

</details>


<div id='q-bio.CB'></div>

# q-bio.CB [[Back]](#toc)

### [195] [Glucagon and insulin production in pancreatic cells modeled using Petri nets and Boolean networks](https://arxiv.org/abs/2504.21578)
*Kamila Barylska,Frank Delaplace,Anna Gogolińska,Ewa Pańkowska*

Main category: q-bio.CB

TL;DR: 本文通过构建Petri网模型，研究了胰岛素和胰高血糖素的分泌机制及其在血糖调节中的作用，并分析了模型的动态行为。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解血糖调节的复杂过程，尤其是胰岛素和胰高血糖素的分泌机制，以揭示糖尿病的病理基础。

Method: 构建了胰腺β细胞分泌胰岛素和α细胞分泌胰高血糖素的Petri网模型，并分析了其动态行为；还将系统转化为布尔网络。

Result: 成功建立了胰岛素和胰高血糖素分泌的模型，并分析了其动态行为，为理解糖尿病提供了基础。

Conclusion: 通过Petri网模型和布尔网络，本文为理解血糖调节和糖尿病机制提供了新的视角。

Abstract: Diabetes is a civilization chronic disease characterized by a constant
elevated concentration of glucose in the blood. Many processes are involved in
the glucose regulation, and their interactions are very complex. To better
understand those processes we set ourselves a goal to create a Petri net model
of the glucose regulation in the whole body. So far we have managed to create a
model of glycolysis and synthesis of glucose in the liver, and the general
overview models of the glucose regulation in a healthy and diabetic person. In
this paper we introduce Petri nets models of insulin secretion in beta cell of
the pancreas, and glucagon in the pancreas alpha cells. Those two hormones have
mutually opposite effects: insulin preventing hyperglycemia, and glucagon
preventing hypoglycemia. Understanding the mechanisms of insulin and glucagon
secretion constitutes the basis for understanding diabetes. We also present a
model in which both processes occur together, depending on the blood glucose
level. The dynamics of each model is analysed. Additionally, we transform the
overall insulin and glucagon secretion system to a Boolean network, following
standard transformation rules.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [196] [Towards Space Group Determination from EBSD Patterns: The Role of Deep Learning and High-throughput Dynamical Simulations](https://arxiv.org/abs/2504.21331)
*Alfred Yan,Muhammad Nur Talha Kilic,Gert Nolze,Ankit Agrawal,Alok Choudhary,Roberto dos Reis,Vinayak Dravid*

Main category: cond-mat.mtrl-sci

TL;DR: 论文提出了一种利用深度学习和Kikuchi衍射技术快速确定晶体对称性的方法，以解决材料表征速度滞后于合成速度的问题。


<details>
  <summary>Details</summary>
Motivation: 材料合成速度远超表征速度，亟需高效方法确定晶体结构，以支持高通量纳米材料发现。

Method: 结合Kikuchi衍射和深度学习，训练神经网络分类空间群对称性，并通过模拟和实验数据验证。

Result: 神经网络在模拟和实验数据上的分类准确率超过90%，证明其可行性。

Conclusion: 深度学习结合Kikuchi衍射技术可高效预测晶体对称性，为材料发现提供新工具。

Abstract: The design of novel materials hinges on the understanding of
structure-property relationships. However, our capability to synthesize a large
number of materials has outpaced the ability and speed needed to characterize
them. While the overall chemical constituents can be readily known during
synthesis, the structural evolution and characterization of newly synthesized
samples remains a bottleneck for the ultimate goal of high throughput
nanomaterials discovery. Thus, scalable methods for crystal symmetry
determination that can analyze a large volume of material samples within a
short time-frame are especially needed. Kikuchi diffraction in the SEM is a
promising technique for this due to its sensitivity to dynamical scattering,
which may provide information beyond just the seven crystal systems and
fourteen Bravais lattices. After diffraction patterns are collected from
material samples, deep learning methods may be able to classify the space group
symmetries using the patterns as input, which paired with the elemental
composition, would help enable the determination of the crystal structure. To
investigate the feasibility of this solution, neural networks were trained to
predict the space group type of background corrected EBSD patterns. Our
networks were first trained and tested on an artificial dataset of EBSD
patterns of 5,148 different cubic phases, created through physics-based
dynamical simulations. Next, Maximum Classifier Discrepancy, an unsupervised
deep learning-based domain adaptation method, was utilized to train neural
networks to make predictions for experimental EBSD patterns. We introduce a
relabeling scheme, which enables our models to achieve accuracy scores higher
than 90% on simulated and experimental data, suggesting that neural networks
are capable of making predictions of crystal symmetry from an EBSD pattern.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [197] [Who Gets the Callback? Generative AI and Gender Bias](https://arxiv.org/abs/2504.21400)
*Sugat Chaturvedi,Rochana Chaturvedi*

Main category: econ.GN

TL;DR: 论文审计了开源大型语言模型在招聘中的性别偏见，发现模型倾向于推荐男性，尤其是在高薪职位中，且与性别刻板印象一致。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI（尤其是LLMs）在招聘中的性别偏见，揭示其对劳动力市场公平性和多样性的潜在影响。

Method: 使用33万条真实招聘广告数据，通过模型推荐面试候选人，分析性别偏见；并通过模拟招聘者身份（如人格特质）进一步研究。

Result: 大多数模型偏向男性，尤其是高薪职位；女性在男性主导职业中回调率较低，而在女性相关职业中较高。

Conclusion: AI驱动的招聘可能加剧劳动力市场偏见，需关注公平性和多样性问题。

Abstract: Generative artificial intelligence (AI), particularly large language models
(LLMs), is being rapidly deployed in recruitment and for candidate
shortlisting. We audit several mid-sized open-source LLMs for gender bias using
a dataset of 332,044 real-world online job postings. For each posting, we
prompt the model to recommend whether an equally qualified male or female
candidate should receive an interview callback. We find that most models tend
to favor men, especially for higher-wage roles. Mapping job descriptions to the
Standard Occupational Classification system, we find lower callback rates for
women in male-dominated occupations and higher rates in female-associated ones,
indicating occupational segregation. A comprehensive analysis of linguistic
features in job ads reveals strong alignment of model recommendations with
traditional gender stereotypes. To examine the role of recruiter identity, we
steer model behavior by infusing Big Five personality traits and simulating the
perspectives of historical figures. We find that less agreeable personas reduce
stereotyping, consistent with an agreeableness bias in LLMs. Our findings
highlight how AI-driven hiring may perpetuate biases in the labor market and
have implications for fairness and diversity within firms.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [198] [CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation](https://arxiv.org/abs/2504.21751)
*Sizhe Wang,Zhengren Wang,Dongsheng Ma,Yongan Yu,Rui Ling,Zhiyu Li,Feiyu Xiong,Wentao Zhang*

Main category: cs.SE

TL;DR: 论文提出了CodeFlowBench，首个用于评估LLMs在多轮代码复用中能力的基准测试，包含5258个问题，并设计了新的评估框架。实验显示LLMs在多轮代码复用场景中表现较差。


<details>
  <summary>Details</summary>
Motivation: 现实开发需要代码具备可读性、可扩展性和可测试性，通过模块化组件和迭代复用代码实现。研究旨在评估LLMs在多轮代码复用中的能力。

Method: 提出CodeFlowBench基准测试，从Codeforces中提取5258个问题，通过自动化管道分解为函数级子问题，并设计多轮代码复用的评估框架。

Result: 实验表明LLMs在多轮代码复用中表现不佳，例如o1-mini在多轮模式下的pass@1为20.8%，而单轮模式为37.8%。

Conclusion: CodeFlowBench为多轮迭代代码生成提供了全面基准，揭示了当前LLMs在代码生成任务中的局限性，指导未来研究。

Abstract: Real world development demands code that is readable, extensible, and
testable by organizing the implementation into modular components and
iteratively reuse pre-implemented code. We term this iterative, multi-turn
process codeflow and introduce CodeFlowBench, the first benchmark designed for
comprehensively evaluating LLMs' ability to perform codeflow, namely to
implement new functionality by reusing existing functions over multiple turns.
CodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously
updated via an automated pipeline that decomposes each problem into a series of
function-level subproblems based on its dependency tree and each subproblem is
paired with unit tests. We further propose a novel evaluation framework with
tasks and metrics tailored to multi-turn code reuse to assess model
performance. In experiments across various LLMs under both multi-turn and
single-turn patterns. We observe models' poor performance on CodeFlowBench,
with a substantial performance drop in the iterative codeflow scenario. For
instance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8%
in single-turn pattern. Further analysis shows that different models excel at
different dependency depths, yet all struggle to correctly solve structurally
complex problems, highlighting challenges for current LLMs to serve as code
generation tools when performing codeflow. Overall, CodeFlowBench offers a
comprehensive benchmark and new insights into LLM capabilities for multi-turn,
iterative code generation, guiding future advances in code generation tasks.

</details>


### [199] [SWE-smith: Scaling Data for Software Engineering Agents](https://arxiv.org/abs/2504.21798)
*John Yang,Kilian Leret,Carlos E. Jimenez,Alexander Wettig,Kabir Khandpur,Yanzhe Zhang,Binyuan Hui,Ofir Press,Ludwig Schmidt,Diyi Yang*

Main category: cs.SE

TL;DR: SWE-smith是一种新型管道，用于大规模生成软件工程训练数据，解决了现有数据集小且难以扩展的问题。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程训练数据集规模小、收集复杂且存储需求高，限制了语言模型在软件工程中的应用。

Method: SWE-smith通过构建执行环境并自动合成任务实例（破坏现有测试），从128个GitHub仓库生成了50k实例的数据集。

Result: 训练出的SWE-agent-LM-32B模型在SWE-bench Verified基准测试中达到40.2% Pass@1，是目前开源模型中的最佳表现。

Conclusion: SWE-smith的开源降低了自动化软件工程研究的门槛，推动了语言模型在该领域的应用。

Abstract: Despite recent progress in Language Models (LMs) for software engineering,
collecting training data remains a significant pain point. Existing datasets
are small, with at most 1,000s of training instances from 11 or fewer GitHub
repositories. The procedures to curate such datasets are often complex,
necessitating hundreds of hours of human labor; companion execution
environments also take up several terabytes of storage, severely limiting their
scalability and usability. To address this pain point, we introduce SWE-smith,
a novel pipeline for generating software engineering training data at scale.
Given any Python codebase, SWE-smith constructs a corresponding execution
environment, then automatically synthesizes 100s to 1,000s of task instances
that break existing test(s) in the codebase. Using SWE-smith, we create a
dataset of 50k instances sourced from 128 GitHub repositories, an order of
magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving
40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art
among open source models. We open source SWE-smith (collection procedure, task
instances, trajectories, models) to lower the barrier of entry for research in
LM systems for automated software engineering. All assets available at
https://swesmith.com.

</details>


### [200] [Assessing LLM code generation quality through path planning tasks](https://arxiv.org/abs/2504.21276)
*Wanyi Chen,Meng-Wen Su,Mary L. Cummings*

Main category: cs.SE

TL;DR: 评估LLM生成代码在安全关键路径规划中的风险，发现现有基准不足，测试六种LLM生成三种算法的代码，结果显示存在严重隐患。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成代码的普及，需评估其在安全关键应用中的风险，现有基准无法反映实际复杂性。

Method: 测试六种LLM生成三种路径规划算法的代码，并在三种难度地图上验证。

Result: LLM生成的代码在路径规划中存在严重安全隐患。

Conclusion: 安全关键应用中需严格测试LLM生成的代码，避免直接使用。

Abstract: As LLM-generated code grows in popularity, more evaluation is needed to
assess the risks of using such tools, especially for safety-critical
applications such as path planning. Existing coding benchmarks are insufficient
as they do not reflect the context and complexity of safety-critical
applications. To this end, we assessed six LLMs' abilities to generate the code
for three different path-planning algorithms and tested them on three maps of
various difficulties. Our results suggest that LLM-generated code presents
serious hazards for path planning applications and should not be applied in
safety-critical contexts without rigorous testing.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [201] [Participatory AI, Public Sector AI, Differential Privacy, Conversational Interfaces, Explainable AI, Citizen Engagement in AI](https://arxiv.org/abs/2504.21297)
*Wenjun Yang,Eyhab Al-Masri*

Main category: cs.IT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces a conversational interface system that enables
participatory design of differentially private AI systems in public sector
applications. Addressing the challenge of balancing mathematical privacy
guarantees with democratic accountability, we propose three key contributions:
(1) an adaptive $\epsilon$-selection protocol leveraging TOPSIS multi-criteria
decision analysis to align citizen preferences with differential privacy (DP)
parameters, (2) an explainable noise-injection framework featuring real-time
Mean Absolute Error (MAE) visualizations and GPT-4-powered impact analysis, and
(3) an integrated legal-compliance mechanism that dynamically modulates privacy
budgets based on evolving regulatory constraints. Our results advance
participatory AI practices by demonstrating how conversational interfaces can
enhance public engagement in algorithmic privacy mechanisms, ensuring that
privacy-preserving AI in public sector governance remains both mathematically
robust and democratically accountable.

</details>


### [202] [Sionna RT: Technical Report](https://arxiv.org/abs/2504.21719)
*Fayçal Aït Aoudia,Jakob Hoydis,Merlin Nimier-David,Sebastian Cammerer,Alexander Keller*

Main category: cs.IT

TL;DR: Sionna 1.0是一个开源、GPU加速的库，集成了可微分的射线追踪器，用于模拟无线电波传播，并优化了速度和内存效率。


<details>
  <summary>Details</summary>
Motivation: 提供高效、可微分的无线电波传播模拟工具，支持系统与环境参数的梯度计算。

Method: 结合SBR与图像方法计算CIR，使用哈希机制消除重复路径；基于SBR计算无线电地图。

Result: 显著提升了射线追踪器的速度、内存效率和可扩展性。

Conclusion: Sionna RT为无线电波传播模拟提供了高效且可扩展的解决方案，但仍存在一些局限性。

Abstract: Sionna is an open-source, GPU-accelerated library that, as of version 0.14,
incorporates a ray tracer for simulating radio wave propagation. A unique
feature of Sionna RT is differentiability, enabling the calculation of
gradients for the channel impulse responses (CIRs), radio maps, and other
related metrics with respect to system and environmental parameters, such as
material properties, antenna patterns, and array geometries. The release of
Sionna 1.0 provides a complete overhaul of the ray tracer, significantly
improving its speed, memory efficiency, and extensibility. This document
details the algorithms employed by Sionna RT to simulate radio wave propagation
efficiently, while also addressing their current limitations. Given that the
computation of CIRs and radio maps requires distinct algorithms, these are
detailed in separate sections. For CIRs, Sionna RT integrates shooting and
bouncing of rays (SBR) with the image method and uses a hashing-based mechanism
to efficiently eliminate duplicate paths. Radio maps are computed using a
purely SBR-based approach.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [203] [Evaluation and Verification of Physics-Informed Neural Models of the Grad-Shafranov Equation](https://arxiv.org/abs/2504.21155)
*Fauzan Nazranda Rizqa,Matthew Hole,Charles Gretton*

Main category: physics.plasm-ph

TL;DR: 论文探讨了在轴对称托卡马克反应堆中，利用物理信息神经网络（PINN）建模Grad-Shafranov方程（GSE）的潜力，并解决了现有研究中未涉及的边界条件泛化问题。通过比较PINN与傅里叶神经算子（FNO）模型的性能，发现PINN更优，并首次验证了此类网络的实用性。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于聚变反应堆中磁流体动力学（MHD）平衡的需求，特别是在轴对称托卡马克反应堆中，GSE的建模对稳定运行至关重要。现有研究未解决边界条件泛化问题。

Method: 采用PINN架构，将边界点作为网络输入，并与FNO模型比较精度和推理速度。使用Marabou工具进行网络验证。

Result: PINN模型在性能和精度上优于FNO模型，验证工作流实用且有效，尽管PyTorch原生评估与Marabou之间存在一些差异。

Conclusion: 研究首次验证了此类网络的实用性，为GSE建模提供了新方法，并展示了验证工作流的可行性。

Abstract: Our contributions are motivated by fusion reactors that rely on maintaining
magnetohydrodynamic (MHD) equilibrium, where the balance between plasma
pressure and confining magnetic fields is required for stable operation. In
axisymmetric tokamak reactors in particular, and under the assumption of
toroidal symmetry, this equilibrium can be mathematically modelled using the
Grad-Shafranov Equation (GSE). Recent works have demonstrated the potential of
using Physics-Informed Neural Networks (PINNs) to model the GSE. Existing
studies did not examine realistic scenarios in which a single network
generalizes to a variety of boundary conditions. Addressing that limitation, we
evaluate a PINN architecture that incorporates boundary points as network
inputs. Additionally, we compare PINN model accuracy and inference speeds with
a Fourier Neural Operator (FNO) model. Finding the PINN model to be the most
performant, and accurate in our setting, we use the network verification tool
Marabou to perform a range of verification tasks. Although we find some
discrepancies between evaluations of the networks natively in PyTorch, compared
to via Marabou, we are able to demonstrate useful and practical verification
workflows. Our study is the first investigation of verification of such
networks.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [204] [Selecting the Right LLM for eGov Explanations](https://arxiv.org/abs/2504.21032)
*Lior Limonad,Fabiana Fournier,Hadar Mulian,George Manias,Spiros Borotis,Danai Kyrkou*

Main category: cs.CY

TL;DR: 本文提出了一种系统方法，用于比较分析不同LLM生成解释的感知质量，并通过税务案例验证其适用性。


<details>
  <summary>Details</summary>
Motivation: 电子政务服务中解释的质量对用户信任和使用至关重要，但选择合适的LLM生成解释是一项复杂任务。

Method: 采用先前开发的量表，通过用户研究（128名受访者）评估不同LLM生成解释的质量。

Result: 研究为选择最合适的LLM提供了方法论基础，并尝试自动化用户反馈过程。

Conclusion: 该方法有助于电子政务服务提供商选择适合的LLM，提升解释质量与用户信任。

Abstract: The perceived quality of the explanations accompanying e-government services
is key to gaining trust in these institutions, consequently amplifying further
usage of these services. Recent advances in generative AI, and concretely in
Large Language Models (LLMs) allow the automation of such content
articulations, eliciting explanations' interpretability and fidelity, and more
generally, adapting content to various audiences. However, selecting the right
LLM type for this has become a non-trivial task for e-government service
providers. In this work, we adapted a previously developed scale to assist with
this selection, providing a systematic approach for the comparative analysis of
the perceived quality of explanations generated by various LLMs. We further
demonstrated its applicability through the tax-return process, using it as an
exemplar use case that could benefit from employing an LLM to generate
explanations about tax refund decisions. This was attained through a user study
with 128 survey respondents who were asked to rate different versions of
LLM-generated explanations about tax refund decisions, providing a
methodological basis for selecting the most appropriate LLM. Recognizing the
practical challenges of conducting such a survey, we also began exploring the
automation of this process by attempting to replicate human feedback using a
selection of cutting-edge predictive techniques.

</details>


### [205] [AI Supply Chains: An Emerging Ecosystem of AI Actors, Products, and Services](https://arxiv.org/abs/2504.20185)
*Aspen Hopkins,Sarah H. Cen,Andrew Ilyas,Isabella Struckman,Luis Videgaray,Aleksander Mądry*

Main category: cs.CY

TL;DR: 该论文首次对AI供应链及其影响进行了形式化研究，通过两个案例展示了AI开发和监管在供应链中的复杂性。


<details>
  <summary>Details</summary>
Motivation: 随着AI的广泛应用，AI供应链（涉及模型、数据等的复杂网络）的重要性日益凸显，但其影响尚未被充分理解。

Method: 论文通过历史视角分析AI供应链的兴起，将其建模为有向图，并通过两个案例研究（信息传递和上游设计选择的影响）展示其复杂性。

Result: 研究发现，AI供应链中的信息传递不完善可能导致误解，而上游设计选择会显著影响下游产品。

Conclusion: 研究强调了进一步研究AI供应链的必要性，以应对其日益显著的社会、经济、监管和技术影响。

Abstract: The widespread adoption of AI in recent years has led to the emergence of AI
supply chains: complex networks of AI actors contributing models, datasets, and
more to the development of AI products and services. AI supply chains have many
implications yet are poorly understood. In this work, we take a first step
toward a formal study of AI supply chains and their implications, providing two
illustrative case studies indicating that both AI development and regulation
are complicated in the presence of supply chains. We begin by presenting a
brief historical perspective on AI supply chains, discussing how their rise
reflects a longstanding shift towards specialization and outsourcing that
signals the healthy growth of the AI industry. We then model AI supply chains
as directed graphs and demonstrate the power of this abstraction by connecting
examples of AI issues to graph properties. Finally, we examine two case studies
in detail, providing theoretical and empirical results in both. In the first,
we show that information passing (specifically, of explanations) along the AI
supply chains is imperfect, which can result in misunderstandings that have
real-world implications. In the second, we show that upstream design choices
(e.g., by base model providers) have downstream consequences (e.g., on AI
products fine-tuned on the base model). Together, our findings motivate further
study of AI supply chains and their increasingly salient social, economic,
regulatory, and technical implications.

</details>


### [206] [LSTM+Geo with xgBoost Filtering: A Novel Approach for Race and Ethnicity Imputation with Reduced Bias](https://arxiv.org/abs/2504.21259)
*S. Chalavadi,A. Pastor,T. Leitch*

Main category: cs.CY

TL;DR: 论文提出LSTM+Geo方法，结合LSTM网络和地理信息，显著提高了种族和民族分类的准确性，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 准确分类种族和民族对分析差异和制定政策至关重要，但现有方法（如BISG）存在系统性偏差。

Method: 引入LSTM+Geo，结合LSTM网络和人口普查地理信息，使用大型选民数据集进行验证。

Result: LSTM+Geo准确率达88.7%，优于LSTM（86.4%）和贝叶斯方法（BISG 82.9%，BIFSG 86.8%），并减少非白人被误分类为白人的比例。

Conclusion: LSTM+Geo作为独立模型或集成组件表现优异，但需谨慎使用。

Abstract: Accurate imputation of race and ethnicity (R&E) is crucial for analyzing
disparities and informing policy. Methods like Bayesian Improved Surname
Geocoding (BISG) are widely used but exhibit limitations, including systematic
misclassification biases linked to socioeconomic status. This paper introduces
LSTM+Geo, a novel approach enhancing Long Short-Term Memory (LSTM) networks
with census tract geolocation information. Using a large voter dataset, we
demonstrate that LSTM+Geo (88.7% accuracy) significantly outperforms standalone
LSTM (86.4%) and Bayesian methods like BISG (82.9%) and BIFSG (86.8%) in
accuracy and F1-score on a held-out validation set. LSTM+Geo reduces the rate
at which non-White individuals are misclassified as White (White FPR 19.3%)
compared to name-only LSTMs (White FPR 24.6%). While sophisticated ensemble
methods incorporating XGBoost achieve the highest overall accuracy (up to
89.4%) and lowest White FPR (17.8%), LSTM+Geo offers strong standalone
performance with improved bias characteristics compared to baseline models.
Integrating LSTM+Geo into an XGBoost ensemble further boosts accuracy,
highlighting its utility as both a standalone model and a component for
advanced systems. We give a caution at the end regarding the appropriate use of
these methods.

</details>


### [207] [Quantitative Auditing of AI Fairness with Differentially Private Synthetic Data](https://arxiv.org/abs/2504.21634)
*Chih-Cheng Rex Yuan,Bow-Yaw Wang*

Main category: cs.CY

TL;DR: 提出了一种基于差分隐私合成数据的AI公平性审计框架，解决了传统审计中的安全和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 传统审计使用真实数据会带来安全和隐私风险，需要一种既能保护隐私又能有效审计公平性的方法。

Method: 利用差分隐私机制生成合成数据，保留原始数据的统计特性，同时确保隐私。

Result: 在真实数据集（如Adult、COMPAS、Diabetes）上验证了合成数据能有效保留公平性指标。

Conclusion: 该框架在保护敏感信息的同时，实现了有效的公平性审计，适用于敏感领域。

Abstract: Fairness auditing of AI systems can identify and quantify biases. However,
traditional auditing using real-world data raises security and privacy
concerns. It exposes auditors to security risks as they become custodians of
sensitive information and targets for cyberattacks. Privacy risks arise even
without direct breaches, as data analyses can inadvertently expose confidential
information. To address these, we propose a framework that leverages
differentially private synthetic data to audit the fairness of AI systems. By
applying privacy-preserving mechanisms, it generates synthetic data that
mirrors the statistical properties of the original dataset while ensuring
privacy. This method balances the goal of rigorous fairness auditing and the
need for strong privacy protections. Through experiments on real datasets like
Adult, COMPAS, and Diabetes, we compare fairness metrics of synthetic and real
data. By analyzing the alignment and discrepancies between these metrics, we
assess the capacity of synthetic data to preserve the fairness properties of
real data. Our results demonstrate the framework's ability to enable meaningful
fairness evaluations while safeguarding sensitive information, proving its
applicability across critical and sensitive domains.

</details>


### [208] [TRIED: Truly Innovative and Effective Detection Benchmark, developed by WITNESS](https://arxiv.org/abs/2504.21489)
*Shirin Anlen,Zuzanna Wojciak*

Main category: cs.CY

TL;DR: WITNESS报告指出当前AI检测工具在现实场景中表现不佳，提出TRIED Benchmark框架以评估工具的实际影响和创新能力，强调需适应多样化需求。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和虚假合成媒体的崛起威胁全球信息生态系统，尤其在发展中国家，现有检测工具因可解释性、公平性等问题表现不足。

Method: 基于前线经验、虚假AI案例和全球咨询，提出TRIED Benchmark框架，评估工具的实用性和创新性。

Result: 报告为开发者、政策制定者等提供实用指南，强调透明、用户为中心的检测方案，并纳入社会技术考量。

Conclusion: 采用TRIED Benchmark可推动创新、增强公众信任和AI素养，提升全球信息可信度。

Abstract: The rise of generative AI and deceptive synthetic media threatens the global
information ecosystem, especially across the Global Majority. This report from
WITNESS highlights the limitations of current AI detection tools, which often
underperform in real-world scenarios due to challenges related to
explainability, fairness, accessibility, and contextual relevance. In response,
WITNESS introduces the Truly Innovative and Effective AI Detection (TRIED)
Benchmark, a new framework for evaluating detection tools based on their
real-world impact and capacity for innovation. Drawing on frontline
experiences, deceptive AI cases, and global consultations, the report outlines
how detection tools must evolve to become truly innovative and relevant by
meeting diverse linguistic, cultural, and technological contexts. It offers
practical guidance for developers, policymakers, and standards bodies to design
accountable, transparent, and user-centered detection solutions, and
incorporate sociotechnical considerations into future AI standards, procedures
and evaluation frameworks. By adopting the TRIED Benchmark, stakeholders can
drive innovation, safeguard public trust, strengthen AI literacy, and
contribute to a more resilient global information credibility.

</details>


### [209] [Characterizing AI Agents for Alignment and Governance](https://arxiv.org/abs/2504.21848)
*Atoosa Kasirzadeh,Iason Gabriel*

Main category: cs.CY

TL;DR: 该论文提出了一个AI代理的框架，围绕自主性、效能、目标复杂性和通用性四个维度，并构建了不同AI代理的“代理性档案”，以帮助解决技术和非技术治理挑战。


<details>
  <summary>Details</summary>
Motivation: 理解AI代理的核心属性及其与治理问题的关系，以支持更有效的AI治理机制。

Method: 提出四个维度的分类框架，并为不同AI代理构建代理性档案。

Result: 框架揭示了不同AI代理的治理挑战，并为开发者、政策制定者和公众提供了治理工具。

Conclusion: 该框架有助于开发更符合社会目标的AI治理方法。

Abstract: The creation of effective governance mechanisms for AI agents requires a
deeper understanding of their core properties and how these properties relate
to questions surrounding the deployment and operation of agents in the world.
This paper provides a characterization of AI agents that focuses on four
dimensions: autonomy, efficacy, goal complexity, and generality. We propose
different gradations for each dimension, and argue that each dimension raises
unique questions about the design, operation, and governance of these systems.
Moreover, we draw upon this framework to construct "agentic profiles" for
different kinds of AI agents. These profiles help to illuminate cross-cutting
technical and non-technical governance challenges posed by different classes of
AI agents, ranging from narrow task-specific assistants to highly autonomous
general-purpose systems. By mapping out key axes of variation and continuity,
this framework provides developers, policymakers, and members of the public
with the opportunity to develop governance approaches that better align with
collective societal goals.

</details>


### [210] [Public Opinion and The Rise of Digital Minds: Perceived Risk, Trust, and Regulation Support](https://arxiv.org/abs/2504.21849)
*Justin B. Bullock,Janet V. T. Pauketat,Hsini Huang,Yi-Fan Wang,Jacy Reese Anthis*

Main category: cs.CY

TL;DR: 研究探讨公众对AI监管的偏好，发现信任政府和风险感知是关键因素。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI带来的社会风险及公众对监管的态度。

Method: 使用2023年AIMS全国代表性调查数据，分析公众对政府、AI公司和技术的信任及监管支持。

Result: 公众普遍支持AI监管，信任政府者倾向监管，信任AI公司者反对限制。风险感知显著影响政策偏好。

Conclusion: AI治理需平衡公众风险担忧与机构信任，为政策制定提供实证基础。

Abstract: Governance institutions must respond to societal risks, including those posed
by generative AI. This study empirically examines how public trust in
institutions and AI technologies, along with perceived risks, shape preferences
for AI regulation. Using the nationally representative 2023 Artificial
Intelligence, Morality, and Sentience (AIMS) survey, we assess trust in
government, AI companies, and AI technologies, as well as public support for
regulatory measures such as slowing AI development or outright bans on advanced
AI. Our findings reveal broad public support for AI regulation, with risk
perception playing a significant role in shaping policy preferences.
Individuals with higher trust in government favor regulation, while those with
greater trust in AI companies and AI technologies are less inclined to support
restrictions. Trust in government and perceived risks significantly predict
preferences for both soft (e.g., slowing development) and strong (e.g., banning
AI systems) regulatory interventions. These results highlight the importance of
public opinion in AI governance. As AI capabilities advance, effective
regulation will require balancing public concerns about risks with trust in
institutions. This study provides a foundational empirical baseline for
policymakers navigating AI governance and underscores the need for further
research into public trust, risk perception, and regulatory strategies in the
evolving AI landscape.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [211] [Estimation of discrete distributions in relative entropy, and the deviations of the missing mass](https://arxiv.org/abs/2504.21787)
*Jaouad Mourtada*

Main category: math.ST

TL;DR: 论文研究了从独立同分布样本中估计有限字母表上的分布问题，以相对熵（KL散度）衡量准确性。分析了经典Laplace估计器，并探讨了高概率风险最优性及稀疏分布的自适应方法。


<details>
  <summary>Details</summary>
Motivation: 解决在高概率下分布估计的性能界限问题，特别是在字母表大小超过样本量的稀疏场景下。

Method: 分析了Laplace估计器的性能，提出了置信依赖平滑技术和数据依赖平滑的自适应估计器。

Result: 证明了Laplace估计器在置信独立估计器中的最优性，并给出了高概率风险的最优界限，发现非渐近风险包含额外对数因子。

Conclusion: 通过数据依赖平滑方法，实现了对稀疏分布的高概率风险界限，并推导了缺失质量的尖锐上界。

Abstract: We study the problem of estimating a distribution over a finite alphabet from
an i.i.d. sample, with accuracy measured in relative entropy (Kullback-Leibler
divergence). While optimal expected risk bounds are known, high-probability
guarantees remain less well-understood. First, we analyze the classical Laplace
(add-$1$) estimator, obtaining matching upper and lower bounds on its
performance and showing its optimality among confidence-independent estimators.
We then characterize the minimax-optimal high-probability risk achievable by
any estimator, which is attained via a simple confidence-dependent smoothing
technique. Interestingly, the optimal non-asymptotic risk contains an
additional logarithmic factor over the ideal asymptotic risk. Next, motivated
by scenarios where the alphabet exceeds the sample size, we investigate methods
that adapt to the sparsity of the distribution at hand. We introduce an
estimator using data-dependent smoothing, for which we establish a
high-probability risk bound depending on two effective sparsity parameters. As
part of the analysis, we also derive a sharp high-probability upper bound on
the missing mass.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [212] [Galvatron: An Automatic Distributed System for Efficient Foundation Model Training](https://arxiv.org/abs/2504.21411)
*Xinyi Liu,Yujie Wang,Shenhan Zhu,Fangcheng Fu,Qingshuo Liu,Guangming Lin,Bin Cui*

Main category: cs.DC

TL;DR: Galvatron是一个用于高效训练大规模基础模型的分布式系统，通过自动选择最优并行策略，显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有框架在训练大规模基础模型时并行策略选择的复杂性和效率问题。

Method: 系统架构包括硬件和模型分析器、基于决策树和动态编程的策略优化搜索引擎，以及高效执行策略的运行时。

Result: 在多种集群上测试显示，Galvatron的吞吐量优于现有框架。

Conclusion: Galvatron通过开源、用户友好的接口和文档，使复杂分布式训练更易用高效。

Abstract: Galvatron is a distributed system for efficiently training large-scale
Foundation Models. It overcomes the complexities of selecting optimal
parallelism strategies by automatically identifying the most efficient hybrid
strategy, incorporating data, tensor, pipeline, sharded data, and sequence
parallelism, along with recomputation. The system's architecture includes a
profiler for hardware and model analysis, a search engine for strategy
optimization using decision trees and dynamic programming, and a runtime for
executing these strategies efficiently. Benchmarking on various clusters
demonstrates Galvatron's superior throughput compared to existing frameworks.
This open-source system offers user-friendly interfaces and comprehensive
documentation, making complex distributed training accessible and efficient.
The source code of Galvatron is available at
https://github.com/PKU-DAIR/Hetu-Galvatron.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [213] [Low-rank computation of the posterior mean in Multi-Output Gaussian Processes](https://arxiv.org/abs/2504.21527)
*Sebastian Esche,Martin Stoll*

Main category: math.NA

TL;DR: 本文提出了一种低秩方法，用于高效计算多输出高斯过程（MOGP）的后验均值，通过空间和时间可分离的协方差函数和Kronecker积分解，结合LRPCG方法和KPIK求解器解决大规模Stein方程。


<details>
  <summary>Details</summary>
Motivation: 多输出高斯过程（MOGP）在机器学习和计算科学中应用广泛，但计算后验均值时面临大规模Stein方程求解的挑战，需要高效方法。

Method: 采用低秩时空数据假设，构建可分离的协方差函数，分解为Kronecker积，结合LRPCG方法和KPIK求解器解决Stein方程。

Result: 在真实世界街道网络图上测试，使用图滤波器作为协方差矩阵，并提出一种度加权平均协方差矩阵以提高收敛效率。

Conclusion: 提出的低秩方法能高效计算MOGP后验均值，适用于大规模问题，并展示了在实际应用中的有效性。

Abstract: Gaussian processes (GP) are a versatile tool in machine learning and
computational science. We here consider the case of multi-output Gaussian
processes (MOGP) and present low-rank approaches for efficiently computing the
posterior mean of a MOGP. Starting from low-rank spatio-temporal data we
consider a structured covariance function, assuming separability across space
and time. This separability, in turn, gives a decomposition of the covariance
matrix into a Kronecker product of individual covariance matrices.
Incorporating the typical noise term to the model then requires the solution of
a large-scale Stein equation for computing the posterior mean. For this, we
propose efficient low-rank methods based on a combination of a LRPCG method
with the Sylvester equation solver KPIK adjusted for solving Stein equations.
We test the developed method on real world street network graphs by using graph
filters as covariance matrices. Moreover, we propose a degree-weighted average
covariance matrix, which can be employed under specific assumptions to achieve
more efficient convergence.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [214] [QAOA Parameter Transferability for Maximum Independent Set using Graph Attention Networks](https://arxiv.org/abs/2504.21135)
*Hanjing Xu,Xiaoyuan Liu,Alex Pothen,Ilya Safro*

Main category: quant-ph

TL;DR: 提出了一种基于图注意力网络（GAT）的QAOA参数迁移方案，用于解决最大独立集（MIS）问题，并结合分布式资源感知算法HyDRA-MIS，在NISQ设备上实现了与经典求解器KaMIS竞争的结果。


<details>
  <summary>Details</summary>
Motivation: QAOA是解决组合优化问题的有前途的量子计算方法，但其参数优化复杂且计算成本高。通过GAT迁移参数和分布式算法，可以扩展其应用范围并提升效率。

Method: 1. 使用GAT迁移12和14顶点图的优化参数至更大图；2. 设计HyDRA-MIS算法，将大问题分解为适合NISQ设备的小问题；3. 结合GAT参数迁移与HyDRA-MIS。

Result: 在数千顶点图上，该方法与经典求解器KaMIS竞争，展示了量子计算在组合优化中的潜力。

Conclusion: GAT参数迁移和HyDRA-MIS的结合为QAOA在NISQ设备上的应用提供了可行路径，扩展了量子计算解决实际问题的能力。

Abstract: The quantum approximate optimization algorithm (QAOA) is one of the promising
variational approaches of quantum computing to solve combinatorial optimization
problems. In QAOA, variational parameters need to be optimized by solving a
series of nonlinear, nonconvex optimization programs. In this work, we propose
a QAOA parameter transfer scheme using Graph Attention Networks (GAT) to solve
Maximum Independent Set (MIS) problems. We prepare optimized parameters for
graphs of 12 and 14 vertices and use GATs to transfer their parameters to
larger graphs. Additionally, we design a hybrid distributed resource-aware
algorithm for MIS (HyDRA-MIS), which decomposes large problems into smaller
ones that can fit onto noisy intermediate-scale quantum (NISQ) computers. We
integrate our GAT-based parameter transfer approach to HyDRA-MIS and
demonstrate competitive results compared to KaMIS, a state-of-the-art classical
MIS solver, on graphs with several thousands vertices.

</details>


### [215] [Efficient Quantum-Safe Homomorphic Encryption for Quantum Computer Programs](https://arxiv.org/abs/2504.21235)
*Ben Goertzel*

Main category: quant-ph

TL;DR: 提出了一种基于格的量子程序同态加密方案，支持量子对抗环境下的安全验证。


<details>
  <summary>Details</summary>
Motivation: 将经典同态加密扩展到量子领域，解决量子计算中的安全与隐私问题。

Method: 使用MLWE格和BNSF掩码隐藏振幅，量子状态存储为MLWE密文对，并通过四混合归约证明安全性。

Result: 性能分析表明，方案适用于当前量子硬件，100量子比特的深度计算仅需10毫秒。

Conclusion: 该方案表明全同态量子推理与近量子云及后量子安全假设兼容。

Abstract: We present a lattice-based scheme for homomorphic evaluation of quantum
programs and proofs that remains secure against quantum adversaries. Classical
homomorphic encryption is lifted to the quantum setting by replacing
composite-order groups with Module Learning-With-Errors (MLWE) lattices and by
generalizing polynomial functors to bounded natural super functors (BNSFs). A
secret depolarizing BNSF mask hides amplitudes, while each quantum state is
stored as an MLWE ciphertext pair. We formalize security with the qIND-CPA game
that allows coherent access to the encryption oracle and give a four-hybrid
reduction to decisional MLWE.
  The design also covers practical issues usually left open. A typed QC-bridge
keeps classical bits produced by measurements encrypted yet still usable as
controls, with weak-measurement semantics for expectation-value workloads.
Encrypted Pauli twirls add circuit privacy. If a fixed knowledge base is
needed, its axioms are shipped as MLWE "capsules"; the evaluator can use them
but cannot read them. A rho-calculus driver schedules encrypted tasks across
several QPUs and records an auditable trace on an RChain-style ledger.
  Performance analysis shows that the extra lattice arithmetic fits inside
today's QPU idle windows: a 100-qubit, depth-10^3 teleportation-based proof
runs in about 10 ms, the public key (seed only) is 32 bytes, and even a
CCA-level key stays below 300 kB. A photonic Dirac-3 prototype that executes
homomorphic teleportation plus knowledge-base-relative amplitude checks appears
feasible with current hardware. These results indicate that fully homomorphic,
knowledge-base-aware quantum reasoning is compatible with near-term quantum
clouds and standard post-quantum security assumptions.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [216] [Scalable Multi-Task Learning for Particle Collision Event Reconstruction with Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2504.21844)
*William Sutcliffe,Marta Calvi,Simone Capelli,Jonas Eschle,Julián García Pardiñas,Abhijit Mathad,Azusa Uzuki,Nicola Serra*

Main category: physics.data-an

TL;DR: 论文提出了一种新型的异构图神经网络（HGNN）架构，用于解决大型强子对撞机（LHC）中粒子碰撞事件重建的挑战。该方法通过多任务学习和图剪枝层提高了性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型强子对撞机亮度增加，粒子碰撞事件的重建和分析面临挑战，包括高粒子多重性、延迟和存储需求增加，以及背景噪声和顶点错误关联等问题。

Method: 提出了一种异构图神经网络（HGNN）架构，结合多任务学习和图剪枝层，用于粒子顶点关联和图形剪枝。

Result: 该HGNN显著提高了美丽强子重建性能，并在单一框架内同时完成顶点关联和图剪枝，同时优化了推理时间。

Conclusion: 该方法为粒子碰撞事件的重建提供了更全面和可扩展的解决方案，特别是在高复杂度事件中表现优异。

Abstract: The growing luminosity frontier at the Large Hadron Collider is challenging
the reconstruction and analysis of particle collision events. Increased
particle multiplicities are straining latency and storage requirements at the
data acquisition stage, while new complications are emerging, including higher
background levels and more frequent particle vertex misassociations. This in
turn necessitates the development of more holistic and scalable reconstruction
methods that take advantage of recent advances in machine learning. We propose
a novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique
representations for diverse particle collision relationships and integrated
graph pruning layers for scalability. Trained with a multi-task paradigm in an
environment mimicking the LHCb experiment, this HGNN significantly improves
beauty hadron reconstruction performance. Notably, it concurrently performs
particle vertex association and graph pruning within a single framework. We
quantify reconstruction and pruning performance, demonstrate enhanced inference
time scaling with event complexity, and mitigate potential performance loss
using a weighted message passing scheme.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [217] [Data-driven operator learning for energy-efficient building control](https://arxiv.org/abs/2504.21243)
*Yuexin Bian,Yuanyuan Shi*

Main category: eess.SY

TL;DR: 提出了一种结合CFD物理精度与机器学习计算效率的数据驱动框架，用于建筑通风控制，实现节能与空气质量平衡。


<details>
  <summary>Details</summary>
Motivation: 传统CFD模拟计算成本高，难以实时应用于建筑管理系统，需高效替代方案。

Method: 训练神经算子变换器，学习控制动作到气流分布的映射，基于梯度优化通风参数。

Result: 相比传统方法，显著节能且保持空气质量。

Conclusion: 该方法实用且可扩展，适用于安全节能的建筑管理。

Abstract: Energy-efficient ventilation control plays a vital role in reducing building
energy consumption while ensuring occupant health and comfort. While
Computational Fluid Dynamics (CFD) simulations offer high-fidelity modeling of
airflow for building HVAC design, their high computational cost makes them
impractical for practical adoption in real-time building management system. In
this work, we present a data-driven framework that combines the physical
accuracy of CFD with the computational efficiency of machine learning to enable
energy-efficient building ventilation control. Our method jointly optimizes
airflow supply rates and vent angles to reduce energy use and adhere to air
quality constraints. We train a neural operator transformer to learn the
mapping from building control actions to airflow field distributions using
high-resolution CFD data. This learned operator enables a gradient-based
control framework capable of optimal decision-making. Experimental results
demonstrate that our approach achieves substantial energy savings compared to
maximum airflow rate control, rule-based control, and data-driven control based
on regional average CO2 predictions, while consistently maintaining safe indoor
air quality. These results highlight the practicality and scalability of our
method for enabling safe and energy-efficient building management.

</details>


### [218] [Power Flow Approximations for Multiphase Distribution Networks using Gaussian Processes](https://arxiv.org/abs/2504.21260)
*Daniel Glover,Parikshit Pareek,Deepjyoti Deka,Anamika Dubey*

Main category: eess.SY

TL;DR: 该论文提出了一种基于高斯过程（GPs）的数据驱动方法，用于近似多相电力潮流模型，通过少量训练数据即可可靠预测非线性电力潮流解，并在训练效率和测试性能上优于深度神经网络方法。


<details>
  <summary>Details</summary>
Motivation: 模型学习需要高效的电力潮流模型近似器，而现有方法在数据效率和鲁棒性上存在不足。

Method: 采用高斯过程（GPs）映射净负载注入到节点电压，近似多相电力潮流模型。

Result: 在IEEE 123总线和8500节点测试中，GP模型以少量训练数据实现高精度预测，训练样本减少85%时，平均绝对误差降低99.9%。

Conclusion: GP方法在数据效率和预测精度上显著优于深度神经网络，适用于电力分配网络的资源管理。

Abstract: Learning-based approaches are increasingly leveraged to manage and coordinate
the operation of grid-edge resources in active power distribution networks.
Among these, model-based techniques stand out for their superior data
efficiency and robustness compared to model-free methods. However, effective
model learning requires a learning-based approximator for the underlying power
flow model. This study extends existing work by introducing a data-driven power
flow method based on Gaussian Processes (GPs) to approximate the multiphase
power flow model, by mapping net load injections to nodal voltages. Simulation
results using the IEEE 123-bus and 8500-node distribution test feeders
demonstrate that the trained GP model can reliably predict the nonlinear power
flow solutions with minimal training data. We also conduct a comparative
analysis of the training efficiency and testing performance of the proposed
GP-based power flow approximator against a deep neural network-based
approximator, highlighting the advantages of our data-efficient approach.
Results over realistic operating conditions show that despite an 85% reduction
in the training sample size (corresponding to a 92.8% improvement in training
time), GP models produce a 99.9% relative reduction in mean absolute error
compared to the baselines of deep neural networks.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [219] [A Memetic Algorithm based on Variational Autoencoder for Black-Box Discrete Optimization with Epistasis among Parameters](https://arxiv.org/abs/2504.21338)
*Aoi Kato,Kenta Kojima,Masahiro Nomura,Isao Ono*

Main category: cs.NE

TL;DR: 提出了一种结合VAE采样与局部搜索的新算法，用于解决高维黑盒离散优化问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决黑盒离散优化中参数间相互依赖（epistasis）的挑战，结合VAE和局部搜索的优势。

Method: 结合VAE采样与局部搜索的模因算法，继承VAE-EDA和局部搜索的优点。

Result: 在NK landscapes基准测试中表现优于现有VAE-EDA方法及P3、DSMGA-II等领先方法。

Conclusion: 新算法有效处理高维问题，计算开销低，性能优于现有方法。

Abstract: Black-box discrete optimization (BB-DO) problems arise in many real-world
applications, such as neural architecture search and mathematical model
estimation. A key challenge in BB-DO is epistasis among parameters where
multiple variables must be modified simultaneously to effectively improve the
objective function. Estimation of Distribution Algorithms (EDAs) provide a
powerful framework for tackling BB-DO problems. In particular, an EDA
leveraging a Variational Autoencoder (VAE) has demonstrated strong performance
on relatively low-dimensional problems with epistasis while reducing
computational cost. Meanwhile, evolutionary algorithms such as DSMGA-II and P3,
which integrate bit-flip-based local search with linkage learning, have shown
excellent performance on high-dimensional problems. In this study, we propose a
new memetic algorithm that combines VAE-based sampling with local search. The
proposed method inherits the strengths of both VAE-based EDAs and local
search-based approaches: it effectively handles high-dimensional problems with
epistasis among parameters without incurring excessive computational overhead.
Experiments on NK landscapes -- a challenging benchmark for BB-DO involving
epistasis among parameters -- demonstrate that our method outperforms
state-of-the-art VAE-based EDA methods, as well as leading approaches such as
P3 and DSMGA-II.

</details>


### [220] [Meta knowledge assisted Evolutionary Neural Architecture Search](https://arxiv.org/abs/2504.21545)
*Yangyang Li,Guanlong Liu,Ronghua Shang,Licheng Jiao*

Main category: cs.NE

TL;DR: 本文提出了一种基于进化计算（EC）的高效神经架构搜索（NAS）方法，通过元学习框架解决高计算成本和固定学习率问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统EC-NAS方法中高计算成本和固定学习率导致的信息损失问题。

Method: 采用元学习率（Meta-LR）方案和自适应代理模型，结合周期性变异算子增强多样性。

Result: 在CIFAR-10、CIFAR-100和ImageNet1K数据集上表现优异，计算成本更低且鲁棒性更强。

Conclusion: 该方法在性能和效率上均优于现有方法，具有较高的实用价值。

Abstract: Evolutionary computation (EC)-based neural architecture search (NAS) has
achieved remarkable performance in the automatic design of neural
architectures. However, the high computational cost associated with evaluating
searched architectures poses a challenge for these methods, and a fixed form of
learning rate (LR) schedule means greater information loss on diverse searched
architectures. This paper introduces an efficient EC-based NAS method to solve
these problems via an innovative meta-learning framework. Specifically, a
meta-learning-rate (Meta-LR) scheme is used through pretraining to obtain a
suitable LR schedule, which guides the training process with lower information
loss when evaluating each individual. An adaptive surrogate model is designed
through an adaptive threshold to select the potential architectures in a few
epochs and then evaluate the potential architectures with complete epochs.
Additionally, a periodic mutation operator is proposed to increase the
diversity of the population, which enhances the generalizability and
robustness. Experiments on CIFAR-10, CIFAR-100, and ImageNet1K datasets
demonstrate that the proposed method achieves high performance comparable to
that of many state-of-the-art peer methods, with lower computational cost and
greater robustness.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [221] [Solving Copyright Infringement on Short Video Platforms: Novel Datasets and an Audio Restoration Deep Learning Pipeline](https://arxiv.org/abs/2504.21772)
*Minwoo Oh,Minsu Park,Eunil Park*

Main category: cs.MM

TL;DR: 提出了一种结合音乐源分离和跨模态视频-音乐检索的新方法，用于解决短视频平台中的版权问题，并引入了两个专用数据集。


<details>
  <summary>Details</summary>
Motivation: 短视频平台中，侵权者常通过添加背景音乐掩盖原声以逃避原创检测，导致版权合规问题。

Method: 整合音乐源分离（MSS）和跨模态视频-音乐检索（CMVMR），分离背景音乐并恢复原声。

Result: 实验表明，该方法能高精度去除背景音乐并恢复原声，确保内容完整性。

Conclusion: 该方法为短视频平台提供了伦理和可扩展的版权问题解决方案。

Abstract: Short video platforms like YouTube Shorts and TikTok face significant
copyright compliance challenges, as infringers frequently embed arbitrary
background music (BGM) to obscure original soundtracks (OST) and evade content
originality detection. To tackle this issue, we propose a novel pipeline that
integrates Music Source Separation (MSS) and cross-modal video-music retrieval
(CMVMR). Our approach effectively separates arbitrary BGM from the original
OST, enabling the restoration of authentic video audio tracks. To support this
work, we introduce two domain-specific datasets: OASD-20K for audio separation
and OSVAR-160 for pipeline evaluation. OASD-20K contains 20,000 audio clips
featuring mixed BGM and OST pairs, while OSVAR160 is a unique benchmark dataset
comprising 1,121 video and mixed-audio pairs, specifically designed for short
video restoration tasks. Experimental results demonstrate that our pipeline not
only removes arbitrary BGM with high accuracy but also restores OSTs, ensuring
content integrity. This approach provides an ethical and scalable solution to
copyright challenges in user-generated content on short video platforms.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [222] [On the Potential of Large Language Models to Solve Semantics-Aware Process Mining Tasks](https://arxiv.org/abs/2504.21074)
*Adrian Rebmann,Fabian David Schmidt,Goran Glavaš,Han van der Aa*

Main category: cs.DB

TL;DR: LLMs在过程挖掘任务中表现出潜力，尤其是在需要语义理解的任务中。通过上下文学习和监督微调，LLMs在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在语义感知过程挖掘任务中的能力，填补现有研究中对LLMs默认状态评估的不足。

Method: 定义五个需要语义理解的过程挖掘任务，并通过上下文学习和监督微调评估LLMs性能。

Result: LLMs在默认状态下表现不佳，但通过微调后能在多种任务和行业中取得强性能。

Conclusion: LLMs在语义感知过程挖掘任务中具有潜力，但需要适当的微调以发挥其能力。

Abstract: Large language models (LLMs) have shown to be valuable tools for tackling
process mining tasks. Existing studies report on their capability to support
various data-driven process analyses and even, to some extent, that they are
able to reason about how processes work. This reasoning ability suggests that
there is potential for LLMs to tackle semantics-aware process mining tasks,
which are tasks that rely on an understanding of the meaning of activities and
their relationships. Examples of these include process discovery, where the
meaning of activities can indicate their dependency, whereas in anomaly
detection the meaning can be used to recognize process behavior that is
abnormal. In this paper, we systematically explore the capabilities of LLMs for
such tasks. Unlike prior work, which largely evaluates LLMs in their default
state, we investigate their utility through both in-context learning and
supervised fine-tuning. Concretely, we define five process mining tasks
requiring semantic understanding and provide extensive benchmarking datasets
for evaluation. Our experiments reveal that while LLMs struggle with
challenging process mining tasks when used out of the box or with minimal
in-context examples, they achieve strong performance when fine-tuned for these
tasks across a broad range of process types and industries.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [223] [Light Weight CNN for classification of Brain Tumors from MRI Images](https://arxiv.org/abs/2504.21188)
*Natnael Alemayehu*

Main category: eess.IV

TL;DR: 提出了一种基于CNN的轻量级深度学习模型，用于MRI扫描中脑肿瘤的多分类，准确率达98.78%。


<details>
  <summary>Details</summary>
Motivation: 开发一种高效、低复杂度的模型，辅助临床早期脑肿瘤诊断。

Method: 结合图像预处理（归一化、数据增强、裁剪）、CNN架构优化及5折交叉验证。

Result: 模型分类准确率为98.78%。

Conclusion: 该方法为脑肿瘤早期诊断提供了有效且低复杂度的解决方案。

Abstract: This study presents a convolutional neural network (CNN)-based approach for
the multi-class classification of brain tumors using magnetic resonance imaging
(MRI) scans. We utilize a publicly available dataset containing MRI images
categorized into four classes: glioma, meningioma, pituitary tumor, and no
tumor. Our primary objective is to build a light weight deep learning model
that can automatically classify brain tumor types with high accuracy. To
achieve this goal, we incorporate image preprocessing steps, including
normalization, data augmentation, and a cropping technique designed to reduce
background noise and emphasize relevant regions. The CNN architecture is
optimized through hyperparameter tuning using Keras Tuner, enabling systematic
exploration of network parameters. To ensure reliable evaluation, we apply
5-fold cross-validation, where each hyperparameter configuration is evaluated
across multiple data splits to mitigate overfitting. Experimental results
demonstrate that the proposed model achieves a classification accuracy of
98.78%, indicating its potential as a diagnostic aid in clinical settings. The
proposed method offers a low-complexity yet effective solution for assisting in
early brain tumor diagnosis.

</details>


### [224] [Gradient Attention Map Based Verification of Deep Convolutional Neural Networks with Application to X-ray Image Datasets](https://arxiv.org/abs/2504.21227)
*Omid Halimi Milani,Amanda Nikho,Lauren Mills,Marouane Tliba,Ahmet Enis Cetin,Mohammed H. Elnagar*

Main category: eess.IV

TL;DR: 提出了一种综合验证框架，通过多种互补策略评估深度学习模型在医学影像中的适用性，确保预测的可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学影像中潜力巨大，但应用于与训练集不同的数据可能导致不可靠预测，影响患者护理。

Method: 1. 基于梯度注意力图（GAM）的方法分析注意力模式；2. 扩展到早期卷积特征图；3. 在分类模型中引入垃圾类以拒绝分布外输入。

Result: 实验表明，这些方法能有效识别不适用模型和输入。

Conclusion: 该框架促进了深度学习在医学影像中更安全可靠的部署。

Abstract: Deep learning models have great potential in medical imaging, including
orthodontics and skeletal maturity assessment. However, applying a model to
data different from its training set can lead to unreliable predictions that
may impact patient care. To address this, we propose a comprehensive
verification framework that evaluates model suitability through multiple
complementary strategies. First, we introduce a Gradient Attention Map
(GAM)-based approach that analyzes attention patterns using Grad-CAM and
compares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine
Similarity, Pearson Correlation, KL Divergence, and Wasserstein Distance.
Second, we extend verification to early convolutional feature maps, capturing
structural mis-alignments missed by attention alone. Finally, we incorporate an
additional garbage class into the classification model to explicitly reject
out-of-distribution inputs. Experimental results demonstrate that these
combined methods effectively identify unsuitable models and inputs, promoting
safer and more reliable deployment of deep learning in medical imaging.

</details>


### [225] [LoC-LIC: Low Complexity Learned Image Coding Using Hierarchical Feature Transforms](https://arxiv.org/abs/2504.21778)
*Ayman A. Ameen,Thomas Richter,André Kaup*

Main category: eess.IV

TL;DR: 提出了一种通过分层特征提取降低复杂度的图像压缩方法，显著减少计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 当前学习的图像压缩模型复杂度高，计算资源需求大，限制了其广泛应用。

Method: 采用分层特征提取变换，减少高空间分辨率输入/特征图的通道数，同时降低大通道数特征图的空间维度。

Result: 复杂度从1256 kMAC/Pixel降至270 kMAC/Pixel，性能未受影响。

Conclusion: 该方法为图像压缩模型在多种设备上高效运行提供了可能，并推动了新技术架构的发展。

Abstract: Current learned image compression models typically exhibit high complexity,
which demands significant computational resources. To overcome these
challenges, we propose an innovative approach that employs hierarchical feature
extraction transforms to significantly reduce complexity while preserving bit
rate reduction efficiency. Our novel architecture achieves this by using fewer
channels for high spatial resolution inputs/feature maps. On the other hand,
feature maps with a large number of channels have reduced spatial dimensions,
thereby cutting down on computational load without sacrificing performance.
This strategy effectively reduces the forward pass complexity from \(1256 \,
\text{kMAC/Pixel}\) to just \(270 \, \text{kMAC/Pixel}\). As a result, the
reduced complexity model can open the way for learned image compression models
to operate efficiently across various devices and pave the way for the
development of new architectures in image compression technology.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [226] [Generalised Label-free Artefact Cleaning for Real-time Medical Pulsatile Time Series](https://arxiv.org/abs/2504.21209)
*Xuhang Chen,Ihsane Olakorede,Stefan Yu Bögli,Wenhao Xu,Erta Beqiri,Xuemeng Li,Chenyu Tang,Zeyu Gao,Shuo Gao,Ari Ercole,Peter Smielewski*

Main category: eess.SP

TL;DR: GenClean是一个无需标签的通用框架，用于实时清理医疗时间序列中的伪影，适用于多种脉动信号，并在实际临床环境中验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 医疗时间序列中的伪影会影响临床决策，现有方法多依赖监督学习且忽略患者层面的分布变化。

Method: 提出GenClean框架，利用18万份动脉血压样本训练，验证其在患者内和患者间分布变化下的鲁棒性，并扩展到光电容积图信号。

Result: 在MIMIC-III数据库的跨疾病队列实验中表现优异，成功集成到临床研究监测软件ICM+中。

Conclusion: GenClean为高分辨率医疗时间序列分析的可靠性提供了基础，推动了精准医学的发展。

Abstract: Artefacts compromise clinical decision-making in the use of medical time
series. Pulsatile waveforms offer probabilities for accurate artefact
detection, yet most approaches rely on supervised manners and overlook
patient-level distribution shifts. To address these issues, we introduce a
generalised label-free framework, GenClean, for real-time artefact cleaning and
leverage an in-house dataset of 180,000 ten-second arterial blood pressure
(ABP) samples for training. We first investigate patient-level generalisation,
demonstrating robust performances under both intra- and inter-patient
distribution shifts. We further validate its effectiveness through challenging
cross-disease cohort experiments on the MIMIC-III database. Additionally, we
extend our method to photoplethysmography (PPG), highlighting its applicability
to diverse medical pulsatile signals. Finally, its integration into ICM+, a
clinical research monitoring software, confirms the real-time feasibility of
our framework, emphasising its practical utility in continuous physiological
monitoring. This work provides a foundational step toward precision medicine in
improving the reliability of high-resolution medical time series analysis

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [227] [Research on CNN-BiLSTM Network Traffic Anomaly Detection Model Based on MindSpore](https://arxiv.org/abs/2504.21008)
*Qiuyan Xiang,Shuang Wu,Dongze Wu,Yuxin Liu,Zhenkai Qin*

Main category: cs.CR

TL;DR: 提出了一种结合CNN和BiLSTM的网络流量异常检测模型，在MindSpore框架上实现，性能优异。


<details>
  <summary>Details</summary>
Motivation: 随着IoT和IIoT的普及，网络架构复杂化，流量激增，传统安全机制难以应对高频、多样且隐蔽的网络攻击。

Method: 集成CNN和BiLSTM的模型，使用NF-BoT-IoT数据集进行实验。

Result: 模型在准确率、精确率、召回率和F1分数上均达到99%，表现出色。

Conclusion: 该模型在网络入侵检测任务中具有强大的性能和鲁棒性。

Abstract: With the widespread adoption of the Internet of Things (IoT) and Industrial
IoT (IIoT) technologies, network architectures have become increasingly
complex, and the volume of traffic has grown substantially. This evolution
poses significant challenges to traditional security mechanisms, particularly
in detecting high-frequency, diverse, and highly covert network attacks. To
address these challenges, this study proposes a novel network traffic anomaly
detection model that integrates a Convolutional Neural Network (CNN) with a
Bidirectional Long Short-Term Memory (BiLSTM) network, implemented on the
MindSpore framework. Comprehensive experiments were conducted using the
NF-BoT-IoT dataset. The results demonstrate that the proposed model achieves
99% across accuracy, precision, recall, and F1-score, indicating its strong
performance and robustness in network intrusion detection tasks.

</details>


### [228] [Semantic-Aware Contrastive Fine-Tuning: Boosting Multimodal Malware Classification with Discriminative Embeddings](https://arxiv.org/abs/2504.21028)
*Ivan Montoya Sanchez,Shaswata Mitra,Aritran Piplai,Sudip Mittal*

Main category: cs.CR

TL;DR: 论文提出了一种对比微调（CFT）方法，通过基于余弦相似度的硬负样本选择优化LLM嵌入，显著提升了恶意软件分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 恶意软件变种的快速演变需要更鲁棒的分类方法，而现有LLM在语义嵌入和行为特征对齐方面存在局限性。

Method: 采用对比微调方法，结合高相似度和中等相似度负样本，优化LLM嵌入，并在MAML框架下构建多模态分类器。

Result: 在CIC-AndMal-2020数据集上，仅用20个样本即达到63.15%的分类准确率，优于基线11-21个百分点。

Conclusion: 该方法通过细粒度语义区分和可扩展框架，推动了恶意软件分类的研究，并为LLM在网络安全中的应用提供了新思路。

Abstract: The rapid evolution of malware variants requires robust classification
methods to enhance cybersecurity. While Large Language Models (LLMs) offer
potential for generating malware descriptions to aid family classification,
their utility is limited by semantic embedding overlaps and misalignment with
binary behavioral features. We propose a contrastive fine-tuning (CFT) method
that refines LLM embeddings via targeted selection of hard negative samples
based on cosine similarity, enabling LLMs to distinguish between closely
related malware families. Our approach combines high-similarity negatives to
enhance discriminative power and mid-tier negatives to increase embedding
diversity, optimizing both precision and generalization. Evaluated on the
CIC-AndMal-2020 and BODMAS datasets, our refined embeddings are integrated into
a multimodal classifier within a Model-Agnostic Meta-Learning (MAML) framework
on a few-shot setting. Experiments demonstrate significant improvements: our
method achieves 63.15% classification accuracy with as few as 20 samples on
CIC-AndMal-2020, outperforming baselines by 11--21 percentage points and
surpassing prior negative sampling strategies. Ablation studies confirm the
superiority of similarity-based selection over random sampling, with gains of
10-23%. Additionally, fine-tuned LLMs generate attribute-aware descriptions
that generalize to unseen variants, bridging textual and binary feature gaps.
This work advances malware classification by enabling nuanced semantic
distinctions and provides a scalable framework for adapting LLMs to
cybersecurity challenges.

</details>


### [229] [PICO: Secure Transformers via Robust Prompt Isolation and Cybersecurity Oversight](https://arxiv.org/abs/2504.21029)
*Ben Goertzel,Paulos Yibelo*

Main category: cs.CR

TL;DR: 提出了一种名为PICO的鲁棒Transformer架构，通过双通道分离系统指令与用户输入，结合安全专家代理和知识图谱，防止提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer模型在提示注入攻击下的安全问题，确保生成响应的可靠性和安全性。

Method: 采用双通道处理系统指令和用户输入，结合MoE框架中的安全专家代理和知识图谱，并通过数学公式和案例研究验证。

Result: PICO框架能有效防止提示注入攻击，同时支持从头训练或低成本微调。

Conclusion: PICO为Transformer模型提供了一种安全可靠的架构设计，适用于对抗性环境。

Abstract: We propose a robust transformer architecture designed to prevent prompt
injection attacks and ensure secure, reliable response generation. Our PICO
(Prompt Isolation and Cybersecurity Oversight) framework structurally separates
trusted system instructions from untrusted user inputs through dual channels
that are processed independently and merged only by a controlled, gated fusion
mechanism. In addition, we integrate a specialized Security Expert Agent within
a Mixture-of-Experts (MoE) framework and incorporate a Cybersecurity Knowledge
Graph (CKG) to supply domain-specific reasoning. Our training design further
ensures that the system prompt branch remains immutable while the rest of the
network learns to handle adversarial inputs safely. This PICO framework is
presented via a general mathematical formulation, then elaborated in terms of
the specifics of transformer architecture, and fleshed out via hypothetical
case studies including Policy Puppetry attacks. While the most effective
implementation may involve training transformers in a PICO-based way from
scratch, we also present a cost-effective fine-tuning approach.

</details>


### [230] [SAGA: A Security Architecture for Governing AI Agentic Systems](https://arxiv.org/abs/2504.21034)
*Georgios Syros,Anshuman Suri,Cristina Nita-Rotaru,Alina Oprea*

Main category: cs.CR

TL;DR: SAGA是一种安全架构，用于管理自主代理系统，提供用户对代理生命周期的监督，确保安全性和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前自主代理系统缺乏用户控制的管理机制，存在潜在安全风险，需要一种实际可行的解决方案。

Method: 提出SAGA架构，通过中央实体（Provider）管理代理注册、访问控制策略，并引入加密机制生成细粒度访问控制令牌。

Result: 在多种任务和环境下评估SAGA，显示其性能开销极小且不影响任务效用。

Conclusion: SAGA为自主代理的安全部署提供了可行方案，适用于敏感环境。

Abstract: Large Language Model (LLM)-based agents increasingly interact, collaborate,
and delegate tasks to one another autonomously with minimal human interaction.
Industry guidelines for agentic system governance emphasize the need for users
to maintain comprehensive control over their agents, mitigating potential
damage from malicious agents. Several proposed agentic system designs address
agent identity, authorization, and delegation, but remain purely theoretical,
without concrete implementation and evaluation. Most importantly, they do not
provide user-controlled agent management. To address this gap, we propose SAGA,
a Security Architecture for Governing Agentic systems, that offers user
oversight over their agents' lifecycle. In our design, users register their
agents with a central entity, the Provider, that maintains agents contact
information, user-defined access control policies, and helps agents enforce
these policies on inter-agent communication. We introduce a cryptographic
mechanism for deriving access control tokens, that offers fine-grained control
over an agent's interaction with other agents, balancing security and
performance consideration. We evaluate SAGA on several agentic tasks, using
agents in different geolocations, and multiple on-device and cloud LLMs,
demonstrating minimal performance overhead with no impact on underlying task
utility in a wide range of conditions. Our architecture enables secure and
trustworthy deployment of autonomous agents, accelerating the responsible
adoption of this technology in sensitive environments.

</details>


### [231] [Can Differentially Private Fine-tuning LLMs Protect Against Privacy Attacks?](https://arxiv.org/abs/2504.21036)
*Hao Du,Shang Liu,Yang Cao*

Main category: cs.CR

TL;DR: 论文研究了在大型语言模型（LLM）微调过程中应用差分隐私（DP）对隐私风险和模型效用的影响，发现DP能显著降低隐私风险，但对不同微调方法的效用影响差异较大。


<details>
  <summary>Details</summary>
Motivation: 微调LLM时可能泄露敏感数据，而DP的理论隐私保护效果在实际应用中尚不明确，尤其是在不同微调方法下的表现。

Method: 通过数据提取和成员推理攻击，系统评估不同微调方法和隐私预算下DP的隐私保护效果。

Result: 1. DP降低模型效用，但影响因微调方法而异；2. 无DP时，不同微调方法的隐私风险差异显著；3. 高隐私预算下DP仍能显著降低风险；4. 不同微调方法的隐私-效用权衡差异大。

Conclusion: 研究为隐私敏感的LLM部署提供了实用指导，并为优化微调方法中的隐私-效用权衡开辟了未来研究方向。

Abstract: Fine-tuning large language models (LLMs) has become an essential strategy for
adapting them to specialized tasks; however, this process introduces
significant privacy challenges, as sensitive training data may be inadvertently
memorized and exposed. Although differential privacy (DP) offers strong
theoretical guarantees against such leakage, its empirical privacy
effectiveness on LLMs remains unclear, especially under different fine-tuning
methods. In this paper, we systematically investigate the impact of DP across
fine-tuning methods and privacy budgets, using both data extraction and
membership inference attacks to assess empirical privacy risks. Our main
findings are as follows: (1) Differential privacy reduces model utility, but
its impact varies significantly across different fine-tuning methods. (2)
Without DP, the privacy risks of models fine-tuned with different approaches
differ considerably. (3) When DP is applied, even a relatively high privacy
budget can substantially lower privacy risk. (4) The privacy-utility trade-off
under DP training differs greatly among fine-tuning methods, with some methods
being unsuitable for DP due to severe utility degradation. Our results provide
practical guidance for privacy-conscious deployment of LLMs and pave the way
for future research on optimizing the privacy-utility trade-off in fine-tuning
methodologies.

</details>


### [232] [Security Bug Report Prediction Within and Across Projects: A Comparative Study of BERT and Random Forest](https://arxiv.org/abs/2504.21037)
*Farnaz Soltaniani,Mohammad Ghafari,Mohammed Sayagh*

Main category: cs.CR

TL;DR: 比较BERT和随机森林（RF）在安全漏洞报告（SBR）预测中的性能，发现RF在项目内预测表现更好，而BERT在跨项目预测中显著优于RF。


<details>
  <summary>Details</summary>
Motivation: 早期检测安全漏洞报告对系统可靠性至关重要，现有机器学习模型仍有改进空间。

Method: 对BERT和随机森林（RF）进行全面比较，评估其在SBR预测中的表现。

Result: RF在项目内预测中平均G-measure比BERT高34%，而BERT在跨项目预测中以62%的G-measure显著优于RF。

Conclusion: BERT在跨项目SBR预测中表现更优，而RF在项目内预测中更具优势。

Abstract: Early detection of security bug reports (SBRs) is crucial for preventing
vulnerabilities and ensuring system reliability. While machine learning models
have been developed for SBR prediction, their predictive performance still has
room for improvement. In this study, we conduct a comprehensive comparison
between BERT and Random Forest (RF), a competitive baseline for predicting
SBRs. The results show that RF outperforms BERT with a 34% higher average
G-measure for within-project predictions. Adding only SBRs from various
projects improves both models' average performance. However, including both
security and nonsecurity bug reports significantly reduces RF's average
performance to 46%, while boosts BERT to its best average performance of 66%,
surpassing RF. In cross-project SBR prediction, BERT achieves a remarkable 62%
G-measure, which is substantially higher than RF.

</details>


### [233] [Prefill-Based Jailbreak: A Novel Approach of Bypassing LLM Safety Boundary](https://arxiv.org/abs/2504.21038)
*Yakai Li,Jiekang Hu,Weiduan Sang,Luping Ma,Jing Xie,Weijuan Zhang,Aimin Yu,Shijie Zhao,Qingjia Huang,Qihang Zhou*

Main category: cs.CR

TL;DR: 本文提出了一种新型的LLM越狱攻击方法，通过操纵预填充特征绕过安全机制，显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 研究越狱方法以揭示LLM的系统性漏洞，指导开发者加强安全性。

Method: 利用预填充特征直接操纵后续令牌的概率分布，提出静态预填充（SP）和优化预填充（OP）两种变体。

Result: 在六种先进LLM上验证，OP方法攻击成功率高达99.82%，显著优于基线方法。

Conclusion: 强调需要更鲁棒的内容验证机制来防范预填充特征的对抗性利用。

Abstract: Large Language Models (LLMs) are designed to generate helpful and safe
content. However, adversarial attacks, commonly referred to as jailbreak, can
bypass their safety protocols, prompting LLMs to generate harmful content or
reveal sensitive data. Consequently, investigating jailbreak methodologies is
crucial for exposing systemic vulnerabilities within LLMs, ultimately guiding
the continuous implementation of security enhancements by developers. In this
paper, we introduce a novel jailbreak attack method that leverages the
prefilling feature of LLMs, a feature designed to enhance model output
constraints. Unlike traditional jailbreak methods, the proposed attack
circumvents LLMs' safety mechanisms by directly manipulating the probability
distribution of subsequent tokens, thereby exerting control over the model's
output. We propose two attack variants: Static Prefilling (SP), which employs a
universal prefill text, and Optimized Prefilling (OP), which iteratively
optimizes the prefill text to maximize the attack success rate. Experiments on
six state-of-the-art LLMs using the AdvBench benchmark validate the
effectiveness of our method and demonstrate its capability to substantially
enhance attack success rates when combined with existing jailbreak approaches.
The OP method achieved attack success rates of up to 99.82% on certain models,
significantly outperforming baseline methods. This work introduces a new
jailbreak attack method in LLMs, emphasizing the need for robust content
validation mechanisms to mitigate the adversarial exploitation of prefilling
features. All code and data used in this paper are publicly available.

</details>


### [234] [Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report](https://arxiv.org/abs/2504.21039)
*Paul Kassianik,Baturay Saglam,Alexander Chen,Blaine Nelson,Anu Vellore,Massimo Aufiero,Fraser Burch,Dhruv Kedia,Avi Zohary,Sajana Weerawardhena,Aman Priyanshu,Adam Swanda,Amy Chang,Hyrum Anderson,Kojin Oshiba,Omar Santos,Yaron Singer,Amin Karbasi*

Main category: cs.CR

TL;DR: 论文介绍了Foundation-Sec-8B，一个基于Llama 3.1架构的网络安全专用大语言模型，通过精心策划的网络安全语料库进行预训练，解决了当前LLMs在网络安全领域应用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏专业训练数据和网络安全知识表示的复杂性，大语言模型在网络安全领域的应用受限。

Method: 基于Llama 3.1架构，通过继续预训练网络安全语料库，构建了Foundation-Sec-8B模型。

Result: 在网络安全任务中，Foundation-Sec-8B表现与Llama 3.1-70B和GPT-4o-mini相当。

Conclusion: 公开模型以促进AI工具在公共和私有网络安全领域的应用。

Abstract: As transformer-based large language models (LLMs) increasingly permeate
society, they have revolutionized domains such as software engineering,
creative writing, and digital arts. However, their adoption in cybersecurity
remains limited due to challenges like scarcity of specialized training data
and complexity of representing cybersecurity-specific knowledge. To address
these gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on
the Llama 3.1 architecture and enhanced through continued pretraining on a
carefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across
both established and new cybersecurity benchmarks, showing that it matches
Llama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By
releasing our model to the public, we aim to accelerate progress and adoption
of AI-driven tools in both public and private cybersecurity contexts.

</details>


### [235] [What's Pulling the Strings? Evaluating Integrity and Attribution in AI Training and Inference through Concept Shift](https://arxiv.org/abs/2504.21042)
*Jiamin Chang,Haoyang Li,Hammond Pearce,Ruoxi Sun,Bo Li,Minhui Xue*

Main category: cs.CR

TL;DR: ConceptLens是一个通用框架，利用预训练多模态模型分析概念漂移，评估AI系统的信任问题，如完整性、隐私、偏见等。


<details>
  <summary>Details</summary>
Motivation: 随着AI的广泛应用，其信任问题（如完整性、隐私、偏见等）日益突出，需一种方法评估和溯源这些威胁。

Method: 通过预训练多模态模型分析概念漂移，检测数据投毒攻击、隐私风险，并识别模型依赖的误导性概念。

Result: ConceptLens能高效检测数据投毒攻击，揭示隐私风险和社会学偏见，并识别模型弱点。

Conclusion: ConceptLens为提升AI系统信任提供了可行见解，加速其应用和创新。

Abstract: The growing adoption of artificial intelligence (AI) has amplified concerns
about trustworthiness, including integrity, privacy, robustness, and bias. To
assess and attribute these threats, we propose ConceptLens, a generic framework
that leverages pre-trained multimodal models to identify the root causes of
integrity threats by analyzing Concept Shift in probing samples. ConceptLens
demonstrates strong detection performance for vanilla data poisoning attacks
and uncovers vulnerabilities to bias injection, such as the generation of
covert advertisements through malicious concept shifts. It identifies privacy
risks in unaltered but high-risk samples, filters them before training, and
provides insights into model weaknesses arising from incomplete or imbalanced
training data. Additionally, at the model level, it attributes concepts that
the target model is overly dependent on, identifies misleading concepts, and
explains how disrupting key concepts negatively impacts the model. Furthermore,
it uncovers sociological biases in generative content, revealing disparities
across sociological contexts. Strikingly, ConceptLens reveals how safe training
and inference data can be unintentionally and easily exploited, potentially
undermining safety alignment. Our study informs actionable insights to breed
trust in AI systems, thereby speeding adoption and driving greater innovation.

</details>


### [236] [CodeBC: A More Secure Large Language Model for Smart Contract Code Generation in Blockchain](https://arxiv.org/abs/2504.21043)
*Lingxiang wang,Hainan Zhang,Qinnan Zhang,Ziwei Wang,Hongwei Zheng,Jin Dong,Zhiming Zheng*

Main category: cs.CR

TL;DR: CodeBC是一个专为生成安全的区块链智能合约设计的代码生成模型，通过三阶段微调方法，无需依赖成对的漏洞标注数据，显著降低了漏洞率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生成代码时缺乏对安全漏洞的理解，尤其在低资源语言（如Solidity）中，标注数据稀缺。

Method: 基于CodeLlama的三阶段微调方法，利用漏洞和安全标签区分代码安全性，无需成对标注。

Result: CodeBC在BLEU、CodeBLEU和编译通过率上优于基线模型，漏洞率显著降低。

Conclusion: CodeBC的三阶段微调策略高效且成本低，为生成安全的智能合约代码提供了可行方案。

Abstract: Large language models (LLMs) excel at generating code from natural language
instructions, yet they often lack an understanding of security vulnerabilities.
This limitation makes it difficult for LLMs to avoid security risks in
generated code, particularly in high-security programming tasks such as smart
contract development for blockchain. Researchers have attempted to enhance the
vulnerability awareness of these models by training them to differentiate
between vulnerable and fixed code snippets. However, this approach relies
heavily on manually labeled vulnerability data, which is only available for
popular languages like Python and C++. For low-resource languages like
Solidity, used in smart contracts, large-scale annotated datasets are scarce
and difficult to obtain. To address this challenge, we introduce CodeBC, a code
generation model specifically designed for generating secure smart contracts in
blockchain. CodeBC employs a three-stage fine-tuning approach based on
CodeLlama, distinguishing itself from previous methods by not relying on
pairwise vulnerability location annotations. Instead, it leverages
vulnerability and security tags to teach the model the differences between
vulnerable and secure code. During the inference phase, the model leverages
security tags to generate secure and robust code. Experimental results
demonstrate that CodeBC outperforms baseline models in terms of BLEU, CodeBLEU,
and compilation pass rates, while significantly reducing vulnerability rates.
These findings validate the effectiveness and cost-efficiency of our
three-stage fine-tuning strategy, making CodeBC a promising solution for
generating secure smart contract code.

</details>


### [237] [AGATE: Stealthy Black-box Watermarking for Multimodal Model Copyright Protection](https://arxiv.org/abs/2504.21044)
*Jianbo Gao,Keke Gai,Jing Yu,Liehuang Zhu,Qi Wu*

Main category: cs.CR

TL;DR: 本文提出了一种名为AGATE的黑盒后门水印框架，用于解决多模态模型版权保护中的隐蔽性和鲁棒性问题。通过生成隐蔽的对抗触发器并引入后变换模块，AGATE在多项任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法易受恶意检测和伪造攻击，导致水印失效，因此需要一种更隐蔽且鲁棒的版权保护方法。

Method: 提出对抗触发器生成方法，生成隐蔽的对抗触发器；引入后变换模块以纠正模型输出；设计两阶段水印验证机制。

Result: 在五个数据集的多模态图像-文本检索和图像分类任务中，AGATE优于现有方法，并在对抗攻击场景下表现出鲁棒性。

Conclusion: AGATE为多模态模型版权保护提供了一种隐蔽且鲁棒的解决方案。

Abstract: Recent advancement in large-scale Artificial Intelligence (AI) models
offering multimodal services have become foundational in AI systems, making
them prime targets for model theft. Existing methods select Out-of-Distribution
(OoD) data as backdoor watermarks and retrain the original model for copyright
protection. However, existing methods are susceptible to malicious detection
and forgery by adversaries, resulting in watermark evasion. In this work, we
propose Model-\underline{ag}nostic Black-box Backdoor W\underline{ate}rmarking
Framework (AGATE) to address stealthiness and robustness challenges in
multimodal model copyright protection. Specifically, we propose an adversarial
trigger generation method to generate stealthy adversarial triggers from
ordinary dataset, providing visual fidelity while inducing semantic shifts. To
alleviate the issue of anomaly detection among model outputs, we propose a
post-transform module to correct the model output by narrowing the distance
between adversarial trigger image embedding and text embedding. Subsequently, a
two-phase watermark verification is proposed to judge whether the current model
infringes by comparing the two results with and without the transform module.
Consequently, we consistently outperform state-of-the-art methods across five
datasets in the downstream tasks of multimodal image-text retrieval and image
classification. Additionally, we validated the robustness of AGATE under two
adversarial attack scenarios.

</details>


### [238] [Leveraging LLM to Strengthen ML-Based Cross-Site Scripting Detection](https://arxiv.org/abs/2504.21045)
*Dennis Miczek,Divyesh Gabbireddy,Suman Saha*

Main category: cs.CR

TL;DR: 该研究提出了一种利用大型语言模型（LLM）生成复杂混淆XSS攻击载荷的方法，显著提高了机器学习模型检测混淆XSS攻击的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种技术用于防御XSS攻击，但混淆XSS攻击仍难以检测，且现有工具生成的混淆代码复杂度有限。

Method: 通过微调LLM自动生成复杂混淆XSS载荷，并用于训练机器学习模型。

Result: 使用LLM生成的混淆数据集训练后，模型准确率达到99.5%，且生成的样本复杂度比现有工具高28.1%。

Conclusion: 该方法显著提升了模型对高级XSS攻击的检测能力，适用于实际应用安全。

Abstract: According to the Open Web Application Security Project (OWASP), Cross-Site
Scripting (XSS) is a critical security vulnerability. Despite decades of
research, XSS remains among the top 10 security vulnerabilities. Researchers
have proposed various techniques to protect systems from XSS attacks, with
machine learning (ML) being one of the most widely used methods. An ML model is
trained on a dataset to identify potential XSS threats, making its
effectiveness highly dependent on the size and diversity of the training data.
A variation of XSS is obfuscated XSS, where attackers apply obfuscation
techniques to alter the code's structure, making it challenging for security
systems to detect its malicious intent. Our study's random forest model was
trained on traditional (non-obfuscated) XSS data achieved 99.8% accuracy.
However, when tested against obfuscated XSS samples, accuracy dropped to 81.9%,
underscoring the importance of training ML models with obfuscated data to
improve their effectiveness in detecting XSS attacks. A significant challenge
is to generate highly complex obfuscated code despite the availability of
several public tools. These tools can only produce obfuscation up to certain
levels of complexity.
  In our proposed system, we fine-tune a Large Language Model (LLM) to generate
complex obfuscated XSS payloads automatically. By transforming original XSS
samples into diverse obfuscated variants, we create challenging training data
for ML model evaluation. Our approach achieved a 99.5% accuracy rate with the
obfuscated dataset. We also found that the obfuscated samples generated by the
LLMs were 28.1% more complex than those created by other tools, significantly
improving the model's ability to handle advanced XSS attacks and making it more
effective for real-world application security.

</details>


### [239] [Phishing URL Detection using Bi-LSTM](https://arxiv.org/abs/2504.21049)
*Sneha Baskota*

Main category: cs.CR

TL;DR: 提出了一种基于Bi-LSTM的深度学习模型，用于分类URL，提高钓鱼攻击检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统钓鱼检测系统存在高误报率和检测类型有限的问题。

Method: 使用双向长短期记忆网络（Bi-LSTM）对URL进行分类，分为良性、钓鱼、篡改和恶意软件四类。

Result: 在包含65万URL的数据集上，模型准确率达到97%，优于传统方法。

Conclusion: Bi-LSTM模型能有效提升钓鱼攻击检测的准确性和适应性。

Abstract: Phishing attacks threaten online users, often leading to data breaches,
financial losses, and identity theft. Traditional phishing detection systems
struggle with high false positive rates and are usually limited by the types of
attacks they can identify. This paper proposes a deep learning-based approach
using a Bidirectional Long Short-Term Memory (Bi-LSTM) network to classify URLs
into four categories: benign, phishing, defacement, and malware. The model
leverages sequential URL data and captures contextual information, improving
the accuracy of phishing detection. Experimental results on a dataset
comprising over 650,000 URLs demonstrate the model's effectiveness, achieving
97% accuracy and significant improvements over traditional techniques.

</details>


### [240] [SFIBA: Spatial-based Full-target Invisible Backdoor Attacks](https://arxiv.org/abs/2504.21052)
*Yangxu Yin,Honglong Chen,Yudong Gao,Peng Sun,Zhishuai Li,Weifeng Liu*

Main category: cs.CR

TL;DR: SFIBA是一种基于空间的全目标隐形后门攻击方法，通过限制触发器的空间区域和形态，并结合频域注入技术，实现了高攻击性能和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有多目标后门攻击在触发特异性和隐蔽性上存在不足，限制了其在实际攻击场景中的有效性。

Method: SFIBA通过快速傅里叶变换、离散小波变换和奇异值分解，在频域中注入触发器，并在像素空间中限制其形态。

Result: 实验表明，SFIBA具有优异的攻击性能和隐蔽性，同时不影响模型在正常样本上的表现，并能绕过现有防御。

Conclusion: SFIBA解决了多目标后门攻击的触发特异性和隐蔽性问题，为实际攻击场景提供了更有效的解决方案。

Abstract: Multi-target backdoor attacks pose significant security threats to deep
neural networks, as they can preset multiple target classes through a single
backdoor injection. This allows attackers to control the model to misclassify
poisoned samples with triggers into any desired target class during inference,
exhibiting superior attack performance compared with conventional backdoor
attacks. However, existing multi-target backdoor attacks fail to guarantee
trigger specificity and stealthiness in black-box settings, resulting in two
main issues. First, they are unable to simultaneously target all classes when
only training data can be manipulated, limiting their effectiveness in
realistic attack scenarios. Second, the triggers often lack visual
imperceptibility, making poisoned samples easy to detect. To address these
problems, we propose a Spatial-based Full-target Invisible Backdoor Attack,
called SFIBA. It restricts triggers for different classes to specific local
spatial regions and morphologies in the pixel space to ensure specificity,
while employing a frequency-domain-based trigger injection method to guarantee
stealthiness. Specifically, for injection of each trigger, we first apply fast
fourier transform to obtain the amplitude spectrum of clean samples in local
spatial regions. Then, we employ discrete wavelet transform to extract the
features from the amplitude spectrum and use singular value decomposition to
integrate the trigger. Subsequently, we selectively filter parts of the trigger
in pixel space to implement trigger morphology constraints and adjust injection
coefficients based on visual effects. We conduct experiments on multiple
datasets and models. The results demonstrate that SFIBA can achieve excellent
attack performance and stealthiness, while preserving the model's performance
on benign samples, and can also bypass existing backdoor defenses.

</details>


### [241] [FFCBA: Feature-based Full-target Clean-label Backdoor Attacks](https://arxiv.org/abs/2504.21054)
*Yangxu Yin,Honglong Chen,Yudong Gao,Peng Sun,Liantao Wu,Zhe Li,Weifeng Liu*

Main category: cs.CR

TL;DR: 本文提出了一种基于特征的多目标干净标签后门攻击方法（FFCBA），包括特征跨越攻击（FSBA）和特征迁移攻击（FMBA），解决了现有干净标签攻击在多目标场景下的性能不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有多目标后门攻击多为脏标签范式，容易被检测，而干净标签攻击在多目标场景下性能不佳。

Method: FFCBA包含FSBA和FMBA两种范式：FSBA利用类条件自编码器生成噪声触发器，FMBA通过两阶段训练生成具有目标类特征的触发器。

Result: 实验表明，FFCBA在多数据集和模型上表现出色，且对现有防御方法具有鲁棒性。

Conclusion: FFCBA为多目标干净标签后门攻击提供了高效且隐蔽的解决方案。

Abstract: Backdoor attacks pose a significant threat to deep neural networks, as
backdoored models would misclassify poisoned samples with specific triggers
into target classes while maintaining normal performance on clean samples.
Among these, multi-target backdoor attacks can simultaneously target multiple
classes. However, existing multi-target backdoor attacks all follow the
dirty-label paradigm, where poisoned samples are mislabeled, and most of them
require an extremely high poisoning rate. This makes them easily detectable by
manual inspection. In contrast, clean-label attacks are more stealthy, as they
avoid modifying the labels of poisoned samples. However, they generally
struggle to achieve stable and satisfactory attack performance and often fail
to scale effectively to multi-target attacks. To address this issue, we propose
the Feature-based Full-target Clean-label Backdoor Attacks (FFCBA) which
consists of two paradigms: Feature-Spanning Backdoor Attacks (FSBA) and
Feature-Migrating Backdoor Attacks (FMBA). FSBA leverages class-conditional
autoencoders to generate noise triggers that align perturbed in-class samples
with the original category's features, ensuring the effectiveness, intra-class
consistency, inter-class specificity and natural-feature correlation of
triggers. While FSBA supports swift and efficient attacks, its cross-model
attack capability is relatively weak. FMBA employs a two-stage
class-conditional autoencoder training process that alternates between using
out-of-class samples and in-class samples. This allows FMBA to generate
triggers with strong target-class features, making it highly effective for
cross-model attacks. We conduct experiments on multiple datasets and models,
the results show that FFCBA achieves outstanding attack performance and
maintains desirable robustness against the state-of-the-art backdoor defenses.

</details>


### [242] [A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage](https://arxiv.org/abs/2504.21035)
*Rui Xin,Niloofar Mireshghallah,Shuyue Stella Li,Michael Duan,Hyunwoo Kim,Yejin Choi,Yulia Tsvetkov,Sewoong Oh,Pang Wei Koh*

Main category: cs.CR

TL;DR: 论文提出了一种新框架，用于评估脱敏数据的再识别风险，揭示现有隐私保护方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本数据脱敏方法仅关注显式标识符的泄漏，忽略细微文本标记导致的隐私风险。

Method: 提出新框架，通过再识别攻击量化隐私风险，并测试商业工具的效果。

Result: 实验显示，Azure的PII移除工具在MedQA数据集中保护失败率达74%，差分隐私虽有效但降低数据实用性。

Conclusion: 当前脱敏技术存在虚假隐私感，需开发更鲁棒的方法以防止语义级信息泄漏。

Abstract: Sanitizing sensitive text data typically involves removing personally
identifiable information (PII) or generating synthetic data under the
assumption that these methods adequately protect privacy; however, their
effectiveness is often only assessed by measuring the leakage of explicit
identifiers but ignoring nuanced textual markers that can lead to
re-identification. We challenge the above illusion of privacy by proposing a
new framework that evaluates re-identification attacks to quantify individual
privacy risks upon data release. Our approach shows that seemingly innocuous
auxiliary information -- such as routine social activities -- can be used to
infer sensitive attributes like age or substance use history from sanitized
data. For instance, we demonstrate that Azure's commercial PII removal tool
fails to protect 74\% of information in the MedQA dataset. Although
differential privacy mitigates these risks to some extent, it significantly
reduces the utility of the sanitized text for downstream tasks. Our findings
indicate that current sanitization techniques offer a \textit{false sense of
privacy}, highlighting the need for more robust methods that protect against
semantic-level information leakage.

</details>


### [243] [Erased but Not Forgotten: How Backdoors Compromise Concept Erasure](https://arxiv.org/abs/2504.21072)
*Jonas Henry Grebe,Tobias Braun,Marcus Rohrbach,Anna Rohrbach*

Main category: cs.CR

TL;DR: 论文提出了一种新的威胁模型Toxic Erasure (ToxE)，揭示了现有机器学习去学习技术可能被针对性后门攻击绕过，导致有害内容生成。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型的扩展引发了对其生成不良或有害内容的担忧，现有去学习技术可能被攻击绕过。

Method: 提出ToxE威胁模型，设计两种后门攻击（针对文本编码器和跨注意力层），并引入新型攻击DISA。

Result: ToxE攻击在名人身份和露骨内容去学习中分别达到82%和9倍的成功率，DISA攻击平均提升2.9倍。

Conclusion: 当前去学习策略存在严重安全漏洞，需进一步研究防御方法。

Abstract: The expansion of large-scale text-to-image diffusion models has raised
growing concerns about their potential to generate undesirable or harmful
content, ranging from fabricated depictions of public figures to sexually
explicit images. To mitigate these risks, prior work has devised machine
unlearning techniques that attempt to erase unwanted concepts through
fine-tuning. However, in this paper, we introduce a new threat model, Toxic
Erasure (ToxE), and demonstrate how recent unlearning algorithms, including
those explicitly designed for robustness, can be circumvented through targeted
backdoor attacks. The threat is realized by establishing a link between a
trigger and the undesired content. Subsequent unlearning attempts fail to erase
this link, allowing adversaries to produce harmful content. We instantiate ToxE
via two established backdoor attacks: one targeting the text encoder and
another manipulating the cross-attention layers. Further, we introduce Deep
Intervention Score-based Attack (DISA), a novel, deeper backdoor attack that
optimizes the entire U-Net using a score-based objective, improving the
attack's persistence across different erasure methods. We evaluate five recent
concept erasure methods against our threat model. For celebrity identity
erasure, our deep attack circumvents erasure with up to 82% success, averaging
57% across all erasure methods. For explicit content erasure, ToxE attacks can
elicit up to 9 times more exposed body parts, with DISA yielding an average
increase by a factor of 2.9. These results highlight a critical security gap in
current unlearning strategies.

</details>


### [244] [SecRepoBench: Benchmarking LLMs for Secure Code Generation in Real-World Repositories](https://arxiv.org/abs/2504.21205)
*Connor Dilgren,Purva Chiniya,Luke Griffith,Yu Ding,Yizheng Chen*

Main category: cs.CR

TL;DR: SecRepoBench是一个评估LLMs在真实代码库中生成安全代码能力的基准，包含318个任务和27个C/C++仓库，覆盖15种CWE。研究发现现有LLMs在生成正确和安全代码方面表现不佳，且提示工程技术在仓库级别效果有限。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在真实代码库中生成安全代码的能力，填补现有基准的不足。

Method: 构建SecRepoBench基准，包含318个任务和27个仓库，评估19种LLMs，并测试提示工程技术。

Result: LLMs在生成安全代码方面表现不佳，提示工程技术效果有限，SecRepoBench是目前最难的基准。

Conclusion: 研究为提升LLMs在真实代码库中生成安全代码的能力提供了方向。

Abstract: This paper introduces SecRepoBench, a benchmark to evaluate LLMs on secure
code generation in real-world repositories. SecRepoBench has 318 code
generation tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 19
state-of-the-art LLMs using our benchmark and find that the models struggle
with generating correct and secure code. In addition, the performance of LLMs
to generate self-contained programs as measured by prior benchmarks do not
translate to comparative performance at generating secure and correct code at
the repository level in SecRepoBench. We show that the state-of-the-art prompt
engineering techniques become less effective when applied to the repository
level secure code generation problem. We conduct extensive experiments,
including an agentic technique to generate secure code, to demonstrate that our
benchmark is currently the most difficult secure coding benchmark, compared to
previous state-of-the-art benchmarks. Finally, our comprehensive analysis
provides insights into potential directions for enhancing the ability of LLMs
to generate correct and secure code in real-world repositories.

</details>


### [245] [Federated One-Shot Learning with Data Privacy and Objective-Hiding](https://arxiv.org/abs/2504.21182)
*Maximilian Egger,Rüdiger Urbanke,Rawad Bitar*

Main category: cs.CR

TL;DR: 提出了一种新方法，同时保护联邦学习中客户端数据和联邦目标隐私，结合知识蒸馏和私有信息检索技术。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中联邦目标隐私保护不足的问题。

Method: 分三阶段：本地计算、客户端共享结果、联邦目标安全检索，结合多方计算和图基私有信息检索。

Result: 方法在适应场景下优于现有工具。

Conclusion: 新方法有效解决了联邦学习中的双重隐私问题。

Abstract: Privacy in federated learning is crucial, encompassing two key aspects:
safeguarding the privacy of clients' data and maintaining the privacy of the
federator's objective from the clients. While the first aspect has been
extensively studied, the second has received much less attention.
  We present a novel approach that addresses both concerns simultaneously,
drawing inspiration from techniques in knowledge distillation and private
information retrieval to provide strong information-theoretic privacy
guarantees.
  Traditional private function computation methods could be used here; however,
they are typically limited to linear or polynomial functions. To overcome these
constraints, our approach unfolds in three stages. In stage 0, clients perform
the necessary computations locally. In stage 1, these results are shared among
the clients, and in stage 2, the federator retrieves its desired objective
without compromising the privacy of the clients' data. The crux of the method
is a carefully designed protocol that combines secret-sharing-based multi-party
computation and a graph-based private information retrieval scheme. We show
that our method outperforms existing tools from the literature when properly
adapted to this setting.

</details>


### [246] [CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks](https://arxiv.org/abs/2504.21228)
*Rui Wang,Junda Wu,Yu Xia,Tong Yu,Ruiyi Zhang,Ryan Rossi,Lina Yao,Julian McAuley*

Main category: cs.CR

TL;DR: 论文提出CachePrune方法，通过修剪KV缓存中的任务触发神经元，防御LLMs的间接提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: LLMs易受间接提示注入攻击，因其无法区分提示中的数据和指令，导致模型偏离用户指令。

Method: 使用基于DPO目标的损失函数进行特征归因，识别并修剪任务触发神经元，避免模型将提示上下文误认为指令。

Result: 实验表明CachePrune显著降低攻击成功率，且不影响响应质量。

Conclusion: CachePrune为开发更安全、鲁棒的AI系统提供了有效防御手段。

Abstract: Large Language Models (LLMs) are identified as being susceptible to indirect
prompt injection attack, where the model undesirably deviates from
user-provided instructions by executing tasks injected in the prompt context.
This vulnerability stems from LLMs' inability to distinguish between data and
instructions within a prompt. In this paper, we propose CachePrune that defends
against this attack by identifying and pruning task-triggering neurons from the
KV cache of the input prompt context. By pruning such neurons, we encourage the
LLM to treat the text spans of input prompt context as only pure data, instead
of any indicator of instruction following. These neurons are identified via
feature attribution with a loss function induced from an upperbound of the
Direct Preference Optimization (DPO) objective. We show that such a loss
function enables effective feature attribution with only a few samples. We
further improve on the quality of feature attribution, by exploiting an
observed triggering effect in instruction following. Our approach does not
impose any formatting on the original prompt or introduce extra test-time LLM
calls. Experiments show that CachePrune significantly reduces attack success
rates without compromising the response quality. Note: This paper aims to
defend against indirect prompt injection attacks, with the goal of developing
more secure and robust AI systems.

</details>


### [247] [Cert-SSB: Toward Certified Sample-Specific Backdoor Defense](https://arxiv.org/abs/2504.21730)
*Ting Qiao,Yingjia Wang,Xing Liu,Sixing Wu,Jianbing Li,Yiming Li*

Main category: cs.CR

TL;DR: 论文提出了一种样本特定的认证后门防御方法Cert-SSB，通过优化每个样本的噪声水平，并结合多个平滑模型的预测，提高了对后门攻击的防御性能。


<details>
  <summary>Details</summary>
Motivation: 现有的随机平滑防御方法假设所有样本与决策边界等距，但实际中这一假设不成立，导致防御性能不佳。

Method: Cert-SSB使用随机梯度上升优化每个样本的噪声水平，训练多个平滑模型，并动态调整认证区域。

Result: 实验表明，Cert-SSB在多个基准数据集上有效提升了防御性能。

Conclusion: Cert-SSB通过样本特定的噪声优化和动态认证方法，显著改进了后门攻击的防御效果。

Abstract: Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an
attacker manipulates a small portion of the training data to implant hidden
backdoors into the model. The compromised model behaves normally on clean
samples but misclassifies backdoored samples into the attacker-specified target
class, posing a significant threat to real-world DNN applications. Currently,
several empirical defense methods have been proposed to mitigate backdoor
attacks, but they are often bypassed by more advanced backdoor techniques. In
contrast, certified defenses based on randomized smoothing have shown promise
by adding random noise to training and testing samples to counteract backdoor
attacks. In this paper, we reveal that existing randomized smoothing defenses
implicitly assume that all samples are equidistant from the decision boundary.
However, it may not hold in practice, leading to suboptimal certification
performance. To address this issue, we propose a sample-specific certified
backdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic
gradient ascent to optimize the noise magnitude for each sample, ensuring a
sample-specific noise level that is then applied to multiple poisoned training
sets to retrain several smoothed models. After that, Cert-SSB aggregates the
predictions of multiple smoothed models to generate the final robust
prediction. In particular, in this case, existing certification methods become
inapplicable since the optimized noise varies across different samples. To
conquer this challenge, we introduce a storage-update-based certification
method, which dynamically adjusts each sample's certification region to improve
certification performance. We conduct extensive experiments on multiple
benchmark datasets, demonstrating the effectiveness of our proposed method. Our
code is available at https://github.com/NcepuQiaoTing/Cert-SSB.

</details>


### [248] [How to Backdoor the Knowledge Distillation](https://arxiv.org/abs/2504.21323)
*Chen Wu,Qian Ma,Prasenjit Mitra,Sencun Zhu*

Main category: cs.CR

TL;DR: 论文挑战了知识蒸馏的传统安全假设，提出了一种新型攻击方法，通过对抗样本隐蔽地植入后门触发器，成功利用干净教师模型中的漏洞。


<details>
  <summary>Details</summary>
Motivation: 传统认为知识蒸馏是安全的，因为教师模型干净且不涉及后门攻击的数据。本文质疑这一假设，探索知识蒸馏过程中的潜在漏洞。

Method: 提出一种新型攻击方法，通过在蒸馏数据集中植入对抗样本和后门触发器，隐蔽地破坏学生模型。

Result: 实验证明该方法具有鲁棒性、隐蔽性和有效性，揭示了知识蒸馏中未被识别的漏洞。

Conclusion: 研究为未来保护知识蒸馏免受后门攻击的研究奠定了基础。

Abstract: Knowledge distillation has become a cornerstone in modern machine learning
systems, celebrated for its ability to transfer knowledge from a large, complex
teacher model to a more efficient student model. Traditionally, this process is
regarded as secure, assuming the teacher model is clean. This belief stems from
conventional backdoor attacks relying on poisoned training data with backdoor
triggers and attacker-chosen labels, which are not involved in the distillation
process. Instead, knowledge distillation uses the outputs of a clean teacher
model to guide the student model, inherently preventing recognition or response
to backdoor triggers as intended by an attacker. In this paper, we challenge
this assumption by introducing a novel attack methodology that strategically
poisons the distillation dataset with adversarial examples embedded with
backdoor triggers. This technique allows for the stealthy compromise of the
student model while maintaining the integrity of the teacher model. Our
innovative approach represents the first successful exploitation of
vulnerabilities within the knowledge distillation process using clean teacher
models. Through extensive experiments conducted across various datasets and
attack settings, we demonstrate the robustness, stealthiness, and effectiveness
of our method. Our findings reveal previously unrecognized vulnerabilities and
pave the way for future research aimed at securing knowledge distillation
processes against backdoor attacks.

</details>


### [249] [Optimizing Mouse Dynamics for User Authentication by Machine Learning: Addressing Data Sufficiency, Accuracy-Practicality Trade-off, and Model Performance Challenges](https://arxiv.org/abs/2504.21415)
*Yi Wang,Chengyv Wu,Yang Liao,Maowei You*

Main category: cs.CR

TL;DR: 提出了一种基于鼠标动态行为的认证方法，通过统计方法和深度学习框架优化数据量和行为模式捕捉，显著提升了认证效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统用户认证方法在可用性、成本和安全性方面存在局限，鼠标动态认证提供了一种低成本、非侵入且适应性强的解决方案，但仍需解决数据量、准确性与实用性平衡等问题。

Method: 使用高斯核密度估计和KL散度确定训练数据量，引入MAU优化行为表示，设计LT-AMouse框架结合1D-ResNet和GRU提取特征和建模时序依赖。

Result: 在Balabit和DFL数据集上显著减少数据规模（DFL减少10倍），认证系统在盲攻击下的AUC达到98.52%（DFL）和94.65%（Balabit），超越现有最优性能。

Conclusion: 提出的方法有效解决了鼠标动态认证中的数据量和行为模式问题，显著提升了认证系统的性能和实用性。

Abstract: User authentication is essential to ensure secure access to computer systems,
yet traditional methods face limitations in usability, cost, and security.
Mouse dynamics authentication, based on the analysis of users' natural
interaction behaviors with mouse devices, offers a cost-effective,
non-intrusive, and adaptable solution. However, challenges remain in
determining the optimal data volume, balancing accuracy and practicality, and
effectively capturing temporal behavioral patterns. In this study, we propose a
statistical method using Gaussian kernel density estimate (KDE) and
Kullback-Leibler (KL) divergence to estimate the sufficient data volume for
training authentication models. We introduce the Mouse Authentication Unit
(MAU), leveraging Approximate Entropy (ApEn) to optimize segment length for
efficient and accurate behavioral representation. Furthermore, we design the
Local-Time Mouse Authentication (LT-AMouse) framework, integrating 1D-ResNet
for local feature extraction and GRU for modeling long-term temporal
dependencies. Taking the Balabit and DFL datasets as examples, we significantly
reduced the data scale, particularly by a factor of 10 for the DFL dataset,
greatly alleviating the training burden. Additionally, we determined the
optimal input recognition unit length for the user authentication system on
different datasets based on the slope of Approximate Entropy. Training with
imbalanced samples, our model achieved a successful defense AUC 98.52% for
blind attack on the DFL dataset and 94.65% on the Balabit dataset, surpassing
the current sota performance.

</details>


### [250] [Traceback of Poisoning Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2504.21668)
*Baolei Zhang,Haoran Xin,Minghong Fang,Zhuqing Liu,Biao Yi,Tong Li,Zheli Liu*

Main category: cs.CR

TL;DR: RAGForensics是一种新型的RAG系统防御机制，通过追溯知识库中的中毒文本来增强安全性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统易受中毒攻击，现有防御措施不足以应对复杂攻击。

Method: RAGForensics通过迭代检索知识库子集，并利用定制提示引导LLM检测中毒文本。

Result: 实验证明RAGForensics能有效对抗先进的中毒攻击。

Conclusion: RAGForensics为RAG系统提供了一种实用的安全增强机制。

Abstract: Large language models (LLMs) integrated with retrieval-augmented generation
(RAG) systems improve accuracy by leveraging external knowledge sources.
However, recent research has revealed RAG's susceptibility to poisoning
attacks, where the attacker injects poisoned texts into the knowledge database,
leading to attacker-desired responses. Existing defenses, which predominantly
focus on inference-time mitigation, have proven insufficient against
sophisticated attacks. In this paper, we introduce RAGForensics, the first
traceback system for RAG, designed to identify poisoned texts within the
knowledge database that are responsible for the attacks. RAGForensics operates
iteratively, first retrieving a subset of texts from the database and then
utilizing a specially crafted prompt to guide an LLM in detecting potential
poisoning texts. Empirical evaluations across multiple datasets demonstrate the
effectiveness of RAGForensics against state-of-the-art poisoning attacks. This
work pioneers the traceback of poisoned texts in RAG systems, providing a
practical and promising defense mechanism to enhance their security.

</details>


### [251] [XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs](https://arxiv.org/abs/2504.21700)
*Marco Arazzi,Vignesh Kumar Kembu,Antonino Nocera,Vinod P*

Main category: cs.CR

TL;DR: 论文提出了一种基于可解释AI的LLM越狱攻击方法XBreaking，通过分析审查机制的行为模式，设计针对性攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在关键应用中的安全威胁限制了其可靠使用，现有越狱方法多为生成-测试策略，缺乏针对性。

Method: 提出XBreaking，利用可解释AI分析审查与未审查模型的行为差异，设计针对性噪声注入攻击。

Result: 实验验证了XBreaking的有效性和性能，揭示了审查机制的行为模式。

Conclusion: XBreaking为LLM安全提供了新见解，展示了针对性攻击的潜力。

Abstract: Large Language Models are fundamental actors in the modern IT landscape
dominated by AI solutions. However, security threats associated with them might
prevent their reliable adoption in critical application scenarios such as
government organizations and medical institutions. For this reason, commercial
LLMs typically undergo a sophisticated censoring mechanism to eliminate any
harmful output they could possibly produce. In response to this, LLM
Jailbreaking is a significant threat to such protections, and many previous
approaches have already demonstrated its effectiveness across diverse domains.
Existing jailbreak proposals mostly adopt a generate-and-test strategy to craft
malicious input. To improve the comprehension of censoring mechanisms and
design a targeted jailbreak attack, we propose an Explainable-AI solution that
comparatively analyzes the behavior of censored and uncensored models to derive
unique exploitable alignment patterns. Then, we propose XBreaking, a novel
jailbreak attack that exploits these unique patterns to break the security
constraints of LLMs by targeted noise injection. Our thorough experimental
campaign returns important insights about the censoring mechanisms and
demonstrates the effectiveness and performance of our attack.

</details>


### [252] [A Comprehensive Study of Exploitable Patterns in Smart Contracts: From Vulnerability to Defense](https://arxiv.org/abs/2504.21480)
*Yuchen Ding,Hongli Peng,Xiaoqi Li*

Main category: cs.CR

TL;DR: 本文分析了以太坊智能合约中的安全风险，重点关注重入和整数溢出漏洞，探讨其机制、攻击场景及防御措施。


<details>
  <summary>Details</summary>
Motivation: 随着区块链技术的发展，智能合约安全问题日益突出，漏洞可能导致重大经济损失，亟需深入研究。

Method: 研究以太坊智能合约（Solidity编写，EVM执行）中的重入和整数溢出漏洞，分析机制、复现攻击场景并评估防御方法。

Result: 揭示了重入和整数溢出漏洞的机制及攻击影响，提出了有效的防御措施。

Conclusion: 智能合约安全风险需持续关注，研究为开发者提供了漏洞防御的实用指导。

Abstract: With the rapid advancement of blockchain technology, smart contracts have
enabled the implementation of increasingly complex functionalities. However,
ensuring the security of smart contracts remains a persistent challenge across
the stages of development, compilation, and execution. Vulnerabilities within
smart contracts not only undermine the security of individual applications but
also pose significant risks to the broader blockchain ecosystem, as
demonstrated by the growing frequency of attacks since 2016, resulting in
substantial financial losses. This paper provides a comprehensive analysis of
key security risks in Ethereum smart contracts, specifically those written in
Solidity and executed on the Ethereum Virtual Machine (EVM). We focus on two
prevalent and critical vulnerability types (reentrancy and integer overflow) by
examining their underlying mechanisms, replicating attack scenarios, and
assessing effective countermeasures.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [253] [Generate-then-Verify: Reconstructing Data from Limited Published Statistics](https://arxiv.org/abs/2504.21199)
*Terrance Liu,Eileen Xiao,Pratiksha Thaker,Adam Smith,Zhiwei Steven Wu*

Main category: stat.ML

TL;DR: 论文研究了从聚合统计数据中重建表格数据的问题，提出了一种部分数据重建方法，即使无法完全重建数据，也能保证部分行或列的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在统计数据丰富时可以完全重建数据，但在统计数据稀疏时无法实现。本文关注后者情况，探索如何在数据稀疏时仍能保证部分数据的准确性。

Method: 提出了一种基于整数规划的方法，首先生成一组声明，然后验证这些声明是否在所有可能的匹配数据集中成立。

Result: 在U.S. Decennial Census的住房微观数据上验证了方法，表明即使数据发布稀疏，隐私泄露风险仍存在。

Conclusion: 部分数据重建方法在统计数据稀疏时仍能有效识别隐私漏洞，为数据发布安全提供了新视角。

Abstract: We study the problem of reconstructing tabular data from aggregate
statistics, in which the attacker aims to identify interesting claims about the
sensitive data that can be verified with 100% certainty given the aggregates.
Successful attempts in prior work have conducted studies in settings where the
set of published statistics is rich enough that entire datasets can be
reconstructed with certainty. In our work, we instead focus on the regime where
many possible datasets match the published statistics, making it impossible to
reconstruct the entire private dataset perfectly (i.e., when approaches in
prior work fail). We propose the problem of partial data reconstruction, in
which the goal of the adversary is to instead output a $\textit{subset}$ of
rows and/or columns that are $\textit{guaranteed to be correct}$. We introduce
a novel integer programming approach that first $\textbf{generates}$ a set of
claims and then $\textbf{verifies}$ whether each claim holds for all possible
datasets consistent with the published aggregates. We evaluate our approach on
the housing-level microdata from the U.S. Decennial Census release,
demonstrating that privacy violations can still persist even when information
published about such data is relatively sparse.

</details>


### [254] [Kernel Density Machines](https://arxiv.org/abs/2504.21419)
*Damir Filipovic,Paul Schneider*

Main category: stat.ML

TL;DR: Kernel density machines (KDM) 是一种新颖的密度比估计器，适用于可数生成可测空间上的概率测度，无需连续性假设或Lebesgue密度存在。通过低秩近似提高计算效率，并具有严格的理论保证。


<details>
  <summary>Details</summary>
Motivation: 提出一种无需严格假设的密度比估计方法，适用于更广泛的概率测度。

Method: 在再生核希尔伯特空间中使用低秩近似，控制误差以提高计算效率。

Result: 理论保证包括渐近一致性、泛函中心极限定理和有限样本误差界，实证结果验证了其有效性和精度。

Conclusion: KDM 是一种高效且理论坚实的密度比估计方法，适用于大规模数据。

Abstract: We introduce kernel density machines (KDM), a novel density ratio estimator
in a reproducing kernel Hilbert space setting. KDM applies to general
probability measures on countably generated measurable spaces without
restrictive assumptions on continuity, or the existence of a Lebesgue density.
For computational efficiency, we incorporate a low-rank approximation with
precisely controlled error that grants scalability to large-sample settings. We
provide rigorous theoretical guarantees, including asymptotic consistency, a
functional central limit theorem, and finite-sample error bounds, establishing
a strong foundation for practical use. Empirical results based on simulated and
real data demonstrate the efficacy and precision of KDM.

</details>


### [255] [Wasserstein-Aitchison GAN for angular measures of multivariate extremes](https://arxiv.org/abs/2504.21438)
*Stéphane Lhaut,Holger Rootzén,Johan Segers*

Main category: stat.ML

TL;DR: 本文提出了一种名为WA-GAN的新方法，用于模拟多维极端事件，并估计其发生概率。该方法结合了极值分析与非参数GAN建模，在模拟和金融数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 经济上负责任地缓解多维极端风险（如极端降雨、股票价格大幅波动等）需要估计这些风险未来发生的概率。

Method: 将观测值转换为单位帕累托尺度，假设其分布是规则变化的，结合极值分析和非参数GAN建模，使用Wasserstein GAN生成新的极端事件模拟值。

Result: WA-GAN在捕捉极端数据依赖结构和生成准确新极端值方面优于现有方法，适用于高达50维的模拟数据和30维金融数据。

Conclusion: WA-GAN为多维极端事件的概率估计提供了一种有效的新方法，具有实际应用潜力。

Abstract: Economically responsible mitigation of multivariate extreme risks -- extreme
rainfall in a large area, huge variations of many stock prices, widespread
breakdowns in transportation systems -- requires estimates of the probabilities
that such risks will materialize in the future. This paper develops a new
method, Wasserstein--Aitchison Generative Adversarial Networks (WA-GAN), which
provides simulated values of future $d$-dimensional multivariate extreme events
and which hence can be used to give estimates of such probabilities. The main
hypothesis is that, after transforming the observations to the unit-Pareto
scale, their distribution is regularly varying in the sense that the
distributions of their radial and angular components (with respect to the
$L_1$-norm) converge and become asymptotically independent as the radius gets
large. The method is a combination of standard extreme value analysis modeling
of the tails of the marginal distributions with nonparametric GAN modeling of
the angular distribution. For the latter, the angular values are transformed to
Aitchison coordinates in a full $(d-1)$-dimensional linear space, and a
Wasserstein GAN is trained on these coordinates and used to generate new
values. A reverse transformation is then applied to these values and gives
simulated values on the original data scale. The method shows good performance
compared to other existing methods in the literature, both in terms of
capturing the dependence structure of the extremes in the data, as well as in
generating accurate new extremes of the data distribution. The comparison is
performed on simulated multivariate extremes from a logistic model in
dimensions up to 50 and on a 30-dimensional financial data set.

</details>


### [256] [A comparison of generative deep learning methods for multivariate angular simulation](https://arxiv.org/abs/2504.21505)
*Jakob Benjamin Wessel,Callum J. R. Murphy-Barltrop,Emma S. Simpson*

Main category: stat.ML

TL;DR: 论文探讨了在多元极值分析中，利用深度学习生成方法（如GAN、标准化流和流匹配）模拟角变量的性能，并与传统的vMF混合模型进行了比较。


<details>
  <summary>Details</summary>
Motivation: 随着多元极值分析中几何和角径向框架的发展，高维角变量的可靠模拟变得重要。传统方法在低维表现良好，但在高维缺乏灵活性和可扩展性。

Method: 研究比较了多种深度学习方法（GAN、标准化流、流匹配）和传统的vMF混合模型，通过多种指标评估性能，并应用于实际气象海洋数据集。

Result: 深度学习方法能够捕捉复杂数据结构，在高维情况下优于传统vMF混合模型。

Conclusion: 深度学习方法在模拟高维角变量时具有潜力，适用于复杂实际数据。

Abstract: With the recent development of new geometric and angular-radial frameworks
for multivariate extremes, reliably simulating from angular variables in
moderate-to-high dimensions is of increasing importance. Empirical approaches
have the benefit of simplicity, and work reasonably well in low dimensions, but
as the number of variables increases, they can lack the required flexibility
and scalability. Classical parametric models for angular variables, such as the
von Mises-Fisher (vMF) distribution, provide an alternative. Exploiting
mixtures of vMF distributions increases their flexibility, but there are cases
where even this is not sufficient to capture the intricate features that can
arise in data. Owing to their flexibility, generative deep learning methods are
able to capture complex data structures; they therefore have the potential to
be useful in the simulation of angular variables. In this paper, we explore a
range of deep learning approaches for this task, including generative
adversarial networks, normalizing flows and flow matching. We assess their
performance via a range of metrics and make comparisons to the more classical
approach of using a mixture of vMF distributions. The methods are also applied
to a metocean data set, demonstrating their applicability to real-world,
complex data structures.

</details>


### [257] [Balancing Interpretability and Flexibility in Modeling Diagnostic Trajectories with an Embedded Neural Hawkes Process Model](https://arxiv.org/abs/2504.21795)
*Yuankang Zhao,Matthew Engelhard*

Main category: stat.ML

TL;DR: 论文提出了一种新型的霍克斯过程（HP）模型，通过事件嵌入空间中的灵活影响核（神经网络实现）来平衡模型的灵活性和可解释性，适用于大规模事件序列。


<details>
  <summary>Details</summary>
Motivation: 传统HP模型通过参数化影响函数实现自增强动态，但灵活性不足；神经网络HP模型虽灵活但牺牲了可解释性，而可解释性在医疗领域至关重要。

Method: 提出一种新型HP模型，利用事件嵌入空间中的神经网络定义灵活影响核，并可添加Transformer层以进一步上下文化事件嵌入。

Result: 模型在模拟中准确恢复影响函数，在MIMIC-IV数据集上表现优异，并在儿童诊断数据集中获得临床有意义的解释。

Conclusion: 灵活影响核能有效捕捉自增强动态，表明在保持性能的同时可维持可解释性。

Abstract: The Hawkes process (HP) is commonly used to model event sequences with
self-reinforcing dynamics, including electronic health records (EHRs).
Traditional HPs capture self-reinforcement via parametric impact functions that
can be inspected to understand how each event modulates the intensity of
others. Neural network-based HPs offer greater flexibility, resulting in
improved fit and prediction performance, but at the cost of interpretability,
which is often critical in healthcare. In this work, we aim to understand and
improve upon this tradeoff. We propose a novel HP formulation in which impact
functions are modeled by defining a flexible impact kernel, instantiated as a
neural network, in event embedding space, which allows us to model large-scale
event sequences with many event types. This approach is more flexible than
traditional HPs yet more interpretable than other neural network approaches,
and allows us to explicitly trade flexibility for interpretability by adding
transformer encoder layers to further contextualize the event embeddings.
Results show that our method accurately recovers impact functions in
simulations, achieves competitive performance on MIMIC-IV procedure dataset,
and gains clinically meaningful interpretation on XX-EHR with children
diagnosis dataset even without transformer layers. This suggests that our
flexible impact kernel is often sufficient to capture self-reinforcing dynamics
in EHRs and other data effectively, implying that interpretability can be
maintained without loss of performance.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [258] [Turning Up the Heat: Assessing 2-m Temperature Forecast Errors in AI Weather Prediction Models During Heat Waves](https://arxiv.org/abs/2504.21195)
*Kelsey E. Ennis,Elizabeth A. Barnes,Marybeth C. Arcodia,Martin A. Fernandez,Eric D. Maloney*

Main category: physics.ao-ph

TL;DR: AIWP模型（如GraphCast和Pangu-Weather）在极端高温预测中表现优于传统NWP模型（UFS GEFS），但仍存在冷偏差。GraphCast在多数情况下表现最佳，为中期和次季节尺度极端高温预测提供了潜力。


<details>
  <summary>Details</summary>
Motivation: 极端高温是美国最致命的天气灾害，且其强度、频率和持续时间不断增加。传统NWP模型在中期和次季节尺度预测中表现不佳，而AIWP模型的潜力尚不明确。

Method: 研究比较了两种AIWP模型（GraphCast和Pangu-Weather）和一种传统NWP模型（UFS GEFS）在60次极端高温事件中的2米温度预测能力，时间跨度为20天。

Result: AIWP模型在多数情况下优于UFS GEFS，但存在冷偏差。GraphCast表现最佳，Pangu-Weather在冬季表现出暖偏差。

Conclusion: AIWP模型在极端高温的中期和次季节预测中具有潜力，但仍需改进偏差问题。

Abstract: Extreme heat is the deadliest weather-related hazard in the United States.
Furthermore, it is increasing in intensity, frequency, and duration, making
skillful forecasts vital to protecting life and property. Traditional numerical
weather prediction (NWP) models struggle with extreme heat for medium-range and
subseasonal-to-seasonal (S2S) timescales. Meanwhile, artificial
intelligence-based weather prediction (AIWP) models are progressing rapidly.
However, it is largely unknown how well AIWP models forecast extremes,
especially for medium-range and S2S timescales. This study investigates 2-m
temperature forecasts for 60 heat waves across the four boreal seasons and over
four CONUS regions at lead times up to 20 days, using two AIWP models (Google
GraphCast and Pangu-Weather) and one traditional NWP model (NOAA United
Forecast System Global Ensemble Forecast System (UFS GEFS)). First, case study
analyses show that both AIWP models and the UFS GEFS exhibit consistent cold
biases on regional scales in the 5-10 days of lead time before heat wave onset.
GraphCast is the more skillful AIWP model, outperforming UFS GEFS and
Pangu-Weather in most locations. Next, the two AIWP models are isolated and
analyzed across all heat waves and seasons, with events split among the model's
testing (2018-2023) and training (1979-2017) periods. There are cold biases
before and during the heat waves in both models and all seasons, except
Pangu-Weather in winter, which exhibits a mean warm bias before heat wave
onset. Overall, results offer encouragement that AIWP models may be useful for
medium-range and S2S predictability of extreme heat.

</details>
